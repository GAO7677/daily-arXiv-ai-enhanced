<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents](https://arxiv.org/abs/2506.14866)
*Thomas Kuntz,Agatha Duzan,Hao Zhao,Francesco Croce,Zico Kolter,Nicolas Flammarion,Maksym Andriushchenko*

Main category: cs.SE

TL;DR: The paper introduces OS-Harm, a benchmark for evaluating the safety of LLM-based computer use agents. Evaluation shows current models are vulnerable to misuse and attacks, highlighting the need for better safety measures.


<details>
  <summary>Details</summary>
Motivation: As LLM-based computer use agents become more popular and can directly interact with graphical user interfaces, ensuring their safe behavior is increasingly important. However, little attention has been paid to their safety, and there is a lack of thorough evaluation methods for potential harm caused by such systems.

Method: The authors introduce OS-Harm, a benchmark designed specifically to test the safety of computer use agents. Built on the OSWorld environment, OS-Harm evaluates agents using 150 tasks across different harm categories: user misuse, prompt injection, and model misbehavior. It tests agents in various OS applications and includes an automated judge with high agreement with human assessors to measure both accuracy and safety.

Result: Frontier models like o4-mini, Claude 3.7 Sonnet, and Gemini 2.5 Pro were evaluated using OS-Harm. All models showed a tendency to comply with harmful user queries, were susceptible to prompt injection attacks, and sometimes exhibited unsafe behaviors. The automated judge performed well, with F1 scores of 0.76 (accuracy) and 0.79 (safety).

Conclusion: Computer use agents, even those based on advanced LLMs, have significant safety vulnerabilities. OS-Harm provides a much-needed framework for systematically evaluating and understanding these risks to inform future improvements in agent design and deployment.

Abstract: Computer use agents are LLM-based agents that can directly interact with a
graphical user interface, by processing screenshots or accessibility trees.
While these systems are gaining popularity, their safety has been largely
overlooked, despite the fact that evaluating and understanding their potential
for harmful behavior is essential for widespread adoption. To address this gap,
we introduce OS-Harm, a new benchmark for measuring safety of computer use
agents. OS-Harm is built on top of the OSWorld environment and aims to test
models across three categories of harm: deliberate user misuse, prompt
injection attacks, and model misbehavior. To cover these cases, we create 150
tasks that span several types of safety violations (harassment, copyright
infringement, disinformation, data exfiltration, etc.) and require the agent to
interact with a variety of OS applications (email client, code editor, browser,
etc.). Moreover, we propose an automated judge to evaluate both accuracy and
safety of agents that achieves high agreement with human annotations (0.76 and
0.79 F1 score). We evaluate computer use agents based on a range of frontier
models - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide
insights into their safety. In particular, all models tend to directly comply
with many deliberate misuse queries, are relatively vulnerable to static prompt
injections, and occasionally perform unsafe actions. The OS-Harm benchmark is
available at https://github.com/tml-epfl/os-harm.

</details>


### [2] [An Empirical Study of Bugs in Data Visualization Libraries](https://arxiv.org/abs/2506.15084)
*Weiqi Lu,Yongqiang Tian,Xiaohan Zhong,Haoyang Ma,Zhenyang Xu,Shing-Chi Cheung,Chengnian Sun*

Main category: cs.SE

TL;DR: The paper provides a comprehensive analysis of 564 bugs from five major data visualization libraries, finding that inaccurate visualizations are prevalent and primarily due to graphic computation errors. It categorizes bugs, pinpoints root causes, suggests new test methods, and evaluates Vision Language Models for bug detection, noting variable effectiveness. This research establishes a foundational understanding and highlights directions for improved testing and automation in DataViz libraries.


<details>
  <summary>Details</summary>
Motivation: Data visualization libraries are central to accurate information presentation and decision-making, yet visual bugs can mislead users without crashing the software. A deep understanding of the unique nature and root causes of such bugs in DataViz libraries is lacking, impeding effective detection and resolution.

Method: Analyzed 564 bug reports from five popular DataViz libraries. Developed a taxonomy by systematically categorizing the bugs based on their symptoms and root causes. Identified common triggering steps and evaluation methods (test oracles). Investigated the potential of Vision Language Models (VLMs) for automated bug detection.

Result: Discovered that incorrect and inaccurate plots are common, with incorrect graphic computation as the main root cause. Eight trigger steps and two DataViz-specific test oracles were documented. Effectiveness of VLMs in detecting bugs ranged from 29% to 57%; richer prompts did not consistently boost performance.

Conclusion: This work offers the first detailed taxonomy of bugs in DataViz libraries and highlights the need for improved automated testing tailored to their unique error patterns. VLMs show promise for automated detection but current performance varies. These findings lay groundwork for better bug detection tools and future research in DataViz software quality.

Abstract: Data visualization (DataViz) libraries play a crucial role in presentation,
data analysis, and application development, underscoring the importance of
their accuracy in transforming data into visual representations. Incorrect
visualizations can adversely impact user experience, distort information
conveyance, and influence user perception and decision-making processes. Visual
bugs in these libraries can be particularly insidious as they may not cause
obvious errors like crashes, but instead mislead users of the underlying data
graphically, resulting in wrong decision making. Consequently, a good
understanding of the unique characteristics of bugs in DataViz libraries is
essential for researchers and developers to detect and fix bugs in DataViz
libraries.
  This study presents the first comprehensive analysis of bugs in DataViz
libraries, examining 564 bugs collected from five widely-used libraries. Our
study systematically analyzes their symptoms and root causes, and provides a
detailed taxonomy. We found that incorrect/inaccurate plots are pervasive in
DataViz libraries and incorrect graphic computation is the major root cause,
which necessitates further automated testing methods for DataViz libraries.
Moreover, we identified eight key steps to trigger such bugs and two test
oracles specific to DataViz libraries, which may inspire future research in
designing effective automated testing techniques. Furthermore, with the recent
advancements in Vision Language Models (VLMs), we explored the feasibility of
applying these models to detect incorrect/inaccurate plots. The results show
that the effectiveness of VLMs in bug detection varies from 29% to 57%,
depending on the prompts, and adding more information in prompts does not
necessarily increase the effectiveness. More findings can be found in our
manuscript.

</details>


### [3] [Program Feature-based Fuzzing Benchmarking](https://arxiv.org/abs/2506.15088)
*Miao Miao*

Main category: cs.SE

TL;DR: The paper introduces a new benchmark for fuzzing that allows for control over fine-grained program features. Testing 11 fuzzers revealed that these features greatly influence performance, suggesting future fuzzing evaluations should account for such details.


<details>
  <summary>Details</summary>
Motivation: Traditional fuzzing benchmarks often overlook the influence of specific program features on fuzzing effectiveness.

Method: The authors identified 7 key program features from 25 recent grey-box fuzzing studies and used them to generate a benchmark set of 153 programs, each with 10 configurable parameters. They evaluated 11 widely-used fuzzers using this benchmark.

Result: Fuzzer performance significantly varies depending on detailed program features and their configurations.

Conclusion: Fuzzing evaluations should incorporate fine-grained program characteristics, as these features substantially affect fuzzer effectiveness.

Abstract: Fuzzing is a powerful software testing technique renowned for its
effectiveness in identifying software vulnerabilities. Traditional fuzzing
evaluations typically focus on overall fuzzer performance across a set of
target programs, yet few benchmarks consider how fine-grained program features
influence fuzzing effectiveness. To bridge this gap, we introduce a novel
benchmark designed to generate programs with configurable, fine-grained program
features to enhance fuzzing evaluations. We reviewed 25 recent grey-box fuzzing
studies, extracting 7 program features related to control-flow and data-flow
that can impact fuzzer performance. Using these features, we generated a
benchmark consisting of 153 programs controlled by 10 fine-grained configurable
parameters. We evaluated 11 popular fuzzers using this benchmark. The results
indicate that fuzzer performance varies significantly based on the program
features and their strengths, highlighting the importance of incorporating
program characteristics into fuzzing evaluations.

</details>


### [4] [Enhancement Report Approval Prediction: A Comparative Study of Large Language Models](https://arxiv.org/abs/2506.15098)
*Haosheng Zuo,Feifei Niu,Chuanyi Li*

Main category: cs.SE

TL;DR: Large language models outperform traditional machine learning in predicting approval of software enhancement reports. Fine-tuning and using creator profiles boost accuracy, with Llama 3.1 8B Instruct reaching top results. LLMs can streamline software improvement processes, though some challenging cases remain for future study.


<details>
  <summary>Details</summary>
Motivation: Enhancement reports are essential for communication between users and developers, but processing them manually is time-consuming and inefficient. There is a need to automate the process to prevent delays and loss of valuable suggestions. Enhancement report approval prediction (ERAP) using machine learning offers a solution.

Method: This study systematically evaluates 18 variants of large language models (LLMs), including encoder models (BERT, RoBERTa, DeBERTa-v3, ELECTRA, XLNet) and decoder models (GPT-3.5-turbo, GPT-4o-mini, Llama 3.1 8B, etc.), comparing them against traditional machine learning approaches such as CNN/LSTM using BERT/GloVe. The study also explores the impact of incorporating creator profiles and utilizing LoRA fine-tuning on performance.

Result: (1) Including creator profiles improves accuracy by 10.8% in unfine-tuned decoder-only models, though it may cause bias. (2) LoRA fine-tuned Llama 3.1 8B Instruct achieves 79% accuracy and 76.1% recall for approved reports, outperforming traditional methods by 5% and addressing class imbalance. The study also examines cases where LLMs underperform, offering insights for future work.

Conclusion: LLMs, particularly when fine-tuned and supplemented with creator profiles, significantly enhance ERAP accuracy and recall, surpassing traditional approaches. This positions LLMs as superior tools for automating enhancement report processing, improving efficiency in software maintenance. Further research should address cases where large models perform poorly.

Abstract: Enhancement reports (ERs) serve as a critical communication channel between
users and developers, capturing valuable suggestions for software improvement.
However, manually processing these reports is resource-intensive, leading to
delays and potential loss of valuable insights. To address this challenge,
enhancement report approval prediction (ERAP) has emerged as a research focus,
leveraging machine learning techniques to automate decision-making. While
traditional approaches have employed feature-based classifiers and deep
learning models, recent advancements in large language models (LLM) present new
opportunities for enhancing prediction accuracy. This study systematically
evaluates 18 LLM variants (including BERT, RoBERTa, DeBERTa-v3, ELECTRA, and
XLNet for encoder models; GPT-3.5-turbo, GPT-4o-mini, Llama 3.1 8B, Llama 3.1
8B Instruct and DeepSeek-V3 for decoder models) against traditional methods
(CNN/LSTM-BERT/GloVe). Our experiments reveal two key insights: (1)
Incorporating creator profiles increases unfine-tuned decoder-only models'
overall accuracy by 10.8 percent though it may introduce bias; (2) LoRA
fine-tuned Llama 3.1 8B Instruct further improve performance, reaching 79
percent accuracy and significantly enhancing recall for approved reports (76.1
percent vs. LSTM-GLOVE's 64.1 percent), outperforming traditional methods by 5
percent under strict chronological evaluation and effectively addressing class
imbalance issues. These findings establish LLM as a superior solution for ERAP,
demonstrating their potential to streamline software maintenance workflows and
improve decision-making in real-world development environments. We also
investigated and summarized the ER cases where the large models underperformed,
providing valuable directions for future research.

</details>


### [5] [Towards Bug-Free Distributed Go Programs](https://arxiv.org/abs/2506.15135)
*Zhengqun Koo*

Main category: cs.SE

TL;DR: The paper introduces a verification framework that statically detects and prevents communication races in message-passing Go programs, making distributed systems more reliable.


<details>
  <summary>Details</summary>
Motivation: Reasoning about concurrency in distributed systems is challenging and often leads to unexpected race conditions, which are difficult to debug and fix. Existing research primarily addresses data races in shared memory systems, but similar issues, called communication races, arise in message-passing systems.

Method: The authors present a verification framework that can statically prove the absence of communication races in distributed programs. This framework is tailored for programs using a subset of the Go programming language, leveraging an extended happens-before model to account for both buffered and unbuffered channels.

Result: The framework enables static reasoning about the execution of distributed Go programs, showing that communication race conditions can be detected and ruled out prior to program execution.

Conclusion: The work provides an effective method for ensuring that distributed Go programs do not suffer from communication races, enhancing reliability and correctness where synchronization is handled by message passing.

Abstract: Programmers of distributed systems need to reason about concurrency to avoid
races. However, reasoning about concurrency is difficult, and unexpected races
show up as bugs. Data race detection in shared memory systems is well-studied
(dynamic data race detection [13], behavioral types [15], dynamic race
detection [31]). Similar to how a data race consists of reads and writes not
related by happens-before at a shared memory location, a communication race
consists of receives and sends not related by happens-before on a shared
channel. Communication races are problematic: a receiver expects a specific
message from a specific sender, but with a communication race, the receiver can
receive a message meant for another receiver, or not receive anything at all.
In this work, we describe a verification framework that can prove the absence
of communication races for distributed programs that use a subset of the Go
programming language, where synchronization is mainly achieved via message
passing. We statically reason about how a distributed program executes, using a
happens-before order, extended to buffered and unbuffered channels.

</details>


### [6] [Advanced approach for Agile/Scrum Process: RetroAI++](https://arxiv.org/abs/2506.15172)
*Maria Spichkova,Kevin Iwan,Madeleine Zwart,Hina Lee,Yuwon Yoon,Xiaohan Qin*

Main category: cs.SE

TL;DR: The paper introduces RetroAI++, an AI-powered tool that automates and enhances sprint planning and retrospectives in Agile/Scrum, offering smart suggestions and insights to improve software project management.


<details>
  <summary>Details</summary>
Motivation: Agile/Scrum projects rely on effective sprint planning and retrospective analysis for success, but these activities can be time-consuming and may benefit from automation and intelligent support.

Method: The authors developed a prototype tool called RetroAI++ that utilizes emerging AI technologies to automate and enhance sprint planning and retrospectives in Agile/Scrum processes.

Result: The RetroAI++ prototype provides intelligent suggestions to organize sprints and offers insightful reflections for retrospectives, aiming to streamline and improve these key Agile/Scrum stages.

Conclusion: Integrating AI-driven automation and insights through RetroAI++ can significantly support Agile/Scrum software teams in planning and reflecting on their work, enhancing overall project management efficiency.

Abstract: In Agile/Scrum software development, sprint planning and retrospective
analysis are the key elements of project management. The aim of our work is to
support software developers in these activities. In this paper, we present our
prototype tool RetroAI++, based on emerging intelligent technologies. In our
RetroAI++ prototype, we aim to automate and refine the practical application of
Agile/Scrum processes within Sprint Planning and Retrospectives. Leveraging AI
insights, our prototype aims to automate and refine the many processes involved
in the Sprint Planning, Development and Retrospective stages of Agile/Scrum
development projects, offering intelligent suggestions for sprint organisation
as well as meaningful insights for retrospective reflection.

</details>


### [7] [Large Language Models for Unit Testing: A Systematic Literature Review](https://arxiv.org/abs/2506.15227)
*Quanjun Zhang,Chunrong Fang,Siqi Gu,Ye Shang,Zhenyu Chen,Liang Xiao*

Main category: cs.SE

TL;DR: This paper systematically reviews how Large Language Models (LLMs) are applied in unit testing, categorizes the tasks LLMs support, discusses integration methods and challenges, and provides guidance for future research. It serves as a foundational resource for researchers exploring LLM-driven unit testing.


<details>
  <summary>Details</summary>
Motivation: Recent advancements in Large Language Models (LLMs) have shown promise in automating unit testing tasks, but the research field is rapidly evolving and fragmented. There is a need for a comprehensive synthesis to help researchers understand current achievements, existing challenges, and future opportunities in LLM-based unit testing.

Method: The paper conducts a systematic literature review of research published up to March 2025 on the application of LLMs to unit testing. It analyzes relevant papers from the dual perspectives of unit testing paradigms and LLM capabilities, categorizes unit testing tasks utilizing LLMs, and examines integration strategies and model adaptations.

Result: The review identifies various unit testing tasks, such as test case and oracle generation, that have benefited from LLM integration. It discusses integration methods, adaptation strategies, and hybrid solutions, summarizes key unresolved challenges in the field, and outlines future research directions.

Conclusion: This systematic review offers the first comprehensive overview of how LLMs are utilized in unit testing, highlighting progress, unresolved issues, and guiding future research initiatives. The artifacts supporting the review are made publicly accessible online.

Abstract: Unit testing is a fundamental practice in modern software engineering, with
the aim of ensuring the correctness, maintainability, and reliability of
individual software components. Very recently, with the advances in Large
Language Models (LLMs), a rapidly growing body of research has leveraged LLMs
to automate various unit testing tasks, demonstrating remarkable performance
and significantly reducing manual effort. However, due to ongoing explorations
in the LLM-based unit testing field, it is challenging for researchers to
understand existing achievements, open challenges, and future opportunities.
This paper presents the first systematic literature review on the application
of LLMs in unit testing until March 2025. We analyze \numpaper{} relevant
papers from the perspectives of both unit testing and LLMs. We first categorize
existing unit testing tasks that benefit from LLMs, e.g., test generation and
oracle generation. We then discuss several critical aspects of integrating LLMs
into unit testing research, including model usage, adaptation strategies, and
hybrid approaches. We further summarize key challenges that remain unresolved
and outline promising directions to guide future research in this area.
Overall, our paper provides a systematic overview of the research landscape to
the unit testing community, helping researchers gain a comprehensive
understanding of achievements and promote future research. Our artifacts are
publicly available at the GitHub repository:
https://github.com/iSEngLab/AwesomeLLM4UT.

</details>


### [8] [Uncovering Intention through LLM-Driven Code Snippet Description Generation](https://arxiv.org/abs/2506.15453)
*Yusuf Sulistyo Nugroho,Farah Danisha Salam,Brittany Reid,Raula Gaikovina Kula,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: This paper analyzes code snippet documentation using a large dataset and measures how well the Llama LLM can generate relevant descriptions. While Llama often captures the main intent, there is still considerable room for improvement in generating precise and relevant documentation.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to improve documentation of code snippets, particularly for third-party libraries, as such documentation is essential for developers and users to understand usage and intent of APIs and code examples. With the rise of Large Language Models (LLMs), the authors are interested in assessing whether LLMs can generate useful code snippet descriptions.

Method: The authors utilize the NPM Code Snippets dataset, which includes over 1 million code snippets from 185,412 packages. They sample 400 code snippets and their respective descriptions, conduct manual classification of these descriptions, and then use the Llama LLM to generate descriptions. They compare the LLM's output to the originals, and analyze both the classification and textual similarity scores.

Result: Manual classification revealed that most original descriptions (55.5%) are example-based. The LLM identified 79.75% as 'Example', closely aligning with human classification, but showed a tendency to generalize. The average similarity score between generated and original descriptions was 0.7173, indicating relevance but also significant room for improvement, as scores below 0.9 suggest some degree of irrelevance.

Conclusion: The study finds that LLMs like Llama can generalize and identify key intents such as usage examples in documentation, but their generated descriptions are not always sufficiently relevant or specific. Depending on the code snippet, documentation intent varies (usage, installation, or learning), and LLMs currently do not capture all nuances needed for fully effective documentation.

Abstract: Documenting code snippets is essential to pinpoint key areas where both
developers and users should pay attention. Examples include usage examples and
other Application Programming Interfaces (APIs), which are especially important
for third-party libraries. With the rise of Large Language Models (LLMs), the
key goal is to investigate the kinds of description developers commonly use and
evaluate how well an LLM, in this case Llama, can support description
generation. We use NPM Code Snippets, consisting of 185,412 packages with
1,024,579 code snippets. From there, we use 400 code snippets (and their
descriptions) as samples. First, our manual classification found that the
majority of original descriptions (55.5%) highlight example-based usage. This
finding emphasizes the importance of clear documentation, as some descriptions
lacked sufficient detail to convey intent. Second, the LLM correctly identified
the majority of original descriptions as "Example" (79.75%), which is identical
to our manual finding, showing a propensity for generalization. Third, compared
to the originals, the produced description had an average similarity score of
0.7173, suggesting relevance but room for improvement. Scores below 0.9
indicate some irrelevance. Our results show that depending on the task of the
code snippet, the intention of the document may differ from being instructions
for usage, installations, or descriptive learning examples for any user of a
library.

</details>


### [9] [cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree](https://arxiv.org/abs/2506.15655)
*Yilin Zhang,Xinran Zhao,Zora Zhiruo Wang,Chenyang Yang,Jiayi Wei,Tongshuang Wu*

Main category: cs.SE

TL;DR: Chunking code for retrieval using Abstract Syntax Trees instead of simple lines preserves semantics, significantly improving performance in code generation tasks.


<details>
  <summary>Details</summary>
Motivation: Retrieval-Augmented Generation (RAG) has become crucial for code generation, relying on external code corpora for more accurate predictions. However, an important aspect—how to best chunk code for retrieval—remains underexplored. Existing line-based chunking often disrupts code structure and semantics, negatively impacting generation quality.

Method: The authors introduce a structure-aware chunking method based on Abstract Syntax Trees (ASTs). Their technique recursively splits large AST nodes and merges sibling nodes within size limits, ensuring the resulting chunks remain semantically coherent and self-contained across languages and tasks.

Result: This chunking method outperforms traditional heuristics, leading to improved results in code generation benchmarks: a Recall@5 increase of 4.3 points on RepoEval retrieval and a Pass@1 increase of 2.67 points on SWE-bench generation tasks.

Conclusion: Structure-aware chunking using ASTs is vital for effective, scalable retrieval-augmented code generation. Their results demonstrate the superiority of this approach, emphasizing the necessity of semantic-preserving document chunking in RAG pipelines for code intelligence.

Abstract: Retrieval-Augmented Generation (RAG) has become essential for large-scale
code generation, grounding predictions in external code corpora to improve
actuality. However, a critical yet underexplored aspect of RAG pipelines is
chunking -- the process of dividing documents into retrievable units. Existing
line-based chunking heuristics often break semantic structures, splitting
functions or merging unrelated code, which can degrade generation quality. We
propose chunking via Abstract Syntax Trees (\ourwork), a structure-aware method
that recursively breaks large AST nodes into smaller chunks and merges sibling
nodes while respecting size limits. This approach generates self-contained,
semantically coherent units across programming languages and tasks, improving
performance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3
points on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation.
Our work highlights the importance of structure-aware chunking for scaling
retrieval-enhanced code intelligence.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [10] [A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in GPUs](https://arxiv.org/abs/2506.15174)
*Hossein Albakri,Kazem Cheshmi*

Main category: cs.PL

TL;DR: A new compiler transformation called enumerate-and-sparse-coarsen helps GPUs handle sparse neural networks much faster (about 2x speedup), by making better use of memory and computing resources during sparse matrix multiplication.


<details>
  <summary>Details</summary>
Motivation: Sparse data structures help reduce the memory footprint in neural networks, but they cause irregular memory access patterns that prevent efficient use of GPU resources, leading to slow computation. This bottleneck motivates the search for more efficient methods for sparse matrix operations on GPUs.

Method: The paper introduces a compiler transformation called enumerate-and-sparse-coarsen, which is designed to accelerate sparse matrix-matrix multiplication (SPMM) on GPUs by improving data reuse in registers and caches and creating more balanced GPU workloads.

Result: The proposed transformation, when tested on sparse convolutional and transformer neural network models using an Nvidia A100 GPU, achieves a geometric mean speedup of 1.84x to 2.27x compared to standard cuBLAS and cuSPARSE library baselines.

Conclusion: The enumerate-and-sparse-coarsen compiler transformation significantly improves the performance of sparse matrix-matrix multiplication on GPUs by addressing key inefficiencies in data reuse and workload balancing, resulting in substantial speedups for neural network applications.

Abstract: Sparse data structures are commonly used in neural networks to reduce the
memory footprint. These data structures are compact but cause irregularities
such as random memory accesses, which prevent efficient use of the memory
hierarchy. GPUs are a common platform for machine learning practitioners, but
running compact data structures on these devices often leads to slow-downs due
to inefficient use of computing and memory resources. This paper proposes a new
compiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse
matrix-matrix multiplication (SPMM) on GPU devices. The transformation
increases data reuse in registers and caches while creating more balanced
workloads for GPU computing resources. The transformation is tested on sparse
neural networks in convolutional and transformer models. On an A100 GPU and
across a columns of matrix B (bCols) in $ A \times B = C$ from range of 32 to
128, the transformation yields a geometric mean speedup of 1.84$\times$ to
2.27$\times$ compared to cuBLAS and cuSPARSE baselines, respectively.

</details>


### [11] [PSM: Policy Synchronised Deterministic Memory](https://arxiv.org/abs/2506.15424)
*Michael Mendler,Marc Pouzet*

Main category: cs.PL

TL;DR: The paper introduces PSM, a new concurrency abstraction for Haskell that enables concurrent, imperative, shareable, and deterministically-behaving data structures by coordinating memory access policies, overcoming previous limitations in existing abstractions.


<details>
  <summary>Details</summary>
Motivation: Balancing concurrency and determinacy is challenging in functional programming languages like Haskell, especially when resources are shared. Existing abstractions either guarantee determinacy with no destructive updates or allow destructive updates with no determinacy guarantee. There is a need for higher-level abstractions that can ensure both properties.

Method: The authors propose a new type context called PSM (policy synchronised memory) for Haskell. PSM allows computations to access and update persistent state with imperative-style side effects, while supporting concurrent threads and shared state. PSM coordinates concurrent accesses using policies, ensuring race-freedom and determinacy by construction. Well-typed transactions within PSM allow creation of concurrent, imperative, and deterministically-behaving shared data structures.

Result: PSM provides a framework where abstract data structures can be shared and updated concurrently in Haskell programs with the assurance of determinacy and race-freedom. This overcomes the limitations seen in previous concurrency abstractions (like Par, LVar, MVar, and TVar).

Conclusion: The PSM context in Haskell successfully enables imperative-style, concurrently shareable, and deterministically-behaving abstract data structures, addressing the inadequacies of current abstractions in balancing concurrency and determinacy.

Abstract: Concurrency and determinacy do not go well with each other when resources
must be shared. Haskell provides parallel programming abstractions such as IVar
and LVar in the Par monad and concurrent abstractions such as MVar and TVar in
the in IO and STM monads, respectively. The former are determinate but have no
destructive updates and the latter have destructive updates but do not
guarantee determinacy. Programming patterns that are both concurrent and
determinate, such as those provided by Kahn or Berry require memory
abstractions at a higher level than is currently available. In this paper we
describe a new type context PSM for policy synchronised memory in Haskell. Like
STM and IO, the computations in PSM can access persistent state and, as a
side-effect, update the memory in imperative style. Like the Par and IO monads,
PSM supports concurrent threads and shared state. However, in contrast to IO,
our PSM contexts are race-free since concurrent accesses are policy coordinated
which guarantees determinacy.Well-typed transactions in the PSM context can
accommodate abstract data structures that are imperative, concurrently
shareable and still behave deterministically, by construction.

</details>
