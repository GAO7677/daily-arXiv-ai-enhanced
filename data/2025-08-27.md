<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Training Language Model Agents to Find Vulnerabilities with CTF-Dojo](https://arxiv.org/abs/2508.18370)
*Terry Yue Zhuo,Dingmin Wang,Hantian Ding,Varun Kumar,Zijian Wang*

Main category: cs.SE

TL;DR: CTF-Dojo provides a large-scale, automated, and reproducible environment for training LLM agents through verifiable execution feedback, leading to state-of-the-art results in CTF-based software engineering benchmarks that approach the performance of proprietary frontier models.


<details>
  <summary>Details</summary>
Motivation: Current training of LLMs with executable environments is limited by the scarcity and inflexibility of such platforms. There is a need for scalable, reproducible environments for verifiable feedback to develop more capable machine learning agents without heavy manual effort or proprietary data.

Method: The authors developed CTF-Dojo, an executable runtime comprising 658 Docker-based CTF challenges, and CTF-Forge, an automated tool for transforming public artifacts into execution environments. They trained LLM agents with execution-verified feedback and evaluated them against established CTF benchmarks.

Result: Training on 486 high-quality trajectories from CTF-Dojo yielded up to 11.6% absolute improvement over strong baselines in three benchmarks, with the top model achieving 31.9% Pass@1. This performance rivals that of leading closed-source models, demonstrating the platform's effectiveness.

Conclusion: Execution-grounded training signals using CTF-style challenges can significantly improve the performance of large language model agents, achieving results competitive with frontier proprietary models.

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities when
trained within executable runtime environments, notably excelling at software
engineering tasks through verified feedback loops. Yet, scalable and
generalizable execution-grounded environments remain scarce, limiting progress
in training more capable ML agents. We introduce CTF-Dojo, the first
large-scale executable runtime tailored for training LLMs with verifiable
feedback, featuring 658 fully functional Capture-The-Flag (CTF)-style
challenges containerized in Docker with guaranteed reproducibility. To enable
rapid scaling without manual intervention, we develop CTF-Forge, an automated
pipeline that transforms publicly available artifacts into ready-to-use
execution environments in minutes, eliminating weeks of expert configuration
traditionally required. We trained LLM-based agents on just 486 high-quality,
execution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute
gains over strong baselines across three competitive benchmarks: InterCode-CTF,
NYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1,
establishing a new open-weight state-of-the-art that rivals frontier models
like DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a
benchmark for executable-agent learning, CTF-Dojo demonstrates that
execution-grounded training signals are not only effective but pivotal in
advancing high-performance ML agents without dependence on costly proprietary
systems.

</details>


### [2] [DTInsight: A Tool for Explicit, Interactive, and Continuous Digital Twin Reporting](https://arxiv.org/abs/2508.18431)
*Kérian Fiter,Louis Malassigné-Onfroy,Bentley Oakes*

Main category: cs.SE

TL;DR: DTInsight is a tool for automatically and continuously reporting the state of Digital Twin systems, featuring visualization, summary generation, and CI/CD integration, thus helping stakeholders maintain up-to-date understanding.


<details>
  <summary>Details</summary>
Motivation: As Digital Twin systems evolve over time, stakeholders need effective tools to track their current characteristics and architecture for better understanding and decision-making.

Method: The paper introduces DTInsight, which systematically and automatically produces continuous reports for Digital Twins using a DT Description Framework. DTInsight features interactive architecture visualization, summary generation using ontological data, and integration with CI/CD pipelines.

Result: DTInsight provides stakeholders with real-time, detailed, and easily accessible reports that improve their understanding of Digital Twin systems.

Conclusion: DTInsight effectively supports stakeholders by delivering timely, thorough insights into evolving Digital Twin systems through automated reporting and visualization.

Abstract: With Digital Twin (DT) construction and evolution occurring over time,
stakeholders require tools to understand the current characteristics and
conceptual architecture of the system at any time. We introduce DTInsight, a
systematic and automated tool and methodology for producing continuous
reporting for DTs. DTInsight offers three key features: (a) an interactive
conceptual architecture visualization of DTs; (b) generation of summaries of DT
characteristics based on ontological data; and (c) integration of these outputs
into a reporting page within a continuous integration and continuous deployment
(CI/CD) pipeline. Given a modeled description of the DT aligning to our DT
Description Framework (DTDF), DTInsight enables up-to-date and detailed reports
for enhanced stakeholder understanding.

</details>


### [3] [Engineering a Digital Twin for the Monitoring and Control of Beer Fermentation Sampling](https://arxiv.org/abs/2508.18452)
*Pierre-Emmanuel Goffi,Raphaël Tremblay,Bentley Oakes*

Main category: cs.SE

TL;DR: This paper reports on engineering an interactive, safety-critical digital twin for beer fermentation, reducing manual work by 91%. It details a stepwise methodology and integration strategies, emphasizing safety, simulation, and collaboration. Findings offer practical guidance for developing real-time, controllable DTs in complex industrial settings.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the complexity of engineering interactive digital twins (DTs) in industrial settings, particularly beyond mere passive monitoring, and to enhance operational efficiency and safety for processes such as beer fermentation.

Method: A systematic methodology was used, featuring a three-phase engineering approach that transitions from passive monitoring to an interactive Type 2 DT. This system integrates hardware (Arduino controllers) and software (Unity visualization), employs multi-layered safety protocols, and applies real-time synchronization and the constellation reporting framework for cross-domain collaboration.

Result: The developed DT system achieved a 91% reduction in manual sampling time and successfully implemented real-time control capabilities for a high-pressure system. The project overcame challenges related to interdisciplinary integration and demonstrated the importance of safety-first and simulation-driven approaches.

Conclusion: The study offers practical, actionable strategies for creating bidirectional, safety-critical DTs, emphasizing the value of safety, simulation, and progressive implementation. The experience and methodology provide guidance for practitioners developing similar systems with real-time interactive controls in industrial environments.

Abstract: Successfully engineering interactive industrial DTs is a complex task,
especially when implementing services beyond passive monitoring. We present
here an experience report on engineering a safety-critical digital twin (DT)
for beer fermentation monitoring, which provides continual sampling and reduces
manual sampling time by 91%. We document our systematic methodology and
practical solutions for implementing bidirectional DTs in industrial
environments. This includes our three-phase engineering approach that
transforms a passive monitoring system into an interactive Type 2 DT with
real-time control capabilities for pressurized systems operating at seven bar.
We contribute details of multi-layered safety protocols, hardware-software
integration strategies across Arduino controllers and Unity visualization, and
real-time synchronization solutions. We document specific engineering
challenges and solutions spanning interdisciplinary integration, demonstrating
how our use of the constellation reporting framework facilitates cross-domain
collaboration. Key findings include the critical importance of safety-first
design, simulation-driven development, and progressive implementation
strategies. Our work thus provides actionable guidance for practitioners
developing DTs requiring bidirectional control in safety-critical applications.

</details>


### [4] [How do Humans and LLMs Process Confusing Code?](https://arxiv.org/abs/2508.18547)
*Youssef Abdelsalam,Norman Peitek,Anna-Maria Maurer,Mariya Toneva,Sven Apel*

Main category: cs.SE

TL;DR: The study shows that humans and LLMs get confused by the same code regions, suggesting LLMs can help identify confusing code for human programmers.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to understand whether human programmers and large language models (LLMs) experience confusion in the same parts of source code. This has important implications for integrating LLMs into software engineering workflows, improving code quality, and enhancing LLMs' design.

Method: The authors conducted an empirical study comparing code comprehension between LLMs and human programmers. Comprehension in LLMs was measured by examining perplexity scores, while comprehension in humans was measured using neurophysiological data (EEG-based fixation-related potentials). They analyzed the alignment of confusion indicators from both sources when processing clean and confusing code.

Result: The study found that spikes in LLM perplexity strongly correlate in both location and amplitude with human neurophysiological signals associated with confusion. In other words, both LLMs and humans tend to be confused by the same kinds of code.

Conclusion: LLMs and humans exhibit similar confusion patterns when reading code, which means LLMs could be valuable in identifying confusing code for humans. This insight can guide the integration of LLMs in software engineering processes as well as improvements in LLM technology.

Abstract: Already today, humans and programming assistants based on large language
models (LLMs) collaborate in everyday programming tasks. Clearly, a
misalignment between how LLMs and programmers comprehend code can lead to
misunderstandings, inefficiencies, low code quality, and bugs.
  A key question in this space is whether humans and LLMs are confused by the
same kind of code. This would not only guide our choices of integrating LLMs in
software engineering workflows, but also inform about possible improvements of
LLMs.
  To this end, we conducted an empirical study comparing an LLM to human
programmers comprehending clean and confusing code. We operationalized
comprehension for the LLM by using LLM perplexity, and for human programmers
using neurophysiological responses (in particular, EEG-based fixation-related
potentials).
  We found that LLM perplexity spikes correlate both in terms of location and
amplitude with human neurophysiological responses that indicate confusion. This
result suggests that LLMs and humans are similarly confused about the code.
Based on these findings, we devised a data-driven, LLM-based approach to
identify regions of confusion in code that elicit confusion in human
programmers.

</details>


### [5] [LaQual: A Novel Framework for Automated Evaluation of LLM App Quality](https://arxiv.org/abs/2508.18636)
*Yan Wang,Xinyi Hou,Yanjie Zhao,Weiguo Lin,Haoyu Wang,Junjun Si*

Main category: cs.SE

TL;DR: LaQual is an automated framework that improves ranking and recommendation in LLM app stores via hierarchical classification, advanced filtering, and dynamic, scenario-based evaluation. Experiments show its results align with human judgments and outperform current baselines in efficiency and user confidence, offering a better way to surface high-quality LLM apps.


<details>
  <summary>Details</summary>
Motivation: Current LLM app stores use static metrics (like user activity and favorites) for ranking and recommendations, which do not efficiently help users find high-quality apps. There is a need for a more robust and intelligent quality evaluation framework for LLM-based apps.

Method: The paper introduces LaQual, a three-stage automated framework: (1) hierarchically labeling and classifying LLM apps for scenario matching; (2) filtering out low-quality apps using advanced static indicators; (3) applying dynamic, scenario-adaptive evaluation where LLMs generate specific metrics, scoring rules, and tasks tailored to each scenario.

Result: LaQual's automated quality scores closely match human assessments (Spearman's rho 0.62 and 0.60 with significant p-values in the evaluated domains). It can reduce the app candidate pool by 66.7% to 81.3%. User studies show LaQual outperforms baselines in decision confidence, efficiency, and perceived value of evaluations.

Conclusion: LaQual provides a scalable, objective, and user-centered framework for evaluating and recommending high-quality LLM apps in app stores, addressing limitations of current static-metric-based approaches.

Abstract: LLM app stores are quickly emerging as platforms that gather a wide range of
intelligent applications based on LLMs, giving users many choices for content
creation, coding support, education, and more. However, the current methods for
ranking and recommending apps in these stores mostly rely on static metrics
like user activity and favorites, which makes it hard for users to efficiently
find high-quality apps. To address these challenges, we propose LaQual, an
automated framework for evaluating the quality of LLM apps. LaQual consists of
three main stages: first, it labels and classifies LLM apps in a hierarchical
way to accurately match them to different scenarios; second, it uses static
indicators, such as time-weighted user engagement and functional capability
metrics, to filter out low-quality apps; and third, it conducts a dynamic,
scenario-adaptive evaluation, where the LLM itself generates scenario-specific
evaluation metrics, scoring rules, and tasks for a thorough quality assessment.
Experiments on a popular LLM app store show that LaQual is effective. Its
automated scores are highly consistent with human judgments (with Spearman's
rho of 0.62 and p=0.006 in legal consulting, and rho of 0.60 and p=0.009 in
travel planning). By effectively screening, LaQual can reduce the pool of
candidate LLM apps by 66.7% to 81.3%. User studies further confirm that LaQual
significantly outperforms baseline systems in decision confidence, comparison
efficiency (with average scores of 5.45 compared to 3.30), and the perceived
value of its evaluation reports (4.75 versus 2.25). Overall, these results
demonstrate that LaQual offers a scalable, objective, and user-centered
solution for finding and recommending high-quality LLM apps in real-world use
cases.

</details>


### [6] [Requirements Development and Formalization for Reliable Code Generation: A Multi-Agent Vision](https://arxiv.org/abs/2508.18675)
*Xu Lu,Weisong Sun,Yiran Zhang,Ming Hu,Cong Tian,Zhi Jin,Yang Liu*

Main category: cs.SE

TL;DR: ReDeFo is a multi-agent framework that integrates formal methods with LLM-based agents to reliably generate code from requirements, bridging the gap between ambiguous natural language and precise executable code for improved quality assurance.


<details>
  <summary>Details</summary>
Motivation: Automated code generation is a highly desired goal in software engineering, but current approaches using only Large Language Models (LLMs) fall short in generating reliable code that meets practical requirements. These methods lack a systematic process for developing and modeling requirements.

Method: The paper introduces ReDeFo, a multi-agent framework that combines LLM-based agents with formal methods. The framework features three agents working together to transform natural language requirements into formal specifications, which are then used for code generation and verification, ensuring higher quality and correctness.

Result: ReDeFo uses formal specifications to rigorously reason about code correctness, uncover hidden bugs, and enforce critical properties. This strengthens quality assurance across the development pipeline and improves the reliability of auto-generated code.

Conclusion: The framework represents a significant step toward achieving reliable automated software generation, addressing the long-standing challenges of requirement ambiguity and lack of formal verification in LLM-generated code.

Abstract: Automated code generation has long been considered the holy grail of software
engineering. The emergence of Large Language Models (LLMs) has catalyzed a
revolutionary breakthrough in this area. However, existing methods that only
rely on LLMs remain inadequate in the quality of generated code, offering no
guarantees of satisfying practical requirements. They lack a systematic
strategy for requirements development and modeling. Recently, LLM-based agents
typically possess powerful abilities and play an essential role in facilitating
the alignment of LLM outputs with user requirements. In this paper, we envision
the first multi-agent framework for reliable code generation based on
\textsc{re}quirements \textsc{de}velopment and \textsc{fo}rmalization, named
\textsc{ReDeFo}. This framework incorporates three agents, highlighting their
augmentation with knowledge and techniques of formal methods, into the
requirements-to-code generation pipeline to strengthen quality assurance. The
core of \textsc{ReDeFo} is the use of formal specifications to bridge the gap
between potentially ambiguous natural language requirements and precise
executable code. \textsc{ReDeFo} enables rigorous reasoning about correctness,
uncovering hidden bugs, and enforcing critical properties throughout the
development process. In general, our framework aims to take a promising step
toward realizing the long-standing vision of reliable, auto-generated software.

</details>


### [7] [LLM as an Execution Estimator: Recovering Missing Dependency for Practical Time-travelling Debugging](https://arxiv.org/abs/2508.18721)
*Yunrui Pei,Hongshu Wang,Wenjie Zhang,Yun Lin,Weiyu Kong,Jin song Dong*

Main category: cs.SE

TL;DR: RecovSlicing is a new method for tracking dynamic data dependencies for debugging, using LLMs and partial instrumentation. It offers higher accuracy and recall than existing tools, requires only a single program run, and enables finding more bugs, making debugging more efficient and feasible in challenging scenarios.


<details>
  <summary>Details</summary>
Motivation: Dynamic data dependency analysis is crucial for debugging: it helps answer why a variable holds a particular value during execution. Existing methods either require broad, invasive instrumentation of all possible variable definitions or multiple program runs, which is inefficient, especially in large, non-deterministic, or library-heavy codebases.

Method: The paper proposes RecovSlicing, a novel approach using partial instrumentation and leveraging LLMs (large language models) to infer unrecorded program behaviors. Given a program execution trace and a particular slicing criterion (a program step and variable of interest), RecovSlicing approximates how a variable's value was defined by reconstructing missing executions and aligning inferred variables to memory states. It also supports tracing implicit variables, such as those inside data structures like list.get(i).

Result: The authors evaluated RecovSlicing on 8,300 data dependencies using three established slicing benchmarks. RecovSlicing achieved high accuracy (80.3%, 91.1%, and 98.3%) compared to current state-of-the-art tools, significantly outperforming them. Its recall was also higher (91.1%, 91.1%, 98.3%). Furthermore, when integrated with a regression bug localizer, RecovSlicing facilitated the identification of 16% more regressions.

Conclusion: RecovSlicing substantially improves dynamic data dependency analysis by allowing effective debugging with partial instrumentation, reducing overhead, increasing accuracy and recall, and making it practical for non-deterministic or library-intensive programs. Its integration into bug localization further demonstrates its practical value.

Abstract: Dynamic data dependency, answering "why a variable has this value?", is
critical for debugging. Given a program step `s` reading a variable `v`,
finding the dynamic definition of `v` is challenging. Traditional methods
require either (1) exhaustive instrumentation of all possible definitions of
`v` in one run or (2) replicating the run to re-examine reads/writes - both
costly. If `v` is defined in a library, instrumentation becomes expensive; for
non-deterministic programs, replication is infeasible.
  We propose RecovSlicing, which computes dynamic data dependency in a single
run with partial instrumentation. We leverage LLMs to infer program behavior
from a partially recorded trace and code context. Given a trace and a slicing
criterion (step `s` and variable `v`), RecovSlicing estimates the runtime
definition of `v` by recovering the missing execution.It also supports implicit
variables, such as those in `list.get(i)`. Technically, RecovSlicing tackles:
(1) recovering runtime values and structures, and (2) aligning recovered
variables with recorded memory to analyze definitions.
  We evaluate RecovSlicing on 8,300 data dependencies across three slicing
benchmarks, comparing it with Slicer4J, ND-Slicer, LLM Slicer, and re-execution
Slicer. RecovSlicing achieves accuracy of 80.3%, 91.1%, and 98.3%,
outperforming the best baseline (39.0%, 82.0%, 59.9%), and also leads in recall
(91.1%, 91.1%, 98.3% vs. 53.4%, 79.1%, 87.1%). Integrated into a regression bug
localizer, it enables finding 16% more regressions.

</details>


### [8] [Does AI Code Review Lead to Code Changes? A Case Study of GitHub Actions](https://arxiv.org/abs/2508.18771)
*Kexin Sun,Hongyu Kuang,Sebastian Baltes,Xin Zhou,He Zhang,Xiaoxing Ma,Guoping Rong,Dong Shao,Christoph Treude*

Main category: cs.SE

TL;DR: AI code review tools are being adopted more, but not all comments lead to code changes. Concise, code-containing, and manually triggered comments—especially from hunk-level tools—are most effective. Better tool design and understanding of key factors can improve these systems.


<details>
  <summary>Details</summary>
Motivation: AI-based code review tools are increasingly used in software development to improve code quality, but there is limited knowledge about their real-world impact and effectiveness.

Method: The authors conducted a large-scale empirical study analyzing over 22,000 review comments from 16 popular AI-based code review tools used in GitHub workflows across 178 repositories. They used a two-stage LLM-assisted framework to classify whether comments led to code changes and employed interpretable machine learning to identify factors influencing effectiveness.

Result: Adoption of AI-based code review tools is increasing, but their effectiveness in prompting code changes varies. Comments that are concise, include code snippets, and are manually triggered, especially those from hunk-level review tools, are more likely to lead to code changes.

Conclusion: Careful design of AI-based code review tools is crucial for effectiveness. The study provides insights into which factors and design choices can enhance the impact of such systems and suggests directions for future improvements.

Abstract: AI-based code review tools automatically review and comment on pull requests
to improve code quality. Despite their growing presence, little is known about
their actual impact. We present a large-scale empirical study of 16 popular
AI-based code review actions for GitHub workflows, analyzing more than 22,000
review comments in 178 repositories. We investigate (1) how these tools are
adopted and configured, (2) whether their comments lead to code changes, and
(3) which factors influence their effectiveness. We develop a two-stage
LLM-assisted framework to determine whether review comments are addressed, and
use interpretable machine learning to identify influencing factors. Our
findings show that, while adoption is growing, effectiveness varies widely.
Comments that are concise, contain code snippets, and are manually triggered,
particularly those from hunk-level review tools, are more likely to result in
code changes. These results highlight the importance of careful tool design and
suggest directions for improving AI-based code review systems.

</details>


### [9] [Dealing with SonarQube Cloud: Initial Results from a Mining Software Repository Study](https://arxiv.org/abs/2508.18816)
*Sabato Nocera,Davide Fucci,Giuseppe Scanniello*

Main category: cs.SE

TL;DR: Most GitHub projects using SonarQube Cloud rely on default code quality settings, but many also customize them to enforce specific goals. There is potential for future research to directly link these customizations to improvements in software quality, allowing for better configuration guidelines.


<details>
  <summary>Details</summary>
Motivation: Static Code Analysis tools like SonarQube Cloud are increasingly used to ensure code quality, but there is limited understanding of how open-source projects actually use and customize these tools.

Method: A mining study was conducted on GitHub projects linked to SonarQube Cloud via GitHub Actions, investigating how these projects connect to and configure SonarQube Cloud.

Result: Out of 321 GitHub projects, 81% were properly connected to SonarQube Cloud. Of 265 accessible projects, 75% used the organization's default quality gate; 55% relied on the built-in (default) quality gate, while 45% customized their configurations. Most enforcements focus on security, maintainability, reliability, and code coverage, especially for new or changed code.

Conclusion: While many projects stick with predefined SonarQube Cloud configurations, a substantial number customize settings to fit their unique quality goals. The study suggests future work should explore how these configuration choices impact real-world software quality outcomes, enabling evidence-based tool recommendations.

Abstract: Background: Static Code Analysis (SCA) tools are widely adopted to enforce
code quality standards. However, little is known about how open-source projects
use and customize these tools. Aims: This paper investigates how GitHub
projects use and customize a popular SCA tool, namely SonarQube Cloud. Method:
We conducted a mining study of GitHub projects that are linked through GitHub
Actions to SonarQube Cloud projects. Results: Among 321 GitHub projects using
SonarQube Cloud, 81% of them are correctly connected to SonarQube Cloud
projects, while others exhibit misconfigurations or restricted access. Among
265 accessible SonarQube Cloud projects, 75% use the organization's default
quality gate, i.e., a set of conditions that deployed source code must meet to
pass automated checks. While 55% of the projects use the built-in quality gate
provided by SonarQube Cloud, 45% of them customize their quality gate with
different conditions. Overall, the most common quality conditions align with
SonarQube Cloud's "Clean as You Code" principle and enforce security,
maintainability, reliability, coverage, and a few duplicates on newly added or
modified source code. Conclusions: Many projects rely on predefined
configurations, yet a significant portion customize their configurations to
meet specific quality goals. Building on our initial results, we envision a
future research agenda linking quality gate configurations to actual software
outcomes (e.g., improvement of software security). This would enable
evidence-based recommendations for configuring SCA tools like SonarQube Cloud
in various contexts.

</details>


### [10] [Interleaving Large Language Models for Compiler Testing](https://arxiv.org/abs/2508.18955)
*Yunbo Ni,Shaohua Li*

Main category: cs.SE

TL;DR: A new framework splits compiler testing into LLM-driven snippet generation and strategic recombination, yielding better test programs, finding tough bugs, and reducing computational effort.


<details>
  <summary>Details</summary>
Motivation: Current AI-driven compiler testing methods suffer from overly simple generated test programs and high computational costs during extensive testing.

Method: A two-phase framework is proposed: (1) Offline LLMs generate small, feature-rich code snippets; (2) Online these snippets are recombined to create complex, valid test cases for compiler testing.

Result: The approach was implemented in LegoFuzz for C compilers, leading to the discovery of 66 bugs in GCC and LLVM, including many serious, hard-to-find miscompilation bugs.

Conclusion: Decoupling test generation and execution using LLMs greatly enhances compiler bug discovery efficiency and could benefit broader software testing applications.

Abstract: Testing compilers with AI models, especially large language models (LLMs),
has shown great promise. However, current approaches struggle with two key
problems: The generated programs for testing compilers are often too simple,
and extensive testing with the LLMs is computationally expensive. In this
paper, we propose a novel compiler testing framework that decouples the testing
process into two distinct phases: an offline phase and an online phase. In the
offline phase, we use LLMs to generate a collection of small but feature-rich
code pieces. In the online phase, we reuse these code pieces by strategically
combining them to build high-quality and valid test programs, which are then
used to test compilers.
  We implement this idea in a tool, LegoFuzz, for testing C compilers. The
results are striking: we found 66 bugs in GCC and LLVM, the most widely used C
compilers. Almost half of the bugs are miscompilation bugs, which are serious
and hard-to-find bugs that none of the existing LLM-based tools could find. We
believe this efficient design opens up new possibilities for using AI models in
software testing beyond just C compilers.

</details>


### [11] [GitTaskBench: A Benchmark for Code Agents Solving Real-World Tasks Through Code Repository Leveraging](https://arxiv.org/abs/2508.18993)
*Ziyi Ni,Huacan Wang,Shuo Zhang,Shuo Lu,Ziyang He,Wang You,Zhenheng Tang,Yuntao Du,Bill Sun,Hongzhang Liu,Sen Hu,Ronghao Chen,Bo Li,Xin Li,Chen Hu,Binxing Jiao,Daxin Jiang,Pin Lyu*

Main category: cs.SE

TL;DR: The paper introduces GitTaskBench, a new benchmark and evaluation harness for assessing code agents on realistic repository-driven software tasks. Results show that advanced agents still struggle with practical workflow management, solving less than half of the tasks. The work provides a challenging new testbed and proposes a metric (alpha-value) to guide future research toward more robust, real-world code agent systems.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for code agents do not adequately evaluate their performance in realistic, workflow-driven scenarios involving practical use of large-scale code repositories, such as those on GitHub, which are critical in real-world software development.

Method: The authors introduce GitTaskBench, a benchmark featuring 54 realistic coding tasks spanning 7 modalities and 7 domains, each paired with a relevant repository and an automated evaluation harness defining practical success criteria. They also propose a new metric called alpha-value to quantify the economic benefit of agent performance by integrating success rates, computational costs, and developer salaries. They evaluate the benchmark using three state-of-the-art agent frameworks paired with several advanced LLMs.

Result: The experiments reveal that current advanced code agents struggle with realistic repository-centric code tasks, with the best-performing system (OpenHands+Claude 3.7) solving only 48.15% of tasks. Over half of task failures are due to basic but crucial steps like environment setup and dependency resolution.

Conclusion: GitTaskBench fills a critical evaluation gap for code agents by focusing on authentic, repository-driven tasks and practical workflow obstacles. The benchmark highlights significant challenges that remain for code agents to solve end-to-end real-world software development problems. The open-source release aims to foster research on repository-aware code reasoning and task execution.

Abstract: Beyond scratch coding, exploiting large-scale code repositories (e.g.,
GitHub) for practical tasks is vital in real-world software development, yet
current benchmarks rarely evaluate code agents in such authentic,
workflow-driven scenarios. To bridge this gap, we introduce GitTaskBench, a
benchmark designed to systematically assess this capability via 54 realistic
tasks across 7 modalities and 7 domains. Each task pairs a relevant repository
with an automated, human-curated evaluation harness specifying practical
success criteria. Beyond measuring execution and task success, we also propose
the alpha-value metric to quantify the economic benefit of agent performance,
which integrates task success rates, token cost, and average developer
salaries. Experiments across three state-of-the-art agent frameworks with
multiple advanced LLMs show that leveraging code repositories for complex task
solving remains challenging: even the best-performing system, OpenHands+Claude
3.7, solves only 48.15% of tasks. Error analysis attributes over half of
failures to seemingly mundane yet critical steps like environment setup and
dependency resolution, highlighting the need for more robust workflow
management and increased timeout preparedness. By releasing GitTaskBench, we
aim to drive progress and attention toward repository-aware code reasoning,
execution, and deployment -- moving agents closer to solving complex,
end-to-end real-world tasks. The benchmark and code are open-sourced at
https://github.com/QuantaAlpha/GitTaskBench.

</details>


### [12] [A Slice-Based Change Impact Analysis for Regression Test Case Prioritization of Object-Oriented Programs](https://arxiv.org/abs/2508.19056)
*S. Panda,D. Munjal,D. P. Mohapatra*

Main category: cs.SE

TL;DR: The paper introduces a static method for prioritizing test cases based on affected component coupling in object-oriented software. By focusing on fault-prone areas (identified via ACC and ASG), the approach helps reveal faults earlier and shows competitive performance compared to other methods, based on results from multiple case studies.


<details>
  <summary>Details</summary>
Motivation: Test case prioritization in software testing is important to detect faults early and reduce retesting time and cost. The motivation is to improve the efficiency of regression testing by ordering test cases based on their potential to detect errors.

Method: The authors propose a static approach for prioritizing test cases by calculating the affected component coupling (ACC) of program parts in object-oriented software. They construct an affected slice graph (ASG) to represent these parts, calculate ACC values for ASG nodes, and assign higher priority to tests covering nodes with higher ACC.

Result: The approach was analyzed using mutation faults and applied to seven case studies. Results show that test cases covering fault-prone program parts (with higher ACC) tend to reveal faults earlier. The proposed technique performs acceptably compared to existing methods.

Conclusion: The static ACC-based prioritization approach is feasible and improves fault detection performance, justifying its use over some current techniques.

Abstract: Test case prioritization focuses on finding a suitable order of execution of
the test cases in a test suite to meet some performance goals like detecting
faults early. It is likely that some test cases execute the program parts that
are more prone to errors and will detect more errors if executed early during
the testing process. Finding an optimal order of execution for the selected
regression test cases saves time and cost of retesting. This paper presents a
static approach to prioritizing the test cases by computing the affected
component coupling (ACC) of the affected parts of object-oriented programs. We
construct a graph named affected slice graph (ASG) to represent these affected
program parts.We determine the fault-proneness of the nodes of ASG by computing
their respective ACC values. We assign higher priority to those test cases that
cover the nodes with higher ACC values. Our analysis with mutation faults shows
that the test cases executing the fault-prone program parts have a higher
chance to reveal faults earlier than other test cases in the test suite. The
result obtained from seven case studies justifies that our approach is feasible
and gives acceptable performance in comparison to some existing techniques.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [13] [A Case Study on the Effectiveness of LLMs in Verification with Proof Assistants](https://arxiv.org/abs/2508.18587)
*Barış Bayazıt,Yao Li,Xujie Si*

Main category: cs.PL

TL;DR: This paper case-studies LLMs' effectiveness at generating formal proofs for verification projects, finding they excel with small proofs, benefit from context, can produce innovative results, but still make odd mistakes and show varying performance across projects.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to investigate the practical effectiveness of large language models (LLMs) in automating proof generation with proof assistants, addressing the gap in understanding their real-world utility for formal verification tasks.

Method: The authors conducted a case study using two mature Rocq projects (hs-to-coq tool and Verdi) to assess LLMs' proof generation abilities. They applied both quantitative and qualitative analyses to evaluate LLM-generated proofs.

Result: The study found that: (1) incorporating external dependencies and relevant context from the same file significantly improves proof generation; (2) LLMs perform very well on smaller proofs but are also capable of generating larger proofs; (3) performance varies across different verification projects; (4) LLMs can create concise proofs and successfully apply classical methods to new definitions but sometimes make peculiar errors.

Conclusion: LLMs demonstrate strong potential in assisting with proof generation for verification projects, particularly for smaller proofs, but their performance varies with context, project type, and complexity, and they are prone to occasional unusual mistakes.

Abstract: Large language models (LLMs) can potentially help with verification using
proof assistants by automating proofs. However, it is unclear how effective
LLMs are in this task. In this paper, we perform a case study based on two
mature Rocq projects: the hs-to-coq tool and Verdi. We evaluate the
effectiveness of LLMs in generating proofs by both quantitative and qualitative
analysis. Our study finds that: (1) external dependencies and context in the
same source file can significantly help proof generation; (2) LLMs perform
great on small proofs but can also generate large proofs; (3) LLMs perform
differently on different verification projects; and (4) LLMs can generate
concise and smart proofs, apply classical techniques to new definitions, but
can also make odd mistakes.

</details>
