<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 15]
- [cs.PL](#cs.PL) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A Survey of AIOps in the Era of Large Language Models](https://arxiv.org/abs/2507.12472)
*Lingzhe Zhang,Tong Jia,Mengxi Jia,Yifan Wu,Aiwei Liu,Yong Yang,Zhonghai Wu,Xuming Hu,Philip S. Yu,Ying Li*

Main category: cs.SE

TL;DR: This paper surveys 183 recent studies on using large language models in AIOps, summarizing current trends, challenges, methods, and evaluation strategies, while mapping out key research gaps and future opportunities.


<details>
  <summary>Details</summary>
Motivation: As large language models (LLMs) become more advanced and widely used, there is growing interest in their application to Artificial Intelligence for IT Operations (AIOps). However, there is a lack of comprehensive understanding regarding the impact, potential, and limitations of LLMs in AIOps.

Method: The authors conducted a detailed survey, analyzing 183 research papers published between January 2020 and December 2024. They structured their analysis around four research questions focusing on data sources, the evolution of AIOps tasks, LLM-based methods for addressing challenges, and evaluation methodologies for LLM-integrated AIOps.

Result: The survey reveals: (1) a variety of failure data sources and advanced LLM-based processing techniques for legacy and new data, (2) emergence and publication trends of new AIOps tasks aided by LLMs, (3) different LLM-based approaches to AIOps challenges, and (4) specialized evaluation methodologies. The findings highlight trends, current advancements, and research gaps in the field.

Conclusion: The study consolidates the state-of-the-art in applying LLMs to AIOps, identifies underexplored areas, and suggests future research directions to better optimize and advance AIOps using LLMs.

Abstract: As large language models (LLMs) grow increasingly sophisticated and
pervasive, their application to various Artificial Intelligence for IT
Operations (AIOps) tasks has garnered significant attention. However, a
comprehensive understanding of the impact, potential, and limitations of LLMs
in AIOps remains in its infancy. To address this gap, we conducted a detailed
survey of LLM4AIOps, focusing on how LLMs can optimize processes and improve
outcomes in this domain. We analyzed 183 research papers published between
January 2020 and December 2024 to answer four key research questions (RQs). In
RQ1, we examine the diverse failure data sources utilized, including advanced
LLM-based processing techniques for legacy data and the incorporation of new
data sources enabled by LLMs. RQ2 explores the evolution of AIOps tasks,
highlighting the emergence of novel tasks and the publication trends across
these tasks. RQ3 investigates the various LLM-based methods applied to address
AIOps challenges. Finally, RQ4 reviews evaluation methodologies tailored to
assess LLM-integrated AIOps approaches. Based on our findings, we discuss the
state-of-the-art advancements and trends, identify gaps in existing research,
and propose promising directions for future exploration.

</details>


### [2] [LLM-Powered Quantum Code Transpilation](https://arxiv.org/abs/2507.12480)
*Nazanin Siavash,Armin Moin*

Main category: cs.SE

TL;DR: Using LLMs to automatically translate quantum programs between different SDKs enables easy interoperability without manual rules, making quantum software more portable and development more efficient.


<details>
  <summary>Details</summary>
Motivation: The diversity of Quantum SDKs (QSDKs) like Qiskit, Cirq, and PennyLane leads to significant challenges in interoperability and cross-platform development for hybrid quantum-classical software systems. Existing rule-based transpilers require deep expertise and are difficult to maintain, creating a barrier for efficient code translation between QSDKs.

Method: The paper explores the use of Large Language Models (LLMs) as automated, programming language-agnostic transpilers that convert quantum programs between different QSDKs. LLMs leverage their pretrained knowledge and contextual reasoning skills to perform these conversions without relying on manually specified transformation rules.

Result: LLMs are demonstrated to successfully act as flexible and automated transpilers, converting quantum programs across various QSDKs while preserving their functionality. This approach removes the necessity for hand-crafted rules and expertise-intensive mapping, providing a scalable alternative for quantum program transformation.

Conclusion: The study shows that LLMs can serve as intelligent, general-purpose transpilers for quantum programming, offering a robust and scalable solution to software portability challenges in the quantum computing landscape.

Abstract: There exist various Software Development Kits (SDKs) tailored to different
quantum computing platforms. These are known as Quantum SDKs (QSDKs). Examples
include but are not limited to Qiskit, Cirq, and PennyLane. However, this
diversity presents significant challenges for interoperability and
cross-platform development of hybrid quantum-classical software systems.
Traditional rule-based transpilers for translating code between QSDKs are
time-consuming to design and maintain, requiring deep expertise and rigid
mappings in the source and destination code. In this study, we explore the use
of Large Language Models (LLMs) as a flexible and automated solution.
Leveraging their pretrained knowledge and contextual reasoning capabilities, we
position LLMs as programming language-agnostic transpilers capable of
converting quantum programs from one QSDK to another while preserving
functional equivalence. Our approach eliminates the need for manually defined
transformation rules and offers a scalable solution to quantum software
portability. This work represents a step toward enabling intelligent,
general-purpose transpilation in the quantum computing ecosystem.

</details>


### [3] [Kodezi Chronos: A Debugging-First Language Model for Repository-Scale, Memory-Driven Code Understanding](https://arxiv.org/abs/2507.12482)
*Ishraq Khan,Assad Chowdary,Sharoz Haseeb,Urvish Patel*

Main category: cs.SE

TL;DR: Kodezi Chronos is a new, advanced architecture for large-scale code understanding and maintenance, overcoming context limitations of traditional LLMs. It uses multi-level memory and retrieval methods to work over entire codebases, outperforming prior models by detecting more bugs and speeding up debugging, marking progress toward autonomous, self-improving software.


<details>
  <summary>Details</summary>
Motivation: Current Large Language Models (LLMs) used for code generation and software automation are limited by their context window sizes and lack the capability for explicit, structured reasoning over complex codebases. This restricts their effectiveness in large-scale, real-world software maintenance and debugging tasks.

Method: The authors introduce Kodezi Chronos, a novel system utilizing a multi-level embedding memory engine that combines vector and graph-based indexing with continuous, code-aware retrieval. This architecture is designed to function across entire codebases and histories without context window limitations. They also present a new Multi Random Retrieval (MRR) benchmark to evaluate the model on realistic software engineering problems.

Result: Kodezi Chronos significantly outperforms previous LLMs and code models, achieving a 23% improvement in real-world bug detection and reducing debugging cycles by up to 40% compared to sequence-based models. It natively integrates with development and deployment environments, supporting advanced code comprehension and maintenance capabilities.

Conclusion: Kodezi Chronos represents a substantial advance in autonomous code understanding, debugging, and maintenance over ultra-long contexts. Its memory engine architecture and superior performance on realistic tasks bring closer the vision of self-sustaining, continuously improving software systems.

Abstract: Large Language Models (LLMs) have advanced code generation and software
automation, but are fundamentally constrained by limited inference-time context
and lack of explicit code structure reasoning. We introduce Kodezi Chronos, a
next-generation architecture for autonomous code understanding, debugging, and
maintenance, designed to operate across ultra-long contexts comprising entire
codebases, histories, and documentation, all without fixed window limits.
Kodezi Chronos leverages a multi-level embedding memory engine, combining
vector and graph-based indexing with continuous code-aware retrieval. This
enables efficient and accurate reasoning over millions of lines of code,
supporting repository-scale comprehension, multi-file refactoring, and
real-time self-healing actions. Our evaluation introduces a novel Multi Random
Retrieval benchmark, specifically tailored to the software engineering domain.
Unlike classical retrieval benchmarks, this method requires the model to
resolve arbitrarily distant and obfuscated associations across code artifacts,
simulating realistic tasks such as variable tracing, dependency migration, and
semantic bug localization. Chronos outperforms prior LLMs and code models,
demonstrating a 23% improvement in real-world bug detection and reducing
debugging cycles by up to 40% compared to traditional sequence-based
approaches. By natively interfacing with IDEs and CI/CD workflows, Chronos
enables seamless, autonomous software maintenance, elevating code reliability
and productivity while reducing manual effort. These results mark a critical
advance toward self-sustaining, continuously optimized software ecosystems.

</details>


### [4] [A Survey of Reinforcement Learning for Software Engineering](https://arxiv.org/abs/2507.12483)
*Dong Wang,Hanmo You,Lingwei Zhu,Kaiwei Lin,Zheng Chen,Chen Yang,Junji Yu,Zan Wang,Junjie Chen*

Main category: cs.SE

TL;DR: This paper provides the first thorough survey of reinforcement learning applications in software engineering, analyzing 115 studies, mapping trends, identifying challenges, and offering guidance for future work, with all findings and artifacts freely accessible online.


<details>
  <summary>Details</summary>
Motivation: The complexity of modern software systems and the demand for enhanced automation in software engineering have driven interest in applying reinforcement learning approaches in this field. Despite increasing research efforts, there is a lack of a comprehensive, systematic review that maps out the state of RL applications in software engineering.

Method: The paper conducts a systematic literature review of 115 peer-reviewed studies from 22 leading software engineering venues since the introduction of deep reinforcement learning. It analyzes publication trends, categorizes relevant software engineering topics and RL algorithms, assesses dataset usage, model design choices, optimization strategies, and evaluation practices. The study also identifies research gaps and outlines future research directions.

Result: The survey provides the first systematic mapping of reinforcement learning applications in software engineering, presenting a detailed landscape of how RL is being used across various SE tasks, current trends, challenges, and best practices. It highlights key topics, algorithms, and evaluation methods, and makes research artifacts publicly available.

Conclusion: This work fills a critical gap by offering a comprehensive overview of RL applications in software engineering, identifying open challenges, and proposing future research directions to guide both researchers and practitioners. The study supports further advancement of the field through accessible resources and systematic insights.

Abstract: Reinforcement Learning (RL) has emerged as a powerful paradigm for sequential
decision-making and has attracted growing interest across various domains,
particularly following the advent of Deep Reinforcement Learning (DRL) in 2015.
Simultaneously, the rapid advancement of Large Language Models (LLMs) has
further fueled interest in integrating RL with LLMs to enable more adaptive and
intelligent systems. In the field of software engineering (SE), the increasing
complexity of systems and the rising demand for automation have motivated
researchers to apply RL to a broad range of tasks, from software design and
development to quality assurance and maintenance. Despite growing research in
RL-for-SE, there remains a lack of a comprehensive and systematic survey of
this evolving field. To address this gap, we reviewed 115 peer-reviewed studies
published across 22 premier SE venues since the introduction of DRL. We
conducted a comprehensive analysis of publication trends, categorized SE topics
and RL algorithms, and examined key factors such as dataset usage, model design
and optimization, and evaluation practices. Furthermore, we identified open
challenges and proposed future research directions to guide and inspire ongoing
work in this evolving area. To summarize, this survey offers the first
systematic mapping of RL applications in software engineering, aiming to
support both researchers and practitioners in navigating the current landscape
and advancing the field. Our artifacts are publicly available:
https://github.com/KaiWei-Lin-lanina/RL4SE.

</details>


### [5] [When Retriever Meets Generator: A Joint Model for Code Comment Generation](https://arxiv.org/abs/2507.12558)
*Tien P. T. Le,Anh M. T. Bui,Huy N. D. Pham,Alessio Bucaioni,Phuong T. Nguyen*

Main category: cs.SE

TL;DR: RAGSum, a unified retrieval-generation model for code comment automation built on CodeT5, significantly outperforms existing methods on multiple benchmarks by better integrating code retrieval and comment generation.


<details>
  <summary>Details</summary>
Motivation: Automatically generating concise, informative comments for source code reduces documentation effort and accelerates program understanding. However, retrieval-augmented methods often suffer from noise because retrieval and generation are optimized separately, allowing irrelevant examples to influence results.

Method: The authors introduce RAGSum, a unified framework that fuses code snippet retrieval and comment generation into a single model, based on CodeT5. The method uses a contrastive pre-training phase to shape code embeddings for effective retrieval and then carries out end-to-end training with a composite loss that rewards accurate retrieval and minimizes generation error. Additionally, a self-refinement loop further improves output quality.

Result: RAGSum was tested on three cross-language (Java, Python, C) benchmarks and compared against three established baselines. The approach outperforms baselines in BLEU, METEOR, and ROUTE-L metrics, demonstrating its effectiveness.

Conclusion: By tightly coupling retrieval and generation within a unified framework, RAGSum substantially improves automatic code comment generation, suggesting higher potential for automation and encouraging future studies in this direction.

Abstract: Automatically generating concise, informative comments for source code can
lighten documentation effort and accelerate program comprehension.
Retrieval-augmented approaches first fetch code snippets with existing comments
and then synthesize a new comment, yet retrieval and generation are typically
optimized in isolation, allowing irrelevant neighbors topropagate noise
downstream. To tackle the issue, we propose a novel approach named RAGSum with
the aim of both effectiveness and efficiency in recommendations. RAGSum is
built on top offuse retrieval and generation using a single CodeT5 backbone. We
report preliminary results on a unified retrieval-generation framework built on
CodeT5. A contrastive pre-training phase shapes code embeddings for
nearest-neighbor search; these weights then seed end-to-end training with a
composite loss that (i) rewards accurate top-k retrieval; and (ii) minimizes
comment-generation error. More importantly, a lightweight self-refinement loop
is deployed to polish the final output. We evaluated theframework on three
cross-language benchmarks (Java, Python, C), and compared it with three
well-established baselines. The results show that our approach substantially
outperforms thebaselines with respect to BLEU, METEOR, and ROUTE-L. These
findings indicate that tightly coupling retrieval and generationcan raise the
ceiling for comment automation and motivateforthcoming replications and
qualitative developer studies.

</details>


### [6] [ROSE: Transformer-Based Refactoring Recommendation for Architectural Smells](https://arxiv.org/abs/2507.12561)
*Samal Nursapa,Anastassiya Samuilova,Alessio Bucaioni. Phuong T. Nguyen*

Main category: cs.SE

TL;DR: The paper demonstrates that fine-tuned transformer models, especially CodeT5, can effectively recommend fixes for architectural smells in code, achieving high accuracy and outperforming other methods. All resources are openly released.


<details>
  <summary>Details</summary>
Motivation: Architectural smells like God Class, Cyclic Dependency, and Hub-like Dependency negatively impact software quality and maintainability. While tools can detect these issues, there is little support for recommending how to fix them.

Method: The authors fine-tune pre-trained transformer models (CodeBERT and CodeT5) for a three-class classification problem to recommend refactorings. The models are trained on over 2 million refactoring examples from 11,149 open-source Java projects.

Result: CodeT5 reaches 96.9% accuracy and 95.2% F1 score, significantly outperforming CodeBERT and traditional baselines.

Conclusion: Transformer-based models, especially CodeT5, effectively recommend actionable refactorings for architectural smells, bridging the gap between detection and repair. The authors release their code and data for reproducibility.

Abstract: Architectural smells such as God Class, Cyclic Dependency, and Hub-like
Dependency degrade software quality and maintainability. Existing tools detect
such smells but rarely suggest how to fix them. This paper explores the use of
pre-trained transformer models--CodeBERT and CodeT5--for recommending suitable
refactorings based on detected smells. We frame the task as a three-class
classification problem and fine-tune both models on over 2 million refactoring
instances mined from 11,149 open-source Java projects. CodeT5 achieves 96.9%
accuracy and 95.2% F1, outperforming CodeBERT and traditional baselines. Our
results show that transformer-based models can effectively bridge the gap
between smell detection and actionable repair, laying the foundation for future
refactoring recommendation systems. We release all code, models, and data under
an open license to support reproducibility and further research.

</details>


### [7] [QSpark: Towards Reliable Qiskit Code Generation](https://arxiv.org/abs/2507.12642)
*Kiana Kheiri,Aamna Aamir,Andriy Miranskyy,Chen Ding*

Main category: cs.SE

TL;DR: The paper fine-tunes a large language model for quantum programming via two RL methods, achieving notably better Qiskit codegeneration accuracy, but still failing on the hardest tasks.


<details>
  <summary>Details</summary>
Motivation: Quantum programming requires error-resilient circuits, but current large language models (LLMs) such as Granite-20B-Code and StarCoder often produce incorrect Qiskit code. Improving the reliability of LLM-generated code for quantum circuits is therefore critical.

Method: The authors fine-tuned a 32B language model for Qiskit code generation using two reinforcement learning approaches: Group Relative Policy Optimization (GRPO) and Odds-Ratio Preference Optimization (ORPO). They trained the model on a synthetic dataset with detailed annotations.

Result: On the Qiskit HumanEval benchmark, ORPO achieved a Pass@1 of 56.29% (about +10 percentage points over Granite-8B-QK), while GRPO reached 49%, both outperforming general-purpose baselines. On the original HumanEval benchmark, ORPO and GRPO scored 65.90% and 63.00%, respectively. GRPO was better on basic tasks, ORPO on intermediate ones, but neither could solve the most advanced tasks.

Conclusion: Fine-tuning with GRPO and ORPO significantly improves quantum code generation over baseline models, though advanced tasks remain unsolved, indicating ongoing challenges in AI-driven quantum programming.

Abstract: Quantum circuits must be error-resilient, yet LLMs like Granite-20B-Code and
StarCoder often output flawed Qiskit code. We fine-tuned a 32 B model with two
RL methods, Group Relative Policy Optimization (GRPO) and Odds-Ratio Preference
Optimization (ORPO), using a richly annotated synthetic dataset. On the Qiskit
HumanEval benchmark, ORPO reaches 56.29\% Pass@1 ($\approx+10$ pp over
Granite-8B-QK) and GRPO hits 49\%, both beating all general-purpose baselines;
on the original HumanEval they score 65.90\% and 63.00\%. GRPO excels on basic
tasks (42/54), ORPO on intermediate ones (41/68), and neither solves the five
advanced tasks, highlighting clear gains yet room for progress in AI-assisted
quantum programming.

</details>


### [8] [A Three-Phase Evaluation Approach for new Information and Data Models in the Smart Grid Domain](https://arxiv.org/abs/2507.12649)
*Christine van Stiphoudt,Sergio Potenciano Menci,Gilbert Fridgen*

Main category: cs.SE

TL;DR: The paper proposes a new three-phase evaluation method for smart grid data models, combining explicit and implicit checks to improve model reliability early in the design phase, addressing a key gap in current practices.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of a clearly defined and combined explicit and implicit evaluation methodology for newly designed information and data models in the smart grid context, especially as digitalisation drives increased automation and data exchange.

Method: The authors use a design science research approach to develop a three-phase evaluation method. This method combines both explicit and implicit evaluation techniques and is demonstrated using the development of an industrial flexibility-focused information and data model.

Result: The result is a new, three-phase evaluation approach that systematically combines explicit and implicit assessment methods for smart grid information and data models during their design. The authors also present lessons learned from applying this approach on a concrete model development case.

Conclusion: The paper concludes that their three-phase, combined evaluation method fills a methodical gap, providing a structured way to assess new information and data models during design in smart grid systems, enhancing reliability and robustness before operational deployment.

Abstract: The ongoing digitalisation of the smart grid is resulting in an increase in
automated information exchanges across distributed energy systems. This process
has led to the development of new information and data models when the existing
ones fall short. To prevent potential disruptions caused by flaws in the newly
designed information and data models, it is essential to evaluate them during
the design process before they are implemented in operation.
  Currently, general explicit evaluation approaches outside the smart grid
domain stay at a high level without defining clear steps. Meanwhile, implicit
evaluation approaches in the smart grid domain focus on testing systems that
utilise information and data models already in use for functionality in terms
of conformance and interoperability. Notably, no combination of explicit and
implicit evaluation approaches for newly designed information and data models
offers a clearly defined set of steps during their design process in the smart
grid context.
  Consequently, we design a three-phase evaluation approach using design
science research to address this gap. Our evaluation approach combines explicit
and implicit evaluation methods and is applicable when developing new
information and data models. We use the development of an information model and
data model focused on industrial flexibility descriptions to refine our
evaluation approach. Additionally, we provide lessons learned from our
experience.

</details>


### [9] [A Fuzzy Approach to Project Success: Measuring What Matters](https://arxiv.org/abs/2507.12653)
*João Granja-Correia,Remedios Hernández-Linares,Luca Ferranti,Arménio Rego*

Main category: cs.SE

TL;DR: This paper presents a new project success evaluation method using fuzzy logic, prioritizing end-user impact over secondary outcomes, potentially improving accuracy and adaptability compared to traditional measures.


<details>
  <summary>Details</summary>
Motivation: Traditional Likert-scale measures do not adequately capture the complex, context-dependent nature of project success. There is a need for a more dynamic and accurate evaluation method that addresses these limitations.

Method: The paper proposes integrating fuzzy logic into project success evaluation, specifically using a hierarchical Type-1 Mamdani fuzzy system. This method prioritizes sustained positive outcomes for end-users over secondary outcomes.

Result: The approach shifts project evaluation focus towards long-term impacts for end-users rather than secondary factors such as stakeholder satisfaction. The system is dynamic and may offer more accurate evaluations, though empirical validation is still pending.

Conclusion: Incorporating fuzzy logic into project evaluation could enhance measurement accuracy and adaptability in complex settings. Further empirical testing is planned to establish efficacy.

Abstract: This paper introduces a novel approach to project success evaluation by
integrating fuzzy logic into an existing construct. Traditional Likert-scale
measures often overlook the context-dependent and multifaceted nature of
project success. The proposed hierarchical Type-1 Mamdani fuzzy system
prioritizes sustained positive impact for end-users, reducing emphasis on
secondary outcomes like stakeholder satisfaction and internal project success.
This dynamic approach may provide a more accurate measure of project success
and could be adaptable to complex evaluations. Future research will focus on
empirical testing and broader applications of fuzzy logic in social science.

</details>


### [10] [Single Conversation Methodology: A Human-Centered Protocol for AI-Assisted Software Development](https://arxiv.org/abs/2507.12665)
*Salvador D. Escobedo*

Main category: cs.SE

TL;DR: The paper introduces Single Conversation Methodology (SCM), a structured way to use LLMs for software development in one long, coherent conversation, improving clarity, documentation, and developer oversight compared to unstructured AI interactions.


<details>
  <summary>Details</summary>
Motivation: Current use of LLMs in software development is often ad hoc and unstructured, leading to passive reliance on AI and undermining developer control, traceability, and documentation. The paper aims to correct this by providing a more deliberate and structured interaction model.

Method: The authors introduce the Single Conversation Methodology, where software development is conducted through one continuous, structured conversation with an LLM covering all project stages, guided by clear methodological principles. They outline phases, best practices, and philosophical underpinnings.

Result: SCM allows for improved cognitive clarity, traceability, modularity, and documentation compared to ad hoc LLM usage. It reasserts the importance of the developer’s active and central role in the process.

Conclusion: The SCM provides a structured way to interact with LLMs in software development, promoting better oversight, modularity, and traceability, while positioning the human developer as an active supervisor rather than a passive consumer.

Abstract: We propose the Single Conversation Methodology (SCM), a novel and pragmatic
approach to software development using large language models (LLMs). In
contrast to ad hoc interactions with generative AI, SCM emphasizes a structured
and persistent development dialogue, where all stages of a project - from
requirements to architecture and implementation - unfold within a single,
long-context conversation. The methodology is grounded on principles of
cognitive clarity, traceability, modularity, and documentation. We define its
phases, best practices, and philosophical stance, while arguing that SCM offers
a necessary correction to the passive reliance on LLMs prevalent in current
practices. We aim to reassert the active role of the developer as architect and
supervisor of the intelligent tool.

</details>


### [11] [Investigating the Performance of Small Language Models in Detecting Test Smells in Manual Test Cases](https://arxiv.org/abs/2507.13035)
*Keila Lucas,Rohit Gheyi,Márcio Ribeiro,Fabio Palomba,Luana Martins,Elvys Soares*

Main category: cs.SE

TL;DR: Small Language Models can automatically and accurately detect test quality issues ('test smells'), provide explanations, and suggest improvements in manual testing instructions, offering a scalable and privacy-friendly alternative to rule-based approaches.


<details>
  <summary>Details</summary>
Motivation: Manual testing is still important for finding issues not easily detected by automation, but manual test cases often suffer from quality problems ('test smells'), which reduce reliability. Existing detection tools need manual rule creation and do not scale well.

Method: The authors evaluated several Small Language Models (SLMs)—Gemma3, Llama3.2, and Phi-4—on their ability to automatically detect test smells in 143 real-world Ubuntu test cases representing seven different test smell types. Performance was measured using pass@2, and further capabilities (explanation, improvement suggestions) were observed.

Result: Phi-4 achieved the highest detection rate with a pass@2 of 97%, while Gemma3 and Llama3.2 achieved about 91%. The SLMs also provided explanations and suggestions for improvements autonomously, even without detailed prompts.

Conclusion: SLMs can effectively and efficiently detect various test smells, explain detected issues, and suggest improvements without the need for extensive rule definitions or syntactic analysis. This provides a scalable, privacy-preserving, and cost-effective way to improve test quality in manual testing contexts.

Abstract: Manual testing, in which testers follow natural language instructions to
validate system behavior, remains crucial for uncovering issues not easily
captured by automation. However, these test cases often suffer from test
smells, quality issues such as ambiguity, redundancy, or missing checks that
reduce test reliability and maintainability. While detection tools exist, they
typically require manual rule definition and lack scalability. This study
investigates the potential of Small Language Models (SLMs) for automatically
detecting test smells. We evaluate Gemma3, Llama3.2, and Phi-4 on 143
real-world Ubuntu test cases, covering seven types of test smells. Phi-4
achieved the best results, reaching a pass@2 of 97% in detecting sentences with
test smells, while Gemma3 and Llama3.2 reached approximately 91%. Beyond
detection, SLMs autonomously explained issues and suggested improvements, even
without explicit prompt instructions. They enabled low-cost, concept-driven
identification of diverse test smells without relying on extensive rule
definitions or syntactic analysis. These findings highlight the potential of
SLMs as efficient tools that preserve data privacy and can improve test quality
in real-world scenarios.

</details>


### [12] [iReDev: A Knowledge-Driven Multi-Agent Framework for Intelligent Requirements Development](https://arxiv.org/abs/2507.13081)
*Dongming Jin,Weisong Sun,Jiangping Huang,Peng Liang,Jifeng Xuan,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: The paper presents iReDev, a multi-agent framework designed to improve software requirements development by incorporating human knowledge and enabling dynamic human-agent collaboration. Evaluations show iReDev outperforms existing methods, promising smarter, more effective requirements engineering.


<details>
  <summary>Details</summary>
Motivation: Requirements development is a crucial and challenging phase in software engineering, involving significant collaboration and conflict resolution among stakeholders. Existing multi-agent solutions lack effective support for this phase, especially in terms of integrating human knowledge and fostering human-agent collaboration.

Method: The paper introduces iReDev, a knowledge-driven multi-agent framework for intelligent requirements development. iReDev comprises six knowledge-driven agents that collaboratively perform tasks required for generating software requirements specifications. Key techniques include event-driven communication via an artifact pool, allowing responsive and autonomous agent actions, and a human-in-the-loop approach to enhance human-agent interactions.

Result: iReDev was evaluated by assessing the artifacts it generated and comparing them to those produced by baseline approaches. Experimental results indicate that iReDev outperforms existing baselines in multiple aspects of requirements development.

Conclusion: The iReDev framework enables more intelligent, collaborative, and human-centric requirements development. Its innovative integration of agent autonomy, human knowledge, and iterative collaboration shows superior performance and promises new research directions to further advance intelligent requirements engineering.

Abstract: Requirements development is a critical phase as it is responsible for
providing a clear understanding of what stakeholders need. It involves
collaboration among stakeholders to extract explicit requirements and address
potential conflicts, which is time-consuming and labor-intensive. Recently,
multi-agent systems for software development have attracted much attention.
However, existing research provides limited support for requirements
development and overlooks the injection of human knowledge into agents and the
human-agent collaboration. % To address these issues, this paper proposes a
knowledge-driven multi-agent framework for intelligent requirement development,
named iReDev. iReDev features: iReDev consists of six knowledge-driven agents
to support the entire requirements development. They collaboratively perform
various tasks to produce a software requirements specification. iReDev focuses
on integrating human knowledge for agents, enabling them to simulate real-world
stakeholders. iReDev uses an event-driven communication mechanism based on an
artifact pool. Agents continuously monitor the pool and autonomously trigger
the next action based on its changes, enabling iReDev to handle new
requirements quickly. iReDev introduces a human-in-the-loop mechanism to
support human-agent collaboration, ensuring that the generated artifacts align
with the expectations of stakeholders. We evaluated the generated artifacts and
results show that iReDev outperforms existing baselines in multiple aspects. We
further envision three key directions and hope this work can facilitate the
development of intelligent requirements development.

</details>


### [13] [A Conceptual Framework for Requirements Engineering of Pretrained-Model-Enabled Systems](https://arxiv.org/abs/2507.13095)
*Dongming Jin,Zhi Jin,Linyu Li,Xiaohong Chen*

Main category: cs.SE

TL;DR: Pretrained models in software systems introduce new challenges that traditional requirements engineering can't address. This paper proposes a new framework and research directions to guide future work in this space.


<details>
  <summary>Details</summary>
Motivation: The integration of large pretrained models as core components in modern software systems has introduced distinctive and challenging characteristics, which are fundamentally different from traditional deterministic logic. This shift challenges longstanding assumptions in requirements engineering and creates a need to reassess current methodologies.

Method: The authors investigate the unique challenges posed by pretrained-model-enabled systems and propose a conceptual framework specifically tailored for requirements engineering in such systems. They also outline promising research directions within this framework.

Result: A conceptual framework for requirements engineering in software systems powered by pretrained models is proposed. The paper also highlights research directions to address the identified challenges.

Conclusion: This work advocates for rethinking traditional requirements engineering methodologies in light of the unique properties of pretrained-model-enabled systems. It provides guidance for researchers and practitioners to adapt to the emerging challenges.

Abstract: Recent advances in large pretrained models have led to their widespread
integration as core components in modern software systems. The trend is
expected to continue in the foreseeable future. Unlike traditional software
systems governed by deterministic logic, systems powered by pretrained models
exhibit distinctive and emergent characteristics, such as ambiguous capability
boundaries, context-dependent behavior, and continuous evolution. These
properties fundamentally challenge long-standing assumptions in requirements
engineering, including functional decomposability and behavioral
predictability. This paper investigates this problem and advocates for a
rethinking of existing requirements engineering methodologies. We propose a
conceptual framework tailored to requirements engineering of
pretrained-model-enabled software systems and outline several promising
research directions within this framework. This vision helps provide a guide
for researchers and practitioners to tackle the emerging challenges in
requirements engineering of pretrained-model-enabled systems.

</details>


### [14] [Inferring Attributed Grammars from Parser Implementations](https://arxiv.org/abs/2507.13117)
*Andreas Pointner,Josef Pichler,Herbert Prähofer*

Main category: cs.SE

TL;DR: This paper introduces a dynamic analysis technique to infer both the syntax and semantics of input processing from parser implementations, resulting in accurate and comprehensive attributed grammars that better capture the full input specification compared to existing approaches.


<details>
  <summary>Details</summary>
Motivation: Software systems frequently lack detailed, current specifications describing both the input syntax and processing semantics. Existing grammar mining works primarily address syntax, leaving input-processing semantics insufficiently explored.

Method: The paper presents a novel method for deriving attributed grammars from the implementations of recursive descent parsers. The approach observes program executions, analyzes the dynamic behavior of the parser, and systematically embeds semantic actions into the grammar rules, reconstructing both syntax and semantics as attributed grammar specifications.

Result: The proposed method was evaluated using a set of initial programs. The results showed that the approach could accurately reproduce program behavior through the inferred attributed grammars, confirming its effectiveness in recovering complete specifications.

Conclusion: The study demonstrates that dynamic analysis of parser executions enables the systematic recovery of both syntactic and semantic specifications in the form of attributed grammars. This approach facilitates more comprehensive and accurate specification mining compared to previous grammar mining techniques focused on syntax alone.

Abstract: Software systems that process structured inputs often lack complete and
up-to-date specifications, which specify the input syntax and the semantics of
input processing. While grammar mining techniques have focused on recovering
syntactic structures, the semantics of input processing remains largely
unexplored. In this work, we introduce a novel approach for inferring
attributed grammars from parser implementations. Given an input grammar, our
technique dynamically analyzes the implementation of recursive descent parsers
to reconstruct the semantic aspects of input handling, resulting in
specifications in the form of attributed grammars. By observing program
executions and mapping the program's runtime behavior to the grammar, we
systematically extract and embed semantic actions into the grammar rules. This
enables comprehensive specification recovery. We demonstrate the feasibility of
our approach using an initial set of programs, showing that it can accurately
reproduce program behavior through the generated attributed grammars.

</details>


### [15] [Detecting LLM-generated Code with Subtle Modification by Adversarial Training](https://arxiv.org/abs/2507.13123)
*Xin Yin,Xinrui Li,Chao Ni,Xiaodan Xu,Xiaohu Yang*

Main category: cs.SE

TL;DR: The paper introduces CodeGPTSensor+, a more robust model for detecting LLM-generated code, even after modifications like variable renaming. Using adversarial training and a new sample generation module, it outperforms previous methods in both detection accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: The increasing reliance on Large Language Model (LLM)-generated code introduces concerns about code provenance, copyright, code quality, and the ability to detect machine-generated code, especially when such code is modified manually (e.g., variable renaming). Existing detection methods lack robustness against these modifications, creating a need for more reliable solutions.

Method: The paper proposes CodeGPTSensor+, an improved version of CodeGPTSensor, designed to detect LLM-generated code even after it has been manually modified. The method uses adversarial training and integrates a module called Multi-objective Identifier and Structure Transformation (MIST) to generate high-quality adversarial samples. These samples help train the model to resist various adversarial attacks and input perturbations.

Result: CodeGPTSensor+ demonstrates significantly better detection accuracy on adversarially modified code using the HMCorp dataset, while retaining high accuracy on unmodified code. This confirms its improved robustness over the original CodeGPTSensor.

Conclusion: CodeGPTSensor+ effectively addresses the challenge of detecting LLM-generated code that has undergone minor modifications, achieving superior robustness and accuracy compared to existing methods. Its approach offers a promising solution for responsible and compliant use of AI-generated code.

Abstract: With the rapid development of Large Language Models (LLMs), their powerful
code-generation capabilities have been widely applied in tasks like code
completion and automated development, demonstrating the value of improving
coding efficiency. However, the extensive use of LLM-generated code also raises
several new challenges. On the one hand, issues such as the regulation of code
provenance, copyright disputes, and code quality have become increasingly
concerning. How to effectively detect LLM-generated code and ensure its
compliant and responsible use has become a critical and urgent issue. On the
other hand, in practical applications, LLM-generated code is often subject to
manual modifications, such as variable renaming or structural adjustments.
Although some recent studies have proposed training-based and zero-shot methods
for detecting LLM-generated code, these approaches show insufficient robustness
when facing modified LLM-generated code, and there is a lack of an effective
solution. To address the real-world scenario where LLM-generated code may
undergo minor modifications, we propose CodeGPTSensor+, an enhanced version of
CodeGPTSensor, which employs adversarial training to improve robustness against
input perturbations. CodeGPTSensor+ integrates an adversarial sample generation
module, Multi-objective Identifier and Structure Transformation (MIST), which
systematically generates both high-quality and representative adversarial
samples. This module effectively enhances the model's resistance against
diverse adversarial attacks. Experimental results on the HMCorp dataset
demonstrate that CodeGPTSensor+ significantly improves detection accuracy on
the adversarial test set while maintaining high accuracy on the original test
set, showcasing superior robustness compared to CodeGPTSensor.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [16] [Dual-Numbers Reverse AD for Functional Array Languages](https://arxiv.org/abs/2507.12640)
*Tom Smeding,Mikołaj Konarski,Simon Peyton Jones,Andrew Fitzgibbon*

Main category: cs.PL

TL;DR: This paper presents an efficient method for reverse-mode AD on array programs by transforming code to a first-order form and supporting specific higher-order operations, achieving minimal performance overhead but at the cost of general higher-order support.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the performance and practicality of reverse-mode automatic differentiation (AD) for array programs. Existing dual-numbers constructions for reverse-mode AD often suffer from performance issues, particularly for arrays.

Method: The paper introduces a new algorithm with three parts: a semantics-preserving vectorisation code transformation (bulk-operation transform, BOT), an extension of the dual-numbers reverse-mode AD to a first-order array language, and symbolic interpretation for an end-to-end compilation pipeline. Only specific higher-order array combinators are supported to ensure first-order transformation.

Result: The proposed method enables first-class support for multidimensional arrays in dual-numbers reverse-mode AD with little to no performance overhead. This is achieved by transforming higher-order code into first-order code, allowing for straightforward 'dual array' lifting.

Conclusion: The approach sacrifices some generalizability, especially regarding general higher-order functions, but achieves practical, efficient reverse-mode AD for array programs by supporting a select set of higher-order combinators and focusing on first-order transformations.

Abstract: The standard dual-numbers construction works well for forward-mode automatic
differentiation (AD) and is attractive due to its simplicity; recently, it also
has been adapted to reverse-mode AD, but practical performance, especially on
array programs, leaves a lot to be desired. In this paper we introduce
first-class support for multidimensional arrays in dual-numbers reverse-mode AD
with little to no performance overhead. The algorithm consists of three
loosely-coupled components: a semantics-preserving vectorisation code
transformation (the bulk-operation transform or BOT), a fairly straightforward
lifting of the basic dual-numbers reverse AD algorithm to a mostly first-order
array language, and symbolic interpretation to achieve an end-to-end
compilation pipeline. Unfortunately, we lose some of the nice generalisable
aspects of dual-numbers AD in the process, most importantly support for
higher-order code.
  We do support some higher-order array combinators, but only a
carefully-chosen set: 'build' (elementwise array construction), 'gather' and
'scatter'. In return, the BOT can eliminate the essential (for AD)
higher-orderness of the input program, meaning that AD gets essentially
presented with a first-order program. This allows the naive trick of lifting
dual numbers to "dual arrays" to work without much modification.

</details>


### [17] [Formal Verification for JavaScript Regular Expressions: a Proven Semantics and its Applications](https://arxiv.org/abs/2507.13091)
*Aurèle Barrière,Victor Deng,Clément Pit-Claudel*

Main category: cs.PL

TL;DR: The paper introduces a fully formalized and mechanized semantics for JavaScript-style regular expressions with backtracking. Proven equivalent to the ECMAScript spec, it supports comprehensive analysis, enables correctness proofs of regex algorithms like PikeVM, and evaluates contextual equivalence for optimizations.


<details>
  <summary>Details</summary>
Motivation: Formalizing the semantics of modern regular expressions, especially those used in JavaScript (ECMAScript), is challenging but important for program verification and understanding engine behaviors, especially with complex features such as backtracking. Prior work lacked practical, complete, or fully proven-faithful semantics.

Method: The authors mechanized the semantics of a regular expression language with backtracking in the Rocq proof assistant. Their approach ensures faithfulness by proving equivalence to a line-by-line translation of the ECMAScript specification. They also track all possible matches and their priorities by capturing the entire backtracking tree, not just the highest priority match.

Result: They developed a proven-faithful and practical mechanized semantics for JavaScript regular expressions. The approach is demonstrated with two applications: (1) introducing contextual equivalence for modern regexes and validating rewrite rules, and (2) providing the first formal proof of the PikeVM algorithm used in industry regex engines.

Conclusion: This work provides the first comprehensive, mechanical, and faithful formalization of modern regular expressions with backtracking, along with practical applications and complete mechanization in Rocq.

Abstract: We present the first mechanized, succinct, practical, complete, and
proven-faithful semantics for a modern regular expression language with
backtracking semantics. We ensure its faithfulness by proving it equivalent to
a preexisting line-by-line embedding of the official ECMAScript specification
of JavaScript regular expressions. We demonstrate its practicality by
presenting two real-world applications. First, a new notion of contextual
equivalence for modern regular expressions, which we use to prove or disprove
rewrites drawn from previous work. Second, the first formal proof of the PikeVM
algorithm used in many real-world engines. In contrast with the specification
and other formalization work, our semantics captures not only the top-priority
match, but a full backtracking tree recording all possible matches and their
respective priority. All our definitions and results have been mechanized in
the Rocq proof assistant.

</details>


### [18] [Towards Formal Verification of LLM-Generated Code from Natural Language Prompts](https://arxiv.org/abs/2507.13290)
*Aaron Councilman,David Fu,Aryan Gupta,Chengxiao Wang,David Grove,Yu-Xiong Wang,Vikram Adve*

Main category: cs.PL

TL;DR: This paper introduces Astrogator, a system for verifying LLM-generated code against user intent using a formal query language and symbolic verification, achieving strong results on Ansible tasks and improving trust in AI-generated code.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used to generate code from natural language, but often produce incorrect code that users struggle to identify and fix. Providing formal correctness guarantees could improve user trust and usability, especially for non-programmers.

Method: The authors introduce a formal query language allowing users to specify intent in a natural, confirmable way, integrated with a system called Astrogator for Ansible. Astrogator includes a calculus for Ansible program behaviors and a symbolic interpreter for code verification against user-specified intent.

Result: Astrogator was evaluated on 21 code-generation tasks and could verify correct code in 83% of cases and identify incorrect code in 92% of cases.

Conclusion: The proposed approach improves the reliability of LLM-generated code by enabling formal, intent-based verification, thus enhancing the experience of AI code assistants and making code generation more accessible and trustworthy.

Abstract: In the past few years LLMs have emerged as a tool that can aid programmers by
taking natural language descriptions and generating code based on it. However,
LLMs often generate incorrect code that users need to fix and the literature
suggests users often struggle to detect these errors. In this work we seek to
offer formal guarantees of correctness to LLM generated code; such guarantees
could improve the experience of using AI Code Assistants and potentially enable
natural language programming for users with little or no programming knowledge.
To address this challenge we propose to incorporate a formal query language
that can represent a user's intent in a formally defined but natural
language-like manner that a user can confirm matches their intent. Then, using
such a query we propose to verify LLM generated code to ensure it matches the
user's intent. We implement these ideas in our system, Astrogator, for the
Ansible programming language which includes such a formal query language, a
calculus for representing the behavior of Ansible programs, and a symbolic
interpreter which is used for the verification. On a benchmark suite of 21
code-generation tasks, our verifier is able to verify correct code in 83% of
cases and identify incorrect code in 92%.

</details>
