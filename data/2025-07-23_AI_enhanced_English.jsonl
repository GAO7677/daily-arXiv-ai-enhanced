{"id": "2507.15887", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15887", "abs": "https://arxiv.org/abs/2507.15887", "authors": ["Ori Press", "Brandon Amos", "Haoyu Zhao", "Yikai Wu", "Samuel K. Ainsworth", "Dominik Krupke", "Patrick Kidger", "Touqir Sajed", "Bartolomeo Stellato", "Jisun Park", "Nathanael Bosch", "Eli Meril", "Albert Steppi", "Arman Zharmagambetov", "Fangzhao Zhang", "David Perez-Pineiro", "Alberto Mercurio", "Ni Zhan", "Talor Abramovich", "Kilian Lieret", "Hanlin Zhang", "Shirley Huang", "Matthias Bethge", "Ofir Press"], "title": "AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?", "comment": null, "summary": "Despite progress in language model (LM) capabilities, evaluations have thus\nfar focused on models' performance on tasks that humans have previously solved,\nincluding in programming (Jimenez et al., 2024) and mathematics (Glazer et al.,\n2024). We therefore propose testing models' ability to design and implement\nalgorithms in an open-ended benchmark: We task LMs with writing code that\nefficiently solves computationally challenging problems in computer science,\nphysics, and mathematics. Our AlgoTune benchmark consists of 155 coding tasks\ncollected from domain experts and a framework for validating and timing\nLM-synthesized solution code, which is compared to reference implementations\nfrom popular open-source packages. In addition, we develop a baseline LM agent,\nAlgoTuner, and evaluate its performance across a suite of frontier models.\nAlgoTuner achieves an average 1.72x speedup against our reference solvers,\nwhich use libraries such as SciPy, sk-learn and CVXPY. However, we find that\ncurrent models fail to discover algorithmic innovations, instead preferring\nsurface-level optimizations. We hope that AlgoTune catalyzes the development of\nLM agents exhibiting creative problem solving beyond state-of-the-art human\nperformance.", "AI": {"tldr": "AlgoTune is a new benchmark for evaluating language models' ability to create efficient algorithms for challenging problems. While current models, including the AlgoTuner agent, can speed up existing solutions, they show little true innovation. The benchmark aims to inspire advancements in models that can surpass human creative problem-solving skills.", "motivation": "Most current evaluations of language models (LMs) focus on tasks already solved by humans, especially in programming and mathematics. There is a lack of benchmarks testing models' ability to develop novel, efficient algorithms for computationally hard problems.", "method": "The authors introduce AlgoTune, an open-ended benchmark comprising 155 coding tasks collected from domain experts in computer science, physics, and mathematics. This includes a framework for validating and timing LM-generated solutions and comparing them to reference implementations from open-source libraries. They also develop a baseline LM agent, AlgoTuner, to evaluate current frontier models.", "result": "AlgoTuner demonstrates an average 1.72x speedup compared to reference solvers using standard libraries. However, existing models typically perform only superficial optimizations and fail to generate novel algorithmic ideas.", "conclusion": "Current language models, as assessed with AlgoTune, tend to optimize existing solutions rather than invent new algorithms. The benchmark provides a platform to drive progress toward LMs with more advanced, creative problem-solving abilities."}}
{"id": "2507.15889", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15889", "abs": "https://arxiv.org/abs/2507.15889", "authors": ["Noah van der Vleuten"], "title": "Dr. Boot: Bootstrapping Program Synthesis Language Models to Perform Repairing", "comment": "Master's thesis, University of Amsterdam, 2023\n  (https://scripties.uba.uva.nl/search?id=record_54126). Code and experiments\n  available at: https://github.com/NoahVl/Dr-Boot", "summary": "Language models for program synthesis are usually trained and evaluated on\nprogramming competition datasets (MBPP, APPS). However, these datasets are\nlimited in size and quality, while these language models are extremely data\nhungry. Additionally, the language models have a misaligned program synthesis\nprocess compared to humans. While humans iteratively develop code with the help\nof a compiler, most program synthesis models currently produce code in one go.\nTo solve these issues, we introduce a bootstrapping algorithm for program\nsynthesis, that supports teaching models how to repair. We show that\nbootstrapping consistently outperforms regular fine-tuning. Compared to other\nwork, our bootstrapped model performs on par with fine-tuned models that are\n68\\% larger. Notably, bootstrapping with repairing also improves non-repairing\nperformance compared to regular bootstrapping during inference. However, on our\nmodels, repairing during inference is likely inferior to simply sampling the\nsame number of solutions. Furthermore, we find that there are issues with the\nexample test cases in the training portion of the APPS dataset that are\nvaluable to the community, as many repairing and reinforcement learning methods\nrely on them.", "AI": {"tldr": "The paper proposes a bootstrapping approach that teaches language models to repair code during training, improving program synthesis performance beyond traditional methods\u2014even with smaller models. The method also reveals issues with commonly used datasets, which may affect future research.", "motivation": "Current program synthesis language models are limited by the small and low-quality datasets and are misaligned with human programming workflows. Humans use iterative development and repair code using compilers, but models generate code in a single step. This motivates developing better training techniques to close this gap.", "method": "The paper introduces a bootstrapping algorithm specifically designed for program synthesis, which teaches models not just to generate code but also to repair it. The approach involves training models to iteratively fix their code using compiler feedback, more closely mimicking human coding practices.", "result": "Bootstrapping resulted in consistently better performance than regular fine-tuning. The bootstrapped models match the performance of much larger models trained with traditional fine-tuning. Moreover, teaching models to repair code also improved their regular code generation abilities. However, directly repairing during inference was less effective than generating multiple samples and choosing the best one. Additionally, the paper identifies flaws in the training test cases of the APPS dataset, which could impact methods relying on them.", "conclusion": "Bootstrapping and teaching models how to repair code leads to more robust program synthesis models, especially with limited data. This method narrows the gap between human and model coding behaviors, and can achieve competitive performance with smaller model sizes. Problems with existing benchmarks (like APPS) need to be addressed for trustworthy further research."}}
{"id": "2507.15892", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15892", "abs": "https://arxiv.org/abs/2507.15892", "authors": ["Elijah Nnorom", "Md Basim Uddin Ahmed", "Jiho Shin", "Hung Viet Pham", "Song Wang"], "title": "StaAgent: An Agentic Framework for Testing Static Analyzers", "comment": null, "summary": "Static analyzers play a critical role in identifying bugs early in the\nsoftware development lifecycle, but their rule implementations are often\nunder-tested and prone to inconsistencies. To address this, we propose\nStaAgent, an agentic framework that harnesses the generative capabilities of\nLarge Language Models (LLMs) to systematically evaluate static analyzer rules.\nStaAgent comprises four specialized agents: a Seed Generation Agent that\ntranslates bug detection rules into concrete, bug-inducing seed programs; a\nCode Validation Agent that ensures the correctness of these seeds; a Mutation\nGeneration Agent that produces semantically equivalent mutants; and an Analyzer\nEvaluation Agent that performs metamorphic testing by comparing the static\nanalyzer's behavior on seeds and their corresponding mutants. By revealing\ninconsistent behaviors, StaAgent helps uncover flaws in rule implementations.\nThis LLM-driven, multi-agent framework offers a scalable and adaptable solution\nto improve the reliability of static analyzers. We evaluated StaAgent with five\nstate-of-the-art LLMs (CodeL-lama, DeepSeek, Codestral, Qwen, and GPT-4o)\nacross five widely used static analyzers (SpotBugs, SonarQube, ErrorProne,\nInfer, and PMD). The experimental results show that our approach can help\nreveal 64 problematic rules in the latest versions of these five static\nanalyzers (i.e., 28 in SpotBugs, 18 in SonarQube, 6 in ErrorProne, 4 in Infer,\nand 8 in PMD). In addition, 53 out of the 64 bugs cannot be detected by the\nSOTA baseline. We have reported all the bugs to developers, with two of them\nalready fixed. Three more have been confirmed by developers, while the rest are\nawaiting response. These results demonstrate the effectiveness of our approach\nand underscore the promise of agentic, LLM-driven data synthesis to advance\nsoftware engineering.", "AI": {"tldr": "This paper presents StaAgent, a novel framework utilizing LLMs to systematically test and uncover flaws in static analyzer rules. The approach outperformed baselines by detecting dozens of previously missed bugs, showing the value of LLM-driven, agent-based testing in software engineering.", "motivation": "Static analyzers are essential for early bug detection in software development, but the detection rules they rely on are often under-tested, which can lead to missed bugs and inconsistencies. Improving the reliability and coverage of these rules is important for developing more robust software.", "method": "The paper introduces StaAgent, an agentic framework that leverages large language models (LLMs) in a multi-agent approach to systematically test static analyzer rules. It involves four agents: Seed Generation, Code Validation, Mutation Generation, and Analyzer Evaluation. These agents work together to generate and validate bug-inducing programs, create mutants, and compare analyzer behaviors to reveal inconsistencies using metamorphic testing.", "result": "The authors evaluated StaAgent using five LLMs and five industry-standard static analyzers. StaAgent successfully identified 64 problematic rules across different analyzers, of which 53 were missed by the current state-of-the-art baseline methods. Several bugs were confirmed or fixed based on their reports, evidencing practical value.", "conclusion": "StaAgent, an LLM-driven and multi-agent system, provides an effective, scalable, and adaptable solution for uncovering flaws in static analyzer rule implementations. Experimental validation demonstrates significant improvement over existing approaches, suggesting a promising direction for combining LLMs with software engineering tools."}}
{"id": "2507.16037", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16037", "abs": "https://arxiv.org/abs/2507.16037", "authors": ["Zhili Zeng", "Kimya Khakzad Shahandashti", "Alvine Boaye Belle", "Song Wang", "Zhen Ming", "Jiang"], "title": "A Pilot Study on LLM-Based Agentic Translation from Android to iOS: Pitfalls and Insights", "comment": null, "summary": "The rapid advancement of mobile applications has led to a significant demand\nfor cross-platform compatibility, particularly between the Android and iOS\nplatforms. Traditional approaches to mobile application translation often rely\non manual intervention or rule-based systems, which are labor-intensive and\ntime-consuming. While recent advancements in machine learning have introduced\nautomated methods, they often lack contextual understanding and adaptability,\nresulting in suboptimal translations. Large Language Models (LLMs) were\nrecently leveraged to enhance code translation at different granularities,\nincluding the method, class, and repository levels. Researchers have\ninvestigated common errors, limitations, and potential strategies to improve\nthese tasks. However, LLM-based application translation across different\nplatforms, such as migrating mobile applications between Android and iOS or\nadapting software across diverse frameworks, remains underexplored.\nUnderstanding the performance, strengths, and limitations of LLMs in\ncross-platform application translation is critical for advancing software\nengineering automation. This study aims to fill this gap by evaluating\nLLM-based agentic approaches for mobile application translation, identifying\nkey failure points, and proposing guidelines to improve translation\nperformance. We developed a chain of agents that account for dependencies,\nspecifications, program structure, and program control flow when translating\napplications from Android to iOS. To evaluate the performance, we manually\nexamined the translated code for syntactic correctness, semantic accuracy, and\nfunctional completeness. For translation failures, we further conducted a\ndetailed root cause analysis to understand the underlying limitations of the\nagentic translation process and identify opportunities for improvement.", "AI": {"tldr": "This paper investigates using LLM-driven agent chains for translating Android apps to iOS, systematically assessing their successes and failures, and providing practical guidelines for improvement.", "motivation": "The growing demand for effective cross-platform mobile application translation (especially between Android and iOS) and the shortcomings of both manual, rule-based, and existing machine learning approaches motivate this work. There is a need for automated solutions that account for contextual understanding and adaptability in translation.", "method": "The paper develops a chain of LLM-based agents to handle Android-to-iOS application translation, explicitly considering dependencies, program specifications, structure, and control flow. Manual examination of translated code is performed to assess syntactic, semantic, and functional accuracy, with root cause analysis for translation failures.", "result": "The study identifies key failure points and limitations in the LLM-based agentic translation process, provides an evaluation of their performance, and proposes guidelines to improve future translations.", "conclusion": "LLM-based agentic approaches show promise for automating cross-platform mobile application translation but currently face limitations in fully capturing context, dependencies, and complex control flows. Detailed evaluation and root cause analysis can inform guidelines for improved future designs."}}
{"id": "2507.16051", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16051", "abs": "https://arxiv.org/abs/2507.16051", "authors": ["Juan Altmayer Pizzorno", "Emery D. Berger"], "title": "RightTyper: Effective and Efficient Type Annotation for Python", "comment": null, "summary": "Python type annotations bring the benefits of static type checking to the\nlanguage. However, manually writing annotations can be time-consuming and\ntedious. The result is that most real-world Python code remains largely\nuntyped. Past approaches to annotating types in Python code fall short in a\nnumber of ways. Static approaches struggle with dynamic features and infer\noverly broad types. AI-based methods are inherently unsound and can miss rare\nor user-defined types. Dynamic methods can impose extreme runtime overheads,\ndegrading performance by up to 270x, abort execution as they exhaust resources,\nand even infer incorrect types that lead to runtime errors. Crucially, all\nprior work assumes implicitly that the code to be annotated is already correct.\nThis assumption is generally unwarranted, especially for large codebases that\nhave been untyped.\n  This paper presents RightTyper, a novel approach for Python that overcomes\nthese disadvantages. RightTyper not only generates precise type annotations\nbased on actual program behavior, improving recall in type checking relative to\nprior approaches. It also turns type checking into anomaly detection, allowing\nthe type checker to identify corner cases that the programmer can audit for\nunintended behavior. RightTyper is also fast and space-efficient, imposing just\n30% performance overhead on average. RightTyper achieves these characteristics\nby a principled yet pervasive use of sampling--guided by self-profiling--along\nwith statistical filtering and careful resolution and aggregation of type\ninformation.", "AI": {"tldr": "RightTyper is a fast, efficient tool that generates precise Python type annotations using sampling and statistical filtering, and also helps developers detect bugs by turning type checking into anomaly detection.", "motivation": "Writing type annotations in Python is tedious and time-consuming, and existing automated annotation methods have significant limitations: static analysis struggles with dynamic code, AI-based methods can be unsound, and dynamic methods are slow and may be inaccurate. Furthermore, previous approaches assume code is correct, which is not always true for untyped codebases.", "method": "The paper proposes RightTyper, a dynamic type annotation tool for Python. RightTyper leverages sampling guided by self-profiling, statistical filtering, and careful resolution and aggregation of type information. It observes actual program behavior with minimal performance overhead to generate precise annotations and identify anomalous cases that may indicate bugs.", "result": "RightTyper generates more precise type annotations and improves the recall of type checking over prior approaches. It has an average runtime overhead of just 30%, is space-efficient, and turns type checking into anomaly detection, enabling discovery of edge cases for further auditing.", "conclusion": "RightTyper overcomes major limitations of previous type annotation techniques in Python by balancing runtime efficiency, accuracy, and the ability to detect anomalies. It is a practical, effective tool for improving type safety in real-world Python codebases."}}
{"id": "2507.16044", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16044", "abs": "https://arxiv.org/abs/2507.16044", "authors": ["Meriem Mastouri", "Emna Ksontini", "Wael Kessentini"], "title": "Making REST APIs Agent-Ready: From OpenAPI to Model Context Protocol Servers for Tool-Augmented LLMs", "comment": null, "summary": "Large Language Models (LLMs) are evolving from passive text generators into\nactive agents that invoke external tools. To support this shift, scalable\nprotocols for tool integration are essential. The Model Context Protocol (MCP),\nintroduced by Anthropic in 2024, offers a schema-driven standard for dynamic\ntool discovery and invocation. Yet, building MCP servers remains manual and\nrepetitive, requiring developers to write glue code, handle authentication, and\nconfigure schemas by hand-replicating much of the integration effort MCP aims\nto eliminate.\n  This paper investigates whether MCP server construction can be meaningfully\nautomated. We begin by analyzing adoption trends: among 22,000+ MCP-tagged\nGitHub repositories created within six months of release, fewer than 5% include\nservers, typically small, single-maintainer projects dominated by repetitive\nscaffolding. To address this gap, we present AutoMCP, a compiler that generates\nMCP servers from OpenAPI 2.0/3.0 specifications. AutoMCP parses REST API\ndefinitions and produces complete server implementations, including schema\nregistration and authentication handling.\n  We evaluate AutoMCP on 50 real-world APIs spanning 5,066 endpoints across\nover 10 domains. From a stratified sample of 1,023 tool calls, 76.5% succeeded\nout of the box. Manual failure analysis revealed five recurring issues, all\nattributable to inconsistencies or omissions in the OpenAPI contracts. After\nminor fixes, averaging 19 lines of spec changes per API, AutoMCP achieved 99.9%\nsuccess.\n  Our findings (i) analyze MCP adoption and quantify the cost of manual server\ndevelopment, (ii) demonstrate that OpenAPI specifications, despite quality\nissues, enable near-complete MCP server automation, and (iii) contribute a\ncorpus of 5,066 callable tools along with insights on repairing common\nspecification flaws.", "AI": {"tldr": "Constructing MCP servers for integrating tools with LLMs is currently a manual, repetitive process slowing broader adoption. This paper presents AutoMCP, a compiler that generates complete MCP servers from OpenAPI specs. Tested on 50 APIs, it succeeded in 76.5% of cases without intervention and 99.9% after small spec fixes. Automating this process is not only possible but highly effective, as long as minor API specification errors are corrected.", "motivation": "The motivation behind this paper is to address the manual and repetitive process involved in constructing Model Context Protocol (MCP) servers, which are crucial for integrating tools with Large Language Models (LLMs). Despite MCP's goal of simplifying integration, current adoption is low and mostly consists of small, repetitive projects. Automating MCP server creation could significantly reduce developer effort and accelerate the adoption and utility of LLM-connected tools.", "method": "The authors analyzed adoption trends of MCP on GitHub, reviewed over 22,000 repositories, and identified the need for automation in server construction. To solve this, they introduced AutoMCP, a compiler that generates MCP servers automatically from OpenAPI 2.0/3.0 specifications by parsing REST API definitions and handling schema registration and authentication. The tool was evaluated on 50 real-world APIs, making 1,023 tool calls to assess its efficacy and diagnosing issues when failures occurred.", "result": "AutoMCP succeeded in 76.5% of tool calls on first attempt. Upon minor manual fixes to OpenAPI specs (averaging 19 additional lines per API), the success rate increased dramatically to 99.9%. The majority of the failures were due to inconsistencies or omissions in the OpenAPI contracts rather than flaws in AutoMCP itself. The study also produced a corpus of 5,066 callable tools and provided insights into common specification repair needs.", "conclusion": "OpenAPI specifications, even with quality issues, can be leveraged to automate nearly all aspects of MCP server construction, substantially reducing the manual labor previously required. Widespread adoption of automation tools like AutoMCP is feasible and beneficial, provided there is minor intervention to fix recurrent specification issues."}}
{"id": "2507.16086", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.16086", "abs": "https://arxiv.org/abs/2507.16086", "authors": ["Andrew Marmaduke", "Apoorv Ingle", "J. Garrett Morris"], "title": "Understanding Haskell-style Overloading via Open Data and Open Functions", "comment": null, "summary": "We present a new, uniform semantics for Haskell-style overloading. We realize\nour approach in a new core language, System F$_\\mathrm{D}$, whose metatheory we\nmechanize in the Lean4 interactive theorem prover. System F$_\\mathrm{D}$ is\ndistinguished by its open data types and open functions, each given by a\ncollection of instances rather than by a single definition. We show that System\nF$_\\mathrm{D}$ can encode advanced features of Haskell's of type class systems,\nmore expressively than current semantics of these features, and without\nassuming additional type equality axioms.", "AI": {"tldr": "The paper introduces System F$_\\mathrm{D}$, a new core language with a uniform semantics for Haskell-style overloading, mechanized in Lean4, that better captures advanced type class features without extra axioms.", "motivation": "Haskell's type class system is powerful but lacks a uniform and well-mechanized semantics, particularly for advanced features. Existing approaches often rely on additional type equality axioms and do not capture the full expressiveness required.", "method": "The authors introduce a new core language, System F$_\\mathrm{D}$, characterized by open data types and open functions defined via collections of instances. They formalize the language's metatheory using the Lean4 interactive theorem prover.", "result": "System F$_\\mathrm{D}$ successfully models advanced aspects of Haskell's type class features and is more expressive compared to prior semantic frameworks. It does this without relying on extra axioms regarding type equality.", "conclusion": "System F$_\\mathrm{D}$ offers a precise and broadly applicable semantics for Haskell-style overloading, improving both expressiveness and formal rigor over existing approaches."}}
{"id": "2507.16063", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16063", "abs": "https://arxiv.org/abs/2507.16063", "authors": ["Yousab Grees", "Polina Iaremchuk", "Ramtin Ehsani", "Esteban Parra", "Preetha Chatterjee", "Sonia Haiduc"], "title": "AI-Powered Commit Explorer (APCE)", "comment": null, "summary": "Commit messages in a version control system provide valuable information for\ndevelopers regarding code changes in software systems. Commit messages can be\nthe only source of information left for future developers describing what was\nchanged and why. However, writing high-quality commit messages is often\nneglected in practice. Large Language Model (LLM) generated commit messages\nhave emerged as a way to mitigate this issue. We introduce the AI-Powered\nCommit Explorer (APCE), a tool to support developers and researchers in the use\nand study of LLM-generated commit messages. APCE gives researchers the option\nto store different prompts for LLMs and provides an additional evaluation\nprompt that can further enhance the commit message provided by LLMs. APCE also\nprovides researchers with a straightforward mechanism for automated and human\nevaluation of LLM-generated messages. Demo link https://youtu.be/zYrJ9s6sZvo", "AI": {"tldr": "The paper presents APCE, a tool that helps generate, enhance, store, and evaluate commit messages produced by large language models, addressing the common neglect of high-quality commit documentation.", "motivation": "Commit messages are important for understanding code changes, but high-quality messages are often neglected. LLMs can generate such messages, but tools to support and evaluate them are lacking.", "method": "The authors introduce APCE, a tool for using, managing prompts, and evaluating LLM-generated commit messages. The tool enables storing prompts, using evaluation prompts, and supports both automated and human evaluations.", "result": "APCE allows users to generate, enhance, store, and evaluate LLM-generated commit messages efficiently.", "conclusion": "APCE can help developers and researchers more effectively leverage and assess LLM-generated commit messages, improving the quality and utility of software documentation."}}
{"id": "2507.16089", "categories": ["cs.PL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.16089", "abs": "https://arxiv.org/abs/2507.16089", "authors": ["Michael J. Sullivan", "Zhibo Chen", "Elvis Pranskevichus", "Robert J. Simmons", "Victor Petrovykh", "Alja\u017e Mur Er\u017een", "Yury Selivanov"], "title": "Querying Graph-Relational Data", "comment": null, "summary": "For applications that store structured data in relational databases, there is\nan impedance mismatch between the flat representations encouraged by relational\ndata models and the deeply nested information that applications expect to\nreceive. In this work, we present the graph-relational database model, which\nprovides a flexible, compositional, and strongly-typed solution to this\n\"object-relational mismatch.\" We formally define the graph-relational database\nmodel and present a static and dynamic semantics for queries. In addition, we\ndiscuss the realization of the graph-relational database model in EdgeQL, a\ngeneral-purpose SQL-style query language, and the Gel system, which compiles\nEdgeQL schemas and queries into PostgreSQL queries. Gel facilitates the kind of\nobject-shaped data manipulation that is frequently provided inefficiently by\nobject-relational mapping (ORM) technologies, while achieving most of the\nefficiency that comes from require writing complex PostgreSQL queries directly.", "AI": {"tldr": "This paper introduces the graph-relational database model, which aligns relational database storage with application-level object structures. Through EdgeQL and the Gel system, it enables expressive object-shaped queries with performance close to custom SQL, overcoming object-relational mismatch issues.", "motivation": "There is a fundamental mismatch between how relational databases store data (in flat tables) and how modern applications expect to handle data (in deeply nested or object-shaped structures), commonly known as the object-relational mismatch. Existing solutions like ORMs are often inefficient or cumbersome.", "method": "The authors propose a new graph-relational database model that is flexible, compositional, and strongly typed. They formally define the model, develop static and dynamic semantics for queries, and implement the approach in the EdgeQL query language and the Gel system, which compiles EdgeQL to efficient PostgreSQL queries.", "result": "The graph-relational model supports object-shaped data manipulation natively, addressing limitations of traditional ORMs. The Gel system achieves much of the efficiency of hand-written SQL while enabling expressive and convenient object-style queries.", "conclusion": "The graph-relational database model and its implementation in EdgeQL and Gel bridge the gap between application data needs and relational storage. They provide both flexibility and efficiency, offering an improved alternative over conventional ORM technologies."}}
{"id": "2507.16166", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16166", "abs": "https://arxiv.org/abs/2507.16166", "authors": ["Nasir U. Eisty", "David E. Bernholdt", "Alex Koufos", "David J. Luet", "Miranda Mundt"], "title": "Ten Essential Guidelines for Building High-Quality Research Software", "comment": null, "summary": "High-quality research software is a cornerstone of modern scientific\nprogress, enabling researchers to analyze complex data, simulate phenomena, and\nshare reproducible results. However, creating such software requires adherence\nto best practices that ensure robustness, usability, and sustainability. This\npaper presents ten guidelines for producing high-quality research software,\ncovering every stage of the development lifecycle. These guidelines emphasize\nthe importance of planning, writing clean and readable code, using version\ncontrol, and implementing thorough testing strategies. Additionally, they\naddress key principles such as modular design, reproducibility, performance\noptimization, and long-term maintenance. The paper also highlights the role of\ndocumentation and community engagement in enhancing software usability and\nimpact. By following these guidelines, researchers can create software that\nadvances their scientific objectives and contributes to a broader ecosystem of\nreliable and reusable research tools. This work serves as a practical resource\nfor researchers and developers aiming to elevate the quality and impact of\ntheir research software.", "AI": {"tldr": "This paper provides ten practical guidelines to help researchers develop high-quality and sustainable research software that advances science through reproducibility, usability, and community engagement.", "motivation": "Scientific advances increasingly depend on high-quality research software, but many researchers lack guidance on best practices for developing and maintaining such software.", "method": "The paper proposes and explains ten guidelines, covering all stages of the software development lifecycle, for creating robust, usable, and sustainable research software.", "result": "The authors describe specific best practices including planning, clean code, version control, testing, modular design, reproducibility, performance, maintenance, documentation, and community engagement, which collectively improve research software quality.", "conclusion": "Following these ten guidelines enables researchers to produce software that is robust, reusable, and impactful, thereby advancing both individual research goals and the broader scientific ecosystem."}}
{"id": "2507.16660", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.16660", "abs": "https://arxiv.org/abs/2507.16660", "authors": ["Xuran Cai"], "title": "Enhancing Compiler Optimization Efficiency through Grammatical Decompositions of Control-Flow Graphs", "comment": null, "summary": "This thesis addresses the complexities of compiler optimizations, such as\nregister allocation and Lifetime-optimal Speculative Partial Redundancy\nElimination (LOSPRE), which are often handled using tree decomposition\nalgorithms. However, these methods frequently overlook important sparsity\naspects of Control Flow Graphs (CFGs) and result in high computational costs.\nWe introduce the SPL (Series-Parallel-Loop) decomposition, a novel framework\nthat offers optimal solutions to these challenges. A key contribution is the\nformulation of a general solution for Partial Constraint Satisfaction Problems\n(PCSPs) within graph structures, applied to three optimization problems. First,\nSPL decomposition enhances register allocation by accurately modeling variable\ninterference graphs, leading to efficient register assignments and improved\nperformance across benchmarks. Second, it optimizes LOSPRE by effectively\nidentifying and eliminating redundancies in program execution. Finally, the\nthesis focuses on optimizing the placement of bank selection instructions to\nenhance data retrieval efficiency and reduce latency. Extensive experimentation\ndemonstrates significant performance improvements over existing methods,\nestablishing SPL decomposition as a powerful tool for complex compiler\noptimizations, including register allocation, LOSPRE, and bank selection.", "AI": {"tldr": "This thesis introduces SPL decomposition, a new framework for compiler optimization that efficiently solves key problems like register allocation and redundancy elimination. The approach leverages graph sparsity more effectively than previous methods, resulting in significant performance improvements across a range of benchmarks.", "motivation": "Traditional tree decomposition algorithms for compiler optimizations, such as register allocation and LOSPRE, often neglect important sparsity properties of Control Flow Graphs (CFGs), resulting in high computational costs and suboptimal performance.", "method": "The paper proposes SPL (Series-Parallel-Loop) decomposition, a new framework for handling compiler optimization problems. It provides a general solution for Partial Constraint Satisfaction Problems (PCSPs) within graphs, and applies this framework to three specific compiler optimization tasks: register allocation, lifetime-optimal speculative partial redundancy elimination (LOSPRE), and bank selection instruction placement.", "result": "SPL decomposition enables more accurate modeling of variable interference for efficient register allocation, more effective elimination of redundancies in program execution for LOSPRE, and improved bank selection instruction placement for better data retrieval and reduced latency. Experiments demonstrate that SPL decomposition significantly outperforms existing methods across benchmarks.", "conclusion": "SPL decomposition is a powerful and generalizable framework for addressing complex compiler optimization problems, yielding substantial empirical performance gains in register allocation, LOSPRE, and bank selection."}}
{"id": "2507.16208", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16208", "abs": "https://arxiv.org/abs/2507.16208", "authors": ["Sohaib Muhammad", "Ashwati Vipin", "Karan Shetti", "Honey Mittal"], "title": "LOCOFY Large Design Models -- Design to code conversion solution", "comment": null, "summary": "Despite rapid advances in Large Language Models and Multimodal Large Language\nModels (LLMs), numerous challenges related to interpretability, scalability,\nresource requirements and repeatability remain, related to their application in\nthe design-to-code space. To address this, we introduce the Large Design Models\n(LDMs) paradigm specifically trained on designs and webpages to enable seamless\nconversion from design-to-code. We have developed a training and inference\npipeline by incorporating data engineering and appropriate model architecture\nmodification. The training pipeline consists of the following: 1)Design\nOptimiser: developed using a proprietary ground truth dataset and addresses\nsub-optimal designs; 2)Tagging and feature detection: using pre-trained and\nfine-tuned models, this enables the accurate detection and classification of UI\nelements; and 3)Auto Components: extracts repeated UI structures into reusable\ncomponents to enable creation of modular code, thus reducing redundancy while\nenhancing code reusability. In this manner, each model addresses distinct but\nkey issues for design-to-code conversion. Separately, our inference pipeline\nprocesses real-world designs to produce precise and interpretable instructions\nfor code generation and ensures reliability. Additionally, our models\nillustrated exceptional end-to-end design-to-code conversion accuracy using a\nnovel preview match score metric. Comparative experiments indicated superior\nperformance of LDMs against LLMs on accuracy of node positioning,\nresponsiveness and reproducibility. Moreover, our custom-trained tagging and\nfeature detection model demonstrated high precision and consistency in\nidentifying UI elements across a wide sample of test designs. Thus, our\nproposed LDMs are a reliable and superior solution to understanding designs\nthat subsequently enable the generation of efficient and reliable\nproduction-ready code.", "AI": {"tldr": "The paper introduces Large Design Models (LDMs), specialized for design-to-code tasks, which outperform general LLMs in accuracy and reliability by using a custom data pipeline focused on UI optimization, feature detection, and component modularity.", "motivation": "Despite advancements in Large Language Models (LLMs) and Multimodal LLMs, there are persistent challenges in interpretability, scalability, resource usage, and repeatability, particularly in automating the transformation from design to code. This motivates the search for more specialized models for design-to-code conversion.", "method": "The authors developed a new paradigm called Large Design Models (LDMs), trained specifically on designs and webpages. They created a specialized pipeline with data engineering and model architecture modifications, which involves three main steps: (1) A proprietary-based Design Optimiser to fix sub-optimal designs; (2) Tagging and feature detection using pre-trained/fine-tuned models to classify UI elements; (3) Auto Components to extract reusable UI structures. The inference pipeline converts real designs into interpretable code-generation instructions.", "result": "The LDMs achieve high end-to-end accuracy in design-to-code conversion, measured by a new preview match score. Comparative experiments show LDMs outperform LLMs on accuracy of node positioning, responsiveness, and reproducibility. The tagging and feature detection models also demonstrate high precision and consistency with UI element identification.", "conclusion": "LDMs provide a reliable and superior approach for automating design-to-code conversion, delivering more accurate, interpretable, and reusable code compared to conventional LLMs. They offer improved precision and reliability for code generation from real-world designs."}}
{"id": "2507.16327", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16327", "abs": "https://arxiv.org/abs/2507.16327", "authors": ["Karoline Nyl\u00e6nder", "Aitor Arrieta", "Shaukat Ali", "Paolo Arcaini"], "title": "Search-based Generation of Waypoints for Triggering Self-Adaptations in Maritime Autonomous Vessels", "comment": "9 pages, 3 figures. Accepted at GECCO 2025 (Genetic and Evolutionary\n  Computation Conference), July 14-18, 2025, Malaga, Spain", "summary": "Self-adaptation in maritime autonomous vessels (AVs) enables them to adapt\ntheir behaviors to address unexpected situations while maintaining\ndependability requirements. During the design of such AVs, it is crucial to\nunderstand and identify the settings that should trigger adaptations, enabling\nvalidation of their implementation. To this end, we focus on the navigation\nsoftware of AVs, which must adapt their behavior during operation through\nadaptations. AVs often rely on predefined waypoints to guide them along\ndesignated routes, ensuring safe navigation. We propose a multiobjective\nsearch-based approach, called WPgen, to generate minor modifications to the\npredefined set of waypoints, keeping them as close as possible to the original\nwaypoints, while causing the AV to navigate inappropriately when navigating\nwith the generated waypoints. WPgen uses NSGA-II as the multi-objective search\nalgorithm with three seeding strategies for its initial population, resulting\nin three variations of WPgen. We evaluated these variations on three AVs (one\noverwater tanker and two underwater). We compared the three variations of WPgen\nwith Random Search as the baseline and with each other. Experimental results\nshowed that the effectiveness of these variations varied depending on the AV.\nBased on the results, we present the research and practical implications of\nWPgen.", "AI": {"tldr": "WPgen is a search-based tool that tweaks waypoints for autonomous vessels to test their adaptability. By causing subtle navigation errors, it helps validate adaptation mechanisms, and its effectiveness varies by vessel type.", "motivation": "Maritime autonomous vessels (AVs) require self-adaptation to handle unexpected situations while maintaining reliable operation. To ensure their correct behavior, it is important to identify critical settings that should trigger adaptations during design and validation of AV navigation software.", "method": "The paper introduces WPgen, a multi-objective search-based approach using NSGA-II as the core search algorithm, to generate slight modifications to predefined waypoints. These modifications are designed to remain close to the original but cause the AV to behave inappropriately, enabling the testing and validation of adaptation mechanisms. WPgen is tested with three different seeding strategies, leading to three variations, compared against Random Search as a baseline.", "result": "The experimental evaluation was performed on three AVs: one overwater tanker and two underwater vehicles. The effectiveness of the three WPgen variations differed depending on the specific AV, showing that no single strategy is universally superior. The comparison with Random Search demonstrates WPgen's capability to generate valuable test cases for adaptation validation.", "conclusion": "WPgen is an effective approach for generating test scenarios that challenge the adaptability of AV navigation systems. The research highlights that different AV types may benefit from different WPgen variants, and implications for both research and practical validation of AV systems are discussed."}}
{"id": "2507.16407", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16407", "abs": "https://arxiv.org/abs/2507.16407", "authors": ["Shuhan Liu", "Xing Hu", "Kerui Huang", "Xiaohu Yang", "David Lo", "Xin Xia"], "title": "Improving Code LLM Robustness to Prompt Perturbations via Layer-Aware Model Editing", "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\ncode generation, where the natural language prompt plays a crucial role in\nconveying user intent to the model. However, prior studies have shown that LLMs\nare highly sensitive to prompt perturbations. Minor modifications in wording,\nsyntax, or formatting can significantly reduce the functional correctness of\ngenerated code. As perturbations frequently occur in real-world scenarios,\nimproving the robustness of LLMs to prompt perturbations is essential for\nensuring reliable performance in practical code generation. In this paper, we\nintroduce CREME (Code Robustness Enhancement via Model Editing), a novel\napproach that enhances LLM robustness through targeted parameter updates. CREME\nfirst identifies robustness-sensitive layers by comparing hidden states between\nan original prompt and its perturbed variant. Then, it performs lightweight\nparameter editing at the identified layer to reduce performance degradation. We\nevaluate CREME on two widely used code generation benchmarks (HumanEval and\nMBPP) along with their perturbed counterparts. Experimental results show that\nCREME improves Pass@1 accuracy by 63% on perturbed prompts while maintaining\nstable performance on clean inputs, with accuracy deviations within 1%. Further\nanalysis reveals that robustness-sensitive layers are primarily concentrated in\nthe middle and deeper layers of the network, and their locations vary across\ndifferent model architectures. These insights provide a valuable foundation for\ndeveloping future robustness-oriented editing strategies.", "AI": {"tldr": "Code generation LLMs are fragile to small prompt changes. CREME selectively edits model layers to boost robustness, greatly improving code accuracy on perturbed prompts while keeping normal accuracy high.", "motivation": "Large language models (LLMs) excel at code generation but are highly sensitive to small changes in prompts, which can seriously reduce code quality. In real-world applications, such prompt variations are common, so improving robustness is crucial.", "method": "The introduced method, CREME (Code Robustness Enhancement via Model Editing), enhances model robustness by (1) identifying network layers most sensitive to prompt perturbations (by comparing hidden states for original vs. altered prompts), and (2) editing parameters specifically at those layers to reduce impact on code quality.", "result": "CREME improves Pass@1 accuracy by 63% on perturbed prompts (compared to baseline) and maintains stable performance on unperturbed inputs in major code generation benchmarks (HumanEval and MBPP). It also finds that robustness-sensitive layers are often in the middle to deeper network layers, varying by model architecture.", "conclusion": "CREME offers an effective way to make code generation LLMs more robust against prompt variations, with minimal compromise on normal performance. Identifying robustness-sensitive layers is key, and the provided insights help inform future improvements."}}
{"id": "2507.16439", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16439", "abs": "https://arxiv.org/abs/2507.16439", "authors": ["Gunnar Larsen", "Carol Wong", "Anthony Peruma"], "title": "Exploring Large Language Models for Analyzing and Improving Method Names in Scientific Code", "comment": "The 19th ACM/IEEE International Symposium on Empirical Software\n  Engineering and Measurement - Emerging Results and Vision Track", "summary": "Research scientists increasingly rely on implementing software to support\ntheir research. While previous research has examined the impact of identifier\nnames on program comprehension in traditional programming environments, limited\nwork has explored this area in scientific software, especially regarding the\nquality of method names in the code. The recent advances in Large Language\nModels (LLMs) present new opportunities for automating code analysis tasks,\nsuch as identifier name appraisals and recommendations. Our study evaluates\nfour popular LLMs on their ability to analyze grammatical patterns and suggest\nimprovements for 496 method names extracted from Python-based Jupyter\nNotebooks. Our findings show that the LLMs are somewhat effective in analyzing\nthese method names and generally follow good naming practices, like starting\nmethod names with verbs. However, their inconsistent handling of\ndomain-specific terminology and only moderate agreement with human annotations\nindicate that automated suggestions require human evaluation. This work\nprovides foundational insights for improving the quality of scientific code\nthrough AI automation.", "AI": {"tldr": "LLMs can assess and suggest improvements for method names in scientific code, generally following best practices. However, their suggestions for domain-specific names are inconsistent and need human oversight, highlighting both the potential and limitations of using AI for scientific code quality.", "motivation": "Research scientists depend more on software, but little is known about how good identifier names affect comprehension in scientific code, especially method names. Recent advancements in LLMs offer a way to automate code analysis, opening new possibilities for improving research code quality.", "method": "The authors evaluated four leading Large Language Models on their capability to analyze and suggest improvements to 496 method names taken from Python-based Jupyter Notebooks. They assessed how well LLMs recognize good grammatical patterns and naming conventions, and compared LLM suggestions to human annotation.", "result": "LLMs are somewhat effective at analyzing method names and usually adhere to best practices, such as beginning method names with verbs. Nonetheless, LLMs are inconsistent with domain-specific terms and only moderately agree with human annotations.", "conclusion": "Automated LLM suggestions can be helpful for naming methods in scientific code, but human review is still necessary due to LLMs' inconsistencies, particularly with specialized terminology."}}
{"id": "2507.16587", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16587", "abs": "https://arxiv.org/abs/2507.16587", "authors": ["Giuseppe Crupi", "Rosalia Tufano", "Alejandro Velasco", "Antonio Mastropaolo", "Denys Poshyvanyk", "Gabriele Bavota"], "title": "On the Effectiveness of LLM-as-a-judge for Code Generation and Summarization", "comment": "Accepted at TSE. IEEE Transactions on Software Engineering", "summary": "Large Language Models have been recently exploited as judges for complex\nnatural language processing tasks, such as Q&A. The basic idea is to delegate\nto an LLM the assessment of the \"quality\" of the output provided by an\nautomated technique for tasks for which: (i) quantitative metrics would only\ntell part of the story, and; (ii) a large-scale human-based evaluation would be\ntoo expensive. LLMs-as-a-judge, if proven effective for a specific task, can\nalso unlock new possibilities for automation, with several LLMs proposing a\nsolution for a given instance of the task and others judging and deciding what\nis the best output to show the user. We study the effectiveness of\nLLMs-as-a-judge for two code-related tasks, namely code generation and code\nsummarization. The rationale for choosing these tasks is two-fold. First,\nquantitative metrics are usually not enough for the assessment of code\nsummarizers/generators. For example, it is well documented that metrics such as\nBLEU are quite weak proxies for the quality of the generated summaries. Second,\neven state-of-the-art techniques still struggle with handling complex instances\nof these tasks, making them good candidates for benefiting from more advanced\nsolutions envisioning collaboration among LLMs. For code generation, we check\nwhether eight LLMs are able to judge the correctness of 1,405 Java methods and\n1,281 Python functions generated by the same LLMs or implemented by humans. For\ncode summarization, we compare the judgment of five LLMs to those provided by\nnine humans for ~1.2k summaries, related to both Java and Python functions. Our\nfindings show that GPT-4-turbo is the best LLM in terms of judging capabilities\nfor both tasks, with \"smaller\" LLMs featuring tens of billions parameters not\nbeing able to cope with judging tasks. However, even the best-performing LLM\nfrequently misjudges the correctness of the code and summary quality.", "AI": {"tldr": "LLMs can help automate evaluation for code tasks where traditional metrics fail, but even top models like GPT-4-turbo are not consistently reliable judges. Current LLMs are helpful, but not yet a substitute for humans in assessing code quality or summaries.", "motivation": "Traditional quantitative metrics (like BLEU) are inadequate for assessing complex NLP tasks such as code generation and summarization, and large-scale human evaluations are prohibitively expensive. This motivates exploring large language models (LLMs) as automated judges for these tasks.", "method": "The authors examined the judgment capabilities of several LLMs (including GPT-4-turbo and smaller models) on the quality of outputs for code generation and code summarization tasks. They compared LLM judgments against human evaluations on thousands of Java and Python code samples and summaries.", "result": "GPT-4-turbo outperformed other LLMs in judging code correctness and summary quality. However, even the best LLM often misjudged outputs, and smaller LLMs struggled significantly with judgment tasks.", "conclusion": "LLMs, particularly advanced ones like GPT-4-turbo, offer potential for automating the evaluation of code-related tasks, but they are not yet reliable enough to replace human judgment. Improvements are needed before trusting LLMs as autonomous judges for quality control in code generation and summarization."}}
{"id": "2507.16661", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16661", "abs": "https://arxiv.org/abs/2507.16661", "authors": ["Tan Bui", "Yan Naing Tun", "Thanh Phuc Nguyen", "Yindu Su", "Ferdian Thung", "Yikun Li", "Han Wei Ang", "Yide Yin", "Frank Liauw", "Lwin Khin Shar", "Eng Lieh Ouh", "Ting Zhang", "David Lo"], "title": "VulCoCo: A Simple Yet Effective Method for Detecting Vulnerable Code Clones", "comment": null, "summary": "Code reuse is common in modern software development, but it can also spread\nvulnerabilities when developers unknowingly copy risky code. The code fragments\nthat preserve the logic of known vulnerabilities are known as vulnerable code\nclones (VCCs). Detecting those VCCs is a critical but challenging task.\nExisting VCC detection tools often rely on syntactic similarity or produce\ncoarse vulnerability predictions without clear explanations, limiting their\npractical utility. In this paper, we propose VulCoCo, a lightweight and\nscalable approach that combines embedding-based retrieval with large language\nmodel (LLM) validation. Starting from a set of known vulnerable functions, we\nretrieve syntactically or semantically similar candidate functions from a large\ncorpus and use an LLM to assess whether the candidates retain the\nvulnerability. Given that there is a lack of reproducible vulnerable code clone\nbenchmarks, we first construct a synthetic benchmark that spans various clone\ntypes.\n  Our experiments on the benchmark show that VulCoCo outperforms prior\nstate-of-the-art methods in terms of Precision@k and mean average precision\n(MAP). In addition, we also demonstrate VulCoCo's effectiveness in real-world\nprojects by submitting 400 pull requests (PRs) to 284 open-source projects.\nAmong them, 75 PRs were merged, and 15 resulted in newly published CVEs. We\nalso provide insights to inspire future work to further improve the precision\nof vulnerable code clone detection.", "AI": {"tldr": "VulCoCo combines code embedding retrieval with LLM validation to detect vulnerable code clones more accurately than existing tools, demonstrated both on a new benchmark and in real-world open-source projects.", "motivation": "Code reuse can cause the spread of software vulnerabilities due to the unintentional copying of risky code. Detecting these 'vulnerable code clones' (VCCs) is important for software security, but current detection tools are limited by their reliance on syntactic similarity and lack of interpretability.", "method": "The paper introduces VulCoCo, a system that first uses embedding-based retrieval to find candidate functions similar to known vulnerable code, and then applies a large language model (LLM) to validate whether the extracted candidates retain the actual vulnerability. The authors also build a synthetic benchmark for reproducible evaluation.", "result": "VulCoCo outperformed previous state-of-the-art methods on key metrics such as Precision@k and mean average precision (MAP) on the constructed benchmark. In real-world applications, VulCoCo led to 400 pull requests, with 75 merged and 15 causing new CVEs.", "conclusion": "VulCoCo is an effective and scalable tool for detecting vulnerable code clones in large codebases, improving upon existing methods both in experimental and real project settings. It advances the field of automated vulnerability detection and sets a foundation for future work on increasing precision."}}
{"id": "2507.16685", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16685", "abs": "https://arxiv.org/abs/2507.16685", "authors": ["Duong Nguyen", "Manh Tran-Duc", "Thanh Le-Cong", "Triet Huynh Minh Le", "M. Ali Babar", "Quyet-Thang Huynh"], "title": "VulGuard: An Unified Tool for Evaluating Just-In-Time Vulnerability Prediction Models", "comment": null, "summary": "We present VulGuard, an automated tool designed to streamline the extraction,\nprocessing, and analysis of commits from GitHub repositories for Just-In-Time\nvulnerability prediction (JIT-VP) research. VulGuard automatically mines commit\nhistories, extracts fine-grained code changes, commit messages, and software\nengineering metrics, and formats them for downstream analysis. In addition, it\nintegrates several state-of-the-art vulnerability prediction models, allowing\nresearchers to train, evaluate, and compare models with minimal setup. By\nsupporting both repository-scale mining and model-level experimentation within\na unified framework, VulGuard addresses key challenges in reproducibility and\nscalability in software security research. VulGuard can also be easily\nintegrated into the CI/CD pipeline. We demonstrate the effectiveness of the\ntool in two influential open-source projects, FFmpeg and the Linux kernel,\nhighlighting its potential to accelerate real-world JIT-VP research and promote\nstandardized benchmarking. A demo video is available at:\nhttps://youtu.be/j96096-pxbs", "AI": {"tldr": "VulGuard is an automated tool that streamlines mining, processing, and benchmarking for Just-In-Time vulnerability prediction research. It handles data extraction from GitHub, supports state-of-the-art models, and can be integrated into development pipelines, enhancing reproducibility, scalability, and standardized evaluation.", "motivation": "Just-In-Time vulnerability prediction (JIT-VP) research lacks standardization and is often hindered by challenges in reproducibility, scalability, and integration with modern software development pipelines. There is a need for an automated and unified tool to mine data and benchmark models efficiently.", "method": "The authors introduce VulGuard, an automated tool that mines commit histories from GitHub repositories, extracts code changes and relevant metadata, and prepares them for analysis. The tool integrates state-of-the-art vulnerability prediction models, enabling streamlined training, evaluation, and benchmarking. It also supports repository-scale mining and model-level experimentation, and can be incorporated into CI/CD workflows.", "result": "VulGuard was tested on two popular open-source projects, FFmpeg and the Linux kernel, demonstrating its effectiveness in automating data extraction, modeling, and benchmarking. VulGuard improved the reproducibility and scalability of JIT-VP experiments.", "conclusion": "VulGuard addresses key hurdles in JIT-VP research by offering a unified, automated pipeline for data extraction, model benchmarking, and experimentation. Its integration capabilities and practical application on large projects validate its utility and potential to standardize future JIT-VP studies."}}
{"id": "2507.16754", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16754", "abs": "https://arxiv.org/abs/2507.16754", "authors": ["Fangjian Lei", "Mariam El Mezouar", "Shayan Noei", "Ying Zou"], "title": "Never Come Up Empty: Adaptive HyDE Retrieval for Improving LLM Developer Support", "comment": null, "summary": "Large Language Models (LLMs) have shown promise in assisting developers with\ncode-related questions; however, LLMs carry the risk of generating unreliable\nanswers. To address this, Retrieval-Augmented Generation (RAG) has been\nproposed to reduce the unreliability (i.e., hallucinations) of LLMs. However,\ndesigning effective pipelines remains challenging due to numerous design\nchoices. In this paper, we construct a retrieval corpus of over 3 million Java\nand Python related Stack Overflow posts with accepted answers, and explore\nvarious RAG pipeline designs to answer developer questions, evaluating their\neffectiveness in generating accurate and reliable responses. More specifically,\nwe (1) design and evaluate 7 different RAG pipelines and 63 pipeline variants\nto answer questions that have historically similar matches, and (2) address new\nquestions without any close prior matches by automatically lowering the\nsimilarity threshold during retrieval, thereby increasing the chance of finding\npartially relevant context and improving coverage for unseen cases. We find\nthat implementing a RAG pipeline combining hypothetical-documentation-embedding\n(HyDE) with the full-answer context performs best in retrieving and answering\nsimilarcontent for Stack Overflow questions. Finally, we apply our optimal RAG\npipeline to 4 open-source LLMs and compare the results to their zero-shot\nperformance. Our findings show that RAG with our optimal RAG pipeline\nconsistently outperforms zero-shot baselines across models, achieving higher\nscores for helpfulness, correctness, and detail with LLM-as-a-judge. These\nfindings demonstrate that our optimal RAG pipelines robustly enhance answer\nquality for a wide range of developer queries including both previously seen\nand novel questions across different LLMs", "AI": {"tldr": "The paper develops and evaluates multiple RAG pipeline designs using a large Stack Overflow dataset to improve LLM-generated answers for developer questions. Their optimal RAG pipeline, especially when incorporating HyDE and full context, outperforms standard LLM responses for both old and new questions in terms of correctness and detail.", "motivation": "Large Language Models (LLMs) are helpful for developer questions but can provide unreliable (hallucinated) answers. Retrieval-Augmented Generation (RAG) can help, but its design is complex and not well-established.", "method": "The authors built a retrieval corpus from over 3 million Stack Overflow posts in Java and Python. They designed and evaluated 7 RAG pipelines and 63 variants. For new questions without similar historical matches, the pipelines lower the similarity threshold to find partially relevant context. The performance was tested across different LLMs and compared to zero-shot responses.", "result": "The best performance was achieved using a RAG pipeline that combines hypothetical-documentation-embedding (HyDE) and full-answer context. This approach outperformed zero-shot baselines across multiple LLMs on helpfulness, correctness, and detail, as judged by LLMs themselves.", "conclusion": "Optimally designed RAG pipelines significantly improve answer quality for developer questions across LLMs, making responses more accurate, helpful, and detailed for both familiar and novel queries."}}
{"id": "2507.16808", "categories": ["cs.SE", "cs.AI", "68N19, 68T05", "B.6.3; D.3.4; I.2.2; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.16808", "abs": "https://arxiv.org/abs/2507.16808", "authors": ["Zhihao Xu", "Bixin Li", "Lulu Wang"], "title": "Rethinking LLM-Based RTL Code Optimization Via Timing Logic Metamorphosis", "comment": "13pages with 9 pictures and 2 tables", "summary": "Register Transfer Level(RTL) code optimization is crucial for achieving high\nperformance and low power consumption in digital circuit design. However,\ntraditional optimization methods often rely on manual tuning and heuristics,\nwhich can be time-consuming and error-prone. Recent studies proposed to\nleverage Large Language Models(LLMs) to assist in RTL code optimization. LLMs\ncan generate optimized code snippets based on natural language descriptions,\npotentially speeding up the optimization process. However, existing approaches\nhave not thoroughly evaluated the effectiveness of LLM-Based code optimization\nmethods for RTL code with complex timing logic. To address this gap, we\nconducted a comprehensive empirical investigation to assess the capability of\nLLM-Based RTL code optimization methods in handling RTL code with complex\ntiming logic. In this study, we first propose a new benchmark for RTL\noptimization evaluation. It comprises four subsets, each corresponding to a\nspecific area of RTL code optimization. Then we introduce a method based on\nmetamorphosis to systematically evaluate the effectiveness of LLM-Based RTL\ncode optimization methods.Our key insight is that the optimization\neffectiveness should remain consistent for semantically equivalent but more\ncomplex code. After intensive experiments, we revealed several key findings.\n(1) LLM-Based RTL optimization methods can effectively optimize logic\noperations and outperform existing compiler-based methods. (2) LLM-Based RTL\noptimization methods do not perform better than existing compiler-based methods\non RTL code with complex timing logic, particularly in timing control flow\noptimization and clock domain optimization. This is primarily attributed to the\nchallenges LLMs face in understanding timing logic in RTL code. Based on these\nfindings, we provide insights for further research in leveraging LLMs for RTL\ncode optimization.", "AI": {"tldr": "LLMs are promising for RTL code logic optimization but struggle with complex timing logic due to limited timing reasoning. New benchmarks and methodologies reveal strengths and weaknesses, guiding future research for better LLM-based RTL optimization.", "motivation": "RTL code optimization is important for high-performance, low-power digital circuit design, but current methods are manual, slow, and error-prone. Recent approaches use Large Language Models (LLMs) for automated RTL code optimization, but their effectiveness, especially for complex timing logic, isn't well understood.", "method": "The study introduces a new benchmark with four subsets targeting different RTL optimization areas. The authors use a metamorphosis-based method to systematically evaluate LLM-based RTL code optimization methods, focusing on their consistency and effectiveness with increasingly complex code that remains functionally equivalent.", "result": "Experiments show LLM-based methods outperform compiler-based approaches in optimizing logic operations, but fall short for complex timing logic, such as timing control flow and clock domain optimization. The main limitation is LLMs' difficulty understanding RTL timing logic.", "conclusion": "LLM-based RTL optimization is effective for logic operations but not for more complex timing logic areas. The findings highlight challenges for LLMs and suggest future research directions to improve their capability for RTL code optimization."}}
