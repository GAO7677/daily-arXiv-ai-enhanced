{"id": "2508.01199", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.01199", "abs": "https://arxiv.org/abs/2508.01199", "authors": ["Avinash Malik"], "title": "Efficient compilation and execution of synchronous programs via type-state programming", "comment": null, "summary": "Synchronous programs are used extensively in implementation of safety\ncritical embedded software. Imperative synchronous programming languages model\nmultiple Finite State Machines (FSMs) executing in lockstep at logical clock\nticks. The synchronous view of time along with the FSM based design enables\neasier formal verification. The synchronous composition of multiple FSMs,\nduring compilation, results in the well known state space explosion problem.\nHence, efficiently compiling imperative synchronous programs into small and\nfast executables is challenging. This paper introduces a novel linear time\ncompilation technique for automata based compilation of synchronous programs.\nGraph based rewrite rules for kernel programming constructs are introduced. A\nlinear time algorithm applies these rules to produce a FSM. The FSM is then\nencoded into a type-state program using template meta-programming in C++.\nExperimental results show that the compilation time and generated binary size\nis comparable, while the execution times are on average 31-60% faster than\ncurrent state-of-the-art compilers.", "AI": {"tldr": "This paper presents a new linear time compilation method for synchronous programs that generates faster executables without increasing binary size, outperforming current compilers in execution speed.", "motivation": "Compiling imperative synchronous programs into efficient, small, and fast executables is challenging due to the state space explosion problem when composing multiple FSMs.", "method": "The paper introduces graph-based rewrite rules for kernel constructs, applied in a linear time algorithm to generate FSMs, and encodes these FSMs into type-state programs using C++ template meta-programming.", "result": "The linear time compilation technique produces binaries comparable in size to existing methods but achieves execution times that are on average 31-60% faster than state-of-the-art compilers.", "conclusion": "The proposed compilation approach provides a more efficient executable generation for synchronous programs by addressing state space explosion and improving runtime performance."}}
{"id": "2508.02305", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.02305", "abs": "https://arxiv.org/abs/2508.02305", "authors": ["Rose Bohrer"], "title": "Proceedings 14th International Workshop on Trends in Functional Programming in Education", "comment": null, "summary": "The goal of TFPIE is to gather researchers, teachers and professionals that\nuse, or are interested in the use of, functional programming in education.\nTFPIE aims to be a venue where novel ideas, classroom-tested ideas and\nwork-in-progress on the use of functional programming in education are\ndiscussed. The one-day workshop will foster a spirit of open discussion by\nhaving a review process for publication after the workshop.", "AI": {"tldr": "TFPIE is a workshop designed to unite people interested in functional programming in education, providing an open forum for sharing new and tested ideas, with publication review done after the event.", "motivation": "There is a growing interest in how functional programming can be effectively used in education to improve teaching and learning outcomes.", "method": "Organize a one-day workshop (TFPIE) that provides a platform for researchers, teachers, and professionals to discuss and share ideas on functional programming in education. The workshop encourages open discussion and uses a post-event review process for publications.", "result": "The workshop brings together participants from various backgrounds to share novel and classroom-tested ideas on the use of functional programming in education, promoting collaboration and discussion.", "conclusion": "TFPIE serves as an important event for the community involved in functional programming education, fostering innovation, discussion, and dissemination of best practices."}}
{"id": "2508.01974", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.01974", "abs": "https://arxiv.org/abs/2508.01974", "authors": ["Jiahao Zhang", "Xiao Cheng", "Yuxiang Lei"], "title": "Flow Sensitivity without Control Flow Graph: An Efficient Andersen-Style Flow-Sensitive Pointer Analysis", "comment": null, "summary": "Flow-sensitive pointer analysis constitutes an essential component of precise\nprogram analysis for accurately modeling pointer behaviors by incorporating\ncontrol flows. Flow-sensitive pointer analysis is extensively used in alias\nanalysis, taint analysis, program understanding, compiler optimization, etc.\nExisting flow-sensitive pointer analysis approaches, which are conducted based\non control flow graphs, have significantly advanced the precision of pointer\nanalysis via sophisticated techniques to leverage control flow information.\nHowever, they inevitably suffer from computational inefficiencies when\nresolving points-to information due to the inherent complex structures of\ncontrol flow graphs. We present CG-FSPTA, a Flow-Sensitive Constraint Graph\n(FSConsG) based flow-sensitive pointer analysis to overcome the inefficiency of\ncontrol-flow-graph-based analysis. CG-FSPTA uses a flow-sensitive variant to\nleverage the structural advantages of set-constraint graphs (which are commonly\nused in flow-insensitive pointer analysis) while keeping the flow sensitivity\nof variable definitions and uses, allowing the incorporation of sophisticated\ngraph optimization and dynamic solving techniques. In this way, CG-FSPTA\nachieves significant efficiency improvements while keeping the precision of\nflow-sensitive analysis. Experimental evaluations on benchmark programs\ndemonstrate that CG-FSPTA, significantly reduces both memory usage and\nexecution time while maintaining precision. In particular, by solving in the\nFSConsG, CG-FSPTA achieves an average memory reduction of 33.05\\% and\naccelerates flow-sensitive pointer analysis by 7.27x compared to the\nstate-of-art method. These experimental results underscore the efficacy of\nCG-FSPTA as a scalable solution to analyze large-scale software systems,\nestablishing a robust foundation for future advancements in efficient program\nanalysis frameworks.", "AI": {"tldr": "CG-FSPTA is a new, flow-sensitive pointer analysis method that replaces control flow graphs with an optimized constraint graph approach, yielding over 7x faster analyses and substantial memory savings without sacrificing precision, making it suitable for large software systems.", "motivation": "Flow-sensitive pointer analysis provides high precision for modeling pointer behaviors in programs, which is crucial for various tasks like alias analysis and compiler optimization. However, existing techniques based on control flow graphs suffer from high computational costs due to their complexity, hampering scalability and efficiency.", "method": "The authors propose CG-FSPTA, a novel approach that utilizes a flow-sensitive constraint graph (FSConsG) instead of traditional control flow graphs. This method adopts set-constraint graphs from flow-insensitive pointer analysis but augments them to retain flow sensitivity, leveraging graph optimizations and dynamic solving techniques to improve efficiency.", "result": "Experimental evaluation on benchmark programs shows that CG-FSPTA reduces memory usage by an average of 33.05% and achieves a 7.27x speedup over state-of-the-art methods, while maintaining the same precision in analysis results.", "conclusion": "CG-FSPTA offers a scalable, efficient, and precise alternative to traditional flow-sensitive pointer analysis, making it practical for analyzing large-scale software systems and paving the way for future improvements in program analysis."}}
{"id": "2508.01255", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01255", "abs": "https://arxiv.org/abs/2508.01255", "authors": ["Cuong Chi Le", "Cuong Duc Van", "Tung Duy Vu", "Thai Minh Pham Vu", "Hoang Nhat Phan", "Huy Nhat Phan", "Tien N. Nguyen"], "title": "TestWeaver: Execution-aware, Feedback-driven Regression Testing Generation with Large Language Models", "comment": null, "summary": "Regression testing ensures that code changes do not unintentionally break\nexisting functionality. While recent advances in large language models (LLMs)\nhave shown promise in automating test generation for regression testing, they\noften suffer from limited reasoning about program execution, resulting in\nstagnated coverage growth - a phenomenon known as the coverage plateau. In this\npaper, we present TestWeaver, a novel LLM-based approach that integrates\nlightweight program analysis to guide test generation more effectively.\nTestWeaver introduces three key innovations: (1) it reduces hallucinations and\nimproves focus by supplying the LLM with the backward slice from the target\nline instead of full program context; (2) it identifies and incorporates close\ntest cases - those that share control-flow similarities with the path to the\ntarget line - to provide execution context within the LLM's context window; and\n(3) it enhances LLM's reasoning with execution in-line annotations that encode\nvariable states as comments along executed paths. By equipping LLMs with these\ntargeted and contextualized inputs, TestWeaver improves coverage-guided test\ngeneration and mitigates redundant explorations. Empirical results demonstrate\nthat TestWeaver accelerates code coverage growth and generates more effective\nregression test cases than existing LLM-based approaches.", "AI": {"tldr": "TestWeaver uses program analysis and contextual annotations to boost code coverage and regression test effectiveness in LLM-based test generation, outperforming previous methods.", "motivation": "Recent advances in large language models (LLMs) show promise in automating test generation for regression testing. However, such approaches often stagnate in improving code coverage due to limited reasoning about program execution, resulting in the so-called coverage plateau.", "method": "The paper introduces TestWeaver, an LLM-based approach that integrates lightweight program analysis to guide test generation. TestWeaver innovates through: (1) providing the LLM with the backward slice from the target line instead of the full context to reduce hallucinations; (2) including close test cases that share control-flow similarities to enhance execution context; and (3) injecting execution in-line annotations to encode variable states as comments along executed paths.", "result": "Empirical results show that TestWeaver improves coverage-guided test generation, accelerates code coverage growth, and produces more effective regression test cases compared to existing LLM-based methods.", "conclusion": "TestWeaver enhances LLM-driven regression test generation by guiding the models with more contextual and targeted program information, thereby overcoming the coverage plateau and generating better tests."}}
{"id": "2508.01337", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01337", "abs": "https://arxiv.org/abs/2508.01337", "authors": ["Wei Liu", "Linqiang Guo", "Yi Wen Heng", "Chenglin Li", "Tse-Hsun", "Chen", "Ahmed E. Hassan"], "title": "Screencast-Based Analysis of User-Perceived GUI Responsiveness", "comment": null, "summary": "GUI responsiveness is critical for a positive user experience in mobile\napplications. Even brief delays in visual feedback can frustrate users and lead\nto negative reviews. However, detecting and quantifying such user-perceived\ndelays remains challenging, especially in industrial testing pipelines that\nevaluate thousands of apps daily across diverse devices and OS versions.\nExisting techniques based on static analysis or system metrics, while useful,\nmay not accurately capture user-perceived issues or scale effectively.\n  In this experience paper, we present \\tool, a lightweight and black-box\ntechnique that measures GUI responsiveness directly from mobile screencasts --\nvideo recordings captured during automated GUI testing. \\tool detects user\ninteractions and visual delays, helping developers identify GUI performance\nissues that affect the user experience. It uses computer vision to detect user\ninteractions and analyzes frame-level visual changes to compute two key\nmetrics: response time (from user action to first visual feedback) and finish\ntime (until visual feedback stabilizes). We evaluate \\tool on a manually\nannotated benchmark of 2,458 interactions from 64 popular Android apps. \\tool\nachieves 0.96 precision and 0.93 recall in detecting interactions, and measures\nresponse and finish times within 50\\,ms and 100\\,ms error, respectively, for\nover 89\\% of interactions. The tool has been deployed in an industrial testing\npipeline and analyzes thousands of screencasts daily, uncovering responsiveness\nissues missed by traditional tools and improving performance debugging\nefficiency.", "AI": {"tldr": "This paper presents a computer vision-based tool that analyzes screencast videos from automated mobile app testing to measure GUI responsiveness, achieving high accuracy and scalability in industrial settings where traditional methods fall short.", "motivation": "Ensuring GUI responsiveness is crucial for mobile app user satisfaction, but it's difficult to detect and quantify user-perceived delays at scale, especially in industrial test environments with many devices and OS variations. Existing methods often fail to accurately capture real user experience or to scale effectively.", "method": "The paper introduces \\tool, a lightweight, black-box approach that analyzes mobile screencast videos taken during automated GUI testing. By using computer vision, \\tool identifies user interactions and detects visual delays directly from the video. It computes two metrics\u2014response time and finish time\u2014by analyzing frame-level changes after user actions.", "result": "\\tool was evaluated on a benchmark dataset with 2,458 interactions from 64 popular Android apps. It achieved high accuracy: 0.96 precision and 0.93 recall in detecting interactions, and measured response and finish times within 50 ms and 100 ms errors for over 89% of cases. \\tool is currently deployed in industrial testing, where it analyzes thousands of screencasts daily and uncovers GUI responsiveness issues missed by traditional methods.", "conclusion": "A lightweight, scalable video analysis tool can accurately measure user-perceived GUI delays in mobile apps, improving detection of performance issues at scale and supporting better debugging in industrial pipelines."}}
{"id": "2508.01357", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01357", "abs": "https://arxiv.org/abs/2508.01357", "authors": ["Yunhao Liang", "Ruixuan Ying", "Takuya Taniguchi", "Guwen Lyu", "Zhe Cui"], "title": "HyClone: Bridging LLM Understanding and Dynamic Execution for Semantic Code Clone Detection", "comment": null, "summary": "Code clone detection is a critical task in software engineering, aimed at\nidentifying duplicated or similar code fragments within or across software\nsystems. Traditional methods often fail to capture functional equivalence,\nparticularly for semantic clones (Type 4), where code fragments implement\nidentical functionality despite differing syntactic structures. Recent advances\nin large language models (LLMs) have shown promise in understanding code\nsemantics. However, directly applying LLMs to code clone detection yields\nsuboptimal results due to their sensitivity to syntactic differences. To\naddress these challenges, we propose a novel two-stage framework that combines\nLLM-based screening with execution-based validation for detecting semantic\nclones in Python programs. In the first stage, an LLM evaluates code pairs to\nfilter out obvious non-clones based on semantic analysis. For pairs not\nidentified as clones, the second stage employs an execution-based validation\napproach, utilizing LLM-generated test inputs to assess functional equivalence\nthrough cross-execution validation. Our experimental evaluation demonstrates\nsignificant improvements in precision, recall, and F1-score compared to direct\nLLM-based detection, highlighting the framework's effectiveness in identifying\nsemantic clones. Future work includes exploring cross-language clone detection\nand optimizing the framework for large-scale applications.", "AI": {"tldr": "The paper introduces a two-stage method combining LLM semantic screening and execution validation, significantly improving semantic clone detection in code. This approach outperforms direct LLM methods in accuracy and shows promise for broader applications.", "motivation": "Traditional code clone detection struggles particularly with 'semantic clones' (Type 4), where similar functionality is implemented with different code structures. Recent progress in large language models improved code understanding, but their sensitivity to syntax means they still perform suboptimally for these cases.", "method": "The paper proposes a two-stage framework: (1) An LLM-based screening stage to filter out obvious non-clone code pairs based on semantic analysis; (2) An execution-based validation stage, where LLM-generated test inputs enable cross-execution validation to check for functional equivalence in the remaining code pairs.", "result": "Experimental evaluation shows the proposed framework significantly improves precision, recall, and F1-score over directly using LLMs for clone detection.", "conclusion": "The proposed two-stage approach\u2014combining LLM semantics with execution-based validation\u2014effectively detects semantic code clones, outperforming prior LLM-only methods. Future work will look into cross-language detection and scalability."}}
{"id": "2508.01358", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01358", "abs": "https://arxiv.org/abs/2508.01358", "authors": ["Elijah Kayode Adejumo", "Brittany Johnson"], "title": "An Empirical Validation of Open Source Repository Stability Metrics", "comment": null, "summary": "Over the past few decades, open source software has been continuously\nintegrated into software supply chains worldwide, drastically increasing\nreliance and dependence. Because of the role this software plays, it is\nimportant to understand ways to measure and promote its stability and potential\nfor sustainability. Recent work proposed the use of control theory to\nunderstand repository stability and evaluate repositories' ability to return to\nequilibrium after a disturbance such as the introduction of a new feature\nrequest, a spike in bug reports, or even the influx or departure of\ncontributors. This approach leverages commit frequency patterns, issue\nresolution rate, pull request merge rate, and community activity engagement to\nprovide a Composite Stability Index (CSI). While this framework has theoretical\nfoundations, there is no empirical validation of the CSI in practice. In this\npaper, we present the first empirical validation of the proposed CSI by\nexperimenting with 100 highly ranked GitHub repositories. Our results suggest\nthat (1) sampling weekly commit frequency pattern instead of daily is a more\nfeasible measure of commit frequency stability across repositories and (2)\nimproved statistical inferences (swapping mean with median), particularly with\nascertaining resolution and review times in issues and pull request, improves\nthe overall issue and pull request stability index. Drawing on our empirical\ndataset, we also derive data-driven half-width parameters that better align\nstability scores with real project behavior. These findings both confirm the\nviability of a control-theoretic lens on open-source health and provide\nconcrete, evidence-backed applications for real-world project monitoring tools.", "AI": {"tldr": "The paper empirically validates a control theory-based Composite Stability Index for open-source repository health using 100 GitHub projects. Weekly sampling and median-based statistics improve accuracy, confirming the framework's real-world utility and offering actionable metrics for project monitoring.", "motivation": "Open source software has become integral to global software supply chains, making it vital to assess and promote its stability and sustainability. The motivation is to empirically validate metrics\u2014specifically the Composite Stability Index (CSI)\u2014previously proposed on theoretical grounds to evaluate the stability of open-source repositories.", "method": "The study conducts the first empirical validation of the Composite Stability Index (CSI) using data from 100 highly ranked GitHub repositories, analyzing metrics like commit frequency patterns, issue resolution rates, and pull request merge rates. The approach compares weekly versus daily commit frequency sampling and evaluates the use of median over mean for better statistical accuracy in measuring resolution and review times.", "result": "The study found that weekly commit frequency sampling is a more feasible and reliable measure of repository stability than daily sampling. Using the median instead of the mean when evaluating issues and pull request resolution and review times improves the stability index's accuracy. The researchers also derived data-driven half-width parameters to better align stability scores with actual project behavior.", "conclusion": "This empirical validation supports the use of control theory approaches, particularly the CSI, for assessing open-source project stability. The findings enhance the practical applicability of these metrics, offering improved, evidence-based guidelines for real-world open-source project monitoring."}}
{"id": "2508.01430", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01430", "abs": "https://arxiv.org/abs/2508.01430", "authors": ["Kaveh Shahedi", "Matthew Khouzam", "Heng Li", "Maxime Lamothe", "Foutse Khomh"], "title": "From Technical Excellence to Practical Adoption: Lessons Learned Building an ML-Enhanced Trace Analysis Tool", "comment": null, "summary": "System tracing has become essential for understanding complex software\nbehavior in modern systems, yet sophisticated trace analysis tools face\nsignificant adoption gaps in industrial settings. Through a year-long\ncollaboration with Ericsson Montr\\'eal, developing TMLL (Trace-Server Machine\nLearning Library, now in the Eclipse Foundation), we investigated barriers to\ntrace analysis adoption. Contrary to assumptions about complexity or automation\nneeds, practitioners struggled with translating expert knowledge into\nactionable insights, integrating analysis into their workflows, and trusting\nautomated results they could not validate. We identified what we called the\nExcellence Paradox: technical excellence can actively impede adoption when\nconflicting with usability, transparency, and practitioner trust. TMLL\naddresses this through adoption-focused design that embeds expert knowledge in\ninterfaces, provides transparent explanations, and enables incremental\nadoption. Validation through Ericsson's experts' feedback, Eclipse Foundation's\nintegration, and a survey of 40 industry and academic professionals revealed\nconsistent patterns: survey results showed that 77.5% prioritize quality and\ntrust in results over technical sophistication, while 67.5% prefer\nsemi-automated analysis with user control, findings supported by qualitative\nfeedback from industrial collaboration and external peer review. Results\nvalidate three core principles: cognitive compatibility, embedded expertise,\nand transparency-based trust. This challenges conventional capability-focused\ntool development, demonstrating that sustainable adoption requires\nreorientation toward adoption-focused design with actionable implications for\nautomated software engineering tools.", "AI": {"tldr": "Industrial adoption of advanced trace analysis tools is hindered more by issues of usability and trust than by technical limitations. Tools like TMLL, which prioritize user control, embedded expertise, and transparency, are more likely to be adopted than technically superior but less usable alternatives. Designing for adoption, not just capability, is essential for successful deployment of automated software engineering tools.", "motivation": "Advanced trace analysis tools for understanding complex software behavior exist, but their adoption in industry remains low. The authors wanted to understand and address the barriers to adoption, as improving technical capability alone was not translating into practical industry use.", "method": "The authors partnered with Ericsson Montr\u00e9al for a year to develop TMLL (Trace-Server Machine Learning Library) and studied real-world adoption challenges. They used expert collaboration, integration into existing workflows, user surveys (40 professionals), and gathered qualitative feedback through collaborations and peer review.", "result": "The research identified the 'Excellence Paradox': higher technical sophistication can hinder adoption if it reduces usability, transparency, or user trust. Survey results showed that users value trust and transparency more than technical complexity, preferring semi-automated tools that allow user control. TMLL, which emphasizes embedded expertise, transparent explanations, and incremental adoption, was validated by positive feedback and successful integration at Ericsson and the Eclipse Foundation.", "conclusion": "Sustainable adoption of automated trace analysis tools requires focusing on usability, transparency, and trust\u2014instead of just technical excellence. Adoption-focused design with embedded expertise and user control is crucial for real-world use."}}
{"id": "2508.01443", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01443", "abs": "https://arxiv.org/abs/2508.01443", "authors": ["Jingzhi Gong", "Rafail Giavrimis", "Paul Brookes", "Vardan Voskanyan", "Fan Wu", "Mari Ashiga", "Matthew Truscott", "Mike Basios", "Leslie Kanthan", "Jie Xu", "Zheng Wang"], "title": "Tuning LLM-based Code Optimization via Meta-Prompting: An Industrial Perspective", "comment": "Submitted to ASE'25 Industry Showcase", "summary": "There is a growing interest in leveraging large language models (LLMs) for\nautomated code optimization. However, industrial platforms deploying multiple\nLLMs face a critical challenge: prompts optimized for one LLM often fail with\nothers, requiring expensive model-specific prompt engineering. This cross-model\nprompt engineering bottleneck severely limits the practical deployment of\nmulti-LLM optimization systems in production environments. To address this, we\nintroduce Meta-Prompted Code Optimization (MPCO), a framework that\nautomatically generates high-quality, task-specific prompts across diverse LLMs\nwhile maintaining industrial efficiency requirements. MPCO leverages\nmeta-prompting to dynamically synthesize context-aware optimization prompts by\nintegrating project metadata, task requirements, and LLM-specific contexts, and\nit seamlessly deploys on the ARTEMIS industrial platform for automated\nvalidation and scaling.\n  Our comprehensive evaluation on five real-world codebases with 366 hours of\nruntime benchmarking demonstrates MPCO's effectiveness: it achieves overall\nperformance improvements up to 19.06% with the best statistical rank across all\nsystems compared to baseline methods. Analysis shows that 96% of the\ntop-performing optimizations stem from meaningful edits. Through systematic\nablation studies and meta-prompter sensitivity analysis, we identify that\ncomprehensive context integration is essential for effective meta-prompting,\nand that all three major LLMs can serve effectively as meta-prompters,\nproviding actionable insights for industrial practitioners.", "AI": {"tldr": "Meta-Prompted Code Optimization (MPCO) automates high-quality, LLM-adaptive code optimization prompts, overcoming prompt engineering bottlenecks in multi-LLM setups. MPCO boosts performance up to 19%, generalizes across top LLMs, and streamlines practical deployment in industry.", "motivation": "Deploying multiple large language models (LLMs) for automatic code optimization is popular, but a major challenge is that prompts optimized for one LLM often do not generalize, requiring costly, model-specific prompt engineering. This limits scalable use in industry.", "method": "The authors propose Meta-Prompted Code Optimization (MPCO), a framework that automatically generates task- and model-specific prompts by integrating project metadata, task requirements, and the particular context of each LLM. It uses meta-prompting for dynamic prompt generation and is deployed on an industrial platform (ARTEMIS) for validation and scalability.", "result": "Experiments across five real codebases (366 hours of runtime) showed MPCO provided up to 19.06% performance improvement and the highest statistical ranking versus baselines. 96% of the top optimizations came from meaningful code edits. Ablation and sensitivity analyses highlight that full context integration is critical, and all major LLMs can serve as effective meta-prompters.", "conclusion": "MPCO effectively overcomes cross-model prompt engineering bottlenecks, delivering significant, validated code optimization benefits and practical deployment guidance for multi-LLM industrial environments."}}
{"id": "2508.01472", "categories": ["cs.SE", "68N99", "D.2.5"], "pdf": "https://arxiv.org/pdf/2508.01472", "abs": "https://arxiv.org/abs/2508.01472", "authors": ["Lukas Kirschner", "Ezekiel Soremekun"], "title": "Directed Grammar-Based Test Generation", "comment": "21 pages, 10 figures, 13 tables, submitted to IEEE Transactions on\n  Software Engineering, for replication package, see\n  https://tinyurl.com/FDLoop-V3", "summary": "To effectively test complex software, it is important to generate\ngoal-specific inputs, i.e., inputs that achieve a specific testing goal.\nHowever, most state-of-the-art test generators are not designed to target\nspecific goals. Notably, grammar-based test generators, which (randomly)\nproduce syntactically valid inputs via an input specification (i.e., grammar)\nhave a low probability of achieving an arbitrary testing goal. This work\naddresses this challenge by proposing an automated test generation approach\n(called FdLoop) which iteratively learns relevant input properties from\nexisting inputs to drive the generation of goal-specific inputs. Given a\ntesting goal, FdLoop iteratively selects, evolves and learn the input\ndistribution of goal-specific test inputs via test feedback and a probabilistic\ngrammar. We concretize FdLoop for four testing goals, namely unique code\ncoverage, input-to-code complexity, program failures (exceptions) and long\nexecution time. We evaluate FdLoop using three (3) well-known input formats\n(JSON, CSS and JavaScript) and 20 open-source software. In most (86%) settings,\nFdLoop outperforms all five tested baselines namely the baseline grammar-based\ntest generators (random, probabilistic and inverse-probabilistic methods),\nEvoGFuzz and DynaMosa. FdLoop is (up to) twice (2X) as effective as the best\nbaseline (EvoGFuzz) in inducing erroneous behaviors. In addition, we show that\nthe main components of FdLoop (i.e., input mutator, grammar mutator and test\nfeedbacks) contribute positively to its effectiveness. Finally, our evaluation\ndemonstrates that FdLoop effectively achieves single testing goals (revealing\nerroneous behaviors, generating complex inputs, or inducing long execution\ntime) and scales to multiple testing goals across varying parameter settings.", "AI": {"tldr": "FdLoop is a test generation approach that uses feedback and probabilistic grammars to create inputs tailored to specific testing goals (like code coverage or causing failures). It outperforms existing methods on diverse software and input types, proving more effective and flexible for complex software testing.", "motivation": "Existing grammar-based test generators struggle to produce inputs tailored to specific testing goals, limiting their effectiveness in uncovering certain behaviors or issues in software. This research aims to address this limitation by developing a method that can generate goal-specific test inputs automatically and efficiently.", "method": "The authors propose FdLoop, an automated test generation approach that uses test feedback and a probabilistic grammar. FdLoop learns from existing inputs by selecting, evolving, and adjusting the input distribution toward goal-specific inputs for each testing goal. It is concretely demonstrated for four goals: unique code coverage, generating complex inputs, causing program exceptions, and inducing long execution time.", "result": "FdLoop was evaluated on three input formats (JSON, CSS, JavaScript) and 20 open-source programs. It outperformed five baseline test generators (random, probabilistic, inverse-probabilistic, EvoGFuzz, DynaMosa) in 86% of cases, sometimes doubling the effectiveness of the best baseline in inducing erroneous behaviors. The individual components of FdLoop contributed positively to its outcomes, and the method is effective for both single and multiple testing goals across parameter settings.", "conclusion": "FdLoop is a superior, scalable approach for goal-specific test input generation, outperforming existing grammar-based and evolutionary test generators in both effectiveness and adaptability."}}
{"id": "2508.01489", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01489", "abs": "https://arxiv.org/abs/2508.01489", "authors": ["SK. Golam Saroar", "Waseefa Ahmed", "Elmira Onagh", "Maleknaz Nayebi"], "title": "GitHub Marketplace: Driving Automation and Fostering Innovation in Software Development", "comment": "SANER 2025 journal first paper", "summary": "GitHub, a central hub for collaborative software development, has\nrevolutionized the open-source software (OSS) ecosystem through its GitHub\nMarketplace, a platform launched in 2017 to host automation tools aimed at\nenhancing the efficiency and scalability of software projects. As the adoption\nof automation in OSS production grows, understanding the trends,\ncharacteristics, and underlying dynamics of this marketplace has become vital.\nFurthermore, despite the rich repository of academic research on software\nautomation, a disconnect persists between academia and industry practices. This\nstudy seeks to bridge this gap by providing a systematic analysis of the GitHub\nMarketplace, comparing trends observed in industry tools with advancements\nreported in academic literature, and identifying areas where academia can\ncontribute to practical innovation.", "AI": {"tldr": "The paper systematically analyzes GitHub Marketplace's automation tools, compares industry trends with academic research, and identifies areas for academic contributions to real-world OSS automation practices.", "motivation": "The study is motivated by the increasing adoption of automation in open-source software (OSS) development and the need to understand the trends, characteristics, and dynamics of GitHub Marketplace, which provides automation tools for OSS projects. There is also a noted disconnect between academic research and industry practices in software automation.", "method": "The study provides a systematic analysis of the GitHub Marketplace, comparing the trends and tools in the industry with developments described in academic literature.", "result": "The analysis identifies trends in the use of automation tools in OSS, highlights differences between academic advancements and industry practices, and pinpoints opportunities for academia to contribute more directly to industry innovation.", "conclusion": "This work bridges the gap between academic research and industry practice in software automation by characterizing the state of the GitHub Marketplace and suggesting how academia can drive practical improvements."}}
{"id": "2508.01492", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01492", "abs": "https://arxiv.org/abs/2508.01492", "authors": ["Angel C. Chavez-Moreno", "Cristina L. Abad"], "title": "OpenLambdaVerse: A Dataset and Analysis of Open-Source Serverless Applications", "comment": "8 pages, 7 figures, 13th IEEE International Conference on Cloud\n  Engineering (IC2E 2025, accepted, to appear)", "summary": "Function-as-a-Service (FaaS) is at the core of serverless computing, enabling\ndevelopers to easily deploy applications without managing computing resources.\nWith an Infrastructure-as-Code (IaC) approach, frameworks like the Serverless\nFramework use YAML configurations to define and deploy APIs, tasks, workflows,\nand event-driven applications on cloud providers, promoting zero-friction\ndevelopment. As with any rapidly evolving ecosystem, there is a need for\nupdated insights into how these tools are used in real-world projects. Building\non the methodology established by the Wonderless dataset for serverless\ncomputing (and applying multiple new filtering steps), OpenLambdaVerse\naddresses this gap by creating a dataset of current GitHub repositories that\nuse the Serverless Framework in applications that contain one or more AWS\nLambda functions. We then analyze and characterize this dataset to get an\nunderstanding of the state-of-the-art in serverless architectures based on this\nstack. Through this analysis we gain important insights on the size and\ncomplexity of current applications, which languages and runtimes they employ,\nhow are the functions triggered, the maturity of the projects, and their\nsecurity practices (or lack of). OpenLambdaVerse thus offers a valuable,\nup-to-date resource for both practitioners and researchers that seek to better\nunderstand evolving serverless workloads.", "AI": {"tldr": "The paper presents OpenLambdaVerse, a new dataset of real-world GitHub projects using the Serverless Framework with AWS Lambda. Through extensive analysis, the work sheds light on current practices, architectures, and challenges in serverless computing, offering an essential resource for both practitioners and researchers.", "motivation": "Serverless computing, especially using Function-as-a-Service (FaaS) like AWS Lambda via the Serverless Framework, is rapidly evolving. However, there is a need for updated, comprehensive insights into real-world usage, architecture, and practices surrounding these tools.", "method": "The paper introduces OpenLambdaVerse, a curated dataset built from current GitHub repositories utilizing the Serverless Framework with AWS Lambda functions. The dataset is created by applying a methodology similar to the Wonderless dataset, with added filtering to ensure relevance and currency. The authors then analyze and characterize these repositories regarding application size, complexity, programming languages, function triggers, project maturity, and security practices.", "result": "The analysis of OpenLambdaVerse provides a detailed understanding of the modern serverless application landscape, including prevalent languages, architectures, trigger types, and the state of security and project maturity. The dataset and findings offer actionable insights into how serverless technologies are currently used in production and research.", "conclusion": "OpenLambdaVerse fills a significant knowledge gap by offering an up-to-date, detailed dataset and analysis of real-world serverless applications using AWS Lambda and the Serverless Framework, facilitating further research and best practice development in the field."}}
{"id": "2508.01523", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.01523", "abs": "https://arxiv.org/abs/2508.01523", "authors": ["Ningzhi Tang", "Emory Smith", "Yu Huang", "Collin McMillan", "Toby Jia-Jun Li"], "title": "Exploring Direct Instruction and Summary-Mediated Prompting in LLM-Assisted Code Modification", "comment": null, "summary": "This paper presents a study of using large language models (LLMs) in\nmodifying existing code. While LLMs for generating code have been widely\nstudied, their role in code modification remains less understood. Although\n\"prompting\" serves as the primary interface for developers to communicate\nintents to LLMs, constructing effective prompts for code modification\nintroduces challenges different from generation. Prior work suggests that\nnatural language summaries may help scaffold this process, yet such approaches\nhave been validated primarily in narrow domains like SQL rewriting. This study\ninvestigates two prompting strategies for LLM-assisted code modification:\nDirect Instruction Prompting, where developers describe changes explicitly in\nfree-form language, and Summary-Mediated Prompting, where changes are made by\nediting the generated summaries of the code. We conducted an exploratory study\nwith 15 developers who completed modification tasks using both techniques\nacross multiple scenarios. Our findings suggest that developers followed an\niterative workflow: understanding the code, localizing the edit, and validating\noutputs through execution or semantic reasoning. Each prompting strategy\npresented trade-offs: direct instruction prompting was more flexible and easier\nto specify, while summary-mediated prompting supported comprehension, prompt\nscaffolding, and control. Developers' choice of strategy was shaped by task\ngoals and context, including urgency, maintainability, learning intent, and\ncode familiarity. These findings highlight the need for more usable prompt\ninteractions, including adjustable summary granularity, reliable summary-code\ntraceability, and consistency in generated summaries.", "AI": {"tldr": "This paper compares two prompting methods for LLM-driven code modification, finding each has strengths and weaknesses shaped by developer needs and context, and calls for better-designed prompt interactions and code summaries to support practical use.", "motivation": "Large language models (LLMs) are increasingly used for code generation, but their application in code modification is less explored. Constructing effective prompts for code modification has unique challenges compared to code generation, and prior work has been limited to narrow domains.", "method": "The study explores two LLM prompting strategies for code modification: Direct Instruction Prompting (developers describe changes explicitly in language) and Summary-Mediated Prompting (developers modify generated code summaries). 15 developers completed modification tasks using both strategies across several scenarios. The study observed developer workflows and analyzed the advantages and disadvantages of each prompting strategy.", "result": "Developers used an iterative workflow involving code understanding, edit localization, and output validation. Direct Instruction Prompting was more flexible and easier to specify, while Summary-Mediated Prompting improved code comprehension and prompt scaffolding. The choice of strategy depended on context, such as urgency and familiarity. Usability challenges remain, including the need for granular, traceable summaries and consistency in summary generation.", "conclusion": "LLM-assisted code modification requires more user-friendly prompt interactions. Both prompting strategies have trade-offs, and the optimal strategy depends on developer goals and context. Improving summary granularity, summary-code traceability, and consistency is necessary for better usability."}}
{"id": "2508.01550", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.01550", "abs": "https://arxiv.org/abs/2508.01550", "authors": ["Zhilong Chen", "Chengzong Zhao", "Boyuan Chen", "Dayi Lin", "Yihao Chen", "Arthur Leung", "Gopi Krishnan Rajbahadur", "Gustavo A. Oliva", "Ahmed E. Hassan"], "title": "RepoForge: Training a SOTA Fast-thinking SWE Agent with an End-to-End Data Curation Pipeline Synergizing SFT and RL at Scale", "comment": null, "summary": "Training software engineering (SWE) LLMs is bottlenecked by expensive\ninfrastructure, inefficient evaluation pipelines, scarce training data, and\ncostly quality control. We present RepoForge, an autonomous, end-to-end\npipeline that generates, evaluates, and trains SWE agents at scale. Our key\ncontributions include: (1) RepoForge-8B-Agent, achieving 17.4\\% on\nSWE-Bench-Verified~\\citep{swebench_verified2024}, establishing new\nstate-of-the-art for $\\leq$8B non-thinking LLMs; (2) 7,304 executable\nenvironments auto-generated from real GitHub commits with zero manual\nintervention; (3) 14$\\times$ storage reduction (1.4GB $\\rightarrow$ 102MB per\ninstance) via intelligent dependency management and image pruning; (4) $>$70\\%\nfaster evaluation using a Ray-powered~\\citep{ray2018} distributed RepoForge\nharness; (5) 19,000$\\times$ cheaper labeling through our automated\nSPICE~\\citep{spice2024} difficulty assessment technique. By unifying\nstorage-efficient sandboxing, Ray-powered evaluation harness, automated data\ngeneration, SPICE-based labeling, and bubble-free RL scaffold, we demonstrate\nthat even $\\leq$8B models can reach new state-of-the-art performance on\ndemanding benchmarks like SWE-Bench-Verified. Our approach addresses critical\nbottlenecks in SWE agent training: high storage costs of container-based\nevaluation, inefficient sequential reward pipelines, limited availability of\nhigh-quality training data, expensive manual labeling, and multi-turn RL\npipeline bottlenecks.", "AI": {"tldr": "RepoForge is an autonomous pipeline that drastically improves the efficiency and performance of software engineering LLM training, achieving state-of-the-art results with smaller models and much lower costs for storage, evaluation, and labeling.", "motivation": "Training large language models (LLMs) for software engineering is hampered by expensive infrastructure, inefficient processes, limited data, and high costs for quality control and labeling. The paper seeks to overcome these bottlenecks to improve LLM training for software engineering tasks.", "method": "The authors propose RepoForge, an autonomous end-to-end pipeline for generating, evaluating, and training software engineering agents at scale. RepoForge integrates storage-efficient sandboxing, a Ray-powered evaluation harness for distributed processing, automated data generation from real GitHub commits, SPICE-based automated data labeling, and an RL training scaffold.", "result": "RepoForge-8B-Agent, trained with the pipeline, achieves 17.4% on the SWE-Bench-Verified benchmark, setting a new state-of-the-art for \u22648B non-thinking LLMs. The approach also generates 7,304 executable environments without manual intervention, achieves 14x storage reduction, improves evaluation speed by over 70%, and reduces labeling costs by 19,000x.", "conclusion": "RepoForge addresses and mitigates major bottlenecks in training SWE LLMs\u2014specifically storage, speed, data quality, and cost\u2014leading to significant advances in efficiency and model performance. The work demonstrates that even models with \u22648B parameters can reach state-of-the-art results when trained with efficient, autonomous pipelines."}}
{"id": "2508.02023", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02023", "abs": "https://arxiv.org/abs/2508.02023", "authors": ["Huashan Lei", "Guanping Xiao", "Yepang Liu", "Zheng Zheng"], "title": "PCREQ: Automated Inference of Compatible Requirements for Python Third-party Library Upgrades", "comment": "52 pages, 33 figures", "summary": "Python third-party libraries (TPLs) are essential in modern software\ndevelopment, but upgrades often cause compatibility issues, leading to system\nfailures. These issues fall into two categories: version compatibility issues\n(VCIs) and code compatibility issues (CCIs). Existing tools mainly detect\ndependency conflicts but overlook code-level incompatibilities, with no\nsolution fully automating the inference of compatible versions for both VCIs\nand CCIs. To fill this gap, we propose PCREQ, the first approach to\nautomatically infer compatible requirements by combining version and code\ncompatibility analysis. PCREQ integrates six modules: knowledge acquisition,\nversion compatibility assessment, invoked APIs and modules extraction, code\ncompatibility assessment, version change, and missing TPL completion. PCREQ\ncollects candidate versions, checks for conflicts, identifies API usage,\nevaluates code compatibility, and iteratively adjusts versions to generate a\ncompatible requirements.txt with a detailed repair report. To evaluate PCREQ,\nwe construct REQBench, a large-scale benchmark with 2,095 upgrade test cases\n(including 406 unsolvable by pip). Results show PCREQ achieves a 94.03%\ninference success rate, outperforming PyEGo (37.02%), ReadPyE (37.16%), and\nLLM-based approaches (GPT-4o, DeepSeek V3/R1) by 18-20%. PCREQ processes each\ncase from REQBench in 60.79s on average, demonstrating practical efficiency.\nPCREQ significantly reduces manual effort in troubleshooting upgrades,\nadvancing Python dependency maintenance automation.", "AI": {"tldr": "PCREQ is a novel tool that automatically finds compatible Python library versions by checking both version and code compatibility. It significantly outperforms existing methods in accuracy and speed, reducing manual work in maintaining Python dependencies.", "motivation": "Upgrading Python third-party libraries (TPLs) often causes compatibility issues, including both version and code compatibility problems, which can result in system failures. Existing tools mostly focus on detecting dependency conflicts but neglect code-level incompatibilities and lack solutions to fully automate finding compatible library versions.", "method": "The proposed approach, PCREQ, automatically infers compatible requirements by combining version and code compatibility analysis. It includes six modules for knowledge acquisition, version compatibility assessment, API extraction, code compatibility assessment, version adjustment, and filling missing TPLs. PCREQ analyzes dependencies, checks for both types of compatibility, and iteratively produces a requirements.txt file and repair report.", "result": "PCREQ was evaluated using REQBench, a benchmark with 2,095 upgrade test cases, including difficult ones unsolvable by pip. PCREQ achieved a 94.03% success rate in inferring compatible requirements, significantly outperforming other tools and LLM-based methods. Each case was processed on average in about 61 seconds.", "conclusion": "PCREQ automates and improves the inference of compatible library versions for Python projects, efficiently addressing both version and code compatibility issues, and substantially reduces manual troubleshooting in dependency upgrades."}}
{"id": "2508.02144", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02144", "abs": "https://arxiv.org/abs/2508.02144", "authors": ["Yusaku Kato", "Norihiro Yoshida", "Erina Makihara", "Katsuro Inoue"], "title": "BiFuzz: A Two-Stage Fuzzing Tool for Open-World Video Games", "comment": "4 pages, 5 figures", "summary": "Open-world video games present a broader search space than other games,\nposing challenges for test automation. Fuzzing, which generates new inputs by\nmutating an initial input, is commonly used to uncover failures. In this study,\nwe proposed BiFuzz, a two-stage fuzzer designed for automated testing of\nopen-world video games, and investigated its effectiveness. The results\nrevealed that BiFuzz mutated the overall strategy of gameplay and test cases,\nincluding actual movement paths, step by step. Consequently, BiFuzz can detect\n`stucking' failures. The tool and its video are at\nhttps://github.com/Yusaku-Kato/BiFuzz.", "AI": {"tldr": "BiFuzz is a specialized automated testing tool for open-world video games that mutates both strategies and actions, helping detect movement-related failures more effectively than traditional fuzzers.", "motivation": "Open-world video games have vast and complex environments, making automated testing difficult. Traditional fuzzing techniques may not effectively explore such broad search spaces to uncover failures.", "method": "The authors proposed BiFuzz, a two-stage fuzzer that mutates both the overall gameplay strategy and specific test cases, like movement paths, in a step-by-step fashion. This approach is tailored for testing open-world video games.", "result": "BiFuzz was able to dynamically mutate strategies and movement patterns, enabling it to detect 'stucking' failures in open-world games. The effectiveness was shown in their study and resources are publicly available.", "conclusion": "BiFuzz improves automated playtesting of open-world video games by effectively detecting specific types of failures such as 'stucking' through advanced input mutations."}}
{"id": "2508.02167", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02167", "abs": "https://arxiv.org/abs/2508.02167", "authors": ["Yuxuan Wang", "Cristian Tirelli", "Giovanni Ansaloni", "Laura Pozzi", "David Atienza"], "title": "An MLIR-based Compilation Framework for Control Flow Management on CGRAs", "comment": null, "summary": "Coarse Grained Reconfigurable Arrays (CGRAs) present both high flexibility\nand efficiency, making them well-suited for the acceleration of intensive\nworkloads. Nevertheless, a key barrier towards their widespread adoption is\nposed by CGRA compilation, which must cope with a multi-dimensional space\nspanning both the spatial and the temporal domains. Indeed, state-of-the-art\ncompilers are limited in scope as they mostly deal with the data flow of\napplications, while having little or no support for control flow. Hence, they\nmostly target the mapping of single loops and/or delegate the management of\ncontrol flow divergences to ad-hoc hardware units.\n  Conversely, in this paper we show that control flow can be effectively\nmanaged and optimized at the compilation level, allowing for a broad set of\napplications to be targeted while being hardware-agnostic and achieving high\nperformance. We embody our methodology in a modular compilation framework\nconsisting of transformation and optimization passes, enabling support for\napplications with arbitrary control flows running on abstract CGRA meshes. We\nalso introduce a novel mapping methodology that acts as a compilation back-end,\naddressing the limitations in available CGRA hardware resources and\nguaranteeing a feasible solution in the compilation process. Our framework\nachieves up to 2.1X speedups over state-of-the-art approaches, purely through\ncompilation optimizations.", "AI": {"tldr": "This paper introduces a CGRA compilation framework that, unlike previous work, manages control flow via modular compilation passes instead of specialized hardware. This approach enables broader application support and achieves significant speedups (up to 2.1X) over current methods, relying on advanced compiler optimizations for performance.", "motivation": "Although CGRAs offer high flexibility and efficiency for accelerating compute-intensive workloads, their widespread use is limited by the complexity of compiling for architectures that span both spatial and temporal dimensions. Current CGRA compilers mainly support data flow with little attention to control flow, restricting application domains and relying on specialized hardware for control divergences.", "method": "The authors propose a modular compilation framework consisting of various transformation and optimization passes. Their method directly addresses control flow management at the compilation level for applications with arbitrary control flow, targeting abstract CGRA meshes in a hardware-agnostic fashion. The approach also introduces a novel mapping methodology as a compilation back-end, actively handling hardware resource constraints and ensuring feasible mappings.", "result": "The proposed framework broadens application support beyond simple data flow to include those with complex control flows, while not relying on specialized hardware. The authors report up to 2.1X speedups over state-of-the-art solutions strictly via compiler optimizations.", "conclusion": "Control flow on CGRAs can be efficiently managed at the compilation level, lifting hardware-specific burdens and enabling high-performance, flexible acceleration of varied workloads. Their compilation framework demonstrates significant performance improvement using solely software-level techniques."}}
{"id": "2508.02176", "categories": ["cs.SE", "cs.HC", "D.2.3; D.2.6; D.2.5; H.5.2"], "pdf": "https://arxiv.org/pdf/2508.02176", "abs": "https://arxiv.org/abs/2508.02176", "authors": ["Andrew Tropin"], "title": "Highly Interactive Testing for Uninterrupted Development Flow", "comment": "12 pages, ICFP-2025", "summary": "Highly interactive development environments (HIDEs) enable uninterrupted\ndevelopment flow through continuous program evolution and rapid hypothesis\nchecking. However, traditional testing approaches -- typically executed\nseparately via CLI -- isolate tests from HIDE tooling (interactive debuggers,\nvalue and stack inspectors, etc.) and introduce disruptive delays due to coarse\nexecution granularity and lack of runtime context. This disconnect breaks\ndevelopment flow by exceeding critical attention thresholds. In this paper we\npresent a library that provides runtime representation for tests, allowing\ntight integration with HIDEs, and enabling immediate access to HIDE tooling in\nthe context of test failure. We then describe development workflows enhanced\nwith testing and demonstrate how they achieve subsecond test reexecution times\ncrucial for maintaining developer focus.", "AI": {"tldr": "By integrating tests directly into highly interactive development environments, this paper's library enables instant feedback and easy access to debugging tools, reducing delays and disruptions in developer workflow.", "motivation": "Traditional testing methods disrupt developer workflow in highly interactive development environments (HIDEs), as tests are executed separately from the HIDE tools, causing delays and context-switching issues.", "method": "The authors present a library that provides a runtime representation of tests. This allows tests to be tightly integrated with HIDE tooling, enabling developers to interact with tests and access debugging tools directly within the development environment upon a test failure.", "result": "The proposed approach enables immediate access to HIDE tools when a test fails and achieves subsecond test re-execution times, significantly improving developer focus.", "conclusion": "Integrating tests into the HIDE through runtime representation maintains uninterrupted development flow and quick feedback, overcoming limitations of traditional, separate test execution."}}
{"id": "2508.02233", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02233", "abs": "https://arxiv.org/abs/2508.02233", "authors": ["Vincenzo De Martino", "Joel Casta\u00f1o", "Fabio Palomba", "Xavier Franch", "Silverio Mart\u00ednez-Fern\u00e1ndez"], "title": "A Methodological Framework for LLM-Based Mining of Software Repositories", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used in software engineering\nresearch, offering new opportunities for automating repository mining tasks.\nHowever, despite their growing popularity, the methodological integration of\nLLMs into Mining Software Repositories (MSR) remains poorly understood.\nExisting studies tend to focus on specific capabilities or performance\nbenchmarks, providing limited insight into how researchers utilize LLMs across\nthe full research pipeline. To address this gap, we conduct a mixed-method\nstudy that combines a rapid review and questionnaire survey in the field of\nLLM4MSR. We investigate (1) the approaches and (2) the threats that affect the\nempirical rigor of researchers involved in this field. Our findings reveal 15\nmethodological approaches, nine main threats, and 25 mitigation strategies.\nBuilding on these findings, we present PRIMES 2.0, a refined empirical\nframework organized into six stages, comprising 23 methodological substeps,\neach mapped to specific threats and corresponding mitigation strategies,\nproviding prescriptive and adaptive support throughout the lifecycle of\nLLM-based MSR studies. Our work contributes to establishing a more transparent\nand reproducible foundation for LLM-based MSR research.", "AI": {"tldr": "This paper analyzes how large language models are methodologically integrated into mining software repositories research. Through a mixed-method study, it identifies key approaches, threats, and mitigation strategies, culminating in the PRIMES 2.0 framework to guide rigorous, reproducible LLM-based MSR research.", "motivation": "Large Language Models (LLMs) are being increasingly applied to software engineering research, especially for automating repository mining. However, there is little understanding of how LLMs are methodologically integrated within the Mining Software Repositories (MSR) field, as existing studies mainly discuss specific capabilities and benchmarks without a holistic view.", "method": "The authors performed a mixed-method study, including a rapid literature review and a questionnaire survey focused on the use of LLMs in Mining Software Repositories (LLM4MSR). They systematically identified methodological approaches, potential threats to empirical rigor, and mitigation strategies.", "result": "The study identified 15 methodological approaches, nine main empirical threats, and 25 mitigation strategies. Based on these insights, they proposed PRIMES 2.0, an empirical framework with six stages and 23 methodological substeps, each tied to identified threats and actionable mitigation strategies.", "conclusion": "The resulting PRIMES 2.0 framework provides structured methodological guidance, helping researchers ensure transparency and reproducibility in LLM-based MSR studies. The work lays a more robust foundation for future research in integrating LLMs methodologically in software repository mining."}}
{"id": "2508.02279", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02279", "abs": "https://arxiv.org/abs/2508.02279", "authors": ["Mikio Nakano", "Hironori Takeuchi", "Sadahiro Yoshikawa", "Yoichi Matsuyama", "Kazunori Komatani"], "title": "Dialogue Systems Engineering: A Survey and Future Directions", "comment": "18 pages, 2 figures", "summary": "This paper proposes to refer to the field of software engineering related to\nthe life cycle of dialogue systems as Dialogue Systems Engineering, and surveys\nthis field while also discussing its future directions. With the advancement of\nlarge language models, the core technologies underlying dialogue systems have\nsignificantly progressed. As a result, dialogue system technology is now\nexpected to be applied to solving various societal issues and in business\ncontexts. To achieve this, it is important to build, operate, and continuously\nimprove dialogue systems correctly and efficiently. Accordingly, in addition to\napplying existing software engineering knowledge, it is becoming increasingly\nimportant to evolve software engineering tailored specifically to dialogue\nsystems. In this paper, we enumerate the knowledge areas of dialogue systems\nengineering based on those of software engineering, as defined in the Software\nEngineering Body of Knowledge (SWEBOK) Version 4.0, and survey each area. Based\non this survey, we identify unexplored topics in each area and discuss the\nfuture direction of dialogue systems engineering.", "AI": {"tldr": "This paper introduces and surveys 'Dialogue Systems Engineering,' mapping it to established software engineering frameworks, and identifies future challenges and directions needed to support the evolving field of dialogue systems.", "motivation": "The motivation of this paper is to address the growing demand for robust development, operation, and maintenance frameworks specific to dialogue systems, especially as advanced large language models enable their widespread application in society and business.", "method": "The authors enumerate and align key knowledge areas of Dialogue Systems Engineering with those from the established Software Engineering Body of Knowledge (SWEBOK) Version 4.0, then systematically survey and analyze these areas to identify gaps and suggest future directions.", "result": "The study provides a comprehensive survey of dialogue systems engineering knowledge areas and highlights unexplored topics and challenges unique to this emerging subfield.", "conclusion": "Dialogue systems present unique challenges that require an evolution of existing software engineering practices. Tailored engineering frameworks for dialogue systems are necessary to ensure their effective, reliable, and sustainable lifecycle management."}}
{"id": "2508.02335", "categories": ["cs.SE", "cs.DL"], "pdf": "https://arxiv.org/pdf/2508.02335", "abs": "https://arxiv.org/abs/2508.02335", "authors": ["Matteo Cancellieri", "Martin Docekal", "David Pride", "Morane Gruenpeter", "David Douard", "Petr Knoth"], "title": "Interoperable verification and dissemination of software assets in repositories using COAR Notify", "comment": "8 pages. Presented at the 20th International Conference on Open\n  Repositories, June 15-18 2025, Chicago, Illinois, USA", "summary": "The discoverability, attribution, and reusability of open research software\nare often hindered by its obscurity within academic manuscripts. To address\nthis, the SoFAIR project (2024-2025) introduces a comprehensive workflow\nleveraging machine learning tools for extracting software mentions from\nresearch papers. The project integrates repository systems, authors, and\nservices like HAL and Software Heritage to ensure proper archiving, citation,\nand accessibility of research software in alignment with FAIR principles. To\nenable interoperable communication across the various systems we present an\nintegration of the COAR Notify Protocol, which facilitates automated,\ninteroperable communication among repositories and authors to validate and\ndisseminate software mentions. This paper outlines the SoFAIR workflow and the\nimplementation of the COAR Notify Protocol, emphasising its potential to\nenhance the visibility and credibility of research software as first-class\nbibliographic records.", "AI": {"tldr": "This paper presents the SoFAIR project, which applies machine learning and interoperable protocols to improve the discoverability, citation, and reuse of research software mentioned in academic papers by linking repositories, authors, and archival services.", "motivation": "Open research software is often difficult to discover, cite, and reuse because it is not clearly referenced in academic papers.", "method": "The SoFAIR project uses machine learning to automatically extract software mentions from research papers and integrates various repository systems and protocols\u2014including HAL, Software Heritage, and COAR Notify Protocol\u2014to archive, cite, and communicate software-related information.", "result": "The implementation of this system enables automated, interoperable communication between repositories and authors, improving the archiving, citation, and access to research software.", "conclusion": "The SoFAIR workflow and its use of the COAR Notify Protocol can greatly improve the visibility, credibility, and FAIRness of research software by making it more discoverable and properly cited as a bibliographic record."}}
{"id": "2508.02338", "categories": ["cs.SE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02338", "abs": "https://arxiv.org/abs/2508.02338", "authors": ["Jiahui Wu", "Chengjie Lu", "Aitor Arrieta", "Shaukat Ali", "Thomas Peyrucain"], "title": "Vision Language Model-based Testing of Industrial Autonomous Mobile Robots", "comment": null, "summary": "Autonomous Mobile Robots (AMRs) are deployed in diverse environments (e.g.,\nwarehouses, retail spaces, and offices), where they work alongside humans.\nGiven that human behavior can be unpredictable and that AMRs may not have been\ntrained to handle all possible unknown and uncertain behaviors, it is important\nto test AMRs under a wide range of human interactions to ensure their safe\nbehavior. Moreover, testing in real environments with actual AMRs and humans is\noften costly, impractical, and potentially hazardous (e.g., it could result in\nhuman injury). To this end, we propose a Vision Language Model (VLM)-based\ntesting approach (RVSG) for industrial AMRs developed by PAL Robotics in Spain.\nBased on the functional and safety requirements, RVSG uses the VLM to generate\ndiverse human behaviors that violate these requirements. We evaluated RVSG with\nseveral requirements and navigation routes in a simulator using the latest AMR\nfrom PAL Robotics. Our results show that, compared with the baseline, RVSG can\neffectively generate requirement-violating scenarios. Moreover, RVSG-generated\nscenarios increase variability in robot behavior, thereby helping reveal their\nuncertain behaviors.", "AI": {"tldr": "The paper introduces a simulated testing method for autonomous mobile robots using vision-language models to safely and efficiently generate diverse, challenging human-robot interaction scenarios that help reveal potential safety failures and unpredictable robot behaviors.", "motivation": "Autonomous Mobile Robots (AMRs) operate in environments shared with humans whose behavior is unpredictable. Testing all possible human-robot interactions is challenging, costly, and can be hazardous in real life. Thus, there is a need for effective, safe, and comprehensive testing methods for AMRs' safety and functionality in response to diverse human behaviors.", "method": "The authors propose a Vision Language Model (VLM)-based testing approach called RVSG. This approach employs VLM to generate a variety of human behaviors that intentionally violate the functional and safety requirements of AMRs. Testing is conducted in a simulator using the latest robot from PAL Robotics, evaluating the ability to generate requirement-violating scenarios across different requirements and navigation routes.", "result": "Compared to the baseline, the RVSG approach can effectively generate scenarios that violate requirements, increasing the variability in robot behavior and helping to uncover previously unknown or uncertain behaviors in AMRs.", "conclusion": "The VLM-based RVSG testing approach is effective for generating diverse, challenging scenarios for AMR testing, improving the identification of requirement-violating and uncertain behaviors without the risks and limitations of real-world testing."}}
{"id": "2508.02397", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02397", "abs": "https://arxiv.org/abs/2508.02397", "authors": ["Lida Zhao", "Chaofan Li", "Yueming Wu", "Lyuye Zhang", "Jiahui Wu", "Chengwei Liu", "Sen Chen", "Yutao Hu", "Zhengzi Xu", "Yi Liu", "Jingquan Ge", "Jun Sun", "Yang Liu"], "title": "JC-Finder: Detecting Java Clone-based Third-Party Library by Class-level Tree Analysis", "comment": null, "summary": "While reusing third-party libraries (TPL) facilitates software development,\nits chaotic management has brought great threats to software maintenance and\nthe unauthorized use of source code also raises ethical problems such as\nmisconduct on copyrighted code. To identify TPL reuse in projects, Software\nComposition Analysis (SCA) is employed, and two categories of SCA techniques\nare used based on how TPLs are introduced: clone-based SCA and\npackage-manager-based SCA (PM-based SCA). Although introducing TPLs by clones\nis prevalent in Java, no clone-based SCA tools are specially designed for Java.\nAlso, directly applying clone-based SCA techniques from other tools is\nproblematic. To fill this gap, we introduce JC-Finder, a novel clone-based SCA\ntool that aims to accurately and comprehensively identify instances of TPL\nreuse introduced by source code clones in Java projects. JC-Finder achieves\nboth accuracy and efficiency in identifying TPL reuse from code cloning by\ncapturing features at the class level, maintaining inter-function\nrelationships, and excluding trivial or duplicated elements. To evaluate the\nefficiency of JC-Finder, we applied it to 9,965 most popular Maven libraries as\nreference data and tested the TPL reuse of 1,000 GitHub projects. The result\nshows that JC-Finder achieved an F1-score of 0.818, outperforming the other\nfunction-level tool by 0.427. The average time taken for resolving TPL reuse is\n14.2 seconds, which is approximately 9 times faster than the other tool. We\nfurther applied JC-Finder to 7,947 GitHub projects, revealing TPL reuse by code\nclones in 789 projects (about 9.89% of all projects) and identifying a total of\n2,142 TPLs. JC-Finder successfully detects 26.20% more TPLs that are not\nexplicitly declared in package managers.", "AI": {"tldr": "JC-Finder is a new tool for Java that efficiently and accurately detects reused third-party libraries introduced by code clones, outperforming existing tools and revealing many libraries not declared in package managers.", "motivation": "Reusing third-party libraries (TPL) is common in software development, but chaotic management and unauthorized use, especially through code clones, cause maintenance and ethical issues. Existing SCA tools either focus on package management or do not cater to Java's prevalent clone-based reuse.", "method": "The authors developed JC-Finder, a clone-based SCA tool designed specifically for Java. It operates by capturing features at the class level, preserving inter-function relationships, and filtering out trivial or duplicated code elements. The tool was evaluated using 9,965 Maven libraries as reference data and tested on over 1,000 GitHub projects.", "result": "JC-Finder achieved an F1-score of 0.818, outperforming a function-level tool by 0.427 and running approximately 9 times faster. When scaled to 7,947 GitHub projects, it identified TPL reuse via code clones in 789 projects and detected 2,142 TPLs, including 26.20% more TPLs not declared in package managers.", "conclusion": "JC-Finder fills a critical gap for Java clone-based SCA, enabling more accurate and efficient detection of third-party library reuse through code cloning, surpassing existing tools in accuracy, speed, and comprehensiveness."}}
{"id": "2508.02407", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02407", "abs": "https://arxiv.org/abs/2508.02407", "authors": ["Xinyi Wang", "Qinghua Xu", "Paolo Arcaini", "Shaukat Ali", "Thomas Peyrucain"], "title": "Quantum Machine Learning-based Test Oracle for Autonomous Mobile Robots", "comment": null, "summary": "Robots are increasingly becoming part of our daily lives, interacting with\nboth the environment and humans to perform their tasks. The software of such\nrobots often undergoes upgrades, for example, to add new functionalities, fix\nbugs, or delete obsolete functionalities. As a result, regression testing of\nrobot software becomes necessary. However, determining the expected correct\nbehavior of robots (i.e., a test oracle) is challenging due to the potentially\nunknown environments in which the robots must operate. To address this\nchallenge, machine learning (ML)-based test oracles present a viable solution.\nThis paper reports on the development of a test oracle to support regression\ntesting of autonomous mobile robots built by PAL Robotics (Spain), using\nquantum machine learning (QML), which enables faster training and the\nconstruction of more precise test oracles. Specifically, we propose a hybrid\nframework, QuReBot, that combines both quantum reservoir computing (QRC) and a\nsimple neural network, inspired by residual connection, to predict the expected\nbehavior of a robot. Results show that QRC alone fails to converge in our case,\nyielding high prediction error. In contrast, QuReBot converges and achieves 15%\nreduction of prediction error compared to the classical neural network\nbaseline. Finally, we further examine QuReBot under different configurations\nand offer practical guidance on optimal settings to support future robot\nsoftware testing.", "AI": {"tldr": "The paper introduces QuReBot, a quantum-classical hybrid test oracle for robot regression testing, which reduces prediction error by 15% over classical neural networks, addressing the challenge of testing autonomous robots in uncertain environments.", "motivation": "Regression testing is necessary for robot software that frequently updates, but it is difficult to define correct robot behavior (test oracles) due to unpredictable environments. Machine learning-based oracles could solve this problem, but further innovation is needed for reliable prediction.", "method": "The authors developed a test oracle using quantum machine learning (QML), specifically a hybrid framework called QuReBot. This framework combines quantum reservoir computing (QRC) and a simple neural network with a residual connection, aiming to predict robot behavior more accurately and efficiently.", "result": "The QRC approach alone failed to converge and yielded high prediction errors. However, the QuReBot hybrid framework successfully converged and reduced prediction error by 15% compared to a classical neural network baseline. The framework's performance was further analyzed under different settings.", "conclusion": "QuReBot, the hybrid quantum-classical framework, outperforms both standalone quantum and classical neural approaches for predicting robot behavior, showing promise for effective regression testing in robot software. Practical recommendations for framework configurations are also provided."}}
{"id": "2508.02455", "categories": ["cs.SE", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.02455", "abs": "https://arxiv.org/abs/2508.02455", "authors": ["Daniele Cipollone", "Egor Bogomolov", "Arie van Deursen", "Maliheh Izadi"], "title": "TreeRanker: Fast and Model-agnostic Ranking System for Code Suggestions in IDEs", "comment": null, "summary": "Token-level code completion is one of the most critical features in modern\nIntegrated Development Environments (IDEs). It assists developers by suggesting\nrelevant identifiers and APIs during coding. While completions are typically\nderived from static analysis, their usefulness depends heavily on how they are\nranked, as correct predictions buried deep in the list are rarely seen by\nusers. Most current systems rely on hand-crafted heuristics or lightweight\nmachine learning models trained on user logs, which can be further improved to\ncapture context information and generalize across projects and coding styles.\nIn this work, we propose a new scoring approach to ranking static completions\nusing language models in a lightweight and model-agnostic way. Our method\norganizes all valid completions into a prefix tree and performs a single greedy\ndecoding pass to collect token-level scores across the tree. This enables a\nprecise token-aware ranking without needing beam search, prompt engineering, or\nmodel adaptations. The approach is fast, architecture-agnostic, and compatible\nwith already deployed models for code completion. These findings highlight a\npractical and effective pathway for integrating language models into already\nexisting tools within IDEs, and ultimately providing smarter and more\nresponsive developer assistance.", "AI": {"tldr": "A novel, efficient method uses language models to better rank code completion suggestions in IDEs, improving relevance without changing existing systems.", "motivation": "Token-level code completion is important for developer productivity in IDEs, but current ranking methods are limited either by hand-crafted heuristics or basic machine learning models that lack deep contextual understanding and generalization.", "method": "The authors propose a lightweight, model-agnostic scoring method for ranking static code completions. Their technique involves organizing valid completions into a prefix tree and using a single greedy decoding pass of language models to score tokens across all completions, eliminating the need for beam search or extensive model adaptation.", "result": "The new approach achieves precise, token-aware ranking of code completion suggestions efficiently and without requiring changes to existing language models or IDE architecture.", "conclusion": "This work demonstrates a practical, fast, and architecture-compatible way to significantly improve code completion ranking in modern IDEs by integrating language model scoring, thus enhancing developer experience."}}
{"id": "2508.02473", "categories": ["cs.SE", "cs.LG", "68N30", "D.2.3; D.1.2; I.2.2"], "pdf": "https://arxiv.org/pdf/2508.02473", "abs": "https://arxiv.org/abs/2508.02473", "authors": ["Xinfang Chen", "Siyang Xiao", "Xianying Zhu", "Junhong Xie", "Ming Liang", "Dajun Chen", "Wei Jiang", "Yong Li", "Peng Di"], "title": "An Efficient and Adaptive Next Edit Suggestion Framework with Zero Human Instructions in IDEs", "comment": "13 pages", "summary": "Code editing, including modifying, refactoring, and maintaining existing\ncode, is the most frequent task in software development and has garnered\nsignificant attention from AI-powered tools. However, existing solutions that\ntranslate explicit natural language instructions into code edits face critical\nlimitations, such as heavy reliance on human instruction input and high\nlatency, which hinder their effective integration into a developer's workflow.\nWe observe that developers' habitual behaviors and coding objectives are often\nreflected in their historical editing patterns, making this data key to\naddressing existing limitations. To leverage these insights, we propose NES\n(Next Edit Suggestion), an LLM-driven code editing framework that delivers an\ninstruction-free and low-latency experience. Built on a dual-model architecture\nand trained with our high-quality SFT and DAPO datasets, NES enhances\nproductivity by understanding developer intent while optimizing inference to\nminimize latency. NES is a scalable, industry-ready solution with a continuous\nTab key interaction workflow, seamlessly adopted by a FinTech company with over\n20,000 developers. Evaluations on real-world datasets show NES achieves 75.6%\nand 81.6% accuracy in two tasks of predicting next edit locations, alongside\n91.36% ES and 27.7% EMR for intent-aligned edits, outperforming SOTA models.\nOur open-sourced SFT and DAPO datasets have been demonstrated to enhance the\nperformance of open-source CodeLLMs. The demonstration of NES is available at\nhttps://youtu.be/yGoyYOe6fbY.", "AI": {"tldr": "NES is a new LLM-driven code editing system that effectively predicts and suggests code edits without explicit instructions, achieving better accuracy and lower latency than existing tools, and is already used at scale in industry.", "motivation": "Existing AI-powered code editing tools require explicit natural language instructions and suffer from high latency, which makes them less effective for seamless developer workflows. There's a need for more efficient and intuitive code editing assistance.", "method": "The authors propose NES (Next Edit Suggestion), an LLM-based code editing framework that does not require explicit instructions. NES uses a dual-model architecture and is trained on two novel datasets (SFT and DAPO), focusing on understanding developers\u2019 intent from historical editing patterns and optimizing for low latency.", "result": "NES achieves high accuracy in predicting code edit locations (75.6% and 81.6% on two tasks) and strong scores for intent-aligned edits (91.36% ES and 27.7% EMR), outperforming state-of-the-art code LLMs. Its datasets also improve open-source model performance.", "conclusion": "NES offers instruction-free, low-latency code editing suggestions that integrate smoothly with developer workflows and scale to large organizations. Its practical deployment and superior performance metrics indicate it is a viable, industry-ready solution for code editing."}}
{"id": "2508.02487", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02487", "abs": "https://arxiv.org/abs/2508.02487", "authors": ["Elijah Kayode Adejumo", "Brittany Johnson", "Mariam Guizani"], "title": "Commit Stability as a Signal for Risk in Open-Source Projects", "comment": null, "summary": "Open source software (OSS) generates trillions of dollars in economic value\nand has become essential to technical infrastructures worldwide. As\norganizations increasingly depend on OSS, understanding project evolution is\ncritical. While existing metrics provide insights into project health, one\ndimension remains understudied: project resilience -- the ability to return to\nnormal operations after disturbances such as contributor departures, security\nvulnerabilities, and bug report spikes. We hypothesize that stable commit\npatterns reflect underlying project characteristics such as mature governance,\nsustained contributors, and robust development processes that enable\nresilience. Building on the Composite Stability Index (CSI) framework, we\nempirically validate commit frequency patterns across 100 highly ranked\nrepositories. Our findings reveal that only 2\\% of repositories exhibit daily\nstability, 29\\% achieve weekly stability, and 50\\% demonstrate monthly\nstability, while half remain unstable across all temporal levels. Programming\nlanguages and blockchain applications were the most stable. We identified two\nexemplary repositories that achieved stability at all three granularities,\nwhose governance models, CI cadence, and release policies could serve as\nreference frameworks. We observed that large yearly commit throughput does not\nnecessarily correlate with stability. Beyond commits, stability can be enriched\nwith issue-resolution times, PR merge rates, and community-engagement metrics\nto broaden resilience assessment and sharpen stability-based risk evaluation.", "AI": {"tldr": "Most open source projects lack stable development patterns, which are linked to resilience and governance quality. Frequent commits alone don't guarantee stability; more comprehensive metrics are needed to assess OSS project risk and health.", "motivation": "Open source software (OSS) is now critical to global technical infrastructure and economic value. As its organizational importance grows, understanding how these projects evolve\u2014especially their resilience to disruptions\u2014is essential, yet understudied.", "method": "The study builds on the Composite Stability Index (CSI) framework, empirically validating commit frequency patterns across 100 top-ranked OSS repositories. The analysis examines stability at daily, weekly, and monthly levels, and investigates correlations with factors like programming language, application domain, and governance models.", "result": "Findings show that only 2% of repositories are stable daily, 29% weekly, and 50% monthly; the rest are unstable. Languages and blockchain projects tend to foster higher stability. Only two repositories showed stability across all granularities, linked to mature governance and consistent development processes. High commit volume does not equate to higher stability. Additional metrics like issue resolution and PR merges offer more depth.", "conclusion": "Stable commit patterns suggest organizational maturity and resilience but are relatively rare among popular OSS projects. Broader, multi-faceted metrics are needed for a complete and reliable measure of resilience and risk in OSS projects."}}
{"id": "2508.02497", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.02497", "abs": "https://arxiv.org/abs/2508.02497", "authors": ["Elijah Kayode Adejumo", "Brittany Johnson", "Mariam Guizani"], "title": "Bridging Language Gaps in Open-Source Documentation with Large-Language-Model Translation", "comment": null, "summary": "While open source communities attract diverse contributors globally, few\nrepositories provide essential documentation in languages other than English.\nLarge language models (LLMs) have demonstrated remarkable capabilities in\nsoftware engineering tasks and translations across domains. However, little is\nknown about LLM capabilities in translating open-source technical\ndocumentation, which mixes natural language, code, URLs, and markdown\nformatting. To understand the need and potential for LLMs in technical\ndocumentation translation, we evaluated community translation activity and\nEnglish-to-German translations of 50 README files using OpenAI's ChatGPT 4 and\nAnthropic's Claude. We found scarce translation activity, mostly in larger\nrepositories and community-driven in nature. LLM performance comparison\nsuggests they can provide accurate translations. However, analysis revealed\nfidelity challenges: both models struggled to preserve structural components\n(e.g., hyperlinks) and exhibited formatting inconsistencies. These findings\nhighlight both promise and challenges of LLM-assisted documentation\ninternationalization. As a first step toward translation-aware continuous\nintegration pipelines, we introduce TRIFID, an early-stage translation fidelity\nscoring framework that automatically checks how well translations preserve\ncode, links, and formatting. Our efforts provide a foundation for automated\nLLM-driven support for creating and maintaining open source documentation.", "AI": {"tldr": "LLMs like ChatGPT 4 and Claude can translate open source documentation with reasonable accuracy, but often struggle to preserve layouts, code, and links. The paper introduces TRIFID, a tool to automatically check translation fidelity, offering a foundation for improved multilingual documentation in open source projects.", "motivation": "Most open source repositories lack essential documentation in languages other than English, limiting accessibility for non-English speakers. The study is motivated by the need to make technical documentation more globally accessible, and explores whether large language models (LLMs) can effectively translate such documentation, which contains a mix of language, code, and formatting.", "method": "The authors analyzed community translation activities within open source repositories and compared English-to-German translations of 50 README files using OpenAI's ChatGPT 4 and Anthropic's Claude. They then evaluated fidelity challenges and introduced TRIFID, a translation fidelity scoring framework that assesses preservation of code, links, and formatting.", "result": "Translation activities are rare and mostly present in larger repositories. LLMs can generate overall accurate translations, but struggle with structural fidelity, such as preserving hyperlinks and maintaining consistent formatting.", "conclusion": "LLMs show promise for assisting in technical documentation translation, but face challenges in structural and formatting fidelity. The newly introduced TRIFID framework provides an effective way to automatically assess translation fidelity, laying the groundwork for further automated, LLM-driven support in open source documentation internationalization."}}
{"id": "2508.02541", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02541", "abs": "https://arxiv.org/abs/2508.02541", "authors": ["Peter Hamfelt", "Ricardo Britto", "Lincoln Rocha", "Camilo Almendra"], "title": "Automatic Identification of Machine Learning-Specific Code Smells", "comment": null, "summary": "Machine learning (ML) has rapidly grown in popularity, becoming vital to many\nindustries. Currently, the research on code smells in ML applications lacks\ntools and studies that address the identification and validity of ML-specific\ncode smells. This work investigates suitable methods and tools to design and\ndevelop a static code analysis tool (MLpylint) based on code smell criteria.\nThis research employed the Design Science Methodology. In the problem\nidentification phase, a literature review was conducted to identify ML-specific\ncode smells. In solution design, a secondary literature review and\nconsultations with experts were performed to select methods and tools for\nimplementing the tool. We evaluated the tool on data from 160 open-source ML\napplications sourced from GitHub. We also conducted a static validation through\nan expert survey involving 15 ML professionals. The results indicate the\neffectiveness and usefulness of the MLpylint. We aim to extend our current\napproach by investigating ways to introduce MLpylint seamlessly into\ndevelopment workflows, fostering a more productive and innovative developer\nenvironment.", "AI": {"tldr": "This paper introduces MLpylint, a static analysis tool designed specifically to detect machine learning code smells. Through literature reviews, expert consultations, and validation on open-source projects, the tool proves effective, with plans for further integration into development workflows.", "motivation": "Current research on code smells in machine learning (ML) applications is lacking in tools and studies focused on ML-specific code smells. As ML grows more vital to various industries, identifying and addressing these smells is increasingly important.", "method": "The paper uses the Design Science Methodology. It starts with a literature review to identify ML-specific code smells, followed by further literature review and expert consultation to design the static code analysis tool (MLpylint). The tool is then evaluated using data from 160 open-source ML projects from GitHub and validated through a survey of 15 ML professionals.", "result": "The evaluation and expert validation demonstrate that MLpylint is effective and useful in identifying ML-specific code smells.", "conclusion": "The developed MLpylint tool is effective for the identification of ML-specific code smells, and future work will focus on integrating this tool into real-world development workflows to enhance developer productivity and innovation."}}
{"id": "2508.02611", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02611", "abs": "https://arxiv.org/abs/2508.02611", "authors": ["Vali Tawosia", "Salwa Alamir", "Xiaomo Liu", "Manuela Veloso"], "title": "Meta-RAG on Large Codebases Using Code Summarization", "comment": null, "summary": "Large Language Model (LLM) systems have been at the forefront of applied\nArtificial Intelligence (AI) research in a multitude of domains. One such\ndomain is software development, where researchers have pushed the automation of\na number of code tasks through LLM agents. Software development is a complex\necosystem, that stretches far beyond code implementation and well into the\nrealm of code maintenance. In this paper, we propose a multi-agent system to\nlocalize bugs in large pre-existing codebases using information retrieval and\nLLMs. Our system introduces a novel Retrieval Augmented Generation (RAG)\napproach, Meta-RAG, where we utilize summaries to condense codebases by an\naverage of 79.8\\%, into a compact, structured, natural language representation.\nWe then use an LLM agent to determine which parts of the codebase are critical\nfor bug resolution, i.e. bug localization. We demonstrate the usefulness of\nMeta-RAG through evaluation with the SWE-bench Lite dataset. Meta-RAG scores\n84.67 % and 53.0 % for file-level and function-level correct localization\nrates, respectively, achieving state-of-the-art performance.", "AI": {"tldr": "The paper introduces Meta-RAG, a multi-agent LLM system that effectively pinpoints bugs in large codebases by condensing code into summarized natural language forms and using LLMs for bug localization, achieving state-of-the-art accuracy on benchmark datasets.", "motivation": "Software development requires not only code implementation but also efficient code maintenance, including bug localization in large codebases, which is challenging due to code complexity and scale. Current automated solutions using LLMs have limitations in handling these tasks effectively.", "method": "The authors propose a multi-agent system for bug localization in large codebases. The core of their approach is Meta-RAG (a novel Retrieval Augmented Generation technique), which summarizes codebases using information retrieval and LLMs to produce a compact natural language representation. An LLM agent then analyzes these summaries to identify crucial code segments related to bugs.", "result": "Using the SWE-bench Lite dataset, Meta-RAG achieved 84.67% accuracy in file-level bug localization and 53.0% in function-level localization, surpassing previous state-of-the-art methods.", "conclusion": "Meta-RAG demonstrates highly effective state-of-the-art bug localization in large codebases, providing a practical tool for code maintenance and paving the way for further LLM-based automation in software development beyond code generation."}}
