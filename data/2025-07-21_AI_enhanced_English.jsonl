{"id": "2507.13494", "categories": ["cs.PL", "stat.CO"], "pdf": "https://arxiv.org/pdf/2507.13494", "abs": "https://arxiv.org/abs/2507.13494", "authors": ["Feras A. Saad", "Wonyeol Lee"], "title": "Random Variate Generation with Formal Guarantees", "comment": null, "summary": "This article introduces a new approach to principled and practical random\nvariate generation with formal guarantees. The key idea is to first specify the\ndesired probability distribution in terms of a finite-precision numerical\nprogram that defines its cumulative distribution function (CDF), and then\ngenerate exact random variates according to this CDF. We present a universal\nand fully automated method to synthesize exact random variate generators given\nany numerical CDF implemented in any binary number format, such as\nfloating-point, fixed-point, and posits. The method is guaranteed to operate\nwith the same precision used to specify the CDF, does not overflow, avoids\nexpensive arbitrary-precision arithmetic, and exposes a consistent API. The\nmethod rests on a novel space-time optimal implementation for the class of\ngenerators that attain the information-theoretically optimal Knuth and Yao\nentropy rate, consuming the least possible number of input random bits per\noutput variate. We develop a random variate generation library using our method\nin C and evaluate it on a diverse set of ``continuous'' and ``discrete''\ndistributions, showing competitive runtime with the state-of-the-art GNU\nScientific Library while delivering higher accuracy, entropy efficiency, and\nautomation.", "AI": {"tldr": "The paper proposes a universal, efficient, and automated algorithm for generating random numbers from any finite-precision CDF, with strong accuracy and entropy guarantees. Their C library outperforms standard tools in efficiency and precision, making random variate generation easier and more reliable.", "motivation": "Random variate generation is a fundamental problem in numerical computing, simulations, and probabilistic modeling. Existing methods often lack formal accuracy or entropy efficiency guarantees, require manual implementation for each distribution, or need expensive high-precision arithmetic. The authors are motivated to create a universal, formally guaranteed, and efficient solution for generating random variates from arbitrary distributions described by finite-precision CDFs.", "method": "The authors propose a universal method that automatically synthesizes exact random variate generators from numerical cumulative distribution functions (CDFs) written in any binary number format (e.g., floating-point, fixed-point, posits). Their method achieves information-theoretically optimal entropy rates (matching the Knuth and Yao lower bound), operates at the same precision as the CDF, avoids overflows and arbitrary-precision arithmetic, and provides a consistent API. They implemented this method as a C library and benchmarked it against a standard library.", "result": "The resulting library supports a wide range of continuous and discrete distributions, provides higher accuracy and better entropy efficiency than the GNU Scientific Library, operates at competitive runtimes, and offers a fully automated workflow.", "conclusion": "The paper presents a practical yet theoretically sound framework for random variate generation from arbitrary numerical CDFs with formal guarantees on precision and entropy efficiency. The approach is universally applicable, efficient, and improves upon existing solutions in accuracy, automation, and resource usage."}}
{"id": "2507.13533", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.13533", "abs": "https://arxiv.org/abs/2507.13533", "authors": ["Priyam Gupta"], "title": "Increasing the Expressiveness of a Gradual Verifier", "comment": "Presented at the 52nd ACM SIGPLAN Symposium on Principles of\n  Programming Languages (POPL 2025) Student Research Competition", "summary": "Static verification provides strong correctness guarantees for code; however,\nfully specifying programs for static verification is a complex, burdensome\nprocess for users. Gradual verification was introduced to make this process\neasier by supporting the verification of partially specified programs. The only\ncurrently working gradual verifier, Gradual C0, successfully verifies heap\nmanipulating programs, but lacks expressiveness in its specification language.\nThis paper describes the design and implementation of an extension to Gradual\nC0 that supports unfolding expressions, which allow more intuitive\nspecifications of recursive heap data structures.", "AI": {"tldr": "This paper enhances Gradual C0 with unfolding expressions, enabling more intuitive and expressive specifications for recursive heap data structures, thus lowering the barriers to gradual verification.", "motivation": "Static verification is reliable but too demanding due to complex specification requirements. Gradual verification aims to ease this process but is limited by the current lack of expressive specification languages, particularly for heap data structures.", "method": "The authors design and implement an extension to the Gradual C0 gradual verification system to support unfolding expressions, enhancing the specification of recursive heap data structures.", "result": "The extended Gradual C0 now supports unfolding expressions, making it possible to specify recursive heap data structures more intuitively.", "conclusion": "The extension improves Gradual C0's expressiveness, thereby making gradual verification more practical for heap-manipulating programs by enabling better and easier specification of recursive data structures."}}
{"id": "2507.13774", "categories": ["cs.PL", "cs.LO", "D.3.1; F.3.2; F.4.1"], "pdf": "https://arxiv.org/pdf/2507.13774", "abs": "https://arxiv.org/abs/2507.13774", "authors": ["Arthur Adjedj", "Meven Lennon-Bertrand", "Thibaut Benjamin", "Kenji Maillard"], "title": "AdapTT: Functoriality for Dependent Type Casts", "comment": null, "summary": "The ability to cast values between related types is a leitmotiv of many\nflavors of dependent type theory, such as observational type theories,\nsubtyping, or cast calculi for gradual typing. These casts all exhibit a common\nstructural behavior that boils down to the pervasive functoriality of type\nformers. We propose and extensively study a type theory, called AdapTT, which\nmakes systematic and precise this idea of functorial type formers, with respect\nto an abstract notion of adapters relating types. Leveraging descriptions for\nfunctorial inductive types in AdapTT, we derive structural laws for type casts\non general inductive type formers.", "AI": {"tldr": "AdapTT is a new type theory that formalizes the functorial structure underlying type casts in dependent type theories, enabling a precise understanding and derivation of their structural laws.", "motivation": "There is a recurring need to cast values between related types in various dependent type theories, but a systematic and precise understanding of the structural properties underlying these casts is lacking.", "method": "The authors design and conduct an extensive study of AdapTT, a new type theory, based on the idea of functorial type formers related by abstract adapters, and utilize descriptions for functorial inductive types to derive general structural laws.", "result": "The study results in a robust framework (AdapTT) that formalizes and enables derivation of structural laws for type casts in general inductive type formers, clarifying the functorial properties involved.", "conclusion": "The AdapTT type theory systematizes the common structural behavior of type casts by employing functorial type formers and abstract adapters, providing a general and precise framework for reasoning about type casting in dependent type theories."}}
{"id": "2507.13792", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.13792", "abs": "https://arxiv.org/abs/2507.13792", "authors": ["Riccardo Bianchini", "Francesco Dagnino", "Paola Giannini", "Elena Zucca"], "title": "Don't exhaust, don't waste", "comment": "Submitted to JFP (Journal of Functional Programming)", "summary": "We extend the semantics and type system of a lambda calculus equipped with\ncommon constructs to be resource-aware. That is, the semantics keep tracks of\nthe usage of resources, and is stuck, besides in case of type errors, if either\na needed resource is exhausted, or a provided resource would be wasted. In such\nway, the type system guarantees, besides standard soundness, that for\nwell-typed programs there is a computation where no resource gets either\nexhausted or wasted.\n  The no-waste extension is parametric on an arbitrary grade algebra, modeling\nan arbitrary assortment of possible usages, and does not require ad-hoc changes\nto the underlying language. To this end, the semantics needs to be formalized\nin big-step style; as a consequence, expressing and proving (resource-aware)\nsoundness is challenging, and is achieved by applying recent techniques based\non coinductive reasoning.", "AI": {"tldr": "This paper proposes a resource-aware extension to the lambda calculus, using a parametric algebra and coinductive reasoning to ensure that well-typed programs do not waste or exhaust resources, all without modifying fundamental language elements.", "motivation": "Traditional lambda calculus type systems do not account for resource usage, leading to the possibility of resource exhaustion or waste. The paper is motivated by the need for a calculus that guarantees resource-aware computations, preventing resource misuse in well-typed programs.", "method": "The authors extend the semantics and type system of a lambda calculus by integrating resource awareness using a parametric grade algebra, and formalize the semantics in a big-step style. The type system and semantics are designed to avoid both exhaustion and wastage of resources, with soundness proved via coinductive reasoning techniques.", "result": "The extended type system and semantics guarantee that well-typed lambda calculus programs have computations where resources are neither exhausted nor wasted. This is achieved in a manner parametric with respect to possible ways resources can be used, without making ad-hoc changes to the language structure.", "conclusion": "The paper provides a general and robust resource-aware extension to lambda calculus type systems and semantics, ensuring sound, efficient resource usage for well-typed programs through a parametric approach and advanced proof techniques."}}
{"id": "2507.13481", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.13481", "abs": "https://arxiv.org/abs/2507.13481", "authors": ["Arthur Bueno", "Bruno Cafeo", "Maria Cagnin", "Awdren Font\u00e3o"], "title": "Socio-Technical Smell Dynamics in Code Samples: A Multivocal Review on Emergence, Evolution, and Co-Occurrence", "comment": "12 pages; 2 figures; Preprint with the original submission accepted\n  for publication at 39th Brazilian Symposium on Software Engineering (SBES)", "summary": "Code samples play a pivotal role in open-source ecosystems (OSSECO), serving\nas lightweight artifacts that support knowledge transfer, onboarding, and\nframework adoption. Despite their instructional relevance, these samples are\noften governed informally, with minimal review and unclear ownership, which\nincreases their exposure to socio-technical degradation. In this context, the\nco-occurrence and longitudinal interplay of code smells (e.g., large classes,\npoor modularity) and community smells (e.g., lone contributors, fragmented\ncommunication) become particularly critical. While each type of smell has been\nstudied in isolation, little is known about how community-level dysfunctions\nanticipate or exacerbate technical anomalies in code samples over time. This\nstudy investigates how code and community smells emerge, co-occur, and evolve\nwithin code samples maintained in OSSECOs. A Multivocal Literature Review\nprotocol was applied, encompassing 30 peer-reviewed papers and 17\npractitioner-oriented sources (2013-2024). Thematic synthesis was conducted to\nidentify recurring socio-technical patterns related to smell dynamics. Nine\npatterns were identified, showing that community smells often precede or\nreinforce technical degradation in code samples. Symptoms such as \"radio\nsilence\" and centralized ownership were frequently associated with persistent\nstructural anomalies. Additionally, limited onboarding, the absence of\ncontinuous refactoring, and informal collaboration emerged as recurring\nconditions for smell accumulation. Conclusion: In OSSECOs, particularly within\ncode samples, community-level dysfunctions not only correlate with but often\nsignal maintainability decay. These findings underscore the need for\nsocio-technical quality indicators and lightweight governance mechanisms\ntailored to shared instructional artifacts.", "AI": {"tldr": "The paper reviews literature to show that in open-source code samples, community-related issues frequently signal or cause technical problems. Early detection of such issues, along with better governance, is key to maintainability.", "motivation": "Code samples are crucial for knowledge sharing in open-source ecosystems, yet governance over them is often informal, leading to concerns about their technical and community quality (socio-technical degradation). The study aims to explore the interplay between code smells and community smells in these samples over time.", "method": "The authors applied a Multivocal Literature Review (MLR) protocol, analyzing 30 peer-reviewed and 17 practitioner-oriented sources from 2013-2024. They used thematic synthesis to identify patterns in how code and community smells co-occur and evolve within code samples in open-source ecosystems.", "result": "Nine recurring socio-technical patterns were identified. The findings show that community smells often precede or reinforce the presence and persistence of technical anomalies ('code smells') in code samples. Symptoms like 'radio silence', centralized ownership, inadequate onboarding, lack of continuous refactoring, and informal collaboration were major contributors to smell accumulation.", "conclusion": "Community-level dysfunctions in open-source code samples not only correlate with technical degradation but can also act as early indicators of it. Socio-technical quality indicators and lightweight governance mechanisms should be developed for instructional artifacts like code samples to maintain their value and reliability."}}
{"id": "2507.13499", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.13499", "abs": "https://arxiv.org/abs/2507.13499", "authors": ["Chandra Maddila", "Negar Ghorbani", "James Saindon", "Parth Thakkar", "Vijayaraghavan Murali", "Rui Abreu", "Jingyue Shen", "Brian Zhou", "Nachiappan Nagappan", "Peter C. Rigby"], "title": "AI-Assisted Fixes to Code Review Comments at Scale", "comment": null, "summary": "Aim. There are 10s of thousands of code review comments each week at Meta. We\ndeveloped Metamate for Code Review (MetaMateCR) that provides AI-assisted fixes\nfor reviewer comments in production at scale.\n  Method. We developed an internal benchmark of 64k <review comment, patch>\ndata points to fine-tune Llama models. Once our models achieve reasonable\noffline results, we roll them into production. To ensure that our AI-assisted\nfixes do not negatively impact the time it takes to do code reviews, we conduct\nrandomized controlled safety trials as well as full production experiments.\n  Offline Results. As a baseline, we compare GPT-4o to our small and large\nLlama models. In offline results, our LargeLSFT model creates an exact match\npatch 68% of the time outperforming GPT-4o by 9 percentage points (pp). The\ninternal models also use more modern Hack functions when compared to the PHP\nfunctions suggested by GPT-4o.\n  Safety Trial. When we roll MetaMateCR into production in a safety trial that\ncompares no AI patches with AI patch suggestions, we see a large regression\nwith reviewers taking over 5% longer to conduct reviews. After investigation,\nwe modify the UX to only show authors the AI patches, and see no regressions in\nthe time for reviews.\n  Production. When we roll LargeLSFT into production, we see an\nActionableToApplied rate of 19.7%, which is a 9.2pp improvement over GPT-4o.\nOur results illustrate the importance of safety trials in ensuring that AI does\nnot inadvertently slow down engineers, and a successful review comment to AI\npatch product running at scale.", "AI": {"tldr": "MetaMateCR, Meta's AI tool for fixing code review comments, outperforms GPT-4o in patch accuracy and actionable fixes. Safety trials revealed that exposing suggestions only to code authors avoids workflow slowdowns, demonstrating the value of careful rollout for scalable AI adoption in code review.", "motivation": "Meta receives tens of thousands of code review comments weekly, making manual review and fixes time-consuming. The motivation is to automate and expedite the code review process with AI-generated fixes while maintaining review efficiency and quality.", "method": "The authors developed 'MetaMate for Code Review' (MetaMateCR), using an internal dataset of 64,000 <review comment, patch> pairs to fine-tune Llama models. They benchmarked model performance against GPT-4o and deployed their models in production, running randomized controlled safety trials and full production experiments to monitor impact.", "result": "The LargeLSFT Llama model produced exact match patches 68% of the time, outperforming GPT-4o by 9 percentage points, and generated more modern code. However, initial deployment slowed reviews by 5% until UX adjustments were made. In production, the AI patch suggestion system raised actionable patch application rates by 9.2 percentage points over GPT-4o.", "conclusion": "AI-assisted code review using internally fine-tuned models can outperform leading general models like GPT-4o in both accuracy and adoption, but thoughtful deployment (especially via safety trials and UX tweaks) is necessary to prevent workflow slowdowns."}}
{"id": "2507.13553", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.13553", "abs": "https://arxiv.org/abs/2507.13553", "authors": ["Pragyan K C", "Rambod Ghandiparsi", "Thomas Herron", "John Heaps", "Mitra Bokaei Hosseini"], "title": "Towards Better Requirements from the Crowd: Developer Engagement with Feature Requests in Open Source Software", "comment": "Accepted at the 9th International Workshop on Crowd-Based\n  Requirements Engineering (CrowdRE'25)", "summary": "As user demands evolve, effectively incorporating feature requests is crucial\nfor maintaining software relevance and user satisfaction. Feature requests,\ntypically expressed in natural language, often suffer from ambiguity or\nincomplete information due to communication gaps or the requester's limited\ntechnical expertise. These issues can lead to misinterpretation, faulty\nimplementation, and reduced software quality. While seeking clarification from\nrequesters is a common strategy to mitigate these risks, little is known about\nhow developers engage in this clarification process in practice-how they\nformulate clarifying questions, seek technical or contextual details, align on\ngoals and use cases, or decide to close requests without attempting\nclarification. This study investigates how feature requests are prone to NL\ndefects (i.e. ambiguous or incomplete) and the conversational dynamics of\nclarification in open-source software (OSS) development, aiming to understand\nhow developers handle ambiguous or incomplete feature requests. Our findings\nsuggest that feature requests published on the OSS platforms do possess\nambiguity and incompleteness, and in some cases, both. We also find that\nexplicit clarification for the resolution of these defects is uncommon;\ndevelopers usually focus on aligning with project goals rather than resolving\nunclear text. When clarification occurs, it emphasizes understanding user\nintent/goal and feasibility, rather than technical details. By characterizing\nthe dynamics of clarification in open-source issue trackers, this work\nidentifies patterns that can improve user-developer collaboration and inform\nbest practices for handling feature requests effectively.", "AI": {"tldr": "Feature requests in OSS are often ambiguous or incomplete, but developers rarely clarify them directly, focusing more on aligning with project goals than on resolving unclear requests. This work highlights patterns in user-developer interactions that could improve handling of feature requests.", "motivation": "In software development, user feature requests are often expressed in natural language, leading to ambiguity or incomplete information because of unclear communication or a lack of technical knowledge from the requester. This can cause misinterpretation and negatively impact software quality. The authors are motivated to better understand how developers handle the clarification of such requests, with the goal of improving software relevance and user satisfaction.", "method": "The study investigates open-source software (OSS) projects to characterize how developers engage in the clarification process for ambiguous or incomplete feature requests. It examines how often ambiguities or incompleteness appear, the types of clarifications developers ask (when they do), and overall conversational dynamics in OSS issue trackers. The approach involves qualitative analysis of feature requests and developer responses to them.", "result": "The findings reveal that feature requests in OSS frequently suffer from ambiguity and/or incompleteness. However, direct clarification to resolve such defects is relatively rare; developers tend to prioritize alignment with project goals over clarifying unclear requests. When clarification happens, it mostly targets user intent and feasibility, not technical details.", "conclusion": "The study concludes that ambiguity and incompleteness are common in OSS feature requests, but explicit clarification is uncommon. Developers focus more on goal alignment than resolving unclear text. Understanding these patterns can help inform best practices to improve communication and collaboration between users and developers during the feature request process."}}
{"id": "2507.13555", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.13555", "abs": "https://arxiv.org/abs/2507.13555", "authors": ["Pragyan K C", "Rambod Ghandiparsi", "Thomas Herron", "John Heaps", "Mitra Bokaei Hosseini"], "title": "Demystifying Feature Requests: Leveraging LLMs to Refine Feature Requests in Open-Source Software", "comment": "Accepted at the 33rd IEEE International Requirements Engineering 2025", "summary": "The growing popularity and widespread use of software applications (apps)\nacross various domains have driven rapid industry growth. Along with this\ngrowth, fast-paced market changes have led to constantly evolving software\nrequirements. Such requirements are often grounded in feature requests and\nenhancement suggestions, typically provided by users in natural language (NL).\nHowever, these requests often suffer from defects such as ambiguity and\nincompleteness, making them challenging to interpret. Traditional validation\nmethods (e.g., interviews and workshops) help clarify such defects but are\nimpractical in decentralized environments like open-source software (OSS),\nwhere change requests originate from diverse users on platforms like GitHub.\nThis paper proposes a novel approach leveraging Large Language Models (LLMs) to\ndetect and refine NL defects in feature requests. Our approach automates the\nidentification of ambiguous and incomplete requests and generates clarification\nquestions (CQs) to enhance their usefulness for developers. To evaluate its\neffectiveness, we apply our method to real-world OSS feature requests and\ncompare its performance against human annotations. In addition, we conduct\ninterviews with GitHub developers to gain deeper insights into their\nperceptions of NL defects, the strategies they use to address these defects,\nand the impact of defects on downstream software engineering (SE) tasks.", "AI": {"tldr": "The paper presents an LLM-based method to detect and clarify defects in natural language feature requests, proving effective compared to humans and valued by developers for improving software engineering processes in open-source environments.", "motivation": "With the rapid evolution of software markets and the prevalence of apps, feature requests and enhancements\u2014mostly written in natural language\u2014are increasingly common. However, these texts often contain defects (ambiguity, incompleteness) that hinder software development, especially in decentralized settings like open-source projects, where traditional clarification techniques are impractical.", "method": "The paper proposes an automated, novel approach using Large Language Models (LLMs) to detect and refine defects in natural language feature requests. The system identifies ambiguous or incomplete requests and generates clarification questions to improve them. The method was evaluated using real OSS feature requests, human annotation comparisons, and interviews with GitHub developers about defect handling and its impact.", "result": "The approach effectively identified and refined ambiguous and incomplete feature requests. It generated helpful clarification questions, and performed comparably to human annotators on real OSS data. Interviews revealed developers' perspectives on managing NL defects and the significant effects these defects have on downstream software engineering.", "conclusion": "LLMs can automate and improve the quality of natural language feature requests in OSS projects by identifying and resolving defects. This aids developers by making requests clearer and reduces the negative impact of ambiguous requirements, especially in decentralized, large-scale environments."}}
{"id": "2507.13661", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.13661", "abs": "https://arxiv.org/abs/2507.13661", "authors": ["Changwen Li", "Joseph Sifakis", "Rongjie Yan", "Jian Zhang"], "title": "Testing Autonomous Driving Systems -- What Really Matters and What Doesn't", "comment": null, "summary": "Despite extensive research, the testing of autonomous driving systems (ADS)\nlandscape remains fragmented, and there is currently no basis for an informed\ntechnical assessment of the importance and contribution of the current state of\nthe art. This paper attempts to address this problem by exploring two\ncomplementary aspects.\n  First, it proposes a framework for comparing existing test methods in terms\nof their intrinsic effectiveness and validity. It shows that many methods do\nnot meet both of these requirements. Either because they are based on criteria\nthat do not allow for rapid, inexpensive, and comprehensive detection of\nfailures, or because the degree of validity of the properties tested cannot be\naccurately estimated. In particular, it is shown that most critical test\nmethods do not take into account the nominal operational capabilities of\nautopilots and generate scenarios that are impossible for the tested vehicles\nto handle, resulting in unjustified rejections.\n  Secondly, the paper shows that test effectiveness and validity are highly\ndependent on how autopilots are designed: how they choose between different\ncontrol policies to perform maneuvers, as well as on the reproducibility of the\nresults. In fact, most test methods take for granted two principles underlying\ntraditional methods, but do not generally apply to ADS. We maintain that the\nabsence of rationality and determinacy significantly impairs the effectiveness\nand validity of test methods, and provide test results on eight open\nautopilots, in which most do not satisfy these properties, thereby illustrating\nthis fact.\n  We conclude that under the current state of the art, it is impossible to\nobtain strong enough guarantees for essential autopilot properties and\nrecommend that autopilots be developed with a view to both rationality and\ndeterminacy.", "AI": {"tldr": "The paper critically reviews test methods for autonomous driving systems, finding most are ineffective or invalid due to unrealistic scenarios and assumptions about autopilot behavior. It calls for new testing frameworks and urges development of autopilots that ensure rational and deterministic operation, as current practices do not provide reliable safety guarantees.", "motivation": "Testing of autonomous driving systems (ADS) is fragmented, with no standard way to assess or compare testing methods. There is a lack of informative technical frameworks to evaluate the effectiveness and validity of current testing approaches for ADS. This undermines the reliability and safety assessment of autopilot systems.", "method": "The paper proposes a framework for comparing test methods based on their intrinsic effectiveness (how well they detect failures) and validity (how accurately the tests represent real operational capabilities). It analyzes existing methods, identifies their limitations, and investigates how test outcomes depend on the design choices of autopilot systems (i.e., their rationality and determinacy). Empirical results are provided from tests on eight open-source autopilots.", "result": "Many current test methods fall short by either failing to detect failures efficiently or by testing scenarios that are not actually representative or valid for the vehicles\u2019 capabilities. Most autopilots tested do not uphold the principles of rationality (consistent, reasoned behavior) and determinacy (reproducibility), leading to diminished test validity and effectiveness.", "conclusion": "Current testing methods for autonomous driving systems cannot guarantee essential autopilot properties. For stronger safety and performance assurances, both the design of autopilots and their test methods must be improved to ensure rationality and determinacy."}}
