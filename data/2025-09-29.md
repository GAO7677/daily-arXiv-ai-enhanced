<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.PL](#cs.PL) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Extracting Conceptual Knowledge to Locate Software Issues](https://arxiv.org/abs/2509.21427)
*Ying Wang,Wenjun Mao,Chong Wang,Zhenhao Zhou,Yicheng Zhou,Wenyun Zhao,Yiling Lou,Xin Peng*

Main category: cs.SE

TL;DR: RepoLens improves LLM-based issue localization in large code repositories by abstracting conceptual knowledge into semantically meaningful clusters, boosting localization performance by up to 504% on rigorous benchmarks and models.


<details>
  <summary>Details</summary>
Motivation: Recent LLM-based and LLM-agent-based issue localization approaches, while accurate, face challenges in large code repositories due to concern mixing (relevant logic buried in large functions) and concern scattering (related logic spread across files). Addressing these challenges is necessary for scalable and effective bug localization.

Method: RepoLens is proposed as a novel approach that creates and leverages a repository-wide conceptual knowledge base. RepoLens operates in two stages: (1) offline extraction and enrichment of conceptual knowledge into a knowledge base, and (2) online retrieval of issue-specific terms, clustering and ranking of concerns, and integration into localization workflows through prompt enhancements.

Result: RepoLens was evaluated on the SWE-Lancer-Loc benchmark (216 tasks) and consistently improved the performance of three state-of-the-art localization tools. It achieved average gains of over 22% in Hit@k and 46% in Recall@k for file- and function-level localization, with maximum observed gains up to 504% (Hit@1) and 376% (Recall@10) across three different language models.

Conclusion: RepoLens addresses the challenges of concern mixing and scattering by abstracting and organizing conceptual knowledge, significantly enhancing issue localization accuracy and reliability in large code repositories. It demonstrates both strong empirical improvements and generalizability across different models.

Abstract: Issue localization, which identifies faulty code elements such as files or
functions, is critical for effective bug fixing. While recent LLM-based and
LLM-agent-based approaches improve accuracy, they struggle in large-scale
repositories due to concern mixing, where relevant logic is buried in large
functions, and concern scattering, where related logic is dispersed across
files.
  To address these challenges, we propose RepoLens, a novel approach that
abstracts and leverages conceptual knowledge from code repositories. RepoLens
decomposes fine-grained functionalities and recomposes them into high-level
concerns, semantically coherent clusters of functionalities that guide LLMs. It
operates in two stages: an offline stage that extracts and enriches conceptual
knowledge into a repository-wide knowledge base, and an online stage that
retrieves issue-specific terms, clusters and ranks concerns by relevance, and
integrates them into localization workflows via minimally intrusive prompt
enhancements. We evaluate RepoLens on SWE-Lancer-Loc, a benchmark of 216 tasks
derived from SWE-Lancer. RepoLens consistently improves three state-of-the-art
tools, namely AgentLess, OpenHands, and mini-SWE-agent, achieving average gains
of over 22% in Hit@k and 46% in Recall@k for file- and function-level
localization. It generalizes across models (GPT-4o, GPT-4o-mini, GPT-4.1) with
Hit@1 and Recall@10 gains up to 504% and 376%, respectively. Ablation studies
and manual evaluation confirm the effectiveness and reliability of the
constructed concerns.

</details>


### [2] [Lost in Transition: The Struggle of Women Returning to Software Engineering Research after Career Breaks](https://arxiv.org/abs/2509.21533)
*Shalini Chakraborty,Sebastian Baltes*

Main category: cs.SE

TL;DR: This paper proposes a multi-country study on the challenges faced by women with software engineering backgrounds returning to academia after career breaks. It seeks to understand the existing barriers, compare returnship policies in different countries, and provide recommendations to improve hiring transparency and support for women in academic research roles.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of supportive pathways in academia for women with software engineering backgrounds who are returning after a career break, compared to more established support systems in the IT industry. The authors are motivated by observed gender inequities and the limited recognition of returnship-friendly policies in academic settings.

Method: The proposed study involves a diverse, multicultural research project conducted across multiple universities and countries. It will investigate challenges faced by women re-entering academia or research roles after a career break, and undertake a comparative analysis of institutional policies and opportunities for women returning to research roles in different countries.

Result: The expected outcome is a deeper understanding of the specific challenges women face re-entering academic roles relative to industry, insights into institutional perspectives, and a comparative overview of related policies. The study also aims to produce concrete recommendations for academic institutions to support more transparent and accessible hiring practices for returning women.

Conclusion: There is a clear gap in academic support structures for women with software engineering backgrounds returning after career breaks, and this research will offer valuable recommendations and evidence to inform policy improvements and transparent practices.

Abstract: The IT industry provides supportive pathways such as returnship programs,
coding boot camps, and buddy systems for women re-entering their job after a
career break. Academia, however, offers limited opportunities to motivate women
to return. We propose a diverse multicultural research project investigating
the challenges faced by women with software engineering (SE) backgrounds
re-entering academia or related research roles after a career break. Career
disruptions due to pregnancy, immigration status, or lack of flexible work
options can significantly impact women's career progress, creating barriers for
returning as lecturers, professors, or senior researchers. Although many
companies promote gender diversity policies, such measures are less prominent
and often under-recognized within academic institutions. Our goal is to explore
the specific challenges women encounter when re-entering academic roles
compared to industry roles; to understand the institutional perspective,
including a comparative analysis of existing policies and opportunities in
different countries for women to return to the field; and finally, to provide
recommendations that support transparent hiring practices. The research project
will be carried out in multiple universities and in multiple countries to
capture the diverse challenges and policies that vary by location.

</details>


### [3] [No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials](https://arxiv.org/abs/2509.21816)
*Yuhang Xie,Jian Mu,Xiaojun Ma,Chaoyun Zhang,Lu Wang,Mengyu Zhou,Mugeng Liu,Si Qin,Qingwei Lin,Saravan Rajmohan,Shi Han,Dongmei Zhang*

Main category: cs.SE

TL;DR: This paper presents the first fully automated system for generating Excel tutorials from user-described tasks. It uses an agent to execute and document tasks, producing readable documents and videos. The system improves success rates and quality over previous methods, drastically lowers time and labor costs, and is validated on over 1,500 real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Excel is extremely versatile but also complex, leading to a strong demand for tutorials. Existing tutorials require manual authoring, frequent updates, and significant labor, while previous automated methods still rely on handcrafted materials.

Method: The paper introduces an automated framework that generates Excel tutorials from natural language task descriptions. It uses an Execution Agent that translates tasks into actions within Excel, gathers artifacts, and outputs both structured documents and video demonstrations. Evaluation involves both LLMs and human reviewers.

Result: The framework generated tutorials for 1,559 real-world tasks, improving task execution success rates by 8.5% compared to baselines. The tutorials were found to have better readability and instructional quality, matching or exceeding expert-authored content. The automated process dramatically reduces labor and time costs (20x faster than manual authoring).

Conclusion: The proposed framework achieves fully automated, scalable, and high-quality tutorial generation for Excel, outperforming prior approaches and making wide-scale tutorial creation practical.

Abstract: Excel is one of the most widely used productivity tools across domains,
offering rich functionality but also overwhelming users with its complexity.
This creates a persistent demand for tutorials to support effective usage.
However, existing tutorials are manually authored by experts, require frequent
updates after each software release, and incur substantial labor costs. Prior
work has not achieved fully automated tutorial generation, since existing
methods still depend on handcrafted operation sequences or example materials.
In this paper, we present the first framework for automatically generating
Excel tutorials directly from natural language task descriptions. Our framework
first instantiates the task. Then a central component of this framework,
Execution Agent, plans and executes the solution in Excel, and collects the
intermediate artifacts required for tutorial construction. These artifacts are
then transformed into both structured Excel documents and video demonstrations.
To build a comprehensive tutorial corpus, we collected 1,559 task descriptions
from real-world scenarios. In addition, we designed a systematic evaluation
framework that integrates assessments from both large language models (LLMs)
and human reviewers. Experimental results show that our framework improves task
execution success rates by 8.5% over state-of-the-art baselines. Moreover, the
generated tutorials demonstrate superior readability and instructional
effectiveness, often approaching or surpassing expert-authored materials.
Importantly, the automated pipeline eliminates manual labor and reduces time
costs to 1/20 of expert authoring, making scalable and high-quality tutorial
generation practical for the first time.

</details>


### [4] [Software Engineering Data Analytics: A Framework Based on a Multi-Layered Abstraction Mechanism](https://arxiv.org/abs/2509.21881)
*Chaman Wijesiriwardana,Prasad Wimalaratne*

Main category: cs.SE

TL;DR: This paper proposes a domain-specific, multi-layered framework for integrating and analyzing heterogeneous software repositories, demonstrated via a case study, to improve software analytics.


<details>
  <summary>Details</summary>
Motivation: There is a need to efficiently analyze and integrate data from heterogeneous software repositories for improved software analytics.

Method: The authors propose a domain-specific framework with multi-layered abstraction and domain-specific operators for querying, modeling, and integrating software repositories. The effectiveness is demonstrated via a case study.

Result: The proposed framework enables advanced querying, modeling, and integration of various software repositories, validated through a case study.

Conclusion: A domain-specific multi-layered framework can facilitate better software analytics by enabling effective integration and analysis of heterogeneous repositories.

Abstract: This paper presents a concept of a domain-specific framework for software
analytics by enabling querying, modeling, and integration of heterogeneous
software repositories. The framework adheres to a multi-layered abstraction
mechanism that consists of domain-specific operators. We showcased the
potential of this approach by employing a case study.

</details>


### [5] [AgentPack: A Dataset of Code Changes, Co-Authored by Agents and Humans](https://arxiv.org/abs/2509.21891)
*Yangtian Zi,Zixuan Wu,Aleksander Boruch-Gruszecki,Jonathan Bell,Arjun Guha*

Main category: cs.SE

TL;DR: The paper presents AgentPack, a new dataset of code edits co-authored by human and AI agents. This more focused, high-quality data leads to better-performing code-editing models compared to those trained on older human-only commit datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for fine-tuning language models for code editing use data from commits and pull requests, but these are often noisy and unfocused, leading to suboptimal training datasets for code-editing models.

Method: The authors introduce AgentPack, a large curated corpus of 1.3 million code edits co-authored by both humans and AI agents (such as Claude Code, OpenAI Codex, and Cursor Agent) from public GitHub repositories. They describe their process for identifying and curating this data, analyze adoption trends of such agents, and structurally evaluate the properties of the edits.

Result: Models fine-tuned using the AgentPack dataset outperform those trained on traditional, human-only commit datasets.

Conclusion: Using code changes co-authored by AI agents and humans, as well as LLM-generated commit messages, produces a higher-quality training resource for code-editing language models than traditional sources. This demonstrates the value of leveraging software engineering agents for future advancements in code-editing model training.

Abstract: Fine-tuning large language models for code editing has typically relied on
mining commits and pull requests. The working hypothesis has been that commit
messages describe human intent in natural language, and patches to code
describe the changes that implement that intent. However, much of the
previously collected data is noisy: commit messages are terse, human-written
commits commingle several unrelated edits, and many commits come from simple,
rule-based bots.
  The recent adoption of software engineering agents changes this landscape.
Code changes co-authored by humans and agents tend to be more narrowly scoped
and focused on clearer goals. Their commit messages, generated by LLMs,
articulate intent and rationale in much greater detail. Moreover, when these
changes land in public repositories, they are implicitly filtered by humans:
maintainers discard low-quality commits to their projects.
  We present AgentPack, a corpus of 1.3M code edits co-authored by Claude Code,
OpenAI Codex, and Cursor Agent across public GitHub projects up to mid-August
2025. We describe the identification and curation pipeline, quantify adoption
trends of these agents, and analyze the structural properties of the edits.
Finally, we show that models fine-tuned on AgentPack can outperform models
trained on prior human-only commit corpora, highlighting the potential of using
public data from software engineering agents to train future code-editing
models.

</details>


### [6] [Unveiling Many Faces of Surrogate Models for Configuration Tuning: A Fitness Landscape Analysis Perspective](https://arxiv.org/abs/2509.21945)
*Pengzhou Chen,Hongyuan Liang,Tao Chen*

Main category: cs.SE

TL;DR: Surrogate model accuracy isn’t enough for effective system performance tuning. This paper uses fitness landscape analysis to evaluate model usefulness, conducts a large-scale empirical study, and introduces Model4Tune—a tool that reliably helps choose the best models for configuration tuning without expensive profiling.


<details>
  <summary>Details</summary>
Motivation: Traditional system configuration tuning is slow and expensive due to reliance on direct system measurements. While surrogate models are used to accelerate tuning, prior research reveals that just improving surrogate model accuracy does not necessarily yield better performance, raising questions about their actual role in the process.

Method: The paper introduces a systematic study using fitness landscape analysis to evaluate surrogate models for configuration tuning. It develops and tests a theoretical framework that assesses model usefulness beyond accuracy. An extensive empirical study (27,000 cases) is conducted to validate the approach. The paper also proposes Model4Tune, an automated predictive tool to estimate the suitability of model-tuner pairs for new systems without expensive profiling.

Result: The empirical study shows that using Model4Tune allows practitioners to select effective model-tuner pairs significantly better than random guessing in 79%-82% of testing cases. The tool provides a practical method for evaluating and choosing surrogate models for configuration tuning.

Conclusion: Accuracy is not the only, or best, measure of a surrogate model’s usefulness for system configuration tuning. Fitness landscape analysis offers a better perspective, allowing practitioners to more effectively select models without costly profiling. The work opens up new research and practical avenues for surrogate-based performance tuning.

Abstract: To efficiently tune configuration for better system performance (e.g.,
latency), many tuners have leveraged a surrogate model to expedite the process
instead of solely relying on the profoundly expensive system measurement. As
such, it is naturally believed that we need more accurate models. However, the
fact of accuracy can lie-a somewhat surprising finding from prior work-has left
us many unanswered questions regarding what role the surrogate model plays in
configuration tuning. This paper provides the very first systematic exploration
and discussion, together with a resolution proposal, to disclose the many faces
of surrogate models for configuration tuning, through the novel perspective of
fitness landscape analysis. We present a theory as an alternative to accuracy
for assessing the model usefulness in tuning, based on which we conduct an
extensive empirical study involving up to 27,000 cases. Drawing on the above,
we propose Model4Tune, an automated predictive tool that estimates which
model-tuner pairs are the best for an unforeseen system without expensive tuner
profiling. Our results suggest that Moldel4Tune, as one of the first of its
kind, performs significantly better than random guessing in 79%-82% of the
cases. Our results not only shed light on the possible future research
directions but also offer a practical resolution that can assist practitioners
in evaluating the most useful model for configuration tuning.

</details>


### [7] [SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios](https://arxiv.org/abs/2509.22097)
*Junkai Chen,Huihui Huang,Yunbo Lyu,Junwen An,Jieke Shi,Chengran Yang,Ting Zhang,Haoye Tian,Yikun Li,Zhenhao Li,Xin Zhou,Xing Hu,David Lo*

Main category: cs.SE

TL;DR: SecureAgentBench reveals significant security shortcomings in LLM-powered code agents, with most failing to reliably generate secure code despite being functionally correct. This new benchmark highlights the pressing need for research into safe automation in software development.


<details>
  <summary>Details</summary>
Motivation: Automated code generation by LLM-powered agents is transforming software engineering, but there are rising concerns over the security vulnerabilities these agents may introduce. Existing benchmarks are inadequate because they lack real-world context and comprehensive evaluation of security and functionality.

Method: The authors introduce SecureAgentBench, a new benchmark of 105 coding tasks that replicate real software engineering scenarios, leveraging realistic multi-file tasks and actual vulnerability contexts. The evaluation involves functional testing, proof-of-concept exploits, and static analysis for new vulnerabilities. They assess multiple agents and LLMs.

Result: Current code agents perform poorly in secure code generation, with the best combination achieving only 15.2% secure solutions. Agents often produce functionally correct code that is still insecure, sometimes introducing entirely new vulnerabilities. Adding explicit security instructions shows limited improvement.

Conclusion: SecureAgentBench offers a rigorous way to evaluate LLM-powered code agents' ability to generate secure code. The disappointing results underscore the urgent need for more research and development of secure, reliable code agents. Explicit security prompts alone do not suffice to produce safe code.

Abstract: Large language model (LLM) powered code agents are rapidly transforming
software engineering by automating tasks such as testing, debugging, and
repairing, yet the security risks of their generated code have become a
critical concern. Existing benchmarks have offered valuable insights but remain
insufficient: they often overlook the genuine context in which vulnerabilities
were introduced or adopt narrow evaluation protocols that fail to capture
either functional correctness or newly introduced vulnerabilities. We therefore
introduce SecureAgentBench, a benchmark of 105 coding tasks designed to
rigorously evaluate code agents' capabilities in secure code generation. Each
task includes (i) realistic task settings that require multi-file edits in
large repositories, (ii) aligned contexts based on real-world open-source
vulnerabilities with precisely identified introduction points, and (iii)
comprehensive evaluation that combines functionality testing, vulnerability
checking through proof-of-concept exploits, and detection of newly introduced
vulnerabilities using static analysis. We evaluate three representative agents
(SWE-agent, OpenHands, and Aider) with three state-of-the-art LLMs (Claude 3.7
Sonnet, GPT-4.1, and DeepSeek-V3.1). Results show that (i) current agents
struggle to produce secure code, as even the best-performing one, SWE-agent
supported by DeepSeek-V3.1, achieves merely 15.2% correct-and-secure solutions,
(ii) some agents produce functionally correct code but still introduce
vulnerabilities, including new ones not previously recorded, and (iii) adding
explicit security instructions for agents does not significantly improve secure
coding, underscoring the need for further research. These findings establish
SecureAgentBench as a rigorous benchmark for secure code generation and a step
toward more reliable software development with LLMs.

</details>


### [8] [SK2Decompile: LLM-based Two-Phase Binary Decompilation from Skeleton to Skin](https://arxiv.org/abs/2509.22114)
*Hanzhuo Tan,Weihao Li,Xiaolong Tian,Siyi Wang,Jiaming Liu,Jing Li,Yuqun Zhang*

Main category: cs.SE

TL;DR: SK2Decompile introduces a two-stage system that first recovers program structure and then assigns meaningful variable names, outperforming previous methods in accuracy and readability on key benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based binary decompilers struggle to recover both the source-level structure and original identifiers of programs, resulting in output that is hard to understand and less usable.

Method: SK2Decompile introduces a two-phase decompilation process. First, a Structure Recovery model translates binary code into an Intermediate Representation (IR) that maintains the program's control flow and data structures but uses generic placeholders for identifiers. Reinforcement learning rewards the model for generating syntactically and semantically valid structures. Second, an Identifier Naming model assigns meaningful names to the generic identifiers, again trained with reinforcement learning to maximize semantic similarity with reference code.

Result: SK2Decompile significantly surpasses state-of-the-art baselines, with a 21.6% higher average re-executability rate than GPT-5-mini on the HumanEval dataset and a 29.4% improvement in R2I over Idioms on the GitHub2025 benchmark.

Conclusion: The novel two-phase approach improves both the correctness and readability of binary decompilation, offering superior performance compared to existing models on multiple benchmarks.

Abstract: Large Language Models (LLMs) have emerged as a promising approach for binary
decompilation. However, the existing LLM-based decompilers still are somewhat
limited in effectively presenting a program's source-level structure with its
original identifiers. To mitigate this, we introduce SK2Decompile, a novel
two-phase approach to decompile from the skeleton (semantic structure) to the
skin (identifier) of programs. Specifically, we first apply a Structure
Recovery model to translate a program's binary code to an Intermediate
Representation (IR) as deriving the program's "skeleton", i.e., preserving
control flow and data structures while obfuscating all identifiers with generic
placeholders. We also apply reinforcement learning to reward the model for
producing program structures that adhere to the syntactic and semantic rules
expected by compilers. Second, we apply an Identifier Naming model to produce
meaningful identifiers which reflect actual program semantics as deriving the
program's "skin". We train the Identifier Naming model with a separate
reinforcement learning objective that rewards the semantic similarity between
its predictions and the reference code. Such a two-phase decompilation process
facilitates advancing the correctness and readability of decompilation
independently. Our evaluations indicate that SK2Decompile, significantly
outperforms the SOTA baselines, achieving 21.6% average re-executability rate
gain over GPT-5-mini on the HumanEval dataset and 29.4% average R2I improvement
over Idioms on the GitHub2025 benchmark.

</details>


### [9] [Leveraging LLM Agents for Automated Video Game Testing](https://arxiv.org/abs/2509.22170)
*Chengjia Wang,Lanling Tang,Ming Yuan,Jiongchi Yu,Xiaofei Xie,Jiajun Bu*

Main category: cs.SE

TL;DR: TITAN is a new LLM-driven game testing framework for MMORPGs that outperforms existing methods in task completion and bug detection, and is already being used in industry QA pipelines.


<details>
  <summary>Details</summary>
Motivation: Testing MMORPGs is challenging due to their complex, frequently updated nature, and the limitations of traditional automated and LLM-based game testing approaches. There is a need for more effective techniques that offer deep reasoning, high state coverage, and efficient bug detection.

Method: The paper proposes TITAN, an LLM-driven agent framework for intelligent MMORPG testing. TITAN includes four components: (1) perception and abstraction of high-dimensional game states, (2) action prioritization, (3) long-horizon reasoning with action trace memory and self-correction, and (4) LLM-based oracles for bug detection and diagnostics. TITAN is implemented and tested on commercial MMORPGs across PC and mobile.

Result: TITAN achieves higher task completion rates (95%), superior bug detection, and finds bugs missed by previous methods. Ablation studies show each component's importance. TITAN has also been deployed in real-world QA pipelines, highlighting its effectiveness.

Conclusion: TITAN advances MMORPG testing with intelligent, LLM-supported techniques, improving efficiency and depth over prior approaches, and has practical industry impact.

Abstract: Testing MMORPGs (Massively Multiplayer Online Role-Playing Games) is a
critical yet labor-intensive task in game development due to their complexity
and frequent updating nature. Traditional automated game testing approaches
struggle to achieve high state coverage and efficiency in these rich,
open-ended environments, while existing LLM-based game-playing approaches are
limited to shallow reasoning ability in understanding complex game state-action
spaces and long-complex tasks. To address these challenges, we propose TITAN,
an effective LLM-driven agent framework for intelligent MMORPG testing. TITAN
incorporates four key components to: (1) perceive and abstract high-dimensional
game states, (2) proactively optimize and prioritize available actions, (3)
enable long-horizon reasoning with action trace memory and reflective
self-correction, and (4) employ LLM-based oracles to detect potential
functional and logic bugs with diagnostic reports.
  We implement the prototype of TITAN and evaluate it on two large-scale
commercial MMORPGs spanning both PC and mobile platforms. In our experiments,
TITAN achieves significantly higher task completion rates (95%) and bug
detection performance compared to existing automated game testing approaches.
An ablation study further demonstrates that each core component of TITAN
contributes substantially to its overall performance. Notably, TITAN detects
four previously unknown bugs that prior testing approaches fail to identify. We
provide an in-depth discussion of these results, which offer guidance for new
avenues of advancing intelligent, general-purpose testing systems. Moreover,
TITAN has been deployed in eight real-world game QA pipelines, underscoring its
practical impact as an LLM-driven game testing framework.

</details>


### [10] [Library Hallucinations in LLMs: Risk Analysis Grounded in Developer Queries](https://arxiv.org/abs/2509.22202)
*Lukas Twist,Jie M. Zhang,Mark Harman,Helen Yannakoudakis*

Main category: cs.SE

TL;DR: This paper systematically shows that LLMs generating code often hallucinate fake libraries and members, especially when faced with natural user prompt errors. These hallucinations are a major risk and demand urgent safeguards.


<details>
  <summary>Details</summary>
Motivation: LLMs increasingly generate code but often hallucinate, particularly by inventing non-existent libraries. These errors can mislead developers, break builds, and introduce supply chain threats. Despite awareness of these risks, the impact of real-world prompt variations on hallucination rates is largely unknown.

Method: The authors systematically study how user-level prompt variations affect library hallucinations in code produced by six diverse LLMs. They analyze two types of hallucinations: invalid imports (library name hallucinations) and invalid calls from valid libraries (library member hallucinations). They use realistic prompt variations extracted from developer forums, including user errors like misspellings and fake names, to measure the effects on LLM hallucination rates.

Result: The study finds systemic vulnerabilities in LLMs: one-character misspellings cause hallucinations in up to 26% of tasks, fake library names are accepted in up to 99% of tasks, and time-related prompts induce hallucinations in up to 84% of cases. While prompt engineering can reduce hallucinations, its effectiveness is inconsistent and depends on the specific LLM.

Conclusion: LLMs are fragile in the face of natural prompt variations, frequently hallucinating libraries and members with significant risks for developers and systems. The work stresses the need for better safeguards against such hallucinations due to their potential for exploitation.

Abstract: Large language models (LLMs) are increasingly used to generate code, yet they
continue to hallucinate, often inventing non-existent libraries. Such library
hallucinations are not just benign errors: they can mislead developers, break
builds, and expose systems to supply chain threats such as slopsquatting.
Despite increasing awareness of these risks, little is known about how
real-world prompt variations affect hallucination rates. Therefore, we present
the first systematic study of how user-level prompt variations impact library
hallucinations in LLM-generated code. We evaluate six diverse LLMs across two
hallucination types: library name hallucinations (invalid imports) and library
member hallucinations (invalid calls from valid libraries). We investigate how
realistic user language extracted from developer forums and how user errors of
varying degrees (one- or multi-character misspellings and completely fake
names/members) affect LLM hallucination rates. Our findings reveal systemic
vulnerabilities: one-character misspellings in library names trigger
hallucinations in up to 26% of tasks, fake library names are accepted in up to
99% of tasks, and time-related prompts lead to hallucinations in up to 84% of
tasks. Prompt engineering shows promise for mitigating hallucinations, but
remains inconsistent and LLM-dependent. Our results underscore the fragility of
LLMs to natural prompt variation and highlight the urgent need for safeguards
against library-related hallucinations and their potential exploitation.

</details>


### [11] [Green Prompt Engineering: Investigating the Energy Impact of Prompt Design in Software Engineering](https://arxiv.org/abs/2509.22320)
*Vincenzo De Martino,Mohammad Amin Zadenoori,Xavier Franch,Alessio Ferrari*

Main category: cs.SE

TL;DR: Simpler, more readable prompts for language models can save energy and maintain good performance, making prompt design an important factor in sustainable AI practices.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore sustainability aspects in using language models for software engineering, specifically focusing on the understudied factor of linguistic complexity in prompts, beyond typical concerns like hardware or prompt length.

Method: The authors conducted an empirical study using open-source Small Language Models, where they varied the readability (linguistic complexity) of prompts for a requirement classification task, and measured both energy consumption and performance (F1-score).

Result: The study found that the readability of prompts significantly affects both the environmental sustainability (energy usage) and the performance (F1-score) of language models. Simple prompts reduce energy use without significantly decreasing F1-score, but there are trade-offs.

Conclusion: The paper concludes that linguistic complexity of prompts is a key design variable affecting language model sustainability and performance. Simple, readable prompts can be both energy-efficient and effective, encouraging the development of guidelines for sustainable prompt design in Green AI.

Abstract: Language Models are increasingly applied in software engineering, yet their
inference raises growing environmental concerns. Prior work has examined
hardware choices and prompt length, but little attention has been paid to
linguistic complexity as a sustainability factor. This paper introduces Green
Prompt Engineering, framing linguistic complexity as a design dimension that
can influence energy consumption and performance. We conduct an empirical study
on requirement classification using open-source Small Language Models, varying
the readability of prompts. Our results reveal that readability affects
environmental sustainability and performance, exposing trade-offs between them.
For practitioners, simpler prompts can reduce energy costs without a
significant F1-score loss; for researchers, it opens a path toward guidelines
and studies on sustainable prompt design within the Green AI agenda.

</details>


### [12] [GPU-Accelerated Loopy Belief Propagation for Program Analysis](https://arxiv.org/abs/2509.22337)
*Haoyu Feng,Xin Zhang*

Main category: cs.SE

TL;DR: This paper introduces a highly efficient, flexible GPU-accelerated LBP algorithm for program analysis, achieving major speedups and maintaining accuracy compared to current leading methods.


<details>
  <summary>Details</summary>
Motivation: Loopy Belief Propagation (LBP) is widely used in probabilistic graphical models for various applications, including program analysis. However, it's computationally intensive for large-scale tasks, and existing GPU-based methods do not effectively support flexible update strategies or integrate logical constraints, resulting in poor performance.

Method: The paper proposes a GPU-accelerated LBP algorithm specifically designed for program analysis. It introduces a unified representation for user-defined update strategies, a dependency analysis algorithm, and an optimized message grouping method that minimizes GPU warp divergence while leveraging the structure of Horn clauses.

Result: The experimental results, focusing on datarace analysis in eight real-world Java programs, demonstrate significant performance improvements: an average speedup of 2.14x over state-of-the-art sequential methods and 5.56x over existing GPU-based methods, with high accuracy maintained.

Conclusion: The proposed GPU-accelerated LBP algorithm substantially improves computational efficiency for program analysis tasks while supporting flexible update strategies and logical constraint integration, outperforming both sequential and previous GPU-based methods.

Abstract: Loopy Belief Propagation (LBP) is a widely used approximate inference
algorithm in probabilistic graphical models, with applications in computer
vision, error correction codes, protein folding, program analysis, etc.
However, LBP faces significant computational challenges when applied to
large-scale program analysis. While GPU (Graphics Processing Unit) parallel
computing provides a promising solution, existing approaches lack support for
flexible update strategies and have yet to integrate logical constraints with
GPU acceleration, leading to suboptimal practical performance.
  This paper presents a GPU-accelerated LBP algorithm for program analysis. To
support the diverse update strategies required by users, we propose a unified
representation for specifying arbitrary user-defined update strategies, along
with a dependency analysis algorithm. Furthermore, building on previous work
that leverages the local structure of Horn clauses to simplify message passing,
we group messages to minimize warp divergence and better utilize GPU resources.
Experimental results on datarace analysis over eight real-world Java programs
show that our approach achieves an average speedup of $2.14\times$ over the
state-of-the-art sequential approach and $5.56\times$ over the state-of-the-art
GPU-based approach, while maintaining high accuracy.

</details>


### [13] [A Multi-Modality Evaluation of the Reality Gap in Autonomous Driving Systems](https://arxiv.org/abs/2509.22379)
*Stefano Carlo Lambertenghi,Mirena Flores Valdez,Andrea Stocco*

Main category: cs.SE

TL;DR: Simulation-based testing for autonomous vehicles is challenged by the 'reality gap.' This study empirically compares SiL, ViL, MR, and real-world testing, finding MR best balances realism and safety. The analysis clarifies each approach's limitations and guides future robust testing of autonomous driving systems.


<details>
  <summary>Details</summary>
Motivation: Simulation-based testing is essential for the development of autonomous driving systems due to its safety and scalability. However, there is a significant challenge called the 'reality gap'—the discrepancies between simulation and real-world performance—which hinders the transferability of test results to real deployments.

Method: The authors conducted an empirical study comparing four testing modalities: Software-in-the-Loop (SiL), Vehicle-in-the-Loop (ViL), Mixed-Reality (MR), and full real-world testing. They used a small-scale physical vehicle equipped with real sensors and its digital twin. Two ADS architectures (modular and end-to-end) were evaluated across diverse indoor driving scenarios. They systematically assessed the impact of each modality on actuation, perception, and behavioral fidelity—the three dimensions of the reality gap.

Result: The results reveal that SiL and ViL oversimplify crucial real-world dynamics and sensing. Mixed-Reality testing enhances perceptual realism while maintaining safety and control. The authors pinpoint specific conditions where failures do not translate between modalities and link these inconsistencies to certain dimensions of the reality gap.

Conclusion: Each testing modality has clear strengths and limitations: SiL and ViL are less representative of real-world challenges, while MR bridges the gap between simulation and reality effectively. The study provides practical insights for choosing appropriate testing approaches, and maps a pathway toward robust and transferable validation of autonomous driving systems.

Abstract: Simulation-based testing is a cornerstone of Autonomous Driving System (ADS)
development, offering safe and scalable evaluation across diverse driving
scenarios. However, discrepancies between simulated and real-world behavior,
known as the reality gap, challenge the transferability of test results to
deployed systems. In this paper, we present a comprehensive empirical study
comparing four representative testing modalities: Software-in-the-Loop (SiL),
Vehicle-in-the-Loop (ViL), Mixed-Reality (MR), and full real-world testing.
Using a small-scale physical vehicle equipped with real sensors (camera and
LiDAR) and its digital twin, we implement each setup and evaluate two ADS
architectures (modular and end-to-end) across diverse indoor driving scenarios
involving real obstacles, road topologies, and indoor environments. We
systematically assess the impact of each testing modality along three
dimensions of the reality gap: actuation, perception, and behavioral fidelity.
Our results show that while SiL and ViL setups simplify critical aspects of
real-world dynamics and sensing, MR testing improves perceptual realism without
compromising safety or control. Importantly, we identify the conditions under
which failures do not transfer across testing modalities and isolate the
underlying dimensions of the gap responsible for these discrepancies. Our
findings offer actionable insights into the respective strengths and
limitations of each modality and outline a path toward more robust and
transferable validation of autonomous driving systems.

</details>


### [14] [Context-Specific Instruction: A Longitudinal Study on Debugging Skill Acquisition and Retention for Novice Programmers](https://arxiv.org/abs/2509.22420)
*Ziyi Zhang,Devjeet Roy,Venera Arnaoudova*

Main category: cs.SE

TL;DR: Context-specific bug localization instruction significantly outperforms abstract/generic methods for novices, leading to higher accuracy, faster completion, less stress, and better strategy retention. Even 1-2 sessions have strong effects; integrating examples with principles bridges theory and practice.


<details>
  <summary>Details</summary>
Motivation: Novices struggle with bug localization due to lack of systematic approaches. Existing instructional methods involve abstract guidelines or generic steps, but it is unclear if context-specific instruction provides additional benefits.

Method: An eight-week longitudinal study with four groups: no instruction (G1), abstract guidelines (G2), concrete steps (G3), and context-specific instruction (G4). 44 undergraduates participated; 41 completed all five sessions. Each session had 2-3 debugging tasks. Measures included correctness, time to completion, and self-reported stress, difficulty, satisfaction, and strategy adherence.

Result: Participants who received context-specific instruction (G4) achieved 80% correctness after one session (versus 20-44% for other groups) and maintained this over three weeks. Their completion time stabilized quicker at 13-15 minutes, compared to 22-27 minutes for others. G4 participants also reported lower stress, higher satisfaction, and better strategy internalization.

Conclusion: Context-specific instruction yields faster skill acquisition and stronger retention in bug localization than abstract or context-agnostic approaches. Even brief exposure brings significant gains, and practice stabilizes performance. Combining contextual examples and abstract principles may better equip novices and make education more equitable.

Abstract: Bug localization is a critical skill, yet novices often lack systematic
approaches. Prior work tested abstract guidelines and general concrete steps;
the impact of context-specific instruction is unclear. We ran an eight-week
longitudinal study with four conditions: no instruction (G1), abstract
guidelines (G2), concrete steps (G3), and our context-specific instruction that
pairs concrete bug-localization steps with problem-specific details (G4).
Forty-four undergraduates participated; 41 completed all five sessions (S1-S5).
Each session included 2-3 debugging tasks to identify the minimal code element
containing a seeded logical fault. We measured correctness (binary), time to
completion, self-perceived scores (stress, difficulty, satisfaction, and
strategy adherence). G4 achieved higher correctness and shorter time to
completion: it reached 80% correctness after one session (vs. 20-44% for other
groups) and maintained 80% after three weeks, outperforming all groups (p <
0.05); its time to completion stabilized at 13-15 minutes in S1, whereas other
groups took 2-3 sessions to stabilize at 22-27 minutes. Qualitative responses
showed lower stress and higher satisfaction in G4, with participants
internalizing strategies via contextual examples. We conclude that
context-specific instruction yields faster skill acquisition and stronger
retention than abstract guidelines or context-agnostic steps. Even 1-2 sessions
produced significant gains, while extended practice optimized and stabilized
performance. Integrating contextual examples with abstract principles may
bridge theory-practice gaps in bug-localization education and provide a more
equitable path for novices.

</details>


### [15] [TreeMind: Automatically Reproducing Android Bug Reports via LLM-empowered Monte Carlo Tree Search](https://arxiv.org/abs/2509.22431)
*Zhengyu Chen,Zhaoyi Meng,Wenxiang Zhao,Wansen Wang,Haoyang Zhao,Jiahao Zhan,Jie Cui,Hong Zhong*

Main category: cs.SE

TL;DR: TreeMind is a new technique that combines LLMs and Monte Carlo Tree Search for reproducing Android app crashes from incomplete bug reports, significantly outperforming existing methods in both controlled experiments and a real-world case study.


<details>
  <summary>Details</summary>
Motivation: Automatically reproducing Android app crashes from textual bug reports is difficult due to incomplete reports and complex modern user interfaces. Existing methods, such as reinforcement learning and LLMs, struggle to reconstruct the necessary user actions for bug reproduction due to limited reasoning and planning abilities.

Method: TreeMind combines Large Language Models (LLMs) with a custom Monte Carlo Tree Search (MCTS) algorithm. It treats bug reproduction as a target-driven search. Two LLM-guided agents are created: Expander (to generate promising next actions) and Simulator (to estimate action success probability). TreeMind uses multi-modal UI inputs and feedback-aware navigation for strategic UI exploration.

Result: TreeMind was evaluated on 93 real-world Android bug reports, outperforming four state-of-the-art baseline techniques in terms of reproduction success rate. The method was further validated with a real-world case study highlighting its effectiveness.

Conclusion: Integrating LLMs for semantic reasoning with MCTS-based external decision-making enables more effective and strategic UI exploration for automated Android bug reproduction, especially where reports are incomplete and the interaction space is complex.

Abstract: Automatically reproducing Android app crashes from textual bug reports is
challenging, particularly when the reports are incomplete and the modern UI
exhibits high combinatorial complexity. Existing approaches based on
reinforcement learning or large language models (LLMs) exhibit limitations in
such scenarios. They struggle to infer unobserved steps and reconstruct the
underlying user action sequences to navigate the vast UI interaction space,
primarily due to limited goal-directed reasoning and planning. We present
TreeMind, a novel technique that integrates LLMs with a customized Monte Carlo
Tree Search (MCTS) algorithm to achieve strategic UI exploration in bug
reproduction. To the best of our knowledge, this is the first work to combine
external decision-making with LLM semantic reasoning for reliable bug
reproduction. We formulate the reproduction task as a target-driven search
problem, leveraging MCTS as the core planning mechanism to iteratively refine
action sequences. To enhance MCTS with semantic reasoning, we introduce two
LLM-guided agents with distinct roles: Expander generates top-k promising
actions based on the current UI state and exploration history, while Simulator
estimates the likelihood that each action leads toward successful reproduction.
By incorporating multi-modal UI inputs and advanced prompting techniques,
TreeMind conducts feedback-aware navigation that identifies missing but
essential user actions and incrementally reconstructs the reproduction paths.
We evaluate TreeMind on a dataset of 93 real-world Android bug reports from
three widely-used benchmarks. Experimental results show that it significantly
outperforms four state-of-the-art baselines in reproduction success rate. A
real-world case study indicates that integrating LLM reasoning with MCTS-based
planning is a compelling direction for automated bug reproduction.

</details>


### [16] [Boosting Pointer Analysis With Large Language Model-Enhanced Allocation Function Detection](https://arxiv.org/abs/2509.22530)
*Baijun Cheng,Kailong Wang,Ling Shi,Haoyu Wang,Peng Di,Yao Guo,Ding Li,Xiangqun Chen*

Main category: cs.SE

TL;DR: AFD automatically identifies and models custom memory allocators in C/C++ code using analysis and LLMs, leading to much more precise and scalable pointer analysis and finding bugs missed by prior approaches, with acceptable overhead.


<details>
  <summary>Details</summary>
Motivation: Many static analysis tasks rely on pointer analysis, but its effectiveness is limited in C/C++ programs due to imprecise modeling of heap allocations from user-defined allocation functions, which are common and often ignored by previous tools.

Method: The authors introduce AFD, a hybrid technique combining value-flow analysis for simple cases and Large Language Models (LLMs) for more complex patterns to automatically identify and accurately model custom allocation functions.

Result: AFD was evaluated on 15 real-world C projects, successfully identifying over 600 custom allocation functions. Integrating AFD led to a 26x increase in modeled heap objects, a 39% reduction in alias set sizes, and improved memory bug detection with only moderate runtime overhead.

Conclusion: Precise, automated handling of custom allocation functions dramatically improves the precision and practicality of pointer analysis, revealing many more heap objects and memory bugs in large codebases.

Abstract: Pointer analysis is foundational for many static analysis tasks, yet its
effectiveness is often hindered by imprecise modeling of heap allocations,
particularly in C/C++ programs where user-defined allocation functions (AFs)
are pervasive. Existing approaches largely overlook these custom allocators,
leading to coarse aliasing and reduced analysis precision. In this paper, we
present AFD, a novel technique that enhances pointer analysis by automatically
identifying and modeling custom allocation functions. AFD employs a hybrid
approach: it uses value-flow analysis to detect straightforward wrappers and
leverages Large Language Models (LLMs) to reason about more complex allocation
patterns with side effects. This targeted enhancement enables precise modeling
of heap objects at each call site, achieving context-sensitivity-like benefits
without the associated overhead. We evaluate AFD on 15 real-world C projects,
identifying over 600 custom AFs. Integrating AFD into a baseline pointer
analysis yields a 26x increase in modeled heap objects and a 39% reduction in
alias set sizes, with only 1.4x runtime overhead. Furthermore, our enhanced
analysis improves indirect call resolution and uncovers 17 previously
undetected memory bugs. These results demonstrate that precise modeling of
custom allocation functions offers a scalable and practical path to improving
pointer analysis in large software systems.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [17] [InvBench: Can LLMs Accelerate Program Verification with Invariant Synthesis?](https://arxiv.org/abs/2509.21629)
*Anjiang Wei,Tarun Suresh,Tianran Sun,Haoze Wu,Ke Wang,Alex Aiken*

Main category: cs.PL

TL;DR: The paper presents a new framework to formally evaluate LLMs in discovering program loop invariants. Current LLM-based verifiers are promising but still lag behind traditional tools like UAutomizer. Performance depends on the LLM used, and techniques like fine-tuning and Best-of-N sampling can boost results. The benchmark poses a challenge for current models, suggesting room for further advancement.


<details>
  <summary>Details</summary>
Motivation: Automatically discovering strong loop invariants for program verification is a long-standing, unsolved challenge. With the rise of large language models (LLMs), there is a need to rigorously evaluate their capabilities in invariant synthesis to determine if they can advance the state-of-the-art.

Method: The paper introduces a principled evaluation framework that leverages a verifier-based decision procedure with formal soundness guarantees. It systematically assesses LLMs and LLM-based verification tools against traditional solvers (such as UAutomizer) on criteria of correctness and verification speedup.

Result: Evaluations of seven state-of-the-art LLMs and existing LLM-based verifiers reveal that, although promising, these approaches currently do not surpass UAutomizer in effectiveness. Model capability significantly affects performance, demonstrated by substantial differences in speedup achievements between models. Supervised fine-tuning and Best-of-N sampling both improve outcomes: fine-tuning notably increases the success rate for Qwen3-Coder-480B, and Best-of-N sampling yields higher speedup cases for Claude-sonnet-4.

Conclusion: While LLM-based approaches point to a promising future in invariant synthesis, they have not yet consistently outperformed traditional solvers. Performance disparities highlight the importance of model choice, and improvement can be achieved through methods like fine-tuning and sampling. Their benchmark establishes a challenge for the current generation of LLMs and a basis for future research.

Abstract: Program verification relies on loop invariants, yet automatically discovering
strong invariants remains a long-standing challenge. We introduce a principled
framework for evaluating LLMs on invariant synthesis. Our approach uses a
verifier-based decision procedure with a formal soundness guarantee and
assesses not only correctness but also the speedup that invariants provide in
verification. We evaluate 7 state-of-the-art LLMs, and existing LLM-based
verifiers against the traditional solver UAutomizer. While LLM-based verifiers
represent a promising direction, they do not yet offer a significant advantage
over UAutomizer. Model capability also proves critical, as shown by sharp
differences in speedups across models, and our benchmark remains an open
challenge for current LLMs. Finally, we show that supervised fine-tuning and
Best-of-N sampling can improve performance: fine-tuning on 3589 instances
raises the percentage of speedup cases for Qwen3-Coder-480B from 8% to 29.2%,
and Best-of-N sampling with N=16 improves Claude-sonnet-4 from 8.8% to 22.1%.

</details>


### [18] [Compiling by Proving: Language-Agnostic Automatic Optimization from Formal Semantics](https://arxiv.org/abs/2509.21793)
*Jianhong Zhao,Everett Hildenbrandt,Juan Conejero,Yongwang Zhao*

Main category: cs.PL

TL;DR: The paper introduces a method to turn program verification proofs into efficient execution rules using symbolic execution and the K framework, resulting in major performance improvements while preserving correctness.


<details>
  <summary>Details</summary>
Motivation: Existing verification proofs are discarded after checking program correctness, even though they encode the complete program behavior. Harnessing these proofs for further optimization is a missed opportunity.

Method: The authors propose 'compiling by proving,' a paradigm that transforms verification proofs into optimized execution rules. They utilize symbolic execution to construct All-Path Reachability Proofs and compile their graph structures to merge many semantic rewrites into single rules. This approach is implemented as a language-agnostic extension to the K framework.

Result: Performance evaluations show significant improvements: consistent speedups at the opcode-level and dramatic, orders-of-magnitude gains with whole-program compilation.

Conclusion: Compiling by proving enables the transformation of verification proofs into highly optimized execution rules, maintaining correctness and achieving substantial performance benefits across various compilation levels.

Abstract: Verification proofs encode complete program behavior, yet we discard them
after checking correctness. We present compiling by proving, a paradigm that
transforms these proofs into optimized execution rules. By constructing
All-Path Reachability Proofs through symbolic execution and compiling their
graph structure, we consolidate many semantic rewrites into single rules while
preserving correctness by construction. We implement this as a
language-agnostic extension to the K framework. Evaluation demonstrates
performance improvements across different compilation scopes: opcode-level
optimizations show consistent speedups, while whole-program compilation
achieves orders of magnitude greater performance gains.

</details>


### [19] [Committing to the bit: Relational programming with semiring arrays and SAT solving](https://arxiv.org/abs/2509.22614)
*Dmitri Volkov,Yafei Yang,Chung-chieh Shan*

Main category: cs.PL

TL;DR: semiringKanren is a new relational programming language using semiring arrays, with type-based SAT compilation for the Boolean semiring. Experiments show it's often faster than miniKanren for Sudoku, demonstrating improved efficiency for certain constraints.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance relational programming by introducing a language and framework capable of leveraging semiring structures, with the goal of improving efficiency, especially in constraint-solving scenarios.

Method: The paper proposes semiringKanren, a new relational programming language. It formalizes a type system that restricts relations to finite semiring arrays and defines a semantics parameterized by the underlying semiring. The approach compiles program types to bitstring representations, allowing the use of SAT solvers in the case of the Boolean semiring.

Result: Experiments conducted using Sudoku puzzles compare semiringKanren to miniKanren. The results show that semiringKanren can be more efficient than miniKanren in these tasks.

Conclusion: semiringKanren offers a general and efficient approach to relational programming, especially when utilizing SAT solvers for the Boolean case. It can outperform existing miniKanren implementations in specific problem domains.

Abstract: We propose semiringKanren, a relational programming language where each
relation expression denotes a semiring array. We formalize a type system that
restricts the arrays to finite size. We then define a semantics that is
parameterized by the semiring that the arrays draw their elements from. We
compile semiringKanren types to bitstring representations. For the Boolean
semiring, this compilation enables us to use an SAT solver to run
semiringKanren programs efficiently. We compare the performance of
semiringKanren and faster miniKanren for solving Sudoku puzzles. Our experiment
shows that semiringKanren can be a more efficient variant of miniKanren.

</details>
