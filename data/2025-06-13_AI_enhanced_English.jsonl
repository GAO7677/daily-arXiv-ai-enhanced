{"id": "2506.10021", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10021", "abs": "https://arxiv.org/abs/2506.10021", "authors": ["Jordi de la Torre"], "title": "From Tool Calling to Symbolic Thinking: LLMs in a Persistent Lisp Metaprogramming Loop", "comment": null, "summary": "We propose a novel architecture for integrating large language models (LLMs)\nwith a persistent, interactive Lisp environment. This setup enables LLMs to\ndefine, invoke, and evolve their own tools through programmatic interaction\nwith a live REPL. By embedding Lisp expressions within generation and\nintercepting them via a middleware layer, the system allows for stateful\nexternal memory, reflective programming, and dynamic tool creation. We present\na design framework and architectural principles to guide future implementations\nof interactive AI systems that integrate symbolic programming with neural\nlanguage generation.", "AI": {"tldr": "The paper proposes integrating LLMs with a persistent Lisp environment, allowing them to create, use, and evolve their own tools interactively. This approach enables stateful memory, reflective programming, and lays the groundwork for more interactive and autonomous AI systems.", "motivation": "Current AI systems, especially those based on large language models (LLMs), lack persistent statefulness and the ability to create or modify their own tools dynamically within interactive environments. The paper is motivated by the need to empower LLMs with capabilities for reflective programming, stateful memory, and autonomous tool evolution by effectively integrating symbolic and neural approaches.", "method": "The authors introduce an architecture that integrates LLMs with a persistent, interactive Lisp environment. In this setup, LLMs generate and execute Lisp expressions within a live REPL, using a middleware layer to intercept and manage these interactions. This allows the language model to define, invoke, and adapt its own tools programmatically while maintaining contextual awareness.", "result": "The system demonstrates that LLMs, when plugged into a symbolic programming environment like Lisp with appropriate middleware, can achieve interactive tool creation, reflective program modification, and persistent state management. The result is a flexible framework for building more interactive and autonomous AI systems.", "conclusion": "The paper concludes that combining LLMs with a live symbolic programming platform enables smarter, more flexible AI agents. The proposed architecture supports ongoing tool evolution and stateful interactions, setting principles for future development of interactive and adaptive AI systems."}}
{"id": "2506.10026", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2506.10026", "abs": "https://arxiv.org/abs/2506.10026", "authors": ["Tesla Zhang", "Sonya Simkin", "Rui Li", "Yue Yao", "Stephanie Balzer"], "title": "A Language-Agnostic Logical Relation for Message-Passing Protocols", "comment": "19 pages, 8 figures", "summary": "Today's computing landscape has been gradually shifting to applications\ntargeting distributed and *heterogeneous* systems, such as cloud computing and\nInternet of Things (IoT) applications. These applications are predominantly\n*concurrent*, employ *message-passing*, and interface with *foreign objects*,\nranging from externally implemented code to actual physical devices such as\nsensors. Verifying that the resulting systems adhere to the intended protocol\nof interaction is challenging -- the usual assumption of a common\nimplementation language, let alone a type system, no longer applies, ruling out\nany verification method based on them. This paper develops a framework for\ncertifying *protocol compliance* of heterogeneous message-passing systems. It\ncontributes the first mechanization of a *language-agnostic logical relation*,\nasserting that its inhabitants comply with the protocol specified. This\ndefinition relies entirely on a labelled transition-based semantics,\naccommodating arbitrary inhabitants, typed and untyped alike, including foreign\nobjects. As a case study, the paper considers two scenarios: (1) *per-instance\nverification* of a specific application or hardware device, and (2)\n*once-and-for-all verification* of well-typed applications for a given type\nsystem. The logical relation and both scenarios are mechanized in the Coq\ntheorem prover.", "AI": {"tldr": "The paper presents a Coq-based, language-agnostic framework for verifying that distributed, message-passing systems (even those mixing different languages, untyped code, and devices) comply with communication protocols, overcoming limitations of previous language- or type-dependent methods.", "motivation": "Modern applications increasingly target distributed and heterogeneous environments, often involving multiple concurrent components that communicate by message-passing and interact with foreign objects, such as external libraries or hardware devices. Verifying that these diverse systems adhere to their communication protocols is challenging because traditional assumptions, like using a single programming language or uniform type system, do not hold.", "method": "The paper introduces a framework for certifying protocol compliance in heterogeneous, message-passing systems. It establishes the first mechanization of a language-agnostic logical relation based on labelled transition system semantics, allowing arbitrary components (typed, untyped, or foreign) to be analyzed. The framework is implemented and verified in the Coq proof assistant.", "result": "The framework supports two verification scenarios: (1) per-instance verification of individual applications or devices, and (2) general, once-and-for-all verification for all well-typed applications under a given type system. Both scenarios and the logical relation itself are mechanized and demonstrated in Coq, confirming their feasibility and effectiveness.", "conclusion": "This work demonstrates that protocol compliance in heterogeneous message-passing systems can be rigorously verified without relying on shared implementation languages or type systems, using a novel logical relation and formal verification in Coq. The approach is broadly applicable to both specific instances and entire classes of well-typed systems."}}
{"id": "2506.10781", "categories": ["cs.PL", "68N15, 68U35", "D.3.0; K.3.2"], "pdf": "https://arxiv.org/pdf/2506.10781", "abs": "https://arxiv.org/abs/2506.10781", "authors": ["Zhiyao Zhong", "Cyrus Omar"], "title": "Hazel Deriver: A Live Editor for Constructing Rule-Based Derivations", "comment": "5 pages, 2 figures, includes a preliminary user study; intended for\n  computer science education and PL/HCI conference audiences", "summary": "Students in programming languages and formal logic courses often struggle\nwith constructing rule-based derivation trees due to the complexity of applying\ninference rules, the lack of immediate feedback, and the manual effort required\nfor handwritten proofs. We present Hazel Deriver, a live, web-based editor\ndesigned to scaffold derivation construction through multiple layers of\nsupport. Built on the Hazel live programming environment, it provides a\nstructured, interactive experience that encourages iterative exploration and\nreal-time feedback. A preliminary user study with former students suggests that\nHazel Deriver reduces the perceived difficulty of derivation tasks while\nimproving conceptual understanding and engagement. We discuss the design of its\nlayered scaffolding features and raise questions about balancing system\nguidance with learner autonomy.", "AI": {"tldr": "Hazel Deriver is a web-based tool that helps students build derivation trees with interactive, real-time support. Early studies show it makes the task easier and improves understanding, though the right balance of guidance is still under consideration.", "motivation": "Many students have difficulty constructing rule-based derivation trees in programming languages and logic courses. The challenges come from complex inference rules, lack of immediate feedback, and the tediousness of manual proof construction.", "method": "The authors introduce Hazel Deriver, a web-based live editor built on the Hazel environment, which offers multilayered scaffolding, structured and interactive experiences, and real-time feedback to walk students through derivation construction.", "result": "A preliminary user study with previous students indicates reduced perceived difficulty, greater conceptual understanding, and increased engagement when using Hazel Deriver.", "conclusion": "Hazel Deriver can make rule-based derivation construction easier and more engaging. The paper discusses how its design supports learning and brings up the issue of balancing system help with student independence."}}
{"id": "2506.10913", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2506.10913", "abs": "https://arxiv.org/abs/2506.10913", "authors": ["Ashley Samuelson", "Andrew K. Hirsch", "Ethan Cecchetti"], "title": "Choreographic Quick Changes: First-Class Location (Set) Polymorphism", "comment": "In submission to OOPSLA 2025", "summary": "Choreographic programming is a promising new paradigm for programming\nconcurrent systems where a developer writes a single centralized program that\ncompiles to individual programs for each node. Existing choreographic\nlanguages, however, lack critical features integral to modern systems, like the\nability of one node to dynamically compute who should perform a computation and\nsend that decision to others. This work addresses this gap with $\\lambda_{QC}$,\nthe first typed choreographic language with \\emph{first class process names}\nand polymorphism over both types and (sets of) locations. $\\lambda_{QC}$ also\nimproves expressive power over previous work by supporting algebraic and\nrecursive data types as well as multiply-located values. We formalize and\nmechanically verify our results in Rocq, including the standard choreographic\nguarantee of deadlock freedom.", "AI": {"tldr": "\u03bb_QC is a newly designed, more expressive choreographic programming language supporting dynamic process computation, polymorphism, and advanced data types, with correctness and deadlock-freedom formally verified.", "motivation": "Existing choreographic languages for concurrent systems lack the ability for nodes to dynamically decide who should perform computations and communicate those decisions, as well as missing support for key features like polymorphism over process names/locations and powerful data types.", "method": "The paper introduces \u03bb_QC, a new typed choreographic language with first-class process names, polymorphism over types and process locations, and support for complex data types. Results are formalized and mechanically verified in Rocq.", "result": "\u03bb_QC is more expressive than prior choreographic languages, enables dynamic process assignment, supports algebraic and recursive data types, and multiply-located values. Formal verification in Rocq demonstrates that the language maintains important properties such as deadlock freedom.", "conclusion": "\u03bb_QC closes an important gap in choreographic programming by providing, for the first time, a typed language with dynamic process naming, location polymorphism, and advanced data structuring, all while retaining critical correctness guarantees."}}
{"id": "2506.10043", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10043", "abs": "https://arxiv.org/abs/2506.10043", "authors": ["Yongqian Sun", "Yu Luo", "Xidao Wen", "Yuan Yuan", "Xiaohui Nie", "Shenglin Zhang", "Tong Liu", "Xi Luo"], "title": "TrioXpert: An automated incident management framework for microservice system", "comment": null, "summary": "Automated incident management plays a pivotal role in large-scale\nmicroservice systems. However, many existing methods rely solely on\nsingle-modal data (e.g., metrics, logs, and traces) and struggle to\nsimultaneously address multiple downstream tasks, including anomaly detection\n(AD), failure triage (FT), and root cause localization (RCL). Moreover, the\nlack of clear reasoning evidence in current techniques often leads to\ninsufficient interpretability. To address these limitations, we propose\nTrioXpert, an end-to-end incident management framework capable of fully\nleveraging multimodal data. TrioXpert designs three independent data processing\npipelines based on the inherent characteristics of different modalities,\ncomprehensively characterizing the operational status of microservice systems\nfrom both numerical and textual dimensions. It employs a collaborative\nreasoning mechanism using large language models (LLMs) to simultaneously handle\nmultiple tasks while providing clear reasoning evidence to ensure strong\ninterpretability. We conducted extensive evaluations on two popular\nmicroservice system datasets, and the experimental results demonstrate that\nTrioXpert achieves outstanding performance in AD (improving by 4.7% to 57.7%),\nFT (improving by 2.1% to 40.6%), and RCL (improving by 1.6% to 163.1%) tasks.", "AI": {"tldr": "TrioXpert is a new incident management framework for microservice systems that outperforms traditional methods by using multimodal data and large language models, improving accuracy and interpretability in multiple incident analysis tasks.", "motivation": "Existing methods for automated incident management in microservice systems are limited because they use only single-modal data (such as metrics, logs, or traces) and cannot effectively perform multiple related tasks at the same time. These methods also generally lack interpretability, providing little clear reasoning evidence for their decisions.", "method": "The paper proposes TrioXpert, an end-to-end framework that integrates multimodal data (from both numerical and textual sources) by designing three separate data processing pipelines tailored to each modality. It uses a collaborative reasoning method powered by large language models (LLMs) to improve simultaneous handling of anomaly detection, failure triage, and root cause localization tasks, while also providing interpretable reasoning evidence.", "result": "TrioXpert was extensively evaluated on two well-known microservice datasets. It showed significant performance improvements across all three tasks: Anomaly Detection (improved by 4.7% to 57.7%), Failure Triage (improved by 2.1% to 40.6%), and Root Cause Localization (improved by 1.6% to 163.1%).", "conclusion": "TrioXpert effectively leverages multimodal data and LLM-based reasoning to outperform existing single-modal approaches in incident management for microservice systems, excelling in anomaly detection, failure triage, and root cause localization, with enhanced interpretability."}}
{"id": "2506.10056", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2506.10056", "abs": "https://arxiv.org/abs/2506.10056", "authors": ["Gabriel Orlanski", "Nicholas Roberts", "Aws Albarghouthi", "Frederic Sala"], "title": "Reward Models Enable Scalable Code Verification by Trading Accuracy for Throughput", "comment": "29 pages, 6 figures, code released here:\n  https://github.com/SprocketLab/orm-code-verifier", "summary": "The standard paradigm for solving coding tasks via large language models\n(LLMs) is to generate-then-rank programs, where the latter step uses a verifier\nin the ranking process. The growing consensus is that a comprehensive verifier\n(e.g., a full test suite) should be prioritized over an outcome reward model\n(ORM) whenever possible, with little consideration given to the trade-offs\ninvolved. We aim to challenge this assumption by systematically exploring the\ntradeoff between speed and accuracy. We find that ORMs play a crucial role in\nscaling verification through trading accuracy for speed, even when a\ncomprehensive verifier is available. Their value becomes especially apparent\nwhen used in a generate-prune-then-rank approach, where a faster but less\naccurate verifier removes incorrect solutions prior to ranking -- leading to a\nsystem that is 11.65x faster while only being 8.33% less accurate than the full\ntest suite. We analyze the generate-prune-then-rank approach and show that it\nworks by filtering out incorrect but highly ranked solutions. These findings\nenable the design of scalable and accurate program ranking systems.", "AI": {"tldr": "Using outcome reward models (ORMs) to quickly prune incorrect solutions before full verification dramatically speeds up LLM code evaluation, with only a small loss in accuracy, challenging assumptions that always favor comprehensive test suites.", "motivation": "The paper aims to challenge the common assumption that a comprehensive verifier is always preferable to an outcome reward model (ORM) when solving coding tasks with large language models, and to explore the trade-offs between accuracy and speed in such verification systems.", "method": "The authors systematically analyze various program verification approaches for LLM-generated code, specifically comparing comprehensive verifiers (full test suites) to ORMs. They introduce and evaluate a generate-prune-then-rank pipeline, where an ORM is used as a fast, less accurate filter before applying a comprehensive verifier for ranking.", "result": "They find that using an ORM in a generate-prune-then-rank setup dramatically increases speed (11.65x faster) while only modestly reducing accuracy (8.33% less accurate) compared to using a full test suite alone. The approach is effective because the ORM-based pruning removes incorrect but highly ranked solutions before final ranking.", "conclusion": "Outcome reward models (ORMs) have significant value even when comprehensive verifiers are available, allowing for scalable and efficient program ranking by trading some accuracy for substantial speed improvements. This insight can inform the design of better LLM-based code generation and ranking systems."}}
{"id": "2506.10049", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10049", "abs": "https://arxiv.org/abs/2506.10049", "authors": ["Francesco Vinci", "Gyunam Park", "Wil van der Aalst", "Massimiliano de Leoni"], "title": "Online Discovery of Simulation Models for Evolving Business Processes (Extended Version)", "comment": null, "summary": "Business Process Simulation (BPS) refers to techniques designed to replicate\nthe dynamic behavior of a business process. Many approaches have been proposed\nto automatically discover simulation models from historical event logs,\nreducing the cost and time to manually design them. However, in dynamic\nbusiness environments, organizations continuously refine their processes to\nenhance efficiency, reduce costs, and improve customer satisfaction. Existing\ntechniques to process simulation discovery lack adaptability to real-time\noperational changes. In this paper, we propose a streaming process simulation\ndiscovery technique that integrates Incremental Process Discovery with Online\nMachine Learning methods. This technique prioritizes recent data while\npreserving historical information, ensuring adaptation to evolving process\ndynamics. Experiments conducted on four different event logs demonstrate the\nimportance in simulation of giving more weight to recent data while retaining\nhistorical knowledge. Our technique not only produces more stable simulations\nbut also exhibits robustness in handling concept drift, as highlighted in one\nof the use cases.", "AI": {"tldr": "The paper introduces an adaptive streaming-based process simulation model discovery technique that balances recent and historical data, showing superior stability and adaptability in dynamic business environments.", "motivation": "Traditional simulation model discovery methods for business processes are not adaptive to real-time changes in dynamic business environments. As organizations frequently update their processes, existing static discovery approaches become insufficient.", "method": "The paper presents a streaming process simulation discovery approach that combines Incremental Process Discovery with Online Machine Learning. The method prioritizes the use of recent data but also retains historical information to maintain adaptability and stability.", "result": "Experiments on four different event logs show that the proposed technique delivers more stable simulation outcomes and better adapts to recent changes, especially in the presence of concept drift. The importance of giving more weight to recent data while preserving historical context is demonstrated.", "conclusion": "The proposed streaming discovery technique effectively adapts business process simulation models to evolving organizational dynamics, offering improved stability and robustness to concept drift compared to traditional approaches."}}
{"id": "2506.10803", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2506.10803", "abs": "https://arxiv.org/abs/2506.10803", "authors": ["Ryan Gibb", "Patrick Ferris", "David Allsopp", "Michael Winston Dales", "Mark Elvers", "Thomas Gazagnaire", "Sadiq Jaffer", "Thomas Leonard", "Jon Ludlam", "Anil Madhavapeddy"], "title": "Solving Package Management via Hypergraph Dependency Resolution", "comment": "Submitted to SPLASH 2025", "summary": "Package managers are everywhere, with seemingly every language and operating\nsystem implementing their own solution. The lack of interoperability between\nthese systems means that multi-lingual projects are unable to express precise\ndependencies across language ecosystems, and external system and hardware\ndependencies are typically implicit and unversioned. We define HyperRes, a\nformal system for describing versioned dependency resolution using a hypergraph\nthat is expressive enough to model many ecosystems and solve dependency\nconstraints across them. We define translations from dozens of existing package\nmanagers to HyperRes and comprehensively demonstrate that dependency resolution\ncan work across ecosystems that are currently distinct. This does not require\nusers to shift their choice of package managers; instead, HyperRes allows for\nthe translation of packaging metadata between ecosystems, and for solving to be\nprecisely specialised to a particular deployment environment.", "AI": {"tldr": "HyperRes is a formal system that uses hypergraphs to unify versioned dependency resolution across many package manager ecosystems, enabling true cross-language and cross-platform dependency management without changing users' current tools.", "motivation": "The motivation is the current lack of interoperability between different package managers, which hampers multi-lingual projects from expressing precise, cross-language dependencies and makes many system and hardware dependencies implicit or unversioned.", "method": "The paper defines HyperRes, a formal system based on a hypergraph model, to describe versioned dependency resolution. It includes translations from existing package managers to HyperRes and models dependency constraints across ecosystems.", "result": "HyperRes is demonstrated to work across currently distinct package manager ecosystems, allowing cross-ecosystem dependency resolution without requiring users to change their choice of package manager.", "conclusion": "HyperRes enables translation of packaging metadata and precise, environment-specialized dependency resolution across multiple ecosystems, enhancing interoperability and reducing fragmentation between package managers."}}
{"id": "2506.10051", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10051", "abs": "https://arxiv.org/abs/2506.10051", "authors": ["Md Istiak Hossain Shihab", "Christopher Hundhausen", "Ahsun Tariq", "Summit Haque", "Yunhan Qiao", "Brian Mulanda"], "title": "The Effects of GitHub Copilot on Computing Students' Programming Effectiveness, Efficiency, and Processes in Brownfield Programming Tasks", "comment": "14 pages, 5 figures", "summary": "When graduates of computing degree programs enter the software industry, they\nwill most likely join teams working on legacy code bases developed by people\nother than themselves. In these so-called brownfield software development\nsettings, generative artificial intelligence (GenAI) coding assistants like\nGitHub Copilot are rapidly transforming software development practices, yet the\nimpact of GenAI on student programmers performing brownfield development tasks\nremains underexplored. This paper investigates how GitHub Copilot influences\nundergraduate students' programming performance, behaviors, and understanding\nwhen completing brownfield programming tasks in which they add new code to an\nunfamiliar code base. We conducted a controlled experiment in which 10\nundergraduate computer science students completed highly similar brownfield\ndevelopment tasks with and without Copilot in a legacy web application. Using a\nmixed-methods approach combining performance analysis, behavioral analysis, and\nexit interviews, we found that students completed tasks 35% faster (p < 0.05)\nand made 50% more solution progress p (< 0.05) when using Copilot. Moreover,\nour analysis revealed that, when using Copilot, students spent 11% less time\nmanually writing code (p < 0.05), and 12% less time conducting web searches (p\n< 0.05), providing evidence of a fundamental shift in how they engaged in\nprogramming. In exit interviews, students reported concerns about not\nunderstanding how or why Copilot suggestions work. This research suggests the\nneed for computing educators to develop new pedagogical approaches that\nleverage GenAI assistants' benefits while fostering reflection on how and why\nGenAI suggestions address brownfield programming tasks. Complete study results\nand analysis are presented at https://ghcopilot-icer.github.io/.", "AI": {"tldr": "GitHub Copilot helps undergraduates complete unfamiliar coding tasks faster and with less manual effort, but students are worried they don\u2019t fully grasp the AI\u2019s solutions. Educators need to adapt teaching so students get the most from AI while still understanding the code.", "motivation": "The increasing integration of generative AI coding assistants like GitHub Copilot in software development is changing industry practices. However, the impact of such tools on undergraduate students' effectiveness in working on legacy (brownfield) code has not been thoroughly studied.", "method": "A controlled experiment was conducted involving 10 undergraduate computer science students. Each student completed similar brownfield development tasks, both with and without the aid of GitHub Copilot, in an unfamiliar legacy web application. A mixed-methods approach was used, combining quantitative performance and behavioral analysis with qualitative exit interviews.", "result": "With Copilot, students completed programming tasks 35% faster and made 50% more progress on solutions. Students also spent 11% less time manually writing code and 12% less time conducting web searches. However, interviews revealed students were concerned about not fully understanding Copilot\u2019s suggestions.", "conclusion": "GenAI tools like Copilot significantly increase undergraduate students' efficiency and progress when working on brownfield programming tasks. There are, however, valid concerns about superficial understanding of generated code. The findings highlight the need for educators to update teaching methods to both utilize GenAI benefits and promote critical reflection about the code such tools generate."}}
{"id": "2506.10204", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10204", "abs": "https://arxiv.org/abs/2506.10204", "authors": ["Andrei Paleyes", "Radzim Sendyka", "Diana Robinson", "Christian Cabrera", "Neil D. Lawrence"], "title": "Prompt Variability Effects On LLM Code Generation", "comment": null, "summary": "Code generation is one of the most active areas of application of Large\nLanguage Models (LLMs). While LLMs lower barriers to writing code and\naccelerate development process, the overall quality of generated programs\ndepends on the quality of given prompts. Specifically, functionality and\nquality of generated code can be sensitive to user's background and familiarity\nwith software development. It is therefore important to quantify LLM's\nsensitivity to variations in the input. To this end we propose a synthetic\nevaluation pipeline for code generation with LLMs, as well as a systematic\npersona-based evaluation approach to expose qualitative differences of LLM\nresponses dependent on prospective user background. Both proposed methods are\ncompletely independent from specific programming tasks and LLMs, and thus are\nwidely applicable. We provide experimental evidence illustrating utility of our\nmethods and share our code for the benefit of the community.", "AI": {"tldr": "The authors present two general evaluation methods for testing code generation by LLMs, focusing on how prompt quality and user background affect output. Their approaches are model- and task-independent, experimentally validated, and made available for community use.", "motivation": "Code generation with Large Language Models (LLMs) is greatly influenced by the quality of prompts, which in turn can be affected by the user's background and programming familiarity. There is a need to assess how sensitive LLMs are to variations in user input and to user personas.", "method": "The authors propose a synthetic evaluation pipeline specifically for code generation with LLMs, alongside a systematic persona-based evaluation approach. These methods aim to identify qualitative differences in LLM outputs depending on user backgrounds, and are designed to be task- and model-agnostic.", "result": "The authors provide experimental evidence supporting the effectiveness and utility of their evaluation methods, and share their codebase for community use.", "conclusion": "The paper demonstrates that their synthetic and persona-based evaluation methods can reveal important differences in LLM code generation quality based on user input variations and backgrounds, offering broadly applicable tools for assessing LLM performance in code generation."}}
{"id": "2506.10280", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.10280", "abs": "https://arxiv.org/abs/2506.10280", "authors": ["Samiha Shimmi", "Hamed Okhravi", "Mona Rahimi"], "title": "AI-Based Software Vulnerability Detection: A Systematic Literature Review", "comment": null, "summary": "Software vulnerabilities in source code pose serious cybersecurity risks,\nprompting a shift from traditional detection methods (e.g., static analysis,\nrule-based matching) to AI-driven approaches. This study presents a systematic\nreview of software vulnerability detection (SVD) research from 2018 to 2023,\noffering a comprehensive taxonomy of techniques, feature representations, and\nembedding methods. Our analysis reveals that 91% of studies use AI-based\nmethods, with graph-based models being the most prevalent. We identify key\nlimitations, including dataset quality, reproducibility, and interpretability,\nand highlight emerging opportunities in underexplored techniques such as\nfederated learning and quantum neural networks, providing a roadmap for future\nresearch.", "AI": {"tldr": "A systematic review shows AI-based, especially graph-based, models leading software vulnerability detection from 2018 to 2023, but points to persistent issues with data, reproducibility, and interpretability, and indicates opportunities in emerging techniques.", "motivation": "Software vulnerabilities are a major cybersecurity threat, and traditional detection methods are being surpassed by AI-driven approaches. There is a need for a systematic review summarizing advances in this field and outlining current techniques, trends, and gaps, to guide future research.", "method": "This paper conducts a systematic literature review of software vulnerability detection research published between 2018 and 2023. It categorizes techniques, feature representations, and embedding methods, analyzes trends, and identifies limitations and future directions.", "result": "A comprehensive taxonomy of SVD research is provided. 91% of studies use AI-based methods, with graph-based models as the most common. Major limitations in the field include dataset quality, lack of reproducibility, and challenges with interpretability. The review also identifies opportunities for future work in areas such as federated learning and quantum neural networks.", "conclusion": "AI-driven methods, especially graph-based models, dominate recent SVD research. However, the field faces challenges in data quality, reproducibility, and model interpretability. Exploring new approaches like federated learning and quantum neural networks represents a promising research direction."}}
{"id": "2506.10322", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10322", "abs": "https://arxiv.org/abs/2506.10322", "authors": ["Xueying Du", "Kai Yu", "Chong Wang", "Yi Zou", "Wentai Deng", "Zuoyu Ou", "Xin Peng", "Lingming Zhang", "Yiling Lou"], "title": "Minimizing False Positives in Static Bug Detection via LLM-Enhanced Path Feasibility Analysis", "comment": null, "summary": "Static bug analyzers play a crucial role in ensuring software quality.\nHowever, existing analyzers for bug detection in large codebases often suffer\nfrom high false positive rates. This is primarily due to the limited\ncapabilities of analyzers in path feasibility validation with multiple\nconditional branches and complex data dependencies. While current LLM-based\napproaches attempt to address this issue, their effectiveness remains limited\ndue to insufficient constraint cascade analysis and scalability challenges in\nlarge projects. To address this challenge, we propose an iterative path\nfeasibility analysis framework LLM4PFA. By leveraging LLM agent based targeted\nconstraint reasoning, and key context-aware analysis driven by agent planning,\nLLM4PFA effectively enhances complex inter-procedural path feasibility analysis\nfor minimizing false positives in static bug detection. Evaluation results show\nthat LLM4PFA precisely filters out 72% to 96% false positives reported during\nstatic bug detection, significantly outperforming all the baselines by 41.1% -\n105.7% improvements; meanwhile LLM4PFA only misses 3 real bugs of 45 true\npositives.", "AI": {"tldr": "LLM4PFA, a novel LLM-based framework, dramatically lowers false positives in static bug detection within large projects by performing advanced, agent-driven path feasibility analysis, and sets a new performance standard over existing techniques.", "motivation": "Existing static bug analyzers have high false positive rates, mainly due to challenges in path feasibility validation amidst conditional branches and complex data dependencies. Current LLM-based approaches also fall short due to weak constraint cascade analysis and scalability issues in large projects.", "method": "The paper proposes LLM4PFA, an iterative path feasibility analysis framework. It leverages LLM agent-based targeted constraint reasoning combined with context-aware analysis, driven by agent planning, to improve complex inter-procedural path feasibility and reduce false positives in static bug detection.", "result": "LLM4PFA was able to filter out 72% to 96% of false positives in static bug detection, outperforming baselines by 41.1% to 105.7%. It missed only 3 real bugs out of 45 true positives, demonstrating high precision and effectiveness.", "conclusion": "LLM4PFA significantly reduces false positives in static bug analysis of large codebases, offering scalable, precise path feasibility validation, and surpasses current state-of-the-art methods."}}
{"id": "2506.10330", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10330", "abs": "https://arxiv.org/abs/2506.10330", "authors": ["Seyed Moein Abtahi", "Akramul Azim"], "title": "Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements", "comment": "Accepted at FORGE 2025", "summary": "This study examined code issue detection and revision automation by\nintegrating Large Language Models (LLMs) such as OpenAI's GPT-3.5 Turbo and\nGPT-4o into software development workflows. A static code analysis framework\ndetects issues such as bugs, vulnerabilities, and code smells within a\nlarge-scale software project. Detailed information on each issue was extracted\nand organized to facilitate automated code revision using LLMs. An iterative\nprompt engineering process is applied to ensure that prompts are structured to\nproduce accurate and organized outputs aligned with the project requirements.\nRetrieval-augmented generation (RAG) is implemented to enhance the relevance\nand precision of the revisions, enabling LLM to access and integrate real-time\nexternal knowledge. The issue of LLM hallucinations - where the model generates\nplausible but incorrect outputs - is addressed by a custom-built \"Code\nComparison App,\" which identifies and corrects erroneous changes before\napplying them to the codebase. Subsequent scans using the static code analysis\nframework revealed a significant reduction in code issues, demonstrating the\neffectiveness of combining LLMs, static analysis, and RAG to improve code\nquality, streamline the software development process, and reduce time and\nresource expenditure.", "AI": {"tldr": "Integrating LLMs and retrieval-augmented techniques with static code analysis automates code issue fixing, significantly improves software quality, and reduces workload, while a custom tool manages LLM inaccuracies.", "motivation": "The motivation is to automate the detection and correction of code issues (bugs, vulnerabilities, code smells) in large-scale software projects to improve code quality and reduce the resources and time required for manual revision.", "method": "The method involves integrating large language models (GPT-3.5 Turbo, GPT-4o) with a static code analysis framework. Detected issues are fed into an LLM-powered revision system, with iterative prompt engineering for improved output quality and RAG (retrieval-augmented generation) for enhanced contextual accuracy. A 'Code Comparison App' is used to filter and correct hallucinated LLM outputs before changes are merged.", "result": "The approach led to a significant reduction in code issues, as observed by repeated runs of the static code analysis tool after automated revisions.", "conclusion": "Combining large language models, static code analysis, and retrieval-augmented generation can effectively automate code issue detection and correction, thereby improving code quality and development efficiency."}}
{"id": "2506.10365", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10365", "abs": "https://arxiv.org/abs/2506.10365", "authors": ["Shuyang Hou", "Zhangxiao Shen", "Huayi Wu", "Haoyue Jiao", "Ziqi Liu", "Lutong Xie", "Chang Liu", "Jianyuan Liang", "Yaxian Qing", "Xiaopu Zhang", "Dehua Peng", "Zhipeng Gui", "Xuefeng Guan"], "title": "AutoGEEval++: A Multi-Level and Multi-Geospatial-Modality Automated Evaluation Framework for Large Language Models in Geospatial Code Generation on Google Earth Engine", "comment": null, "summary": "Geospatial code generation is becoming a key frontier in integrating\nartificial intelligence with geo-scientific analysis, yet standardised\nautomated evaluation tools for this task remain absent. This study presents\nAutoGEEval++, an enhanced framework building on AutoGEEval, and the first\nautomated assessment system for large language models (LLMs) generating\ngeospatial code on Google Earth Engine (GEE). It supports diverse data\nmodalities and varying task complexities. Built on the GEE Python API,\nAutoGEEval++ features a benchmark dataset-AutoGEEval++-Bench-with 6,365 test\ncases across 26 data types and three task categories: unit, combo, and theme\ntests. It includes a submission programme and a judge module to realise an\nend-to-end automated evaluation pipeline from code generation to\nexecution-based validation. The framework adopts multi-dimensional\nmetrics-accuracy, resource usage, run-time efficiency, and error\ntypes-balancing hallucination control and efficiency, and enabling boundary\ntesting and error pattern analysis. Using AutoGEEval++, we evaluate 24\nstate-of-the-art LLMs (as of June 2025), including general-purpose,\nreasoning-enhanced, code-centric, and geoscience-specific models. Results\nreveal clear performance, stability, and error differences across task types,\nmodel designs, and deployment settings, confirming AutoGEEval++'s practical\nvalue and scalability in vertical-domain code generation. This work establishes\nthe first standardised evaluation protocol and foundational benchmark for\nGEE-based LLM code generation, providing a unified basis for performance\ncomparison and a methodological framework for systematic, domain-specific code\nevaluation.", "AI": {"tldr": "AutoGEEval++ is a novel, comprehensive automated system and benchmark for evaluating large language models generating geospatial code in Google Earth Engine, providing standardized, detailed, and scalable performance assessment across 24 models, and setting the standard for domain-specific code evaluation.", "motivation": "With the rise of artificial intelligence in geoscientific code generation, current methods lack standardized, automated evaluation tools\u2014specifically for geospatial code written for platforms like Google Earth Engine (GEE). This creates a need for reliable benchmarks and protocols to assess the performance of large language models (LLMs) in this specialized domain.", "method": "The authors introduce AutoGEEval++, an enhanced evaluation framework and benchmark dataset for assessing LLMs that generate geospatial code on GEE. The system includes a comprehensive suite of 6,365 test cases, spanning multiple data types and complexity levels, and evaluates code across various metrics\u2014accuracy, resource usage, run-time, and error analysis. AutoGEEval++ includes submission and judge modules, enabling full automation from code generation to execution-based validation.", "result": "AutoGEEval++ was used to systematically evaluate 24 state-of-the-art LLMs from different categories, demonstrating differences in performance, stability, and errors across models and tasks. The results validated both the practical utility and scalability of AutoGEEval++ for evaluating domain-specific code generation by LLMs.", "conclusion": "This research provides the first standardized evaluation protocol and large-scale benchmark for GEE-based LLM code generation. AutoGEEval++ supports more rigorous and unified model comparison and establishes a methodological foundation for future developments in automated, domain-specific geospatial code assessment."}}
{"id": "2506.10376", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.10376", "abs": "https://arxiv.org/abs/2506.10376", "authors": ["Fan Wu", "Cuiyun Gao", "Shuqing Li", "Xin-Cheng Wen", "Qing Liao"], "title": "MLLM-Based UI2Code Automation Guided by UI Layout Information", "comment": "Accepted by the 34th International Symposium on Software Testing and\n  Analysis (ISSTA 2025)", "summary": "Converting user interfaces into code (UI2Code) is a crucial step in website\ndevelopment, which is time-consuming and labor-intensive. The automation of\nUI2Code is essential to streamline this task, beneficial for improving the\ndevelopment efficiency. There exist deep learning-based methods for the task;\nhowever, they heavily rely on a large amount of labeled training data and\nstruggle with generalizing to real-world, unseen web page designs. The advent\nof Multimodal Large Language Models (MLLMs) presents potential for alleviating\nthe issue, but they are difficult to comprehend the complex layouts in UIs and\ngenerate the accurate code with layout preserved. To address these issues, we\npropose LayoutCoder, a novel MLLM-based framework generating UI code from\nreal-world webpage images, which includes three key modules: (1) Element\nRelation Construction, which aims at capturing UI layout by identifying and\ngrouping components with similar structures; (2) UI Layout Parsing, which aims\nat generating UI layout trees for guiding the subsequent code generation\nprocess; and (3) Layout-Guided Code Fusion, which aims at producing the\naccurate code with layout preserved. For evaluation, we build a new benchmark\ndataset which involves 350 real-world websites named Snap2Code, divided into\nseen and unseen parts for mitigating the data leakage issue, besides the\npopular dataset Design2Code. Extensive evaluation shows the superior\nperformance of LayoutCoder over the state-of-the-art approaches. Compared with\nthe best-performing baseline, LayoutCoder improves 10.14% in the BLEU score and\n3.95% in the CLIP score on average across all datasets.", "AI": {"tldr": "The paper introduces LayoutCoder, a novel framework using multimodal large language models to convert webpage images into accurate UI code while preserving layout. It significantly outperforms previous methods, aided by new techniques for layout understanding and a robust new benchmark dataset.", "motivation": "Converting user interfaces to code (UI2Code) is a time-consuming, labor-intensive step in website development. Automating this process would significantly improve development efficiency, but existing deep learning methods require extensive labeled data and fail to generalize to new real-world designs. Multimodal Large Language Models (MLLMs) could help but struggle with complex layouts and accurate code generation.", "method": "The paper proposes LayoutCoder, a novel MLLM-based framework for generating UI code from real-world webpage images. LayoutCoder has three main modules: (1) Element Relation Construction that groups UI components by similar structures, (2) UI Layout Parsing to generate layout trees, and (3) Layout-Guided Code Fusion for accurate code with preserved layouts. They also introduce a new dataset, Snap2Code, of 350 real-world websites split into seen and unseen sets.", "result": "LayoutCoder demonstrates superior performance to state-of-the-art methods. Empirical results show a 10.14% improvement in BLEU score and a 3.95% improvement in CLIP score, averaged across datasets, compared with the best-performing baseline. Extensive evaluations were conducted on both the new Snap2Code dataset and the established Design2Code dataset.", "conclusion": "LayoutCoder outperforms existing UI2Code methods by effectively capturing and preserving complex UI layouts in generated code, thanks to its layout-aware MLLM framework. Its superior generalization and code accuracy are validated across multiple datasets, demonstrating practical benefits for real-world website development."}}
{"id": "2506.10397", "categories": ["cs.SE", "cs.CY", "cs.DC"], "pdf": "https://arxiv.org/pdf/2506.10397", "abs": "https://arxiv.org/abs/2506.10397", "authors": ["Mir Mohammad Yousuf", "Shabir Ahmad Sofi"], "title": "Bug Classification in Quantum Software: A Rule-Based Framework and Its Evaluation", "comment": "25 pages, 5 figures", "summary": "Accurate classification of software bugs is essential for improving software\nquality. This paper presents a rule-based automated framework for classifying\nissues in quantum software repositories by bug type, category, severity, and\nimpacted quality attributes, with additional focus on quantum-specific bug\ntypes. The framework applies keyword and heuristic-based techniques tailored to\nquantum computing. To assess its reliability, we manually classified a\nstratified sample of 4,984 issues from a dataset of 12,910 issues across 36\nQiskit repositories. Automated classifications were compared with ground truth\nusing accuracy, precision, recall, and F1-score. The framework achieved up to\n85.21% accuracy, with F1-scores ranging from 0.7075 (severity) to 0.8393\n(quality attribute). Statistical validation via paired t-tests and Cohen's\nKappa showed substantial to almost perfect agreement for bug type (k = 0.696),\ncategory (k = 0.826), quality attribute (k = 0.818), and quantum-specific bug\ntype (k = 0.712). Severity classification showed slight agreement (k = 0.162),\nsuggesting room for improvement. Large-scale analysis revealed that classical\nbugs dominate (67.2%), with quantum-specific bugs at 27.3%. Frequent bug\ncategories included compatibility, functional, and quantum-specific defects,\nwhile usability, maintainability, and interoperability were the most impacted\nquality attributes. Most issues (93.7%) were low severity; only 4.3% were\ncritical. A detailed review of 1,550 quantum-specific bugs showed that over\nhalf involved quantum circuit-level problems, followed by gate errors and\nhardware-related issues.", "AI": {"tldr": "A specialized, automated framework accurately classifies bugs in quantum software repositories, revealing that classical bugs predominate and pointing to areas needing further improvement, especially severity detection.", "motivation": "The increasing importance of quantum software demands higher quality and reliability, making accurate classification of software bugs critical to guide improvements and maintenance efforts. Existing approaches lack robust methods tuned to the unique characteristics and challenges of quantum software.", "method": "The paper develops a rule-based automated framework that uses keyword and heuristic-based strategies tailored specifically for quantum computing. It classifies issues in quantum software repositories by various dimensions (bug type, category, severity, impacted quality attributes, and quantum-specific bug types). The framework's reliability is evaluated by manually classifying a stratified sample of 4,984 issues and comparing automated results to this ground truth using accuracy metrics and statistical validation.", "result": "The framework achieved up to 85.21% accuracy, with F1-scores between 0.7075 (severity) and 0.8393 (quality attribute). Cohen's Kappa showed strong agreement for most dimensions, except severity, which had only slight agreement. Large-scale analysis revealed that classical bugs are more common than quantum-specific ones, and most bugs are low severity. The most frequent bug categories were compatibility, functional, and quantum-specific defects, and the main impacted quality attributes were usability, maintainability, and interoperability.", "conclusion": "The automated classification framework is effective for distinguishing between various bug types and categories in quantum software, with strong agreement to manual classification except for severity. Classical bugs dominate quantum software repositories, but quantum-specific bugs constitute a significant portion. Severity classification requires further enhancement. The analysis provides actionable insights for improving quantum software quality."}}
{"id": "2506.10426", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10426", "abs": "https://arxiv.org/abs/2506.10426", "authors": ["Xiao Yu", "Haoxuan Chen", "Feifei Niu", "Xing Hu", "Jacky Wai Keung", "Xin Xia"], "title": "Towards Understanding Bugs in Distributed Training and Inference Frameworks for Large Language Models", "comment": null, "summary": "With the rapid development of large language models (LLMs), distributed\ntraining and inference frameworks like DeepSpeed have become essential for\nscaling model training and inference across multiple GPUs or nodes. However,\nthe increasing complexity of these frameworks brings non-trivial software bugs,\nwhich may degrade training performance, cause unexpected failures, and result\nin significant resource waste. Understanding framework bugs' characteristics is\nfundamental for quality assurance, allowing the design of more effective\ndebugging and repair methods. Thus, our paper conducts the first large-scale\nempirical analysis of 308 fixed bugs across three popular distributed\ntraining/inference frameworks: DeepSpeed, Megatron-LM, and Colossal-AI. We\nexamine bug symptoms, root causes, bug identification and fixing efforts, and\ncommon low-effort fixing strategies. Additionally, the distributed nature of\nthese frameworks introduces unique bug root causes, such as allocation strategy\nerror and distributed communication error. Diagnosing and fixing complex bugs\nremains challenging due to factors like the disconnect between symptoms and\nroot causes, high bug reproduction costs, and low-level or cross-component\ninteractions. Interestingly, we observe that 48% of bug fixes require minimal\ncode changes (<=10 LOC) and follow simple strategies such as conditional logic\noptimization, parameter handling enhancement, or version compatibility\nhandling, indicating potential for automation. Based on these insights, we\noffer several implications for improving the reliability of both distributed\ntraining and inference frameworks and their dependent LLM projects, while also\nidentifying opportunities to leverage LLM-based tools for automated debugging\nand repair.", "AI": {"tldr": "This paper empirically analyzed 308 fixed bugs in distributed LLM frameworks, finding that while many bugs are challenging due to system complexity, nearly half require only simple fixes. There is significant potential to automate debugging and repairs using LLM-based tools.", "motivation": "The complexity of distributed training and inference frameworks for large language models leads to more software bugs, impacting performance and reliability. There is a need to systematically understand these bugs to improve quality and inform more effective debugging and repair methods.", "method": "A large-scale empirical analysis of 308 fixed bugs was conducted across three major distributed frameworks: DeepSpeed, Megatron-LM, and Colossal-AI. The study examined aspects such as bug symptoms, root causes, how bugs were identified and fixed, and common strategies for low-effort fixes.", "result": "The analysis revealed unique root causes tied to distributed systems (like allocation strategy and communication errors). Diagnosing and fixing these bugs is challenging due to symptom-root cause disconnects, high reproduction costs, and complex interactions. However, 48% of bug fixes required minimal code changes (<=10 LOC) and used straightforward strategies like logic or parameter tweaks, suggesting automation is feasible.", "conclusion": "The findings highlight both the difficulties of debugging distributed frameworks and the significant potential for automation, especially via LLM-based tools. The insights will help improve the reliability of such frameworks and offer guidance for future research on automated debugging and repair."}}
{"id": "2506.10484", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10484", "abs": "https://arxiv.org/abs/2506.10484", "authors": ["Fangwen Mu", "Junjie Wang", "Lin Shi", "Song Wang", "Shoubin Li", "Qing Wang"], "title": "EXPEREPAIR: Dual-Memory Enhanced LLM-based Repository-Level Program Repair", "comment": null, "summary": "Automatically repairing software issues remains a fundamental challenge at\nthe intersection of software engineering and AI. Although recent advancements\nin Large Language Models (LLMs) have demonstrated potential for\nrepository-level repair tasks, current methodologies exhibit two notable\nlimitations: (1) they often address issues in isolation, neglecting to\nincorporate insights from previously resolved issues, and (2) they rely on\nstatic and rigid prompting strategies, which constrain their ability to\ngeneralize across diverse and evolving issue scenarios. Inspired by the dual\nmemory systems of human cognition, where episodic and semantic memories work\nsynergistically to support human reasoning and decision-making, we propose\nExpeRepair, a novel LLM-based approach that continuously learns from historical\nrepair experiences through dual-channel knowledge accumulation. ExpeRepair\norganizes historical repair experiences into two complementary memories: an\nepisodic memory that stores concrete repair demonstrations, and a semantic\nmemory that encodes abstract reflective insights. At inference time, ExpeRepair\nactivates both memory systems by retrieving relevant demonstrations from\nepisodic memory and recalling high-level repair insights from semantic memory.\nIt further enhances adaptability through dynamic prompt composition,\nsynergistically integrating both memory types to replace static prompts with\ncontext-aware, experience-driven prompts. Experiments on the SWE-bench Lite\nbenchmark demonstrate that ExpeRepair achieves a pass@1 score of 49.3% with\nClaude 3.7 Sonnet, outperforming all state-of-the-art open-source methods.", "AI": {"tldr": "ExpeRepair enhances automated software repairs by learning from past experiences using both episodic and semantic memories, and outperforms existing methods with adaptive, context-aware prompts.", "motivation": "Current LLM-based software repair techniques often treat issues in isolation and use static prompting, limiting their adaptability and generalization. There is a need to leverage past repair experiences to improve automated issue resolution.", "method": "ExpeRepair, a novel approach inspired by the dual memory model of human cognition, uses large language models and organizes repair history into episodic (concrete examples) and semantic (abstract insights) memories. During inference, it dynamically retrieves and integrates relevant information from both memory types to construct adaptive, context-aware prompts.", "result": "Experiments on the SWE-bench Lite benchmark show that ExpeRepair, using Claude 3.7 Sonnet, achieves a pass@1 score of 49.3%, surpassing all current state-of-the-art open-source solutions.", "conclusion": "ExpeRepair's dual-memory, experience-driven prompting substantially boosts automated software repair performance, highlighting the advantages of marrying episodic and semantic memory with dynamic prompt construction for LLM-based repair systems."}}
{"id": "2506.10501", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10501", "abs": "https://arxiv.org/abs/2506.10501", "authors": ["Surya Jasper", "Minh Luu", "Evan Pan", "Aakash Tyagi", "Michael Quinn", "Jiang Hu", "David Kebo Houngninou"], "title": "BugGen: A Self-Correcting Multi-Agent LLM Pipeline for Realistic RTL Bug Synthesis", "comment": null, "summary": "Hardware complexity continues to strain verification resources, motivating\nthe adoption of machine learning (ML) methods to improve debug efficiency.\nHowever, ML-assisted debugging critically depends on diverse and scalable bug\ndatasets, which existing manual or automated bug insertion methods fail to\nreliably produce. We introduce BugGen, a first of its kind, fully autonomous,\nmulti-agent pipeline leveraging Large Language Models (LLMs) to systematically\ngenerate, insert, and validate realistic functional bugs in RTL. BugGen\npartitions modules, selects mutation targets via a closed-loop agentic\narchitecture, and employs iterative refinement and rollback mechanisms to\nensure syntactic correctness and functional detectability. Evaluated across\nfive OpenTitan IP blocks, BugGen produced 500 unique bugs with 94% functional\naccuracy and achieved a throughput of 17.7 validated bugs per hour-over five\ntimes faster than typical manual expert insertion. Additionally, BugGen\nidentified 104 previously undetected bugs in OpenTitan regressions,\nhighlighting its utility in exposing verification coverage gaps. Compared\nagainst Certitude, BugGen demonstrated over twice the syntactic accuracy,\ndeeper exposure of testbench blind spots, and more functionally meaningful and\ncomplex bug scenarios. Furthermore, when these BugGen-generated datasets were\nemployed to train ML-based failure triage models, we achieved high\nclassification accuracy (88.1%-93.2%) across different IP blocks, confirming\nthe practical utility and realism of generated bugs. BugGen thus provides a\nscalable solution for generating high-quality bug datasets, significantly\nenhancing verification efficiency and ML-assisted debugging.", "AI": {"tldr": "BugGen is an automated system that uses AI to generate and validate realistic hardware bugs, producing accurate bug datasets much faster than humans and existing tools. It helps reveal gaps in verification and boosts the performance of machine learning models for debugging, making it a valuable new resource for hardware verification teams.", "motivation": "As hardware systems grow in complexity, verifying their correctness becomes increasingly difficult and resource-intensive. There is a clear need for better, scalable methods to generate realistic bug datasets, which are essential for improving the effectiveness of machine learning (ML)-assisted debugging and verification. Existing approaches to bug insertion\u2014whether manual or automated\u2014often fail to produce diverse and reliable bugs at scale, limiting the potential of ML-based solutions.", "method": "The authors introduce BugGen, an autonomous multi-agent pipeline that uses Large Language Models (LLMs) to generate, insert, and validate realistic functional bugs in RTL (Register Transfer Level) designs. BugGen automatically partitions RTL modules, selects mutation targets with a closed-loop agent-based approach, and applies iterative refinement and rollback mechanisms to ensure each bug is both syntactically correct and functionally detectable. Evaluation is performed on five OpenTitan IP blocks.", "result": "BugGen produced 500 unique bugs with a 94% functional accuracy rate and a throughput of 17.7 validated bugs per hour, which is more than five times faster than manual expert insertion. It also revealed 104 previously undetected bugs in OpenTitan regressions, demonstrating its effectiveness in uncovering verification gaps. Compared to Certitude, BugGen achieved over twice the syntactic accuracy and generated more complex and functionally relevant bug scenarios. When datasets generated by BugGen were used to train ML-based failure triage models, these models achieved classification accuracy rates of 88.1% to 93.2% across various IP blocks.", "conclusion": "BugGen offers a scalable, autonomous solution for generating high-quality bug datasets, which greatly improves the efficiency of hardware verification and the performance of ML-assisted debugging. Its ability to rapidly create diverse and realistic bugs not only enhances ML training but also exposes critical verification blind spots that are often missed by traditional approaches."}}
{"id": "2506.10525", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10525", "abs": "https://arxiv.org/abs/2506.10525", "authors": ["Junhang Cheng", "Fang Liu", "Chengru Wu", "Li Zhang"], "title": "AdaptiveLLM: A Framework for Selecting Optimal Cost-Efficient LLM for Code-Generation Based on CoT Length", "comment": "Accepted by Internetware 2025", "summary": "While Large Language Models (LLMs) have significantly advanced code\ngeneration efficiency, they face inherent challenges in balancing performance\nand inference costs across diverse programming tasks. Dynamically selecting the\noptimal LLM based on task difficulty and resource constraints offers a\npromising approach to achieve an optimal balance between efficiency and\nperformance. However, existing model selection methods are resource-intensive\nand often neglect cost efficiency. Moreover, these approaches rely on\nhuman-annotated difficulty labels that are frequently inaccessible in\nreal-world settings and may not align with the LLM's own assessment of task\ndifficulty. In this paper, we introduce AdaptiveLLM, a framework that\ndynamically selects optimal LLMs for a given coding task by automatically\nassessing task difficulty. Our framework first estimates task difficulty using\nChain-of-Thought lengths generated by reasoning model, clusters these into\nthree difficulty levels via k-means, and fine-tunes CodeBERT to embed\ndifficulty-aware features. A trained XGBoost classifier then selects the best\nmodel for each problem, optimizing the performance-cost trade-off. Experimental\nresults show that AdaptiveLLM achieves a 7.86% improvement in pass@1 score\nwhile reducing resource consumption by 88.9% compared to baseline method\nComplexityNet. When compared to a single model, AdaptiveLLM demonstrates an\napproximately 15% accuracy improvement, while maintaining the same level of\ncost consumption. Apart from that, the difficulty assessment using CoT provides\nmore reliable selection criteria than human evaluation. Our replication package\nis available at https://github.com/cjhCoder7/AdaptiveLLM.", "AI": {"tldr": "AdaptiveLLM dynamically selects the best LLM for code tasks by automatically estimating task difficulty, boosting accuracy by up to 15% and cutting computing costs nearly 90%, all without needing human-labeled data.", "motivation": "Large Language Models (LLMs) improve code generation, but struggle to balance high performance with low inference costs for varied programming tasks. Existing solutions for dynamic model selection are resource-intensive and depend on human-labeled difficulty data, which may not match the LLM's understanding or be easily available in real-world applications.", "method": "This paper proposes AdaptiveLLM, which uses automatically measured Chain-of-Thought (CoT) lengths from a reasoning model to estimate code task difficulty. Using k-means clustering, tasks are divided into three difficulty levels. CodeBERT is fine-tuned to recognize difficulty-aware features, and an XGBoost classifier selects the most efficient LLM for each task based on this information, optimizing efficiency and performance trade-offs.", "result": "AdaptiveLLM improves code generation accuracy (7.86% better pass@1 score) and slashes computational resource usage by 88.9% compared to the previous method (ComplexityNet). It also achieves about 15% higher accuracy than using a single LLM model, with comparable cost. The automatic difficulty assessment using CoT lengths is shown to be more reliable than human evaluations.", "conclusion": "AdaptiveLLM addresses both the cost and performance challenges of dynamic LLM selection for code generation by auto-assessing task difficulty, leading to both higher accuracy and greatly reduced computational costs. The proposed approach enables practical deployment of adaptive model selection in real-world settings without relying on human-labeled data."}}
{"id": "2506.10624", "categories": ["cs.SE", "cs.AR"], "pdf": "https://arxiv.org/pdf/2506.10624", "abs": "https://arxiv.org/abs/2506.10624", "authors": ["Lukas J\u00fcnger", "Jan Henrik Weinstock", "Tim Kraus"], "title": "Scalable Software Testing in Fast Virtual Platforms: Leveraging SystemC, QEMU and Containerization", "comment": "Published in DVCon China 2025", "summary": "The ever-increasing complexity of HW/SW systems presents a persistent\nchallenge, particularly in safety-critical domains like automotive, where\nextensive testing is imperative. However, the availability of hardware often\nlags behind, hindering early-stage software development. To address this,\nVirtual Platforms (VPs) based on the SystemC TLM-2.0 standard have emerged as a\npivotal solution, enabling pre-silicon execution and testing of unmodified\ntarget software. In this study, we propose an approach leveraging\ncontainerization to encapsulate VPs in order to reduce environment dependencies\nand enable cloud deployment for fast, parallelized test execution, as well as\nopen-source VP technologies such as QEMU and VCML to obviate the need for seat\nlicenses. To demonstrate the efficacy of our approach, we present an Artificial\nIntelligence (AI) accelerator VP case study. Through our research, we offer a\nrobust solution to address the challenges posed by the complexity of HW/SW\nsystems, with practical implications for accelerating HW/SW co-development.", "AI": {"tldr": "The paper proposes using containerized, open-source Virtual Platforms for cloud-based, parallel software testing before hardware is available, demonstrated in an AI accelerator case study, to accelerate HW/SW system development, especially in safety-critical fields.", "motivation": "The increasing complexity of hardware/software (HW/SW) systems, especially in safety-critical industries like automotive, creates a need for extensive testing. However, hardware is often unavailable early on, which restricts timely software development and testing.", "method": "The authors propose encapsulating Virtual Platforms (VPs) based on the SystemC TLM-2.0 standard using containerization technologies. This reduces environment dependencies and enables deployment in the cloud for faster and parallelized test execution. Open-source tools like QEMU and VCML are leveraged to eliminate licensing costs. The proposed method is demonstrated via a case study of an AI accelerator VP.", "result": "The approach enables efficient pre-silicon execution and testing of unmodified target software, supporting fast, parallelized test execution in the cloud and eliminating the need for expensive licenses. The AI accelerator VP case study demonstrates practical applicability and robustness of the approach.", "conclusion": "Containerized, open-source, cloud-deployable VPs provide a robust and practical solution for addressing HW/SW co-development challenges, specifically enabling early-stage, scalable software development and testing. This can significantly accelerate the development cycle for complex HW/SW systems."}}
{"id": "2506.10654", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10654", "abs": "https://arxiv.org/abs/2506.10654", "authors": ["Abir Bouraffa", "Carolin Brandt", "Andy Zaidmann", "Walid Maalej"], "title": "Not One to Rule Them All: Mining Meaningful Code Review Orders From GitHub", "comment": null, "summary": "Developers use tools such as GitHub pull requests to review code, discuss\nproposed changes, and request modifications. While changed files are commonly\npresented in alphabetical order, this does not necessarily coincide with the\nreviewer's preferred navigation sequence. This study investigates the different\nnavigation orders developers follow while commenting on changes submitted in\npull requests. We mined code review comments from 23,241 pull requests in 100\npopular Java and Python repositories on GitHub to analyze the order in which\nthe reviewers commented on the submitted changes. Our analysis shows that for\n44.6% of pull requests, the reviewers comment in a non-alphabetical order.\nAmong these pull requests, we identified traces of alternative meaningful\norders: 20.6% (2,134) followed a largest-diff-first order, 17.6% (1,827) were\ncommented in the order of the files' similarity to the pull request's title and\ndescription, and 29% (1,188) of pull requests containing changes to both\nproduction and test files adhered to a test-first order. We also observed that\nthe proportion of reviewed files to total submitted files was significantly\nhigher in non-alphabetically ordered reviews, which also received slightly\nfewer approvals from reviewers, on average. Our findings highlight the need for\nadditional support during code reviews, particularly for larger pull requests,\nwhere reviewers are more likely to adopt complex strategies rather than\nfollowing a single predefined order.", "AI": {"tldr": "Most code reviewers don't comment on files in alphabetical order, preferring more meaningful approaches like focusing on larger changes or file relevance. These complex strategies are common in larger pull requests, suggesting review tools should offer better navigation flexibility aligned with actual reviewer preferences.", "motivation": "Traditional tools display changed files in pull requests alphabetically, but this order may not suit reviewers' preferred navigation and review strategies. The study aims to understand actual navigation behaviors during code review, potentially informing tool design improvements.", "method": "The researchers mined code review comments from 23,241 pull requests spanning 100 popular Java and Python repositories on GitHub. They analyzed the sequence in which reviewers commented on files to identify patterns beyond simple alphabetical ordering.", "result": "44.6% of pull requests had comments in a non-alphabetical order. Among these, 20.6% followed a largest-diff-first sequence, 17.6% were guided by file similarity to the pull request title/description, and 29% (where both production and test files changed) used a test-first order. Non-alphabetical reviews tended to have a higher proportion of reviewed files but received slightly fewer approvals.", "conclusion": "Many reviewers do not follow alphabetical order; instead, they adopt meaningful, complex navigation strategies, particularly for larger pull requests. Code review tools should support flexible navigation to align with reviewer behavior."}}
{"id": "2506.10704", "categories": ["cs.SE", "cs.AI", "D.2.1; D.2.4; D.2.10; F.4.1; F.4.3"], "pdf": "https://arxiv.org/pdf/2506.10704", "abs": "https://arxiv.org/abs/2506.10704", "authors": ["Arshad Beg", "Diarmuid O'Donoghue", "Rosemary Monahan"], "title": "Formalising Software Requirements using Large Language Models", "comment": "Accepted and presented as a poster in ADAPT Annual Conference\n  (AACS2025) on 15th of May, 2025", "summary": "This paper is a brief introduction to our recently initiated project named\nVERIFAI: Traceability and verification of natural language requirements. The\nproject addresses the challenges in the traceability and verification of formal\nspecifications through providing support for the automatic generation of the\nformal specifications and the traceability of the requirements from the initial\nsoftware design stage through the systems implementation and verification.\nApproaches explored in this project include Natural Language Processing, use of\nontologies to describe the software system domain, reuse of existing software\nartefacts from similar systems (i.e. through similarity based reuse) and large\nlanguage models to identify and declare the specifications as well as use of\nartificial intelligence to guide the process.", "AI": {"tldr": "VERIFAI is a project to automatically generate and trace formal specifications from natural language requirements using NLP, ontologies, LLMs, and AI, addressing key challenges in software requirements verification.", "motivation": "Natural language requirements are difficult to trace and verify throughout the software development lifecycle. Manual translation to formal specifications is error-prone and time-consuming, and ensuring traceability from requirements to implementation remains a key challenge.", "method": "The project explores automatic generation of formal specifications from natural language requirements. It leverages Natural Language Processing (NLP), ontologies to define system domains, similarity-based reuse of artefacts from similar systems, large language models to extract specifications, and artificial intelligence techniques to guide verification and traceability.", "result": "The project has initiated research into integrating NLP, ontologies, similarity-based reuse, LLMs, and AI to automatically generate formal specifications and maintain traceability from design to verification.", "conclusion": "VERIFAI aims to improve traceability and automated verification, promising more reliable software systems by bridging the gap between natural language requirements and formal software specifications using state-of-the-art AI methods."}}
{"id": "2506.10770", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10770", "abs": "https://arxiv.org/abs/2506.10770", "authors": ["Joran Leest", "Claudia Raibulet", "Patricia Lago", "Ilias Gerostathopoulos"], "title": "From Tea Leaves to System Maps: Context-awareness in Monitoring Operational Machine Learning Models", "comment": null, "summary": "Machine learning (ML) models in production do not fail due to statistical\nanomalies in their input data; they fail due to contextual misalignment -- when\ntheir environment deviates from training assumptions, leading to unreliable\npredictions. Effective ML monitoring requires rich contextual information to\nmove beyond detecting statistical shifts toward meaningful alerts and\nsystematic root-cause analysis. Yet, surprisingly, despite extensive research\nin ML monitoring and related disciplines (drift detection, data validation,\nout-of-distribution detection), there is no shared understanding of how to use\ncontextual information -- striking, given that monitoring involves\ninterpretation of information in context. In response, this paper presents a\nsystematic review to characterize and structure the various types of contextual\ninformation in this domain. Our analysis examines 94 primary studies across\ndata mining, databases, software engineering, and ML. We introduce the\nContextual System--Aspect--Representation (C-SAR) framework, a conceptual model\nthat synthesizes our findings. We also identify 20 recurring and potentially\nreusable patterns of specific system, aspect, and representation combinations,\nand map them to the monitoring activities they support. This study provides a\nnew perspective on ML monitoring: from interpreting \"tea leaves\" of\nobservational statistics into constructing and managing \"system maps\" that\nenable systematic and reliable ML monitoring practices.", "AI": {"tldr": "ML model monitoring often fails due to lack of context awareness. This paper reviews the field and introduces the C-SAR framework, offering new ways to use contextual information for reliable, systematic ML monitoring, moving beyond simple anomaly detection.", "motivation": "Traditional ML model monitoring focuses on detecting statistical anomalies but often misses failures caused by contextual misalignment, where real-world use deviates from original training assumptions. There is a lack of a shared framework for leveraging contextual information in ML monitoring.", "method": "The authors conduct a systematic review of 94 primary studies from fields such as machine learning, data mining, databases, and software engineering. They synthesize their findings into a conceptual framework (C-SAR), which classifies types of contextual information relevant to ML monitoring.", "result": "The study proposes the Contextual System--Aspect--Representation (C-SAR) framework to structure how contextual information is used in ML monitoring. They also identify 20 recurring, reusable patterns of system, aspect, and representation combinations, mapping these to specific monitoring activities.", "conclusion": "The C-SAR framework offers a new approach for ML monitoring, shifting from basic anomaly detection to a more meaningful interpretation of context, which enables systematic root-cause analysis and more reliable monitoring. This perspective enables practitioners to construct \"system maps\" for comprehensive ML monitoring beyond simple statistical alerts."}}
{"id": "2506.10785", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10785", "abs": "https://arxiv.org/abs/2506.10785", "authors": ["Vinaik Chhetri", "Krishna Upadhyay", "A. B. Siddique", "Umar Farooq"], "title": "What Users Value and Critique: Large-Scale Analysis of User Feedback on AI-Powered Mobile Apps", "comment": "12 pages, 6 figures, 5 tables", "summary": "Artificial Intelligence (AI)-powered features have rapidly proliferated\nacross mobile apps in various domains, including productivity, education,\nentertainment, and creativity. However, how users perceive, evaluate, and\ncritique these AI features remains largely unexplored, primarily due to the\noverwhelming volume of user feedback. In this work, we present the first\ncomprehensive, large-scale study of user feedback on AI-powered mobile apps,\nleveraging a curated dataset of 292 AI-driven apps across 14 categories with\n894K AI-specific reviews from Google Play. We develop and validate a\nmulti-stage analysis pipeline that begins with a human-labeled benchmark and\nsystematically evaluates large language models (LLMs) and prompting strategies.\nEach stage, including review classification, aspect-sentiment extraction, and\nclustering, is validated for accuracy and consistency. Our pipeline enables\nscalable, high-precision analysis of user feedback, extracting over one million\naspect-sentiment pairs clustered into 18 positive and 15 negative user topics.\nOur analysis reveals that users consistently focus on a narrow set of themes:\npositive comments emphasize productivity, reliability, and personalized\nassistance, while negative feedback highlights technical failures (e.g.,\nscanning and recognition), pricing concerns, and limitations in language\nsupport. Our pipeline surfaces both satisfaction with one feature and\nfrustration with another within the same review. These fine-grained,\nco-occurring sentiments are often missed by traditional approaches that treat\npositive and negative feedback in isolation or rely on coarse-grained analysis.\nTo this end, our approach provides a more faithful reflection of the real-world\nuser experiences with AI-powered apps. Category-aware analysis further uncovers\nboth universal drivers of satisfaction and domain-specific frustrations.", "AI": {"tldr": "The paper introduces a validated, large-scale analysis pipeline for AI mobile app reviews, enabling nuanced insights by extracting and clustering over a million sentiment-aspect pairs. It uncovers common themes in user satisfaction (productivity, reliability) and frustration (technical issues, pricing) that are often missed by traditional review analysis approaches.", "motivation": "The motivation of this paper is to understand how users perceive, evaluate, and critique AI-powered features in mobile applications, which has not been thoroughly explored due to the massive quantity of user feedback.", "method": "The paper develops and validates a multi-stage analysis pipeline that leverages a large, curated dataset of 292 AI-driven apps with 894K AI-specific reviews from Google Play. The pipeline includes human-labeled benchmarking and systematic assessment of large language models (LLMs) and various prompting strategies, with stages for review classification, aspect-sentiment extraction, and clustering. Each step is validated for accuracy and consistency.", "result": "The pipeline enables high-precision, scalable analysis of user feedback, extracting over a million aspect-sentiment pairs and clustering them into 18 positive and 15 negative user topics. Key findings show that user feedback centers around a narrow set of themes: positive feedback highlights productivity, reliability, and personalization, while negative feedback identifies technical failures, pricing, and language support issues. Fine-grained analysis surfaces both satisfaction and frustration within individual reviews and provides a nuanced understanding missed by traditional analyses.", "conclusion": "The proposed analysis pipeline offers a more nuanced and faithful reflection of real-world user experiences in AI-driven mobile apps, identifies both universal and category-specific drivers of satisfaction and frustration, and demonstrates the value of fine-grained, aspect-based sentiment analysis over traditional methods."}}
{"id": "2506.10833", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10833", "abs": "https://arxiv.org/abs/2506.10833", "authors": ["Fabian C. Pe\u00f1a", "Steffen Herbold"], "title": "Evaluating Large Language Models on Non-Code Software Engineering Tasks", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode understanding and generation; however, their effectiveness on non-code\nSoftware Engineering (SE) tasks remains underexplored. We present the first\ncomprehensive benchmark, which we name `Software Engineering Language\nUnderstanding' (SELU), for evaluating LLMs on 17 non-code tasks, spanning from\nidentifying whether a requirement is functional or non-functional to estimating\nthe effort and complexity of backlog items. SELU covers classification,\nregression, Named Entity Recognition (NER), and Masked Language Modeling (MLM)\ntargets, with data drawn from diverse sources such as code repositories, issue\ntracking systems, and developer forums. We fine-tune 22 open-source LLMs,\nprompt two proprietary alternatives, and train two baselines. Performance is\nmeasured using metrics such as F1-macro, SMAPE, F1-micro, and accuracy, and\ncompared via the Bayesian signed-rank test. Our results show that\nmoderate-scale decoder-only models consistently form a top-tier, exhibiting\nhigh mean performance and low across-task variance, while domain adaptation via\ncode-focused pre-training might yield only modest improvements. These insights\nguide model selection for non-code SE workflows and highlight directions for\nexpanding SELU to generative and design-oriented scenarios.", "AI": {"tldr": "SELU is a new benchmark for assessing LLMs on non-code SE tasks. Moderate decoder-only models excel, and code-focused pre-training adds minor benefits. The results advise model choices for real-world SE tasks and set groundwork for future benchmarks.", "motivation": "Large Language Models (LLMs) are already proven in code understanding and generation, but their effectiveness for non-code Software Engineering (SE) tasks is not well studied. There is a need to systematically evaluate LLMs on such non-code SE tasks.", "method": "The authors introduce SELU, a comprehensive benchmark suite with 17 non-code SE tasks covering classification, regression, NER, and MLM. Data is collected from various software artifacts. They fine-tune 22 open-source LLMs, prompt 2 proprietary models, train 2 baselines, and use multiple evaluation metrics. They compare performances with statistical tests.", "result": "Moderate-scale decoder-only models consistently perform best across non-code SE tasks, showing both high average results and little variation across tasks. Domain adaptation (code-centric pre-training) yields only modest gains.", "conclusion": "The findings recommend moderate-scale decoder-only models for non-code SE workflows, suggest that code pre-training isn't always crucial, and propose expanding the SELU benchmark for more generative and design-oriented future tasks."}}
{"id": "2506.10869", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10869", "abs": "https://arxiv.org/abs/2506.10869", "authors": ["Quinn Thibeault", "Giulia Pedrielli"], "title": "MultiCoSim: A Python-based Multi-Fidelity Co-Simulation Framework", "comment": null, "summary": "Simulation is a foundational tool for the analysis and testing of\ncyber-physical systems (CPS), underpinning activities such as algorithm\ndevelopment, runtime monitoring, and system verification. As CPS grow in\ncomplexity and scale, particularly in safety-critical and learning-enabled\nsettings, accurate analysis and synthesis increasingly rely on the rapid use of\nsimulation experiments. Because CPS inherently integrate hardware, software,\nand physical processes, simulation platforms must support co-simulation of\nheterogeneous components at varying levels of fidelity. Despite recent advances\nin high-fidelity modeling of hardware, firmware, and physics, co-simulation in\ndiverse environments remains challenging. These limitations hinder the\ndevelopment of reusable benchmarks and impede the use of simulation for\nautomated and comparative evaluation.\n  Existing simulation tools often rely on rigid configurations, lack automation\nsupport, and present obstacles to portability and modularity. Many are\nconfigured through static text files or impose constraints on how simulation\ncomponents are represented and connected, making it difficult to flexibly\ncompose systems or integrate components across platforms.\n  To address these challenges, we introduce MultiCoSim, a Python-based\nsimulation framework that enables users to define, compose, and configure\nsimulation components programmatically. MultiCoSim supports distributed,\ncomponent-based co-simulation and allows seamless substitution and\nreconfiguration of components. We demonstrate the flexibility of MultiCoSim\nthrough case studies that include co-simulations involving custom\nautomaton-based controllers, as well as integration with off-the-shelf\nplatforms like the PX4 autopilot for aerial robotics. These examples highlight\nMultiCoSim's capability to streamline CPS simulation pipelines for research and\ndevelopment.", "AI": {"tldr": "MultiCoSim is a new Python-based framework that makes cyber-physical system co-simulation more flexible, automated, and modular, addressing key limitations of prior tools and enabling easier integration and evaluation of diverse CPS components.", "motivation": "As cyber-physical systems (CPS) become more complex and incorporate diverse hardware, software, and physical processes, existing simulation tools struggle with co-simulation, modularity, and automation. This limits the development of reusable benchmarks and impedes comprehensive evaluation, especially in safety-critical and learning-enabled domains.", "method": "The authors present MultiCoSim, a Python-based simulation framework that enables programmatic definition, composition, and configuration of simulation components. MultiCoSim supports distributed, component-based co-simulation and allows easy substitution and reconfiguration of components. Case studies showcase its flexibility through integration with custom controllers and established platforms like PX4 for aerial robotics.", "result": "MultiCoSim simplifies the process of composing and managing simulations involving heterogeneous CPS components. The authors successfully demonstrate, via real-world case studies, that MultiCoSim streamlines co-simulation tasks and supports integration with both custom components and off-the-shelf solutions.", "conclusion": "MultiCoSim overcomes key limitations of prior simulation tools, enabling greater automation, flexibility, and modularity in CPS co-simulation. This supports more efficient and reusable simulation pipelines for research, development, and evaluation in increasingly complex CPS environments."}}
{"id": "2506.10954", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10954", "abs": "https://arxiv.org/abs/2506.10954", "authors": ["Lianghong Guo", "Yanlin Wang", "Caihua Li", "Pengyu Yang", "Jiachi Chen", "Wei Tao", "Yingtian Zou", "Duyu Tang", "Zibin Zheng"], "title": "SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks", "comment": null, "summary": "Constructing large-scale datasets for the GitHub issue resolution task is\ncrucial for both training and evaluating the software engineering capabilities\nof Large Language Models (LLMs). However, the traditional process for creating\nsuch benchmarks is notoriously challenging and labor-intensive, particularly in\nthe stages of setting up evaluation environments, grading test outcomes, and\nvalidating task instances. In this paper, we propose SWE-Factory, an automated\npipeline designed to address these challenges. To tackle these issues, our\npipeline integrates three core automated components. First, we introduce\nSWE-Builder, a multi-agent system that automates evaluation environment\nconstruction, which employs four specialized agents that work in a\ncollaborative, iterative loop and leverages an environment memory pool to\nenhance efficiency. Second, we introduce a standardized, exit-code-based\ngrading method that eliminates the need for manually writing custom parsers.\nFinally, we automate the fail2pass validation process using these reliable exit\ncode signals. Experiments on 671 issues across four programming languages show\nthat our pipeline can effectively construct valid task instances; for example,\nwith GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at $0.045 per\ninstance, while with Gemini-2.5-flash, it achieves comparable performance at\nthe lowest cost of $0.024 per instance. We also demonstrate that our\nexit-code-based grading achieves 100% accuracy compared to manual inspection,\nand our automated fail2pass validation reaches a precision of 0.92 and a recall\nof 1.00. We hope our automated pipeline will accelerate the collection of\nlarge-scale, high-quality GitHub issue resolution datasets for both training\nand evaluation. Our code and datasets are released at\nhttps://github.com/DeepSoftwareAnalytics/swe-factory.", "AI": {"tldr": "SWE-Factory is an automated pipeline that efficiently builds large-scale, accurate datasets for GitHub issue resolution, drastically reducing manual effort and cost. Its grading and validation are highly reliable, enabling rapid dataset creation for software engineering LLMs.", "motivation": "Constructing large-scale, high-quality datasets for the GitHub issue resolution task is essential for training and evaluating the software engineering capabilities of LLMs. However, traditional methods are labor-intensive and challenging, especially in setting up environments, grading, and task validation.", "method": "The paper introduces SWE-Factory, an automated pipeline composed of three core components: (1) SWE-Builder, a multi-agent system for automating the evaluation environment setup, (2) a standard exit-code-based grading method instead of manual custom parsers, and (3) automated fail2pass validation using exit code signals. The pipeline was tested on 671 issues across four programming languages, comparing LLMs like GPT-4.1-mini and Gemini-2.5-flash.", "result": "SWE-Factory constructed 269 valid task instances with GPT-4.1-mini at $0.045 each and achieved similar results with Gemini-2.5-flash at $0.024 each. The exit-code-based grading method achieved 100% accuracy compared to manual inspection, and the automated fail2pass validation showed a precision of 0.92 and a recall of 1.00.", "conclusion": "The proposed automated pipeline significantly streamlines the creation of large-scale GitHub issue resolution datasets with high accuracy and reduced cost, facilitating robust training and evaluation for LLMs in software engineering. The code and data are publicly available."}}
{"id": "2506.10021", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10021", "abs": "https://arxiv.org/abs/2506.10021", "authors": ["Jordi de la Torre"], "title": "From Tool Calling to Symbolic Thinking: LLMs in a Persistent Lisp Metaprogramming Loop", "comment": null, "summary": "We propose a novel architecture for integrating large language models (LLMs)\nwith a persistent, interactive Lisp environment. This setup enables LLMs to\ndefine, invoke, and evolve their own tools through programmatic interaction\nwith a live REPL. By embedding Lisp expressions within generation and\nintercepting them via a middleware layer, the system allows for stateful\nexternal memory, reflective programming, and dynamic tool creation. We present\na design framework and architectural principles to guide future implementations\nof interactive AI systems that integrate symbolic programming with neural\nlanguage generation.", "AI": {"tldr": "The paper proposes a new way to combine LLMs with live Lisp environments, so LLMs can write, use, and improve their own tools in real time, advancing the integration of programmatic and neural AI.", "motivation": "Current LLMs lack the ability to dynamically create, use, and modify tools in a persistent, programmable environment. Integrating symbolic programming with neural generation remains an open challenge.", "method": "A new architecture integrates LLMs with a live, interactive Lisp REPL. The system embeds Lisp code in LLM outputs and intercepts these calls with a middleware layer, enabling stateful, programmatic interaction and dynamic tool creation.", "result": "Demonstrated architecture supports stateful external memory, reflective programming, and allows LLMs to create and evolve tools interactively. Presents a framework and design principles for future systems that blend symbolic and neural AI.", "conclusion": "Integrating LLMs with a persistent Lisp environment enables more powerful, self-improving, and interactive AI systems through dynamic tool use and creation, providing a pathway for advanced symbolic-neural integration."}}
{"id": "2506.10026", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2506.10026", "abs": "https://arxiv.org/abs/2506.10026", "authors": ["Tesla Zhang", "Sonya Simkin", "Rui Li", "Yue Yao", "Stephanie Balzer"], "title": "A Language-Agnostic Logical Relation for Message-Passing Protocols", "comment": "19 pages, 8 figures", "summary": "Today's computing landscape has been gradually shifting to applications\ntargeting distributed and *heterogeneous* systems, such as cloud computing and\nInternet of Things (IoT) applications. These applications are predominantly\n*concurrent*, employ *message-passing*, and interface with *foreign objects*,\nranging from externally implemented code to actual physical devices such as\nsensors. Verifying that the resulting systems adhere to the intended protocol\nof interaction is challenging -- the usual assumption of a common\nimplementation language, let alone a type system, no longer applies, ruling out\nany verification method based on them. This paper develops a framework for\ncertifying *protocol compliance* of heterogeneous message-passing systems. It\ncontributes the first mechanization of a *language-agnostic logical relation*,\nasserting that its inhabitants comply with the protocol specified. This\ndefinition relies entirely on a labelled transition-based semantics,\naccommodating arbitrary inhabitants, typed and untyped alike, including foreign\nobjects. As a case study, the paper considers two scenarios: (1) *per-instance\nverification* of a specific application or hardware device, and (2)\n*once-and-for-all verification* of well-typed applications for a given type\nsystem. The logical relation and both scenarios are mechanized in the Coq\ntheorem prover.", "AI": {"tldr": "This paper presents a new framework for checking protocol compliance in distributed, heterogeneous systems, regardless of programming language or typing. The method, mechanized in Coq, enables formal verification even when systems are composed of mixed, foreign, or untyped objects.", "motivation": "The motivation is to address the challenges of verifying protocol compliance in distributed and heterogeneous systems, such as cloud computing and IoT, where components often use differing languages and may interact with external objects.", "method": "The paper creates a framework for protocol compliance certification, introducing a language-agnostic logical relation based on labelled transition semantics that works for both typed and untyped components, including foreign objects. The framework and its scenarios are implemented in the Coq theorem prover.", "result": "The authors successfully mechanize their logical relation in Coq and demonstrate its utility in two scenarios: verifying individual (possibly hardware) instances, and general well-typed applications under a specific type system.", "conclusion": "The paper establishes the feasibility of language-agnostic protocol verification in heterogeneous message-passing systems using logical relations and formalizes this in Coq, paving the way for reliable verification across diverse environments."}}
{"id": "2506.10781", "categories": ["cs.PL", "68N15, 68U35", "D.3.0; K.3.2"], "pdf": "https://arxiv.org/pdf/2506.10781", "abs": "https://arxiv.org/abs/2506.10781", "authors": ["Zhiyao Zhong", "Cyrus Omar"], "title": "Hazel Deriver: A Live Editor for Constructing Rule-Based Derivations", "comment": "5 pages, 2 figures, includes a preliminary user study; intended for\n  computer science education and PL/HCI conference audiences", "summary": "Students in programming languages and formal logic courses often struggle\nwith constructing rule-based derivation trees due to the complexity of applying\ninference rules, the lack of immediate feedback, and the manual effort required\nfor handwritten proofs. We present Hazel Deriver, a live, web-based editor\ndesigned to scaffold derivation construction through multiple layers of\nsupport. Built on the Hazel live programming environment, it provides a\nstructured, interactive experience that encourages iterative exploration and\nreal-time feedback. A preliminary user study with former students suggests that\nHazel Deriver reduces the perceived difficulty of derivation tasks while\nimproving conceptual understanding and engagement. We discuss the design of its\nlayered scaffolding features and raise questions about balancing system\nguidance with learner autonomy.", "AI": {"tldr": "Hazel Deriver is a web-based tool that helps students create rule-based derivations more easily by providing interactive, layered support and feedback, leading to better understanding and engagement in a user study.", "motivation": "Students find it difficult to construct rule-based derivation trees in programming languages and formal logic courses due to the complexity of inference rules, lack of immediate feedback, and challenges with manual, handwritten proofs.", "method": "The authors developed Hazel Deriver, a live, web-based editor built on the Hazel environment. It provides structured, interactive, scaffolded support for constructing derivations, with real-time feedback and iterative exploration.", "result": "A user study with former students indicated that Hazel Deriver reduced the perceived difficulty of derivation tasks, increased conceptual understanding, and improved engagement.", "conclusion": "Hazel Deriver is effective for scaffolding derivation tasks in programming and logic courses, but balancing automated guidance with learner autonomy remains an open question."}}
{"id": "2506.10913", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2506.10913", "abs": "https://arxiv.org/abs/2506.10913", "authors": ["Ashley Samuelson", "Andrew K. Hirsch", "Ethan Cecchetti"], "title": "Choreographic Quick Changes: First-Class Location (Set) Polymorphism", "comment": "In submission to OOPSLA 2025", "summary": "Choreographic programming is a promising new paradigm for programming\nconcurrent systems where a developer writes a single centralized program that\ncompiles to individual programs for each node. Existing choreographic\nlanguages, however, lack critical features integral to modern systems, like the\nability of one node to dynamically compute who should perform a computation and\nsend that decision to others. This work addresses this gap with $\\lambda_{QC}$,\nthe first typed choreographic language with \\emph{first class process names}\nand polymorphism over both types and (sets of) locations. $\\lambda_{QC}$ also\nimproves expressive power over previous work by supporting algebraic and\nrecursive data types as well as multiply-located values. We formalize and\nmechanically verify our results in Rocq, including the standard choreographic\nguarantee of deadlock freedom.", "AI": {"tldr": "The paper introduces $\\lambda_{QC}$, a choreographic language with advanced features like first-class process names and polymorphism, addresses major limitations in prior languages for concurrent systems, and proves its properties (like deadlock freedom) with formal mechanical verification.", "motivation": "Current choreographic programming languages lack features such as dynamic determination of computational responsibility (i.e., letting a node decide who should compute and communicating that decision), which are important for modern concurrent systems.", "method": "The authors present $\\lambda_{QC}$, a new typed choreographic language that introduces first-class process names, polymorphism over types and over sets of locations, supports algebraic and recursive data types, and multiply-located values. They formalize and mechanically verify their results using Rocq.", "result": "$\\lambda_{QC}$ extends the expressive power of choreographic programming languages, accommodates critical modern concurrency features, and maintains deadlock freedom, which is mechanically verified in Rocq.", "conclusion": "$\\lambda_{QC}$ is the first choreographic language to offer both first-class process names and advanced polymorphism, thus bridging gaps in expressiveness and practical usability for modern concurrent systems."}}
{"id": "2506.10043", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10043", "abs": "https://arxiv.org/abs/2506.10043", "authors": ["Yongqian Sun", "Yu Luo", "Xidao Wen", "Yuan Yuan", "Xiaohui Nie", "Shenglin Zhang", "Tong Liu", "Xi Luo"], "title": "TrioXpert: An automated incident management framework for microservice system", "comment": null, "summary": "Automated incident management plays a pivotal role in large-scale\nmicroservice systems. However, many existing methods rely solely on\nsingle-modal data (e.g., metrics, logs, and traces) and struggle to\nsimultaneously address multiple downstream tasks, including anomaly detection\n(AD), failure triage (FT), and root cause localization (RCL). Moreover, the\nlack of clear reasoning evidence in current techniques often leads to\ninsufficient interpretability. To address these limitations, we propose\nTrioXpert, an end-to-end incident management framework capable of fully\nleveraging multimodal data. TrioXpert designs three independent data processing\npipelines based on the inherent characteristics of different modalities,\ncomprehensively characterizing the operational status of microservice systems\nfrom both numerical and textual dimensions. It employs a collaborative\nreasoning mechanism using large language models (LLMs) to simultaneously handle\nmultiple tasks while providing clear reasoning evidence to ensure strong\ninterpretability. We conducted extensive evaluations on two popular\nmicroservice system datasets, and the experimental results demonstrate that\nTrioXpert achieves outstanding performance in AD (improving by 4.7% to 57.7%),\nFT (improving by 2.1% to 40.6%), and RCL (improving by 1.6% to 163.1%) tasks.", "AI": {"tldr": "TrioXpert is a new framework for incident management in microservices that uses multimodal data and large language models. It outperforms existing methods in anomaly detection, failure triage, and root cause localization, while also offering better interpretability.", "motivation": "Current automated incident management methods in microservice systems usually use only single-modal data (like metrics, logs, or traces), making it hard to handle multiple tasks (anomaly detection, failure triage, and root cause localization) effectively. Additionally, these approaches often lack interpretability due to unclear reasoning evidence.", "method": "TrioXpert, the proposed framework, is end-to-end and utilizes multimodal data. It features three separate data pipelines, each tailored to the characteristics of different data modalities. The approach integrates a collaborative reasoning mechanism powered by large language models, allowing it to manage multiple tasks concurrently while delivering clear and interpretable reasoning evidence.", "result": "Extensive experiments on two popular microservice datasets show that TrioXpert significantly outperforms existing approaches in all evaluated tasks, with improvements ranging from 4.7% to 57.7% for anomaly detection, 2.1% to 40.6% for failure triage, and 1.6% to 163.1% for root cause localization.", "conclusion": "TrioXpert is effective for automated incident management in large-scale microservice systems, achieving superior performance across multiple tasks and improving interpretability by leveraging multimodal data and collaborative reasoning."}}
{"id": "2506.10056", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2506.10056", "abs": "https://arxiv.org/abs/2506.10056", "authors": ["Gabriel Orlanski", "Nicholas Roberts", "Aws Albarghouthi", "Frederic Sala"], "title": "Reward Models Enable Scalable Code Verification by Trading Accuracy for Throughput", "comment": "29 pages, 6 figures, code released here:\n  https://github.com/SprocketLab/orm-code-verifier", "summary": "The standard paradigm for solving coding tasks via large language models\n(LLMs) is to generate-then-rank programs, where the latter step uses a verifier\nin the ranking process. The growing consensus is that a comprehensive verifier\n(e.g., a full test suite) should be prioritized over an outcome reward model\n(ORM) whenever possible, with little consideration given to the trade-offs\ninvolved. We aim to challenge this assumption by systematically exploring the\ntradeoff between speed and accuracy. We find that ORMs play a crucial role in\nscaling verification through trading accuracy for speed, even when a\ncomprehensive verifier is available. Their value becomes especially apparent\nwhen used in a generate-prune-then-rank approach, where a faster but less\naccurate verifier removes incorrect solutions prior to ranking -- leading to a\nsystem that is 11.65x faster while only being 8.33% less accurate than the full\ntest suite. We analyze the generate-prune-then-rank approach and show that it\nworks by filtering out incorrect but highly ranked solutions. These findings\nenable the design of scalable and accurate program ranking systems.", "AI": {"tldr": "The paper demonstrates that using outcome reward models in combination with traditional verifiers\u2014specifically, through a generate-prune-then-rank approach\u2014substantially increases speed while keeping accuracy loss minimal, suggesting a re-evaluation of current program ranking paradigms for LLM-based coding tasks.", "motivation": "The paper is motivated by the current practice of using comprehensive verifiers (such as full test suites) over outcome reward models (ORMs) in program synthesis via LLMs, with little attention to the trade-offs involving speed and accuracy.", "method": "The authors systematically explore the trade-off between speed and accuracy by comparing different ranking approaches, specifically focusing on the generate-prune-then-rank method, which employs a faster but less accurate verifier in the pruning step before final ranking.", "result": "The generate-prune-then-rank approach achieves a significant speedup (11.65 times faster) while only sacrificing a modest amount of accuracy (8.33% less than the full verifier), demonstrating that ORMs are valuable even when comprehensive verifiers are available.", "conclusion": "ORMs can effectively scale the verification process by offering a compelling trade-off between speed and accuracy, thus enabling the design of more scalable and still accurate program ranking systems."}}
{"id": "2506.10049", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10049", "abs": "https://arxiv.org/abs/2506.10049", "authors": ["Francesco Vinci", "Gyunam Park", "Wil van der Aalst", "Massimiliano de Leoni"], "title": "Online Discovery of Simulation Models for Evolving Business Processes (Extended Version)", "comment": null, "summary": "Business Process Simulation (BPS) refers to techniques designed to replicate\nthe dynamic behavior of a business process. Many approaches have been proposed\nto automatically discover simulation models from historical event logs,\nreducing the cost and time to manually design them. However, in dynamic\nbusiness environments, organizations continuously refine their processes to\nenhance efficiency, reduce costs, and improve customer satisfaction. Existing\ntechniques to process simulation discovery lack adaptability to real-time\noperational changes. In this paper, we propose a streaming process simulation\ndiscovery technique that integrates Incremental Process Discovery with Online\nMachine Learning methods. This technique prioritizes recent data while\npreserving historical information, ensuring adaptation to evolving process\ndynamics. Experiments conducted on four different event logs demonstrate the\nimportance in simulation of giving more weight to recent data while retaining\nhistorical knowledge. Our technique not only produces more stable simulations\nbut also exhibits robustness in handling concept drift, as highlighted in one\nof the use cases.", "AI": {"tldr": "The paper introduces a streaming simulation discovery method that uses online machine learning and incremental process discovery, enabling business process simulations to better adapt to real-time changes and handle evolving process dynamics.", "motivation": "Existing business process simulation discovery techniques are not adaptive to real-time operational changes, which is a problem since organizations constantly update and refine their processes.", "method": "The paper proposes a streaming process simulation discovery technique that combines Incremental Process Discovery with Online Machine Learning. This method gives priority to recent data while retaining historical knowledge, allowing the simulation to adapt as the business process evolves.", "result": "Experiments on four different event logs show that the proposed technique leads to more stable simulations and better handles concept drift compared to existing methods. It achieves a balance between adapting to new data and not forgetting historical patterns.", "conclusion": "The proposed streaming process simulation technique improves adaptability and robustness in dynamic business environments by considering both recent and historical data, making simulations more reliable and effective under changing conditions."}}
{"id": "2506.10803", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2506.10803", "abs": "https://arxiv.org/abs/2506.10803", "authors": ["Ryan Gibb", "Patrick Ferris", "David Allsopp", "Michael Winston Dales", "Mark Elvers", "Thomas Gazagnaire", "Sadiq Jaffer", "Thomas Leonard", "Jon Ludlam", "Anil Madhavapeddy"], "title": "Solving Package Management via Hypergraph Dependency Resolution", "comment": "Submitted to SPLASH 2025", "summary": "Package managers are everywhere, with seemingly every language and operating\nsystem implementing their own solution. The lack of interoperability between\nthese systems means that multi-lingual projects are unable to express precise\ndependencies across language ecosystems, and external system and hardware\ndependencies are typically implicit and unversioned. We define HyperRes, a\nformal system for describing versioned dependency resolution using a hypergraph\nthat is expressive enough to model many ecosystems and solve dependency\nconstraints across them. We define translations from dozens of existing package\nmanagers to HyperRes and comprehensively demonstrate that dependency resolution\ncan work across ecosystems that are currently distinct. This does not require\nusers to shift their choice of package managers; instead, HyperRes allows for\nthe translation of packaging metadata between ecosystems, and for solving to be\nprecisely specialised to a particular deployment environment.", "AI": {"tldr": "The paper introduces HyperRes, a formal system that enables cross-ecosystem package dependency resolution by translating metadata among diverse package managers using hypergraph models, improving interoperability for complex multi-language projects.", "motivation": "There are many package managers, each tailored to specific languages and systems, resulting in poor interoperability. Projects spanning multiple languages cannot clearly define cross-ecosystem dependencies, and external requirements are often not versioned or explicitly stated.", "method": "The authors propose HyperRes, a formal system using hypergraphs to model versioned dependency resolution across multiple ecosystems. They develop translations from many popular package managers to this system.", "result": "HyperRes enables dependency resolution to work across currently distinct ecosystems, allowing translation of packaging metadata and cross-ecosystem dependency management without forcing users to abandon their existing tools.", "conclusion": "HyperRes provides a unifying approach to multi-language and cross-ecosystem dependency resolution, making it possible to manage dependencies more precisely and interoperably without requiring significant changes to existing workflows."}}
{"id": "2506.10051", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10051", "abs": "https://arxiv.org/abs/2506.10051", "authors": ["Md Istiak Hossain Shihab", "Christopher Hundhausen", "Ahsun Tariq", "Summit Haque", "Yunhan Qiao", "Brian Mulanda"], "title": "The Effects of GitHub Copilot on Computing Students' Programming Effectiveness, Efficiency, and Processes in Brownfield Programming Tasks", "comment": "14 pages, 5 figures", "summary": "When graduates of computing degree programs enter the software industry, they\nwill most likely join teams working on legacy code bases developed by people\nother than themselves. In these so-called brownfield software development\nsettings, generative artificial intelligence (GenAI) coding assistants like\nGitHub Copilot are rapidly transforming software development practices, yet the\nimpact of GenAI on student programmers performing brownfield development tasks\nremains underexplored. This paper investigates how GitHub Copilot influences\nundergraduate students' programming performance, behaviors, and understanding\nwhen completing brownfield programming tasks in which they add new code to an\nunfamiliar code base. We conducted a controlled experiment in which 10\nundergraduate computer science students completed highly similar brownfield\ndevelopment tasks with and without Copilot in a legacy web application. Using a\nmixed-methods approach combining performance analysis, behavioral analysis, and\nexit interviews, we found that students completed tasks 35% faster (p < 0.05)\nand made 50% more solution progress p (< 0.05) when using Copilot. Moreover,\nour analysis revealed that, when using Copilot, students spent 11% less time\nmanually writing code (p < 0.05), and 12% less time conducting web searches (p\n< 0.05), providing evidence of a fundamental shift in how they engaged in\nprogramming. In exit interviews, students reported concerns about not\nunderstanding how or why Copilot suggestions work. This research suggests the\nneed for computing educators to develop new pedagogical approaches that\nleverage GenAI assistants' benefits while fostering reflection on how and why\nGenAI suggestions address brownfield programming tasks. Complete study results\nand analysis are presented at https://ghcopilot-icer.github.io/.", "AI": {"tldr": "GitHub Copilot helps students complete brownfield coding tasks more quickly and efficiently, but may compromise their understanding of the code. Educators need to adapt teaching methods to harness GenAI benefits while ensuring students grasp why and how code solutions work.", "motivation": "The motivation behind this paper is to address the gap in understanding how generative AI coding assistants, like GitHub Copilot, impact undergraduate students working on brownfield software development tasks, which are common in the software industry and involve modifying existing legacy code.", "method": "The authors conducted a controlled experiment with 10 undergraduate computer science students, having them perform similar brownfield development tasks on a legacy web application both with and without GitHub Copilot. A mixed-methods approach was used, combining quantitative performance and behavioral analysis with qualitative exit interviews.", "result": "Students using Copilot completed tasks 35% faster and made 50% more progress compared to not using Copilot. They also spent 11% less time manually coding and 12% less time on web searches. However, students expressed concerns in interviews about not fully understanding the suggestions Copilot provided.", "conclusion": "GitHub Copilot significantly improves student performance and efficiency in brownfield programming tasks, but it may reduce their understanding of code. The findings highlight an urgent need for educators to create teaching strategies that balance GenAI's benefits with a focus on comprehension and critical thinking."}}
{"id": "2506.10204", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10204", "abs": "https://arxiv.org/abs/2506.10204", "authors": ["Andrei Paleyes", "Radzim Sendyka", "Diana Robinson", "Christian Cabrera", "Neil D. Lawrence"], "title": "Prompt Variability Effects On LLM Code Generation", "comment": null, "summary": "Code generation is one of the most active areas of application of Large\nLanguage Models (LLMs). While LLMs lower barriers to writing code and\naccelerate development process, the overall quality of generated programs\ndepends on the quality of given prompts. Specifically, functionality and\nquality of generated code can be sensitive to user's background and familiarity\nwith software development. It is therefore important to quantify LLM's\nsensitivity to variations in the input. To this end we propose a synthetic\nevaluation pipeline for code generation with LLMs, as well as a systematic\npersona-based evaluation approach to expose qualitative differences of LLM\nresponses dependent on prospective user background. Both proposed methods are\ncompletely independent from specific programming tasks and LLMs, and thus are\nwidely applicable. We provide experimental evidence illustrating utility of our\nmethods and share our code for the benefit of the community.", "AI": {"tldr": "The paper presents new evaluation techniques for code-generating LLMs that test their sensitivity to input and user background, with experimental validation and open-source code for community adoption.", "motivation": "While Large Language Models (LLMs) have advanced code generation, the effectiveness of generated code varies based on the quality of prompts, particularly considering the user's programming background.", "method": "The paper proposes a synthetic evaluation pipeline for code generation with LLMs and introduces a systematic persona-based evaluation approach. These methods assess how LLM-generated code quality varies depending on user background and input changes, and are designed to be independent of specific programming tasks or LLM models.", "result": "Experimental results show that their evaluation methods can effectively highlight qualitative differences in LLM responses based on varying user personas. The authors also release their code for community use.", "conclusion": "The proposed synthetic and persona-based evaluation methods provide robust, widely applicable tools for assessing and understanding the sensitivity of LLM-generated code to input and user background differences."}}
{"id": "2506.10280", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.10280", "abs": "https://arxiv.org/abs/2506.10280", "authors": ["Samiha Shimmi", "Hamed Okhravi", "Mona Rahimi"], "title": "AI-Based Software Vulnerability Detection: A Systematic Literature Review", "comment": null, "summary": "Software vulnerabilities in source code pose serious cybersecurity risks,\nprompting a shift from traditional detection methods (e.g., static analysis,\nrule-based matching) to AI-driven approaches. This study presents a systematic\nreview of software vulnerability detection (SVD) research from 2018 to 2023,\noffering a comprehensive taxonomy of techniques, feature representations, and\nembedding methods. Our analysis reveals that 91% of studies use AI-based\nmethods, with graph-based models being the most prevalent. We identify key\nlimitations, including dataset quality, reproducibility, and interpretability,\nand highlight emerging opportunities in underexplored techniques such as\nfederated learning and quantum neural networks, providing a roadmap for future\nresearch.", "AI": {"tldr": "A detailed review of 2018-2023 software vulnerability detection research shows that AI, especially graph-based models, dominates, but the field still faces reproducibility and dataset issues. The paper outlines future research possibilities, including federated learning and quantum neural networks.", "motivation": "As software vulnerabilities are a major cybersecurity threat and traditional detection methods are limited, there is a need to comprehensively analyze recent advances in AI-driven vulnerability detection.", "method": "This paper conducts a systematic review of literature on software vulnerability detection from 2018 to 2023, classifying methods, feature representations, and embedding strategies.", "result": "91% of studies reviewed employ AI methods, with graph-based models most common. The study also identifies limitations in current research and suggests new research opportunities.", "conclusion": "Most current research on software vulnerability detection utilizes AI-based methods, especially graph-based models. However, the field faces challenges regarding dataset quality, reproducibility, and interpretability. New techniques like federated learning and quantum neural networks offer promising directions."}}
{"id": "2506.10322", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10322", "abs": "https://arxiv.org/abs/2506.10322", "authors": ["Xueying Du", "Kai Yu", "Chong Wang", "Yi Zou", "Wentai Deng", "Zuoyu Ou", "Xin Peng", "Lingming Zhang", "Yiling Lou"], "title": "Minimizing False Positives in Static Bug Detection via LLM-Enhanced Path Feasibility Analysis", "comment": null, "summary": "Static bug analyzers play a crucial role in ensuring software quality.\nHowever, existing analyzers for bug detection in large codebases often suffer\nfrom high false positive rates. This is primarily due to the limited\ncapabilities of analyzers in path feasibility validation with multiple\nconditional branches and complex data dependencies. While current LLM-based\napproaches attempt to address this issue, their effectiveness remains limited\ndue to insufficient constraint cascade analysis and scalability challenges in\nlarge projects. To address this challenge, we propose an iterative path\nfeasibility analysis framework LLM4PFA. By leveraging LLM agent based targeted\nconstraint reasoning, and key context-aware analysis driven by agent planning,\nLLM4PFA effectively enhances complex inter-procedural path feasibility analysis\nfor minimizing false positives in static bug detection. Evaluation results show\nthat LLM4PFA precisely filters out 72% to 96% false positives reported during\nstatic bug detection, significantly outperforming all the baselines by 41.1% -\n105.7% improvements; meanwhile LLM4PFA only misses 3 real bugs of 45 true\npositives.", "AI": {"tldr": "LLM4PFA, an LLM agent-based path feasibility analysis framework, dramatically reduces false positives in static bug detection for large codebases, outperforming previous methods by a wide margin with minimal loss of true positives.", "motivation": "Existing static bug analyzers for large codebases suffer from high false positive rates due to limited capabilities in validating path feasibility, especially with complex branches and data dependencies. LLM-based approaches have not sufficiently solved these issues due to weaknesses in constraint analysis and scalability.", "method": "The paper proposes an iterative path feasibility analysis framework called LLM4PFA, which uses LLM agent-based targeted constraint reasoning and context-aware analysis driven by agent planning to enhance inter-procedural path feasibility evaluation.", "result": "LLM4PFA filters out 72% to 96% of false positives in static bug detection, outperforming all baselines by 41.1% to 105.7% improvement rates, and only misses 3 real bugs out of 45 true positives.", "conclusion": "The LLM4PFA framework significantly reduces false positives in static bug detection while maintaining high true positive rates, making it more effective than current methods."}}
{"id": "2506.10330", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10330", "abs": "https://arxiv.org/abs/2506.10330", "authors": ["Seyed Moein Abtahi", "Akramul Azim"], "title": "Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements", "comment": "Accepted at FORGE 2025", "summary": "This study examined code issue detection and revision automation by\nintegrating Large Language Models (LLMs) such as OpenAI's GPT-3.5 Turbo and\nGPT-4o into software development workflows. A static code analysis framework\ndetects issues such as bugs, vulnerabilities, and code smells within a\nlarge-scale software project. Detailed information on each issue was extracted\nand organized to facilitate automated code revision using LLMs. An iterative\nprompt engineering process is applied to ensure that prompts are structured to\nproduce accurate and organized outputs aligned with the project requirements.\nRetrieval-augmented generation (RAG) is implemented to enhance the relevance\nand precision of the revisions, enabling LLM to access and integrate real-time\nexternal knowledge. The issue of LLM hallucinations - where the model generates\nplausible but incorrect outputs - is addressed by a custom-built \"Code\nComparison App,\" which identifies and corrects erroneous changes before\napplying them to the codebase. Subsequent scans using the static code analysis\nframework revealed a significant reduction in code issues, demonstrating the\neffectiveness of combining LLMs, static analysis, and RAG to improve code\nquality, streamline the software development process, and reduce time and\nresource expenditure.", "AI": {"tldr": "The paper presents an automated approach to detect and correct code problems in large software projects using LLMs, static analysis, retrieval-augmented generation, and post-processing checks. This method significantly improves code quality and streamlines development.", "motivation": "Code issue detection and correction are essential in software development but remain labor-intensive and error-prone. Automating these processes using advanced AI like large language models (LLMs) could greatly improve efficiency and quality while lowering costs.", "method": "The study integrates LLMs (such as GPT-3.5 Turbo and GPT-4o) into a software workflow by using static code analysis to detect issues. Detected issues are organized and revised automatically by LLMs, with iterative prompt engineering to optimize LLM responses. Retrieval-augmented generation (RAG) is added to supply relevant real-time knowledge. A custom Code Comparison App reviews and corrects LLM-generated code changes to mitigate errors before codebase integration. Results are validated by rescanning with the original static analysis tools.", "result": "Combining LLMs, static code analysis, RAG, and post-processing significantly reduces the number of code issues in large software projects and increases the efficiency and reliability of the revision process.", "conclusion": "Integrating LLMs with static code analysis and retrieval-augmented generation, alongside corrective code comparison tools, can effectively automate detecting and fixing code issues, thus improving software quality and reducing development resources."}}
{"id": "2506.10365", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10365", "abs": "https://arxiv.org/abs/2506.10365", "authors": ["Shuyang Hou", "Zhangxiao Shen", "Huayi Wu", "Haoyue Jiao", "Ziqi Liu", "Lutong Xie", "Chang Liu", "Jianyuan Liang", "Yaxian Qing", "Xiaopu Zhang", "Dehua Peng", "Zhipeng Gui", "Xuefeng Guan"], "title": "AutoGEEval++: A Multi-Level and Multi-Geospatial-Modality Automated Evaluation Framework for Large Language Models in Geospatial Code Generation on Google Earth Engine", "comment": null, "summary": "Geospatial code generation is becoming a key frontier in integrating\nartificial intelligence with geo-scientific analysis, yet standardised\nautomated evaluation tools for this task remain absent. This study presents\nAutoGEEval++, an enhanced framework building on AutoGEEval, and the first\nautomated assessment system for large language models (LLMs) generating\ngeospatial code on Google Earth Engine (GEE). It supports diverse data\nmodalities and varying task complexities. Built on the GEE Python API,\nAutoGEEval++ features a benchmark dataset-AutoGEEval++-Bench-with 6,365 test\ncases across 26 data types and three task categories: unit, combo, and theme\ntests. It includes a submission programme and a judge module to realise an\nend-to-end automated evaluation pipeline from code generation to\nexecution-based validation. The framework adopts multi-dimensional\nmetrics-accuracy, resource usage, run-time efficiency, and error\ntypes-balancing hallucination control and efficiency, and enabling boundary\ntesting and error pattern analysis. Using AutoGEEval++, we evaluate 24\nstate-of-the-art LLMs (as of June 2025), including general-purpose,\nreasoning-enhanced, code-centric, and geoscience-specific models. Results\nreveal clear performance, stability, and error differences across task types,\nmodel designs, and deployment settings, confirming AutoGEEval++'s practical\nvalue and scalability in vertical-domain code generation. This work establishes\nthe first standardised evaluation protocol and foundational benchmark for\nGEE-based LLM code generation, providing a unified basis for performance\ncomparison and a methodological framework for systematic, domain-specific code\nevaluation.", "AI": {"tldr": "AutoGEEval++ delivers the first automated and standardized framework for evaluating how large language models generate geospatial code on Google Earth Engine. Using a large benchmark and comprehensive metrics, it reveals performance differences across leading LLMs, setting a foundation for future development and comparison in this field.", "motivation": "There is a growing need for integrating AI into geospatial scientific analysis, especially through geospatial code generation. However, there is a lack of standardized and automated evaluation tools for assessing this integration, particularly regarding the performance of large language models (LLMs) in generating geospatial code on platforms like Google Earth Engine (GEE).", "method": "The authors introduce AutoGEEval++, an enhanced framework based on AutoGEEval, designed for automated assessment of LLMs generating geospatial code on GEE. The framework uses the GEE Python API and includes a comprehensive benchmark dataset (AutoGEEval++-Bench) with 6,365 test cases across 26 data types and three task categories (unit, combo, and theme). The evaluation pipeline comprises a code generation, submission, and judge module, and employs multi-dimensional metrics including accuracy, resource usage, run-time efficiency, and error type analysis.", "result": "AutoGEEval++ was used to evaluate 24 leading LLMs as of June 2025, representing diverse model types. The results showed differences in performance, stability, and error tendencies depending on task category, model design, and deployment context, validating the framework's value and scalability for code generation in the geospatial domain.", "conclusion": "AutoGEEval++ establishes the first standardized evaluation protocol and benchmark for GEE-based LLM code generation, enabling consistent performance comparison and providing a structured framework for systematic evaluation in domain-specific code generation."}}
{"id": "2506.10376", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.10376", "abs": "https://arxiv.org/abs/2506.10376", "authors": ["Fan Wu", "Cuiyun Gao", "Shuqing Li", "Xin-Cheng Wen", "Qing Liao"], "title": "MLLM-Based UI2Code Automation Guided by UI Layout Information", "comment": "Accepted by the 34th International Symposium on Software Testing and\n  Analysis (ISSTA 2025)", "summary": "Converting user interfaces into code (UI2Code) is a crucial step in website\ndevelopment, which is time-consuming and labor-intensive. The automation of\nUI2Code is essential to streamline this task, beneficial for improving the\ndevelopment efficiency. There exist deep learning-based methods for the task;\nhowever, they heavily rely on a large amount of labeled training data and\nstruggle with generalizing to real-world, unseen web page designs. The advent\nof Multimodal Large Language Models (MLLMs) presents potential for alleviating\nthe issue, but they are difficult to comprehend the complex layouts in UIs and\ngenerate the accurate code with layout preserved. To address these issues, we\npropose LayoutCoder, a novel MLLM-based framework generating UI code from\nreal-world webpage images, which includes three key modules: (1) Element\nRelation Construction, which aims at capturing UI layout by identifying and\ngrouping components with similar structures; (2) UI Layout Parsing, which aims\nat generating UI layout trees for guiding the subsequent code generation\nprocess; and (3) Layout-Guided Code Fusion, which aims at producing the\naccurate code with layout preserved. For evaluation, we build a new benchmark\ndataset which involves 350 real-world websites named Snap2Code, divided into\nseen and unseen parts for mitigating the data leakage issue, besides the\npopular dataset Design2Code. Extensive evaluation shows the superior\nperformance of LayoutCoder over the state-of-the-art approaches. Compared with\nthe best-performing baseline, LayoutCoder improves 10.14% in the BLEU score and\n3.95% in the CLIP score on average across all datasets.", "AI": {"tldr": "UI-to-code conversion is slow and existing methods don't generalize well. LayoutCoder is a new MLLM-based framework with specialized modules for layout preservation, and it achieves much better results than prior methods on benchmark datasets.", "motivation": "Converting user interfaces to code is a critical but time-consuming part of web development. Existing deep learning methods for automating this process require large labeled datasets and struggle to handle novel, real-world web designs. Multimodal Large Language Models (MLLMs) could improve automation but have difficulty understanding complex layouts and generating accurate code.", "method": "The authors introduce LayoutCoder, an MLLM-based framework for generating UI code from real-world webpage images. It has three core modules: (1) Element Relation Construction to capture layouts by grouping similar components, (2) UI Layout Parsing to build layout trees for guiding code generation, and (3) Layout-Guided Code Fusion to produce layout-preserving code. The team also built a new benchmark dataset, Snap2Code, and used Design2Code for evaluation.", "result": "LayoutCoder outperforms state-of-the-art approaches, increasing BLEU score by 10.14% and CLIP score by 3.95% on average across datasets compared to the best-performing baseline.", "conclusion": "LayoutCoder effectively addresses the challenges in UI2Code tasks for real-world webpages. Its innovative architecture and new benchmark dataset advance the automation of UI code generation."}}
{"id": "2506.10397", "categories": ["cs.SE", "cs.CY", "cs.DC"], "pdf": "https://arxiv.org/pdf/2506.10397", "abs": "https://arxiv.org/abs/2506.10397", "authors": ["Mir Mohammad Yousuf", "Shabir Ahmad Sofi"], "title": "Bug Classification in Quantum Software: A Rule-Based Framework and Its Evaluation", "comment": "25 pages, 5 figures", "summary": "Accurate classification of software bugs is essential for improving software\nquality. This paper presents a rule-based automated framework for classifying\nissues in quantum software repositories by bug type, category, severity, and\nimpacted quality attributes, with additional focus on quantum-specific bug\ntypes. The framework applies keyword and heuristic-based techniques tailored to\nquantum computing. To assess its reliability, we manually classified a\nstratified sample of 4,984 issues from a dataset of 12,910 issues across 36\nQiskit repositories. Automated classifications were compared with ground truth\nusing accuracy, precision, recall, and F1-score. The framework achieved up to\n85.21% accuracy, with F1-scores ranging from 0.7075 (severity) to 0.8393\n(quality attribute). Statistical validation via paired t-tests and Cohen's\nKappa showed substantial to almost perfect agreement for bug type (k = 0.696),\ncategory (k = 0.826), quality attribute (k = 0.818), and quantum-specific bug\ntype (k = 0.712). Severity classification showed slight agreement (k = 0.162),\nsuggesting room for improvement. Large-scale analysis revealed that classical\nbugs dominate (67.2%), with quantum-specific bugs at 27.3%. Frequent bug\ncategories included compatibility, functional, and quantum-specific defects,\nwhile usability, maintainability, and interoperability were the most impacted\nquality attributes. Most issues (93.7%) were low severity; only 4.3% were\ncritical. A detailed review of 1,550 quantum-specific bugs showed that over\nhalf involved quantum circuit-level problems, followed by gate errors and\nhardware-related issues.", "AI": {"tldr": "The paper introduces a rule-based automated framework for classifying issues in quantum software repositories, validated with Qiskit data. The system reliably identifies most bug types, categories, and quality attributes, though severity classification requires enhancement. The analysis reveals that most bugs are classical and low severity, but a significant portion are unique to quantum software, providing valuable direction for improving quantum software quality.", "motivation": "Despite the growing complexity of quantum software, there is a lack of systematic and automated methods for classifying software bugs, particularly those specific to quantum computing. Accurate bug classification can significantly enhance software quality, maintenance, and development but requires approaches tailored to the unique characteristics of quantum software.", "method": "The authors developed a rule-based, automated classification framework that uses keyword and heuristic techniques customized for quantum computing. They validated the framework by manually labeling a stratified sample of issues from Qiskit repositories, then compared automated and manual results using accuracy, precision, recall, F1-score, and kappa statistics.", "result": "The framework achieved up to 85.21% accuracy in classifying bugs, with F1-scores varying by classification dimension (0.7075 for severity, 0.8393 for quality attributes). Cohen's Kappa indicated substantial to almost perfect agreement for most labels except for severity, which had only slight agreement (k=0.162). The analysis found most bugs were classical (67.2%), with 27.3% quantum-specific. Most issues were low severity (93.7%). Half of quantum-specific bugs related to circuit-level problems.", "conclusion": "The proposed rule-based framework is effective for automated classification of both classical and quantum-specific bugs in quantum software repositories, showing strong agreement with manual labeling for most categories. However, severity classification needs improvement. The study provides new insights into the distribution and types of bugs in quantum software, highlighting important targets for future quality assurance efforts."}}
{"id": "2506.10426", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10426", "abs": "https://arxiv.org/abs/2506.10426", "authors": ["Xiao Yu", "Haoxuan Chen", "Feifei Niu", "Xing Hu", "Jacky Wai Keung", "Xin Xia"], "title": "Towards Understanding Bugs in Distributed Training and Inference Frameworks for Large Language Models", "comment": null, "summary": "With the rapid development of large language models (LLMs), distributed\ntraining and inference frameworks like DeepSpeed have become essential for\nscaling model training and inference across multiple GPUs or nodes. However,\nthe increasing complexity of these frameworks brings non-trivial software bugs,\nwhich may degrade training performance, cause unexpected failures, and result\nin significant resource waste. Understanding framework bugs' characteristics is\nfundamental for quality assurance, allowing the design of more effective\ndebugging and repair methods. Thus, our paper conducts the first large-scale\nempirical analysis of 308 fixed bugs across three popular distributed\ntraining/inference frameworks: DeepSpeed, Megatron-LM, and Colossal-AI. We\nexamine bug symptoms, root causes, bug identification and fixing efforts, and\ncommon low-effort fixing strategies. Additionally, the distributed nature of\nthese frameworks introduces unique bug root causes, such as allocation strategy\nerror and distributed communication error. Diagnosing and fixing complex bugs\nremains challenging due to factors like the disconnect between symptoms and\nroot causes, high bug reproduction costs, and low-level or cross-component\ninteractions. Interestingly, we observe that 48% of bug fixes require minimal\ncode changes (<=10 LOC) and follow simple strategies such as conditional logic\noptimization, parameter handling enhancement, or version compatibility\nhandling, indicating potential for automation. Based on these insights, we\noffer several implications for improving the reliability of both distributed\ntraining and inference frameworks and their dependent LLM projects, while also\nidentifying opportunities to leverage LLM-based tools for automated debugging\nand repair.", "AI": {"tldr": "This paper analyzes 308 bugs in popular distributed LLM training/inference frameworks, revealing common causes, challenges in debugging, and the potential for automation. Almost half of fixes are simple, suggesting LLM-based tools could aid in automating many repairs and improving reliability across the ecosystem.", "motivation": "The rapid growth of large language models has made distributed training and inference frameworks essential, but their increasing complexity introduces significant and costly software bugs. Systematically understanding the nature of these bugs is necessary to develop better quality assurance, debugging, and repair methods.", "method": "The authors conducted a large-scale empirical analysis of 308 fixed bugs from three major distributed LLM frameworks (DeepSpeed, Megatron-LM, Colossal-AI). Their analysis included studying bug symptoms, root causes, identification and fixing processes, as well as common low-effort repair strategies.", "result": "They found that distributed frameworks exhibit unique bug root causes, such as allocation strategy and distributed communication errors. Almost half (48%) of bug fixes required only minor code changes, typically using simple strategies like conditional logic, parameter handling, or version compatibility updates. However, diagnosing and fixing bugs can be difficult due to complex root-symptom disconnects, high costs of bug reproduction, and interactions across low-level or multiple components.", "conclusion": "The study provides actionable insights for increasing the reliability of distributed LLM frameworks and their applications. It also identifies promising avenues to automate debugging and repair using LLM-based tools, suggesting significant potential to reduce bug-fixing workloads."}}
{"id": "2506.10484", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10484", "abs": "https://arxiv.org/abs/2506.10484", "authors": ["Fangwen Mu", "Junjie Wang", "Lin Shi", "Song Wang", "Shoubin Li", "Qing Wang"], "title": "EXPEREPAIR: Dual-Memory Enhanced LLM-based Repository-Level Program Repair", "comment": null, "summary": "Automatically repairing software issues remains a fundamental challenge at\nthe intersection of software engineering and AI. Although recent advancements\nin Large Language Models (LLMs) have demonstrated potential for\nrepository-level repair tasks, current methodologies exhibit two notable\nlimitations: (1) they often address issues in isolation, neglecting to\nincorporate insights from previously resolved issues, and (2) they rely on\nstatic and rigid prompting strategies, which constrain their ability to\ngeneralize across diverse and evolving issue scenarios. Inspired by the dual\nmemory systems of human cognition, where episodic and semantic memories work\nsynergistically to support human reasoning and decision-making, we propose\nExpeRepair, a novel LLM-based approach that continuously learns from historical\nrepair experiences through dual-channel knowledge accumulation. ExpeRepair\norganizes historical repair experiences into two complementary memories: an\nepisodic memory that stores concrete repair demonstrations, and a semantic\nmemory that encodes abstract reflective insights. At inference time, ExpeRepair\nactivates both memory systems by retrieving relevant demonstrations from\nepisodic memory and recalling high-level repair insights from semantic memory.\nIt further enhances adaptability through dynamic prompt composition,\nsynergistically integrating both memory types to replace static prompts with\ncontext-aware, experience-driven prompts. Experiments on the SWE-bench Lite\nbenchmark demonstrate that ExpeRepair achieves a pass@1 score of 49.3% with\nClaude 3.7 Sonnet, outperforming all state-of-the-art open-source methods.", "AI": {"tldr": "ExpeRepair leverages dual memories (episodic and semantic) to dynamically generate experience-driven prompts for software repair using LLMs, significantly surpassing previous methods in benchmark performance.", "motivation": "Current automatic software repair methods using LLMs ignore prior repair experiences and use static prompt strategies, limiting their adaptability and performance. There's a need for a system that learns from history and adapts prompts for more effective repairs.", "method": "The paper proposes ExpeRepair, an LLM-based approach that utilizes a dual-memory system: episodic memory for storing concrete past repair demonstrations, and semantic memory for abstract repair insights. During inference, both memories are activated and dynamically integrated to generate context-aware, adaptable prompts for software repair tasks.", "result": "On the SWE-bench Lite benchmark, ExpeRepair with Claude 3.7 Sonnet achieved a pass@1 score of 49.3%, outperforming all state-of-the-art open-source software repair methods.", "conclusion": "Dual-channel memory accumulation and dynamic prompt composition significantly boost LLM-based automated software repair, making ExpeRepair more effective and adaptable than existing approaches."}}
{"id": "2506.10501", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10501", "abs": "https://arxiv.org/abs/2506.10501", "authors": ["Surya Jasper", "Minh Luu", "Evan Pan", "Aakash Tyagi", "Michael Quinn", "Jiang Hu", "David Kebo Houngninou"], "title": "BugGen: A Self-Correcting Multi-Agent LLM Pipeline for Realistic RTL Bug Synthesis", "comment": null, "summary": "Hardware complexity continues to strain verification resources, motivating\nthe adoption of machine learning (ML) methods to improve debug efficiency.\nHowever, ML-assisted debugging critically depends on diverse and scalable bug\ndatasets, which existing manual or automated bug insertion methods fail to\nreliably produce. We introduce BugGen, a first of its kind, fully autonomous,\nmulti-agent pipeline leveraging Large Language Models (LLMs) to systematically\ngenerate, insert, and validate realistic functional bugs in RTL. BugGen\npartitions modules, selects mutation targets via a closed-loop agentic\narchitecture, and employs iterative refinement and rollback mechanisms to\nensure syntactic correctness and functional detectability. Evaluated across\nfive OpenTitan IP blocks, BugGen produced 500 unique bugs with 94% functional\naccuracy and achieved a throughput of 17.7 validated bugs per hour-over five\ntimes faster than typical manual expert insertion. Additionally, BugGen\nidentified 104 previously undetected bugs in OpenTitan regressions,\nhighlighting its utility in exposing verification coverage gaps. Compared\nagainst Certitude, BugGen demonstrated over twice the syntactic accuracy,\ndeeper exposure of testbench blind spots, and more functionally meaningful and\ncomplex bug scenarios. Furthermore, when these BugGen-generated datasets were\nemployed to train ML-based failure triage models, we achieved high\nclassification accuracy (88.1%-93.2%) across different IP blocks, confirming\nthe practical utility and realism of generated bugs. BugGen thus provides a\nscalable solution for generating high-quality bug datasets, significantly\nenhancing verification efficiency and ML-assisted debugging.", "AI": {"tldr": "BugGen automates the generation of realistic hardware bugs using LLMs, producing high-quality datasets much faster and more accurately than manual or traditional methods, greatly benefiting machine learning-based verification and debugging workflows.", "motivation": "The increasing hardware complexity is overburdening verification processes, prompting a need for scalable and realistic bug datasets to improve machine learning-assisted debugging. Existing bug insertion methods (manual or automated) are inadequate at producing diverse, realistic bugs efficiently.", "method": "The paper introduces BugGen, an autonomous, multi-agent pipeline that employs Large Language Models (LLMs) to systematically generate, mutate, and validate realistic bugs in Register Transfer Level (RTL) hardware code. BugGen features module partitioning, mutation target selection via a closed-loop agentic architecture, and uses iterative refinement and rollback mechanisms to ensure both syntactic and functional validity of the inserted bugs.", "result": "Across five OpenTitan IP blocks, BugGen generated 500 unique bugs with 94% functional accuracy and a throughput of 17.7 validated bugs per hour\u2014a significant speedup over manual expert insertion. It also uncovered 104 previously unseen bugs and outperformed Certitude in syntactic accuracy, depth of testbench coverage, and bug scenario quality. Training ML-based failure triage models with BugGen-generated datasets resulted in classification accuracy between 88.1% and 93.2%.", "conclusion": "BugGen is a scalable and autonomous solution for generating high-quality, realistic hardware bug datasets. It significantly improves verification efficiency, enhances ML-assisted debugging, and exposes coverage gaps more effectively than existing methods."}}
{"id": "2506.10525", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10525", "abs": "https://arxiv.org/abs/2506.10525", "authors": ["Junhang Cheng", "Fang Liu", "Chengru Wu", "Li Zhang"], "title": "AdaptiveLLM: A Framework for Selecting Optimal Cost-Efficient LLM for Code-Generation Based on CoT Length", "comment": "Accepted by Internetware 2025", "summary": "While Large Language Models (LLMs) have significantly advanced code\ngeneration efficiency, they face inherent challenges in balancing performance\nand inference costs across diverse programming tasks. Dynamically selecting the\noptimal LLM based on task difficulty and resource constraints offers a\npromising approach to achieve an optimal balance between efficiency and\nperformance. However, existing model selection methods are resource-intensive\nand often neglect cost efficiency. Moreover, these approaches rely on\nhuman-annotated difficulty labels that are frequently inaccessible in\nreal-world settings and may not align with the LLM's own assessment of task\ndifficulty. In this paper, we introduce AdaptiveLLM, a framework that\ndynamically selects optimal LLMs for a given coding task by automatically\nassessing task difficulty. Our framework first estimates task difficulty using\nChain-of-Thought lengths generated by reasoning model, clusters these into\nthree difficulty levels via k-means, and fine-tunes CodeBERT to embed\ndifficulty-aware features. A trained XGBoost classifier then selects the best\nmodel for each problem, optimizing the performance-cost trade-off. Experimental\nresults show that AdaptiveLLM achieves a 7.86% improvement in pass@1 score\nwhile reducing resource consumption by 88.9% compared to baseline method\nComplexityNet. When compared to a single model, AdaptiveLLM demonstrates an\napproximately 15% accuracy improvement, while maintaining the same level of\ncost consumption. Apart from that, the difficulty assessment using CoT provides\nmore reliable selection criteria than human evaluation. Our replication package\nis available at https://github.com/cjhCoder7/AdaptiveLLM.", "AI": {"tldr": "AdaptiveLLM automatically picks the best LLM for code tasks using an innovative, automated task difficulty measure and machine learning. It significantly improves code generation accuracy while greatly reducing computation costs, outperforming previous model selection methods that rely on human judgment.", "motivation": "Large Language Models improve code generation but are restricted by the need to balance computational cost and performance. Existing methods for choosing the right LLM per task are inefficient and rely on subjective human difficulty ratings, which are often unavailable or unreliable.", "method": "The authors propose AdaptiveLLM, a framework that first estimates code task difficulty using Chain-of-Thought (CoT) reasoning lengths, clusters these into difficulty levels with k-means, encodes difficulty-aware features via a fine-tuned CodeBERT, and selects the optimal LLM with an XGBoost classifier to optimize for accuracy and cost.", "result": "Experimental results show AdaptiveLLM improves pass@1 code accuracy by 7.86% and cuts resource costs by 88.9% versus ComplexityNet. It also boosts accuracy by ~15% over using a single model, with cost held constant, and its automatic task difficulty assessment outperforms human evaluations for model selection.", "conclusion": "AdaptiveLLM is an effective and efficient solution for dynamically matching LLMs to coding tasks, achieving a better balance between accuracy and computational cost than prior systems, and providing a more robust, automated method of assessing task difficulty via CoT-based signals rather than unreliable human labels."}}
{"id": "2506.10624", "categories": ["cs.SE", "cs.AR"], "pdf": "https://arxiv.org/pdf/2506.10624", "abs": "https://arxiv.org/abs/2506.10624", "authors": ["Lukas J\u00fcnger", "Jan Henrik Weinstock", "Tim Kraus"], "title": "Scalable Software Testing in Fast Virtual Platforms: Leveraging SystemC, QEMU and Containerization", "comment": "Published in DVCon China 2025", "summary": "The ever-increasing complexity of HW/SW systems presents a persistent\nchallenge, particularly in safety-critical domains like automotive, where\nextensive testing is imperative. However, the availability of hardware often\nlags behind, hindering early-stage software development. To address this,\nVirtual Platforms (VPs) based on the SystemC TLM-2.0 standard have emerged as a\npivotal solution, enabling pre-silicon execution and testing of unmodified\ntarget software. In this study, we propose an approach leveraging\ncontainerization to encapsulate VPs in order to reduce environment dependencies\nand enable cloud deployment for fast, parallelized test execution, as well as\nopen-source VP technologies such as QEMU and VCML to obviate the need for seat\nlicenses. To demonstrate the efficacy of our approach, we present an Artificial\nIntelligence (AI) accelerator VP case study. Through our research, we offer a\nrobust solution to address the challenges posed by the complexity of HW/SW\nsystems, with practical implications for accelerating HW/SW co-development.", "AI": {"tldr": "Containerizing open-source virtual platforms allows for early, scalable, and dependency-free software testing in the cloud, helping accelerate HW/SW co-development for complex systems.", "motivation": "The complexity of hardware/software (HW/SW) systems, especially in safety-critical fields like automotive, demands extensive testing. However, hardware availability often lags, delaying software development. The motivation is to improve early-stage software development and testing by overcoming hardware unavailability and complex environment dependencies.", "method": "The paper proposes encapsulating Virtual Platforms (VPs) using containerization to reduce environment dependencies and allow cloud-based, parallel test execution. It leverages open-source VP solutions (QEMU, VCML) to avoid proprietary license requirements. The approach is demonstrated via an AI accelerator VP case study.", "result": "The case study validates the approach, showing that containerized, cloud-deployed VPs effectively support fast, parallelized HW/SW co-development and pre-silicon testing without dependency or license issues.", "conclusion": "The study provides a robust method for addressing HW/SW co-development challenges, enabling earlier, scalable, and more practical pre-silicon software testing through containerization and open-source tools."}}
{"id": "2506.10654", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10654", "abs": "https://arxiv.org/abs/2506.10654", "authors": ["Abir Bouraffa", "Carolin Brandt", "Andy Zaidmann", "Walid Maalej"], "title": "Not One to Rule Them All: Mining Meaningful Code Review Orders From GitHub", "comment": null, "summary": "Developers use tools such as GitHub pull requests to review code, discuss\nproposed changes, and request modifications. While changed files are commonly\npresented in alphabetical order, this does not necessarily coincide with the\nreviewer's preferred navigation sequence. This study investigates the different\nnavigation orders developers follow while commenting on changes submitted in\npull requests. We mined code review comments from 23,241 pull requests in 100\npopular Java and Python repositories on GitHub to analyze the order in which\nthe reviewers commented on the submitted changes. Our analysis shows that for\n44.6% of pull requests, the reviewers comment in a non-alphabetical order.\nAmong these pull requests, we identified traces of alternative meaningful\norders: 20.6% (2,134) followed a largest-diff-first order, 17.6% (1,827) were\ncommented in the order of the files' similarity to the pull request's title and\ndescription, and 29% (1,188) of pull requests containing changes to both\nproduction and test files adhered to a test-first order. We also observed that\nthe proportion of reviewed files to total submitted files was significantly\nhigher in non-alphabetically ordered reviews, which also received slightly\nfewer approvals from reviewers, on average. Our findings highlight the need for\nadditional support during code reviews, particularly for larger pull requests,\nwhere reviewers are more likely to adopt complex strategies rather than\nfollowing a single predefined order.", "AI": {"tldr": "Code reviewers often don't follow the default alphabetical order when examining files in pull requests. Instead, they use meaningful strategies (like prioritizing big changes or test files), especially in larger reviews. This suggests a need for better tool support to match real reviewer habits.", "motivation": "Code reviewers typically see changed files in alphabetical order, but this order may not match reviewers' preferences for effective navigation during a review. The study aims to understand how reviewers actually choose the order in which they comment on files in GitHub pull requests.", "method": "The authors mined code review comments from 23,241 pull requests across 100 popular Java and Python GitHub repositories. They analyzed the sequence of file comments to determine if reviewers followed non-alphabetical orders, and if so, whether these orders reflected specific strategies (like commenting on largest diffs first, or files similar to PR titles, etc.). They also compared the proportion of reviewed files and approval rates between different navigation orders.", "result": "44.6% of pull requests had reviewers commenting in non-alphabetical order. Of these, some showed clear strategies: 20.6% followed a largest-diff-first approach, 17.6% focused on files similar to the PR title/description, and 29% with both production and test files followed a test-first order. Non-alphabetical reviews involved a higher proportion of reviewed files but received slightly fewer approvals on average.", "conclusion": "Reviewers frequently deviate from alphabetical file order when commenting on pull requests, often using more meaningful or strategic review sequences. More sophisticated support is needed for large pull requests to accommodate these diverse reviewer strategies."}}
{"id": "2506.10704", "categories": ["cs.SE", "cs.AI", "D.2.1; D.2.4; D.2.10; F.4.1; F.4.3"], "pdf": "https://arxiv.org/pdf/2506.10704", "abs": "https://arxiv.org/abs/2506.10704", "authors": ["Arshad Beg", "Diarmuid O'Donoghue", "Rosemary Monahan"], "title": "Formalising Software Requirements using Large Language Models", "comment": "Accepted and presented as a poster in ADAPT Annual Conference\n  (AACS2025) on 15th of May, 2025", "summary": "This paper is a brief introduction to our recently initiated project named\nVERIFAI: Traceability and verification of natural language requirements. The\nproject addresses the challenges in the traceability and verification of formal\nspecifications through providing support for the automatic generation of the\nformal specifications and the traceability of the requirements from the initial\nsoftware design stage through the systems implementation and verification.\nApproaches explored in this project include Natural Language Processing, use of\nontologies to describe the software system domain, reuse of existing software\nartefacts from similar systems (i.e. through similarity based reuse) and large\nlanguage models to identify and declare the specifications as well as use of\nartificial intelligence to guide the process.", "AI": {"tldr": "The VERIFAI project aims to automate and enhance the traceability and verification of software requirements by leveraging AI and NLP technologies, especially large language models, for automatically linking natural language requirements to formal specifications.", "motivation": "There is a significant challenge in ensuring that formal specifications accurately reflect and trace back to the original natural language requirements throughout the software development process.", "method": "The project explores methods such as Natural Language Processing, ontologies for domain description, reuse of existing software artefacts via similarity, large language models for requirement specifications, and AI-driven guidance for traceability and verification.", "result": "The paper introduces the VERIFAI project and describes its planned methodologies, but does not present experimental results yet as it is an initiation phase.", "conclusion": "The VERIFAI project aims to address traceability and automated verification of requirements by integrating AI, NLP, and ontology-based tools, with a focus on automating and improving the generation and maintenance of formal specifications from natural language requirements."}}
{"id": "2506.10770", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10770", "abs": "https://arxiv.org/abs/2506.10770", "authors": ["Joran Leest", "Claudia Raibulet", "Patricia Lago", "Ilias Gerostathopoulos"], "title": "From Tea Leaves to System Maps: Context-awareness in Monitoring Operational Machine Learning Models", "comment": null, "summary": "Machine learning (ML) models in production do not fail due to statistical\nanomalies in their input data; they fail due to contextual misalignment -- when\ntheir environment deviates from training assumptions, leading to unreliable\npredictions. Effective ML monitoring requires rich contextual information to\nmove beyond detecting statistical shifts toward meaningful alerts and\nsystematic root-cause analysis. Yet, surprisingly, despite extensive research\nin ML monitoring and related disciplines (drift detection, data validation,\nout-of-distribution detection), there is no shared understanding of how to use\ncontextual information -- striking, given that monitoring involves\ninterpretation of information in context. In response, this paper presents a\nsystematic review to characterize and structure the various types of contextual\ninformation in this domain. Our analysis examines 94 primary studies across\ndata mining, databases, software engineering, and ML. We introduce the\nContextual System--Aspect--Representation (C-SAR) framework, a conceptual model\nthat synthesizes our findings. We also identify 20 recurring and potentially\nreusable patterns of specific system, aspect, and representation combinations,\nand map them to the monitoring activities they support. This study provides a\nnew perspective on ML monitoring: from interpreting \"tea leaves\" of\nobservational statistics into constructing and managing \"system maps\" that\nenable systematic and reliable ML monitoring practices.", "AI": {"tldr": "Current ML monitoring focuses too much on statistical data and overlooks context, leading to missed failures. This paper systematically reviews the use of contextual information, introduces the C-SAR framework for structuring context in monitoring, and identifies reusable patterns, paving the way for more robust, context-aware ML monitoring practices.", "motivation": "Machine learning models in production often fail not because of statistical anomalies, but due to contextual misalignments, highlighting gaps in current monitoring approaches that lack meaningful contextual analysis.", "method": "The authors conduct a systematic review of 94 primary studies from various related fields. They analyze these studies to characterize and structure types of contextual information relevant to ML monitoring, and introduce the Contextual System--Aspect--Representation (C-SAR) conceptual framework. They also identify 20 reusable patterns and map them to ML monitoring activities.", "result": "The study introduces the C-SAR framework, structures the use of contextual information in ML monitoring, identifies 20 recurring patterns, and maps these to specific monitoring activities, providing a more systematic approach to ML monitoring.", "conclusion": "The paper offers a new perspective on ML monitoring by shifting from purely statistical alerting to context-informed, systematic practices, enabling deeper root-cause analysis and more reliable ML system management."}}
{"id": "2506.10785", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10785", "abs": "https://arxiv.org/abs/2506.10785", "authors": ["Vinaik Chhetri", "Krishna Upadhyay", "A. B. Siddique", "Umar Farooq"], "title": "What Users Value and Critique: Large-Scale Analysis of User Feedback on AI-Powered Mobile Apps", "comment": "12 pages, 6 figures, 5 tables", "summary": "Artificial Intelligence (AI)-powered features have rapidly proliferated\nacross mobile apps in various domains, including productivity, education,\nentertainment, and creativity. However, how users perceive, evaluate, and\ncritique these AI features remains largely unexplored, primarily due to the\noverwhelming volume of user feedback. In this work, we present the first\ncomprehensive, large-scale study of user feedback on AI-powered mobile apps,\nleveraging a curated dataset of 292 AI-driven apps across 14 categories with\n894K AI-specific reviews from Google Play. We develop and validate a\nmulti-stage analysis pipeline that begins with a human-labeled benchmark and\nsystematically evaluates large language models (LLMs) and prompting strategies.\nEach stage, including review classification, aspect-sentiment extraction, and\nclustering, is validated for accuracy and consistency. Our pipeline enables\nscalable, high-precision analysis of user feedback, extracting over one million\naspect-sentiment pairs clustered into 18 positive and 15 negative user topics.\nOur analysis reveals that users consistently focus on a narrow set of themes:\npositive comments emphasize productivity, reliability, and personalized\nassistance, while negative feedback highlights technical failures (e.g.,\nscanning and recognition), pricing concerns, and limitations in language\nsupport. Our pipeline surfaces both satisfaction with one feature and\nfrustration with another within the same review. These fine-grained,\nco-occurring sentiments are often missed by traditional approaches that treat\npositive and negative feedback in isolation or rely on coarse-grained analysis.\nTo this end, our approach provides a more faithful reflection of the real-world\nuser experiences with AI-powered apps. Category-aware analysis further uncovers\nboth universal drivers of satisfaction and domain-specific frustrations.", "AI": {"tldr": "The paper develops a validated, scalable analysis pipeline to comprehensively extract nuanced user sentiments from nearly 900,000 reviews of AI-powered mobile apps, revealing both general and domain-specific satisfaction and frustrations, and significantly improving understanding compared to traditional, less detailed sentiment analysis methods.", "motivation": "With the rapid increase in AI-powered features in mobile apps across many domains, there is little understanding of how users actually perceive and critique these features, mainly due to the overwhelming quantity and complexity of user feedback. Existing analysis methods often miss the nuanced, fine-grained sentiments users express.", "method": "The paper presents a comprehensive, large-scale study of user feedback on AI-enabled mobile apps. It uses a curated dataset of 292 AI-driven apps from Google Play, encompassing 894K AI-specific reviews. They develop a multi-stage analysis pipeline\u2014starting with a human-labeled benchmark set\u2014which systematically evaluates large language models, various prompting strategies, and validates each step (review classification, aspect-sentiment extraction, clustering) for accuracy and consistency.", "result": "The pipeline analyzed user feedback at scale, extracting over one million aspect-sentiment pairs and clustering them into 18 positive and 15 negative user topics. Key findings include users highlighting productivity, reliability, and personalized assistance as positives; while negatives focus on technical failures, pricing, and language support limitations. The approach reveals co-occurring positive and negative sentiments within the same feedback, which are often missed in traditional, coarse methods. Category-wise analysis identifies universal sources of satisfaction and domain-specific frustrations.", "conclusion": "This work introduces a validated, scalable analysis pipeline that enables nuanced, high-precision analysis of massive user feedback regarding AI-powered apps, revealing detailed user sentiments and providing a truer representation of user experience. The approach outperforms traditional methods that overlook complex, co-occurring sentiments."}}
{"id": "2506.10833", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10833", "abs": "https://arxiv.org/abs/2506.10833", "authors": ["Fabian C. Pe\u00f1a", "Steffen Herbold"], "title": "Evaluating Large Language Models on Non-Code Software Engineering Tasks", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode understanding and generation; however, their effectiveness on non-code\nSoftware Engineering (SE) tasks remains underexplored. We present the first\ncomprehensive benchmark, which we name `Software Engineering Language\nUnderstanding' (SELU), for evaluating LLMs on 17 non-code tasks, spanning from\nidentifying whether a requirement is functional or non-functional to estimating\nthe effort and complexity of backlog items. SELU covers classification,\nregression, Named Entity Recognition (NER), and Masked Language Modeling (MLM)\ntargets, with data drawn from diverse sources such as code repositories, issue\ntracking systems, and developer forums. We fine-tune 22 open-source LLMs,\nprompt two proprietary alternatives, and train two baselines. Performance is\nmeasured using metrics such as F1-macro, SMAPE, F1-micro, and accuracy, and\ncompared via the Bayesian signed-rank test. Our results show that\nmoderate-scale decoder-only models consistently form a top-tier, exhibiting\nhigh mean performance and low across-task variance, while domain adaptation via\ncode-focused pre-training might yield only modest improvements. These insights\nguide model selection for non-code SE workflows and highlight directions for\nexpanding SELU to generative and design-oriented scenarios.", "AI": {"tldr": "This paper introduces the SELU benchmark for evaluating LLMs on 17 non-code SE tasks and finds that moderate-scale decoder-only models work best; code-pretraining helps only a little for non-code tasks.", "motivation": "While Large Language Models (LLMs) are known for their success in code-related tasks, their performance on non-code Software Engineering (SE) tasks is not well studied. The paper aims to address this gap by evaluating LLMs specifically on non-code SE tasks.", "method": "The authors introduce a new benchmark called SELU, which covers 17 non-code SE tasks including classification, regression, named entity recognition, and masked language modeling. The study involves fine-tuning 22 open-source LLMs, using two proprietary models, and training two baseline models. They evaluate performance with several metrics and compare results using the Bayesian signed-rank test.", "result": "Moderate-scale decoder-only LLMs consistently perform the best, showing high mean performance and low variance across tasks. Code-focused pre-training offers only slight improvements for non-code SE tasks.", "conclusion": "The SELU benchmark provides the first comprehensive evaluation of LLMs on non-code SE tasks. Moderate-scale decoder-only models are recommended for such tasks, and there is potential for expanding SELU to generative and design-oriented areas."}}
{"id": "2506.10869", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10869", "abs": "https://arxiv.org/abs/2506.10869", "authors": ["Quinn Thibeault", "Giulia Pedrielli"], "title": "MultiCoSim: A Python-based Multi-Fidelity Co-Simulation Framework", "comment": null, "summary": "Simulation is a foundational tool for the analysis and testing of\ncyber-physical systems (CPS), underpinning activities such as algorithm\ndevelopment, runtime monitoring, and system verification. As CPS grow in\ncomplexity and scale, particularly in safety-critical and learning-enabled\nsettings, accurate analysis and synthesis increasingly rely on the rapid use of\nsimulation experiments. Because CPS inherently integrate hardware, software,\nand physical processes, simulation platforms must support co-simulation of\nheterogeneous components at varying levels of fidelity. Despite recent advances\nin high-fidelity modeling of hardware, firmware, and physics, co-simulation in\ndiverse environments remains challenging. These limitations hinder the\ndevelopment of reusable benchmarks and impede the use of simulation for\nautomated and comparative evaluation.\n  Existing simulation tools often rely on rigid configurations, lack automation\nsupport, and present obstacles to portability and modularity. Many are\nconfigured through static text files or impose constraints on how simulation\ncomponents are represented and connected, making it difficult to flexibly\ncompose systems or integrate components across platforms.\n  To address these challenges, we introduce MultiCoSim, a Python-based\nsimulation framework that enables users to define, compose, and configure\nsimulation components programmatically. MultiCoSim supports distributed,\ncomponent-based co-simulation and allows seamless substitution and\nreconfiguration of components. We demonstrate the flexibility of MultiCoSim\nthrough case studies that include co-simulations involving custom\nautomaton-based controllers, as well as integration with off-the-shelf\nplatforms like the PX4 autopilot for aerial robotics. These examples highlight\nMultiCoSim's capability to streamline CPS simulation pipelines for research and\ndevelopment.", "AI": {"tldr": "The paper presents MultiCoSim, a Python-based framework enabling flexible and automated co-simulation of diverse CPS components. It overcomes the rigidity and lack of modularity in traditional tools, as shown in various case studies, making CPS simulation more effective for research and development.", "motivation": "As cyber-physical systems (CPS) become more complex and integrated, especially in safety-critical and learning-enabled domains, there is a growing need for flexible and rapid simulation tools. Current simulation platforms struggle with heterogeneity, modularity, and automation, making analysis, benchmarking, and component integration difficult.", "method": "The authors introduce MultiCoSim, a Python-based simulation framework. MultiCoSim enables programmatic definition, composition, and configuration of simulation components, supporting distributed and component-based co-simulation. The framework offers seamless substitution and reconfiguration of elements and is demonstrated through case studies involving both custom controllers and integration with platforms like PX4 for aerial robotics.", "result": "MultiCoSim allows for flexible, modular, and automated CPS simulation. The case studies show that the framework can handle heterogeneous co-simulations, integrate with existing platforms, and streamline the research and development process for CPS.", "conclusion": "MultiCoSim overcomes the limitations of existing simulation tools by providing a flexible, programmable, and modular co-simulation environment for CPS, enabling improved benchmarking, comparative evaluation, and integration across diverse domains."}}
{"id": "2506.10954", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10954", "abs": "https://arxiv.org/abs/2506.10954", "authors": ["Lianghong Guo", "Yanlin Wang", "Caihua Li", "Pengyu Yang", "Jiachi Chen", "Wei Tao", "Yingtian Zou", "Duyu Tang", "Zibin Zheng"], "title": "SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks", "comment": null, "summary": "Constructing large-scale datasets for the GitHub issue resolution task is\ncrucial for both training and evaluating the software engineering capabilities\nof Large Language Models (LLMs). However, the traditional process for creating\nsuch benchmarks is notoriously challenging and labor-intensive, particularly in\nthe stages of setting up evaluation environments, grading test outcomes, and\nvalidating task instances. In this paper, we propose SWE-Factory, an automated\npipeline designed to address these challenges. To tackle these issues, our\npipeline integrates three core automated components. First, we introduce\nSWE-Builder, a multi-agent system that automates evaluation environment\nconstruction, which employs four specialized agents that work in a\ncollaborative, iterative loop and leverages an environment memory pool to\nenhance efficiency. Second, we introduce a standardized, exit-code-based\ngrading method that eliminates the need for manually writing custom parsers.\nFinally, we automate the fail2pass validation process using these reliable exit\ncode signals. Experiments on 671 issues across four programming languages show\nthat our pipeline can effectively construct valid task instances; for example,\nwith GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at $0.045 per\ninstance, while with Gemini-2.5-flash, it achieves comparable performance at\nthe lowest cost of $0.024 per instance. We also demonstrate that our\nexit-code-based grading achieves 100% accuracy compared to manual inspection,\nand our automated fail2pass validation reaches a precision of 0.92 and a recall\nof 1.00. We hope our automated pipeline will accelerate the collection of\nlarge-scale, high-quality GitHub issue resolution datasets for both training\nand evaluation. Our code and datasets are released at\nhttps://github.com/DeepSoftwareAnalytics/swe-factory.", "AI": {"tldr": "The authors introduce SWE-Factory, an automated, cost-effective pipeline for building large GitHub issue resolution datasets, eliminating much of the manual work through multi-agent systems and exit-code grading. It produces valid, high-quality data efficiently and accurately, with code and datasets available for community use.", "motivation": "Constructing large-scale, high-quality datasets for GitHub issue resolution is essential for advancing and evaluating the software engineering capability of Large Language Models (LLMs). Traditional dataset creation methods are difficult, time-consuming, and require significant manual labor, especially in environment setup, test grading, and task validation.", "method": "The authors propose SWE-Factory, an automated pipeline with three main components: (1) SWE-Builder, a multi-agent system that collaboratively and iteratively builds evaluation environments using an environment memory pool; (2) an exit-code-based grading approach, which standardizes test outcome grading and removes the need for manual custom parsers; (3) an automated fail2pass validation mechanism that relies on exit code signals.", "result": "Experiments covering 671 GitHub issues in four programming languages demonstrate that SWE-Factory can efficiently produce valid task instances. For example, using GPT-4.1-mini, SWE-Builder constructs 269 valid instances at $0.045 each, whereas Gemini-2.5-flash achieves similar results at a lower cost of $0.024 per instance. Their exit-code-based grading method matches 100% with manual inspection, and automated fail2pass validation achieves 0.92 precision and 1.00 recall.", "conclusion": "SWE-Factory automates and streamlines the creation of large-scale GitHub issue resolution datasets, significantly reducing manual labor and cost while maintaining high accuracy and efficiency. The released code and datasets can accelerate future research and evaluation for LLMs in software engineering tasks."}}
