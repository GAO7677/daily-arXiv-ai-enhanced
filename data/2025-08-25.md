<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 17]
- [cs.PL](#cs.PL) [Total: 5]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A Systematic Literature Review of Machine Learning Approaches for Migrating Monolithic Systems to Microservices](https://arxiv.org/abs/2508.15941)
*Imen Trabelsi,Brahim Mahmoudi,Jean Baptiste Minani,Naouel Moha,Yann-Gaël Guéhéneuc*

Main category: cs.SE

TL;DR: This systematic review aggregates and analyzes research on applying ML to monolith-to-microservice migration, finding that although some steps benefit from ML, others lack solutions. Challenges include data scarcity, scaling difficulties, and insufficient tool support, pointing to a need for more comprehensive approaches.


<details>
  <summary>Details</summary>
Motivation: Monolithic system architectures face scalability and maintainability issues, driving the need to migrate to microservices. However, the migration process is complex and labor-intensive, and there is a lack of systematic insight about how machine learning (ML) can automate or assist in this migration.

Method: The authors conduct a Systematic Literature Review (SLR) of 81 primary studies published between 2015 and 2024, adhering to the PRISMA methodology. They extract and analyze data to classify and synthesize how ML techniques are used in various phases of migrating monolithic systems to microservices.

Result: Some migration phases, such as monitoring and service identification, are well-explored with ML techniques, while others, like microservice packaging, are understudied. Key challenges in applying ML to migration include limited data, scalability constraints, lack of tools, and absence of standardized benchmarks.

Conclusion: While ML techniques have advanced certain migration phases, significant gaps remain, especially in underexplored processes and tool support. Addressing these issues is vital for more comprehensive, automated migration solutions.

Abstract: Scalability and maintainability challenges in monolithic systems have led to
the adoption of microservices, which divide systems into smaller, independent
services. However, migrating existing monolithic systems to microservices is a
complex and resource-intensive task, which can benefit from machine learning
(ML) to automate some of its phases. Choosing the right ML approach for
migration remains challenging for practitioners. Previous works studied
separately the objectives, artifacts, techniques, tools, and benefits and
challenges of migrating monolithic systems to microservices. No work has yet
investigated systematically existing ML approaches for this migration to
understand the \revised{automated migration phases}, inputs used, ML techniques
applied, evaluation processes followed, and challenges encountered. We present
a systematic literature review (SLR) that aggregates, synthesises, and
discusses the approaches and results of 81 primary studies (PSs) published
between 2015 and 2024. We followed the Preferred Reporting Items for Systematic
Review and Meta-Analysis (PRISMA) statement to report our findings and answer
our research questions (RQs). We extract and analyse data from these PSs to
answer our RQs. We synthesise the findings in the form of a classification that
shows the usage of ML techniques in migrating monolithic systems to
microservices. The findings reveal that some phases of the migration process,
such as monitoring and service identification, are well-studied, while others,
like packaging microservices, remain unexplored. Additionally, the findings
highlight key challenges, including limited data availability, scalability and
complexity constraints, insufficient tool support, and the absence of
standardized benchmarking, emphasizing the need for more holistic solutions.

</details>


### [2] [Breaking Barriers in Software Testing: The Power of AI-Driven Automation](https://arxiv.org/abs/2508.16025)
*Saba Naqvi,Mohammad Baqar*

Main category: cs.SE

TL;DR: An AI-driven framework automates and optimizes software testing, resulting in higher defect detection, less manual effort, and faster releases, demonstrating that AI can make testing more reliable and scalable.


<details>
  <summary>Details</summary>
Motivation: Traditional software testing is slow, costly, and incomplete, which motivates the need for more efficient, effective, and comprehensive testing approaches.

Method: The paper introduces an AI-driven framework that combines natural language processing, reinforcement learning, and predictive models within a policy-based trust and fairness model to automate the generation, optimization, and validation of test cases.

Result: Case studies show improved defect detection, reduced testing workload, and expedited software release cycles by using the proposed framework.

Conclusion: The AI-driven framework significantly enhances software testing efficiency and reliability, transforming it from a manual, reactive process to a proactive, adaptive system that meets the demands of complex software environments.

Abstract: Software testing remains critical for ensuring reliability, yet traditional
approaches are slow, costly, and prone to gaps in coverage. This paper presents
an AI-driven framework that automates test case generation and validation using
natural language processing (NLP), reinforcement learning (RL), and predictive
models, embedded within a policy-driven trust and fairness model. The approach
translates natural language requirements into executable tests, continuously
optimizes them through learning, and validates outcomes with real-time analysis
while mitigating bias. Case studies demonstrate measurable gains in defect
detection, reduced testing effort, and faster release cycles, showing that
AI-enhanced testing improves both efficiency and reliability. By addressing
integration and scalability challenges, the framework illustrates how AI can
shift testing from a reactive, manual process to a proactive, adaptive system
that strengthens software quality in increasingly complex environments.

</details>


### [3] [ARSP: Automated Repair of Verilog Designs via Semantic Partitioning](https://arxiv.org/abs/2508.16517)
*Bingkun Yao,Ning Wang,Xiangfeng Liu,Yuxin Du,Yuchen Hu,Hong Gao,Zhe Jiang,Nan Guan*

Main category: cs.SE

TL;DR: ARSP, a system using two LLMs for semantic partitioning and patching, greatly improves Verilog bug debugging on large code by focusing on smaller fragments, beating current commercial tools and closing the 'bug signal dilution' gap.


<details>
  <summary>Details</summary>
Motivation: Debugging functional Verilog bugs is a time-consuming process for front-end design. Although LLMs have shown promise in automating debugging, their effectiveness drops for large-scale, industrial modules due to 'bug signal dilution', where relevant information is buried within large amounts of unrelated code.

Method: The paper introduces ARSP, a two-stage system that combats bug signal dilution. In the first stage, a Partition LLM divides a Verilog module into semantically tight fragments. In the second stage, a Repair LLM patches these fragments, and then combines their edits without disturbing unrelated code. Training data is synthetically generated to cover many bug types, design styles, and module sizes.

Result: ARSP achieves 77.92% pass@1 and 83.88% pass@5, significantly outperforming leading commercial LLMs (like Claude-3.7) and state-of-the-art automated Verilog debuggers like Strider and MEIC. The semantic partitioning approach improves debugging success rates by 11.6% (pass@1) and 10.2% (pass@5) compared to analyzing whole modules at once.

Conclusion: By using semantics-guided fragmentation, ARSP substantially mitigates bug signal dilution, enhancing LLM-based automated debugging performance for industrial-scale Verilog modules. This demonstrates fragment-level scope reduction is highly effective for large codebase debugging tasks.

Abstract: Debugging functional Verilog bugs consumes a significant portion of front-end
design time. While Large Language Models (LLMs) have demonstrated great
potential in mitigating this effort, existing LLM-based automated debugging
methods underperform on industrial-scale modules. A major reason for this is
bug signal dilution in long contexts, where a few bug-relevant tokens are
overwhelmed by hundreds of unrelated lines, diffusing the model's attention. To
address this issue, we introduce ARSP, a two-stage system that mitigates
dilution via semantics-guided fragmentation. A Partition LLM splits a module
into semantically tight fragments; a Repair LLM patches each fragment; edits
are merged without altering unrelated logic. A synthetic data framework
generates fragment-level training pairs spanning bug types, design styles, and
scales to supervise both models. Experiments show that ARSP achieves 77.92%
pass@1 and 83.88% pass@5, outperforming mainstream commercial LLMs including
Claude-3.7 and SOTA automated Verilog debugging tools Strider and MEIC. Also,
semantic partitioning improves pass@1 by 11.6% and pass@5 by 10.2% over
whole-module debugging, validating the effectiveness of fragment-level scope
reduction in LLM-based Verilog debugging.

</details>


### [4] [Measuring the effectiveness of code review comments in GitHub repositories: A machine learning approach](https://arxiv.org/abs/2508.16053)
*Shadikur Rahman,Umme Ayman Koana,Hasibul Karim Shanto,Mahmuda Akter,Chitra Roy,Aras M. Ismael*

Main category: cs.SE

TL;DR: The paper evaluates seven machine learning methods for classifying code review sentiments using labelled GitHub data, finding that the Linear SVC performs best. Automating sentiment classification can help developers interpret code reviews more accurately and avoid errors.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper is to assist programmers in identifying errors and understanding code review sentiment, which can help them avoid misconceptions and improve coding quality. Classifying the sentiment polarity of code review comments can enhance awareness and decision making during software development.

Method: The authors manually labelled 13,557 code review comments from three open-source GitHub projects for sentiment polarity. They applied and empirically compared seven machine learning algorithms, including the Linear Support Vector Classifier (SVC), to classify the semantic meaning of these comments.

Result: Among the tested machine learning algorithms, the Linear Support Vector Classifier (SVC) demonstrated the highest accuracy in classifying the sentiment polarity of code review comments.

Conclusion: The study concludes that the Linear SVC is the most effective machine learning technique among those tested for classifying code review sentiment. This classification can improve developers' understanding of code feedback, helping them avoid misconceptions and make more informed decisions.

Abstract: This paper illustrates an empirical study of the working efficiency of
machine learning techniques in classifying code review text by semantic
meaning. The code review comments from the source control repository in GitHub
were extracted for development activity from the existing year for three
open-source projects. Apart from that, programmers need to be aware of their
code and point out their errors. In that case, it is a must to classify the
sentiment polarity of the code review comments to avoid an error. We manually
labelled 13557 code review comments generated by three open source projects in
GitHub during the existing year. In order to recognize the sentiment polarity
(or sentiment orientation) of code reviews, we use seven machine learning
algorithms and compare those results to find the better ones. Among those
Linear Support Vector Classifier(SVC) classifier technique achieves higher
accuracy than others. This study will help programmers to make any solution
based on code reviews by avoiding misconceptions.

</details>


### [5] [From Benchmark Data To Applicable Program Repair: An Experience Report](https://arxiv.org/abs/2508.16071)
*Mahinthan Chandramohan,Jovan Jancic,Yuntong Zhang,Padmanabhan Krishnan*

Main category: cs.SE

TL;DR: The paper proposes combining automated repair techniques with formal specifications to improve unit test generation and repair effectiveness, showing better benchmark results but revealing a persistent gap for real-world defects. Existing methods and specification languages are not expressive enough, and passing more tests doesn't guarantee correctness. Future work targets advanced specifications and integrating human feedback.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve automated program repair, especially addressing the gap between results on academic benchmarks and actual industry needs. Industry defects are not properly solved by existing techniques, motivating an exploration of enhanced methods.

Method: They combine various literature techniques for automated repair, and augment code with formal specifications to help LLMs generate superior unit tests, particularly for complex code. They also experiment with JML and consider new avenues like contract automata, programming by example, and testcase repair, including human feedback integration.

Result: Their combined approach outperforms other techniques on standard benchmarks. Augmenting code with specifications helps LLMs generate better-quality unit tests, especially for complex cases (logic/string errors), but is less useful for well-understood errors. In real-world scenarios, the expressiveness of JML is a bottleneck and correct test passes do not guarantee actual repair success. Practical adoption remains an issue.

Conclusion: While their approach is effective on benchmarks and gains some benefits with specification augmentation, it is still hampered by limitations in specification languages and real-world repair guarantees. There remains a significant gap between what benchmarks measure and what industry requires. Their future work is focused on closing this gap.

Abstract: This paper describes our approach to automated program repair. We combine
various techniques from the literature to achieve this. Our experiments show
that our approach performs better than other techniques on standard benchmarks.
However, on closer inspection, none of these techniques work on realistic
defects that we see in industry.
  We find that augmenting code with formal specifications enables LLMs to
generate higher-quality unit tests, especially for complex production code with
improved coverage of edge cases and exception handling. However, specifications
add little value for well-understood errors (e.g., null pointer, index out of
bounds), but are beneficial for logic and string manipulation errors. Despite
encouraging benchmark results, real-world adoption is limited since passing
tests do not guarantee correct patches. Current challenges include insufficient
expressiveness of the JML specification language, necessitating advanced
verification tools and richer predicates. Our ongoing work is exploring
contract automata, programming by example, and testcase repair, with a focus on
integrating human feedback and measuring productivity gains - highlighting the
gap between academic benchmarks and practical industry needs

</details>


### [6] [Validating Terrain Models in Digital Twins for Trustworthy sUAS Operations](https://arxiv.org/abs/2508.16104)
*Arturo Miguel Russell Bernal,Maureen Petterson,Pedro Antonio Alarcon Granadeno,Michael Murphy,James Mason,Jane Cleland-Huang*

Main category: cs.SE

TL;DR: The paper introduces a new, multi-dimensional process to robustly validate terrain models for drone operations, using real-world and simulated tests. The approach improves safety and reliability for systems that help drones navigate complex environments.


<details>
  <summary>Details</summary>
Motivation: Small unmanned aircraft systems (sUAS) are increasingly used in unfamiliar and complex environments, requiring accurate Environmental Digital Twins (EDT) for safe navigation and advanced capabilities like geolocation. Deploying sUAS in the real world, however, introduces uncertainty, making robust validation of EDT components (especially terrain models) essential.

Method: The paper proposes a three-dimensional validation process based on software engineering principles. This process evaluates terrain models by testing at varying granularities, transitioning from simulation to real-world conditions, and analyzing from basic to more extreme operational scenarios. Terrain models are created by fusing USGS datasets and satellite imagery with high-resolution environmental data, and validated in a multi-sUAS platform with a Terrain-Aware Digital Shadow.

Result: The proposed validation approach is demonstrated successfully on a multi-sUAS platform. The process addresses key validation challenges such as data granularity, terrain discontinuities, sensor inaccuracies, and operational constraints, facilitating robust terrain model validation for real-world sUAS operations.

Conclusion: A robust, software engineering-based three-dimension validation strategy for terrain models is practical and addresses real-world challenges encountered in sUAS missions, enhancing the reliability and operational safety of Environmental Digital Twins in fielded sUAS systems.

Abstract: With the increasing deployment of small Unmanned Aircraft Systems (sUAS) in
unfamiliar and complex environments, Environmental Digital Twins (EDT) that
comprise weather, airspace, and terrain data are critical for safe flight
planning and for maintaining appropriate altitudes during search and
surveillance operations. With the expansion of sUAS capabilities through edge
and cloud computing, accurate EDT are also vital for advanced sUAS
capabilities, like geolocation. However, real-world sUAS deployment introduces
significant sources of uncertainty, necessitating a robust validation process
for EDT components. This paper focuses on the validation of terrain models, one
of the key components of an EDT, for real-world sUAS tasks. These models are
constructed by fusing U.S. Geological Survey (USGS) datasets and satellite
imagery, incorporating high-resolution environmental data to support mission
tasks. Validating both the terrain models and their operational use by sUAS
under real-world conditions presents significant challenges, including limited
data granularity, terrain discontinuities, GPS and sensor inaccuracies, visual
detection uncertainties, as well as onboard resources and timing constraints.
We propose a 3-Dimensions validation process grounded in software engineering
principles, following a workflow across granularity of tests, simulation to
real world, and the analysis of simple to edge conditions. We demonstrate our
approach using a multi-sUAS platform equipped with a Terrain-Aware Digital
Shadow.

</details>


### [7] [The Fools are Certain; the Wise are Doubtful: Exploring LLM Confidence in Code Completion](https://arxiv.org/abs/2508.16131)
*Zoe Kotti,Konstantina Dritsa,Diomidis Spinellis,Panos Louridas*

Main category: cs.SE

TL;DR: This paper shows that code LLM confidence (measured by perplexity) varies most by programming language and model, with strongly-typed languages and Java being easiest for LLMs. Intrinsic metrics like perplexity reliably assess and compare model performance for code completion.


<details>
  <summary>Details</summary>
Motivation: The motivation is to evaluate intrinsic metrics such as perplexity to assess the confidence and utility of code LLMs for code completion tasks across languages and models, addressing limitations of existing, complex, and unreliable downstream metrics.

Method: They measured code perplexity across various programming languages, LLMs, and datasets by generating code on a sample of 1008 files from 657 GitHub projects, comparing perplexity values and their implications across languages, models, and the presence of code comments.

Result: Strongly-typed languages generally show lower perplexity than dynamically typed ones. Scripting languages and especially Perl exhibit higher perplexity, while Java is low. Perplexity is more dependent on LLM choice than on code dataset. Although comments increase perplexity, language ranking remains consistent.

Conclusion: Perplexity is a useful intrinsic metric for evaluating LLM confidence in code generation across languages and models. These findings help stakeholders gauge the suitability of LLM-based code completion for different projects, depending on language, model, and code characteristics.

Abstract: Code completion entails the task of providing missing tokens given a
surrounding context. It can boost developer productivity while providing a
powerful code discovery tool. Following the Large Language Model (LLM) wave,
code completion has been approached with diverse LLMs fine-tuned on code (code
LLMs). The performance of code LLMs can be assessed with downstream and
intrinsic metrics. Downstream metrics are usually employed to evaluate the
practical utility of a model, but can be unreliable and require complex
calculations and domain-specific knowledge. In contrast, intrinsic metrics such
as perplexity, entropy, and mutual information, which measure model confidence
or uncertainty, are simple, versatile, and universal across LLMs and tasks, and
can serve as proxies for functional correctness and hallucination risk in
LLM-generated code. Motivated by this, we evaluate the confidence of LLMs when
generating code by measuring code perplexity across programming languages,
models, and datasets using various LLMs, and a sample of 1008 files from 657
GitHub projects. We find that strongly-typed languages exhibit lower perplexity
than dynamically typed languages. Scripting languages also demonstrate higher
perplexity. Perl appears universally high in perplexity, whereas Java appears
low. Code perplexity depends on the employed LLM, but not on the code dataset.
Although code comments often increase perplexity, the language ranking based on
perplexity is barely affected by their presence. LLM researchers, developers,
and users can employ our findings to assess the benefits and suitability of
LLM-based code completion in specific software projects based on how language,
model choice, and code characteristics impact model confidence.

</details>


### [8] [Towards Recommending Usability Improvements with Multimodal Large Language Models](https://arxiv.org/abs/2508.16165)
*Sebastian Lubos,Alexander Felfernig,Gerhard Leitner,Julian Schwazer*

Main category: cs.SE

TL;DR: This paper explores using multimodal LLMs to automatically evaluate UI usability, comparing their recommendations to expert assessments. Results show potential, suggesting LLMs could make usability evaluation more accessible and less resource-dependent.


<details>
  <summary>Details</summary>
Motivation: Usability evaluation is important for improving human-computer interaction, but traditional methods are resource-intensive and require expert involvement, making them inaccessible for smaller organizations. There is a need for more efficient and widely accessible usability evaluation methods.

Method: The authors reframe usability evaluation as a recommendation task and use multimodal large language models (LLMs) to rank usability issues by severity. They conduct a proof-of-concept study comparing LLM-generated usability recommendations to those of human experts.

Result: The study found that LLMs can generate usability improvement recommendations that show promise when compared to expert assessments.

Conclusion: Multimodal LLMs have the potential to provide faster and more cost-effective usability evaluation, offering a practical alternative where expert resources are limited.

Abstract: Usability describes a set of essential quality attributes of user interfaces
(UI) that influence human-computer interaction. Common evaluation methods, such
as usability testing and inspection, are effective but resource-intensive and
require expert involvement. This makes them less accessible for smaller
organizations. Recent advances in multimodal LLMs offer promising opportunities
to automate usability evaluation processes partly by analyzing textual, visual,
and structural aspects of software interfaces. To investigate this possibility,
we formulate usability evaluation as a recommendation task, where multimodal
LLMs rank usability issues by severity. We conducted an initial
proof-of-concept study to compare LLM-generated usability improvement
recommendations with usability expert assessments. Our findings indicate the
potential of LLMs to enable faster and more cost-effective usability
evaluation, which makes it a practical alternative in contexts with limited
expert resources.

</details>


### [9] [LLM-Assisted Semantic Alignment and Integration in Collaborative Model-Based Systems Engineering Using SysML v2](https://arxiv.org/abs/2508.16181)
*Zirui Li,Stephan Husung,Haoze Wang*

Main category: cs.SE

TL;DR: This paper introduces a structured, prompt-driven method to help Large Language Models (like GPT) align the semantics of SysML v2 models for cross-organizational Model-Based Systems Engineering. The approach uses SysML v2 features and demonstrates improvements in semantic integration, although some limitations persist.


<details>
  <summary>Details</summary>
Motivation: Model-Based Systems Engineering (MBSE) increasingly involves cross-organization collaboration, which suffers from difficulties in achieving semantic alignment due to independent model development. Traditional approaches lack effective solutions for semantic interoperability, motivating exploration of new tools and methods.

Method: The authors propose a structured approach using Large Language Models (LLMs), specifically GPT-based, for semantic alignment of SysML v2 models. This prompt-driven method involves iteratively developing alignment strategies, using model extraction, semantic matching, and verification. Key SysML v2 features (alias, import, metadata) are leveraged to enable traceable, soft integration.

Result: The approach is demonstrated through an example measurement system model using a GPT-based LLM, which successfully assists in aligning system semantics. The method allows for structured, interactive semantic integration, with both advantages and limitations identified through the case study.

Conclusion: A prompt-driven, LLM-assisted semantic alignment strategy effectively supports the integration of independently developed SysML v2 models. SysML v2’s modular and formalized constructs, combined with LLM capabilities, can substantially aid semantic interoperability in MBSE. Some challenges and areas for improvement remain, as discussed.

Abstract: Cross-organizational collaboration in Model-Based Systems Engineering (MBSE)
faces many challenges in achieving semantic alignment across independently
developed system models. SysML v2 introduces enhanced structural modularity and
formal semantics, offering a stronger foundation for interoperable modeling.
Meanwhile, GPT-based Large Language Models (LLMs) provide new capabilities for
assisting model understanding and integration. This paper proposes a
structured, prompt-driven approach for LLM-assisted semantic alignment of SysML
v2 models. The core contribution lies in the iterative development of an
alignment approach and interaction prompts, incorporating model extraction,
semantic matching, and verification. The approach leverages SysML v2 constructs
such as alias, import, and metadata extensions to support traceable, soft
alignment integration. It is demonstrated with a GPT-based LLM through an
example of a measurement system. Benefits and limitations are discussed.

</details>


### [10] [A Systematic Mapping Study on Smart Cities Modeling Approaches](https://arxiv.org/abs/2508.16273)
*Maria Teresa Rossi,Martina De Sanctis,Ludovico Iovino,Manuel Wimmer*

Main category: cs.SE

TL;DR: This paper systematically maps modeling approaches for smart cities, finding governance is the most studied aspect, models often remain untested in practice, and publications are highly dispersed. Results guide future research and help understand current methods.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the complexity and interdisciplinary nature of smart cities research, specifically by investigating how smart cities and their subsystems are designed and modeled across various domains.

Method: The method employed is a systematic mapping study following Petersen et al.'s guidelines, analyzing published literature on smart city modeling approaches to categorize trends and directions.

Result: The analysis revealed that smart governance is the most researched dimension, business/architectural/ontological modeling are the predominant approaches, most technologies remain untested in real-world environments, and research outputs are widely dispersed across venues.

Conclusion: The study provides an overview of the state-of-the-art in smart city modeling, helping researchers navigate the interdisciplinary field and laying groundwork for future targeted studies, particularly for the Model-Driven Engineering community.

Abstract: The Smart City concept was introduced to define an idealized city
characterized by automation and connection. It then evolved rapidly by
including further aspects, such as economy, environment. Since then, many
publications have explored various aspects of Smart Cities across different
application domains and research communities, acknowledging the
interdisciplinary nature of this subject. In particular, our interest focuses
on how smart cities are designed and modeled, as a whole or as regards with
their subsystems, when dealing with the accomplishment of the research goals in
this complex and heterogeneous domain. To this aim, we performed a systematic
mapping study on smart cities modeling approaches identifying the relevant
contributions (i) to get an overview of existing research approaches, (ii) to
identify whether there are any publication trends, and (iii) to identify
possible future research directions. We followed the guidelines for conducting
systematic mapping studies by Petersen et al. to analyze smart cities modeling
publications. Our analysis revealed the following main findings: (i) smart
governance is the most investigated and modeled smart city dimension; (ii) the
most used modeling approaches are business, architectural, and ontological
modeling approaches, spanning multiple application fields; (iii) the great
majority of existing technologies for modeling smart cities are not yet proven
in operational environments; (iv) diverse research communities publish their
results in a multitude of different venues which further motivates the
presented literature study. Researchers can use our results for better
understanding the state-of-the-art in modeling smart cities, and as a
foundation for further analysis of specific approaches about smart cities
modeling. Lastly, we also discuss the impact of our analysis for the
Model-Driven Engineering community.

</details>


### [11] [Metamorphic Coverage](https://arxiv.org/abs/2508.16307)
*Jinsheng Ba,Yuancheng Jiang,Manuel Rigger*

Main category: cs.SE

TL;DR: The paper introduces Metamorphic Coverage (MC), a new metric specifically designed for metamorphic testing. MC better correlates with bug detection, distinguishes between testing methods more effectively, and is far less computationally expensive than mutation testing or line coverage. Empirical results show MC both identifies more bug-prone code and enables more efficient and effective test-case generation, making it a valuable tool in software testing.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitations of existing code coverage and mutation testing metrics when evaluating the effectiveness of metamorphic testing methods. Code coverage cannot precisely measure the code truly validated by tests, while mutation testing is prohibitively expensive, motivating the need for a better, more efficient metric.

Method: The paper proposes Metamorphic Coverage (MC), a new coverage metric that assesses the distinct code exercised by pairs of test inputs in metamorphic testing. The authors systematically evaluate MC on five widely used metamorphic testing techniques across domains such as database engines, compilers, and constraint solvers. Empirical studies compare MC with traditional line coverage and mutation testing in terms of correlation with real bug detection, sensitivity, and computational cost.

Result: MC overlapped with the bug-fix locations of 50 out of 64 bugs and showed a stronger positive correlation with bug numbers than line coverage. MC was found to be 4 times more sensitive than line coverage for distinguishing test method effectiveness, requires significantly less computational resources than mutation testing (359x less), and is able to guide testing more effectively (41% more bugs found in a case study). It captures the most relevant program areas with a much smaller metric value than line coverage.

Conclusion: Metamorphic Coverage is a more precise, sensitive, and efficient metric than traditional code coverage and mutation testing for evaluating metamorphic testing methods. MC not only overlaps well with real bug locations, better guides test generation, and distinguishes testing method effectiveness, but also drastically reduces computational effort needed to assess test quality. This metric has significant potential to advance the practical evaluation of metamorphic testing and automated test-case generation.

Abstract: Metamorphic testing is a widely used methodology that examines an expected
relation between pairs of executions to automatically find bugs, such as
correctness bugs. We found that code coverage cannot accurately measure the
extent to which code is validated and mutation testing is computationally
expensive for evaluating metamorphic testing methods. In this work, we propose
Metamorphic Coverage (MC), a coverage metric that examines the distinct code
executed by pairs of test inputs within metamorphic testing. Our intuition is
that, typically, a bug can be observed if the corresponding code is executed
when executing either test input but not the other one, so covering more
differential code covered by pairs of test inputs might be more likely to
expose bugs. While most metamorphic testing methods have been based on this
general intuition, our work defines and systematically evaluates MC on five
widely used metamorphic testing methods for testing database engines,
compilers, and constraint solvers. The code measured by MC overlaps with the
bug-fix locations of 50 of 64 bugs found by metamorphic testing methods, and MC
has a stronger positive correlation with bug numbers than line coverage. MC is
4x more sensitive than line coverage in distinguishing testing methods'
effectiveness, and the average value of MC is 6x smaller than line coverage
while still capturing the part of the program that is being tested. MC required
359x less time than mutation testing. Based on a case study for an automated
database system testing approach, we demonstrate that when used for feedback
guidance, MC significantly outperforms code coverage, by finding 41\% more
bugs. Consequently, this work might have broad applications for assessing
metamorphic testing methods and improving test-case generation.

</details>


### [12] [SATORI: Static Test Oracle Generation for REST APIs](https://arxiv.org/abs/2508.16318)
*Juan C. Alonso,Alberto Martin-Lopez,Sergio Segura,Gabriele Bavota,Antonio Ruiz-Cortés*

Main category: cs.SE

TL;DR: REST API test tools usually lack rich test oracles. SATORI uses large language models on OpenAPI specs to infer test oracles, converting them into executable assertions. SATORI outperforms dynamic approaches, finds many real bugs, and, when combined with other tools, covers most ground-truth oracles for tested APIs.


<details>
  <summary>Details</summary>
Motivation: Current REST API test case generation tools are good at generating test data, but lack support for diverse test oracles, limiting verification to crashes, regressions, and specification violations. There is a need for approaches that can generate richer test oracles automatically.

Method: The paper presents SATORI, a black-box tool that uses large language models to infer test oracles by analyzing the OpenAPI Specification of REST APIs, particularly the names and descriptions of response fields. SATORI integrates with PostmanAssertify to convert inferred oracles into executable assertions. SATORI's effectiveness is evaluated on multiple industrial APIs, and compared to AGORA+, a dynamic oracle inference method.

Result: SATORI automatically generated hundreds of valid oracles per API operation, achieving an F1-score of 74.3%, outperforming AGORA+ (69.3%). Combining both static (SATORI) and dynamic (AGORA+) methods resulted in coverage of 90% of ground-truth oracles. SATORI found 18 undocumented bugs, prompting real updates to API documentation.

Conclusion: SATORI substantially advances REST API test oracle generation by providing a scalable, automated, and effective solution leveraging static analysis and LLMs. Its generated oracles are both plentiful and high-quality, and complement dynamic methods. SATORI's real-world impact is demonstrated by bug discoveries and documentation improvements in major APIs.

Abstract: REST API test case generation tools are evolving rapidly, with growing
capabilities for the automated generation of complex tests. However, despite
their strengths in test data generation, these tools are constrained by the
types of test oracles they support, often limited to crashes, regressions, and
noncompliance with API specifications or design standards. This paper
introduces SATORI (Static API Test ORacle Inference), a black-box approach for
generating test oracles for REST APIs by analyzing their OpenAPI Specification.
SATORI uses large language models to infer the expected behavior of an API by
analyzing the properties of the response fields of its operations, such as
their name and descriptions. To foster its adoption, we extended the
PostmanAssertify tool to automatically convert the test oracles reported by
SATORI into executable assertions. Evaluation results on 17 operations from 12
industrial APIs show that SATORI can automatically generate up to hundreds of
valid test oracles per operation. SATORI achieved an F1-score of 74.3%,
outperforming the state-of-the-art dynamic approach AGORA+ (69.3%)-which
requires executing the API-when generating comparable oracle types. Moreover,
our findings show that static and dynamic oracle inference methods are
complementary: together, SATORI and AGORA+ found 90% of the oracles in our
annotated ground-truth dataset. Notably, SATORI uncovered 18 bugs in popular
APIs (Amadeus Hotel, Deutschebahn, FDIC, GitLab, Marvel, OMDb and Vimeo)
leading to documentation updates by the API maintainers.

</details>


### [13] [The (C)omprehensive (A)rchitecture (P)attern (I)ntegration method: Navigating the sea of technology](https://arxiv.org/abs/2508.16341)
*Sebastian Copei,Oliver Hohlfeld,Jens Kosiol*

Main category: cs.SE

TL;DR: CAPI, a decision-tree based method, makes technology selection in software projects easier by recommending architectural patterns instead of specific tools, leading to less complexity and better decision-making, as validated by studies with academics and industry professionals.


<details>
  <summary>Details</summary>
Motivation: Technology selection and architectural design are increasingly complex due to the rapidly changing technological landscape and the vast number of available tools. This paper aims to address the difficulty in making informed decisions for large-scale software projects.

Method: The authors introduce CAPI, a Comprehensive Architecture Pattern Integration method. CAPI utilizes a diagnostic decision tree to suggest relevant architectural patterns based on user needs, thereby reducing complexity in technology selection. The method was developed iteratively and evaluated through small studies with academic participants, followed by a user-study involving industry representatives.

Result: The study found that technology selection is frequently conducted by trial and error. CAPI was seen as uniformly helpful by users and was capable of recreating the productive architectural environments experienced by participants.

Conclusion: CAPI simplifies technology selection in software projects by focusing on architectural patterns rather than individual tools, reducing decision complexity and improving outcomes for both academic and industry participants.

Abstract: The technological landscape changes daily, making it nearly impossible for a
single person to be aware of all trends or available tools that may or may not
be suitable for their software project. This makes tool selection and
architectural design decisions a complex problem, especially for large-scale
software systems. To tackle this issue, we introduce CAPI, the Comprehensive
Architecture Pattern Integration method that uses a diagnostic decision tree to
suggest architectural patterns depending on user needs. By suggesting patterns
instead of tools, the overall complexity for further decisions is lower as
there are fewer architectural patterns than tools due to the abstract nature of
patterns. Moreover, since tools implement patterns, each non-proposed pattern
reduces the number of tools to choose from, reducing complexity. We iteratively
developed CAPI, evaluating its understandability and usability in small studies
with academic participants. When satisfied with the outcome, we performed a
user-study with industry representatives to investigate the state-of-the-art in
technology selection and the effectiveness of our proposed method. We find that
technology selection is largely performed via trial and error, that CAPI is
uniformly perceived as helpful, and that CAPI is able to reproduce the
productive architectural environments of our participants.

</details>


### [14] [AetherCode: Evaluating LLMs' Ability to Win In Premier Programming Competitions](https://arxiv.org/abs/2508.16402)
*Zihan Wang,Jiaze Chen,Zhicheng Liu,Markus Mak,Yidi Du,Geonsik Moon,Luoqi Xu,Aaron Tua,Kunshuo Peng,Jiayi Lu,Mingfei Xia,Boqian Zou,Chenyang Ran,Guang Tian,Shoutai Zhu,Yeheng Duan,Zhenghui Kang,Zhenxing Lin,Shangshu Li,Qiang Luo,Qingshen Long,Zhiyong Chen,Yihan Xiao,Yurong Wu,Daoguang Zan,Yuyi Fu,Mingxuan Wang,Ming Ding*

Main category: cs.SE

TL;DR: Previous benchmarks for coding by LLMs are too easy and biased. AetherCode introduces tougher problems from real competitions and high-quality test sets, providing a truer test of LLM code reasoning.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for evaluating LLMs in competitive programming are too easy and contain biases due to low-quality test cases. This overestimates LLM abilities compared to top human programmers, masking the real performance gap.

Method: The authors introduce AetherCode, a new benchmark based on challenging problems from leading competitions (IOI, ICPC) and backed by thorough, expert-validated test suites created via automated and manual methods.

Result: AetherCode delivers broader problem coverage, higher difficulty, and more rigorous evaluation compared to previous benchmarks, resulting in a more accurate assessment of LLM performance in coding tasks.

Conclusion: AetherCode addresses key flaws in previous benchmarks and sets a new, higher standard for evaluating code reasoning skills in LLMs, better reflecting their true capabilities and limitations.

Abstract: Competitive programming has emerged as a critical benchmark for evaluating
the reasoning and coding capabilities of Large Language Models (LLMs). Despite
impressive progress on existing benchmarks, we argue that current evaluations
overstate model proficiency, masking a substantial gap between LLMs and elite
human programmers. This gap arises from two key limitations: insufficient
difficulty and scope of benchmark problems, and evaluation bias from
low-quality test cases. To address these shortcomings, we present AetherCode, a
new benchmark that draws problems from premier programming competitions such as
IOI and ICPC, offering broader coverage and higher difficulty. AetherCode
further incorporates comprehensive, expert-validated test suites built through
a hybrid of automated generation and human curation, ensuring rigorous and
reliable assessment. By combining challenging problem design with robust
evaluation, AetherCode provides a more faithful measure of LLM capabilities and
sets a new standard for future research in code reasoning.

</details>


### [15] [LLM-GUARD: Large Language Model-Based Detection and Repair of Bugs and Security Vulnerabilities in C++ and Python](https://arxiv.org/abs/2508.16419)
*Akshay Mhatre,Noujoud Nader,Patrick Diehl,Deepti Gupta*

Main category: cs.SE

TL;DR: The study benchmarks ChatGPT-4, Claude 3, and LLaMA 4 for bug and vulnerability detection in C++/Python code. LLMs excel at basic error detection, performing best as educational or first-pass review tools, but struggle with advanced security flaws. ChatGPT-4 and Claude 3 are generally more context-aware than LLaMA 4, revealing current limitations in LLM-driven code analysis.


<details>
  <summary>Details</summary>
Motivation: With the rise of LLMs like ChatGPT-4, Claude 3, and LLaMA 4 in software development, the study aims to assess how effectively these models detect a range of software bugs, especially complex, security-relevant vulnerabilities, an area that lacks systematic evaluation.

Method: The authors systematically and empirically evaluate three leading LLMs on a curated benchmark involving foundational errors, classic security flaws, and advanced bugs in C++ and Python, sourced from SEED Labs, OpenSSL, and PyBugHive, validated through compilation/testing. A context-aware, multi-stage prompting protocol simulates realistic debugging scenarios, with a graded rubric used to measure accuracy, reasoning, and remediation.

Result: All models perform well at identifying basic syntactic and semantic mistakes, showing promise for educational contexts and preliminary code audits. Their effectiveness drops significantly for complex security flaws and large-scale production bugs, with ChatGPT-4 and Claude 3 exhibiting stronger contextual understanding than LLaMA 4.

Conclusion: While LLMs demonstrate potential as tools for code analysis, especially for educational purposes and simple bug detection, they are currently limited in fully addressing complex vulnerabilities and real-world production needs. Their reliability as advanced code auditing tools is constrained.

Abstract: Large Language Models (LLMs) such as ChatGPT-4, Claude 3, and LLaMA 4 are
increasingly embedded in software/application development, supporting tasks
from code generation to debugging. Yet, their real-world effectiveness in
detecting diverse software bugs, particularly complex, security-relevant
vulnerabilities, remains underexplored. This study presents a systematic,
empirical evaluation of these three leading LLMs using a benchmark of
foundational programming errors, classic security flaws, and advanced,
production-grade bugs in C++ and Python. The dataset integrates real code from
SEED Labs, OpenSSL (via the Suresoft GLaDOS database), and PyBugHive, validated
through local compilation and testing pipelines. A novel multi-stage,
context-aware prompting protocol simulates realistic debugging scenarios, while
a graded rubric measures detection accuracy, reasoning depth, and remediation
quality. Our results show that all models excel at identifying syntactic and
semantic issues in well-scoped code, making them promising for educational use
and as first-pass reviewers in automated code auditing. Performance diminishes
in scenarios involving complex security vulnerabilities and large-scale
production code, with ChatGPT-4 and Claude 3 generally providing more nuanced
contextual analyses than LLaMA 4. This highlights both the promise and the
present constraints of LLMs in serving as reliable code analysis tools.

</details>


### [16] [Using LLMs and Essence to Support Software Practice Adoption](https://arxiv.org/abs/2508.16445)
*Sonia Nicoletti,Paolo Ciancarini*

Main category: cs.SE

TL;DR: A chatbot combining the Essence software engineering framework with LLMs and a retrieval-augmented generation system yields better support for practical software engineering tasks than general-purpose LLMs, improving access to best practices, though more user studies are needed.


<details>
  <summary>Details</summary>
Motivation: Although NLP and AI have achieved progress in domains like code generation, little work has facilitated the adoption of best practices or monitored process health in software engineering. The motivation is to bridge this gap by making theoretical frameworks like Essence more accessible for practical application.

Method: The authors developed a specialized chatbot integrating the Essence framework with large language models (LLMs), using a retrieval-augmented generation (RAG) system to enhance context retrieval. They tested four different LLMs, each both as a base model and with RAG augmentation. Evaluation focused on the contextual relevance of retrieved information and the quality of generated responses.

Result: The proposed chatbot system, especially when augmented with the RAG system, consistently outperformed baseline general-purpose LLMs in domain-specific tasks, delivering more relevant context and higher-quality responses for software engineering practice adoption.

Conclusion: Integrating Essence with LLMs via a RAG-augmented chatbot provides effective automated support for learning and applying software engineering practices. This bridges theoretical and practical gaps, potentially improving process management. Further user-based validation is suggested.

Abstract: Recent advancements in natural language processing (NLP) have enabled the
development of automated tools that support various domains, including software
engineering. However, while NLP and artificial intelligence (AI) research has
extensively focused on tasks such as code generation, less attention has been
given to automating support for the adoption of best practices, the evolution
of ways of working, and the monitoring of process health. This study addresses
this gap by exploring the integration of Essence, a standard and thinking
framework for managing software engineering practices, with large language
models (LLMs). To this end, a specialised chatbot was developed to assist
students and professionals in understanding and applying Essence. The chatbot
employs a retrieval-augmented generation (RAG) system to retrieve relevant
contextual information from a curated knowledge base. Four different LLMs were
used to create multiple chatbot configurations, each evaluated both as a base
model and augmented with the RAG system. The system performance was evaluated
through both the relevance of retrieved context and the quality of generated
responses. Comparative analysis against the general-purpose LLMs demonstrated
that the proposed system consistently outperforms its baseline counterpart in
domain-specific tasks. By facilitating access to structured software
engineering knowledge, this work contributes to bridging the gap between
theoretical frameworks and practical application, potentially improving process
management and the adoption of software development practices. While further
validation through user studies is required, these findings highlight the
potential of LLM-based automation to enhance learning and decision-making in
software engineering.

</details>


### [17] [How Small is Enough? Empirical Evidence of Quantized Small Language Models for Automated Program Repair](https://arxiv.org/abs/2508.16499)
*Kazuki Kusama,Honglin Shu,Masanari Kondo,Yasutaka Kamei*

Main category: cs.SE

TL;DR: Small language models are as accurate as large ones in automated program repair tasks and require less computational power. Memory can be reduced further with quantization, without affecting performance.


<details>
  <summary>Details</summary>
Motivation: LLMs have improved APR but require high computational resources. The authors are motivated to investigate whether smaller, more resource-efficient models (SLMs) can perform competitively in automated program repair tasks.

Method: Experiments were conducted using the QuixBugs benchmark to compare the bug-fixing accuracy of SLMs and LLMs. Additionally, the impact of int8 quantization on APR performance and memory usage was analyzed.

Result: Latest SLMs can fix bugs with accuracy comparable to or exceeding LLMs. Int8 quantization substantially reduces memory requirements but minimally affects APR accuracy.

Conclusion: SLMs are a viable alternative to LLMs for automated program repair, providing similar or better accuracy with lower resource requirements. Quantization further increases efficiency without sacrificing effectiveness.

Abstract: Background: Large language models (LLMs) have greatly improved the accuracy
of automated program repair (APR) methods. However, LLMs are constrained by
high computational resource requirements. Aims: We focus on small language
models (SLMs), which perform well even with limited computational resources
compared to LLMs. We aim to evaluate whether SLMs can achieve competitive
performance in APR tasks. Method: We conducted experiments on the QuixBugs
benchmark to compare the bug-fixing accuracy of SLMs and LLMs. We also analyzed
the impact of int8 quantization on APR performance. Results: The latest SLMs
can fix bugs as accurately as--or even more accurately than--LLMs. Also, int8
quantization had minimal effect on APR accuracy while significantly reducing
memory requirements. Conclusions: SLMs present a viable alternative to LLMs for
APR, offering competitive accuracy with lower computational costs, and
quantization can further enhance their efficiency without compromising
effectiveness.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [18] [Correctness-Guaranteed Code Generation via Constrained Decoding](https://arxiv.org/abs/2508.15866)
*Lingxiao Li,Salar Rahili,Yiwei Zhao*

Main category: cs.PL

TL;DR: The paper proposes a new constrained decoding approach using a context-sensitive parser for language models, enabling the generation of guaranteed correct code, validated in a strongly typed Lua setting for game development tasks.


<details>
  <summary>Details</summary>
Motivation: There is a pressing need for code generation methods that guarantee correctness in domains like video games and robotics, where errors in generated code are unacceptable at runtime. Existing language models often produce imperfect code, but critical applications require one-shot correctness.

Method: The authors introduce a constrained decoding algorithm that integrates a context-sensitive parser. This parser outputs a regular expression at each generation step, which enforces a critical property ensuring that the next token is always part of a potentially correct program. The parser is implemented as a dynamic tree of modular, context-enriched context-free grammars, enabling handling of code ambiguity and context (e.g., variable scopes, type constraints) during parsing.

Result: They validate their approach using sLua, a strongly-typed Lua variant, demonstrating the ability to generate semantically correct programs for various scripting APIs. The semantic guarantees were tested in a roguelike video game, showing runtime correctness of generated game mechanics.

Conclusion: The presented method successfully guides code generation towards semantically and runtime-correct programs, making it suitable for safety-critical coding domains like robotics and game engines. The approach is practical, generalizable, and improves over traditional language model generation by enforcing stronger correctness guarantees.

Abstract: Language Models (LMs) are increasingly being used for code generation, but
ensuring the correctness of generated programs remains a significant challenge.
Although imperfect code may be acceptable during software development with
human oversight, domains such as video games and robotics require one-shot
correctness for runtime-critical components. We present a constrained decoding
algorithm for generating semantically correct programs that incorporates a
context-sensitive parser, which, at each step, outputs a regular expression
that satisfies a critical non-extensible property to guide the generation of
the next token sequence that can continue to a correct program. To build such a
context-sensitive parser, we propose a framework of a dynamic tree of parsers
(ToP) during parsing, where each parser corresponds to a modular context-free
grammar enriched with contextual information such as variable scopes and type
constraints, with tree branches representing ambiguity in the future code
segment. We demonstrate our approach through sLua, a strongly typed variant of
Lua, showing that our method can generate semantically correct programs
conforming to any prescribed scripting API. We further show that, with careful
design, our semantic guarantees extend to runtime correctness, as validated in
the application of generating game mechanics for a roguelike video game.

</details>


### [19] [Automated Formal Verification of a Software Fault Isolation System](https://arxiv.org/abs/2508.15898)
*Matthew Sotoudeh,Zachary Yedidia*

Main category: cs.PL

TL;DR: Formal methods are used to prove that the LFI SFI verifier does not accept programs that could access memory outside the sandbox, significantly strengthening SFI security guarantees.


<details>
  <summary>Details</summary>
Motivation: Software Fault Isolation (SFI) is crucial for sandboxing untrusted code, but relies heavily on the verifier's correctness. If the verifier has bugs, the security promises of SFI can be void, allowing untrusted code access to protected memory. This potential vulnerability motivates the authors to formally analyze the verifier for a recent SFI system.

Method: The authors perform automated formal verification on the Lightweight Fault Isolation (LFI) system's verifier. They use formal methods to prove that programs accepted by the LFI verifier cannot access memory outside their allocated sandbox.

Result: They successfully verify that programs accepted by the LFI verifier are memory-safe with respect to the sandbox boundary—in other words, such programs never read or write memory outside the designated region.

Conclusion: Automated formal verification can assure the soundness of SFI verifiers such as that in LFI, thereby eliminating a critical security risk.

Abstract: Software fault isolation (SFI) is a popular way to sandbox untrusted
software. A key component of SFI is the verifier that checks the untrusted code
is written in a subset of the machine language that guarantees it never reads
or writes outside of a region of memory dedicated to the sandbox. Soundness
bugs in the SFI verifier would break the SFI security model and allow the
supposedly sandboxed code to read protected memory. In this paper, we address
the concern of SFI verifier bugs by performing an automated formal verification
of a recent SFI system called Lightweight Fault Isolation (LFI). In particular,
we formally verify that programs accepted by the LFI verifier never read or
write to memory outside of a designated sandbox region.

</details>


### [20] [Synthesizing DSLs for Few-Shot Learning](https://arxiv.org/abs/2508.16063)
*Paul Krogmeier,P. Madhusudan*

Main category: cs.PL

TL;DR: This work proves that it's possible to automatically synthesize DSLs for few-shot learning in symbolic domains under certain conditions, leveraging tree automata. They also handle some relaxed problem settings.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable automatic generation of efficient and expressive domain-specific languages tailored to few-shot learning problems in symbolic domains, ensuring small and generalizable solutions.

Method: The authors use formal language theory, specifically tree automata and grammar analysis, to study the decidability of the DSL synthesis problem. They analyze the structure of languages and grammars conducive to decidable synthesis and establish theoretical results based on these methods.

Result: The paper proves that the DSL synthesis problem is decidable for a class of languages evaluated by tree automata when expression size corresponds to parse tree depth. The set of suitable grammars is regular. They also demonstrate decidability for weaker variants of the problem.

Conclusion: The paper establishes that synthesizing DSLs for few-shot learning is decidable under certain conditions, and grammars solving the problem form a regular set. Additional decidability results are provided for problem variants involving macro grammars and relaxed requirements.

Abstract: We study the problem of synthesizing domain-specific languages (DSLs) for
few-shot learning in symbolic domains. Given a base language and instances of
few-shot learning problems, where each instance is split into training and
testing samples, the DSL synthesis problem asks for a grammar over the base
language that guarantees that small expressions solving training samples also
solve corresponding testing samples. We prove that the problem is decidable for
a class of languages whose semantics over fixed structures can be evaluated by
tree automata and when expression size corresponds to parse tree depth in the
grammar, and, furthermore, the grammars solving the problem correspond to a
regular set of trees. We also prove decidability results for variants of the
problem where DSLs are only required to express solutions for input learning
problems and where DSLs are defined using macro grammars.

</details>


### [21] [Leveraging Large Language Models to Detect Missed Peephole Optimizations](https://arxiv.org/abs/2508.16125)
*Zhenyang Xu,Hongxu Xu,Yongqiang Tian,Xintong Zhou,Chengnian Sun*

Main category: cs.PL

TL;DR: Lampo uses LLMs and formal verification to automate detection of missed peephole optimizations in LLVM, outperforming previous tools and helping improve code quality in compilers.


<details>
  <summary>Details</summary>
Motivation: Peephole optimization is a crucial compiler technique that improves code by replacing inefficient instruction sequences with optimized equivalents. However, discovering new, effective peephole optimizations is difficult because of the complexity and diversity of instruction sets. Existing methods scale poorly or only handle a limited range of optimizations. Thus, a better, scalable solution is needed.

Method: The authors propose Lampo, an automated framework that uses Large Language Models (LLMs) to suggest new code optimizations and utilizes translation validation tools to rigorously check correctness. Lampo operates in a feedback-driven iterative manner to enhance the reliability of LLM-generated optimizations.

Result: Lampo detected up to 17 out of 25 previously reported missed peephole optimizations in LLVM on average, and potentially up to 22 out of 25 depending on the LLM used. By comparison, Souper, the state-of-the-art superoptimizer for LLVM, found 15. Over a seven-month period, Lampo discovered 26 missed optimizations, 15 confirmed and 6 already fixed.

Conclusion: Lampo significantly improves the detection of missed peephole optimizations, outperforming state-of-the-art approaches and showing promise as a continuous discovery tool for compiler optimization.

Abstract: By replacing small, suboptimal instruction sequences within programs with a
more efficient equivalent, peephole optimization can not only directly optimize
code size and performance, but also potentially enables further transformations
in the subsequent optimization pipeline. Although peephole optimization is a
critical class of compiler optimizations, discovering new and effective
peephole optimizations is challenging as the instruction sets can be extremely
complex and diverse. Previous methods either do not scale well or can only
capture a limited subset of peephole optimizations. In this work, we leverage
Large Language Models (LLMs) to detect missed peephole optimizations. We
propose Lampo, a novel automated framework that synergistically combines the
creative but unreliable code optimization ability of LLMs with rigorous
correctness verification performed by translation validation tools, integrated
in a feedback-driven iterative process. Through a comprehensive evaluation
within LLVM ecosystems, we show that Lampo can successfully detect up to 17 out
of 25 previously reported missed optimizations in LLVM on average, and that 22
out of 25 can potentially be found by Lampo with different LLMs. For
comparison, the state-of-the-art superoptimizer for LLVM, Souper, identified 15
of them. Moreover, within seven months of development and intermittent
experiments, Lampo found 26 missed peephole optimizations, 15 of which have
been confirmed and 6 already fixed. These results demonstrate Lampo's strong
potential in continuously detecting missed peephole optimizations.

</details>


### [22] [On the Duality of Task and Actor Programming Models](https://arxiv.org/abs/2508.16522)
*Rohan Yadav,Joseph Guman,Sean Treichler,Michael Garland,Alex Aiken,Fredrik Kjolstad,Michael Bauer*

Main category: cs.PL

TL;DR: The paper shows that task-based and actor-based distributed programming models are duals, and by applying specific optimizations, task-based runtimes (Realm and Legion) can achieve performance close to actor-based systems like Charm++ and MPI, while retaining higher productivity.


<details>
  <summary>Details</summary>
Motivation: Modern workloads often run on distributed and heterogeneous machines, requiring programming models that balance productivity and performance. Task-based and actor-based models are widely used, each with distinct advantages and limitations. The motivation is to understand these differences and explore if their benefits can be combined.

Method: The authors analyze the duality between task-based and actor-based programming models, comparing their functional and performance characteristics. They introduce techniques to make task-based runtimes more competitive in performance with actor-based systems, then implement these optimizations in Realm and Legion.

Result: Performance overhead in the Realm runtime was reduced by 1.7–5.3x, achieving overheads within a factor of two compared to optimized actor-based systems like Charm++ and MPI. The techniques also improved strong scaling in Legion applications by 1.3–5.0x without modifying the applications.

Conclusion: Task-based and actor-based programming models, despite superficial differences, are fundamentally duals. Through proposed techniques, task-based systems can approach or match the performance of actor-based systems without losing their advantages in productivity. This enables developers to choose models for productivity while getting close to peak performance.

Abstract: Programming models for distributed and heterogeneous machines are rapidly
growing in popularity to meet the demands of modern workloads. Task and actor
models are common choices that offer different trade-offs between development
productivity and achieved performance. Task-based models offer better
productivity and composition of software, whereas actor-based models routinely
deliver better peak performance due to lower overheads. While task-based and
actor-based models appear to be different superficially, we demonstrate these
programming models are duals of each other. Importantly, we show that this
duality extends beyond functionality to performance, and elucidate techniques
that let task-based systems deliver performance competitive with actor-based
systems without compromising productivity. We apply these techniques to both
Realm, an explicitly parallel task-based runtime, as well as Legion, an
implicitly parallel task-based runtime. We show these techniques reduce Realm's
overheads by between 1.7-5.3x, coming within a factor of two of the overheads
imposed by heavily optimized actor-based systems like Charm++ and MPI. We
further show that our techniques enable between 1.3-5.0x improved strong
scaling of unmodified Legion applications.

</details>
