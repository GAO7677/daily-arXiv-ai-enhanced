{"id": "2506.10984", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10984", "abs": "https://arxiv.org/abs/2506.10984", "authors": ["Ahilan Ayyachamy Nadar Ponnusamy"], "title": "Application Modernization with LLMs: Addressing Core Challenges in Reliability, Security, and Quality", "comment": null, "summary": "AI-assisted code generation tools have revolutionized software development,\noffering unprecedented efficiency and scalability. However, multiple studies\nhave consistently highlighted challenges such as security vulnerabilities,\nreliability issues, and inconsistencies in the generated code. Addressing these\nconcerns is crucial to unlocking the full potential of this transformative\ntechnology. While advancements in foundational and code-specialized language\nmodels have made notable progress in mitigating some of these issues,\nsignificant gaps remain, particularly in ensuring high-quality, trustworthy\noutputs.\n  This paper builds upon existing research on leveraging large language models\n(LLMs) for application modernization. It explores an opinionated approach that\nemphasizes two core capabilities of LLMs: code reasoning and code generation.\nThe proposed framework integrates these capabilities with human expertise to\ntackle application modernization challenges effectively. It highlights the\nindispensable role of human involvement and guidance in ensuring the success of\nAI-assisted processes.\n  To demonstrate the framework's utility, this paper presents a detailed case\nstudy, walking through its application in a real-world scenario. The analysis\nincludes a step-by-step breakdown, assessing alternative approaches where\napplicable. This work aims to provide actionable insights and a robust\nfoundation for future research in AI-driven application modernization. The\nreference implementation created for this paper is available on GitHub.", "AI": {"tldr": "This paper proposes and tests a framework that blends large language models' code reasoning/generation abilities with essential human oversight to improve AI-assisted application modernization, demonstrating its value through a real-world case study and providing an open-source implementation.", "motivation": "AI-assisted code generation tools have improved software development efficiency, but face critical challenges like security vulnerabilities, reliability issues, and inconsistent output. Addressing these issues is essential for realizing the technology's full potential, especially as existing models still leave gaps in trustworthy, high-quality code.", "method": "The paper builds on prior research using large language models (LLMs) for application modernization. It proposes a framework that integrates two main LLM capabilities\u2014code reasoning and code generation\u2014with active human involvement to boost effectiveness and trustworthiness. The framework's application is demonstrated via a real-world case study with detailed step-by-step analysis and comparisons to alternative methods.", "result": "The framework effectively combines human expertise with LLM capabilities to address application modernization challenges, improving quality and trustworthiness. The practical utility of this approach is demonstrated in a real-world scenario, with detailed methodology provided. A reference implementation is available for public use on GitHub.", "conclusion": "AI-assisted code generation can be made more reliable and trustworthy by merging advanced LLM features with human guidance. This combined approach addresses existing limitations and serves as a strong foundation for future work in AI-driven application modernization."}}
{"id": "2506.10985", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10985", "abs": "https://arxiv.org/abs/2506.10985", "authors": ["Raman Mohammed Hussein", "Bryar A. Hassan"], "title": "Collaboration Tools and their Role in Agile Software Projects", "comment": "https://www.middleeastconference.org/_files/ugd/614b1f_82fa5f91169a44278723a921b27e2864.pdf\n  ISBN: 979-8-89695-015-8", "summary": "The purpose of this review is to understand the importance of collaboration\ntools which are Slack, Microsoft Teams, Confluence in Agile and software\nprojects. Agile methodologies rely on flexibility, using cycles and integration\nthroughout various levels of developing cycles. However, it is still a great\nproblem for many teams to collaborate and communicate even if staff members and\nteams are working remotely. In terms of collaboration, the applications and\ntechnologies mean better organization of work, increased mutually\nunderstandable openness and fast and efficient inter team and interpersonal\ninteractions to enhance results of projects into productivity. This paper\nexamines how these tools fit the Agile principles, how they facilitate\niterative development, and encouraging effective initiation and tracking of\ntasks in small and large projects. The insights focus on how Slack, Microsoft\nTeams, and Confluence are essential for gaining better task coordination,\nsupporting knowledge sharing, and adopting agile values across cross-functional\ncontexts.", "AI": {"tldr": "This review highlights that using Slack, Microsoft Teams, and Confluence boosts productivity in Agile software projects by improving coordination, communication, and alignment with Agile principles, particularly for remote teams.", "motivation": "The paper is motivated by the challenges teams face in effective collaboration and communication while working remotely within agile and software projects.", "method": "This is a review paper that examines how specific collaboration tools (Slack, Microsoft Teams, and Confluence) align with Agile principles and support agile practices in project environments.", "result": "The paper finds that Slack, Microsoft Teams, and Confluence are essential for better task coordination and knowledge sharing, supporting agile values in cross-functional teams.", "conclusion": "Collaboration tools like Slack, Microsoft Teams, and Confluence are crucial for successful Agile project execution, enabling better organization, openness, and interaction, and ultimately enhancing productivity."}}
{"id": "2506.10986", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10986", "abs": "https://arxiv.org/abs/2506.10986", "authors": ["Mouna Dhaouadi", "Bentley James Oakes", "Michalis Famelis"], "title": "CoMRAT: Commit Message Rationale Analysis Tool", "comment": null, "summary": "In collaborative open-source development, the rationale for code changes is\noften captured in commit messages, making them a rich source of valuable\ninformation. However, research on rationale in commit messages remains limited.\nIn this paper, we present CoMRAT, a tool for analyzing decision and rationale\nsentences rationale in commit messages. CoMRAT enables a) researchers to\nproduce metrics and analyses on rationale information in any Github module, and\nb) developers to check the amount of rationale in their commit messages. A\npreliminary evaluation suggests the tool's usefulness and usability in both\nthese research and development contexts.", "AI": {"tldr": "CoMRAT is a tool that analyzes rationale in Github commit messages, helping researchers and developers understand the reasoning behind code changes. Early evaluation shows it is effective and user-friendly.", "motivation": "Rationale for code changes is crucial in collaborative open-source development, and commit messages often contain this rationale. However, current research on extracting and analyzing rationale from commit messages is lacking.", "method": "The paper introduces CoMRAT, a tool designed to analyze decision and rationale sentences in Github commit messages. It allows both researchers and developers to extract metrics and check the rationale content in any Github repository's commit messages.", "result": "The preliminary evaluation of CoMRAT indicates it is useful and usable for both researchers analyzing rationale and developers checking rationale quantity in commit messages.", "conclusion": "CoMRAT offers a practical solution for extracting and analyzing rationale from commit messages, supporting both research and software development needs."}}
{"id": "2506.10987", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10987", "abs": "https://arxiv.org/abs/2506.10987", "authors": ["Shaoyi Yang"], "title": "Chain of Draft for Software Engineering: Challenges in Applying Concise Reasoning to Code Tasks", "comment": null, "summary": "Large language models (LLMs) have become vital tools for software\ndevelopment, but they often require verbose intermediate reasoning for complex\ncode tasks, leading to high latency and costs. This research extends the Chain\nof Draft (CoD) method to software engineering, designing and evaluating\nmultiple CoD variants tailored for code tasks. Through comprehensive\nexperiments on all 300 samples from the SWE-bench benchmark, we found that all\nCoD variants used significantly fewer tokens than Chain of Thought (CoT), with\nBaseline CoD being most efficient at 55.4% of CoT's tokens. While this\nrepresents substantial efficiency gains - translating to approximately 45%\nreduction in processing time and API costs - it differs from the extreme 7.6%\nreported in the original CoD paper for mathematical reasoning. This difference\nstems from the inherent complexity and context-dependency of software tasks,\nwhich require more detailed reasoning to maintain solution quality. Our\nmulti-dimensional quality assessment revealed that CoD variants maintain over\n90% of CoT's code quality across key metrics including correctness,\ncompatibility, and maintainability, making them practical alternatives for\nreal-world development scenarios where efficiency matters. This research\ndemonstrates how domain-specific characteristics influence prompting strategy\neffectiveness and provides a framework for balancing efficiency with solution\nquality in software engineering applications. Our findings offer practical\nguidance for optimizing LLM-based development workflows through appropriate\nprompting strategy selection based on project requirements.", "AI": {"tldr": "Chain of Draft (CoD) prompting methods can make large language models for coding tasks far more efficient, cutting computation needs by 45% while keeping over 90% of code quality compared to the popular Chain of Thought (CoT). Use CoD variants to save costs in software engineering without a big loss in accuracy.", "motivation": "Large language models are widely used for software development, but complex code tasks require extensive intermediate reasoning, leading to high computation cost and latency. The paper aims to find more efficient reasoning methods without compromising solution quality.", "method": "The study extends the Chain of Draft (CoD) prompting method for software engineering tasks, creating and evaluating multiple CoD variants on 300 samples from the SWE-bench benchmark. Token usage and code quality were assessed and compared against the traditional Chain of Thought (CoT) approach across multidimensional metrics.", "result": "All CoD variants use significantly fewer tokens than CoT, with the most efficient variant using only 55.4% as many tokens. Code quality remains high, with over 90% of CoT's performance across correctness, compatibility, and maintainability. Efficiency gains were notable but less dramatic than in prior work on mathematical tasks, due to the complexity of software engineering problems.", "conclusion": "CoD prompting strategies substantially reduce token usage, processing time, and API costs while maintaining most of the code quality, making them suitable for real-world software development. Choice of prompting strategy should be domain-specific, and the paper provides a decision framework for optimizing LLM-based workflows."}}
{"id": "2506.11209", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2506.11209", "abs": "https://arxiv.org/abs/2506.11209", "authors": ["Zhengyang Liu", "Vinod Grover"], "title": "A Performance Model for Warp Specialization Kernels", "comment": null, "summary": "This paper presents a performance model tailored for warp specialization\nkernels, focusing on factors such as warp size, tilling size, input matrix\nsize, memory bandwidth, and thread divergence. Our model offers accurate\npredictions of execution time by leveraging differential equations validated\nthrough simulations and experiments. The insights gained from this model not\nonly enhance our understanding of warp specialization techniques but also have\npractical implications for optimizing GPU-accelerated applications through\ncompiler optimizations, kernel parameter tuning, and algorithm design.", "AI": {"tldr": "A new performance model for GPU warp specialization kernels is introduced, accurately predicting execution time using differential equations. This model is validated experimentally and helps optimize GPU performance.", "motivation": "The motivation is to provide an accurate performance model for warp specialization kernels, addressing how various factors (warp size, tiling size, matrix size, bandwidth, thread divergence) affect performance and aiding in GPU application optimization.", "method": "The paper develops a performance model using differential equations. The model is validated through simulations and experiments, focusing on predicting the execution time of warp specialization kernels.", "result": "The performance model accurately predicts execution time given different kernel parameters, validated by experiments and simulations.", "conclusion": "The model deepens the understanding of warp specialization and provides practical tools for optimizing GPU-accelerated applications via compiler, tuning, and algorithm design."}}
{"id": "2506.10988", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10988", "abs": "https://arxiv.org/abs/2506.10988", "authors": ["Bowen Tian", "Zhengyang Xu", "Mingqiang Wu", "Songning Lai", "Yutai Yue"], "title": "You Only Train Once: A Flexible Training Framework for Code Vulnerability Detection Driven by Vul-Vector", "comment": "Under Review", "summary": "With the pervasive integration of computer applications across industries,\nthe presence of vulnerabilities within code bases poses significant risks. The\ndiversity of software ecosystems coupled with the intricate nature of modern\nsoftware engineering has led to a shift from manual code vulnerability\nidentification towards the adoption of automated tools. Among these, deep\nlearning-based approaches have risen to prominence due to their superior\naccuracy; however, these methodologies encounter several obstacles. Primarily,\nthey necessitate extensive labeled datasets and prolonged training periods, and\ngiven the rapid emergence of new vulnerabilities, the frequent retraining of\nmodels becomes a resource-intensive endeavor, thereby limiting their\napplicability in cutting-edge scenarios. To mitigate these challenges, this\npaper introduces the \\underline{\\textbf{YOTO}}--\\underline{\\textbf{Y}}ou\n\\underline{\\textbf{O}}nly \\underline{\\textbf{T}}rain \\underline{\\textbf{O}}nce\nframework. This innovative approach facilitates the integration of multiple\ntypes of vulnerability detection models via parameter fusion, eliminating the\nneed for joint training. Consequently, YOTO enables swift adaptation to newly\ndiscovered vulnerabilities, significantly reducing both the time and\ncomputational resources required for model updates.", "AI": {"tldr": "YOTO is a novel framework that integrates multiple vulnerability detection models without joint retraining, addressing inefficiencies in current deep learning-based approaches, and enabling fast, resource-efficient updates as new vulnerabilities emerge.", "motivation": "The motivation of this paper is to address the challenges faced by deep learning-based code vulnerability detection tools, particularly their reliance on large labeled datasets, long training times, and the frequent need to retrain models as new vulnerabilities appear. These challenges make traditional approaches inefficient and less applicable in fast-evolving environments.", "method": "The paper introduces YOTO (You Only Train Once), a framework that allows the integration of multiple vulnerability detection models by parameter fusion. This approach does not require joint training, enabling models to be updated or extended without retraining from scratch.", "result": "YOTO allows for rapid adaptation to new vulnerabilities, greatly reducing the time and computational resources required for updates. The framework eliminates the need for frequent retraining, improving applicability in emerging scenarios.", "conclusion": "The YOTO framework offers a resource-efficient and flexible solution for code vulnerability detection by allowing integration of different models via parameter fusion and removing the necessity for repeated training."}}
{"id": "2506.11701", "categories": ["cs.PL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.11701", "abs": "https://arxiv.org/abs/2506.11701", "authors": ["Lukas Gehring", "Sebastian Rehms", "Florian Tschorsch"], "title": "PermRust: A Token-based Permission System for Rust", "comment": "11 pages", "summary": "Permission systems which restrict access to system resources are a\nwell-established technology in operating systems, especially for smartphones.\nHowever, as such systems are implemented in the operating system they can at\nmost manage access on the process-level. Since moderns software often (re)uses\ncode from third-parties libraries, a permission system for libraries can be\ndesirable to enhance security. In this short-paper, we adapt concepts from\ncapability systems building a novel theoretical foundation for permission\nsystem at the level of the programming language. This leads to PermRust, a\ntoken-based permission system for the Rust programming language as a zero cost\nabstraction on top of its type-system. With it access to system resources can\nbe managed per library.", "AI": {"tldr": "PermRust provides library-level permission control within Rust by leveraging its type system, enabling fine-grained and efficient security beyond traditional process-level OS permissions.", "motivation": "Traditional OS-level permission systems can only restrict resources access at the process level, which does not provide fine-grained control, especially when third-party libraries are involved. There is a need for permission systems that work at the library level to improve security.", "method": "The paper adapts concepts from capability systems and builds a theoretical foundation for permission systems at the programming language level. It introduces PermRust, a token-based permission system using the Rust language's type system as a zero-cost abstraction.", "result": "PermRust enables resource access management at the granularity of individual libraries within Rust applications, without incurring extra runtime costs.", "conclusion": "The approach demonstrates that permission systems can be effectively integrated into the type system of a programming language (Rust), allowing fine-grained, library-level access control and enhanced security."}}
{"id": "2506.10989", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10989", "abs": "https://arxiv.org/abs/2506.10989", "authors": ["Rogelio Cruz", "Jonatan Contreras", "Francisco Guerrero", "Ezequiel Rodriguez", "Carlos Valdez", "Citlali Carrillo"], "title": "Prompt engineering and framework: implementation to increase code reliability based guideline for LLMs", "comment": null, "summary": "In this paper, we propose a novel prompting approach aimed at enhancing the\nability of Large Language Models (LLMs) to generate accurate Python code.\nSpecifically, we introduce a prompt template designed to improve the quality\nand correctness of generated code snippets, enabling them to pass tests and\nproduce reliable results. Through experiments conducted on two state-of-the-art\nLLMs using the HumanEval dataset, we demonstrate that our approach outperforms\nwidely studied zero-shot and Chain-of-Thought (CoT) methods in terms of the\nPass@k metric. Furthermore, our method achieves these improvements with\nsignificantly reduced token usage compared to the CoT approach, making it both\neffective and resource-efficient, thereby lowering the computational demands\nand improving the eco-footprint of LLM capabilities. These findings highlight\nthe potential of tailored prompting strategies to optimize code generation\nperformance, paving the way for broader applications in AI-driven programming\ntasks.", "AI": {"tldr": "A new prompt template significantly improves Python code generation by LLMs, achieving higher test pass rates and lower token usage than existing methods like CoT and zero-shot prompting.", "motivation": "Existing techniques for generating Python code with Large Language Models (LLMs), such as zero-shot and Chain-of-Thought (CoT) prompting, have notable limitations in terms of code correctness and computational efficiency. There is a need to improve the quality, reliability, and resource usage of code generated by LLMs.", "method": "The paper proposes a novel prompt template specifically designed for LLMs to enhance Python code generation. This template is evaluated by comparing it with zero-shot and Chain-of-Thought prompting methods using the HumanEval benchmark dataset. Two state-of-the-art LLMs are tested, focusing on the ability of generated code to pass functional tests.", "result": "The proposed prompting method outperforms both zero-shot and CoT approaches on the Pass@k metric, indicating higher rates of code correctness. Additionally, it achieves this with significantly reduced token usage compared to the CoT method.", "conclusion": "Tailored prompting strategies can meaningfully improve the accuracy, efficiency, and environmental footprint of LLM-driven code generation. This opens up new opportunities for efficient and reliable AI-driven programming."}}
{"id": "2506.11794", "categories": ["cs.PL", "math.PR"], "pdf": "https://arxiv.org/pdf/2506.11794", "abs": "https://arxiv.org/abs/2506.11794", "authors": ["Baltasar Tranc\u00f3n y Widemann", "Markus Lepper"], "title": "ALEA IACTA EST: A Declarative Domain-Specific Language for Manually Performable Random Experiments", "comment": null, "summary": "Random experiments that are simple and clear enough to be performed by human\nagents feature prominently in the teaching of elementary stochastics as well as\nin games. We present Alea, a domain-specific language for the specification of\nrandom experiments. Alea code can either be analyzed statically to obtain and\ninspect probability distributions of outcomes, or be executed with a source\npseudo-randomness for simulation or as a game assistant. The language is\nintended for ease of use by non-expert programmers, in particular students of\nelementary stochastics, and players and designers of games of chance, by\nfocusing on concepts common to functional programming and basic mathematics.\nBoth the design of the language and the implementation of runtime environments\nare work in progress.", "AI": {"tldr": "Alea is a new, easy-to-use language for specifying and analyzing random experiments, aimed at students and game designers, with ongoing tool development.", "motivation": "Random experiments are central to teaching elementary stochastics and feature in games, but there is a lack of simple tools for specifying and analyzing these experiments accessible to non-experts.", "method": "The authors propose Alea, a domain-specific language tailored for specifying random experiments. Alea supports both static analysis to obtain probability distributions and execution with pseudo-randomness for simulations. It borrows concepts from functional programming and basic mathematics and targets beginners in stochastics and game players/designers.", "result": "A prototype of the Alea language has been developed to allow specification, analysis, and simulation of random experiments, though both the language design and runtime implementation are still under development.", "conclusion": "Alea offers a beginner-friendly way to specify and analyze random experiments, helping students and game enthusiasts more easily interact with stochastic concepts; further development is ongoing."}}
{"id": "2506.10990", "categories": ["cs.SE", "cs.AI", "cs.CE", "cs.DC", "I.2.0"], "pdf": "https://arxiv.org/pdf/2506.10990", "abs": "https://arxiv.org/abs/2506.10990", "authors": ["Roberto Vergallo", "Lu\u00eds Cruz", "Alessio Errico", "Luca Mainetti"], "title": "On the Effectiveness of the 'Follow-the-Sun' Strategy in Mitigating the Carbon Footprint of AI in Cloud Instances", "comment": "24 pages, 4 figures, 10 tables", "summary": "'Follow-the-Sun' (FtS) is a theoretical computational model aimed at\nminimizing the carbon footprint of computer workloads. It involves dynamically\nmoving workloads to regions with cleaner energy sources as demand increases and\nenergy production relies more on fossil fuels. With the significant power\nconsumption of Artificial Intelligence (AI) being a subject of extensive\ndebate, FtS is proposed as a strategy to mitigate the carbon footprint of\ntraining AI models. However, the literature lacks scientific evidence on the\nadvantages of FtS to mitigate the carbon footprint of AI workloads. In this\npaper, we present the results of an experiment conducted in a partial synthetic\nscenario to address this research gap. We benchmarked four AI algorithms in the\nanomaly detection domain and measured the differences in carbon emissions in\nfour cases: no strategy, FtS, and two strategies previously introduced in the\nstate of the art, namely Flexible Start and Pause and Resume. To conduct our\nexperiment, we utilized historical carbon intensity data from the year 2021 for\nseven European cities. Our results demonstrate that the FtS strategy not only\nachieves average reductions of up to 14.6% in carbon emissions (with peaks of\n16.3%) but also helps in preserving the time needed for training.", "AI": {"tldr": "This paper provides evidence that the Follow-the-Sun (FtS) strategy can reduce the carbon emissions of AI training by up to 16.3% without increasing training time, using a benchmark of AI algorithms and real-world carbon intensity data across Europe.", "motivation": "Artificial Intelligence (AI) training consumes significant power, leading to increased carbon emissions. Given the ongoing debate on AI's environmental impact, strategies to minimize the carbon footprint are needed. Although 'Follow-the-Sun' (FtS) has been theorized as a way to reduce emissions by moving workloads to regions with cleaner energy, there is a lack of scientific evidence supporting its effectiveness for AI workloads.", "method": "The authors conducted an experiment using a partial synthetic scenario. They benchmarked four AI algorithms in the anomaly detection domain and compared carbon emissions in four scenarios: no strategy, FtS, and two previously known strategies ('Flexible Start' and 'Pause and Resume'). Historical carbon intensity data from 2021 for seven European cities were used to simulate the scenarios.", "result": "The experiment demonstrated that using the FtS strategy resulted in average carbon emission reductions of up to 14.6%, with peaks of 16.3%. Additionally, FtS helped to maintain the training time required for AI models.", "conclusion": "FtS is an effective strategy to reduce the carbon footprint of AI model training without negatively impacting training time. The results provide the first scientific evidence supporting FtS for reducing AI-related carbon emissions."}}
{"id": "2506.10991", "categories": ["cs.SE", "D.2.9; H.4.1"], "pdf": "https://arxiv.org/pdf/2506.10991", "abs": "https://arxiv.org/abs/2506.10991", "authors": ["Hoang Vu", "Henrik Leopold", "Han van der Aa"], "title": "What is Business Process Automation Anyway?", "comment": "Accepted at HICSS 2023", "summary": "Many organizations strive to increase the level of automation in their\nbusiness processes. While automation historically was mainly concerned with\nautomating physical labor, current automation efforts mostly focus on\nautomation in a digital manner, thus targeting work that is related to the\ninteraction between humans and computers. This type of automation, commonly\nreferred to as business process automation, has many facets. Yet, academic\nliterature mainly focuses on Robotic Process Automation, a specific automation\ncapability. Recognizing that leading vendors offer automation capabilities\ngoing way beyond that, we use this paper to develop a detailed understanding of\nbusiness process automation in industry. To this end, we conduct a structured\nmarket analysis of the 18 predominant vendors of business process automation\nsolutions as identified by Gartner. As a result, we provide a comprehensive\noverview of the business process automation capabilities currently offered by\nindustrial vendors. We show which types and facets of automation exist and\nwhich aspects represent promising directions for the future.", "AI": {"tldr": "This paper analyzes and categorizes the business process automation capabilities of 18 major vendors, showing industry solutions go well beyond Robotic Process Automation and outlining promising future directions.", "motivation": "Many organizations are increasingly seeking to automate business processes. While traditional automation focused on physical work, digital automation is now most common\u2014especially in business processes involving human-computer interactions. However, academic research has mostly focused on Robotic Process Automation (RPA), even as industry offerings go beyond RPA. This paper aims to fill that gap by investigating the broader spectrum of business process automation capabilities available from industry vendors.", "method": "The authors conducted a structured market analysis of 18 major business process automation vendors, as identified by Gartner. This involved systematically examining and comparing the automation capabilities offered by these vendors.", "result": "The paper delivers a comprehensive overview of current business process automation solutions in the industry. It categorizes different types and facets of automation available, going beyond just RPA. The study also highlights which aspects of current solutions are promising areas for future development.", "conclusion": "The study concludes that business process automation is a multifaceted field with capabilities extending far beyond Robotic Process Automation. By mapping out the current landscape, the paper provides valuable direction for both future research and practical advancements in automation technologies."}}
{"id": "2506.10992", "categories": ["cs.SE", "D.2.9"], "pdf": "https://arxiv.org/pdf/2506.10992", "abs": "https://arxiv.org/abs/2506.10992", "authors": ["Hoang Vu", "Jennifer Haase", "Henrik Leopold", "Jan Mendling"], "title": "Towards a Theory on Process Automation Effects", "comment": "Accepted at HICSS 2023", "summary": "Process automation is a crucial strategy for improving business processes,\nbut little attention has been paid to the effects that automation has once it\nis operational. This paper addresses this research problem by reviewing the\nliterature on human-automation interaction. Although many of the studies in\nthis field have been conducted in different domains, they provide a foundation\nfor developing propositions about process automation effects. Our analysis\nfocuses on how humans perceive automation technology when working within a\nprocess, allowing us to propose an effective engagement model between\ntechnology, process participants, process managers, and software developers.\nThis paper offers insights and recommendations that can help organizations\noptimize their use of process automation. We further derive novel research\nquestions for a discourse within the process automation community.", "AI": {"tldr": "The paper reviews human-automation interaction literature to analyze the post-implementation effects of process automation in businesses, proposes an engagement model for stakeholders, and offers recommendations to optimize automation use.", "motivation": "The motivation is to address the research gap regarding how process automation impacts organizations after it is operational, specifically focusing on the interaction between humans and automation.", "method": "A literature review of human-automation interaction studies from various domains is conducted to inform propositions about process automation effects.", "result": "The analysis highlights how humans perceive and interact with automation technologies in business processes, leading to the development of an effective engagement model among technology, process participants, process managers, and software developers. The paper also identifies new research questions for the field.", "conclusion": "The paper concludes that understanding human perceptions and interactions with automation is essential for optimizing process automation outcomes. Recommendations and a conceptual engagement model are provided to guide organizations in effectively leveraging process automation."}}
{"id": "2506.10993", "categories": ["cs.SE", "cs.FL", "68N30, 68Q60", "D.2.4; D.2.1; F.3.1"], "pdf": "https://arxiv.org/pdf/2506.10993", "abs": "https://arxiv.org/abs/2506.10993", "authors": ["Muhammad Naeem", "Cristina Seceleanu"], "title": "Contract-based Verification of Digital Twins", "comment": "Accepted at ICECCS 2025, to appear in Lecture Notes in Computer\n  Science (LNCS), Springer", "summary": "Digital twins are becoming powerful tools in industrial applications,\noffering virtual representations of cyber-physical systems. However,\nverification of these models remains a significant challenge due to the\npotentially large datasets used by the digital twin. This paper introduces an\ninnovative methodology for verifying neural network-based digital twin models,\nin a black-box fashion, by integrating model checking into the process. The\nlatter relies on defining and applying system-level contracts that capture the\nsystem's requirements, to verify the behavior of digital twin models,\nimplemented in Simulink. We develop an automated solution that simulates the\ndigital twin model for certain inputs, and feeds the predicted outputs together\nwith the inputs to the contract model described as a network of timed automata\nin the UPPAAL model checker. The latter verifies whether the predicted outputs\nfulfill the specified contracts. This approach allows us to identify scenarios\nwhere the digital twin's behavior fails to meet the contracts, without\nrequiring the digital twin's design technicalities. We apply our method to a\nboiler system case study for which we identify prediction errors via contract\nverification. Our work demonstrates the effectiveness of integrating model\nchecking with digital twin models for continuous improvement.", "AI": {"tldr": "This paper presents a new, automated method for verifying neural network-based digital twins in Simulink using system-level contracts and model checking via UPPAAL, successfully detecting errors in a real-world case study without needing access to the model's details.", "motivation": "Verifying digital twin models is challenging due to large datasets and complex model architectures, especially when the models use neural networks and function as black boxes. Existing verification methods are insufficient, prompting the need for more effective, scalable solutions.", "method": "The proposed methodology combines black-box verification of neural network-based digital twin models with model checking. System-level contracts, expressed as timed automata in UPPAAL, are used to verify if the models' outputs meet the system's requirements. Automated simulation and contract verification are performed without needing details of the digital twin's inner structure.", "result": "The methodology is successfully applied to a boiler system case study, uncovering scenarios where the digital twin model fails the specified contracts, demonstrating the practical effectiveness of the approach.", "conclusion": "Integrating model checking with digital twin models significantly enhances their verification process and enables the identification of prediction errors without requiring access to model internals."}}
{"id": "2506.10994", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10994", "abs": "https://arxiv.org/abs/2506.10994", "authors": ["April Clarke"], "title": "Improving Software Team Communication Through Social Interventions in Project Management Tools", "comment": "ICSE 2025 Doctoral Track. arXiv admin note: substantial text overlap\n  with arXiv:2502.01923", "summary": "Productive software engineering teams require effective communication and\nbalanced contributions between team members. However, teams are often\nineffective at these skills, which is detrimental to project success.\nProject-based university courses are an opportunity for students to practise\nthese skills, but we have yet to establish how we can guide students towards\nimproving their communication and coordination. We aim to develop project\nmanagement tool features, informed by social network analysis, that nudge\nstudents in software engineering group projects towards beneficial behaviours.\nTo do this, we will first evaluate the suitability of social network analysis\ntechniques for identifying areas of improvement in teams' communication. Then,\nwe will develop features in a project management tool that aid students in\nidentifying and addressing these areas of improvement, and evaluate them in the\ncontext of a software engineering group project.", "AI": {"tldr": "The paper investigates how social network analysis can improve communication and teamwork in student SE group projects by building and testing new project management tool features to help students recognize and address collaboration issues.", "motivation": "Many software engineering teams lack effective communication and balanced contributions, leading to project failure. University project-based courses offer a chance for students to improve these skills, but there is limited knowledge on how to consistently guide and enhance student communication and coordination.", "method": "The authors propose evaluating the suitability of social network analysis techniques for identifying weaknesses in team communication. Subsequently, they will develop and integrate tool features into a project management system to help students identify and address these issues. Finally, these new features will be evaluated in actual software engineering student group projects.", "result": "The expected result is the creation and validation of project management tool features\u2014based on social network analysis\u2014that can effectively point out communication issues and guide students toward better teamwork.", "conclusion": "This study aims to offer practical insights and tools for improving communication and coordination in student software engineering teams by leveraging social network analyses within project management software."}}
{"id": "2506.10995", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10995", "abs": "https://arxiv.org/abs/2506.10995", "authors": ["Jorge Martinez-Gil"], "title": "Evaluating Small-Scale Code Models for Code Clone Detection", "comment": "20 pages", "summary": "Detecting code clones is relevant to software maintenance and code\nrefactoring. This challenge still presents unresolved cases, mainly when\nstructural similarity does not reflect functional equivalence, though recent\ncode models show promise. Therefore, this research aims to systematically\nmeasure the performance of several newly introduced small code models in\nclassifying code pairs as clones or non-clones. The evaluation is based on five\ndatasets: BigCloneBench, CodeJam, Karnalim, POJ104, and PoolC, as well as six\ncode models: CodeBERT, GraphCodeBERT, Salesforce T5, UniXCoder, PLBART, and\nPolycoder. Most models performed well across standard metrics, including\naccuracy, precision, recall, and F1-score. However, a marginal fraction of\nclones remains challenging to detect, especially when the code looks similar\nbut performs different operations. The source code that illustrates our\napproach is available at:\nhttps://github.com/jorge-martinez-gil/small-code-models", "AI": {"tldr": "Recent small code models are generally effective in detecting code clones across multiple benchmarks, but struggle with cases where similar-looking code performs different functions, highlighting an ongoing limitation in code clone detection.", "motivation": "Code clone detection is crucial for software maintenance and refactoring but remains a challenge, especially when structural similarity does not imply functional equivalence. Recent advances in code models motivate a systematic evaluation of their effectiveness in detecting code clones.", "method": "The study systematically evaluates the code clone detection performance of six small code models (CodeBERT, GraphCodeBERT, Salesforce T5, UniXCoder, PLBART, and Polycoder) using five datasets (BigCloneBench, CodeJam, Karnalim, POJ104, PoolC). Standard performance metrics such as accuracy, precision, recall, and F1-score are computed.", "result": "Most code models perform well across the evaluated metrics. However, detecting clones remains a challenge when code is structurally similar but functionally different.", "conclusion": "While state-of-the-art code models are effective in code clone detection in most cases, distinguishing code with high structural similarity but different functionality is still a challenging aspect requiring further research."}}
{"id": "2506.10996", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10996", "abs": "https://arxiv.org/abs/2506.10996", "authors": ["Saadiq Rauf Khan", "Vinit Chandak", "Sougata Mukherjea"], "title": "Evaluating LLMs for Visualization Tasks", "comment": null, "summary": "Information Visualization has been utilized to gain insights from complex\ndata. In recent times, Large Language Models (LLMs) have performed very well in\nmany tasks. In this paper, we showcase the capabilities of different popular\nLLMs to generate code for visualization based on simple prompts. We also\nanalyze the power of LLMs to understand some common visualizations by answering\nsimple questions. Our study shows that LLMs could generate code for some\nvisualizations as well as answer questions about them. However, LLMs also have\nseveral limitations. We believe that our insights can be used to improve both\nLLMs and Information Visualization systems.", "AI": {"tldr": "The paper evaluates how well popular LLMs can generate visualization code and interpret common visualizations from simple prompts/questions. While LLMs show promising abilities, they also face several limitations, and insights from this work may help improve future LLMs and visualization tools.", "motivation": "The paper is motivated by the emergence of Large Language Models (LLMs) and their promising abilities in diverse tasks, particularly exploring their potential role in the domain of Information Visualization, where generating code and understanding visualizations are valuable skills.", "method": "The authors conduct a study using various popular LLMs, prompting them to generate code for information visualizations and to answer simple questions about common types of visualizations, in order to assess their performance and capabilities.", "result": "The results show that while LLMs can generate code for some visualizations and answer basic questions about them, they also exhibit notable limitations in both tasks.", "conclusion": "The paper concludes that, despite observed limitations, insights obtained from evaluating LLMs can be utilized to enhance both LLM development and Information Visualization systems."}}
{"id": "2506.10997", "categories": ["cs.SE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.10997", "abs": "https://arxiv.org/abs/2506.10997", "authors": ["Hanumanthrao Kannan", "Alejandro Salado"], "title": "A Theory-driven Interpretation and Elaboration of Verification and Validation", "comment": null, "summary": "This paper presents a formal theory of verification and validation (V&V)\nwithin systems engineering, grounded in the axiom that V&V are fundamentally\nknowledge-building activities. Using dynamic epistemic modal logic, we develop\nprecise definitions of verification and validation, articulating their roles in\nconfirming and contextualizing knowledge about systems. The theory formalizes\nthe interplay between epistemic states, evidence, and reasoning processes,\nallowing for the derivation of theorems that clarify the conceptual\nunderpinnings of V&V. By providing a formal foundation, this work addresses\nambiguities in traditional V&V practices, offering a structured framework to\nenhance precision and consistency in systems engineering methodologies. The\ninsights gained have implications for both academic research and practical\napplications, fostering a deeper understanding of V&V as critical components of\nengineering knowledge generation.", "AI": {"tldr": "This paper introduces a formal theory using dynamic epistemic modal logic to clearly define and contextualize verification and validation (V&V) in systems engineering. The approach resolves ambiguities in traditional practices and provides a structured, rigorous framework for V&V as knowledge-building processes.", "motivation": "Verification and validation (V&V) in systems engineering often suffer from conceptual ambiguities, lacking a structured and precise theoretical foundation. The motivation of the paper is to address these ambiguities and to redefine V&V as core knowledge-building activities within systems engineering.", "method": "The paper employs dynamic epistemic modal logic to formally define verification and validation. This approach models epistemic states, evidence, and reasoning processes to create a theoretical structure and enables the derivation of theorems illustrating the relationships and underpinnings of V&V.", "result": "The authors deliver precise definitions of verification and validation, clarify their respective roles in knowledge generation, and formalize the interplay between knowledge states, evidence, and reasoning. Their framework enables greater rigor and can resolve ambiguities found in conventional V&V practices.", "conclusion": "By providing a formal, logic-based theoretical foundation for V&V, the paper enhances the consistency and precision of systems engineering methodologies. This enriches both academic understanding and practical implementation of V&V as essential activities for generating engineering knowledge."}}
{"id": "2506.10998", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10998", "abs": "https://arxiv.org/abs/2506.10998", "authors": ["Kangping Xu", "Yifan Luo", "Yang Yuan", "Andrew Chi-Chih Yao"], "title": "Towards Automated Formal Verification of Backend Systems with LLMs", "comment": null, "summary": "Software testing plays a critical role in ensuring that systems behave as\nintended. However, existing automated testing approaches struggle to match the\ncapabilities of human engineers due to key limitations such as test locality,\nlack of general reliability, and business logic blindness. In this work, we\npropose a novel framework that leverages functional programming and type\nsystems to translate Scala backend code into formal Lean representations. Our\npipeline automatically generates theorems that specify the intended behavior of\nAPIs and database operations, and uses LLM-based provers to verify them. When a\ntheorem is proved, the corresponding logic is guaranteed to be correct and no\nfurther testing is needed. If the negation of a theorem is proved instead, it\nconfirms a bug. In cases where neither can be proved, human intervention is\nrequired. We evaluate our method on realistic backend systems and find that it\ncan formally verify over 50% of the test requirements, which suggests that half\nof a testing engineer's workload can be automated. Additionally, with an\naverage cost of only $2.19 per API, LLM-based verification is significantly\nmore cost-effective than manual testing and can be scaled easily through\nparallel execution. Our results indicate a promising direction for scalable,\nAI-powered software testing, with the potential to greatly improve engineering\nproductivity as models continue to advance.", "AI": {"tldr": "This paper introduces a novel AI-assisted framework that translates Scala code into formal proofs, automates verification of backend logic, and can halve manual testing costs and efforts, marking a scalable breakthrough for software QA.", "motivation": "Automated software testing is limited by issues such as test locality, lack of general reliability, and failure to capture business logic, hindering its ability to match human engineers' effectiveness. There is a need for more robust, reliable, and scalable testing solutions that can reduce manual workload.", "method": "The paper proposes a framework that translates Scala backend code into formal Lean representations using functional programming and type systems. This pipeline automatically generates formal theorems describing the system's intended behavior and uses LLM-based provers to attempt to verify these theorems. If proven, the component is confirmed correct; proven negation confirms a bug; otherwise, human intervention is necessary.", "result": "The framework was evaluated on realistic backend systems, successfully verifying over 50% of all test requirements automatically. The average cost of LLM-based verification per API is just $2.19, making it much cheaper and more scalable than manual testing, and supporting parallel execution.", "conclusion": "The proposed method can potentially automate up to half of a testing engineer's workload, offering a scalable, cost-effective, and productive approach to software testing as LLMs improve. This indicates strong potential for AI-driven, formal verification-based testing in real-world backend systems."}}
{"id": "2506.10999", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10999", "abs": "https://arxiv.org/abs/2506.10999", "authors": ["Atul Kumar", "Diptikalyan Saha", "Toshikai Yasue", "Kohichi Ono", "Saravanan Krishnan", "Sandeep Hans", "Fumiko Satoh", "Gerald Mitchell", "Sachin Kumar"], "title": "Automated Validation of COBOL to Java Transformation", "comment": "arXiv admin note: text overlap with arXiv:2504.10548", "summary": "Recent advances in Large Language Model (LLM) based Generative AI techniques\nhave made it feasible to translate enterpriselevel code from legacy languages\nsuch as COBOL to modern languages such as Java or Python. While the results of\nLLM-based automatic transformation are encouraging, the resulting code cannot\nbe trusted to correctly translate the original code. We propose a framework and\na tool to help validate the equivalence of COBOL and translated Java. The\nresults can also help repair the code if there are some issues and provide\nfeedback to the AI model to improve. We have developed a\nsymbolic-execution-based test generation to automatically generate unit tests\nfor the source COBOL programs which also mocks the external resource calls. We\ngenerate equivalent JUnit test cases with equivalent mocking as COBOL and run\nthem to check semantic equivalence between original and translated programs.", "AI": {"tldr": "This paper presents a testing framework to validate and improve LLM-generated translations of COBOL to Java, by generating and running equivalent unit tests on both original and translated code to check for semantic equivalence and repair errors.", "motivation": "Large Language Model (LLM)-based AI can now translate legacy enterprise code (like COBOL) to modern languages (such as Java or Python), but the automatically generated code may not be semantically equivalent or trustworthy. There is a need to validate and improve the correctness of such code translations.", "method": "The authors propose a framework and tool that uses symbolic execution to automatically generate unit tests for COBOL programs, including mocking external resource calls. Equivalent JUnit test cases are generated for the translated Java code, and these are run to check for semantic equivalence between the original and translated programs.", "result": "The generated tools successfully validate whether the COBOL and Java programs are semantically equivalent. If discrepancies are found, the tool can help repair the translated code and provide feedback for improving the LLM-based translation process.", "conclusion": "The proposed framework enhances trust and reliability in LLM-based code translation by systematically validating semantic equivalence and facilitating iterative improvements through code repair and feedback."}}
{"id": "2506.11000", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11000", "abs": "https://arxiv.org/abs/2506.11000", "authors": ["Ketai Qiu"], "title": "Ever-Improving Test Suite by Leveraging Large Language Models", "comment": "Accepted by 33rd ACM International Conference on the Foundations of\n  Software Engineering (FSE Companion '25), June 23--28, 2025, Trondheim,\n  Norway", "summary": "Augmenting test suites with test cases that reflect the actual usage of the\nsoftware system is extremely important to sustain the quality of long lasting\nsoftware systems. In this paper, we propose E-Test, an approach that\nincrementally augments a test suite with test cases that exercise behaviors\nthat emerge in production and that are not been tested yet. E-Test leverages\nLarge Language Models to identify already-tested, not-yet-tested, and\nerror-prone unit execution scenarios, and augment the test suite accordingly.\nOur experimental evaluation shows that E-Test outperforms the main\nstate-of-the-art approaches to identify inadequately tested behaviors and\noptimize test suites.", "AI": {"tldr": "E-Test uses LLMs to analyze production behavior and incrementally add untested or error-prone cases to test suites, outperforming current methods in both coverage and quality.", "motivation": "Test suites need to evolve to reflect how software is used in production, as this is essential for maintaining software quality over time. Traditional test suites may miss real-world behaviors that emerge after deployment.", "method": "The paper introduces E-Test, an incremental test suite augmentation approach. E-Test leverages Large Language Models (LLMs) to analyze execution scenarios in production, classify them as already-tested, not-yet-tested, or error-prone, and then generate and add relevant test cases to the suite.", "result": "E-Test was evaluated experimentally and shown to outperform leading state-of-the-art methods in identifying inadequately tested behaviors and optimizing test suites.", "conclusion": "E-Test effectively augments test suites by identifying and adding tests for previously untested or error-prone behaviors found in production. Using LLMs in this process leads to improved coverage and quality compared to existing techniques."}}
{"id": "2506.11001", "categories": ["cs.SE", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11001", "abs": "https://arxiv.org/abs/2506.11001", "authors": ["S. Tucker Browne", "Mark M. Bailey"], "title": "Rethinking Technological Readiness in the Era of AI Uncertainty", "comment": "12 pages", "summary": "Artificial intelligence (AI) is poised to revolutionize military combat\nsystems, but ensuring these AI-enabled capabilities are truly mission-ready\npresents new challenges. We argue that current technology readiness assessments\nfail to capture critical AI-specific factors, leading to potential risks in\ndeployment. We propose a new AI Readiness Framework to evaluate the maturity\nand trustworthiness of AI components in military systems. The central thesis is\nthat a tailored framework - analogous to traditional Technology Readiness\nLevels (TRL) but expanded for AI - can better gauge an AI system's reliability,\nsafety, and suitability for combat use. Using current data evaluation tools and\ntesting practices, we demonstrate the framework's feasibility for near-term\nimplementation. This structured approach provides military decision-makers with\nclearer insight into whether an AI-enabled system has met the necessary\nstandards of performance, transparency, and human integration to be deployed\nwith confidence, thus advancing the field of defense technology management and\nrisk assessment.", "AI": {"tldr": "Traditional technology assessments miss key AI issues in military systems. The new AI Readiness Framework helps evaluate AI maturity, trustworthiness, and readiness, giving military leaders better confidence before deployment.", "motivation": "Current technology readiness assessments are inadequate for evaluating AI-specific challenges in military systems, potentially leading to deployment risks.", "method": "The authors propose a new AI Readiness Framework, which adapts the concept of Technology Readiness Levels (TRL) for AI-enabled military systems, and demonstrate its feasibility using current data evaluation tools and testing practices.", "result": "The framework can be practically implemented in the near term, offering a more comprehensive assessment of AI systems' maturity, reliability, and safety for combat use.", "conclusion": "A specialized AI Readiness Framework enables more accurate evaluation of AI components in military systems, enhancing decision-making and risk management for defense technology deployment."}}
{"id": "2506.11002", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11002", "abs": "https://arxiv.org/abs/2506.11002", "authors": ["Roberto Verdecchia", "Justus Bogner"], "title": "Notes On Writing Effective Empirical Software Engineering Papers: An Opinionated Primer", "comment": null, "summary": "While mastered by some, good scientific writing practices within Empirical\nSoftware Engineering (ESE) research appear to be seldom discussed and\ndocumented. Despite this, these practices are implicit or even explicit\nevaluation criteria of typical software engineering conferences and journals.\nIn this pragmatic, educational-first document, we want to provide guidance to\nthose who may feel overwhelmed or confused by writing ESE papers, but also\nthose more experienced who still might find an opinionated collection of\nwriting advice useful. The primary audience we had in mind for this paper were\nour own BSc, MSc, and PhD students, but also students of others. Our documented\nadvice therefore reflects a subjective and personal vision of writing ESE\npapers. By no means do we claim to be fully objective, generalizable, or\nrepresentative of the whole discipline. With that being said, writing papers in\nthis way has worked pretty well for us so far. We hope that this guide can at\nleast partially do the same for others.", "AI": {"tldr": "The paper offers subjective yet practical writing guidance for students and researchers in Empirical Software Engineering, aiming to fill a gap in explicit training on good scientific writing in the field.", "motivation": "Good scientific writing practices are critical in Empirical Software Engineering (ESE), but they are rarely discussed or documented systematically, despite being important evaluation criteria in the field.", "method": "The paper takes a pragmatic, educational-first approach, presenting subjective and opinionated writing advice specifically intended for ESE researchers, particularly students (BSc, MSc, PhD).", "result": "The paper provides a guide for better ESE paper writing, based on the authors' own experience and perspectives, intended to be practical and helpful, especially for students but also for other researchers.", "conclusion": "Although not claiming full objectivity or generalizability, the authors believe their writing approach has been effective for them and propose that it will be helpful for others engaged in ESE research."}}
{"id": "2506.11003", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11003", "abs": "https://arxiv.org/abs/2506.11003", "authors": ["Ruiyang Xu", "Jialun Cao", "Mingyuan Wu", "Wenliang Zhong", "Yaojie Lu", "Ben He", "Xianpei Han", "Shing-Chi Cheung", "Le Sun"], "title": "EmbedAgent: Benchmarking Large Language Models in Embedded System Development", "comment": "21 pages", "summary": "Large Language Models (LLMs) have shown promise in various tasks, yet few\nbenchmarks assess their capabilities in embedded system development.In this\npaper, we introduce EmbedAgent, a paradigm designed to simulate real-world\nroles in embedded system development, such as Embedded System Programmer,\nArchitect, and Integrator. This paradigm enables LLMs to be tested in tasks\nthat bridge the gap between digital and physical systems, allowing for a more\ncomprehensive assessment of their capabilities. To evaluate LLMs on these\ntasks, we propose Embedbench, the first comprehensive benchmark for embedded\nsystem programming, circuit design, and cross-platform migration.Embedbench\nconsists of 126 cases, covering 9 electronic components across 3 hardware\nplatforms. Through extensive experiments on 10 mainstream LLMs, we uncover\nseveral key findings. Surprisingly, despite the simplicity of the cases,\nDeepSeek-R1 achieves only a 55.6% pass@1 rate when provided with schematic\ninformation, and 50.0% when tasked with generating the schematics itself. In\nthe cross-platform migration tasks, LLMs show relatively strong performance\nwith MicroPython on the Raspberry Pi Pico (with the top model achieving 73.8%\npass@1), but perform poorly on ESP-IDF, where the best model reaches only 29.4%\npass@1.Interestingly, we observe that general-purpose chat LLMs like\nDeepSeek-V3 often fail to utilize relevant pre-trained knowledge in this\ndomain, while reasoning LLMs tend to overthink and overlook efficient knowledge\nduring pretraining. Based on these insights, we propose two strategies:\nretrieval augmented generation and compiler feedback-to enhance LLM\nperformance. These strategies result in significant improvements, with\nDeepseek-R1 reaching a 65.1% pass@1 with correct schematics, and 53.1% without.\nAdditionally, the accuracy of the Arduino to ESP32 migration task improves from\n21.4% to 27.8%.", "AI": {"tldr": "The paper introduces Embedbench and EmbedAgent to benchmark LLMs for embedded system programming, exposing their weaknesses and improvement methods, with new strategies significantly boosting performance on challenging tasks.", "motivation": "Large Language Models (LLMs) show potential for various tasks but lack specific benchmarks to evaluate their effectiveness in the domain of embedded system development. There is a need to simulate real-world engineering roles and assess LLMs' capacity to bridge digital and physical systems.", "method": "The authors introduced EmbedAgent, a role-based simulation paradigm for embedded system development, and created Embedbench, a comprehensive benchmark with 126 cases across programming, circuit design, and cross-platform migration for 9 components and 3 platforms. Ten mainstream LLMs were evaluated, and improvement strategies (retrieval augmented generation and compiler feedback) were tested.", "result": "LLMs perform inconsistently across tasks: DeepSeek-R1 achieves only 55.6% pass@1 (schematic provided) and 50.0% (self-generated schematics). Performance is better on MicroPython/Raspberry Pi Pico (up to 73.8%), but poor on ESP-IDF (29.4%). General LLMs struggle to apply relevant knowledge, and reasoning LLMs overthink simple tasks. The proposed enhancement strategies improved DeepSeek-R1 to 65.1% (with schematics) and Arduino to ESP32 migration accuracy from 21.4% to 27.8%.", "conclusion": "Embedbench provides a novel and comprehensive way to assess LLMs in embedded system-related tasks, revealing their current limitations and highlighting the potential of augmentation strategies for improved performance. Significant domain-specific gaps remain, especially for less common platforms and migration tasks."}}
{"id": "2506.11005", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11005", "abs": "https://arxiv.org/abs/2506.11005", "authors": ["Mouna Dhaouadi", "Bentley Oakes", "Michalis Famelis"], "title": "Automated Extraction and Analysis of Developer's Rationale in Open Source Software", "comment": null, "summary": "Contributors to open source software must deeply understand a project's\nhistory to make coherent decisions which do not conflict with past reasoning.\nHowever, inspecting all related changes to a proposed contribution requires\nintensive manual effort, and previous research has not yet produced an\nautomated mechanism to expose and analyze these conflicts. In this article, we\npropose such an automated approach for rationale analyses, based on an\ninstantiation of Kantara, an existing high-level rationale extraction and\nmanagement architecture. Our implementation leverages pre-trained models and\nLarge Language Models, and includes structure-based mechanisms to detect\nreasoning conflicts and problems which could cause design erosion in a project\nover time. We show the feasibility of our extraction and analysis approach\nusing the OOM-Killer module of the Linux Kernel project, and investigate the\napproach's generalization to five other highly active open source projects. The\nresults confirm that our automated approach can support rationale analyses with\nreasonable performance, by finding interesting relationships and to detect\npotential conflicts and reasoning problems. We also show the effectiveness of\nthe automated extraction of decision and rationale sentences and the prospects\nfor generalizing this to other open source projects. This automated approach\ncould therefore be used by open source software developers to proactively\naddress hidden issues and to ensure that new changes do not conflict with past\ndecisions.", "AI": {"tldr": "The paper presents an automated system that uses Large Language Models to extract and analyze decision rationale in open source projects, helping developers avoid conflicting changes and maintain project coherence.", "motivation": "Contributors to open source projects need to understand project history and rationale behind past decisions, but manually analyzing related changes is time-consuming and currently lacks automated support.", "method": "The authors propose an automated approach for rationale analysis based on instantiating Kantara, a high-level rationale extraction and management architecture. This implementation uses pre-trained models, Large Language Models, and structure-based mechanisms to extract decision rationale and detect conflicts. The approach is tested on the OOM-Killer module of the Linux Kernel as well as five other active open source projects.", "result": "The automated approach demonstrated reasonable performance in supporting rationale analysis, successfully identifying relationships, potential conflicts, and reasoning issues. The system also effectively extracted decision and rationale sentences, suggesting promise for generalization to other open source projects.", "conclusion": "The proposed automated system for rationale extraction and conflict detection can assist open source developers in detecting hidden conflicts and reasoning problems, improving decision coherence and reducing design erosion, and is feasible for generalization across various projects."}}
{"id": "2506.11006", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11006", "abs": "https://arxiv.org/abs/2506.11006", "authors": ["Sai Krishna", "Balvinder Singh", "Sujoy Roychowdhury", "Giriprasad Sridhara", "Sourav Mazumdar", "Magnus Sandelin", "Dimitris Rentas", "Maciej Nalepa", "Karol Sawicki", "Jakub Gajda"], "title": "Test code generation at Ericsson using Program Analysis Augmented Fine Tuned LLMs", "comment": "Accepted at International Conference on Evaluation and Assessment in\n  Software Engineering (EASE), 2025", "summary": "We describe test code generation using Large Language Models (LLMs) in\nEricsson. Our input is a test step in natural language (English) and our output\nis code (Java) which accomplishes the test step. We describe how straight\nforward prompting does not suffice and results in LLM assuming functions and\nsignatures which are not present in the code repository. We then show how we\nalleviate the problem by a combination of Retrieval Augmented Generation (RAG)\nalong with prompt engineering that expanded the simple prompt with additional\ncontextual information using static program analysis. We then describe further\nimprovements that we obtained by fine-tuning the underlying LLM. The fine\ntuning is done based on a custom designed prompt template which has\npre-dependent classes, their public methods as well two exemplar outputs\nobtained from RAG. Our results establish that our fine tuned models help\nimprove the correspondence or conformity with the original developer written\ntest code as measured by the traditional metrics of F1-score based on the\nmethods used in the generated code. Fine tuning of a 8x7b Mixture of Experts\n(MoE) model leads to an average improvement of 8\\% over the base model and is\ncomparable to the scores on a much larger 8x22b MoE model.", "AI": {"tldr": "The paper presents an approach for generating accurate Java test code from natural language test steps using LLMs enhanced with retrieval (RAG), prompt engineering, and model fine-tuning, resulting in significant improvements in code accuracy and efficiency, even with smaller models.", "motivation": "Generating test code from natural language is valuable but challenging due to LLMs inventing functions/methods not present in the codebase. There is a need to reliably translate test steps in English to precise, workable Java code based on existing code context.", "method": "The authors use Retrieval Augmented Generation (RAG) and prompt engineering to provide additional context from the codebase to the LLM, augmented with static program analysis. They also fine-tune the LLM using custom prompt templates containing pre-dependent classes, their methods, and exemplar RAG outputs.", "result": "Their fine-tuned LLM improves correspondence with developer-written test code, as measured by F1-score for method usage. The fine-tuned 8x7b MoE model improves F1 by 8% over the base model and achieves scores close to the much larger 8x22b MoE model.", "conclusion": "Combining RAG, prompt engineering, and custom fine-tuning significantly improves LLM performance in generating contextually appropriate test code from natural language, making smaller models perform comparably to larger models while increasing accuracy and adherence to the codebase."}}
{"id": "2506.11007", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11007", "abs": "https://arxiv.org/abs/2506.11007", "authors": ["Rock Sabetto", "Emily Escamilla", "Devesh Agarwal", "Sujay Kandwal", "Justin F. Brunelle", "Scott Rosen", "Nitin Naik", "Samruddhi Thaker", "Eric O. Scott", "Jacob Zimmer", "Amit Madan", "Arun Sridharan", "Doug Wendt", "Michael Doyle", "Christopher Glasz", "Jasper Phillips", "William Macke", "Colin Diggs", "Michael Bartholf", "Zachary Robin", "Paul Ursino"], "title": "Impact of Comments on LLM Comprehension of Legacy Code", "comment": null, "summary": "Large language models (LLMs) have been increasingly integrated into software\nengineering and maintenance tasks due to their high performance with software\nengineering tasks and robust understanding of modern programming languages.\nHowever, the ability of LLMs to comprehend code written with legacy languages\nremains a research gap challenged by real-world legacy systems lacking or\ncontaining inaccurate documentation that may impact LLM comprehension. To\nassess LLM comprehension of legacy languages, there is a need for objective LLM\nevaluation. In order to objectively measure LLM comprehension of legacy\nlanguages, we need an efficient, quantitative evaluation method. We leverage\nmultiple-choice question answering (MCQA), an emerging LLM evaluation\nmethodology, to evaluate LLM comprehension of legacy code and the impact of\ncomment prevalence and inaccurate comments. In this work, we present\npreliminary findings on the impact of documentation on LLM comprehension of\nlegacy code and outline strategic objectives for future work.", "AI": {"tldr": "This paper explores how well large language models understand legacy programming code, especially when documentation is missing or inaccurate. Using multiple-choice questions to test comprehension, the authors find that documentation matters and suggest future research directions.", "motivation": "Large language models (LLMs) are becoming increasingly important for software engineering tasks, but their ability to understand code in legacy programming languages is still uncertain. Real-world legacy systems often lack proper or accurate documentation, making it even harder to evaluate LLMs' performance on such code.", "method": "The study uses multiple-choice question answering (MCQA) as a quantitative and efficient evaluation method. MCQA is used to assess how well LLMs understand legacy code and how the presence or inaccuracy of comments and documentation affect this comprehension.", "result": "Preliminary findings indicate that the presence and quality of code documentation do impact LLM comprehension of legacy code, though detailed quantitative results are not specified in the abstract. The study also identifies areas for further investigation.", "conclusion": "The research highlights the importance of proper code documentation for improving LLM comprehension of legacy languages and establishes MCQA as a promising evaluation methodology. It also sets forth objectives for further research in this area."}}
{"id": "2506.11008", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11008", "abs": "https://arxiv.org/abs/2506.11008", "authors": ["David Noever"], "title": "Encoding Software For Perpetuity: A Compact Representation Of Apollo 11 Guidance Code", "comment": null, "summary": "This brief note presents a novel method for encoding historic Apollo 11 Lunar\nModule guidance computer code into a single, compact Quick Response Code (QR\ncode) format, creating an accessible digital artifact for transmission and\narchival purposes. By applying tokenization, selective content preservation,\nand minimal HTML/JavaScript techniques, we successfully compressed key\ncomponents of the original Assembly Language Code (AGC) into a shareable,\npreservable, and scannable 3 kilobyte (KB) image. We evaluate multiple\ncompression strategies and their tradeoffs in terms of size, readability, and\nhistorical significance. This method addresses the challenge of making\nhistorically significant software artifacts available through modern mobile\ndevices without requiring specialized hardware or internet connectivity. While\nnumerous digital preservation methods exist for historic software, this\napproach balances accessibility with historical significance, offering a\ncomplementary method to traditional archival techniques. This work contributes\nto the broader field of computing heritage preservation by demonstrating how\nlandmark software can be made accessible instantly through contemporary mobile\ntechnologies.", "AI": {"tldr": "The paper presents a method to encode Apollo 11 Lunar Module guidance code into a single, scannable QR code using compression and selective preservation, making historic software instantly accessible through mobile devices without the need for internet or special hardware.", "motivation": "There is a need to make historically significant software, such as Apollo 11 Lunar Module guidance computer code, accessible and preservable, especially through modern mobile devices, without dependence on specialized hardware or internet.", "method": "The authors developed a novel encoding technique that uses tokenization, selective content preservation, and minimal HTML/JavaScript to compress the assembly code into a single 3KB QR code. They evaluated various compression strategies based on size, readability, and preservation of historical content.", "result": "A successful demonstration of encoding key components of the Apollo 11 guidance computer assembly code into a compact, easily scannable QR code. The resulting digital artifact is both shareable and preservable, accessible through mobile devices without internet.", "conclusion": "This method offers a new, accessible way to digitally preserve and share landmark computing artifacts, complementing traditional archival methods, and advances the field of software heritage preservation by leveraging contemporary mobile technology."}}
{"id": "2506.11009", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11009", "abs": "https://arxiv.org/abs/2506.11009", "authors": ["Jirat Pasuksmit", "Wannita Takerngsaksiri", "Patanamon Thongtanunam", "Chakkrit Tantithamthavorn", "Ruixiong Zhang", "Shiyan Wang", "Fan Jiang", "Jing Li", "Evan Cook", "Kun Chen", "Ming Wu"], "title": "Human-In-The-Loop Software Development Agents: Challenges and Future Directions", "comment": "The International Conference on Mining Software Repositories (MSR)\n  2025, Industry track", "summary": "Multi-agent LLM-driven systems for software development are rapidly gaining\ntraction, offering new opportunities to enhance productivity. At Atlassian, we\ndeployed Human-in-the-Loop Software Development Agents to resolve Jira work\nitems and evaluated the generated code quality using functional correctness\ntesting and GPT-based similarity scoring. This paper highlights two major\nchallenges: the high computational costs of unit testing and the variability in\nLLM-based evaluations. We also propose future research directions to improve\nevaluation frameworks for Human-In-The-Loop software development tools.", "AI": {"tldr": "The paper discusses deploying LLM-driven agents at Atlassian to automate Jira work item resolution, evaluating their code via functional tests and GPT-based scoring. It finds issues in testing costs and evaluation consistency, suggesting better assessment methods are needed.", "motivation": "There is increasing interest in using multi-agent LLM-driven systems to boost software development productivity. Evaluating the effectiveness and practicality of these systems is crucial, particularly within enterprise environments like Atlassian.", "method": "Human-in-the-Loop Software Development Agents were deployed to address Jira work items. The quality of code generated by these agents was assessed via functional correctness tests and similarity scoring by GPT models.", "result": "Two primary challenges were identified: (1) high computational cost associated with unit testing, and (2) inconsistent outcomes in LLM-based evaluation approaches.", "conclusion": "Future work should focus on developing improved and more efficient evaluation frameworks for Human-in-the-Loop software development systems."}}
{"id": "2506.11011", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11011", "abs": "https://arxiv.org/abs/2506.11011", "authors": ["Abhi Desai"], "title": "Enhancing Inventory Management with Progressive Web Applications (PWAs): A Scalable Solution for Small and Large Enterprises", "comment": null, "summary": "Efficient inventory management is crucial for both small and large\nenterprises to optimize operational workflows and reduce overhead costs. This\npaper explores the development and implementation of a Progressive Web\nApplication (PWA) designed to enhance the inventory management experience. The\napplication integrates key functionalities such as barcode and QR code\nscanning, geolocation-based warehouse identification, and cross-device\naccessibility. By leveraging PWA technology, the solution ensures offline\ncapabilities, responsive user experience, and seamless adaptability across\nvarious platforms. The study discusses the challenges and benefits of\nimplementing PWA in inventory management systems, including its limitations in\nperformance compared to native applications. Insights from the development\nprocess provide a roadmap for future developers looking to integrate PWA\ntechnology into enterprise applications. This research contributes to the\ngrowing domain of web-based inventory solutions, offering a scalable and\ncost-effective alternative to traditional inventory management software.", "AI": {"tldr": "This paper presents a PWA that improves inventory management with cross-device access, barcode/QR code scanning, and geolocation features, offering a flexible and affordable solution. It discusses both advantages and limitations, serving as a guide for developers and enterprises considering PWAs for inventory systems.", "motivation": "Enterprises need efficient inventory management to streamline operations and reduce costs, but existing solutions may lack cross-device accessibility, offline capabilities, and may be expensive or platform-dependent.", "method": "The paper develops and implements a Progressive Web Application (PWA) with functionalities such as barcode/QR code scanning, geolocation-based warehouse identification, and cross-device access. It analyzes both technical implementation and performance considerations, comparing PWA to native apps.", "result": "The PWA proved to be scalable, cost-effective, responsive, and capable of running offline across diverse platforms. However, the study notes that PWAs have some limitations in performance compared to native applications. Insights are given for future enterprise-level PWA development.", "conclusion": "PWA technology offers an effective, adaptable, and affordable approach for inventory management, although some trade-offs exist in performance relative to native solutions. The paper provides valuable guidance for future adoption in enterprise environments."}}
{"id": "2506.11013", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11013", "abs": "https://arxiv.org/abs/2506.11013", "authors": ["Filipe Fernandes", "Cl\u00e1udia Werner"], "title": "Toward a Brazilian Research Agenda in Quantum Software Engineering: A Systematic Mapping Study", "comment": "11 pages, 13 figures", "summary": "Context: Quantum Software Engineering (QSE) has emerged as a key field to\nsupport the development of reliable, maintainable, and scalable quantum\napplications, bridging advances in quantum computing with established practices\nin software engineering. Problem: Despite its growth, the field still suffers\nfrom fragmented knowledge, with a lack of standardized methodologies, tools,\nand guidelines tailored to the unique features of the quantum paradigm.\nAdditionally, countries like Brazil have had limited participation in the\ndevelopment of this emerging domain. Objective: This study aims to map the\nstate of the art in QSE by identifying current research trends, recurring\ncontributions, and existing gaps that can guide future investigations and\nstrategic initiatives. Methodology: A systematic mapping study was conducted\nanalyzing selected publications based on inclusion and exclusion criteria.\nArticles were categorized by study type, research type, and alignment with the\nSWEBOK knowledge areas. Results: Most of the reviewed studies are primary\nresearch articles written in English, with a strong focus on Software\nEngineering Models and Methods, Software Architecture, and Software Testing.\nConceptual proposals and technical solutions predominate, while empirical\nvalidations remain limited. Conclusions: Findings confirm that QSE is a\npromising but still maturing field. The standardization of practices, expansion\nof empirical studies, and inclusion of researchers from developing countries\nare crucial for advancing the discipline. Additionally, Brazilian contributions\nare still scarce, highlighting the urgent need to establish a national research\nagenda. As a main contribution, this study proposes a Brazilian Research Agenda\nin QSE, outlining priority areas and opportunities to foster a local scientific\ncommunity and accelerate progress in this emerging field.", "AI": {"tldr": "This paper systematically maps current Quantum Software Engineering research, revealing fragmentation, a shortage of empirical studies, and little involvement from Brazil. It proposes a Brazilian research agenda to address gaps and foster greater participation in this emerging field.", "motivation": "There is fragmented knowledge in QSE, with a lack of unified methodologies, tools, and guidelines tailored for quantum software, along with limited involvement from certain countries like Brazil. This fragmentary state hampers progress in the discipline.", "method": "The study uses a systematic mapping approach, selecting and analyzing relevant papers according to set inclusion and exclusion criteria. Studies were categorized by study type, research focus, and alignment with recognized software engineering knowledge areas (SWEBOK).", "result": "Research in QSE is mostly conceptual and technical, focusing on models, architecture, and software testing, but lacks empirical validation. Most research originates from outside Brazil, with scarce contributions from Brazilian scholars.", "conclusion": "Quantum Software Engineering (QSE) is a promising but still immature field, requiring more standardized practices, empirical validation, and broader global participation. Brazilian involvement remains limited, pointing to the need for a national research agenda."}}
{"id": "2506.11014", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11014", "abs": "https://arxiv.org/abs/2506.11014", "authors": ["Benedetta Donato", "Leonardo Mariani", "Daniela Micucci", "Oliviero Riganelli", "Marco Somaschini"], "title": "MultiMind: A Plug-in for the Implementation of Development Tasks Aided by AI Assistants", "comment": null, "summary": "The integration of AI assistants into software development workflows is\nrapidly evolving, shifting from automation-assisted tasks to collaborative\ninteractions between developers and AI. Large Language Models (LLMs) have\ndemonstrated their effectiveness in several development activities, including\ncode completion, test case generation, and documentation production. However,\nembedding AI-assisted tasks within Integrated Development Environments (IDEs)\npresents significant challenges. It requires designing mechanisms to invoke AI\nassistants at the appropriate time, coordinate interactions with multiple\nassistants, process the generated outputs, and present feedback in a way that\nseamlessly integrates with the development workflow. To address these issues,\nwe introduce MultiMind, a Visual Studio Code plug-in that streamlines the\ncreation of AI-assisted development tasks. MultiMind provides a modular and\nextensible framework, enabling developers to cost-effectively implement and\nexperiment with new AI-powered interactions without the need for complex IDE\ncustomizations. MultiMind has been tested in two use cases: one for the\nautomatic generation of code comments and the other about the definition of\nAI-powered chat.", "AI": {"tldr": "MultiMind is a VSCode plugin framework that makes it easy for developers to integrate, use, and experiment with AI-powered coding assistants without complex IDE modifications, as demonstrated in two real-world use cases.", "motivation": "Integrating AI assistants into software development, especially within IDEs, is challenging despite the demonstrated effectiveness of LLMs in coding tasks. Existing tools often require complex customizations to support new AI-powered interactions.", "method": "The authors introduce MultiMind, a Visual Studio Code plug-in designed as a modular and extensible framework. It simplifies the development and integration of AI-assisted tasks into the IDE by enabling easy invocation and interaction with multiple AI assistants, handling output processing, and presenting feedback seamlessly.", "result": "MultiMind was tested on two use cases: automatic generation of code comments and the definition of AI-powered chat, demonstrating its flexibility and utility for integrating AI into developer workflows.", "conclusion": "MultiMind provides a practical, cost-effective solution for integrating and experimenting with AI-powered development tools in IDEs, lowering the barrier to entry for developers to leverage AI assistance."}}
{"id": "2506.11016", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11016", "abs": "https://arxiv.org/abs/2506.11016", "authors": ["Lelanthran Manickum"], "title": "ZjsComponent: A Pragmatic Approach to Modular, Reusable UI Fragments for Web Development", "comment": "12 pages, 7 figures", "summary": "In this paper, I present ZjsComponent, a lightweight and framework-agnostic\nweb component designed for creating modular, reusable UI elements with minimal\ndeveloper overhead. ZjsComponent is an example implementation of an approach to\ncreating components and object instances that can be used purely from HTML.\nUnlike traditional approaches to components, the approach implemented by\nZjsComponent does not require build-steps, transpiling, pre-compilation, any\nspecific ecosystem or any other dependency. All that is required is that the\nbrowser can load and execute Javascript as needed by Web Components.\nZjsComponent allows dynamic loading and isolation of HTML+JS fragments,\noffering developers a simple way to build reusable interfaces with ease. This\napproach is dependency-free, provides significant DOM and code isolation, and\nsupports simple lifecycle hooks as well as traditional methods expected of an\ninstance of a class.", "AI": {"tldr": "ZjsComponent is a dependency-free, framework-agnostic web component for easily building modular, reusable UI elements directly in HTML, without the need for builds or special ecosystems.", "motivation": "The motivation of this paper is to simplify the creation of modular and reusable UI elements, reducing developer overhead and eliminating dependencies, build steps, or complex setup.", "method": "The method involves designing and implementing ZjsComponent, a framework-agnostic web component that can be used purely from HTML by leveraging standard browser support for JavaScript and Web Components.", "result": "ZjsComponent enables dynamic loading and isolation of HTML and JS code fragments, provides dependency-free operation, and supports reusable interfaces with simple lifecycle hooks, along with code and DOM isolation.", "conclusion": "ZjsComponent offers a straightforward, lightweight solution for creating reusable UI components without any build process or ecosystem requirements, making UI development simpler and more accessible."}}
{"id": "2506.11018", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11018", "abs": "https://arxiv.org/abs/2506.11018", "authors": ["Grigory Tsiperman"], "title": "Formation of requirements traceability in the process of information systems design", "comment": "12 pages, 4 figures, 2025 the 8th International Conference on\n  Information Management", "summary": "The traceability of requirements in the information system design process is\nconsidered an essential property of the project, one of its quality\ncharacteristics. The point here is that traceability provides the methods of\nvalidation and verification of software systems, and that the system model\nbased on requirements traceability reduces the system's dependence on\ndevelopers and, in general, makes it as straightforward as possible. One of the\nchallenges of the traceability process, dubbed \"The grand challenge of\ntraceability\" among traceability researchers, is its integration into the\ndesign process. In this paper, to achieve this goal, we propose the application\nof the Adaptive Clustering Method (ACM) of Information Systems developed by the\nauthor, which is based on the idea of a seamless system architecture that\nprovides explicit interconnection of project artifacts of different levels of\nabstraction.", "AI": {"tldr": "Tracing requirements is hard to integrate into design; the authors use a new clustering method connecting artifacts across abstraction levels to make traceability easier and less developer-dependent.", "motivation": "Requirements traceability is crucial for software quality, enabling validation and verification, but is challenging to integrate smoothly into the design process of information systems.", "method": "The paper proposes using the Adaptive Clustering Method (ACM), developed by the author, which offers seamless system architecture and explicit interconnection among project artifacts at different abstraction levels.", "result": "Using ACM facilitates the integration of traceability into the system design process, improving traceability and making the system less dependent on individual developers.", "conclusion": "The application of ACM effectively addresses the integration challenge of requirements traceability in the system design process by providing a clear and seamless connection between project artifacts."}}
{"id": "2506.11019", "categories": ["cs.SE", "68T01, 68N30", "I.2.6; D.2.7"], "pdf": "https://arxiv.org/pdf/2506.11019", "abs": "https://arxiv.org/abs/2506.11019", "authors": ["Vincent Koc", "Jacques Verre", "Douglas Blank", "Abigail Morgan"], "title": "Mind the Metrics: Patterns for Telemetry-Aware In-IDE AI Application Development using the Model Context Protocol (MCP)", "comment": "16 pages, 5 figures, conference preprint submission. Conceptual\n  systems architecture paper on telemetry-driven prompt optimization and IDE\n  design patterns for AI development. Builds on Opik MCP open-source\n  architecture and Comet trace infrastructure", "summary": "AI development environments are evolving into observability first platforms\nthat integrate real time telemetry, prompt traces, and evaluation feedback into\nthe developer workflow. This paper introduces telemetry aware integrated\ndevelopment environments (IDEs) enabled by the Model Context Protocol (MCP), a\nsystem that connects IDEs with prompt metrics, trace logs, and versioned\ncontrol for real time refinement. We present design patterns for local prompt\niteration, CI based optimization, and autonomous agents that adapt behavior\nusing telemetry. Rather than focusing on a single algorithm, we describe an\narchitecture that supports integration with frameworks like DSPy, PromptWizard,\nand Prompts as Programs. We demonstrate this through Opik, an open source MCP\nserver for LLM telemetry, and position our approach within the emerging LLMOps\necosystem. This work lays a foundation for future research on prompt\noptimization, IDE agent tooling, and empirical benchmarking in telemetry rich\nAI development workflows.", "AI": {"tldr": "The paper introduces telemetry-aware IDEs powered by the Model Context Protocol, integrating real-time AI telemetry into developer workflows. With the open-source Opik server, the authors demonstrate improved LLM development practices, setting a foundation for future AI observability tools and research.", "motivation": "There is a growing need for AI development environments that can provide real-time observability, such as telemetry, prompt traces, and evaluation feedback, to streamline and improve the developer workflow for AI and LLM applications.", "method": "The authors propose telemetry-aware IDEs enabled by the Model Context Protocol (MCP), which connects the IDE with prompt metrics, trace logs, and version control for real-time refinement. They present design patterns for prompt iteration, CI-based optimization, and the use of autonomous agents that adapt to telemetry. The architecture supports integration with existing frameworks and is demonstrated through Opik, an open source MCP server.", "result": "The paper demonstrates the feasibility of the MCP architecture through Opik and shows its compatibility with popular LLMOps frameworks. This approach enables real-time prompt refinement, telemetry-driven development, and paves the way for advanced tools in AI workflows.", "conclusion": "This work establishes a foundational architecture for observability-first AI development platforms, facilitating prompt optimization, rich developer feedback, and empirical benchmarking within AI workflows. It also highlights future directions for research in telemetry-rich AI development and the LLMOps ecosystem."}}
{"id": "2506.11020", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11020", "abs": "https://arxiv.org/abs/2506.11020", "authors": ["Thayn\u00e1 Camargo da Silva"], "title": "Extracting Knowledge Graphs from User Stories using LangChain", "comment": "Master thesis work", "summary": "This thesis introduces a novel methodology for the automated generation of\nknowledge graphs from user stories by leveraging the advanced capabilities of\nLarge Language Models. Utilizing the LangChain framework as a basis, the User\nStory Graph Transformer module was developed to extract nodes and relationships\nfrom user stories using an LLM to construct accurate knowledge graphs.This\ninnovative technique was implemented in a script to fully automate the\nknowledge graph extraction process. Additionally, the evaluation was automated\nthrough a dedicated evaluation script, utilizing an annotated dataset for\nassessment. By enhancing the visualization and understanding of user\nrequirements and domain concepts, this method fosters better alignment between\nsoftware functionalities and user expectations, ultimately contributing to more\neffective and user-centric software development processes.", "AI": {"tldr": "This thesis presents an automated method to generate knowledge graphs from user stories using LLMs and LangChain, enabling better visualization and alignment of requirements, and supporting more effective user-centric software development.", "motivation": "Extracting knowledge from user stories is crucial for aligning software functionality with user needs, but manual extraction is tedious and error-prone.", "method": "A User Story Graph Transformer module utilizing Large Language Models (LLMs) with the LangChain framework was developed to extract nodes and relationships from user stories, automate knowledge graph creation, and evaluate results using an annotated dataset.", "result": "The approach successfully automated the extraction and evaluation of knowledge graphs from user stories, improving the visualization and understanding of user requirements and domain concepts.", "conclusion": "The methodology enables fully automated, accurate knowledge graph generation from user stories, fostering improved alignment between software and user expectations and supporting more user-centric software development."}}
{"id": "2506.11021", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11021", "abs": "https://arxiv.org/abs/2506.11021", "authors": ["Chaitanya Ravuri", "Saman Amarasinghe"], "title": "Eliminating Hallucination-Induced Errors in LLM Code Generation with Functional Clustering", "comment": "9 pages, 1 figure", "summary": "Modern code-generation LLMs can already solve a large fraction of programming\nproblems, yet they still hallucinate subtle bugs that make their outputs unsafe\nfor autonomous deployment. We present functional clustering, a black-box\nwrapper that eliminates nearly all hallucination-induced errors while providing\na tunable confidence score. The wrapper samples many candidate programs,\nexecutes each on a self-generated test suite, and clusters candidates whose I/O\nbehavior is identical; the empirical mass of the largest cluster serves as an\nexact confidence estimate. A single scalar threshold on this estimate lets\nusers trade coverage for reliability with exponential guarantees. On\nLiveCodeBench our verifier preserves baseline pass@1 on solvable tasks yet\nslashes the error rate of returned answers from ~65% to 2%, and drives it to 0%\nat a conservative threshold while still answering 15.6% of prompts. Manual\naudits show that the few residual mistakes stem from prompt misinterpretation,\nnot random generation noise, narrowing future work to specification clarity.\nBecause the method requires only sampling and sandbox execution, it applies\nunchanged to closed-source APIs and future models, offering a practical path\ntoward dependable, autonomous code generation. Our code is available on Github\n(https://github.com/20ChaituR/functional-clustering).", "AI": {"tldr": "Functional clustering is a black-box method that drastically reduces hallucination errors in code-generation LLMs, providing tunable reliability without modifying models. It offers a significant step toward safe, autonomous code generation and is readily usable by the community.", "motivation": "Current large language models (LLMs) for code generation can solve many programming tasks but often introduce subtle, hard-to-catch bugs, making their outputs unreliable for unsupervised use.", "method": "The paper introduces 'functional clustering,' a black-box wrapper that samples many code outputs from the model, executes them with auto-generated tests, and clusters outputs by identical input/output (I/O) behavior. The largest cluster's size provides a confidence estimate, which users can threshold to balance reliability and coverage.", "result": "Functional clustering dramatically reduces the proportion of hallucination-induced errors. On the LiveCodeBench benchmark, it keeps the correct answer rate similar to the baseline but reduces the error rate from approximately 65% to 2%. Setting a conservative confidence threshold eliminates all errors in 15.6% of cases. Manual review shows that remaining faults are due to ambiguous prompts, narrowing the future challenge to prompt specification.", "conclusion": "Functional clustering offers a practical and model-agnostic way to make code-generation LLMs reliable for autonomous deployment. The approach is robust, does not require model internals, and can be used with any model or API. Code is publicly available for use and further research."}}
{"id": "2506.11022", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11022", "abs": "https://arxiv.org/abs/2506.11022", "authors": ["Shivani Shukla", "Himanshu Joshi", "Romilla Syed"], "title": "Security Degradation in Iterative AI Code Generation -- A Systematic Analysis of the Paradox", "comment": "Keywords - Large Language Models, Security Vulnerabilities,\n  AI-Generated Code, Iterative Feedback, Software Security, Secure Coding\n  Practices, Feedback Loops, LLM Prompting Strategies", "summary": "The rapid adoption of Large Language Models(LLMs) for code generation has\ntransformed software development, yet little attention has been given to how\nsecurity vulnerabilities evolve through iterative LLM feedback. This paper\nanalyzes security degradation in AI-generated code through a controlled\nexperiment with 400 code samples across 40 rounds of \"improvements\" using four\ndistinct prompting strategies. Our findings show a 37.6% increase in critical\nvulnerabilities after just five iterations, with distinct vulnerability\npatterns emerging across different prompting approaches. This evidence\nchallenges the assumption that iterative LLM refinement improves code security\nand highlights the essential role of human expertise in the loop. We propose\npractical guidelines for developers to mitigate these risks, emphasizing the\nneed for robust human validation between LLM iterations to prevent the\nparadoxical introduction of new security issues during supposedly beneficial\ncode \"improvements\".", "AI": {"tldr": "Repeatedly refining LLM-generated code can make it less secure, not more. Human review is vital to prevent security issues from accumulating during code iteration.", "motivation": "The motivation is to investigate how security vulnerabilities change during iterative feedback and refinement of code generated by Large Language Models (LLMs), a topic previously underexplored despite the widespread use of LLMs in software development.", "method": "The study conducted a controlled experiment using 400 code samples subjected to 40 rounds of iterative improvements via four distinct prompting strategies. The security of the code samples was analyzed after each round to observe any evolution in vulnerabilities.", "result": "A 37.6% increase in critical security vulnerabilities was observed after just five iterations. Different prompting strategies led to distinct patterns in the emergence of vulnerabilities, suggesting that not all refinement strategies are equally secure.", "conclusion": "Iterative refinement of LLM-generated code does not necessarily improve\u2014and may actually decrease\u2014security. Human expertise remains essential, and robust human validation should be emphasized between LLM iterations to avoid unintentionally introducing new security risks."}}
{"id": "2506.11051", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11051", "abs": "https://arxiv.org/abs/2506.11051", "authors": ["Sung Une Lee", "Liming Dong", "Zhenchang Xing", "Muhammad Ejaz Ahmed", "Stefan Avgoustakis"], "title": "Software Security Mapping Framework: Operationalization of Security Requirements", "comment": "28 pages, 13 figures, 6 tables", "summary": "The escalating complexity of modern software development environments has\nheightened concerns around supply chain security. However, existing frameworks\noften fall short in translating abstract security principles into concrete,\nactionable practices. This paper introduces the Software Security Mapping\nFramework, a structured solution designed to operationalize security\nrequirements across hierarchical levels -- from high-level regulatory standards\n(e.g., ISM, Australia cybersecurity standard published by the Australian\nSignals Directorate), through mid-level frameworks (e.g., NIST SSDF, the U.S.\nSecure Software Development Framework), to fine-grained technical activities\n(e.g., SLSA, a software supply chain security framework). Developed through\ncollaborative research with academic experts and industry practitioners, the\nframework systematically maps 131 refined security requirements to over 400\nactionable operational steps spanning the software development lifecycle. It is\ngrounded in four core security goals: Secure Software Environment, Secure\nSoftware Development, Software Traceability, and Vulnerability Management. Our\napproach leverages the KAOS goal modeling methodology to establish traceable\nlinkages between strategic goals and tactical operations, enhancing clarity,\naccountability, and practical implementation. To facilitate adoption, we\nprovide a web-based navigation tool for interactive exploration of the\nframework. A real-world case study based on the Log4j vulnerability illustrates\nthe framework's utility by generating a tailored checklist aligned with\nindustry best practices. Additionally, we offer a structured, machine-readable\nOSCAL Catalog Model of the Software Security Mapping Framework, enabling\norganizations to automate implementation, streamline compliance processes, and\nrespond effectively to evolving security risks.", "AI": {"tldr": "This paper presents a comprehensive framework and toolkit that translates high-level software supply chain security requirements into practical, trackable development practices, addressing real-world risks and facilitating compliance and automation in organizations.", "motivation": "The increasing complexity of software development environments has raised concerns about supply chain security. Existing frameworks fail to effectively translate high-level security concepts into actionable developer practices, leaving a gap in practical implementation.", "method": "The authors developed the Software Security Mapping Framework through collaboration with academic experts and industry practitioners. They systematically mapped 131 security requirements to over 400 operational steps using the KAOS goal modeling methodology, and validated the framework in a real-world case study. They also provided a web tool and a machine-readable OSCAL Catalog Model for interactive use and automation.", "result": "The framework enables traceable and practical mapping from high-level regulatory standards to technical activities, structured around four security goals. The Log4j case study demonstrated its effectiveness in generating an actionable, best practices checklist. The OSCAL Catalog Model and web tool further facilitate adoption and automation for security and compliance.", "conclusion": "The Software Security Mapping Framework bridges the gap between abstract security mandates and actionable development practices, providing organizations with a comprehensive, structured approach to operationalizing supply chain security requirements and enhancing software security management."}}
{"id": "2506.11058", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11058", "abs": "https://arxiv.org/abs/2506.11058", "authors": ["Ziga Kovacic", "Celine Lee", "Justin Chiu", "Wenting Zhao", "Kevin Ellis"], "title": "Refactoring Codebases through Library Design", "comment": "26 pages", "summary": "Maintainable and general software allows developers to build robust\napplications efficiently, yet achieving these qualities often requires\nrefactoring specialized solutions into reusable components. This challenge\nbecomes particularly relevant as code agents become increasingly accurate at\nsolving isolated programming problems. We investigate code agents' capacity to\nrefactor code in ways supporting growth and reusability. We present both a\nmethod and a benchmark for refactoring: Librarian, a sample-and-rerank method\nfor generating reusable libraries, and Minicode, a benchmark where code agents\nmust minimize and refactor multiple independent solutions into a joint library.\nCompared to state-of-the-art code agents, Librarian achieves strong results on\nboth compression and correctness on Minicode, obtaining compression rates\n1.6-2x better than coding agents while also improving correctness. We\nopen-source our code and benchmark at https://code-refactor.github.io/.", "AI": {"tldr": "Librarian is a new method that helps code agents refactor and consolidate code into reusable libraries, with a new benchmark (Minicode) proving it outperforms existing solutions in both code efficiency and correctness.", "motivation": "Modern software development demands maintainable and general solutions, but transitioning specialized code into reusable components is challenging. This need is heightened as code agents excel at solving isolated tasks but struggle with broader software engineering goals such as refactoring for reusability.", "method": "The authors propose a sample-and-rerank method called Librarian for generating reusable libraries and introduce a new benchmark, Minicode, where code agents must refactor and consolidate multiple solutions into a single library. The methodology involves evaluating code agents on their ability to minimize code and enhance reusability while maintaining correctness.", "result": "Librarian significantly outperforms state-of-the-art code agents both in terms of code compression (achieving 1.6-2x better rates) and correctness on the Minicode benchmark.", "conclusion": "Librarian, along with the Minicode benchmark, advances the field of AI-assisted code refactoring for reusability, showing that automated agents can be improved for software engineering tasks beyond isolated problem-solving."}}
{"id": "2506.11059", "categories": ["cs.SE", "cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11059", "abs": "https://arxiv.org/abs/2506.11059", "authors": ["Hanxi Guo", "Siyuan Cheng", "Kaiyuan Zhang", "Guangyu Shen", "Xiangyu Zhang"], "title": "CodeMirage: A Multi-Lingual Benchmark for Detecting AI-Generated and Paraphrased Source Code from Production-Level LLMs", "comment": null, "summary": "Large language models (LLMs) have become integral to modern software\ndevelopment, producing vast amounts of AI-generated source code. While these\nmodels boost programming productivity, their misuse introduces critical risks,\nincluding code plagiarism, license violations, and the propagation of insecure\nprograms. As a result, robust detection of AI-generated code is essential. To\nsupport the development of such detectors, a comprehensive benchmark that\nreflects real-world conditions is crucial. However, existing benchmarks fall\nshort -- most cover only a limited set of programming languages and rely on\nless capable generative models. In this paper, we present CodeMirage, a\ncomprehensive benchmark that addresses these limitations through three major\nadvancements: (1) it spans ten widely used programming languages, (2) includes\nboth original and paraphrased code samples, and (3) incorporates outputs from\nten state-of-the-art production-level LLMs, including both reasoning and\nnon-reasoning models from six major providers. Using CodeMirage, we evaluate\nten representative detectors across four methodological paradigms under four\nrealistic evaluation configurations, reporting results using three\ncomplementary metrics. Our analysis reveals nine key findings that uncover the\nstrengths and weaknesses of current detectors, and identify critical challenges\nfor future work. We believe CodeMirage offers a rigorous and practical testbed\nto advance the development of robust and generalizable AI-generated code\ndetectors.", "AI": {"tldr": "The paper introduces CodeMirage, a large benchmark addressing the shortcomings of existing datasets for detecting AI-generated code. Spanning ten languages and multiple LLMs, it provides rigorous evaluation of detectors, revealing important insights and guiding the development of better detection tools.", "motivation": "The motivation behind this paper is the growing prevalence of AI-generated source code by large language models (LLMs) in modern software development, which introduces risks such as plagiarism, license violations, and insecure code. Existing benchmarks for detecting AI-generated code are limited and do not adequately reflect real-world scenarios.", "method": "The authors developed CodeMirage, a benchmark designed to evaluate AI-generated code detectors more rigorously. CodeMirage covers ten popular programming languages, includes both original and paraphrased code samples, and features outputs from ten state-of-the-art LLMs. They used CodeMirage to test ten different detectors across four methodological paradigms, using four evaluation settings and three metrics.", "result": "The analysis led to nine key findings on the performance of current code detectors, highlighting both their strengths and shortcomings, and pinpointing important challenges for future research.", "conclusion": "CodeMirage serves as a comprehensive and practical benchmark that will facilitate the advancement of robust and generalizable detectors for AI-generated code."}}
{"id": "2506.11060", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11060", "abs": "https://arxiv.org/abs/2506.11060", "authors": ["Ramneet Singh", "Sathvik Joel", "Abhav Mehrotra", "Nalin Wadhwa", "Ramakrishna B Bairi", "Aditya Kanade", "Nagarajan Natarajan"], "title": "Code Researcher: Deep Research Agent for Large Systems Code and Commit History", "comment": null, "summary": "Large Language Model (LLM)-based coding agents have shown promising results\non coding benchmarks, but their effectiveness on systems code remains\nunderexplored. Due to the size and complexities of systems code, making changes\nto a systems codebase is a daunting task, even for humans. It requires\nresearching about many pieces of context, derived from the large codebase and\nits massive commit history, before making changes. Inspired by the recent\nprogress on deep research agents, we design the first deep research agent for\ncode, called Code Researcher, and apply it to the problem of generating patches\nfor mitigating crashes reported in systems code. Code Researcher performs\nmulti-step reasoning about semantics, patterns, and commit history of code to\ngather sufficient context. The context is stored in a structured memory which\nis used for synthesizing a patch. We evaluate Code Researcher on kBenchSyz, a\nbenchmark of Linux kernel crashes, and show that it significantly outperforms\nstrong baselines, achieving a crash-resolution rate of 58%, compared to 37.5%\nby SWE-agent. On an average, Code Researcher explores 10 files in each\ntrajectory whereas SWE-agent explores only 1.33 files, highlighting Code\nResearcher's ability to deeply explore the codebase. Through another experiment\non an open-source multimedia software, we show the generalizability of Code\nResearcher. Our experiments highlight the importance of global context\ngathering and multi-faceted reasoning for large codebases.", "AI": {"tldr": "Code Researcher is a new LLM-based coding agent designed to patch crashes in large systems codebases by deeply gathering code context and commit history. It outperforms previous agents, particularly in complex settings like the Linux kernel, underscoring the need for comprehensive context understanding in automated code repair.", "motivation": "LLM-based coding agents have shown promise, but their effectiveness in the complex domain of systems code (such as making changes to large, intricate codebases like the Linux kernel) is not well understood. Successfully patching systems code requires synthesizing a lot of contextual information, which is a significant challenge for both humans and AI agents.", "method": "The authors introduce 'Code Researcher,' a deep research agent for code. It performs multi-step reasoning over code semantics, coding patterns, and historical commits, storing gathered context in a structured memory to synthesize patches. The agent's performance is evaluated on the kBenchSyz benchmark (Linux kernel crashes) and on open-source multimedia software to test generalizability.", "result": "Code Researcher significantly outperforms baselines like the SWE-agent, achieving a 58% crash-resolution rate (vs. 37.5% for SWE-agent) on Linux kernel patching tasks. It explores notably more files per trajectory, demonstrating deeper exploration and superior context gathering. Code Researcher also generalizes to other codebases beyond the kernel.", "conclusion": "Global context gathering and multi-step, multi-faceted reasoning are crucial for effectively patching large systems codebases; Code Researcher demonstrates strong capabilities in these aspects and sets a new state-of-the-art on related benchmarks."}}
{"id": "2506.11066", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11066", "abs": "https://arxiv.org/abs/2506.11066", "authors": ["Jiahui Geng", "Fengyu Cai", "Shaobo Cui", "Qing Li", "Liangwei Chen", "Chenyang Lyu", "Haonan Li", "Derui Zhu", "Walter Pretschner", "Heinz Koeppl", "Fakhri Karray"], "title": "CoQuIR: A Comprehensive Benchmark for Code Quality-Aware Information Retrieval", "comment": null, "summary": "Code retrieval is essential in modern software development, as it boosts code\nreuse and accelerates debugging. However, current benchmarks primarily\nemphasize functional relevance while neglecting critical dimensions of software\nquality. Motivated by this gap, we introduce CoQuIR, the first large-scale,\nmultilingual benchmark specifically designed to evaluate quality-aware code\nretrieval across four key dimensions: correctness, efficiency, security, and\nmaintainability. CoQuIR provides fine-grained quality annotations for 42,725\nqueries and 134,907 code snippets in 11 programming languages, and is\naccompanied by two quality-centric evaluation metrics: Pairwise Preference\nAccuracy and Margin-based Ranking Score. Using CoQuIR, we benchmark 23\nretrieval models, covering both open-source and proprietary systems, and find\nthat even top-performing models frequently fail to distinguish buggy or\ninsecure code from their more robust counterparts. Furthermore, we conduct\npreliminary investigations into training methods that explicitly encourage\nretrievers to recognize code quality. Using synthetic datasets, we demonstrate\npromising improvements in quality-aware metrics across various models, without\nsacrificing semantic relevance. Downstream code generation experiments further\nvalidate the effectiveness of our approach. Overall, our work highlights the\nimportance of integrating quality signals into code retrieval systems, laying\nthe groundwork for more trustworthy and robust software development tools.", "AI": {"tldr": "The paper introduces CoQuIR, a comprehensive benchmark for evaluating code retrieval systems on code quality, not just functional relevance. Results show current models often miss low-quality code, but new quality-aware approaches perform better, emphasizing the need for quality integration in future tools.", "motivation": "Current code retrieval benchmarks focus mainly on functional relevance, overlooking essential software quality aspects such as correctness, efficiency, security, and maintainability. This creates a gap in the evaluation and development of retrieval systems that need to recognize and prioritize high-quality code.", "method": "The paper introduces CoQuIR, a multilingual benchmark with 42,725 queries and 134,907 code snippets annotated for four code quality dimensions across 11 programming languages. Two new evaluation metrics, Pairwise Preference Accuracy and Margin-based Ranking Score, are proposed. The authors evaluate 23 code retrieval models and explore training methods using synthetic datasets to enhance quality-aware code retrieval.", "result": "Existing top-performing retrieval models often cannot distinguish between insecure or buggy code and higher quality alternatives. The experiments with quality-aware training methods show promising improvements in quality sensitive metrics while maintaining semantic retrieval relevance, and the approach is also validated through downstream code generation tasks.", "conclusion": "Integrating code quality signals into code retrieval systems is both necessary and effective, demonstrating improved trustworthiness and robustness in retrieval and generation tasks. CoQuIR sets a foundation for research in quality-aware software tools."}}
{"id": "2506.11076", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11076", "abs": "https://arxiv.org/abs/2506.11076", "authors": ["Minyu Chen", "Guoqiang Li", "Ling-I Wu", "Ruibang Liu"], "title": "DCE-LLM: Dead Code Elimination with Large Language Models", "comment": "Accepted by regular paper in NAACL 2025, with 13 pages, 5 figures", "summary": "Dead code introduces several challenges in software development, such as\nincreased binary size and maintenance difficulties. It can also obscure logical\nerrors and be exploited for obfuscation in malware. For LLM-based code-related\ntasks, dead code introduces vulnerabilities that can mislead these models,\nraising security concerns. Although modern compilers and IDEs offer dead code\nelimination, sophisticated patterns can bypass these tools. A universal\napproach that includes classification, location, explanation, and correction is\nneeded, yet current tools often require significant manual effort. We present\nDCE-LLM, a framework for automated dead code elimination using a small CodeBERT\nmodel with an attribution-based line selector to efficiently locate suspect\ncode. LLMs then generate judgments and explanations, fine-tuned on a\nlarge-scale, annotated dead code dataset to provide detailed explanations and\npatches. DCE-LLM outperforms existing tools, with advanced unreachability\ndetection, automated correction, and support for multiple programming\nlanguages. Experimental results show DCE-LLM achieves over 94% F1 scores for\nunused and unreachable code, significantly surpassing GPT-4o by 30%.", "AI": {"tldr": "DCE-LLM is a new automated framework for detecting and fixing dead code by combining CodeBERT and LLMs, outperforming existing tools and GPT-4o by a large margin in both accuracy and language support.", "motivation": "Dead code in software complicates maintenance, increases binary size, introduces security risks for LLM-based code tasks, and can be exploited by malware. Current elimination tools have limited effectiveness and require much manual intervention, necessitating a more comprehensive automated solution.", "method": "The paper proposes DCE-LLM, a framework that uses a lightweight CodeBERT model with an attribution-based line selector to identify suspicious dead code lines. Large Language Models are then employed to generate explanations and patches, trained on a large annotated dead code dataset. The approach automates classification, location, explanation, and correction of dead code.", "result": "DCE-LLM achieves over 94% F1 scores in detecting unused and unreachable code and significantly surpasses GPT-4o by 30%. It offers advanced features including detection of complex dead code patterns, automated corrections, and multi-language support.", "conclusion": "DCE-LLM provides a comprehensive and automated solution for dead code elimination, outperforming state-of-the-art tools and reducing the need for manual effort in handling dead code."}}
{"id": "2506.11084", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11084", "abs": "https://arxiv.org/abs/2506.11084", "authors": ["Yordan Kalmukov"], "title": "Research and Analysis of Employers' Opinion on the Necessary Skills that Students in the Field of Web Programming Should Possess", "comment": null, "summary": "In the era of artificial intelligence (AI) and chatbots, based on large\nlanguage models that can generate programming code in any language, write texts\nand summarize information, it is obvious that the requirements of employers for\ngraduating students have already changed. The modern IT world offers\nsignificant automation of programming through software frameworks and a huge\nset of third-party libraries and application programming interfaces (APIs). All\nthese tools provide most of the necessary functionality out of the box (already\nimplemented), and quite naturally the question arises as to what is more useful\nfor students - to teach how to use these ready-made tools or the basic\nprinciples of working and development of web applications from scratch. This\npaper analyzes the results of a survey conducted among IT employers, aimed to\nidentify what, in their opinion, are the necessary technical skills that\ngraduating students in the field of Web Programming should possess in order to\njoin the company's work as quickly and effectively as possible.", "AI": {"tldr": "This paper surveys IT employers to determine which Web Programming skills are most important for new graduates, weighing the value of foundational coding knowledge versus proficiency with modern tools and frameworks in a rapidly automating industry.", "motivation": "In the AI era, programming is increasingly automated using frameworks, libraries, and APIs, prompting a reevaluation of what skills are most valuable for IT graduates. There is a debate between focusing on teaching students to use these tools or on foundational programming skills.", "method": "The paper analyzes the results of a survey conducted among IT employers to identify the technical skills they consider essential for graduating students in Web Programming to quickly and effectively integrate into company work.", "result": "The survey results highlight employers' perspectives on the specific skills and knowledge that new IT graduates should have in the area of Web Programming, helping clarify current industry expectations.", "conclusion": "Understanding employer requirements can inform educational priorities, ensuring that curriculum in Web Programming aligns with the rapidly changing demands of the IT industry in the AI and automation age."}}
{"id": "2506.11085", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.IR", "cs.LG", "cs.LO", "I.2.6; H.3.3; I.2.3"], "pdf": "https://arxiv.org/pdf/2506.11085", "abs": "https://arxiv.org/abs/2506.11085", "authors": ["Justin Asher"], "title": "LeanExplore: A search engine for Lean 4 declarations", "comment": "16 pages, 1 figure. Project website: https://www.leanexplore.com/ ,\n  Code: https://github.com/justincasher/lean-explore", "summary": "The expanding Lean 4 ecosystem poses challenges for navigating its vast\nlibraries. This paper introduces LeanExplore, a search engine for Lean 4\ndeclarations. LeanExplore enables users to semantically search for statements,\nboth formally and informally, across select Lean 4 packages (including\nBatteries, Init, Lean, Mathlib, PhysLean, and Std). This search capability is\npowered by a hybrid ranking strategy, integrating scores from a multi-source\nsemantic embedding model (capturing conceptual meaning from formal Lean code,\ndocstrings, AI-generated informal translations, and declaration titles), BM25+\nfor keyword-based lexical relevance, and a PageRank-based score reflecting\ndeclaration importance and interconnectedness. The search engine is accessible\nvia a dedicated website (https://www.leanexplore.com/) and a Python API\n(https://github.com/justincasher/lean-explore). Furthermore, the database can\nbe downloaded, allowing users to self-host the service. LeanExplore integrates\neasily with LLMs via the model context protocol (MCP), enabling users to chat\nwith an AI assistant about Lean declarations or utilize the search engine for\nbuilding theorem-proving agents. This work details LeanExplore's architecture,\ndata processing, functionalities, and its potential to enhance Lean 4 workflows\nand AI-driven mathematical research", "AI": {"tldr": "LeanExplore is a new semantic search engine for Lean 4 libraries, combining AI-driven and traditional search approaches. It makes finding definitions and theorems easier, is accessible via web or API, can be self-hosted, integrates with AI tools, and improves both human and automated research workflows.", "motivation": "The growing Lean 4 ecosystem has made it difficult for users to effectively find and navigate relevant declarations and statements within its large libraries. Consequently, there's a need for a robust, user-friendly search engine that accommodates both formal and informal queries, improving accessibility and usability for mathematicians, researchers, and AI applications.", "method": "The paper introduces LeanExplore, which uses a hybrid ranking strategy for semantic search. This strategy combines: (1) multi-source semantic embedding models for conceptual understanding of Lean code, docstrings, AI-generated informal translations, and declaration titles; (2) BM25+ for keyword-based lexical matching; and (3) a PageRank-based score to reflect the importance and connectivity of declarations. The system is accessible via a web interface and Python API and can be self-hosted. It also provides easy integration with LLMs via the model context protocol (MCP).", "result": "LeanExplore enables effective semantic and lexical search across major Lean 4 packages, enhancing discoverability of declarations. It is available as a web-based tool, Python API, and downloadable database, and can be integrated with AI and theorem-proving agents. The paper describes its architecture, data processing pipeline, and its integration with LLMs and user workflows.", "conclusion": "LeanExplore represents an important tool for the Lean 4 community, improving accessibility, navigation, and searchability in the expanding ecosystem. Its hybrid ranking and integration with AI assistants facilitate more efficient research and theorem proving, promising to benefit both humans and automated agents."}}
{"id": "2506.11107", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11107", "abs": "https://arxiv.org/abs/2506.11107", "authors": ["Weibo Gao", "Qi Liu", "Rui Li", "Yuze Zhao", "Hao Wang", "Linan Yre", "Fangzhou Yao", "Zheng Zhang"], "title": "Denoising Programming Knowledge Tracing with a Code Graph-based Tuning Adaptor", "comment": "Accepted by KDD August 2025", "summary": "Programming Knowledge Tracking (PKT) aims to dynamically diagnose learners'\nmastery levels of programming knowledge based on their coding activities,\nfacilitating more effective and personalized programming education. However,\ncurrent PKT studies primarily focus on the implicit relationship between code\ncontent and knowledge assessment, often overlooking two types of noise signals\nin long-term programming activities: unwanted signals from unrelated\nsubmissions and weak signals from minor modifications. This practical challenge\nsignificantly limits model performance and application. To address this issue,\nwe propose Coda, a Code graph-based tuning adaptor designed to enhance existing\nPKT models by identifying and mitigating the impact of noise. Specifically,\nCoda first transforms the loose code sequences submitted by each learner into a\ncompact code graph. By leveraging this code graph, unwanted signals can be\nidentified from a semantic similarity perspective. We then apply a\ncluster-aware GCN to the code graph, which improves the discrimination of weak\nsignals and enables their clustering for identification. Finally, a lightweight\nyet effective adaptor is incorporated into the PKT task through optimization\nwith two noise feature-based constraints and a navigational regularization\nterm, to correct knowledge states affected by noise. It is worth mentioning\nthat the Coda framework is model-agnostic and can be adapted to most existing\nPKT solutions. Extensive experimental results on four real-world datasets\ndemonstrate that Coda effectively performs the PKT task in the presence of\nnoisy programming records, outperforming typical baselines.", "AI": {"tldr": "Coda uses code graphs and GCN to reduce noise from unrelated or minor coding activities in programming knowledge tracking, leading to improved, model-agnostic tracking accuracy on real coding data.", "motivation": "Existing PKT models struggle with noise in learners' programming data \u2014 especially unrelated submissions and small code tweaks \u2014 which hampers performance and practical application.", "method": "Coda introduces a code graph to structure learners' sequential code submissions, applies cluster-aware Graph Convolutional Networks (GCN) for better identification of signal types, and integrates a lightweight adaptor with noise-sensitive constraints and regularization to correct misdiagnosed knowledge states. The framework operates independently of specific PKT models.", "result": "Coda significantly improves PKT performance on noisy programming datasets, outperforming typical baseline models across four real-world datasets.", "conclusion": "Coda provides an effective, model-agnostic approach for enhancing PKT by identifying and mitigating noise, offering superior knowledge tracking under realistic conditions."}}
{"id": "2506.11141", "categories": ["cs.SE", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.11141", "abs": "https://arxiv.org/abs/2506.11141", "authors": ["Philippe J. Giabbanelli", "John Beverley", "Istvan David", "Andreas Tolk"], "title": "From over-reliance to smart integration: using Large-Language Models as translators between specialized modeling and simulation tools", "comment": "Accepted at the Winter Simulation conference 2025, December, Seattle\n  USA", "summary": "Large Language Models (LLMs) offer transformative potential for Modeling &\nSimulation (M&S) through natural language interfaces that simplify workflows.\nHowever, over-reliance risks compromising quality due to ambiguities, logical\nshortcuts, and hallucinations. This paper advocates integrating LLMs as\nmiddleware or translators between specialized tools to mitigate complexity in\nM&S tasks. Acting as translators, LLMs can enhance interoperability across\nmulti-formalism, multi-semantics, and multi-paradigm systems. We address two\nkey challenges: identifying appropriate languages and tools for modeling and\nsimulation tasks, and developing efficient software architectures that\nintegrate LLMs without performance bottlenecks. To this end, the paper explores\nLLM-mediated workflows, emphasizes structured tool integration, and recommends\nLow-Rank Adaptation-based architectures for efficient task-specific\nadaptations. This approach ensures LLMs complement rather than replace\nspecialized tools, fostering high-quality, reliable M&S processes.", "AI": {"tldr": "The paper suggests using LLMs as smart middleware in modeling and simulation, connecting existing tools more effectively. This improves workflow efficiency and reliability without replacing specialized tools, thanks to careful integration strategies.", "motivation": "Large Language Models (LLMs) can simplify Modeling & Simulation (M&S) workflows, but their unrestrained use introduces risks such as logical errors and hallucinations.", "method": "The paper proposes using LLMs as middleware or translators that connect specialized M&S tools. It discusses structured workflow integration, focusing on efficient software architecture using Low-Rank Adaptation-based approaches.", "result": "LLMs, when used as translators, improve interoperability across diverse M&S systems and tools, reducing complexity without introducing performance issues.", "conclusion": "Integrating LLMs as complementary middleware, not replacements, leads to more robust, high-quality M&S processes. Structured integration and task-specific adaptations are essential."}}
{"id": "2506.11153", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11153", "abs": "https://arxiv.org/abs/2506.11153", "authors": ["Changxin Ke", "Rui Zhang", "Shuo Wang", "Li Ding", "Guangli Li", "Yuanbo Wen", "Shuoming Zhang", "Ruiyuan Xu", "Jin Qin", "Jiaming Guo", "Chenxi Wang", "Ling Li", "Qi Guo", "Yunji Chen"], "title": "Mutual-Supervised Learning for Sequential-to-Parallel Code Translation", "comment": "28 pages", "summary": "The rise of GPU-based high-performance computing (HPC) has driven the\nwidespread adoption of parallel programming models such as CUDA. Yet, the\ninherent complexity of parallel programming creates a demand for the automated\nsequential-to-parallel approaches. However, data scarcity poses a significant\nchallenge for machine learning-based sequential-to-parallel code translation.\nAlthough recent back-translation methods show promise, they still fail to\nensure functional equivalence in the translated code. In this paper, we propose\na novel Mutual-Supervised Learning (MSL) framework for sequential-to-parallel\ncode translation to address the functional equivalence issue. MSL consists of\ntwo models, a Translator and a Tester. Through an iterative loop consisting of\nCo-verify and Co-evolve steps, the Translator and the Tester mutually generate\ndata for each other and improve collectively. The Tester generates unit tests\nto verify and filter functionally equivalent translated code, thereby evolving\nthe Translator, while the Translator generates translated code as augmented\ninput to evolve the Tester. Experimental results demonstrate that MuSL\nsignificantly enhances the performance of the base model: when applied to\nQwen2.5-Coder, it not only improves Pass@1 by up to 28.91% and boosts Tester\nperformance by 68.90%, but also outperforms the previous state-of-the-art\nmethod CodeRosetta by 1.56 and 6.92 in BLEU and CodeBLEU scores, while\nachieving performance comparable to DeepSeek-R1 and GPT-4.1. Our code is\navailable at https://github.com/kcxain/musl.", "AI": {"tldr": "The paper introduces MuSL, a mutual-supervised framework for translating sequential to parallel code. By having translation and testing models teach each other, MuSL ensures functionally correct output, significantly boosting performance against previous methods and achieving results comparable to leading AI models.", "motivation": "Despite the proliferation of GPU-based high-performance computing (HPC) and CUDA, the complexity of parallel programming calls for automated tools to translate sequential code to parallel code. However, limited data for training and challenges in ensuring functional equivalence impede current machine learning-based methods.", "method": "The authors propose a Mutual-Supervised Learning (MSL) framework, which comprises two models: a Translator (for code generation) and a Tester (for generating unit tests). The system operates in an iterative loop with 'Co-verify' and 'Co-evolve' steps, enabling the Translator and Tester to mutually generate and refine data to enhance each other's performance.", "result": "When applied to Qwen2.5-Coder, MuSL improves the Pass@1 metric by up to 28.91%, enhances Tester performance by 68.90%, and surpasses previous top methods like CodeRosetta in BLEU and CodeBLEU scores, while being competitive with advanced models such as DeepSeek-R1 and GPT-4.1.", "conclusion": "The Mutual-Supervised Learning (MSL) framework effectively addresses functional equivalence in sequential-to-parallel code translation, resulting in significant improvements over prior state-of-the-art techniques."}}
{"id": "2506.11176", "categories": ["cs.SE", "cs.DC", "cs.DM", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.11176", "abs": "https://arxiv.org/abs/2506.11176", "authors": ["Anatoly A. Krasnovsky", "Alexander Zorkin"], "title": "Model Discovery and Graph Simulation: A Lightweight Alternative to Chaos Engineering", "comment": null, "summary": "Microservice applications are prone to cascading failures because of dense\ninter-service dependencies. Ensuring resilience usually demands fault-injection\nexperiments in production-like setups. We propose \\textit{model discovery} --\nan automated CI/CD step that extracts a live dependency graph from trace data\n-- and show that this lightweight representation is sufficient for accurate\nresilience prediction. Using the DeathStarBench Social Network, we build the\ngraph, simulate failures via Monte-Carlo, and run matching chaos experiments on\nthe real system. The graph model closely matches reality: with no replication,\n16 trials yield an observed resilience of 0.186 versus a predicted 0.161; with\nreplication, both observed and predicted values converge to 0.305 (mean\nabsolute error \\leq 0.0004). These results indicate that even a simple,\nautomatically discovered graph can estimate microservice availability with high\nfidelity, offering rapid design-time insight without full-scale failure\ntesting.", "AI": {"tldr": "Automated extraction of system dependency graphs from trace data enables fast, accurate resilience predictions for microservices, minimizing the need for expensive failure experiments.", "motivation": "Microservice applications often suffer from cascading failures due to complex inter-service dependencies. Traditional approaches to ensure resilience require resource-intensive fault-injection experiments in production-like environments.", "method": "The authors propose an automated model discovery technique integrated into CI/CD pipelines, which extracts a live dependency graph from trace data. They use this lightweight graph to simulate failures via Monte Carlo methods and then validate predictions through chaos experiments on the DeathStarBench Social Network application.", "result": "The automatically discovered graph model predicts system resilience with high accuracy. For example, the predicted and observed resilience values are very close, especially in replicated setups (mean absolute error \u2264 0.0004).", "conclusion": "A simple, automatically discovered dependency graph can accurately estimate microservice availability, reducing the need for costly and time-consuming real-world failure tests."}}
{"id": "2506.11180", "categories": ["cs.SE", "cs.AI", "cs.ET", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.11180", "abs": "https://arxiv.org/abs/2506.11180", "authors": ["Luis Miguel Vieira da Silva", "Aljosha K\u00f6cher", "Felix Gehlhoff"], "title": "Beyond Formal Semantics for Capabilities and Skills: Model Context Protocol in Manufacturing", "comment": null, "summary": "Explicit modeling of capabilities and skills -- whether based on ontologies,\nAsset Administration Shells, or other technologies -- requires considerable\nmanual effort and often results in representations that are not easily\naccessible to Large Language Models (LLMs). In this work-in-progress paper, we\npresent an alternative approach based on the recently introduced Model Context\nProtocol (MCP). MCP allows systems to expose functionality through a\nstandardized interface that is directly consumable by LLM-based agents. We\nconduct a prototypical evaluation on a laboratory-scale manufacturing system,\nwhere resource functions are made available via MCP. A general-purpose LLM is\nthen tasked with planning and executing a multi-step process, including\nconstraint handling and the invocation of resource functions via MCP. The\nresults indicate that such an approach can enable flexible industrial\nautomation without relying on explicit semantic models. This work lays the\nbasis for further exploration of external tool integration in LLM-driven\nproduction systems.", "AI": {"tldr": "Instead of relying on difficult, manual modeling of skills and capabilities for LLMs, the authors show that the Model Context Protocol allows LLMs to flexibly access and use resources in industrial automation. Their tests with a prototype system indicate that it is feasible and effective, opening new directions for LLM-driven automation without explicit semantics.", "motivation": "Current approaches to modeling capabilities and skills in industrial automation are labor-intensive and often incompatible with Large Language Models (LLMs), limiting flexibility and scalability.", "method": "The authors introduce and evaluate the Model Context Protocol (MCP) as a means for exposing system functionality via a standardized interface, directly accessible by LLM-based agents. They test this in a lab-scale manufacturing environment, where resource functions are accessed through MCP, and an LLM plans and executes a multi-step process, including managing constraints and invoking resource functions.", "result": "The approach demonstrates that LLMs can flexibly plan and execute complex tasks in industrial automation settings without the need for detailed, explicit semantic models. MCP facilitates this interaction with external tools/functions.", "conclusion": "MCP offers a promising alternative to manual semantic modeling for LLM-driven production systems, enabling easier integration of external functionalities and flexible automation. This could pave the way for broader adoption of LLMs in industry."}}
{"id": "2506.11237", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.11237", "abs": "https://arxiv.org/abs/2506.11237", "authors": ["Ngoc Phuoc An Vo", "Brent Paulovicks", "Vadim Sheinin"], "title": "LLM-as-a-Judge for Reference-less Automatic Code Validation and Refinement for Natural Language to Bash in IT Automation", "comment": "10 pages", "summary": "In an effort to automatically evaluate and select the best model and improve\ncode quality for automatic incident remediation in IT Automation, it is crucial\nto verify if the generated code for remediation action is syntactically and\nsemantically correct and whether it can be executed correctly as intended.\nThere are three approaches: 1) conventional methods use surface form similarity\nmetrics (token match, exact match, etc.) which have numerous limitations, 2)\nexecution-based evaluation focuses more on code functionality based on\npass/fail judgments for given test-cases, and 3) LLM-as-a-Judge employs LLMs\nfor automated evaluation to judge if it is a correct answer for a given problem\nbased on pre-defined metrics. In this work, we focused on enhancing\nLLM-as-a-Judge using bidirectional functionality matching and logic\nrepresentation for reference-less automatic validation and refinement for Bash\ncode generation to select the best model for automatic incident remediation in\nIT Automation. We used execution-based evaluation as ground-truth to evaluate\nour LLM-as-a-Judge metrics. Results show high accuracy and agreement with\nexecution-based evaluation (and up to 8% over baseline). Finally, we built\nReflection code agents to utilize judgments and feedback from our evaluation\nmetrics which achieved significant improvement (up to 24% increase in accuracy)\nfor automatic code refinement.", "AI": {"tldr": "This paper introduces improved LLM-based evaluation for automatically selecting and refining Bash code in IT automation, achieving better accuracy and supporting robust, reference-less code validation and refinement.", "motivation": "Automatic incident remediation in IT automation requires high-quality code generation. Evaluating and selecting the best models for this task is challenging, as existing evaluation methods either focus only on syntactic similarity or require execution-based judgments that may be costly or limited. There is a need for more effective, automated, and reliable evaluation methods.", "method": "The paper explores three approaches for code evaluation: surface form similarity metrics, execution-based evaluation, and LLM-as-a-Judge. The focus is on enhancing LLM-as-a-Judge by introducing bidirectional functionality matching and logic representation for automatic, reference-less validation of Bash code. Execution-based evaluation is used as ground-truth for validating the proposed LLM-as-a-Judge metrics. Reflection code agents are developed to leverage these metrics for automated code refinement.", "result": "The enhanced LLM-as-a-Judge method demonstrates high accuracy and strong agreement with execution-based evaluation, outperforming the baseline by up to 8%. The Reflection code agents, utilizing this evaluation approach, achieve up to a 24% increase in accuracy for automatic code refinement tasks.", "conclusion": "The proposed enhancements to LLM-as-a-Judge make it possible to automatically and accurately validate and select models for Bash code generation in IT automation, without relying on reference code. The Reflection code agents significantly improve code quality and robustness in automatic incident remediation."}}
{"id": "2506.11266", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11266", "abs": "https://arxiv.org/abs/2506.11266", "authors": ["Benjamin Elder", "Anupama Murthi", "Jungkoo Kang", "Ankita Rajaram Naik", "Kiran Kate", "Kinjal Basu", "Danish Contractor"], "title": "Invocable APIs derived from NL2SQL datasets for LLM Tool-Calling Evaluation", "comment": "10+32 pages, 5 figures", "summary": "Large language models (LLMs) are routinely deployed as agentic systems, with\naccess to tools that interact with live environments to accomplish tasks. In\nenterprise deployments these systems need to interact with API collections that\ncan be extremely large and complex, often backed by databases. In order to\ncreate datasets with such characteristics, we explore how existing NL2SQL\n(Natural Language to SQL query) datasets can be used to automatically create\nNL2API datasets. Specifically, this work describes a novel data generation\npipeline that exploits the syntax of SQL queries to construct a functionally\nequivalent sequence of API calls. We apply this pipeline to one of the largest\nNL2SQL datasets, BIRD-SQL to create a collection of over 2500 APIs that can be\nserved as invocable tools or REST-endpoints. We pair natural language queries\nfrom BIRD-SQL to ground-truth API sequences based on this API pool. We use this\ncollection to study the performance of 10 public LLMs and find that all models\nstruggle to determine the right set of tools (consisting of tasks of intent\ndetection, sequencing with nested function calls, and slot-filling). We find\nthat models have extremely low task completion rates (7-47 percent - depending\non the dataset) which marginally improves to 50 percent when models are\nemployed as ReACT agents that interact with the live API environment. The best\ntask completion rates are far below what may be required for effective\ngeneral-use tool-calling agents, suggesting substantial scope for improvement\nin current state-of-the-art tool-calling LLMs. We also conduct detailed\nablation studies, such as assessing the impact of the number of tools available\nas well as the impact of tool and slot-name obfuscation. We compare the\nperformance of models on the original SQL generation tasks and find that\ncurrent models are sometimes able to exploit SQL better than APIs.", "AI": {"tldr": "The paper introduces a method to convert natural language to SQL datasets into API-call datasets, creating real-world-like benchmarks for evaluating LLMs on complex tool use. Testing on 10 LLMs reveals that current models struggle with API selection and sequencing, achieving low task completion rates. Enhanced evaluation datasets and new benchmarks are needed to advance tool-using LLMs.", "motivation": "Large language models (LLMs) are increasingly used as agents in environments where they must interact with complex, large collections of APIs, similar to enterprise settings. There is a lack of datasets that adequately reflect these real-world, large-scale API scenarios, limiting the ability to evaluate and improve LLM performance for tool use.", "method": "The authors propose a novel data generation pipeline that converts existing NL2SQL (natural language to SQL) datasets into NL2API datasets. They do this by translating SQL queries into functionally equivalent API call sequences. This pipeline was applied to the BIRD-SQL dataset, resulting in the creation of a large API pool and paired natural language to API sequences. They then evaluated 10 public LLMs on this data, including studying effects of variable tool numbers and tool/slot-name obfuscation via ablation studies.", "result": "All tested LLMs showed significant difficulty in determining the correct API tools and constructing proper call sequences, achieving low task completion rates (7-47%, depending on conditions), which increased to 50% with ReACT agent interaction. None of the LLMs approached desirable performance for general-purpose tool-calling situations. Additional studies highlight models' reliance on SQL compared to APIs and the challenges posed by larger toolsets and naming obfuscations.", "conclusion": "Current state-of-the-art tool-calling LLMs are far from effective in handling complex API selection and composition tasks, especially in enterprise-like environments. There is substantial room for improvement. Converting NL2SQL datasets into NL2API datasets is a practical step to create challenging benchmarks and drive progress."}}
{"id": "2506.11295", "categories": ["cs.SE", "cs.AI", "cs.LG", "D.2.11; D.2.8; I.2.0"], "pdf": "https://arxiv.org/pdf/2506.11295", "abs": "https://arxiv.org/abs/2506.11295", "authors": ["Renato Cordeiro Ferreira"], "title": "A Tale of Two Systems: Characterizing Architectural Complexity on Machine Learning-Enabled Systems", "comment": "8 pages, 3 figures (3 diagrams), submitted to the ECSA2025. arXiv\n  admin note: substantial text overlap with arXiv:2506.08153", "summary": "How can the complexity of ML-enabled systems be managed effectively? The goal\nof this research is to investigate how complexity affects ML-Enabled Systems\n(MLES). To address this question, this research aims to introduce a\nmetrics-based architectural model to characterize the complexity of MLES. The\ngoal is to support architectural decisions, providing a guideline for the\ninception and growth of these systems. This paper brings, side-by-side, the\narchitecture representation of two systems that can be used as case studies for\ncreating the metrics-based architectural model: the SPIRA and the Ocean Guard\nMLES.", "AI": {"tldr": "This paper proposes a metrics-based architectural model to manage and characterize the complexity of ML-enabled systems, demonstrated through two case studies, aiming to support better architectural decisions.", "motivation": "Managing the complexity of machine learning-enabled systems (MLES) is challenging and crucial for their effective development and growth.", "method": "The research introduces a metrics-based architectural model by analyzing the architectures of two case study systems, SPIRA and Ocean Guard, to characterize and measure complexity in MLES.", "result": "A metrics-based architectural model is presented, along with comparative architecture representations of SPIRA and Ocean Guard to demonstrate the model's application.", "conclusion": "The proposed model helps guide architectural decisions and supports the inception and growth of ML-enabled systems by providing a clearer understanding of their complexity."}}
{"id": "2506.11400", "categories": ["cs.SE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.11400", "abs": "https://arxiv.org/abs/2506.11400", "authors": ["Yupeng Jiang", "Yao Deng", "Sebastian Schroder", "Linfeng Liang", "Suhaas Gambhir", "Alice James", "Avishkar Seth", "James Pirrie", "Yihao Zhang", "Xi Zheng"], "title": "A Step-by-Step Guide to Creating a Robust Autonomous Drone Testing Pipeline", "comment": null, "summary": "Autonomous drones are rapidly reshaping industries ranging from aerial\ndelivery and infrastructure inspection to environmental monitoring and disaster\nresponse. Ensuring the safety, reliability, and efficiency of these systems is\nparamount as they transition from research prototypes to mission-critical\nplatforms. This paper presents a step-by-step guide to establishing a robust\nautonomous drone testing pipeline, covering each critical stage:\nSoftware-in-the-Loop (SIL) Simulation Testing, Hardware-in-the-Loop (HIL)\nTesting, Controlled Real-World Testing, and In-Field Testing. Using practical\nexamples, including the marker-based autonomous landing system, we demonstrate\nhow to systematically verify drone system behaviors, identify integration\nissues, and optimize performance. Furthermore, we highlight emerging trends\nshaping the future of drone testing, including the integration of Neurosymbolic\nand LLMs, creating co-simulation environments, and Digital Twin-enabled\nsimulation-based testing techniques. By following this pipeline, developers and\nresearchers can achieve comprehensive validation, minimize deployment risks,\nand prepare autonomous drones for safe and reliable real-world operations.", "AI": {"tldr": "This paper introduces a comprehensive, multi-stage testing pipeline for autonomous drones, combining proven and emerging techniques to achieve safer, more reliable, and efficient systems ready for critical real-world tasks.", "motivation": "As autonomous drones become increasingly deployed in critical applications, there is a crucial need for robust methods to ensure their safety, reliability, and operational efficiency.", "method": "The paper introduces a step-by-step validation and testing pipeline for autonomous drones, incorporating Software-in-the-Loop (SIL) Simulation Testing, Hardware-in-the-Loop (HIL) Testing, Controlled Real-World Testing, and In-Field Testing. It demonstrates these stages with practical scenarios such as marker-based autonomous landing systems, and discusses emerging testing trends like Neurosymbolic integration, Large Language Models (LLMs), co-simulation, and Digital Twin-enabled techniques.", "result": "Applying this systematic pipeline allows for comprehensive testing, identification of integration challenges, and optimization of drone behavior, reducing the risks associated with real-world deployment. New trends enhance the rigor and fidelity of drone testing environments.", "conclusion": "The proposed stepwise pipeline enables developers and researchers to thoroughly validate autonomous drones, optimize performance, and ensure safer, more reliable deployment across diverse applications."}}
{"id": "2506.11442", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11442", "abs": "https://arxiv.org/abs/2506.11442", "authors": ["Yiyang Jin", "Kunzhao Xu", "Hang Li", "Xueting Han", "Yanmin Zhou", "Cheng Li", "Jing Bai"], "title": "ReVeal: Self-Evolving Code Agents via Iterative Generation-Verification", "comment": null, "summary": "Recent advances in reinforcement learning (RL) with verifiable outcome\nrewards have significantly improved the reasoning capabilities of large\nlanguage models (LLMs), especially when combined with multi-turn tool\ninteractions. However, existing methods lack both meaningful verification\nsignals from realistic environments and explicit optimization for verification,\nleading to unreliable self-verification. To address these limitations, we\npropose ReVeal, a multi-turn reinforcement learning framework that interleaves\ncode generation with explicit self-verification and tool-based evaluation.\nReVeal enables LLMs to autonomously generate test cases, invoke external tools\nfor precise feedback, and improves performance via a customized RL algorithm\nwith dense, per-turn rewards. As a result, ReVeal fosters the co-evolution of a\nmodel's generation and verification capabilities through RL training, expanding\nthe reasoning boundaries of the base model, demonstrated by significant gains\nin Pass@k on LiveCodeBench. It also enables test-time scaling into deeper\ninference regimes, with code consistently evolving as the number of turns\nincreases during inference, ultimately surpassing DeepSeek-R1-Zero-Qwen-32B.\nThese findings highlight the promise of ReVeal as a scalable and effective\nparadigm for building more robust and autonomous AI agents.", "AI": {"tldr": "ReVeal is a new RL framework that interleaves code generation with self-verification and external tool evaluations, enabling LLMs to improve reasoning and reliability. It outperforms previous benchmarks and demonstrates scalable, robust AI agent development.", "motivation": "Current RL methods for enhancing large language model reasoning are limited by weak verification signals and a lack of explicit self-verification optimization. This results in unreliable outputs that restrict the reasoning abilities of LLMs, especially in realistic environments where robust verification is crucial.", "method": "The authors propose ReVeal, a multi-turn RL framework that integrates code generation with explicit self-verification and tool-based evaluation. ReVeal trains LLMs to autonomously create test cases and use external tools for feedback, optimizing performance with a custom RL algorithm that provides dense, per-turn rewards.", "result": "ReVeal demonstrates significant gains in Pass@k on the LiveCodeBench benchmark and achieves better results as the number of inference turns increases, ultimately outperforming DeepSeek-R1-Zero-Qwen-32B. The model effectively learns to co-evolve its generation and verification skills, leading to more robust and reliable outputs.", "conclusion": "ReVeal offers a scalable and effective approach for developing AI agents with enhanced reasoning and self-verification abilities, setting a new standard for robust and autonomous performance in complex environments."}}
{"id": "2506.11451", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11451", "abs": "https://arxiv.org/abs/2506.11451", "authors": ["Md Nahidul Islam Opu", "Md Shahidul Islam", "Sara Rouhani", "Shaiful Chowdhury"], "title": "Understanding the Issue Types in Open Source Blockchain-based Software Projects with the Transformer-based BERTopic", "comment": null, "summary": "Blockchain-based software systems are increasingly deployed across diverse\ndomains, yet a systematic understanding of their development challenges remains\nlimited. This paper presents a large-scale empirical study of 497,742 issues\nmined from 1,209 open-source blockchain projects hosted on GitHub. Employing\nBERTopic, a transformer-based topic modeling technique, we identify 49 distinct\nissue topics and organize them hierarchically into 11 major subcategories. Our\nanalysis reveals that both general software development issues and\nblockchain-specific concerns are nearly equally represented, with Wallet\nManagement and UI Enhancement emerging as the most prominent topics. We further\nexamine the temporal evolution of issue categories and resolution times,\nfinding that Wallet issues not only dominate in frequency but also exhibit the\nlongest resolution time. Conversely, Mechanisms issues are resolved\nsignificantly faster. Issue frequency surged after 2016 with the rise of\nEthereum and decentralized applications, but declined after 2022. These\nfindings enhance our understanding of blockchain software maintenance,\ninforming the development of specialized tools and practices to improve\nrobustness and maintainability.", "AI": {"tldr": "The paper analyzes nearly 500,000 issues from blockchain GitHub projects, finding Wallet and UI issues most common and hardest to resolve. It highlights the need for tailored maintenance tools to address both general and blockchain-specific challenges.", "motivation": "Blockchain technology is being rapidly adopted across numerous fields, but there is a lack of comprehensive knowledge about the unique and general development challenges faced in blockchain-based software systems.", "method": "The authors conducted a large-scale empirical study of 497,742 issues from 1,209 open-source blockchain projects on GitHub, utilizing BERTopic, a transformer-based topic modeling technique, to identify and categorize the most common issue types.", "result": "The study found that general software issues and blockchain-specific concerns occur in almost equal measure. Wallet Management and UI Enhancement are the most prominent issue topics. Wallet-related issues appear most frequently and take the longest to resolve, while Mechanisms issues are resolved fastest. Issue numbers grew rapidly after 2016 with the popularity of Ethereum, but decreased after 2022.", "conclusion": "Both general and blockchain-specific issues are major challenges in blockchain software. Maintenance practices should be informed by these insights to improve the development and upkeep of blockchain systems. Specialized tools and improved procedures are needed for robustness and maintainability."}}
{"id": "2506.11484", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11484", "abs": "https://arxiv.org/abs/2506.11484", "authors": ["Haoshen", "Ming Hu", "Xiaofei Xie", "Jiaye Li", "Mingsong Chen"], "title": "VulStamp: Vulnerability Assessment using Large Language Model", "comment": null, "summary": "Although modern vulnerability detection tools enable developers to\nefficiently identify numerous security flaws, indiscriminate remediation\nefforts often lead to superfluous development expenses. This is particularly\ntrue given that a substantial portion of detected vulnerabilities either\npossess low exploitability or would incur negligible impact in practical\noperational environments. Consequently, vulnerability severity assessment has\nemerged as a critical component in optimizing software development efficiency.\nExisting vulnerability assessment methods typically rely on manually crafted\ndescriptions associated with source code artifacts. However, due to variability\nin description quality and subjectivity in intention interpretation, the\nperformance of these methods is seriously limited. To address this issue, this\npaper introduces VulStamp, a novel intention-guided framework, to facilitate\ndescription-free vulnerability assessment. Specifically, VulStamp adopts static\nanalysis together with Large Language Model (LLM) to extract the intention\ninformation of vulnerable code. Based on the intention information, VulStamp\nuses a prompt-tuned model for vulnerability assessment. Furthermore, to\nmitigate the problem of imbalanced data associated with vulnerability types,\nVulStamp integrates a Reinforcement Learning (RL)-based prompt-tuning method to\ntrain the assessment model.", "AI": {"tldr": "Manual descriptions of vulnerabilities are often unreliable for assessing severity. VulStamp uses static analysis and LLMs to automatically extract code intention and leverages RL-based prompt tuning for improved, description-free vulnerability assessment, optimizing remediation efforts and reducing wasted development resources.", "motivation": "Modern vulnerability detection tools identify many security vulnerabilities, leading to high development costs when all are remediated. Not all vulnerabilities are equally severe, so accurate severity assessment is important. Current methods rely on manually written descriptions, which are subjective and inconsistent, limiting their performance.", "method": "The paper proposes VulStamp, a new framework that does not require vulnerability descriptions for assessment. VulStamp uses static analysis and Large Language Models (LLMs) to extract intention information from code. It then leverages a prompt-tuned model for vulnerability severity assessment. In addition, VulStamp addresses data imbalance in vulnerability types by incorporating Reinforcement Learning (RL)-based prompt tuning during training.", "result": "VulStamp can accurately assess vulnerability severity without relying on potentially low-quality or inconsistent human-written descriptions. The system leverages code intention information and deals with class imbalance, leading to improved assessment performance.", "conclusion": "VulStamp provides an effective, description-free framework for assessing vulnerability severity. This approach optimizes developer resources by accurately evaluating which vulnerabilities require remediation, thus improving software development efficiency."}}
{"id": "2506.11525", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11525", "abs": "https://arxiv.org/abs/2506.11525", "authors": ["Michael Grohs", "Nadine Cordes", "Jana-Rebecca Rehse"], "title": "A Procedural Framework for Assessing the Desirability of Process Deviations", "comment": null, "summary": "Conformance checking techniques help process analysts to identify where and\nhow process executions deviate from a process model. However, they cannot\ndetermine the desirability of these deviations, i.e., whether they are\nproblematic, acceptable or even beneficial for the process. Such desirability\nassessments are crucial to derive actions, but process analysts typically\nconduct them in a manual, ad-hoc way, which can be time-consuming, subjective,\nand irreplicable. To address this problem, this paper presents a procedural\nframework to guide process analysts in systematically assessing deviation\ndesirability. It provides a step-by-step approach for identifying which input\nfactors to consider in what order to categorize deviations into mutually\nexclusive desirability categories, each linked to action recommendations. The\nframework is based on a review and conceptualization of existing literature on\ndeviation desirability, which is complemented by empirical insights from\ninterviews with process analysis practitioners and researchers. We evaluate the\nframework through a desirability assessment task conducted with practitioners,\nindicating that the framework effectively enables them to streamline the\nassessment for a thorough yet concise evaluation.", "AI": {"tldr": "This paper introduces and validates a procedural framework that helps analysts systematically assess whether process deviations are problematic, acceptable, or beneficial, making deviation evaluation more consistent and actionable.", "motivation": "Traditional conformance checking shows where processes deviate from models, but does not assess whether deviations are problematic, acceptable, or beneficial. Current desirability assessments are usually manual and inconsistent, creating the need for a more systematic approach.", "method": "The authors present a procedural framework, built from literature review and interviews with practitioners and researchers, to guide analysts in systematically assessing the desirability of process deviations.", "result": "The proposed framework was evaluated through a desirability assessment task with practitioners, who found it effective in streamlining and improving the quality and consistency of deviation assessments.", "conclusion": "The framework provides a step-by-step, replicable method for categorizing process deviations by desirability and linking them to action recommendations, addressing subjectivity and inefficiency in current practices."}}
{"id": "2506.11548", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11548", "abs": "https://arxiv.org/abs/2506.11548", "authors": ["Fabian C. Pe\u00f1a"], "title": "Augmenting the Generality and Performance of Large Language Models for Software Engineering", "comment": null, "summary": "Large Language Models (LLMs) are revolutionizing software engineering (SE),\nwith special emphasis on code generation and analysis. However, their\napplications to broader SE practices including conceptualization, design, and\nother non-code tasks, remain partially underexplored. This research aims to\naugment the generality and performance of LLMs for SE by (1) advancing the\nunderstanding of how LLMs with different characteristics perform on various\nnon-code tasks, (2) evaluating them as sources of foundational knowledge in SE,\nand (3) effectively detecting hallucinations on SE statements. The expected\ncontributions include a variety of LLMs trained and evaluated on\ndomain-specific datasets, new benchmarks on foundational knowledge in SE, and\nmethods for detecting hallucinations. Initial results in terms of performance\nimprovements on various non-code tasks are promising.", "AI": {"tldr": "This paper explores and improves how large language models handle non-code software engineering tasks, providing new models, benchmarks, and methods to detect misinformation, and shows early promising results.", "motivation": "While LLMs are proving transformative in code-related tasks in software engineering, their effectiveness in broader non-code SE activities such as conceptualization and design has not been sufficiently studied. There is also a need to better understand their reliability as knowledge sources and to detect when they produce inaccurate ('hallucinated') information.", "method": "The research advances understanding by: (1) evaluating LLMs\u2014varied in type and characteristics\u2014on non-code SE tasks, (2) assessing their value as repositories of foundational software engineering knowledge, and (3) creating and applying methods for hallucination detection. This involves training and evaluating various LLMs using SE-specific datasets and creating new benchmarks.", "result": "Initial results demonstrate promising improvements in LLM performance for non-code SE tasks. The project also develops multiple LLMs, new benchmarks, and hallucination detection methods.", "conclusion": "The study expands the application of LLMs in software engineering to include non-code tasks, establishes their potential as knowledge providers, and provides tools to identify inaccuracies, thus enhancing reliability."}}
{"id": "2506.11559", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11559", "abs": "https://arxiv.org/abs/2506.11559", "authors": ["G\u00e1bor Antal", "D\u00e9nes B\u00e1n", "Martin Isztin", "Rudolf Ferenc", "P\u00e9ter Heged\u0171s"], "title": "Leveraging GPT-4 for Vulnerability-Witnessing Unit Test Generation", "comment": null, "summary": "In the life-cycle of software development, testing plays a crucial role in\nquality assurance. Proper testing not only increases code coverage and prevents\nregressions but it can also ensure that any potential vulnerabilities in the\nsoftware are identified and effectively fixed. However, creating such tests is\na complex, resource-consuming manual process. To help developers and security\nexperts, this paper explores the automatic unit test generation capability of\none of the most widely used large language models, GPT-4, from the perspective\nof vulnerabilities. We examine a subset of the VUL4J dataset containing real\nvulnerabilities and their corresponding fixes to determine whether GPT-4 can\ngenerate syntactically and/or semantically correct unit tests based on the code\nbefore and after the fixes as evidence of vulnerability mitigation. We focus on\nthe impact of code contexts, the effectiveness of GPT-4's self-correction\nability, and the subjective usability of the generated test cases. Our results\nindicate that GPT-4 can generate syntactically correct test cases 66.5\\% of the\ntime without domain-specific pre-training. Although the semantic correctness of\nthe fixes could be automatically validated in only 7. 5\\% of the cases, our\nsubjective evaluation shows that GPT-4 generally produces test templates that\ncan be further developed into fully functional vulnerability-witnessing tests\nwith relatively minimal manual effort.\n  Therefore, despite the limited data, our initial findings suggest that GPT-4\ncan be effectively used in the generation of vulnerability-witnessing tests. It\nmay not operate entirely autonomously, but it certainly plays a significant\nrole in a partially automated process.", "AI": {"tldr": "GPT-4 can automatically generate vulnerability-focused unit test templates for software code about two-thirds of the time. While only a small percentage are fully correct automatically, the generated tests significantly reduce developer effort for creating vulnerability witnesses.", "motivation": "Manual creation of software tests for vulnerability detection is complex and resource-intensive. Automating this process could help developers and security experts improve software quality and security faster and at lower cost.", "method": "The authors evaluated GPT-4's ability to automatically generate unit tests targeting software vulnerabilities, using a portion of the VUL4J dataset that includes real vulnerability cases and their fixes. They assessed the syntactic and semantic correctness of GPT-4-generated tests, investigated the influence of code context, examined self-correction features, and conducted subjective usability evaluations of the test cases produced.", "result": "GPT-4 generated syntactically correct unit tests 66.5% of the time without requiring domain-specific training. Only 7.5% of the semantic correctness cases were automatically validated. Subjective evaluation indicated that the generated test templates are useful and can be easily adapted to become fully functional vulnerability tests with minimal manual work.", "conclusion": "GPT-4 is a promising tool for partially automating the creation of vulnerability-focused tests in software, producing helpful test templates even if some manual intervention is still required."}}
{"id": "2506.11561", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.11561", "abs": "https://arxiv.org/abs/2506.11561", "authors": ["G\u00e1bor Antal", "Bence Bogenf\u00fcrst", "Rudolf Ferenc", "P\u00e9ter Heged\u0171s"], "title": "Identifying Helpful Context for LLM-based Vulnerability Repair: A Preliminary Study", "comment": null, "summary": "Recent advancements in large language models (LLMs) have shown promise for\nautomated vulnerability detection and repair in software systems. This paper\ninvestigates the performance of GPT-4o in repairing Java vulnerabilities from a\nwidely used dataset (Vul4J), exploring how different contextual information\naffects automated vulnerability repair (AVR) capabilities. We compare the\nlatest GPT-4o's performance against previous results with GPT-4 using identical\nprompts. We evaluated nine additional prompts crafted by us that contain\nvarious contextual information such as CWE or CVE information, and manually\nextracted code contexts. Each prompt was executed three times on 42\nvulnerabilities, and the resulting fix candidates were validated using Vul4J's\nautomated testing framework.\n  Our results show that GPT-4o performed 11.9\\% worse on average than GPT-4\nwith the same prompt, but was able to fix 10.5\\% more distinct vulnerabilities\nin the three runs together. CVE information significantly improved repair\nrates, while the length of the task description had minimal impact. Combining\nCVE guidance with manually extracted code context resulted in the best\nperformance. Using our \\textsc{Top}-3 prompts together, GPT-4o repaired 26\n(62\\%) vulnerabilities at least once, outperforming both the original baseline\n(40\\%) and its reproduction (45\\%), suggesting that ensemble prompt strategies\ncould improve vulnerability repair in zero-shot settings.", "AI": {"tldr": "GPT-4o, when aided by contextual prompts (especially CVE and manual code context), outperforms previous baselines in automated software vulnerability repair, though it sometimes lags behind GPT-4 per prompt. Combining top prompts and contextual information enables GPT-4o to repair more vulnerabilities in zero-shot scenarios.", "motivation": "Advancements in large language models (LLMs) like GPT-4o show potential for automating vulnerability repair in software, but there is limited understanding of how contextual information and different prompts affect the repair performance in real-world datasets.", "method": "The authors evaluate GPT-4o on the Vul4J Java vulnerability dataset, comparing its repair abilities with GPT-4 using identical prompts. Additionally, they design nine new prompts incorporating various contextual cues such as CWE/CVE information and manual code contexts, executing each prompt three times per vulnerability and validating repairs with an automated test suite.", "result": "GPT-4o performed 11.9% worse than GPT-4 on average with the same prompt, but it succeeded in fixing 10.5% more unique vulnerabilities across repeated trials. Including CVE information significantly boosted repair success, while the length of task descriptions was less important. The best results were achieved by combining CVE guidance with manual code context, and using ensemble (top-3) prompts GPT-4o repaired 62% of vulnerabilities, surpassing previous baselines.", "conclusion": "Contextual information, especially CVE guidance and diverse prompt strategies, significantly enhances LLM-based automated vulnerability repair. Ensemble prompting yields higher fix rates than previous one-shot approaches, making it a promising direction for future AVR research."}}
{"id": "2506.11588", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11588", "abs": "https://arxiv.org/abs/2506.11588", "authors": ["Simone Romano", "Alberto Conforti", "Gloria Guidetti", "Sara Viotti", "Rachele Ceschin", "Giuseppe Scanniello"], "title": "MBSR at Work: Perspectives from an Instructor and Software Developers", "comment": null, "summary": "In this paper, we present the preliminary findings from a qualitative study\n(i.e., semi-structured interviews) on how a Mindfulness-Based Stress Reduction\n(MBSR) program, carried out in the Software Development (SD) working context,\nis perceived by the software developers of a multinational company who\nparticipated in the MBSR program and by the instructor who led it. MBSR is a\ndeeply personal and experiential practice in helping individuals manage stress,\nparticularly in high-pressure environments such as workplaces, healthcare\nsettings, education, and other demanding professional or personal situations.\nAlthough MBSR has been experimented in different working contexts;\nsurprisingly, it has never been studied in the SD working context where there\nare several stress factors that developers experience (e.g., time pressure and\nuncertainty about the content of a particular task and its outcome). In this\nrespect, qualitative research can generate valuable insights into the\napplication of MBSR in the SD working context that cannot be captured by\nstandardized quantitative measures. Being MBSR instructors and software\ndevelopers the key stakeholders in delivering an MBSR program in the SD working\ncontext, understanding their first-hand experiences can provide a more detailed\npicture of the investigated phenomenon. The most important takeaway result of\nour research can be summarized as follows: despite initial skepticism, the\ndevelopers recognized personal improvements due to the MBSR practice, though\nthe integration of MBSR techniques in the working context remained challenging.", "AI": {"tldr": "This study used interviews to explore software developers' experiences with mindfulness training. Developers saw personal benefits but found it hard to consistently apply mindfulness techniques at work.", "motivation": "The motivation is to explore how Mindfulness-Based Stress Reduction (MBSR), which has been studied in other fields but not in software development, is perceived and experienced by software developers and instructors within a multinational company's SD context.", "method": "The study uses a qualitative research approach, specifically semi-structured interviews, to gather insights from software developers who participated in an MBSR program and the instructor who led it.", "result": "Developers reported personal improvements after MBSR practice, despite initial skepticism. However, seamlessly integrating MBSR techniques into the actual work context proved to be challenging.", "conclusion": "MBSR can offer personal benefits to software developers even if its practical, ongoing integration into the workplace is not straightforward. The qualitative approach provides unique insights that quantitative studies might miss."}}
{"id": "2506.11591", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11591", "abs": "https://arxiv.org/abs/2506.11591", "authors": ["Hyunsun Hong", "Jongmoon Baik"], "title": "Retrieval-Augmented Code Review Comment Generation", "comment": null, "summary": "Automated code review comment generation (RCG) aims to assist developers by\nautomatically producing natural language feedback for code changes. Existing\napproaches are primarily either generation-based, using pretrained language\nmodels, or information retrieval-based (IR), reusing comments from similar past\nexamples. While generation-based methods leverage code-specific pretraining on\nlarge code-natural language corpora to learn semantic relationships between\ncode and natural language, they often struggle to generate low-frequency but\nsemantically important tokens due to their probabilistic nature. In contrast,\nIR-based methods excel at recovering such rare tokens by copying from existing\nexamples but lack flexibility in adapting to new code contexts-for example,\nwhen input code contains identifiers or structures not found in the retrieval\ndatabase. To bridge the gap between generation-based and IR-based methods, this\nwork proposes to leverage retrieval-augmented generation (RAG) for RCG by\nconditioning pretrained language models on retrieved code-review exemplars. By\nproviding relevant examples that illustrate how similar code has been\npreviously reviewed, the model is better guided to generate accurate review\ncomments. Our evaluation on the Tufano et al. benchmark shows that RAG-based\nRCG outperforms both generation-based and IR-based RCG. It achieves up to\n+1.67% higher exact match and +4.25% higher BLEU scores compared to\ngeneration-based RCG. It also improves the generation of low-frequency\nground-truth tokens by up to 24.01%. We additionally find that performance\nimproves as the number of retrieved exemplars increases.", "AI": {"tldr": "This paper introduces a retrieval-augmented approach for automatic code review comment generation, combining strengths from both generation-based and retrieval-based methods, and shows notable improvements in accuracy and rare token handling on standard benchmarks.", "motivation": "Automated code review comment generation assists developers by generating natural language feedback for code modifications. Existing methods are either generation-based, which struggle with rare but significant tokens, or retrieval-based, which lack flexibility for new contexts. Bridging these complementary strengths is needed for improved performance.", "method": "The paper proposes using retrieval-augmented generation (RAG) for code review comment generation. The approach conditions pretrained language models on relevant retrieved examples of past code reviews, aiming to combine the flexibility of generation-based models with the rare token accuracy of retrieval-based models.", "result": "On the Tufano et al. benchmark, RAG-based comment generation outperforms both generation-only and retrieval-only approaches. It achieves up to 1.67% higher exact match and 4.25% higher BLEU score compared to generation models, and improves rare token generation by up to 24.01%. More retrieved examples further boost performance.", "conclusion": "Leveraging retrieval-augmented generation significantly enhances automated code review comment generation, effectively merging the strengths of both generation and retrieval approaches. The system yields better overall accuracy, greater coverage of semantically important tokens, and benefits further from increased relevant exemplar retrieval."}}
{"id": "2506.11597", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11597", "abs": "https://arxiv.org/abs/2506.11597", "authors": ["Simone Romano", "Francesco Paolo Sferratore", "Giuseppe Scanniello"], "title": "Further Evidence on a Controversial Topic about Human-Based Experiments: Professionals vs. Students", "comment": null, "summary": "Most Software Engineering (SE) human-based controlled experiments rely on\nstudents as participants, raising concerns about their external validity.\nSpecifically, the realism of results obtained from students and their\napplicability to the software industry remains in question. In this short\npaper, we bring further evidence on this controversial point. To do so, we\ncompare 62 students and 42 software professionals on a bug-fixing task on the\nsame Java program. The students were enrolled in a Bachelor's program in\nComputer Science, while the professionals were employed by two multinational\ncompanies (for one of them, the professionals were from two offices). Some\nvariations in the experimental settings of the two groups (students and\nprofessionals) were present. For instance, the experimental environment of the\nexperiment with professionals was more realistic; i.e., they faced some stress\nfactors such as interruptions during the bug-fixing task. Considering the\ndifferences between the two groups of participants, the gathered data show that\nthe students outperformed the professionals in fixing bugs. This diverges to\nsome extent from past empirical evidence. Rather than presenting definitive\nconclusions, our results aim to catalyze the discussion on the use of students\nin experiments and pave the way for future investigations. Specifically, our\nresults encourage us to examine the complex factors influencing SE tasks,\nmaking experiments as more realistic as possible.", "AI": {"tldr": "Students outperformed professionals in a bug-fixing experiment, challenging assumptions about the external validity of using students in software engineering research and calling for more realistic experimental settings in future studies.", "motivation": "The motivation behind this paper is to address the ongoing debate regarding the external validity of using students as participants in software engineering (SE) human-based controlled experiments. There is concern about whether results from student participants generalize to real-world software industry contexts.", "method": "The authors conduct a controlled experiment comparing two groups performing the same bug-fixing task on a Java program: 62 students from a Computer Science Bachelor's program and 42 professional software engineers from multinational companies. There were some differences in the experimental environments, with professionals facing a more realistic setting including distractions and interruptions.", "result": "Contrary to previous empirical findings, the data indicates that students outperformed professionals in the bug-fixing task. The divergent result is highlighted, with an emphasis on the differences in experimental settings.", "conclusion": "The study does not provide definitive conclusions but highlights that the use of students in SE experiments requires further scrutiny. The results catalyze discussion and call for deeper inquiry into the factors influencing SE tasks, advocating for experiments that closely reflect real-world scenarios."}}
{"id": "2506.11598", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11598", "abs": "https://arxiv.org/abs/2506.11598", "authors": ["Ahmed Zaki", "Cristian Cadar"], "title": "Understanding API Usage and Testing: An Empirical Study of C Libraries", "comment": "The 29th International Conference on Evaluation and Assessment in\n  Software Engineering, 17 to 20 June, 2025, Istanbul, Turkey", "summary": "For library developers, understanding how their Application Programming\nInterfaces (APIs) are used in the field can be invaluable. Knowing how clients\nare using their APIs allows for data-driven decisions on prioritising bug\nreports, feature requests, and testing activities. For example, the priority of\na bug report concerning an API can be partly determined by how widely that API\nis used.\n  In this paper, we present an empirical study in which we analyse API usage\nacross 21 popular open-source C libraries, such as OpenSSL and SQLite, with a\ncombined total of 3,061 C/C++ clients. We compare API usage by clients with how\nwell library test suites exercise the APIs to offer actionable insights for\nlibrary developers. To our knowledge, this is the first study that compares API\nusage and API testing at scale for the C/C++ ecosystem. Our study shows that\nlibrary developers do not prioritise their effort based on how clients use\ntheir API, with popular APIs often poorly tested. For example, in LMDB, a\npopular key-value store, 45% of the APIs are used by clients but not tested by\nthe library test suite. We further show that client test suites can be\nleveraged to improve library testing e.g., improving coverage in LMDB by 14.7%\nwith the important advantage that those tests are representative of how the\nAPIs are used in the field.\n  For our empirical study, we have developed LibProbe, a framework that can be\nused to analyse a large corpus of clients for a given library and produce\nvarious metrics useful to library developers.", "AI": {"tldr": "The paper reveals that open-source C/C++ libraries often under-test the APIs most used by real-world clients. By analyzing client codebases and test suites with LibProbe, the authors show that integrating client tests can significantly improve library test coverage and ensure testing aligns with actual usage trends.", "motivation": "Library developers need to make data-driven decisions regarding bug reports, feature requests, and testing, and understanding real-world API usage is crucial for prioritizing their development efforts. However, it is unclear whether current testing practices align with API usage in the field.", "method": "The authors conducted an empirical study analyzing API usage across 21 popular open-source C libraries, examining a total of 3,061 C/C++ client projects. They compared actual API usage by clients to the extent these APIs are exercised by existing library test suites, utilizing a new framework called LibProbe to automate the analysis and generate metrics.", "result": "The study found that there is a disconnect between API usage by clients and API testing efforts in libraries. Frequently used APIs are often under-tested. For example, 45% of LMDB APIs used by clients are not covered by library tests. Using client test suites can improve library test coverage\u2014LMDB's coverage could be increased by 14.7% by leveraging client tests, with the added benefit that these tests reflect real-world API usage.", "conclusion": "Library developers often do not test widely used APIs adequately, missing opportunities to align their testing priorities with actual usage in the field. Incorporating tests from client projects can substantially improve test coverage and make testing more representative of real-world usage patterns."}}
{"id": "2506.11614", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11614", "abs": "https://arxiv.org/abs/2506.11614", "authors": ["Yonggang Tao", "Jingling Xue"], "title": "Accelerating Delta Debugging through Probabilistic Monotonicity Assessment", "comment": "Accepted by EASE 2025 (The 29th International Conference on\n  Evaluation and Assessment in Software Engineering), 17-20 June 2025,\n  Istanbul, Turkey. 11 pages", "summary": "Delta debugging assumes search space monotonicity: if a program causes a\nfailure, any supersets of that program will also induce the same failure,\npermitting the exclusion of subsets of non-failure-inducing programs. However,\nthis assumption does not always hold in practice. This paper introduces\nProbabilistic Monotonicity Assessment (PMA), enhancing the efficiency of\nDDMIN-style algorithms without sacrificing effectiveness. PMA dynamically\nmodels and assesses the search space's monotonicity based on prior tests tried\nduring the debugging process and uses a confidence function to quantify\nmonotonicity, thereby enabling the probabilistic exclusion of subsets of\nnon-failure-inducing programs. Our approach significantly reduces redundant\ntests that would otherwise be performed, without compromising the quality of\nthe reduction.\n  We evaluated PMA against two leading DDMIN-style tools, CHISEL and ProbDD.\nOur findings indicate that PMA cuts processing time by 59.2% compared to\nCHISEL, accelerates the reduction process (i.e., the number of tokens deleted\nper second) by 3.32x, and decreases the sizes of the final reduced programs by\n6.7%. Against ProbDD, PMA reduces processing time by 22.0%, achieves a 1.34x\nspeedup in the reduction process, and further decreases the sizes of the final\nreduced programs by 3.0%. These findings affirm PMA's role in significantly\nimproving delta debugging's efficiency while maintaining or enhancing its\neffectiveness.", "AI": {"tldr": "The paper presents a new approach (PMA) to make delta debugging much more efficient by probabilistically assessing monotonicity, which sharply cuts down processing time and redundant testing while maintaining or improving the quality of program reduction.", "motivation": "Delta debugging relies on a monotonicity assumption, which isn't always valid. This limitation can cause inefficiencies during debugging, as unnecessary tests may be performed.", "method": "The paper proposes Probabilistic Monotonicity Assessment (PMA), an algorithm that models and evaluates the monotonicity of the search space in real-time during the debugging process, using a confidence function based on prior test outcomes to drive efficient pruning of search subsets.", "result": "PMA significantly reduces redundant tests, processing time, and the size of reduced programs compared to state-of-the-art tools. Specifically, it reduces processing time by 59.2% (vs. CHISEL), increases reduction speed by 3.32x, and reduces final program size by 6.7%. Against ProbDD, PMA further shows improved processing time (22% reduction), speed (1.34x), and result size (3% reduction).", "conclusion": "PMA enhances the efficiency and effectiveness of delta debugging by enabling probabilistic exclusion of non-failure-inducing subsets, thus reducing redundant testing while achieving equal or better program reduction."}}
{"id": "2506.11659", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11659", "abs": "https://arxiv.org/abs/2506.11659", "authors": ["Simin Sun", "Yuchuan Jin", "Miroslaw Staron"], "title": "An Empirical study on LLM-based Log Retrieval for Software Engineering Metadata Management", "comment": null, "summary": "Developing autonomous driving systems (ADSs) involves generating and storing\nextensive log data from test drives, which is essential for verification,\nresearch, and simulation. However, these high-frequency logs, recorded over\nvarying durations, pose challenges for developers attempting to locate specific\ndriving scenarios. This difficulty arises due to the wide range of signals\nrepresenting various vehicle components and driving conditions, as well as\nunfamiliarity of some developers' with the detailed meaning of these signals.\nTraditional SQL-based querying exacerbates this challenge by demanding both\ndomain expertise and database knowledge, often yielding results that are\ndifficult to verify for accuracy.\n  This paper introduces a Large Language Model (LLM)-supported approach that\ncombines signal log data with video recordings from test drives, enabling\nnatural language based scenario searches while reducing the need for\nspecialized knowledge. By leveraging scenario distance graphs and relative gap\nindicators, it provides quantifiable metrics to evaluate the reliability of\nquery results. The method is implemented as an API for efficient database\nquerying and retrieval of relevant records, paired with video frames for\nintuitive visualization. Evaluation on an open industrial dataset demonstrates\nimproved efficiency and reliability in scenario retrieval, eliminating\ndependency on a single data source and conventional SQL.", "AI": {"tldr": "The paper presents an LLM-powered API that enables intuitive, reliable searches of complex ADS log data by combining signals and video, thus simplifying scenario retrieval for developers and outperforming standard SQL approaches.", "motivation": "Developers of autonomous driving systems (ADSs) face difficulties locating specific driving scenarios within vast, high-frequency log data. The complexity of vehicle signals, lack of signal domain knowledge, and limitations of traditional SQL querying (which requires expertise and yields hard-to-verify results) exacerbate the problem.", "method": "The paper proposes an approach that utilizes Large Language Models (LLMs) to enable natural language scenario searches in log data. It fuses signal logs with video data, employs scenario distance graphs and relative gap indicators for reliability evaluation, and implements the solution as an API for efficient querying and record retrieval with video-based visualization.", "result": "Evaluation on an open industrial dataset demonstrated that the approach improves scenario retrieval efficiency and reliability, offering quantifiable metrics for result trustworthiness and removing the need for specialized database or signal knowledge.", "conclusion": "The proposed LLM-based API solution enhances the process of retrieving specific driving scenarios from complex autonomous vehicle log data by making it accessible via natural language and offering visual validation, outperforming traditional SQL-based methods."}}
{"id": "2506.11697", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11697", "abs": "https://arxiv.org/abs/2506.11697", "authors": ["Yiwei Hu", "Zhen Li", "Kedie Shu", "Shenghua Guan", "Deqing Zou", "Shouhuai Xu", "Bin Yuan", "Hai Jin"], "title": "SoK: Automated Vulnerability Repair: Methods, Tools, and Assessments", "comment": "The full version of \"SoK: Automated Vulnerability Repair: Methods,\n  Tools, and Assessments\" accepted by the 34th USENIX Security Symposium\n  (USENIX Security 2025)", "summary": "The increasing complexity of software has led to the steady growth of\nvulnerabilities. Vulnerability repair investigates how to fix software\nvulnerabilities. Manual vulnerability repair is labor-intensive and\ntime-consuming because it relies on human experts, highlighting the importance\nof Automated Vulnerability Repair (AVR). In this SoK, we present the\nsystematization of AVR methods through the three steps of AVR workflow:\nvulnerability analysis, patch generation, and patch validation. We assess AVR\ntools for C/C++ and Java programs as they have been widely studied by the\ncommunity. Since existing AVR tools for C/C++ programs are evaluated with\ndifferent datasets, which often consist of a few vulnerabilities, we construct\nthe first C/C++ vulnerability repair benchmark dataset, dubbed Vul4C, which\ncontains 144 vulnerabilities as well as their exploits and patches. We use\nVul4C to evaluate seven AVR tools for C/C++ programs and use the third-party\nVul4J dataset to evaluate two AVR tools for Java programs. We also discuss\nfuture research directions.", "AI": {"tldr": "The paper reviews and systematizes current Automated Vulnerability Repair (AVR) methods, creates a new C/C++ vulnerability benchmark (Vul4C), and uses it to evaluate several AVR tools, offering a snapshot of the field and key future directions.", "motivation": "The complexity of software is increasing, which leads to more vulnerabilities. Manual vulnerability repair is both labor-intensive and time-consuming, necessitating more efficient and automated solutions.", "method": "This paper systematizes Automated Vulnerability Repair (AVR) methods based on a three-step workflow: vulnerability analysis, patch generation, and patch validation. It assesses existing AVR tools for C/C++ and Java, constructs a new benchmark dataset Vul4C for C/C++ vulnerabilities, and evaluates AVR tools using Vul4C and the Vul4J dataset.", "result": "The authors created Vul4C, the first comprehensive C/C++ benchmark dataset with 144 vulnerabilities, their exploits, and patches. They evaluated seven AVR tools for C/C++ and two for Java using standardized datasets, contributing new comparative insights.", "conclusion": "The paper provides a structured evaluation of AVR methods and tools, introduces a valuable new benchmark for C/C++ repair research, and highlights areas and directions for future work in automated software vulnerability repair."}}
{"id": "2506.11722", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.11722", "abs": "https://arxiv.org/abs/2506.11722", "authors": ["Eduard C. Groen", "Fabiano Dalpiaz", "Martijn van Vliet", "Boris Winter", "Joerg Doerr", "Sjaak Brinkkemper"], "title": "Classification of Quality Characteristics in Online User Feedback using Linguistic Analysis, Crowdsourcing and LLMs", "comment": "Accepted at the Journal of Systems and Software (JSS); online\n  appendix and supplementary material available at\n  https://doi.org/10.5281/zenodo.15604749", "summary": "Software qualities such as usability or reliability are among the strongest\ndeterminants of mobile app user satisfaction and constitute a significant\nportion of online user feedback on software products, making it a valuable\nsource of quality-related feedback to guide the development process. The\nabundance of online user feedback warrants the automated identification of\nquality characteristics, but the online user feedback's heterogeneity and the\nlack of appropriate training corpora limit the applicability of supervised\nmachine learning. We therefore investigate the viability of three approaches\nthat could be effective in low-data settings: language patterns (LPs) based on\nquality-related keywords, instructions for crowdsourced micro-tasks, and large\nlanguage model (LLM) prompts. We determined the feasibility of each approach\nand then compared their accuracy. For the complex multiclass classification of\nquality characteristics, the LP-based approach achieved a varied precision\n(0.38-0.92) depending on the quality characteristic, and low recall;\ncrowdsourcing achieved the best average accuracy in two consecutive phases\n(0.63, 0.72), which could be matched by the best-performing LLM condition\n(0.66) and a prediction based on the LLMs' majority vote (0.68). Our findings\nshow that in this low-data setting, the two approaches that use crowdsourcing\nor LLMs instead of involving experts achieve accurate classifications, while\nthe LP-based approach has only limited potential. The promise of crowdsourcing\nand LLMs in this context might even extend to building training corpora.", "AI": {"tldr": "This paper compares three low-data approaches for extracting software quality insights from mobile app user reviews. Crowdsourcing and LLM methods offer strong classification accuracy, showing promise for both classification tasks and for bootstrapping training datasets, while simple keyword methods are less effective.", "motivation": "There is a need to automatically identify software quality characteristics (e.g., usability, reliability) in mobile app user feedback, as user reviews provide vital quality-related insights. However, feedback is heterogenous and lacks sufficient annotated data, making supervised machine learning approaches difficult to apply.", "method": "The paper investigates and compares three alternative methods suitable for low-data scenarios: (1) language patterns using quality-related keyword lists (LPs), (2) crowdsourced micro-task instructions, and (3) prompts for large language models (LLMs). The feasibility and classification accuracy of each method for multiclass quality characteristic identification are evaluated.", "result": "The LP-based (keyword) approach showed highly variable precision (0.38-0.92 depending on the quality characteristic) and generally low recall. Crowdsourcing delivered the best average accuracy across two phases (0.63, 0.72), which was closely matched by the best LLM configuration (0.66) and an LLM majority vote ensemble (0.68).", "conclusion": "In low-data environments, crowdsourcing and LLM-based methods (without expert involvement) can accurately classify quality-related feedback in user reviews, while the LP keyword-based approach is limited. Crowdsourcing and LLMs also show potential for constructing labeled training datasets."}}
{"id": "2506.11874", "categories": ["cs.SE", "D.2.1; D.2.4; D.2.10; F.4.1; F.4.3"], "pdf": "https://arxiv.org/pdf/2506.11874", "abs": "https://arxiv.org/abs/2506.11874", "authors": ["Arshad Beg", "Diarmuid O'Donoghue", "Rosemary Monahan"], "title": "A Short Survey on Formalising Software Requirements using Large Language Models", "comment": "Submitted to SAIV 2025 as extended abstract and received valuable\n  comments improving our draft. This version is the improved one after\n  addressing suggestions from reviewers for improving the draft", "summary": "This paper presents a focused literature survey on the use of large language\nmodels (LLM) to assist in writing formal specifications for software. A summary\nof thirty-five key papers is presented, including examples for specifying\nprograms written in Dafny, C and Java. This paper arose from the project\nVERIFAI - Traceability and verification of natural language requirements that\naddresses the challenges in writing formal specifications from requirements\nthat are expressed in natural language. Our methodology employed multiple\nacademic databases to identify relevant research. The AI-assisted tool Elicit\nfacilitated the initial paper selection, which were manually screened for final\nselection. The survey provides valuable insights and future directions for\nutilising LLMs while formalising software requirements.", "AI": {"tldr": "A survey of thirty-five papers reviews how large language models help convert natural language requirements into formal software specifications, providing key insights and future research directions.", "motivation": "Transforming natural language software requirements into formal specifications is a challenge in software engineering. Large language models (LLMs) offer potential to aid this process, but systematic understanding of their usefulness is needed.", "method": "A focused literature survey was conducted, examining thirty-five key papers identified from multiple academic databases. The AI-assisted tool Elicit was used for initial paper selection, followed by manual screening for final inclusion. The survey includes illustrative examples for Dafny, C, and Java.", "result": "The survey synthesizes current research on how LLMs aid in specifying software, offering examples and a comprehensive overview. It identifies valuable insights as well as future directions for research and tool development in this domain.", "conclusion": "LLMs show promise in supporting the transition from natural language requirements to formal specifications. The survey highlights the state of the field, practical examples, and avenues for improving LLM-assisted formalization of software requirements."}}
{"id": "2506.11928", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11928", "abs": "https://arxiv.org/abs/2506.11928", "authors": ["Zihan Zheng", "Zerui Cheng", "Zeyu Shen", "Shang Zhou", "Kaiyuan Liu", "Hansen He", "Dongruixuan Li", "Stanley Wei", "Hangyi Hao", "Jianzhu Yao", "Peiyao Sheng", "Zixuan Wang", "Wenhao Chai", "Aleksandra Korolova", "Peter Henderson", "Sanjeev Arora", "Pramod Viswanath", "Jingbo Shang", "Saining Xie"], "title": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?", "comment": "Project Page at https://livecodebenchpro.com/", "summary": "Recent reports claim that large language models (LLMs) now outperform elite\nhumans in competitive programming. Drawing on knowledge from a group of\nmedalists in international algorithmic contests, we revisit this claim,\nexamining how LLMs differ from human experts and where limitations still\nremain. We introduce LiveCodeBench Pro, a benchmark composed of problems from\nCodeforces, ICPC, and IOI that are continuously updated to reduce the\nlikelihood of data contamination. A team of Olympiad medalists annotates every\nproblem for algorithmic categories and conducts a line-by-line analysis of\nfailed model-generated submissions. Using this new data and benchmark, we find\nthat frontier models still have significant limitations: without external\ntools, the best model achieves only 53% pass@1 on medium-difficulty problems\nand 0% on hard problems, domains where expert humans still excel. We also find\nthat LLMs succeed at implementation-heavy problems but struggle with nuanced\nalgorithmic reasoning and complex case analysis, often generating confidently\nincorrect justifications. High performance appears largely driven by\nimplementation precision and tool augmentation, not superior reasoning.\nLiveCodeBench Pro thus highlights the significant gap to human grandmaster\nlevels, while offering fine-grained diagnostics to steer future improvements in\ncode-centric LLM reasoning.", "AI": {"tldr": "Despite reports, leading language models do not yet match expert humans in competitive programming, especially on hard problems requiring deep reasoning. A new human-annotated benchmark shows LLMs are strong at implementation but weak on complex, nuanced tasks. There is still a big gap to grandmaster-level human performance.", "motivation": "Recent claims suggest large language models (LLMs) outperform elite humans in competitive programming. This paper seeks to rigorously reevaluate these claims, identifying differences between LLMs and human experts and characterizing the current limitations of LLMs in this domain.", "method": "The authors develop LiveCodeBench Pro, a new benchmark comprising continuously updated competitive programming problems from Codeforces, ICPC, and IOI, with careful annotation by Olympiad medalists. They analyze model performance, including detailed error analysis through expert annotation and comparison to human performance.", "result": "The best current LLM achieves only 53% pass@1 on medium problems and 0% on hard problems without external tools, while expert humans excel at these. LLMs perform better on implementation-heavy tasks but perform poorly on problems requiring nuanced algorithmic reasoning. Errors often feature confidently incorrect justifications. LLM high performance is largely due to implementation skills and external tool usage, not reasoning.", "conclusion": "There remains a significant gap between LLMs and human grandmasters in competitive programming, particularly in algorithmic reasoning. The new benchmark provides granular diagnostics to guide further LLM development in code reasoning."}}
