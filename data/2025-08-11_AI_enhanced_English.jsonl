{"id": "2508.05693", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05693", "abs": "https://arxiv.org/abs/2508.05693", "authors": ["Siamak Farshidi", "Amir Saberhabibi", "Behbod Eskafi", "Niloofar Nikfarjam", "Sadegh Eskandari", "Slinger Jansen", "Michel Chaudron", "Bedir Tekinerdogan"], "title": "Empirical Evaluation of AI-Assisted Software Package Selection: A Knowledge Graph Approach", "comment": null, "summary": "Selecting third-party software packages in open-source ecosystems like Python\nis challenging due to the large number of alternatives and limited transparent\nevidence for comparison. Generative AI tools are increasingly used in\ndevelopment workflows, but their suggestions often overlook dependency\nevaluation, emphasize popularity over suitability, and lack reproducibility.\nThis creates risks for projects that require transparency, long-term\nreliability, maintainability, and informed architectural decisions. This study\nformulates software package selection as a Multi-Criteria Decision-Making\n(MCDM) problem and proposes a data-driven framework for technology evaluation.\nAutomated data pipelines continuously collect and integrate software metadata,\nusage trends, vulnerability information, and developer sentiment from GitHub,\nPyPI, and Stack Overflow. These data are structured into a decision model\nrepresenting relationships among packages, domain features, and quality\nattributes. The framework is implemented in PySelect, a decision support system\nthat uses large language models to interpret user intent and query the model to\nidentify contextually appropriate packages. The approach is evaluated using\n798,669 Python scripts from 16,887 GitHub repositories and a user study based\non the Technology Acceptance Model. Results show high data extraction\nprecision, improved recommendation quality over generative AI baselines, and\npositive user evaluations of usefulness and ease of use. This work introduces a\nscalable, interpretable, and reproducible framework that supports\nevidence-based software selection using MCDM principles, empirical data, and\nAI-assisted intent modeling.", "AI": {"tldr": "The paper presents PySelect, a decision support system that leverages empirical data, multi-criteria analysis, and AI-assisted user intent modeling to improve the selection of Python packages. It outperforms existing AI tools in transparency, recommendation quality, and user satisfaction.", "motivation": "Developers face challenges in selecting third-party Python packages due to an overwhelming number of choices, insufficient transparent comparison data, and shortcomings in generative AI tool recommendations, which often prioritize popularity over suitability and lack reproducibility. There is a need for a more transparent, reliable, and evidence-based approach to support informed package selection.", "method": "The authors formulate package selection as a Multi-Criteria Decision-Making (MCDM) problem. They build a data-driven framework using automated pipelines to gather and integrate software metadata, usage trends, vulnerabilities, and developer sentiment from GitHub, PyPI, and Stack Overflow. The data forms a decision model connecting packages, domain features, and quality attributes. PySelect, the proposed system, uses large language models to interpret user queries and select appropriate packages. The framework is evaluated with large-scale script analysis and a user study.", "result": "The framework shows high precision in data extraction, delivers better package recommendations than generative AI baselines, and receives positive user feedback regarding usefulness and ease of use. The system is demonstrated on 798,669 Python scripts from 16,887 GitHub repositories.", "conclusion": "This work provides a scalable, interpretable, and reproducible system for evidence-based software package selection using MCDM, empirical data, and AI-assisted decision support, enabling better transparency and reliability in development workflows."}}
{"id": "2508.05997", "categories": ["cs.PL", "cs.LO", "I.2.2; I.2.4"], "pdf": "https://arxiv.org/pdf/2508.05997", "abs": "https://arxiv.org/abs/2508.05997", "authors": ["Aditi Kabra", "Jonathan Laurent", "Stefan Mitsch", "Andr\u00e9 Platzer"], "title": "Hybrid Game Control Envelope Synthesis", "comment": null, "summary": "Control problems for embedded systems like cars and trains can be modeled by\ntwo-player hybrid games. Control envelopes, which are families of safe control\nsolutions, correspond to nondeterministic winning policies of hybrid games,\nwhere each deterministic specialization of the policy is a control solution.\nThis paper synthesizes nondeterministic winning policies for hybrid games that\nare as permissive as possible. It introduces subvalue maps, a compositional\nrepresentation of such policies that enables verification and synthesis along\nthe structure of the game. An inductive logical characterization in\ndifferential game logic (dGL) checks whether a subvalue map induces a sound\ncontrol envelope which always induces a winning play. A policy is said to win\nif it always achieves the desirable outcome when the player follows it, no\nmatter what actions the opponent plays. The maximal subvalue map, which allows\nthe most action options while still winning, is shown to exist and satisfy a\nlogical characterization. A family of algorithms for nondeterministic policy\nsynthesis can be obtained from the inductive subvalue map soundness\ncharacterization. An implementation of these findings is evaluated on examples\nthat use the expressivity of dGL to model a range of diverse control\nchallenges.", "AI": {"tldr": "The paper introduces a logical and compositional method to automatically synthesize the most permissive safe control strategies for hybrid games in embedded systems, using subvalue maps and differential game logic, and validates the approach with practical examples.", "motivation": "Embedded systems such as cars and trains involve complex control problems, which can be challenging to model and verify for safety. Using two-player hybrid games for modeling, there is a need for systematic approaches to synthesize control envelopes (collections of safe control solutions) that are maximally permissive without compromising safety.", "method": "The paper introduces subvalue maps, a compositional and inductive representation of nondeterministic winning policies for hybrid games. The method employs differential game logic (dGL) to logically characterize and check whether a subvalue map corresponds to a sound (safe and winning) control envelope. Algorithms for synthesizing these policies are derived from the inductive soundness characterization.", "result": "The paper proves the existence of the maximal subvalue map, which provides the most permissive set of control options that still guarantee winning outcomes. This logical framework allows for systematic synthesis of nondeterministic policies. The approach is implemented and demonstrated on diverse examples modeled with dGL, showing its applicability to various embedded control problems.", "conclusion": "The proposed approach enables the synthesis of maximally permissive, nondeterministic control policies for hybrid games in embedded systems, ensuring safety and correctness by leveraging differential game logic. The resulting methods are general, compositional, and supported by implementation and experimental validation."}}
{"id": "2508.05710", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05710", "abs": "https://arxiv.org/abs/2508.05710", "authors": ["Jia Fu", "Xinyu Yang", "Hongzhi Zhang", "Yahui Liu", "Jingyuan Zhang", "Qi Wang", "Fuzheng Zhang", "Guorui Zhou"], "title": "Klear-CodeTest: Scalable Test Case Generation for Code Reinforcement Learning", "comment": "21 pages, 11 figures", "summary": "Precise, correct feedback is crucial for effectively training large language\nmodels (LLMs) in code reinforcement learning. However, synthesizing\nhigh-quality test cases remains a profoundly challenging and unsolved problem.\nIn this work, we present Klear-CodeTest, a comprehensive test case synthesis\nframework featuring rigorous verification to ensure quality and reliability of\ntest cases. Our approach achieves broad coverage of programming problems via a\nnovel Generator-Validation (G-V) framework, ensuring correctness through a\nconsistency validation mechanism that verifies outputs against gold solutions.\nThe proposed G-V framework generates comprehensive test cases including both\nregular and corner cases, enhancing test coverage and discriminative power for\nsolution correctness assessment in code reinforcement learning. In addition, we\ndesign a multi-layered security sandbox system optimized for online\nverification platforms, guaranteeing safe and reliable code execution. Through\ncomprehensive experiments, we demonstrate the effectiveness of our curated\ndataset, showing significant improvements in model performance and training\nstability. The source codes, curated dataset and sandbox system are available\nat: https://github.com/Kwai-Klear/CodeTest.", "AI": {"tldr": "Klear-CodeTest is a new framework that generates and validates high-quality test cases for training code language models, resulting in better and more stable model performance. The approach covers more problem scenarios and ensures safe code execution, with all resources publicly available.", "motivation": "Obtaining accurate and diverse feedback is a major bottleneck in training large language models (LLMs) for code via reinforcement learning. High-quality test case generation is especially challenging yet critical for evaluating code solutions effectively.", "method": "The paper introduces Klear-CodeTest, a test case synthesis framework based on a novel Generator-Validation (G-V) method. This approach generates extensive test cases, including regular and edge cases, and verifies outputs using a consistency validation mechanism. Additionally, a multi-layered security sandbox is developed for safe code execution during validation.", "result": "Experiments show that the Klear-CodeTest dataset and framework lead to significant improvements in model performance and training stability in code reinforcement learning tasks.", "conclusion": "Klear-CodeTest provides a reliable and comprehensive test case generation and validation system, advancing the state-of-the-art in code RL by enabling more precise feedback and robust evaluation. The proposed solution improves both the performance and stability of models trained with it."}}
{"id": "2508.05747", "categories": ["cs.SE", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.05747", "abs": "https://arxiv.org/abs/2508.05747", "authors": ["Rohaizah Abdul Wahid", "Muhamad Said Nizamuddin Nadim", "Suliana Sulaiman", "Syahmi Akmal Shaharudin", "Muhammad Danial Jupikil", "Iqqwan Jasman Su Azlan Su"], "title": "Utilizing Composer Packages to Accelerate Laravel-Based Project Development Among Students: A Pedagogical and Practical Framework", "comment": null, "summary": "Laravel has emerged as a foundational framework in university web development\ncurricula. However, despite its scaffolding capabilities, students often\nstruggle to complete projects within limited academic timelines. This\nconceptual paper introduces Composer, PHP's standard dependency manager, and\ncategorizes a curated selection of Composer packages that significantly reduce\ndevelopment effort while fostering professional software practices. Grounded in\npractical and pedagogical considerations, the paper illustrates how educators\nand learners can strategically leverage these tools to build typical academic\nor personal Laravel-based systems. Central to this approach is maintaining code\nquality and reinforcing conceptual understanding. The paper also addresses\npotential risks such as package conflicts and over-reliance on tools, providing\nbest-practice recommendations to mitigate them. While the goal is to accelerate\ndevelopment, the deeper objective is to reinforce professional workflows and\nindustry readiness. Exposure to Composer packages enhances curriculum relevance\nand smooths the transition from academia to the workplace. However, effective\nintegration requires deliberate instructional design aligned with learning\nobjectives. Without guidance, students may treat packages as black boxes. Thus,\neducators must teach not only how to use these tools, but also when and why,\nencouraging critical evaluation of their utility and limitations. This ensures\nthat practical convenience supports rather than supplants deep learning.", "AI": {"tldr": "Introducing Composer packages into Laravel curricula can help students finish projects faster and learn professional software practices, but requires careful teaching to avoid superficial use and ensure deep understanding.", "motivation": "Students learning Laravel in universities often struggle to complete web development projects within tight academic timelines, even with built-in scaffolding features. There is a need to both accelerate development and reinforce professional practices.", "method": "This is a conceptual paper that catalogs and categorizes Composer, PHP's dependency manager, and a curated set of Composer packages. It provides practical and pedagogical guidelines, and best-practice recommendations for integrating these tools into academic settings.", "result": "Composer packages, when strategically integrated and taught with deliberate instructional design, can reduce students' workload, support professional development, and better prepare them for industry needs. However, risks such as package conflicts and over-reliance require mitigation through critical evaluation and guided learning.", "conclusion": "The effective use of Composer and selected packages can both speed up development and deepen professional practices in web development education. Teachers must emphasize not just tool usage but also critical thinking about when and why to use them, ensuring that practical efficiency does not undermine deep conceptual learning."}}
{"id": "2508.05799", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.05799", "abs": "https://arxiv.org/abs/2508.05799", "authors": ["Yoseph Berhanu Alebachew"], "title": "AI-Guided Exploration of Large-Scale Codebases", "comment": null, "summary": "Understanding large-scale, complex software systems is a major challenge for\ndevelopers, who spend a significant portion of their time on program\ncomprehension. Traditional tools such as static visualizations and reverse\nengineering techniques provide structural insights but often lack\ninteractivity, adaptability, and integration with contextual information.\nRecent advancements in large language models (LLMs) offer new opportunities to\nenhance code exploration workflows, yet their lack of grounding and integration\nwith structured views limits their effectiveness. This work introduces a hybrid\napproach that integrates deterministic reverse engineering with LLM-guided,\nintent-aware visual exploration. The proposed system combines UML-based\nvisualization, dynamic user interfaces, historical context, and collaborative\nfeatures into an adaptive tool for code comprehension. By interpreting user\nqueries and interaction patterns, the LLM helps developers navigate and\nunderstand complex codebases more effectively. A prototype implementation for\nJava demonstrates the feasibility of this approach. Future work includes\nempirical evaluation, scaling to polyglot systems, and exploring GUI-driven LLM\ninteraction models. This research lays the groundwork for intelligent,\ninteractive environments that align with developer cognition and collaborative\nworkflows.", "AI": {"tldr": "The paper proposes a system combining traditional code analysis and large language models to create interactive, context-aware software visualization tools. This hybrid approach helps developers understand complex software more easily and sets the stage for smarter, collaborative programming environments.", "motivation": "Developers face challenges understanding large, complex software systems using traditional tools, which lack interactivity, context, and adaptability. There is a need for more effective program comprehension tools that align with how developers work and think.", "method": "The authors introduce a hybrid system that integrates deterministic reverse engineering with LLM-guided, intent-aware visual exploration. This system combines UML visualizations, adaptive user interfaces, historical context tracking, and collaborative features. It employs large language models to interpret developer queries, guide exploration, and adapt to interaction patterns.", "result": "They developed a prototype for Java codebases showing the feasibility of their approach. The system enables more effective navigation and comprehension of complex code, but empirical evaluation and extension to multi-language support are proposed for future work.", "conclusion": "Integrating LLMs with structured reverse engineering techniques can create intelligent, adaptive code comprehension environments that better support developer cognition and collaboration. This lays a foundation for future interactive tools that address limitations of traditional software visualization."}}
{"id": "2508.05923", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05923", "abs": "https://arxiv.org/abs/2508.05923", "authors": ["Yanusha Mehendran", "Maolin Tang", "Yi Lu"], "title": "Enhancing Software Vulnerability Detection Through Adaptive Test Input Generation Using Genetic Algorithm", "comment": "26 Pages, 3 figures, 6 Tables, Submitted to Empirical Software\n  Engineering and it is under review", "summary": "Software vulnerabilities continue to undermine the reliability and security\nof modern systems, particularly as software complexity outpaces the\ncapabilities of traditional detection methods. This study introduces a genetic\nalgorithm-based method for test input generation that innovatively integrates\ngenetic operators and adaptive learning to enhance software vulnerability\ndetection. A key contribution is the application of the crossover operator,\nwhich facilitates exploration by searching across a broader space of potential\ntest inputs. Complementing this, an adaptive feedback mechanism continuously\nlearns from the system's execution behavior and dynamically guides input\ngeneration toward promising areas of the input space. Rather than relying on\nfixed or randomly selected inputs, the approach evolves a population of\nstructurally valid test cases using feedback-driven selection, enabling deeper\nand more effective code traversal. This strategic integration of exploration\nand exploitation ensures that both diverse and targeted test inputs are\ndeveloped over time. Evaluation was conducted across nine open-source\nJSON-processing libraries. The proposed method achieved substantial\nimprovements in coverage compared to a benchmark evolutionary fuzzing method,\nwith average gains of 39.8% in class coverage, 62.4% in method coverage, 105.0%\nin line coverage, 114.0% in instruction coverage, and 166.0% in branch\ncoverage. These results highlight the method's capacity to detect deeper and\nmore complex vulnerabilities, offering a scalable and adaptive solution to\nsoftware security testing.", "AI": {"tldr": "A novel genetic algorithm-based method with adaptive feedback significantly boosts vulnerability detection and code coverage, outperforming traditional fuzzing approaches on real-world software.", "motivation": "Traditional methods for detecting software vulnerabilities struggle to keep up with increasing software complexity. More effective and adaptive testing techniques are needed to find deeper and more complex vulnerabilities.", "method": "The study proposes a genetic algorithm-based approach for generating test inputs, using genetic operators (especially crossover) and an adaptive learning feedback mechanism. The technique evolves a population of valid test cases by learning from system execution and adjusting input generation dynamically. This enables a balance of exploration (broad search) and exploitation (targeted search) during software testing.", "result": "When evaluated on nine open-source JSON-processing libraries, the proposed method significantly outperformed a benchmark evolutionary fuzzing method. It achieved average improvements of 39.8% in class coverage, 62.4% in method coverage, 105.0% in line coverage, 114.0% in instruction coverage, and 166.0% in branch coverage.", "conclusion": "The new approach notably improves software vulnerability detection by enabling deeper and more complex code exploration. The results show that adaptive, feedback-driven genetic algorithms can be a scalable and effective solution for software security testing."}}
{"id": "2508.05949", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.05949", "abs": "https://arxiv.org/abs/2508.05949", "authors": ["Jialin Yang", "Zainab Saad", "Jiajun Wu", "Xiaoguang Niu", "Henry Leung", "Steve Drew"], "title": "A Survey on Task Scheduling in Carbon-Aware Container Orchestration", "comment": "Submitted to ACM Computing Surveys", "summary": "The soaring energy demands of large-scale software ecosystems and cloud data\ncenters, accelerated by the intensive training and deployment of large language\nmodels, have driven energy consumption and carbon footprint to unprecedented\nlevels. In response, both industry and academia are increasing efforts to\nreduce the carbon emissions associated with cloud computing through more\nefficient task scheduling and infrastructure orchestration. In this work, we\npresent a systematic review of various Kubernetes scheduling strategies,\ncategorizing them into hardware-centric and software-centric, annotating each\nwith its sustainability objectives, and grouping them according to the\nalgorithms they use. We propose a comprehensive taxonomy for cloud task\nscheduling studies, with a particular focus on the environmental sustainability\naspect. We analyze emerging research trends and open challenges, and our\nfindings provide critical insight into the design of sustainable scheduling\nsolutions for next-generation cloud computing systems.", "AI": {"tldr": "This paper systematically reviews Kubernetes scheduling methods and provides a taxonomy focused on environmental sustainability, aiming to guide the design of greener cloud computing systems in face of rising energy demands and carbon emissions.", "motivation": "Due to the rapid growth in energy demands by software ecosystems and data centers, especially from large language models, there is an urgent need to address the associated environmental impact, particularly carbon emissions. Industry and research are increasingly focused on making cloud computing more sustainable through smarter task scheduling and resource management.", "method": "The paper conducts a systematic review of Kubernetes scheduling strategies, classifying them as hardware-centric or software-centric, describing their sustainability goals, and organizing them by their underlying algorithms. It also develops a taxonomy focused on environmental sustainability for cloud task scheduling studies.", "result": "The study offers a comprehensive taxonomy of scheduling strategies and highlights current research trends, sustainability objectives, and open challenges in Kubernetes-based cloud scheduling for environmental sustainability.", "conclusion": "The insights provided can inform the design of more sustainable scheduling solutions for future cloud systems, helping reduce energy consumption and carbon emissions."}}
{"id": "2508.05970", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05970", "abs": "https://arxiv.org/abs/2508.05970", "authors": ["Yanzhou Li", "Shangqing Liu", "Kangjie Chen", "Tianwei Zhang", "Yang Liu"], "title": "Impact-driven Context Filtering For Cross-file Code Completion", "comment": null, "summary": "Retrieval-augmented generation (RAG) has recently demonstrated considerable\npotential for repository-level code completion, as it integrates cross-file\nknowledge with in-file preceding code to provide comprehensive contexts for\ngeneration. To better understand the contribution of the retrieved cross-file\ncontexts, we introduce a likelihood-based metric to evaluate the impact of each\nretrieved code chunk on the completion. Our analysis reveals that, despite\nretrieving numerous chunks, only a small subset positively contributes to the\ncompletion, while some chunks even degrade performance. To address this issue,\nwe leverage this metric to construct a repository-level dataset where each\nretrieved chunk is labeled as positive, neutral, or negative based on its\nrelevance to the target completion. We then propose an adaptive retrieval\ncontext filtering framework, CODEFILTER, trained on this dataset to mitigate\nthe harmful effects of negative retrieved contexts in code completion.\nExtensive evaluation on the RepoEval and CrossCodeLongEval benchmarks\ndemonstrates that CODEFILTER consistently improves completion accuracy compared\nto approaches without filtering operations across various tasks. Additionally,\nCODEFILTER significantly reduces the length of the input prompt, enhancing\ncomputational efficiency while exhibiting strong generalizability across\ndifferent models. These results underscore the potential of CODEFILTER to\nenhance the accuracy, efficiency, and attributability of repository-level code\ncompletion.", "AI": {"tldr": "Not all retrieved code contexts help code completion. The authors introduce CODEFILTER, a filtering system that uses a likelihood-based metric to assess code chunk value, leading to higher accuracy and efficiency in repository-level code generation.", "motivation": "The paper addresses the challenge in retrieval-augmented code generation systems: not all retrieved cross-file code contexts are helpful, and some may even harm completion performance. This motivates the need for mechanisms to filter or assess the quality of retrieved contexts.", "method": "The authors introduce a likelihood-based metric to evaluate each retrieved code chunk's impact on code completion. Using this metric, they label the retrieved chunks as positive, neutral, or negative. They build a repository-level dataset with these labels and develop CODEFILTER, an adaptive filtering framework trained on this labeled dataset to select relevant contexts and filter out harmful ones.", "result": "CODEFILTER is shown to consistently improve code completion accuracy across multiple benchmarks (RepoEval and CrossCodeLongEval) compared to methods without filtering. It also reduces input prompt length, improving efficiency, and demonstrates generalizability across models.", "conclusion": "The CODEFILTER framework enhances repository-level code completion by leveraging a filtering mechanism to select beneficial retrieved contexts. This leads to improved accuracy, efficiency, and better attribution in code generation systems."}}
{"id": "2508.06017", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06017", "abs": "https://arxiv.org/abs/2508.06017", "authors": ["Xiangzhe Xu", "Shiwei Feng", "Zian Su", "Chengpeng Wang", "Xiangyu Zhang"], "title": "Position: Intelligent Coding Systems Should Write Programs with Justifications", "comment": "The first two authors contributed equally to this work", "summary": "Intelligent coding systems are transforming software development by enabling\nusers to specify code behavior in natural language. However, the opaque\ndecision-making of AI-driven coders raises trust and usability concerns,\nparticularly for non-expert users who cannot inspect low-level implementations.\nWe argue that these systems should not only generate code but also produce\nclear, consistent justifications that bridge model reasoning and user\nunderstanding. To this end, we identify two critical justification\nproperties-cognitive alignment and semantic faithfulness-and highlight the\nlimitations of existing methods, including formal verification, static\nanalysis, and post-hoc explainability. We advocate exploring neuro-symbolic\napproaches for justification generation, where symbolic constraints guide model\nbehavior during training and program semantics are enriched through neural\nrepresentations, enabling automated consistency checks at inference time.", "AI": {"tldr": "AI coding tools need to justify their outputs in ways users can understand. The paper identifies essential properties of good justifications and suggests neuro-symbolic methods as the path forward, since current techniques fall short.", "motivation": "The motivation comes from the growing adoption of AI-driven coding tools that are often opaque in their reasoning, creating trust and usability problems for users, especially for non-experts.", "method": "The authors conduct a conceptual analysis, identifying justification properties (cognitive alignment and semantic faithfulness), critically evaluating current approaches like formal verification, static analysis, and explainability, and propose neuro-symbolic methods as a promising direction.", "result": "The key results are the identification of critical properties for trustworthy AI-generated code justifications and an argument that neuro-symbolic methods can bridge the gap between model reasoning and user understanding more effectively than current practices.", "conclusion": "The paper concludes that intelligent coding systems should not only generate code but also provide justifications that users can understand, emphasizing the need for cognitive alignment and semantic faithfulness in these justifications. Existing methods are insufficient, and neuro-symbolic approaches are recommended for future work."}}
{"id": "2508.06192", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.06192", "abs": "https://arxiv.org/abs/2508.06192", "authors": ["Lantian Li", "Yuyu Chen", "Jingwen Wu", "Yue Pan", "Zhongxing Yu"], "title": "Understanding Inconsistent State Update Vulnerabilities in Smart Contracts", "comment": "31 pages, 11 figures", "summary": "Smart contracts enable contract terms to be automatically executed and\nverified on the blockchain, and recent years have witnessed numerous\napplications of them in areas such as financial institutions and supply chains.\nThe execution logic of a smart contract is closely related to the contract\nstate, and thus the correct and safe execution of the contract depends heavily\non the precise control and update of the contract state. However, the contract\nstate update process can have issues. In particular, inconsistent state update\nissues can arise for reasons such as unsynchronized modifications. Inconsistent\nstate update bugs have been exploited by attackers many times, but existing\ndetection tools still have difficulty in effectively identifying them. This\npaper conducts the first large-scale empirical study about inconsistent state\nupdate vulnerabilities (that is, inconsistent state update bugs that are\nexploitable) in smart contracts, aiming to shed light for developers,\nresearchers, tool builders, and language or library designers in order to avoid\ninconsistent state update vulnerabilities. We systematically investigate 116\ninconsistent state update vulnerabilities in 352 real-world smart contract\nprojects, summarizing their root causes, fix strategies, and exploitation\nmethods. Our study provides 11 original and important findings, and we also\ngive the implications of our findings. To illustrate the potential benefits of\nour research, we also develop a proof-of-concept checker based on one of our\nfindings. The checker effectively detects issues in 64 popular GitHub projects,\nand 19 project owners have confirmed the detected issues at the time of\nwriting. The result demonstrates the usefulness and importance of our findings\nfor avoiding inconsistent state update vulnerabilities in smart contracts.", "AI": {"tldr": "This paper presents the first large-scale study of inconsistent state update vulnerabilities in smart contracts, analyzing 116 cases across 352 projects, revealing root causes and solutions, and demonstrating a checker that effectively detects real-world issues.", "motivation": "Inconsistent state updates in smart contracts can lead to critical vulnerabilities exploited by attackers. Existing detection tools struggle to identify these flaws, motivating the need for a comprehensive investigation and better solutions.", "method": "A large-scale empirical study was conducted, systematically analyzing 116 vulnerabilities in 352 smart contract projects. The study identified root causes, fix strategies, exploitation methods, and developed a proof-of-concept checker to detect issues.", "result": "The study summarized root causes and exploitation methods of vulnerabilities, revealed 11 key findings, and developed a checker that detected issues in 64 popular projects, with 19 owners confirming the findings. This demonstrates the effectiveness of the approach.", "conclusion": "The research provides crucial findings and practical solutions to help avoid inconsistent state update vulnerabilities in smart contracts, demonstrating that their approach can effectively detect such issues in real-world projects."}}
{"id": "2508.06299", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.06299", "abs": "https://arxiv.org/abs/2508.06299", "authors": ["Henrique Henriques", "Hugo Louren\u00e7o", "Vasco Amaral", "Miguel Goul\u00e3o"], "title": "Improving the Developer Experience with a Low-Code Process Modelling Language", "comment": "Preprint", "summary": "Context: The OutSystems Platform is a development environment composed of\nseveral DSLs, used to specify, quickly build, and validate web and mobile\napplications. The DSLs allow users to model different perspectives such as\ninterfaces and data models, define custom business logic and construct process\nmodels. Problem: The DSL for process modelling (Business Process Technology\n(BPT)), has a low adoption rate and is perceived as having usability problems\nhampering its adoption. This is problematic given the language maintenance\ncosts. Method: We used a combination of interviews, a critical review of BPT\nusing the \"Physics of Notation\" and empirical evaluations of BPT using the\nSystem Usability Scale (SUS) and the NASA Task Load indeX (TLX), to develop a\nnew version of BPT, taking these inputs and Outsystems' engineers' culture into\naccount. Results: Evaluations conducted with 25 professional software engineers\nshowed an increase of the semantic transparency on the new version, from 31% to\n69%, an increase in the correctness of responses, from 51% to 89%, an increase\nin the SUS score, from 42.25 to 64.78, and a decrease of the TLX score, from\n36.50 to 20.78. These differences were statistically significant. Conclusions:\nThese results suggest that the new version of BPT significantly improved the\ndeveloper experience of the previous version. The end users' background with\nOutSystems had a relevant impact on the final concrete syntax choices and\nachieved usability indicators.", "AI": {"tldr": "Low usability of OutSystems\u2019 process modelling DSL (BPT) was tackled by combining usability research and redesign methods. The new version significantly improved usability and adoption indicators, suggesting enhanced developer experience and reduced maintenance costs.", "motivation": "BPT, the process modelling DSL in OutSystems, had low adoption and usability issues, posing problems due to ongoing maintenance costs. Improving usability was necessary to foster higher adoption and reduce maintenance overhead.", "method": "The study used interviews, a physics-of-notation review, System Usability Scale (SUS) assessments, and NASA Task Load Index (TLX) evaluations to analyze and redesign the BPT DSL. Empirical evaluations were conducted with 25 software engineers.", "result": "After redesigning BPT, semantic transparency increased from 31% to 69%, response correctness from 51% to 89%, SUS scores from 42.25 to 64.78, and TLX scores decreased from 36.50 to 20.78. All improvements were statistically significant.", "conclusion": "The new version of Business Process Technology (BPT) considerably improved the developer experience over the previous version, with significant increases in usability scores and semantic transparency. OutSystems users' background influenced the final design and usability outcomes."}}
{"id": "2508.06365", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.06365", "abs": "https://arxiv.org/abs/2508.06365", "authors": ["Toufique Ahmed", "Jatin Ganhotra", "Avraham Shinnar", "Martin Hirzel"], "title": "Execution-Feedback Driven Test Generation from SWE Issues", "comment": null, "summary": "A software engineering issue (SWE issue) is easier to resolve when\naccompanied by a reproduction test. Unfortunately, most issues do not come with\nfunctioning reproduction tests, so this paper explores how to generate them\nautomatically. The primary challenge in this setting is that the code to be\ntested is either missing or wrong, as evidenced by the existence of the issue\nin the first place. This has held back test generation for this setting:\nwithout the correct code to execute, it is difficult to leverage execution\nfeedback to generate good tests. This paper introduces novel techniques for\nleveraging execution feedback to get around this problem, implemented in a new\nreproduction test generator called e-Otter++. Experiments show that e-Otter++\nrepresents a leap ahead in the state-of-the-art for this problem, generating\ntests with an average fail-to-pass rate of 63% on the TDD-Bench Verified\nbenchmark.", "AI": {"tldr": "The paper presents e-Otter++, a tool that uses execution feedback to generate reproduction tests even when code is missing or wrong. It achieves a 63% fail-to-pass rate on a standard benchmark, representing a significant step forward for automated test generation in difficult scenarios.", "motivation": "Most software engineering issues are hard to resolve because they lack functional reproduction tests, and generating such tests is challenging when the target code is missing or incorrect.", "method": "The paper proposes novel techniques that exploit execution feedback to generate reproduction tests, even when the code under test is incomplete or buggy. These techniques are implemented in a new test generator tool called e-Otter++.", "result": "Experiments with e-Otter++ on the TDD-Bench Verified benchmark demonstrated that it can produce reproduction tests with an average fail-to-pass rate of 63%, which is a significant improvement over existing approaches.", "conclusion": "The approach and implementation of e-Otter++ advance the generation of reproduction tests for software issues where code is missing or broken, showing strong performance and overcoming key challenges in the field."}}
{"id": "2508.06414", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.06414", "abs": "https://arxiv.org/abs/2508.06414", "authors": ["Dongze Li", "Songqiang Chen", "Jialun Cao", "Shing-Chi Cheung"], "title": "What Builds Effective In-Context Examples for Code Generation?", "comment": null, "summary": "In-Context Learning (ICL) has emerged as a promising solution to enhance the\ncode generation capabilities of Large Language Models (LLMs), which\nincorporates code examples inside the prompt to let LLMs learn from\ndemonstrations. However, despite the substantial effectiveness of the code\nexample-based ICL approach, the specific features (e.g., identifier naming\nstyles, code formatting, solution insight) within the ICL-provided code\nexamples that significantly contribute to the ICL's effectiveness remain\nunclear. This paper systematically investigates the impact of various code\nfeatures on ICL with code examples through controlled ablation studies. Our\nfindings reveal that the appropriate naming of variables and functions is\ncrucial for effective code generation, with their elimination leading to\nperformance decreases of up to 30 percentage points. We further demonstrate\nthat LLMs prioritize semantically meaningful identifier names over formatting\nconventions, with language-specific preferences regarding identifier verbosity.\nAdditionally, our investigation into ICL's potential for enhancing reflection\nand inference capabilities reveals that current LLMs struggle to extract\ngeneralizable problem-solving insights from similar code solutions, despite\nbeing capable of utilizing direct information effectively. These findings are\nexpected to provide valuable insights for optimizing ICL systems in code\ngeneration applications and highlight fundamental challenges in\nreflection-based learning for code generation tasks.", "AI": {"tldr": "The paper finds that good variable/function naming in code examples is critical for in-context learning in code generation with LLMs, while formatting matters less. LLMs still struggle with extracting generalizable insights for problem-solving, highlighting areas for future improvement.", "motivation": "There is growing interest in using in-context learning (ICL) with code examples to enhance the code generation abilities of large language models (LLMs). However, it remains unclear which specific code features in these examples most affect the success of ICL.", "method": "The paper conducts controlled ablation studies, systematically varying code features (e.g., naming styles, formatting, solution insights) in the code examples used for ICL, to assess their impact on code generation performance.", "result": "Appropriate naming of variables and functions is crucial; their removal reduces performance by up to 30 percentage points. LLMs prioritize semantically meaningful identifier names over formatting. There are language-specific preferences for identifier verbosity. Current LLMs can use direct information from code, but struggle to infer generalizable problem-solving insights from similar solutions.", "conclusion": "Optimizing naming in code examples is key for maximizing ICL effectiveness in code generation. Improving reflection-based learning remains a significant challenge. The findings provide guidance for designing ICL code prompts and reveal limitations in current LLMs' ability to generalize problem-solving knowledge."}}
