{"id": "2507.09539", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.09539", "abs": "https://arxiv.org/abs/2507.09539", "authors": ["Anna Bolotina", "Christoph M. Kirsch", "Stefanie Muroya Lei", "Matthias Pleschinger"], "title": "Bounded Model Checking of RISC-V Machine Code with Context-Free-Language Ordered Binary Decision Diagrams", "comment": null, "summary": "Symbolic execution is a powerful technique for analyzing the behavior of\nsoftware yet scalability remains a challenge due to state explosion in control\nand data flow. Existing tools typically aim at managing control flow\ninternally, often at the expense of completeness, while offloading reasoning\nover data flow to SMT solvers. Moreover, reasoning typically happens on source\ncode or intermediate representation level to leverage structural information,\nmaking machine code generation part of the trust base. We are interested in\nchanging the equation in two non-trivial ways: pushing reasoning down to\nmachine code level, and then offloading reasoning entirely into SMT solvers and\nother, possibly more efficient solver technology. In more abstract terms, we\nare asking if bit-precise reasoning technology can be made scalable on\nsoftware, and not just hardware. For this purpose, we developed two tools\ncalled rotor and bitme for model generation and bounded model checking,\nrespectively. We chose RISC-V restricted to integer arithmetic as modeling\ntarget for rotor since RISC-V integer semantics is essentially equivalent to\nestablished SMT semantics over bitvectors and arrays of bitvectors. While\nstate-of-the-art SMT solvers struggle in our experiments, we have evidence that\nthere is potential for improvement. To show the potential, we have slightly\ngeneralized and then implemented in bitme two types of binary decision diagrams\n(BDDs): algebraic decision diagrams (ADDs) and context-free-language ordered\nbinary decision diagrams (CFLOBDDs). Bitme uses BDDs to propagate program input\nthrough models, essentially generalizing constant propagation to domain\npropagation. SMT solvers only get involved when model input cannot be\npropagated, significanly speeding up SMT solving. We then study the impact on\nstate explosion of CFLOBDDs, which are potentially more scalable than ADDs.", "AI": {"tldr": "The paper proposes shifting symbolic execution reasoning fully to the machine code level using BDDs before invoking SMT solvers. The tools rotor and bitme, especially leveraging CFLOBDDs, show potential for scalable, bit-precise software analysis, alleviating state explosion and outperforming standard SMT solver approaches.", "motivation": "Symbolic execution is powerful for software analysis, but suffers from scalability issues due to state explosion. Existing approaches manage control flow internally and use SMT solvers for data flow, typically at source or IR level, making machine code generation part of the trust base. The paper seeks to shift reasoning directly to machine code and fully offload it to solvers, questioning whether bit-precise reasoning can scale for software as it does for hardware.", "method": "They developed two tools, rotor and bitme, targeting RISC-V machine code. Rotor is used for model generation, and bitme for bounded model checking. They implemented two kinds of binary decision diagrams (BDDs) in bitme: algebraic decision diagrams (ADDs) and context-free-language ordered binary decision diagrams (CFLOBDDs). Bitme uses these BDDs for domain propagation, only invoking SMT solvers when necessary.", "result": "Experiments show that current SMT solvers struggle on their tasks, but using BDDs (especially CFLOBDDs) in bitme significantly improves symbolic execution by speeding up SMT solving and addressing state explosion. CFLOBDDs may offer better scalability compared to ADDs.", "conclusion": "Offloading reasoning to BDDs at the machine code level can improve symbolic execution scalability, reducing the reliance on SMT solvers and potentially offering more efficient analysis. This opens up new avenues for scalable, bit-precise software analysis directly on machine code."}}
{"id": "2507.09883", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.09883", "abs": "https://arxiv.org/abs/2507.09883", "authors": ["Swarn Priya", "Fr\u00e9d\u00e9ric Besson", "Connor Sughrue", "Tim Steenvoorden", "Jamie Fulford", "Freek Verbeek", "Binoy Ravindran"], "title": "BeePL: Correct-by-compilation kernel extensions", "comment": "45 pages, 18 figures", "summary": "eBPF is a technology that allows developers to safely extend kernel\nfunctionality without modifying kernel source code or developing loadable\nkernel modules. Since the kernel governs critical system operations and\nenforces isolation boundaries between user space and privileged data, any\nmechanism that modifies its behavior must meet the highest standards of safety\nand correctness. To this end, the eBPF toolchain includes a verifier, which\nstatically checks safety properties such as memory access validity, bounded\nloops, and type correctness before loading the program into the kernel.\nHowever, the existing verifier is both overly conservative in some\ncases-rejecting valid programs-and unsound in others, permitting unsafe\nbehavior that violates the intended semantics of the kernel interface.\n  To address these challenges, we introduce BeePL, a domain-specific language\nfor eBPF with a formally verified type system. The BeePL type system, along\nwith the language design, statically enforces key safety properties such as\ntype-correct memory access, safe pointer usage, absence of unbounded loops, and\nstructured control flow. These guarantees are backed by formal type soundness\nproofs, ensuring that well-typed programs satisfy the safety invariants\nrequired by the eBPF execution environment. BeePL also proves that well-typed\nsource programs meet critical eBPF-specific properties related to memory\nsafety, termination, and control flow, enabling high-level reasoning prior to\ncompilation. For properties not fully enforceable statically-such as dynamic\nbounds and undefined behavior-BeePL inserts semantics-preserving runtime checks\nduring compilation. We develop a verified compilation strategy that extends\nCompCert to generate BPF bytecode from BeePL programs, establishing a\nprincipled foundation for an end-to-end verifiable toolchain for safe kernel\nextensions.", "AI": {"tldr": "BeePL is a new, formally verified language and toolchain for eBPF programs, guaranteeing key safety properties through static analysis and runtime checks, and addressing flaws in the current eBPF verifier.", "motivation": "The paper is motivated by the shortcomings of the existing eBPF verifier, which is either too conservative (rejecting valid programs) or unsound (allowing unsafe programs). Since eBPF is used to extend kernel functionality, higher standards for safety and correctness are essential.", "method": "The authors introduce BeePL, a domain-specific language for eBPF. BeePL has a formally verified type system that statically enforces important safety properties. The authors also provide formal type soundness proofs and a verified compilation strategy extending CompCert for generating BPF bytecode from BeePL programs. Runtime checks are inserted by the compiler to handle properties that can't be ensured statically.", "result": "BeePL ensures type-correct memory access, safe pointer usage, structured control flow, and absence of unbounded loops. Formal proofs guarantee safety properties are upheld for well-typed programs. The verified compiler completes an end-to-end verifiable toolchain for safely extending kernel functionality.", "conclusion": "BeePL provides a principled, formally verified approach to safe eBPF programming and toolchain construction, addressing both soundness and completeness issues in existing verifier implementations."}}
{"id": "2507.10301", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.10301", "abs": "https://arxiv.org/abs/2507.10301", "authors": ["Wenhao Tang", "Sam Lindley"], "title": "Rows and Capabilities as Modal Effects", "comment": null, "summary": "Effect handlers allow programmers to model and compose computational effects\nmodularly. Effect systems statically guarantee that all effects are handled.\nSeveral recent practical effect systems are based on either row polymorphism or\ncapabilities. However, there remains a gap in understanding the precise\nrelationship between effect systems with such disparate foundations. The main\ndifficulty is that in both row-based and capability-based systems, effect\ntracking is typically entangled with other features such as functions.\n  We propose a uniform framework for encoding, analysing, and comparing effect\nsystems. Our framework exploits and generalises modal effect types, a recent\nnovel effect system which decouples effect tracking from functions via\nmodalities. Modalities offer fine-grained control over when and how effects are\ntracked, enabling us to express different strategies for effect tracking. We\ngive encodings as macro translations from existing row-based and\ncapability-based effect systems into our framework and show that these\nencodings preserve types and semantics. Our encodings reveal the essence of\neffect tracking mechanisms in different effect systems, enable a direct\nanalysis on their differences, and provide valuable insights on language\ndesign.", "AI": {"tldr": "The authors introduce a unified framework based on modal effect types to compare and analyze row-polymorphic and capability-based effect systems. Their translations are type- and semantics-preserving, clarifying the relationships between different effect tracking approaches and informing future language design.", "motivation": "There is a lack of understanding regarding the precise relationship between effect systems based on row polymorphism and those based on capabilities, mainly because effect tracking is often coupled with other language features. This complicates comparative analysis.", "method": "The authors propose a uniform framework that leverages modal effect types to decouple effect tracking from other features, like functions. They use modal types to encode, analyze, and compare different effect systems by providing macro translations from both row-based and capability-based effect systems into their unified framework. They further show that these translations preserve both types and semantics.", "result": "The proposed encodings successfully translate existing row-based and capability-based effect systems into the unified modal effect type framework while preserving their types and semantics. This enables clear comparison and analysis of the core mechanisms of effect tracking in these systems.", "conclusion": "The modal effect type framework captures the essence of effect tracking strategies in various effect systems, allowing detailed comparison and analysis. It leads to valuable insights for designing programming languages with effect systems and clarifies the relationships between previously disparate approaches."}}
{"id": "2507.10482", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.10482", "abs": "https://arxiv.org/abs/2507.10482", "authors": ["Simon Guilloud", "Viktor Kun\u010dak"], "title": "Orthologic Type Systems", "comment": null, "summary": "We propose to use orthologic as the basis for designing type systems\nsupporting intersection, union, and negation types in the presence of subtyping\nassumptions. We show how to extend orthologic to support monotonic and\nantimonotonic functions, supporting the use of type constructors in such type\nsystems. We present a proof system for orthologic with function symbols,\nshowing that it admits partial cut elimination. Using these insights, we\npresent an $\\mathcal O(n^2(1+m))$ algorithm for deciding the subtyping relation\nunder $m$ assumptions. We also show $O(n^2)$ polynomial-time normalization\nalgorithm, allowing simplification of types to their minimal canonical form.", "AI": {"tldr": "The paper introduces orthologic as a logical foundation for type systems with advanced type operations, providing efficient algorithms for subtyping and type normalization. This allows type systems to support intersection, union, and negation types under subtyping assumptions in a practical and canonical way.", "motivation": "Current type systems that incorporate intersection, union, and negation types struggle with efficient reasoning when subtyping and type constructors are involved. There is a need for a logical foundation that naturally supports these features and allows practical decision and normalization procedures.", "method": "The paper uses orthologic, an extension of logic, as the foundation for type system design. It extends orthologic to support monotonic and antimonotonic functions for robust handling of type constructors. The authors develop a proof system with function symbols, analyze its properties (such as partial cut elimination), and devise algorithms for subtyping decision and type normalization.", "result": "The research yields an O(n^2(1+m)) algorithm for deciding subtyping under m assumptions and an O(n^2) polynomial-time normalization algorithm for simplifying types to minimal canonical form. The approach allows efficient and canonical handling of intersection, union, and negation types.", "conclusion": "Orthologic can effectively underpin type systems with complex type operations and subtyping, providing both theoretical soundness and efficient algorithms for core operations like subtyping decision and type normalization."}}
{"id": "2507.08943", "categories": ["cs.SE", "D.2.7"], "pdf": "https://arxiv.org/pdf/2507.08943", "abs": "https://arxiv.org/abs/2507.08943", "authors": ["Pedro Lopes", "Paola Accioly", "Paulo Borba", "Vitor Menezes"], "title": "Choosing the Right Git Workflow: A Comparative Analysis of Trunk-based vs. Branch-based Approaches", "comment": "11 pages with 3 figures", "summary": "Git has become one of the most widely used version control systems today.\nAmong its distinguishing features, its ability to easily and quickly create\nbranches stands out, allowing teams to customize their workflows. In this\ncontext, various formats of collaborative development workflows using Git have\nemerged and gained popularity among software engineers. We can categorize such\nworkflows into two main types: branch-based workflows and trunk-based\nworkflows. Branch-based workflows typically define a set of remote branches\nwith well-defined objectives, such as feature branches, a branch for feature\nintegration, and a main branch. The goal is to migrate changes from the most\nisolated branch to the main one shared by all as the code matures. In this\ncategory, GitFlow stands out as the most popular example. In contrast,\ntrunk-based workflows have a single remote branch where developers integrate\ntheir changes directly. In this range of options, choosing a workflow that\nmaximizes team productivity while promoting software quality becomes a\nnon-trivial task. Despite discussions on forums, social networks, and blogs,\nfew scientific articles have explored this topic. In this work, we provide\nevidence on how Brazilian developers work with Git workflows and what factors\nfavor or hinder the use of each model. To this end, we conducted\nsemi-structured interviews and a survey with software developers. Our results\nindicate that trunk-based development favors fast-paced projects with\nexperienced and smaller teams, while branch-based development suits less\nexperienced and larger teams better, despite posing management challenges.", "AI": {"tldr": "This paper studies how Brazilian developers choose between branch-based and trunk-based Git workflows. Through interviews and a survey, it finds that trunk-based is best for small, experienced, fast-moving teams, while branch-based fits larger, less experienced groups despite more management overhead.", "motivation": "Despite the widespread adoption of Git, there is limited scientific research on how different Git workflows (branch-based vs. trunk-based) affect software teams, especially regarding which contexts favor each workflow type.", "method": "The authors conducted semi-structured interviews and a survey with Brazilian software developers to gather evidence on their workflow preferences and the factors influencing the adoption of each model.", "result": "Trunk-based workflows are better suited for fast-paced projects with experienced, smaller teams. Branch-based workflows are more appropriate for larger teams with less experience, even though they introduce additional management complexity.", "conclusion": "Choice of Git workflow strongly depends on team size, experience, and pace of development. There is no one-size-fits-all solution; workflow should be selected to match team characteristics and project needs."}}
{"id": "2507.08992", "categories": ["cs.SE", "cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.08992", "abs": "https://arxiv.org/abs/2507.08992", "authors": ["Abdelhalim Dahou", "Ansgar Scherp", "Sebastian Kurten", "Brigitte Mathiak", "Madhu Chauhan"], "title": "Semantic Source Code Segmentation using Small and Large Language Models", "comment": "18 pages, 4 figures", "summary": "Source code segmentation, dividing code into functionally coherent segments,\nis crucial for knowledge retrieval and maintenance in software development.\nWhile enabling efficient navigation and comprehension of large codebases,\nmanual and syntactic analysis approaches have become impractical as\nrepositories grow, especially for low-resource languages like R and their\nresearch domains (e.g., social sciences, psychology).This paper introduces an\nautomated, domain-specific approach for research R code segmentation using\nLarge and Small Language Models (LLMs/SLMs). It presents two novel approaches\nand a human-annotated dataset, StatCodeSeg. We explore two distinct approaches:\nline-by-line analysis with context and range-based segment determination. We\nexperiment with LLMs and fine-tuned SLMs. To support the generalizability of\nour approaches, we also include experiments on Python code from the computer\nscience domain.Our results show that context-based line-by-line analysis is\nsuperior over range-based segmentation.Using smaller language models like\nCodeBERT and an encoder-only version of CodeT5+ are better than their LLM\ncounterparts. Most notably, these two best-performing models did not see R code\nduring pre-training versus the LLMs but were only fine-tuned on 4,130 lines of\nmanually annotated code.", "AI": {"tldr": "The paper presents automated R code segmentation methods using language models, showing that fine-tuned smaller models with context-based line-by-line analysis are most effective, outperforming larger models even without R-specific pre-training.", "motivation": "Source code segmentation is essential for efficient knowledge retrieval and maintenance in large codebases, especially in low-resource programming languages like R, where existing manual and syntactic methods have become impractical.", "method": "The paper introduces two novel automated methods for domain-specific R code segmentation using language models: (1) context-based line-by-line analysis and (2) range-based segment determination. Experiments are conducted with both large language models (LLMs) and smaller language models (SLMs), leveraging a newly constructed human-annotated dataset, StatCodeSeg. Experiments are also extended to Python code to test generalizability.", "result": "Context-based line-by-line analysis outperforms range-based segmentation. Smaller models such as CodeBERT and an encoder-only version of CodeT5+ perform better than larger LLMs, despite being fine-tuned exclusively on a modest amount (4,130 lines) of manually annotated code and not being pre-trained on R code.", "conclusion": "Automated code segmentation for R is feasible and effective using fine-tuned SLMs and context-based methods, with smaller models outperforming larger LLMs even without prior R-specific pre-training."}}
{"id": "2507.09023", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.09023", "abs": "https://arxiv.org/abs/2507.09023", "authors": ["Yao Fehlis", "Charles Crain", "Aidan Jensen", "Michael Watson", "James Juhasz", "Paul Mandel", "Betty Liu", "Shawn Mahon", "Daren Wilson", "Nick Lynch-Jonely", "Ben Leedom", "David Fuller"], "title": "Accelerating Drug Discovery Through Agentic AI: A Multi-Agent Approach to Laboratory Automation in the DMTA Cycle", "comment": null, "summary": "The pharmaceutical industry faces unprecedented challenges in drug discovery,\nwith traditional approaches struggling to meet modern therapeutic development\ndemands. This paper introduces a novel AI framework, Tippy, that transforms\nlaboratory automation through specialized AI agents operating within the\nDesign-Make-Test-Analyze (DMTA) cycle. Our multi-agent system employs five\nspecialized agents - Supervisor, Molecule, Lab, Analysis, and Report, with\nSafety Guardrail oversight - each designed to excel in specific phases of the\ndrug discovery pipeline. Tippy represents the first production-ready\nimplementation of specialized AI agents for automating the DMTA cycle,\nproviding a concrete example of how AI can transform laboratory workflows. By\nleveraging autonomous AI agents that reason, plan, and collaborate, we\ndemonstrate how Tippy accelerates DMTA cycles while maintaining scientific\nrigor essential for pharmaceutical research. The system shows significant\nimprovements in workflow efficiency, decision-making speed, and\ncross-disciplinary coordination, offering a new paradigm for AI-assisted drug\ndiscovery.", "AI": {"tldr": "Tippy introduces AI agent automation to drug discovery, dramatically improving workflow speed and efficiency by using a multi-agent system throughout the DMTA cycle.", "motivation": "Traditional drug discovery approaches are increasingly unable to meet the accelerating demands of modern therapeutic development, creating an urgent need for more efficient, automated solutions.", "method": "The paper proposes Tippy, a novel multi-agent AI framework comprised of five specialized agents (Supervisor, Molecule, Lab, Analysis, Report) with an additional Safety Guardrail agent. These agents are designed to autonomously handle different stages of the Design-Make-Test-Analyze (DMTA) cycle in drug discovery, facilitating reasoning, planning, and collaboration.", "result": "The implementation of Tippy leads to notable improvements in workflow efficiency, faster decision-making, and better coordination across disciplines within the DMTA cycle.", "conclusion": "Tippy demonstrates that specialized AI agents can significantly accelerate and streamline the laboratory automation process in pharmaceutical research, setting a new standard for AI-driven drug discovery."}}
{"id": "2507.09039", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09039", "abs": "https://arxiv.org/abs/2507.09039", "authors": ["Aakash Sorathiya", "Gouri Ginde"], "title": "Towards Extracting Software Requirements from App Reviews using Seq2seq Framework", "comment": null, "summary": "Mobile app reviews are a large-scale data source for software improvements. A\nkey task in this context is effectively extracting requirements from app\nreviews to analyze the users' needs and support the software's evolution.\nRecent studies show that existing methods fail at this task since app reviews\nusually contain informal language, grammatical and spelling errors, and a large\namount of irrelevant information that might not have direct practical value for\ndevelopers. To address this, we propose a novel reformulation of requirements\nextraction as a Named Entity Recognition (NER) task based on the\nsequence-to-sequence (Seq2seq) generation approach. With this aim, we propose a\nSeq2seq framework, incorporating a BiLSTM encoder and an LSTM decoder, enhanced\nwith a self-attention mechanism, GloVe embeddings, and a CRF model. We\nevaluated our framework on two datasets: a manually annotated set of 1,000\nreviews (Dataset 1) and a crowdsourced set of 23,816 reviews (Dataset 2). The\nquantitative evaluation of our framework showed that it outperformed existing\nstate-of-the-art methods with an F1 score of 0.96 on Dataset 2, and achieved\ncomparable performance on Dataset 1 with an F1 score of 0.47.", "AI": {"tldr": "The paper introduces a deep learning-based sequence approach for extracting requirements from noisy app reviews, outperforming previous methods and helping developers better understand user needs.", "motivation": "Existing methods struggle to effectively extract software requirements from mobile app reviews due to their informal language, errors, and irrelevant content, making it difficult for developers to understand user needs and improve software.", "method": "The paper reformulates requirements extraction from app reviews as a Named Entity Recognition (NER) task using a sequence-to-sequence (Seq2seq) framework. The proposed model consists of a BiLSTM encoder, an LSTM decoder, self-attention mechanism, GloVe embeddings, and a CRF model. This framework was evaluated on two annotated datasets of app reviews.", "result": "The proposed Seq2seq NER framework outperformed current state-of-the-art methods, with an F1 score of 0.96 on a large dataset of 23,816 app reviews, and achieved comparable performance (F1 score of 0.47) on a smaller, manually annotated dataset of 1,000 reviews.", "conclusion": "Reformulating requirements extraction as an NER task and utilizing an enhanced Seq2seq framework significantly improves the extraction of meaningful requirements from noisy and informal mobile app reviews."}}
{"id": "2507.09049", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09049", "abs": "https://arxiv.org/abs/2507.09049", "authors": ["Aakash Sorathiya", "Gouri Ginde"], "title": "CMER: A Context-Aware Approach for Mining Ethical Concern-related App Reviews", "comment": null, "summary": "With the increasing proliferation of mobile applications in our daily lives,\nthe concerns surrounding ethics have surged significantly. Users communicate\ntheir feedback in app reviews, frequently emphasizing ethical concerns, such as\nprivacy and security. Incorporating these reviews has proved to be useful for\nmany areas of software engineering (e.g., requirement engineering, testing,\netc.). However, app reviews related to ethical concerns generally use\ndomain-specific language and are typically overshadowed by more generic\ncategories of user feedback, such as app reliability and usability. Thus,\nmaking automated extraction a challenging and time-consuming effort.\n  This study proposes CMER (A \\underline{C}ontext-Aware Approach for\n\\underline{M}ining \\underline{E}thical Concern-related App\n\\underline{R}eviews), a novel approach that combines Natural Language Inference\n(NLI) and a decoder-only (LLaMA-like) Large Language Model (LLM) to extract\nethical concern-related app reviews at scale. In CMER, NLI provides\ndomain-specific context awareness by using domain-specific hypotheses, and the\nLlama-like LLM eliminates the need for labeled data in the classification task.\nWe evaluated the validity of CMER by mining privacy and security-related\nreviews (PSRs) from the dataset of more than 382K app reviews of mobile\ninvestment apps. First, we evaluated four NLI models and compared the results\nof domain-specific hypotheses with generic hypotheses. Next, we evaluated three\nLLMs for the classification task. Finally, we combined the best NLI and LLM\nmodels (CMER) and extracted 2,178 additional PSRs overlooked by the previous\nstudy using a keyword-based approach, thus demonstrating the effectiveness of\nCMER. These reviews can be further refined into actionable requirement\nartifacts.", "AI": {"tldr": "The paper proposes CMER, a novel framework that combines natural language inference and a large language model to efficiently extract privacy and security concerns from mobile app reviews, outperforming keyword-based methods and enabling better handling of ethical issues in app development.", "motivation": "As mobile apps become more widespread, users increasingly raise ethical concerns such as privacy and security in their app reviews. However, identifying these concerns is difficult due to domain-specific language and the prevalence of more generic feedback, making automated extraction laborious.", "method": "The authors introduce CMER, a context-aware method leveraging Natural Language Inference (NLI) with domain-specific hypotheses and a decoder-only Large Language Model (LLM, LLaMA-like) to identify ethical concern-related app reviews without requiring labeled data. The approach is evaluated on a large dataset of investment app reviews, comparing different NLI and LLM models, then combining the best-performing ones.", "result": "CMER successfully extracted 2,178 additional privacy and security-related reviews that a traditional keyword-based approach missed, illustrating its effectiveness in surfacing overlooked ethical concerns.", "conclusion": "CMER provides an improved way to mine ethical concern-related app reviews, outperforming existing keyword-based methods and offering valuable input for further software engineering processes."}}
{"id": "2507.09051", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09051", "abs": "https://arxiv.org/abs/2507.09051", "authors": ["Aakash Sorathiya", "Gouri Ginde"], "title": "SAGE: A Context-Aware Approach for Mining Privacy Requirements Relevant Reviews from Mental Health Apps", "comment": null, "summary": "Mental health (MH) apps often require sensitive user data to customize\nservices for mental wellness needs. However, such data collection practices in\nsome MH apps raise significant privacy concerns for users. These concerns are\noften mentioned in app reviews, but other feedback categories, such as\nreliability and usability, tend to take precedence. This poses a significant\nchallenge in automatically identifying privacy requirements-relevant reviews\n(privacy reviews) that can be utilized to extract privacy requirements and\naddress users' privacy concerns. Thus, this study introduces SAGE, a\ncontext-aware approach to automatically mining privacy reviews from MH apps\nusing Natural Language Inference (NLI) with MH domain-specific privacy\nhypotheses (provides domain-specific context awareness) and a GPT model\n(eliminates the need for fine-tuning). The quantitative evaluation of SAGE on a\ndataset of 204K app reviews achieved an F1 score of 0.85 without any\nfine-tuning, outperforming the fine-tuned baseline classifiers BERT and T5.\nFurthermore, SAGE extracted 748 privacy reviews previously overlooked by\nkeyword-based methods, demonstrating its effectiveness through qualitative\nevaluation. These reviews can later be refined into actionable privacy\nrequirement artifacts.", "AI": {"tldr": "SAGE, leveraging domain-aware NLI and a GPT model, effectively identifies privacy-relevant reviews in mental health apps without fine-tuning, outperforming other classifiers and uncovering previously missed user privacy concerns.", "motivation": "Mental health apps require sensitive personal data, leading to privacy concerns among users. However, privacy-related user feedback is often overshadowed by issues like reliability and usability, making it difficult to automatically identify reviews relevant to privacy requirements.", "method": "The study introduces SAGE, an automated, context-aware approach that uses Natural Language Inference (NLI) with mental health domain-specific privacy hypotheses and a GPT model to mine privacy-relevant reviews from app feedback, without requiring model fine-tuning.", "result": "SAGE achieved an F1 score of 0.85 on a dataset of 204K app reviews, surpassing fine-tuned classifiers like BERT and T5. It also identified 748 privacy-related reviews that previous keyword-based methods missed, highlighting its improved effectiveness.", "conclusion": "SAGE is a superior, fine-tuning-free method for mining privacy requirements from user reviews in mental health apps, allowing for more accurate extraction of actionable privacy requirement artifacts."}}
{"id": "2507.09063", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09063", "abs": "https://arxiv.org/abs/2507.09063", "authors": ["Avi Arora", "Jinu Jang", "Roshanak Zilouchian Moghaddam"], "title": "SetupBench: Assessing Software Engineering Agents' Ability to Bootstrap Development Environments", "comment": null, "summary": "Modern Large Language Model (LLM) agents promise end to end assistance with\nreal-world software tasks, yet existing benchmarks evaluate LLM agents almost\nexclusively in pre-baked environments where every dependency is pre-installed.\nTo fill this gap, we introduce SetupBench, a 93 instance benchmark that\nisolates the environment-bootstrap skill: starting from a bare Linux sandbox,\nan agent must install packages, resolve dependency conflicts, initialize\ndatabases, and configure background services. Our tasks span seven language\necosystems, five database engines, and multi-service orchestration scenarios,\neach accompanies by a natural language problem statement and a deterministic\nsuccess command. Through evaluation of OpenHands, a state-of-the-art coding\nagent, we find low success rates across task categories, with particular\nchallenges in repository setup (38.9-57.4%) and local database configuration\n(20.0-53.3%). Our analysis reveals systematic failure modes including\nincomplete development tooling installation, hallucinated task constraints, and\nnon-persistent environment modifications that break agent-human collaboration\nworkflows. We identify substantial inefficiencies in agent exploration\nstrategies, with 38-89% of actions being unnecessary compared to optimal human\nbehavior. These findings highlight gaps in current agents' practical\nenvironment-bootstrap capabilities. By targeting this critical yet\nunder-evaluated capability, SetupBench provides a rigorous yard-stick for the\nnext generation of software developer agents aiming to solve end to end\nreal-wold tasks.", "AI": {"tldr": "SetupBench is a new benchmark evaluating LLM agents' abilities to set up real software environments from scratch. Current state-of-the-art agents perform poorly, struggling with tasks like package installation and database setup, and acting inefficiently. The benchmark highlights major gaps in LLM agents\u2019 real-world software task capabilities.", "motivation": "Large Language Model (LLM) agents promise to assist with end-to-end real-world software tasks. However, current benchmarks only test them in controlled environments with all dependencies pre-installed, failing to reflect real-world scenarios where software setup and configuration are required. There is a need for benchmarks that assess agents' ability to bootstrap environments from scratch.", "method": "The authors introduce SetupBench, a benchmark with 93 instances requiring agents to bootstrap environments on bare Linux sandboxes. Tasks involve installing packages, resolving dependency conflicts, setting up databases, and configuring services across multiple programming languages and database engines. Each task has a natural language description and a success criterion. The benchmark is used to evaluate OpenHands, a leading coding agent.", "result": "OpenHands exhibited low success rates, particularly struggling with repository setup (38.9-57.4%) and local database configuration (20.0-53.3%). Common failure modes included incomplete installation of development tools, hallucinated task constraints, and changes that do not persist, which disrupt human-agent workflows. Furthermore, agents took 38-89% more actions than optimal human solutions, indicating inefficient exploration strategies.", "conclusion": "Current LLM agents lack robust environment-bootstrap capabilities, a critical skill for real-world software development tasks. SetupBench exposes these weaknesses and provides a challenging, realistic benchmark for improving agent performance in practical, end-to-end development scenarios."}}
{"id": "2507.09108", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09108", "abs": "https://arxiv.org/abs/2507.09108", "authors": ["Aaditya Bhatia", "Gustavo A. Oliva", "Gopi Krishnan Rajbahadur", "Haoxiang Zhang", "Yihao Chen", "Zhilong Chen", "Arthur Leung", "Dayi Lin", "Boyuan Chen", "Ahmed E. Hassan"], "title": "SPICE: An Automated SWE-Bench Labeling Pipeline for Issue Clarity, Test Coverage, and Effort Estimation", "comment": null, "summary": "High-quality labeled datasets are crucial for training and evaluating\nfoundation models in software engineering, but creating them is often\nprohibitively expensive and labor-intensive. We introduce SPICE, a scalable,\nautomated pipeline for labeling SWE-bench-style datasets with annotations for\nissue clarity, test coverage, and effort estimation. SPICE combines\ncontext-aware code navigation, rationale-driven prompting, and multi-pass\nconsensus to produce labels that closely approximate expert annotations.\nSPICE's design was informed by our own experience and frustration in labeling\nmore than 800 instances from SWE-Gym. SPICE achieves strong agreement with\nhuman-labeled SWE-bench Verified data while reducing the cost of labeling 1,000\ninstances from around $100,000 (manual annotation) to just $5.10. These results\ndemonstrate SPICE's potential to enable cost-effective, large-scale dataset\ncreation for SE-focused FMs. To support the community, we release both SPICE\ntool and SPICE Bench, a new dataset of 6,802 SPICE-labeled instances curated\nfrom 291 open-source projects in SWE-Gym (over 13x larger than SWE-bench\nVerified).", "AI": {"tldr": "SPICE is an automated pipeline that enables accurate, low-cost labeling of software engineering datasets, reducing costs over 19,000x while maintaining expert-level quality; it facilitates large-scale dataset creation for research and practice.", "motivation": "High-quality labeled datasets are essential for training and evaluating foundation models in software engineering, but manual annotation is extremely costly and labor-intensive.", "method": "The paper introduces SPICE, an automated and scalable pipeline that labels SWE-bench-style datasets for issue clarity, test coverage, and effort estimation using context-aware code navigation, rationale-driven prompting, and multi-pass consensus methods.", "result": "SPICE achieves strong agreement with expert annotations, decreases the cost of labeling 1,000 instances from around $100,000 to $5.10, and enables large-scale dataset generation. The authors have also released the SPICE tool and a large labeled dataset (SPICE Bench) for the community.", "conclusion": "SPICE provides an effective, scalable, and low-cost solution for creating high-quality labeled datasets in software engineering, making broad dataset creation practical and enabling advances in SE-focused foundation models."}}
{"id": "2507.09135", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09135", "abs": "https://arxiv.org/abs/2507.09135", "authors": ["Yalong Du", "Chaozheng Wang", "Huaijin Wang"], "title": "Position Paper: Programming Language Techniques for Bridging LLM Code Generation Semantic Gaps", "comment": null, "summary": "Large Language Models have demonstrated remarkable capabilities in automated\ncode generation, yet their statistical nature and black-box characteristics\ncreate significant semantic gaps manifested through syntax errors, semantic\nhallucinations, and reliability concerns. This position paper argues that\nprincipled integration of Programming Language (PL) techniques is essential for\nbridging these gaps. Through structured program representations, formal\ncorrectness guarantees, and robust verification mechanisms, PL techniques can\nelevate LLM-generated code from statistical pattern matching to truly reliable\nand trustworthy levels. This integration is crucial for developing systems that\ngenerate code that is not only functionally correct but also interpretable,\nverifiable, and ultimately trustworthy.", "AI": {"tldr": "Current LLMs for code generation have reliability issues due to their statistical nature. This paper argues that integrating Programming Language (PL) techniques\u2014like structure, verification, and correctness guarantees\u2014can significantly improve the trustworthiness and reliability of AI-generated code.", "motivation": "Large Language Models (LLMs) excel at generating code, but due to their black-box and statistical nature, their output may suffer from syntax errors, unreliable behavior, and lack of trustworthiness. This creates a need to address the semantic reliability of code produced by LLMs.", "method": "The paper advocates for integrating Programming Language (PL) techniques with LLMs. This includes leveraging structured program representations, providing formal correctness guarantees, and implementing robust verification mechanisms to improve the reliability of generated code.", "result": "The integration of PL techniques with LLMs can elevate code generation from simple statistical pattern matching to producing code that is more reliable, interpretable, and verifiable. This approach would address the shortcomings of current LLM-based code generation models, enhancing trustworthiness and practical utility.", "conclusion": "To produce code that is not only functionally correct but also interpretable, verifiable, and trustworthy, it is essential to combine LLMs with principled PL techniques. This integration is fundamental in moving beyond the current limitations of black-box generative models in code generation."}}
{"id": "2507.09186", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09186", "abs": "https://arxiv.org/abs/2507.09186", "authors": ["Minhaj Uddin Ahmad", "Akid Abrar", "Sagar Dasgupta", "Mizanur Rahman"], "title": "OpenCAMS: An Open-Source Connected and Automated Mobility Co-Simulation Platform for Advanced Transportation Research", "comment": null, "summary": "We introduce OpenCAMS (Open-Source Connected and Automated Mobility\nCo-Simulation Platform), an open-source, synchronized, and extensible\nco-simulation framework that tightly couples three best-in-class simulation\ntools: (i) SUMO, (ii) CARLA, and (iii) OMNeT++. OpenCAMS is designed to support\nadvanced research in transportation safety, mobility, and cybersecurity by\ncombining the strengths of each simulation domain. Specifically, SUMO provides\nlarge-scale, microscopic traffic modeling; CARLA offers high-fidelity 3D\nperception, vehicle dynamics, and control simulation; and OMNeT++ enables\nmodular, event-driven network communication, such as cellular\nvehicle-to-everything (C-V2X). OpenCAMS employs a time-synchronized,\nbidirectional coupling architecture that ensures coherent simulation\nprogression across traffic, perception, and communication domains while\npreserving modularity and reproducibility. For example, CARLA can simulate and\nrender a subset of vehicles that require detailed sensor emulation and control\nlogic; SUMO orchestrates network-wide traffic flow, vehicle routing, and\ntraffic signal management; and OMNeT++ dynamically maps communication nodes to\nboth mobile entities (e.g., vehicles) and static entities (e.g., roadside\nunits) to enable C-V2X communication. While these three simulators form the\nfoundational core of OpenCAMS, the platform is designed to be expandable and\nfuture-proof, allowing additional simulators to be integrated on top of this\ncore without requiring fundamental changes to the system architecture. The\nOpenCAMS platform is fully open-source and publicly available through its\nGitHub repository https://github.com/minhaj6/carla-sumo-omnetpp-cosim,\nproviding the research community with an accessible, flexible, and\ncollaborative environment for advancing next-generation intelligent\ntransportation systems.", "AI": {"tldr": "OpenCAMS is a flexible open-source co-simulation platform that unites SUMO, CARLA, and OMNeT++ for synchronized study of traffic, vehicle perception, and communication, offering a comprehensive, expandable tool for intelligent transportation system research.", "motivation": "The increasing complexity of intelligent transportation systems (ITS) requires advanced simulation platforms that can accurately model interactions between traffic, perception, and communication domains for research in safety, mobility, and cybersecurity.", "method": "OpenCAMS employs a tightly coupled, time-synchronized, bidirectional co-simulation framework that integrates three established simulators: SUMO for traffic modeling, CARLA for high-fidelity vehicle perception and control, and OMNeT++ for event-driven network communication. The architecture maintains modularity and allows future expansions without significant redesign.", "result": "OpenCAMS successfully synchronizes and combines SUMO, CARLA, and OMNeT++ to offer a comprehensive environment for ITS research. It supports advanced simulations such as detailed sensor emulation, network-wide traffic management, and C-V2X communication, while remaining fully open-source and extensible for future needs.", "conclusion": "OpenCAMS provides the research community with an accessible, modular, and synchronized open-source platform to advance intelligent transportation systems, enabling collaborative, future-proof research in transportation safety, mobility, and cybersecurity."}}
{"id": "2507.09199", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09199", "abs": "https://arxiv.org/abs/2507.09199", "authors": ["Huihui Huang", "Ratnadira Widyasari", "Ting Zhang", "Ivana Clairine Irsan", "Jieke Shi", "Han Wei Ang", "Frank Liauw", "Eng Lieh Ouh", "Lwin Khin Shar", "Hong Jin Kang", "David Lo"], "title": "Back to the Basics: Rethinking Issue-Commit Linking with LLM-Assisted Retrieval", "comment": null, "summary": "Issue-commit linking, which connects issues with commits that fix them, is\ncrucial for software maintenance. Existing approaches have shown promise in\nautomatically recovering these links. Evaluations of these techniques assess\ntheir ability to identify genuine links from plausible but false links.\nHowever, these evaluations overlook the fact that, in reality, when a\nrepository has more commits, the presence of more plausible yet unrelated\ncommits may interfere with the tool in differentiating the correct fix commits.\nTo address this, we propose the Realistic Distribution Setting (RDS) and use it\nto construct a more realistic evaluation dataset that includes 20 open-source\nprojects. By evaluating tools on this dataset, we observe that the performance\nof the state-of-the-art deep learning-based approach drops by more than half,\nwhile the traditional Information Retrieval method, VSM, outperforms it.\n  Inspired by these observations, we propose EasyLink, which utilizes a vector\ndatabase as a modern Information Retrieval technique. To address the\nlong-standing problem of the semantic gap between issues and commits, EasyLink\nleverages a large language model to rerank the commits retrieved from the\ndatabase. Under our evaluation, EasyLink achieves an average Precision@1 of\n75.91%, improving over the state-of-the-art by over four times. Additionally,\nthis paper provides practical guidelines for advancing research in issue-commit\nlink recovery.", "AI": {"tldr": "This paper shows that current issue-commit linking tools perform poorly in realistic settings with many unrelated commits. The authors propose a new evaluation benchmark and introduce EasyLink, a tool using a vector database and a language model, which greatly outperforms existing methods.", "motivation": "Existing evaluations of issue-commit linking tools do not consider the more complex, realistic scenarios where a repository contains many unrelated plausible commits, making it harder for tools to identify the correct links. There is a need for more realistic benchmarks and improved techniques for this task.", "method": "The authors introduce the Realistic Distribution Setting (RDS) to create a more challenging evaluation dataset from 20 open-source projects. They evaluate current tools, propose EasyLink\u2014an approach that uses a vector database for information retrieval and a large language model for semantic reranking\u2014and compare its performance to existing methods.", "result": "When tools are evaluated under the RDS, deep learning-based approaches perform much worse than previously reported, with their performance dropping by more than half, and a traditional IR method (VSM) outperforming deep learning models. EasyLink, combining modern IR with a language model for reranking, significantly outperforms both, achieving a Precision@1 of 75.91%, over four times the current state-of-the-art.", "conclusion": "Realistic evaluation settings reveal substantial weaknesses in current state-of-the-art deep learning models for issue-commit linking. The EasyLink approach, which leverages information retrieval and language model reranking, offers a robust and effective solution. The paper also presents guidelines to drive further research in this field."}}
{"id": "2507.09220", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09220", "abs": "https://arxiv.org/abs/2507.09220", "authors": ["Syed Tauhid Ullah Shah", "Mohammad Hussein", "Ann Barcomb", "Mohammad Moshirpour"], "title": "Explainability as a Compliance Requirement: What Regulated Industries Need from AI Tools for Design Artifact Generation", "comment": null, "summary": "Artificial Intelligence (AI) tools for automating design artifact generation\nare increasingly used in Requirements Engineering (RE) to transform textual\nrequirements into structured diagrams and models. While these AI tools,\nparticularly those based on Natural Language Processing (NLP), promise to\nimprove efficiency, their adoption remains limited in regulated industries\nwhere transparency and traceability are essential. In this paper, we\ninvestigate the explainability gap in AI-driven design artifact generation\nthrough semi-structured interviews with ten practitioners from safety-critical\nindustries. We examine how current AI-based tools are integrated into workflows\nand the challenges arising from their lack of explainability. We also explore\nmitigation strategies, their impact on project outcomes, and features needed to\nimprove usability. Our findings reveal that non-explainable AI outputs\nnecessitate extensive manual validation, reduce stakeholder trust, struggle to\nhandle domain-specific terminology, disrupt team collaboration, and introduce\nregulatory compliance risks, often negating the anticipated efficiency\nbenefits. To address these issues, we identify key improvements, including\nsource tracing, providing clear justifications for tool-generated decisions,\nsupporting domain-specific adaptation, and enabling compliance validation. This\nstudy outlines a practical roadmap for improving the transparency, reliability,\nand applicability of AI tools in requirements engineering workflows,\nparticularly in regulated and safety-critical environments where explainability\nis crucial for adoption and certification.", "AI": {"tldr": "AI tools for generating design artifacts in requirements engineering struggle with transparency and explainability, limiting their use in regulated industries. Interviews with practitioners reveal significant challenges and the need for specific improvements to enhance trust, collaboration, and compliance. The paper provides a roadmap to make these tools more practical and adoptable in safety-critical domains.", "motivation": "AI tools are increasingly used to automate the generation of design artifacts from textual requirements in Requirements Engineering (RE). While promising increased efficiency, such tools see limited adoption in regulated industries due to the need for transparency and traceability.", "method": "The study uses semi-structured interviews with ten practitioners from safety-critical industries to investigate the explainability gap in AI-driven design artifact generation.", "result": "The lack of explainability in current AI tools causes extensive manual validation, reduced stakeholder trust, difficulties with domain-specific terminology, disrupted team collaboration, and poses regulatory compliance risks, undermining their efficiency benefits.", "conclusion": "The paper identifies critical improvements needed for AI tools in RE: source tracing, transparent decision-making justifications, support for domain-specific adaptation, and compliance validation features. These improvements are essential for increasing the adoption and reliability of AI tools in safety-critical and regulated environments."}}
{"id": "2507.09315", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09315", "abs": "https://arxiv.org/abs/2507.09315", "authors": ["Yongqian Sun", "Weihua Kuang", "Chao Shen", "Xidao Wen", "Tinghua Zheng", "Heng Liu", "Shenglin Zhang", "Bo Wu", "Dan Pei"], "title": "Enhancing Interpretability in Software Change Management with Chain-of-Thought Reasoning", "comment": "22 pages, 19 figures", "summary": "In modern online services, frequent software changes introduce significant\nrisks. To tackle this challenge, we propose SCELM (Software Change Evaluation\nand Lifecycle Management), an end-to-end automated framework for software\nchange management. SCELM aims to manage software changes efficiently and\nprecisely, significantly reducing service failures and economic losses.", "AI": {"tldr": "SCELM is an automated framework designed to efficiently manage software changes in online services, leading to fewer failures and lower costs.", "motivation": "Frequent software changes in modern online services introduce substantial risks. Traditional change management methods struggle to efficiently and accurately handle these continuous updates, leading to increased service failures and economic losses.", "method": "The proposed approach is SCELM, an end-to-end automated framework specifically designed for software change evaluation and lifecycle management. It automates and streamlines the management of software changes.", "result": "Applying SCELM results in more efficient and precise handling of software changes. This leads to a significant reduction in both service failures and economic losses.", "conclusion": "SCELM effectively addresses the risks associated with frequent software changes in online services, improving reliability and reducing negative impacts."}}
{"id": "2507.09414", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09414", "abs": "https://arxiv.org/abs/2507.09414", "authors": ["Khizra Sohail", "Atif Aftab Ahmed Jilani", "Nigar Azhar Butt"], "title": "Enhancing NeuroEvolution-Based Game Testing: A Branch Coverage Approach for Scratch Programs", "comment": null, "summary": "Automated test generation for game-like programs presents unique challenges\ndue to their non-deterministic behavior and complex control structures. The\nNEATEST framework has been used for automated testing in Scratch games,\nemploying neuroevolution-based test generation optimized for statement\ncoverage. However, statement coverage alone is often insufficient for fault\ndetection, as it does not guarantee execution of all logical branches. This\npaper introduces a branch coverage-based fitness function to enhance test\neffectiveness in automated game testing. We extend NEATEST by integrating a\nbranch fitness function that prioritizes control-dependent branches, guiding\nthe neuroevolution process to maximize branch exploration. To evaluate the\neffectiveness of this approach, empirical experiments were conducted on 25\nScratch games, comparing Neatest with Statement Coverage (NSC) against Neatest\nwith Branch Coverage (NBC). A mutation analysis was also performed to assess\nthe fault detection capabilities of both techniques. The results demonstrate\nthat NBC achieves higher branch coverage than NSC in 13 out of 25 games,\nparticularly in programs with complex conditional structures. Moreover, NBC\nachieves a lower false positive rate in mutation testing, making it a more\nreliable approach for identifying faulty behavior in game programs. These\nfindings confirm that branch coverage-based test generation improves test\ncoverage and fault detection in Scratch programs.", "AI": {"tldr": "This paper improves automated testing for Scratch games by replacing statement coverage with branch coverage in the NEATEST framework. The new approach finds more faults and covers more logical branches, especially in complex games.", "motivation": "Automated test generation for game-like programs is difficult due to non-determinism and complex control logic. Traditional statement coverage is insufficient for detecting all faults, as it may miss certain logical branches.", "method": "The authors extend the NEATEST framework\u2014originally used for automated Scratch game testing\u2014by integrating a branch coverage-based fitness function. The method prioritizes control-dependent branches to improve neuroevolution-driven test exploration. Comparative experiments on 25 Scratch games, as well as mutation analysis, assess the new approach (NBC) against the previous statement coverage method (NSC).", "result": "NBC (the new branch coverage approach) achieves higher branch coverage in 13 out of 25 games, especially in those with complex conditional logic, and produces a lower false positive rate in mutation testing than NSC.", "conclusion": "Branch coverage-based automated test generation improves the effectiveness and reliability of testing in Scratch game programs, out-performing statement coverage by covering more logical branches and reducing false positives in mutation testing."}}
{"id": "2507.09481", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09481", "abs": "https://arxiv.org/abs/2507.09481", "authors": ["Yuheng Huang", "Da Song", "Zhenlan Ji", "Shuai Wang", "Lei Ma"], "title": "Evaluating LLMs on Sequential API Call Through Automated Test Generation", "comment": null, "summary": "By integrating tools from external APIs, Large Language Models (LLMs) have\nexpanded their promising capabilities in a diverse spectrum of complex\nreal-world tasks. However, testing, evaluation, and analysis of LLM tool use\nremain in their early stages. Most existing benchmarks rely on manually\ncollected test cases, many of which cannot be automatically checked for\nsemantic correctness and instead depend on static methods such as string\nmatching. Additionally, these benchmarks often overlook the complex\ninteractions that occur between sequential API calls, which are common in\nreal-world applications. To fill the gap, in this paper, we introduce StateGen,\nan automated framework designed to generate diverse coding tasks involving\nsequential API interactions. StateGen combines state-machine-based API\nconstraint solving and validation, energy-based sampling, and control-flow\ninjection to generate executable programs. These programs are then translated\ninto human-like natural language task descriptions through a collaboration of\ntwo LLM agents. Utilizing StateGen, we construct StateEval, a benchmark\nencompassing 120 verified test cases spanning across three representative\nscenarios: Session Service, Tensor Operation, and ElevenLabs MCP. Experimental\nresults confirm that StateGen can effectively generate challenging and\nrealistic API-oriented tasks, highlighting areas for improvement in current\nLLMs incorporating APIs.", "AI": {"tldr": "The paper introduces StateGen, an automated system that creates and evaluates challenging coding tasks centered on sequential API interactions. This approach addresses limitations in current benchmarks and enables more thorough testing of LLMs with external tool use.", "motivation": "Although LLMs can integrate with external APIs to perform complex real-world tasks, the evaluation of their API tool use is still preliminary. Current benchmarks rely heavily on manual test cases and simplistic evaluation methods, often ignoring the nuanced interactions between sequential API calls common in actual applications.", "method": "The paper introduces StateGen, an automated framework that generates diverse coding tasks involving sequential API interactions. StateGen leverages state-machine-based API constraint solving, validation, energy-based sampling, and control-flow injection to create executable programs, which are then turned into natural language task descriptions by collaborating LLM agents.", "result": "Using StateGen, the authors build StateEval, a benchmark containing 120 verified test cases across scenarios like Session Service, Tensor Operation, and ElevenLabs MCP. Experiments show that StateGen can produce realistic, challenging API-oriented tasks, and reveal improvement opportunities for LLMs using APIs.", "conclusion": "StateGen provides a robust and automated method to generate and evaluate API-related tasks for LLMs, filling a significant gap in current benchmark coverage by focusing on complex, sequential API interactions and automatic semantic evaluation."}}
{"id": "2507.09490", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09490", "abs": "https://arxiv.org/abs/2507.09490", "authors": ["Yan Zhao", "Chiwei Tang"], "title": "Towards LLM-Based Automatic Playtest", "comment": null, "summary": "Playtesting is the process in which people play a video game for testing. It\nis critical for the quality assurance of gaming software. Manual playtesting is\ntime-consuming and expensive. However, automating this process is challenging,\nas playtesting typically requires domain knowledge and problem-solving skills\nthat most conventional testing tools lack. Recent advancements in artificial\nintelligence (AI) have opened up new possibilities for applying Large Language\nModels (LLMs) to playtesting. However, significant challenges remain: current\nLLMs cannot visually perceive game environments, and most existing research\nfocuses on text-based games or games with robust APIs. Many non-text games lack\nAPIs to provide textual descriptions of game states, making it almost\nimpossible to naively apply LLMs for playtesting. This paper introduces Lap,\nour novel approach to LLM-based Automatic Playtesting, which uses ChatGPT to\ntest match-3 games, a category of games where players match three or more\nidentical tiles in a row or column to earn points. Lap encompasses three key\nphases: processing of game environments, prompting-based action generation, and\naction execution. Given a match-3 game, Lap takes a snapshot of the game board\nand converts it to a numeric matrix. It then prompts the ChatGPT-O1-mini API to\nsuggest moves based on that matrix and tentatively applies the suggested moves\nto earn points and trigger changes in the game board. It repeats the\nabove-mentioned three steps iteratively until timeout. For evaluation, we\nconducted a case study using Lap on an open-source match-3 game, CasseBonbons,\nand empirically compared it with three existing tools. Our results are\npromising: Lap outperformed existing tools by achieving higher code coverage\nand triggering more program crashes. This research sheds light on the future of\nautomatic testing and LLM applications.", "AI": {"tldr": "The paper presents Lap, a method that uses ChatGPT for automatic playtesting of match-3 games without APIs. By converting game boards to numeric matrices and prompting the LLM, Lap surpasses existing tools in code coverage and crash detection, showing that LLMs can effectively automate game testing processes even for non-text games.", "motivation": "Manual playtesting of video games is resource-intensive and current automated tools lack the necessary domain knowledge and problem-solving skills. There is a need to enhance automatic playtesting, particularly for non-text-based games that lack APIs or direct textual representations of game states.", "method": "The paper introduces 'Lap', an approach that uses Large Language Models (specifically ChatGPT) for automatic playtesting of match-3 games. Lap processes game environments by converting game board snapshots into numeric matrices, uses LLM prompting for move generation, and iteratively performs actions within the game.", "result": "Lap was evaluated on the open-source match-3 game CasseBonbons. It was compared with three existing tools, showing that Lap achieved higher code coverage and triggered more program crashes, indicating more thorough and effective testing.", "conclusion": "Lap demonstrates the feasibility and effectiveness of employing LLMs (ChatGPT) for automated playtesting in non-text-based games without requiring robust APIs. This approach can enhance quality assurance in games and highlights new opportunities for LLM applications in software testing."}}
{"id": "2507.09529", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09529", "abs": "https://arxiv.org/abs/2507.09529", "authors": ["Yunqian Wang", "Xiaohong Li", "Yao Zhang", "Yuekang Li", "Zhiping Zhou", "Ruitao Feng"], "title": "It Only Gets Worse: Revisiting DL-Based Vulnerability Detectors from a Practical Perspective", "comment": null, "summary": "With the growing threat of software vulnerabilities, deep learning (DL)-based\ndetectors have gained popularity for vulnerability detection. However, doubts\nremain regarding their consistency within declared CWE ranges, real-world\neffectiveness, and applicability across scenarios. These issues may lead to\nunreliable detection, high false positives/negatives, and poor adaptability to\nemerging vulnerabilities. A comprehensive analysis is needed to uncover\ncritical factors affecting detection and guide improvements in model design and\ndeployment. In this paper, we present VulTegra, a novel evaluation framework\nthat conducts a multidimensional comparison of scratch-trained and\npre-trained-based DL models for vulnerability detection. VulTegra reveals that\nstate-of-the-art (SOTA) detectors still suffer from low consistency, limited\nreal-world capabilities, and scalability challenges. Contrary to common belief,\npre-trained models are not consistently better than scratch-trained models but\nexhibit distinct strengths in specific contexts.Importantly, our study exposes\nthe limitations of relying solely on CWE-based classification and identifies\nkey factors that significantly affect model performance. Experimental results\nshow that adjusting just one such factor consistently improves recall across\nall seven evaluated detectors, with six also achieving better F1 scores. Our\nfindings provide deeper insights into model behavior and emphasize the need to\nconsider both vulnerability types and inherent code features for effective\ndetection.", "AI": {"tldr": "DL-based vulnerability detectors are not as reliable or adaptable as assumed. Performance and consistency issues remain, and pre-trained models are not always better. VulTegra framework reveals that minor adjustments can boost detection performance, and effective vulnerability detection requires consideration beyond CWE labels.", "motivation": "Deep learning-based detectors for software vulnerabilities are popular, but there are doubts about their reliability, consistency, and real-world applicability. Current models often produce high false positive/negative rates and may not adapt well to new vulnerabilities, necessitating a rigorous evaluation to identify improvement areas.", "method": "The authors present VulTegra, a multidimensional evaluation framework that systematically compares both scratch-trained and pre-trained deep learning models for vulnerability detection. The analysis is done across various scenarios, with a focus on factors influencing detection performance and model consistency.", "result": "VulTegra reveals that even state-of-the-art detectors have low consistency, are limited in real-world scenarios, and face scalability challenges. Pre-trained models are not always superior to scratch-trained ones; each has unique strengths in specific situations. The study identifies critical factors affecting performance, demonstrating that tuning even a single factor leads to recall improvements across all tested models, and better F1 scores for most.", "conclusion": "The paper underscores that existing DL-based vulnerability detectors have notable limitations, especially regarding consistency and adaptability. Improvements depend on understanding the effect of code features and vulnerability types, rather than reliance on broad CWE classifications. There is no universally superior model, and tailored improvements based on real-world use cases are essential."}}
{"id": "2507.09583", "categories": ["cs.SE", "cs.AI", "I.2.7; J.1"], "pdf": "https://arxiv.org/pdf/2507.09583", "abs": "https://arxiv.org/abs/2507.09583", "authors": ["Taniv Ashraf"], "title": "A Serverless Architecture for Real-Time Stock Analysis using Large Language Models: An Iterative Development and Debugging Case Study", "comment": "6 pages. The live application can be viewed at\n  https://codepen.io/tanivashraf/pen/GgpgxBY and the source code is available\n  at https://github.com/TanivAshraf/ai-stock-analyzer", "summary": "The advent of powerful, accessible Large Language Models (LLMs) like Google's\nGemini presents new opportunities for democratizing financial data analysis.\nThis paper documents the design, implementation, and iterative debugging of a\nnovel, serverless system for real-time stock analysis. The system leverages the\nGemini API for qualitative assessment, automates data ingestion and processing\nvia GitHub Actions, and presents the findings through a decoupled, static\nfrontend. We detail the architectural evolution of the system, from initial\nconcepts to a robust, event-driven pipeline, highlighting the practical\nchallenges encountered during deployment. A significant portion of this paper\nis dedicated to a case study on the debugging process, covering common software\nerrors, platform-specific permission issues, and rare, environment-level\nplatform bugs. The final architecture operates at a near-zero cost,\ndemonstrating a viable model for individuals to build sophisticated AI-powered\nfinancial tools. The operational application is publicly accessible, and the\ncomplete source code is available for review. We conclude by discussing the\nrole of LLMs in financial analysis, the importance of robust debugging\nmethodologies, and the emerging paradigm of human-AI collaboration in software\ndevelopment.", "AI": {"tldr": "This paper presents a nearly zero-cost, AI-driven serverless system for real-time stock analysis using Google's Gemini LLM. It covers the system's design, debugging, and deployment, demonstrating how powerful and accessible tools can be developed for financial analysis by individuals.", "motivation": "The motivation is to make financial data analysis more accessible and democratized by leveraging the power of new Large Language Models (LLMs) such as Google's Gemini. The authors aim to show that individuals can build advanced, low-cost, AI-powered financial tools.", "method": "The authors designed and implemented a novel, serverless, real-time stock analysis system. This system uses Gemini's API for qualitative financial analysis, automates data ingestion and processing through GitHub Actions, and presents the results via a decoupled static frontend. The paper includes a detailed case study of the debugging process, addressing both common and rare software and platform issues.", "result": "The resulting system is robust, event-driven, serverless, and operates at nearly zero cost. It is publicly available for use, and the complete source code is accessible. The architecture supports real-time AI-powered financial analysis.", "conclusion": "The paper concludes that LLMs can play a significant role in democratizing financial analysis by enabling the creation of sophisticated, low-cost tools by individuals. Importantly, it highlights the value of robust debugging methodologies and human-AI collaboration in software development."}}
{"id": "2507.09594", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09594", "abs": "https://arxiv.org/abs/2507.09594", "authors": ["Aydin Homay"], "title": "How to Define Design in Industrial Control and Automation Software", "comment": null, "summary": "Design is a fundamental aspect of engineering, enabling the creation of\nproducts, systems, and organizations to meet societal and/or business needs.\nHowever, the absence of a scientific foundation in design often results in\nsubjective decision-making, reducing both efficiency and innovation. This\nchallenge is particularly evident in the software industry and, by extension,\nin the domain of industrial control and automation systems (iCAS).\n  In this study, first we review the existing design definitions within the\nsoftware industry, challenge prevailing misconceptions about design, review\ndesign definition in the field of design theory and address key questions such\nas: When does design begin? How can design be defined scientifically? What\nconstitutes good design? and the difference between design and design language\nby relying on advancements in the field of design theory. We also evaluate the\ndistinction between ad-hoc and systematic design approaches, and present\narguments on how to balance complementary operational concerns while resolving\nconflicting evolutionary concerns.", "AI": {"tldr": "The paper critiques current, often subjective, design practices in software and automation. By applying design theory, it offers clearer definitions, distinctions, and approaches for more scientific, systematic, and innovative design.", "motivation": "The motivation comes from the lack of a scientific foundation in design, which leads to subjective decisions, hampering efficiency and innovation\u2014especially notable in the software and industrial control/automation sectors.", "method": "The paper conducts a review of existing design definitions in the software industry, critiques misconceptions, explores scientific definitions inspired by design theory, and addresses key questions like the onset, nature, and quality of design. It also discusses the distinction between ad-hoc and systematic approaches and how to balance operational and evolutionary concerns.", "result": "The study clarifies when design begins, defines design scientifically, distinguishes between good and poor design, differentiates design from design language, and evaluates the merits of ad-hoc vs. systematic design in balancing operational and evolutionary aspects.", "conclusion": "A more scientific and systematic approach to design, grounded in advances from design theory, can help overcome subjectivity, improve efficiency and innovation, and provide better balance in addressing operational and evolutionary requirements in software and automation systems."}}
{"id": "2507.09596", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09596", "abs": "https://arxiv.org/abs/2507.09596", "authors": ["Aydin Homay"], "title": "The Mythical Good Software", "comment": null, "summary": "Good software has high cohesion and low coupling is clumsy, obscure, and in\nsome certain cases could be actually a harmful state of being. It is clumsy\nbecause there is no perfect correlation between higher cohesiveness and optimum\ndesign, and it is obscure because it conveys the message that coupling and\ncohesion are two distinct design principles, while there are in principle the\nsame design approaches, and only the time and space differ between them, and it\ncould also be a harmful state of being because we should not always aim for\nhigher cohesiveness without considering its cost.\n  In the course of this study, we aim to elucidate for the readers the meaning\nand underlying philosophy of the aforementioned paragraph.", "AI": {"tldr": "The paper challenges the traditional view of cohesion and coupling as distinct software design goals, arguing that they are fundamentally related and that an uncritical pursuit of higher cohesion may be counterproductive.", "motivation": "To reevaluate the traditional software engineering principles of cohesion and coupling, questioning their distinctness and practical application.", "method": "The authors provide a theoretical analysis and philosophical discussion on the relationship between cohesion and coupling in software design.", "result": "The study suggests that considering cohesion and coupling as entirely separate principles is misleading, as they are fundamentally related. Additionally, the pursuit of higher cohesion should not be done blindly, as it may come with trade-offs or costs.", "conclusion": "Designers should not strictly pursue high cohesion and low coupling as separate goals, but rather understand the nuanced, interconnected relationship between them and make balanced design decisions."}}
{"id": "2507.09599", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09599", "abs": "https://arxiv.org/abs/2507.09599", "authors": ["Aydin Homay"], "title": "Complexity and Coupling: A Functional Domain Approach", "comment": null, "summary": "This paper provides a precise and scientific definition of complexity and\ncoupling, grounded in the functional domain, particularly within industrial\ncontrol and automation systems (iCAS). We highlight the widespread ambiguity in\ndefining complexity and coupling, emphasizing that many existing definitions\nrooted in physical attributes lead to confusion and inconsistencies.\nFurthermore, we re-exhibit why coupled design inherently increases complexity\nand how potentially this complexity could be reduced. Drawing on examples from\nvarious disciplines, such as software engineering, industrial automation, and\nmechanical design, we demonstrate that complexity does not necessarily\ncorrelate with system size or the number of components, and coupling, unlike\ncommon belief in software engineering, actually does not occur in the physical\ndomain but in the functional domain. We conclude that effective design\nnecessitates addressing coupling and complexity within the functional domain.", "AI": {"tldr": "Traditional definitions of complexity and coupling in engineering are unclear and inconsistent. This paper proposes clear, scientifically grounded definitions focused on the functional domain, showing that complexity and coupling are functionally, not physically, based. Effective design depends on addressing these issues functionally.", "motivation": "There is widespread ambiguity and inconsistency in the definitions of complexity and coupling, especially in industrial control and automation systems (iCAS). Existing definitions, often based on physical attributes, contribute to confusion. The paper seeks to provide clarity and scientific grounding for these concepts.", "method": "The authors offer a new precise definition of complexity and coupling, specifically tailored to the functional domain. They illustrate their arguments using examples from various fields such as software engineering, industrial automation, and mechanical design. The analysis highlights the disconnect between commonly held beliefs and scientific reality.", "result": "The paper shows that complexity is not directly related to system size or the number of components, and that coupling should be understood within the functional, not physical, domain. Coupled design increases complexity, but this complexity can potentially be managed and reduced by focusing on functional aspects.", "conclusion": "Defining and addressing complexity and coupling in the functional domain is essential for effective design, especially in industrial control and automation systems."}}
{"id": "2507.09637", "categories": ["cs.SE", "cs.HC", "D.2.0; D.2.3; K.4.3"], "pdf": "https://arxiv.org/pdf/2507.09637", "abs": "https://arxiv.org/abs/2507.09637", "authors": ["Lo Gullstrand Heander", "Emma S\u00f6derberg", "Christofer Rydenf\u00e4lt"], "title": "Code Review as Decision-Making -- Building a Cognitive Model from the Questions Asked During Code Review", "comment": "39 pages, 14 figures Submitted to Empirical Software Engineering,\n  Springer Nature", "summary": "Code review is a well-established and valued practice in the software\nengineering community contributing to both code quality and interpersonal\nbenefits. However, there are challenges in both tools and processes that give\nrise to misalignments and frustrations. Recent research seeks to address this\nby automating code review entirely, but we believe that this risks losing the\nmajority of the interpersonal benefits such as knowledge transfer and shared\nownership.\n  We believe that by better understanding the cognitive processes involved in\ncode review, it would be possible to improve tool support, with out without AI,\nand make code review both more efficient, more enjoyable, while increasing or\nmaintaining all of its benefits. In this paper, we conduct an ethnographic\nthink-aloud study involving 10 participants and 34 code reviews. We build a\ncognitive model of code review bottom up through thematic, statistical,\ntemporal, and sequential analysis of the transcribed material. Through the\ndata, the similarities between the cognitive process in code review and\ndecision-making processes, especially recognition-primed decision-making,\nbecome apparent.\n  The result is the Code Review as Decision-Making (CRDM) model that shows how\nthe developers move through two phases during the code review; first an\norientation phase to establish context and rationale and then an analytical\nphase to understand, assess, and plan the rest of the review. Throughout the\nprocess several decisions must be taken, on writing comments, finding more\ninformation, voting, running the code locally, verifying continuous integration\nresults, etc.\n  Analysis software and process-coded data publicly available at:\nhttps://doi.org/10.5281/zenodo.15758266", "AI": {"tldr": "This paper presents a cognitive model of code review constructed from an ethnographic study. It shows that code review involves decision-making similar to recognition-primed decision-making. Understanding this can inspire better tool support to improve efficiency and enjoyment while preserving essential social benefits.", "motivation": "Although code review is crucial for code quality and team dynamics, existing tools and processes present challenges that cause inefficiencies and frustrations. Full automation of code review risks losing significant interpersonal benefits, such as knowledge sharing and collective code ownership. The authors aim to better understand the underlying cognitive processes to guide improved tool development and preserve these benefits.", "method": "The study uses an ethnographic think-aloud approach, involving 10 participants and 34 code reviews. The researchers conduct thematic, statistical, temporal, and sequential analyses of transcribed review sessions to construct a cognitive model of code review.", "result": "The authors create the Code Review as Decision-Making (CRDM) model. It describes code review as a process with two main phases: orientation (establishing context and rationale) and analytical (deep analysis and planning). Throughout, reviewers make various decisions, such as when to comment, seek more information, or verify results. The model highlights similarities with recognition-primed decision-making processes.", "conclusion": "By understanding code review as a decision-making process, tool and process improvements can be better targeted to support the actual reviewer workflows and cognitive needs, potentially increasing efficiency, enjoyment, and preservation of interpersonal benefits. Improved tools\u2014whether AI-driven or not\u2014should respect and facilitate these phases of cognitive work rather than automate away vital human interactions."}}
{"id": "2507.09665", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09665", "abs": "https://arxiv.org/abs/2507.09665", "authors": ["Saima Afrin", "Bowen Xu", "Antonio Mastropaolo"], "title": "Is Quantization a Deal-breaker? Empirical Insights from Large Code Models", "comment": null, "summary": "The growing scale of large language models (LLMs) not only demands extensive\ncomputational resources but also raises environmental concerns due to their\nincreasing carbon footprint. Model quantization emerges as an effective\napproach that can reduce the resource demands of LLMs by decreasing parameter\nprecision without substantially affecting performance (e.g., 16 bit to 4 bit).\nWhile recent studies have established quantization as a promising approach for\noptimizing large code models (LCMs), a specialized subset of LLMs tailored for\nautomated software engineering, their findings offer only limited insights into\nits practical implications. Specifically, current investigations focus only on\nthe functional correctness of the code generated by quantized models,\nneglecting how quantization impacts critical aspects of code quality such as\nreliability, maintainability, and security. To bridge this gap, our study\ninvestigates the effects of quantization on the qualitative aspects of\nautomatically generated code. We apply Activation-aware Weight Quantization\n(AWQ) to two widely used code models, CodeLlama and DeepSeekCoder, to generate\nJava and Python code. Using state-of-the-art static analysis tools, we evaluate\nsoftware quality metrics and static features including cyclomatic complexity,\ncognitive complexity, and lines of code. Our findings reveal that quantization\nis a robust technique that not only preserves functional correctness, but also\nretains key qualitative code attributes sought after by developers, such as\nmaintainability and structural simplicity.", "AI": {"tldr": "Applying activation-aware quantization to code LLMs reduces their resource footprint without harming their ability to generate functionally correct and high-quality code in terms of maintainability and simplicity.", "motivation": "Large language models (LLMs) require extensive computational resources and contribute to significant environmental impact due to their large carbon footprint. Model quantization lowers the precision of model parameters, reducing resource needs. Previous work mainly addressed functional correctness of quantized models, neglecting critical code quality aspects.", "method": "The authors applied Activation-aware Weight Quantization (AWQ) to two code-specific LLMs, CodeLlama and DeepSeekCoder. They used these quantized models to generate Java and Python code, and assessed the output using static analysis tools to evaluate quality metrics like cyclomatic complexity, cognitive complexity, and lines of code.", "result": "Quantization did not only preserve functional correctness but also maintained important code quality attributes, such as maintainability and structural simplicity, in the generated outputs.", "conclusion": "Model quantization, specifically AWQ, proves effective for code LLMs by reducing resource demands and carbon footprint while retaining both functional and qualitative aspects of code quality."}}
{"id": "2507.09682", "categories": ["cs.SE", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.09682", "abs": "https://arxiv.org/abs/2507.09682", "authors": ["Laura Baird", "Armin Moin"], "title": "OrQstrator: An AI-Powered Framework for Advanced Quantum Circuit Optimization", "comment": "IEEE International Conference on Quantum Computing and Engineering\n  (QCE) 2025 - Extended Abstract", "summary": "We propose a novel approach, OrQstrator, which is a modular framework for\nconducting quantum circuit optimization in the Noisy Intermediate-Scale Quantum\n(NISQ) era. Our framework is powered by Deep Reinforcement Learning (DRL). Our\norchestration engine intelligently selects among three complementary circuit\noptimizers: A DRL-based circuit rewriter trained to reduce depth and gate count\nvia learned rewrite sequences; a domain-specific optimizer that performs\nefficient local gate resynthesis and numeric optimization; a parameterized\ncircuit instantiator that improves compilation by optimizing template circuits\nduring gate set translation. These modules are coordinated by a central\norchestration engine that learns coordination policies based on circuit\nstructure, hardware constraints, and backend-aware performance features such as\ngate count, depth, and expected fidelity. The system outputs an optimized\ncircuit for hardware-aware transpilation and execution, leveraging techniques\nfrom an existing state-of-the-art approach, called the NISQ Analyzer, to adapt\nto backend constraints.", "AI": {"tldr": "OrQstrator is a new Deep RL-powered system that smartly combines multiple quantum circuit optimization techniques to produce hardware-aware, efficient quantum circuits for today's constrained quantum computers.", "motivation": "Quantum circuit optimization is crucial in the NISQ era, where hardware is limited, noisy, and resources are constrained. There is a need for smarter, adaptive, and hardware-aware optimization strategies to maximize circuit performance on current quantum devices.", "method": "The authors introduce OrQstrator, a modular quantum circuit optimization framework powered by Deep Reinforcement Learning (DRL). It uses an orchestration engine to coordinate three optimization modules: a DRL-based circuit rewriter, a domain-specific local optimizer, and a parameterized template circuit instantiator. The orchestration engine learns which optimizer to apply based on circuit and hardware features.", "result": "OrQstrator produces quantum circuits that are optimized for hardware-aware criteria such as reduced gate count, lower depth, and improved expected fidelity. The framework adapts to backend constraints using insights from the existing NISQ Analyzer tool.", "conclusion": "OrQstrator demonstrates a promising and modular approach to quantum circuit optimization in the NISQ era by combining machine learning and domain-specific strategies, efficiently adapting to hardware limitations for practical implementation."}}
{"id": "2507.09790", "categories": ["cs.SE", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.09790", "abs": "https://arxiv.org/abs/2507.09790", "authors": ["Helge Spieker", "Th\u00e9o Matricon", "Nassim Belmecheri", "J\u00f8rn Eirik Betten", "Gauthier Le Bartz Lyan", "Heraldo Borges", "Quentin Mazouni", "Dennis Gross", "Arnaud Gotlieb", "Mathieu Acher"], "title": "Prompting for Performance: Exploring LLMs for Configuring Software", "comment": null, "summary": "Software systems usually provide numerous configuration options that can\naffect performance metrics such as execution time, memory usage, binary size,\nor bitrate. On the one hand, making informed decisions is challenging and\nrequires domain expertise in options and their combinations. On the other hand,\nmachine learning techniques can search vast configuration spaces, but with a\nhigh computational cost, since concrete executions of numerous configurations\nare required. In this exploratory study, we investigate whether large language\nmodels (LLMs) can assist in performance-oriented software configuration through\nprompts. We evaluate several LLMs on tasks including identifying relevant\noptions, ranking configurations, and recommending performant configurations\nacross various configurable systems, such as compilers, video encoders, and SAT\nsolvers. Our preliminary results reveal both positive abilities and notable\nlimitations: depending on the task and systems, LLMs can well align with expert\nknowledge, whereas hallucinations or superficial reasoning can emerge in other\ncases. These findings represent a first step toward systematic evaluations and\nthe design of LLM-based solutions to assist with software configuration.", "AI": {"tldr": "The paper explores using large language models (LLMs) to help configure software for better performance. LLMs show potential in some cases but also have significant limitations, such as making mistakes or using superficial logic. This work is an initial step toward harnessing LLMs for reliable software configuration advice.", "motivation": "Configuring software to achieve optimal performance is difficult due to a large number of configuration options and their interactions; traditional approaches require substantial domain expertise or high computational effort, making automation desirable.", "method": "The authors conducted an exploratory study using prompts to evaluate the ability of several large language models (LLMs) to aid in performance-oriented software configuration tasks, such as identifying, ranking, and recommending configuration options across different systems.", "result": "Preliminary results showed that LLMs sometimes align well with expert assessments but may also produce errors, such as hallucinations or shallow reasoning, highlighting both their utility and limitations for these tasks.", "conclusion": "LLMs offer promising but currently imperfect assistance for configuring software systems to optimize performance; further systematic evaluation and tailored designs are needed to make LLM-based support reliable."}}
{"id": "2507.09820", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.09820", "abs": "https://arxiv.org/abs/2507.09820", "authors": ["Jia Yi Goh", "Shaun Khoo", "Nyx Iskandar", "Gabriel Chua", "Leanne Tan", "Jessica Foo"], "title": "Measuring What Matters: A Framework for Evaluating Safety Risks in Real-World LLM Applications", "comment": null, "summary": "Most safety testing efforts for large language models (LLMs) today focus on\nevaluating foundation models. However, there is a growing need to evaluate\nsafety at the application level, as components such as system prompts,\nretrieval pipelines, and guardrails introduce additional factors that\nsignificantly influence the overall safety of LLM applications. In this paper,\nwe introduce a practical framework for evaluating application-level safety in\nLLM systems, validated through real-world deployment across multiple use cases\nwithin our organization. The framework consists of two parts: (1) principles\nfor developing customized safety risk taxonomies, and (2) practices for\nevaluating safety risks in LLM applications. We illustrate how the proposed\nframework was applied in our internal pilot, providing a reference point for\norganizations seeking to scale their safety testing efforts. This work aims to\nbridge the gap between theoretical concepts in AI safety and the operational\nrealities of safeguarding LLM applications in practice, offering actionable\nguidance for safe and scalable deployment.", "AI": {"tldr": "This paper presents a practical, tested framework for evaluating and managing application-level safety in LLM systems, moving beyond model-level safety to provide actionable guidance for organizations deploying LLM applications.", "motivation": "Current safety evaluations of large language models mostly focus on the models themselves, ignoring additional risks introduced at the application level (e.g., through system prompts, retrieval mechanisms, or guardrails). There is a need for frameworks that address application-specific safety concerns as these components can significantly impact overall LLM safety.", "method": "The authors propose a practical evaluation framework for LLM application-level safety, which includes: (1) a method for developing customized safety risk taxonomies tailored to particular applications, and (2) concrete practices for evaluating safety risks in deployed LLM systems. The framework's effectiveness is demonstrated via real-world deployment across several organizational use cases.", "result": "The framework was successfully validated through internal pilots within the organization, demonstrating its utility in helping organizations systematically evaluate and manage application-level safety risks in LLM-based systems.", "conclusion": "The proposed framework addresses the gap between theoretical AI safety concepts and real-world deployment needs, offering actionable procedures and guiding principles for safe and scalable LLM application deployment."}}
{"id": "2507.09866", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09866", "abs": "https://arxiv.org/abs/2507.09866", "authors": ["Wei Zhang", "Jian Yang", "Jiaxi Yang", "Ya Wang", "Zhoujun Li", "Zeyu Cui", "Binyuan Hui", "Junyang Lin"], "title": "Turning the Tide: Repository-based Code Reflection", "comment": null, "summary": "Code large language models (LLMs) enhance programming by understanding and\ngenerating code across languages, offering intelligent feedback, bug detection,\nand code updates through reflection, improving development efficiency and\naccessibility. While benchmarks (e.g. HumanEval/LiveCodeBench) evaluate code\ngeneration and real-world relevance, previous works ignore the scenario of\nmodifying code in repositories. Considering challenges remaining in improving\nreflection capabilities and avoiding data contamination in dynamic benchmarks,\nwe introduce LiveRepoReflection, a challenging benchmark for evaluating code\nunderstanding and generation in multi-file repository contexts, featuring 1,888\nrigorously filtered test cases across $6$ programming languages to ensure\ndiversity, correctness, and high difficulty. Further, we create\nRepoReflection-Instruct, a large-scale, quality-filtered instruction-tuning\ndataset derived from diverse sources, used to train RepoReflectionCoder through\na two-turn dialogue process involving code generation and error-driven repair.\nThe leaderboard evaluates over 40 LLMs to reflect the model performance of\nrepository-based code reflection.", "AI": {"tldr": "This paper introduces a new, realistic benchmark and dataset for evaluating code LLMs in real-world repository modification contexts, advancing the measurement of models' code reflection and generation abilities, with extensive experimental comparison across more than 40 models.", "motivation": "Current code LLM benchmarks often overlook the practical scenario of modifying code within repositories, focusing instead on code generation and limited practical application. There are also concerns regarding reflection capabilities and data leaks in dynamic benchmarks.", "method": "The authors introduce 'LiveRepoReflection,' a new benchmark containing 1,888 rigorously selected test cases across six programming languages, specifically targeting code understanding and generation within multi-file repository settings. They also create 'RepoReflection-Instruct,' a large-scale, instruction-tuning dataset for model training, and train 'RepoReflectionCoder' using a two-turn dialogue for code generation and error repair. Benchmarking involves over 40 LLMs.", "result": "LiveRepoReflection provides a challenging, diverse, and high-quality benchmark for testing code LLMs in repository modification tasks. RepoReflectionCoder, trained on the new instruction set, participates in evaluations, presenting comparative results among 40+ LLMs. The approach highlights the varied capabilities of different models in repository-based code reflection.", "conclusion": "The study addresses a significant gap in current LLM benchmarking by proposing a realistic, repository-based evaluation for code reflection tasks, offering a new benchmark and dataset for more relevant assessment and LLM development."}}
{"id": "2507.09892", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2507.09892", "abs": "https://arxiv.org/abs/2507.09892", "authors": ["Zimu Chen", "Di Wang"], "title": "PathFuzzing: Worst Case Analysis by Fuzzing Symbolic-Execution Paths", "comment": "10 pages, 1 figure", "summary": "Estimating worst-case resource consumption is a critical task in software\ndevelopment. The worst-case analysis (WCA) problem is an optimization-based\nabstraction of this task. Fuzzing and symbolic execution are widely used\ntechniques for addressing the WCA problem. However, improving code coverage in\nfuzzing or managing path explosion in symbolic execution within the context of\nWCA poses significant challenges. In this paper, we propose PathFuzzing, aiming\nto combine the strengths of both techniques to design a WCA method. The key\nidea is to transform a program into a symbolic one that takes an execution path\n(encoded as a binary string) and interprets the bits as branch decisions.\nPathFuzzing then applies evolutionary fuzzing techniques to the transformed\nprogram to search for binary strings that represent satisfiable path conditions\nand lead to high resource consumption. We evaluate the performance of\nPathFuzzing experimentally on a benchmark suite that consists of prior work's\nbenchmarks and some added by us. Results show that PathFuzzing generally\noutperforms a fuzzing and a symbolic-execution baseline.", "AI": {"tldr": "PathFuzzing is a novel method for worst-case analysis in software, leveraging both fuzzing and symbolic execution. It shows better performance over traditional approaches in experimental benchmarks.", "motivation": "Worst-case resource consumption estimation is essential in software development to ensure reliability and efficiency. Existing approaches like fuzzing and symbolic execution have limitations in terms of code coverage and path explosion, respectively, making worst-case analysis (WCA) challenging.", "method": "The authors introduce 'PathFuzzing,' which transforms the original program into a symbolic version that interprets execution paths as binary strings. It applies evolutionary fuzzing methods to search for binary strings (representing branch decisions) that correspond to satisfiable paths with high resource consumption.", "result": "Experimental results on a benchmark suite indicate that PathFuzzing generally outperforms standard fuzzing and symbolic execution baselines in identifying worst-case resource consuming paths.", "conclusion": "PathFuzzing effectively addresses the WCA problem by combining the strengths of fuzzing and symbolic execution, resulting in superior identification of worst-case resource consumption scenarios."}}
{"id": "2507.09907", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09907", "abs": "https://arxiv.org/abs/2507.09907", "authors": ["Thomas Hansper", "Kevin Phong Pham", "Michael Neumann"], "title": "Modelling Interrelations Between Agile Practices: The Agile Map", "comment": null, "summary": "Agile methods are defined through guidelines comprising various practices\nintended to enable agile ways of working. These guidelines further comprise a\nspecific set of agile practices aiming to enable teams for an agile way of\nworking. However, due to its wide-spread use in practice we know that agile\npractices are adopted and tailored intensively, which lead to a high variety of\nagile practices in terms of their level of detail. Problem: A high variety of\nagile practices can be challenging as we do not know how different agile\npractices are interrelated with each other. To be more precise, tailoring and\nadopting agile practices may lead to the challenge, that the combinatorial use\nof several agile practices can only be successful to a limited extent, as\npractices support or even require each other for a effective use in practice.\nObjective: Our study aims to provide an enabler for this problem. We want to\nidentify interrelations between agile practices and describe them in a\nsystematic manner. Contribution: The core contribution of this paper is the\nAgile Map, a theoretical model describing relations between agile practices\nfollowing a systematic approach aiming to provide an overview of coherences\nbetween agile practices. The model aims to support practitioners in selecting\nand combining agile practices in a meaningful way.", "AI": {"tldr": "The paper addresses the complexity of combining agile practices by creating the Agile Map, a model that shows how these practices are interrelated, helping teams select and combine them more effectively.", "motivation": "There is a wide variety in how agile practices are adopted and tailored in practice, making it challenging to understand how different agile practices are interrelated. This can limit the successful combination and use of such practices, as some depend on or support others.", "method": "The study systematically investigates interrelations between agile practices. The authors develop a model\u2014called the Agile Map\u2014that describes these interrelations based on a systematic approach.", "result": "The result is the development of the Agile Map: a theoretical model that systematically describes relationships between various agile practices.", "conclusion": "The Agile Map provides an overview of how agile practices are related, supporting practitioners in meaningful selection and combination of agile practices."}}
{"id": "2507.09911", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09911", "abs": "https://arxiv.org/abs/2507.09911", "authors": ["Marvin Auf der Landwehr", "Julia Topp", "Michael Neumann"], "title": "When Less is More: A systematic review of four-day workweek conceptualizations and their effects on organizational performance", "comment": null, "summary": "Context: Agile IT organizations, which are characterized by self-organization\nand collaborative social interactions, require motivating, efficient and\nflexible work environments to maximize value creation. Compressed work\nschedules such as the four-day workweek have evolved into multiple facets over\nthe last decades and are associated with various benefits for organizations and\ntheir employees. Objective: Our objective in this study is to deepen our\ncomprehension of the impact of compressed work schedules on the operational\nefficacy of IT enterprises, while concurrently developing a comprehensive\nframework delineating the intricacies of compressed work schedules.Method: We\nconducted a systematic review of available conceptualizations related to\nfour-day workweek schedules and elaborate on their organizational and social\neffects. To cover scientific and practice-oriented literature, our review\ncombined a systematic literature review and a web content analysis. Results:\nBased on the generated insights, we derive a meta-framework that matches\nconceptualizations and effects, finally guiding the adoption of compressed work\nschedules based on individual managerial prerequisites and circumstances.", "AI": {"tldr": "This paper systematically reviews literature on four-day workweeks in IT, develops a meta-framework relating various compressed schedule models to their organizational/social effects, and offers practical guidance for adoption tailored to managerial needs.", "motivation": "Agile IT organizations need work environments that are motivating, efficient, and flexible to optimize value creation. There is growing interest in compressed work schedules (like the four-day workweek), which are believed to offer benefits for both organizations and employees.", "method": "A systematic review was conducted of both academic and practice-oriented literature. The study combined a systematic literature review with a web content analysis to examine four-day workweek schedules and their effects on organizations and employees.", "result": "The study generated a meta-framework that links different types of compressed work schedules with their organizational and social effects. This framework can be used to guide managers in adopting compressed work schedules according to their specific needs and circumstances.", "conclusion": "Compressed work schedules, such as the four-day workweek, have meaningful impacts on IT organizations. The meta-framework developed in this study can help decision-makers evaluate and implement these schedules effectively, considering both organizational goals and employee well-being."}}
{"id": "2507.10054", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10054", "abs": "https://arxiv.org/abs/2507.10054", "authors": ["Emir Bosnak", "Sahand Moslemi", "Mayasah Lami", "Anil Koyuncu"], "title": "Explicit Vulnerability Generation with LLMs: An Investigation Beyond Adversarial Attacks", "comment": "Accepted to ICSME 2025", "summary": "Large Language Models (LLMs) are increasingly used as code assistants, yet\ntheir behavior when explicitly asked to generate insecure code remains poorly\nunderstood. While prior research has focused on unintended vulnerabilities or\nadversarial prompting techniques, this study examines a more direct threat\nscenario: open-source LLMs generating vulnerable code when prompted either\ndirectly or indirectly. We propose a dual experimental design: (1) Dynamic\nPrompting, which systematically varies vulnerability type, user persona, and\ndirectness across structured templates; and (2) Reverse Prompting, which\nderives prompts from real vulnerable code samples to assess vulnerability\nreproduction accuracy. We evaluate three open-source 7B-parameter models\n(Qwen2, Mistral, and Gemma) using ESBMC static analysis to assess both the\npresence of vulnerabilities and the correctness of the generated vulnerability\ntype. Results show all models frequently produce vulnerable outputs, with Qwen2\nachieving highest correctness rates. User persona significantly affects\nsuccess, where student personas achieved higher vulnerability rates than\nprofessional roles, while direct prompts were marginally more effective.\nVulnerability reproduction followed an inverted-U pattern with cyclomatic\ncomplexity, peaking at moderate ranges. Our findings expose limitations of\nsafety mechanisms in open-source models, particularly for seemingly benign\neducational requests.", "AI": {"tldr": "Open-source LLMs often generate insecure code when asked, especially for student-like users and moderate-code complexity. Their safety systems are insufficient, highlighting real risks for code assistance applications.", "motivation": "The motivation is to understand how open-source large language models (LLMs) generate insecure or vulnerable code when explicitly prompted, a behavior that is not fully understood, especially in direct threat scenarios beyond unintended vulnerabilities or complex adversarial prompts.", "method": "The study employs a dual experimental design: (1) Dynamic Prompting, which creates prompts by varying vulnerability types, user personas (like students versus professionals), and prompt directness using structured templates; and (2) Reverse Prompting, which generates prompts from real-world vulnerable code samples to analyze how accurately models reproduce those vulnerabilities. Three 7B-parameter open-source LLMs (Qwen2, Mistral, Gemma) are evaluated using static analysis (ESBMC) to detect vulnerabilities and assess type correctness.", "result": "All tested models frequently generated vulnerable code. Qwen2 produced the most correct vulnerability types. Student-like personas in prompts led to higher rates of vulnerabilities than professional personas, and direct prompts were only slightly more effective than indirect ones. Vulnerability reproduction rates followed an inverted-U relationship with cyclomatic complexity, peaking at moderate complexity.", "conclusion": "Open-source LLMs' safety mechanisms are inadequate, especially in responding to benign-sounding educational prompts, as they still readily generate vulnerable code under various prompting scenarios, indicating a significant security risk."}}
{"id": "2507.10062", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10062", "abs": "https://arxiv.org/abs/2507.10062", "authors": ["Erg\u00fcn Batuhan Kaynak", "Mayasah Lami", "Sahand Moslemi", "Anil Koyuncu"], "title": "LLMShot: Reducing snapshot testing maintenance via LLMs", "comment": "Accepted to ICSME 2025", "summary": "Snapshot testing has emerged as a critical technique for UI validation in\nmodern software development, yet it suffers from substantial maintenance\noverhead due to frequent UI changes causing test failures that require manual\ninspection to distinguish between genuine regressions and intentional design\nchanges. This manual triage process becomes increasingly burdensome as\napplications evolve, creating a need for automated analysis solutions. This\npaper introduces LLMShot, a novel framework that leverages vision-based Large\nLanguage Models to automatically analyze snapshot test failures through\nhierarchical classification of UI changes. To evaluate LLMShot's effectiveness,\nwe developed a comprehensive dataset using a feature-rich iOS application with\nconfigurable feature flags, creating realistic scenarios that produce authentic\nsnapshot differences representative of real development workflows. Our\nevaluation using Gemma3 models demonstrates strong classification performance,\nwith the 12B variant achieving over 84% recall in identifying failure root\ncauses while the 4B model offers practical deployment advantages with\nacceptable performance for continuous integration environments. However, our\nexploration of selective ignore mechanisms revealed significant limitations in\ncurrent prompting-based approaches for controllable visual reasoning. LLMShot\nrepresents the first automated approach to semantic snapshot test analysis,\noffering developers structured insights that can substantially reduce manual\ntriage effort and advance toward more intelligent UI testing paradigms.", "AI": {"tldr": "LLMShot is a new AI framework that automates the analysis of UI snapshot test failures using visual language models, greatly reducing manual effort with high accuracy, though challenges remain for controllable visual reasoning.", "motivation": "Snapshot testing is important for UI validation, but frequent UI changes lead to high maintenance overhead. Manual inspection is needed to distinguish between regressions and design changes, which is burdensome as applications evolve. There is a clear need for automated solutions.", "method": "The paper introduces LLMShot, a framework that uses vision-based Large Language Models to classify UI changes in snapshot test failures. The authors built a dataset from a complex iOS app with feature flags to create realistic test scenarios. They evaluated LLMShot using different Gemma3 model variants and also explored selective ignore mechanisms for controllable visual reasoning.", "result": "The 12B Gemma3 model in LLMShot achieved over 84% recall in identifying root causes of failures. The 4B model showed practical performance for continuous integration environments. The study also exposed limitations in current prompting-based selective ignore approaches.", "conclusion": "LLMShot is the first automated, semantics-aware approach for analyzing snapshot test failures. It provides developers with structured, actionable insights to reduce manual triage workload and points the way toward more intelligent UI testing."}}
{"id": "2507.10103", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.10103", "abs": "https://arxiv.org/abs/2507.10103", "authors": ["Hanyang Guo", "Xiaoheng Xie", "Hong-Ning Dai", "Peng Di", "Yu Zhang", "Bishenghui Tao", "Zibin Zheng"], "title": "Accelerating Automatic Program Repair with Dual Retrieval-Augmented Fine-Tuning and Patch Generation on Large Language Models", "comment": null, "summary": "Automated Program Repair (APR) is essential for ensuring software reliability\nand quality while enhancing efficiency and reducing developers' workload.\nAlthough rule-based and learning-based APR methods have demonstrated their\neffectiveness, their performance was constrained by the defect type of repair,\nthe quality of training data, and the size of model parameters. Recently, Large\nLanguage Models (LLMs) combined with Retrieval-Augmented-Generation (RAG) have\nbeen increasingly adopted in APR tasks. However, current code LLMs and RAG\ndesigns neither fully address code repair tasks nor consider code-specific\nfeatures. To overcome these limitations, we propose SelRepair, a novel APR\napproach with integration of a fine-tuned LLM with a newly-designed dual RAG\nmodule. This approach uses a bug-fix pair dataset for fine-tuning and\nincorporates semantic and syntactic/structural similarity information through\nan RAG selection gate. This design ensures relevant information is retrieved\nefficiently, thereby reducing token length and inference time. Evaluations on\nJava datasets show SelRepair outperforms other APR methods, achieving 26.29%\nand 17.64% in terms of exact match (EM) on different datasets while reducing\ninference time by at least 6.42% with controlled input lengths.", "AI": {"tldr": "This paper introduces SelRepair, an APR method leveraging a fine-tuned LLM and a dual RAG module to efficiently incorporate code-specific semantic and syntactic features, resulting in better repair accuracy and faster inference than previous methods.", "motivation": "Automated Program Repair (APR) methods are crucial for software quality and developer efficiency, but existing approaches are limited by repair type, training data quality, and model size. Recent advances, including LLMs combined with RAG, have not fully addressed code repair tasks or incorporated code-specific features.", "method": "The authors propose SelRepair, which integrates a fine-tuned LLM with a dual Retrieval-Augmented-Generation module. The approach utilizes a bug-fix pair dataset for fine-tuning and a selection gate that retrieves relevant information based on semantic and syntactic/structural similarity, optimizing token usage and inference time.", "result": "On Java datasets, SelRepair achieves higher exact match rates (26.29% and 17.64% on different datasets) and reduces inference time by at least 6.42% by controlling input lengths, outperforming previous APR methods.", "conclusion": "SelRepair, with its fine-tuned LLM and code-aware dual RAG design, addresses limitations of prior APR methods and shows improved accuracy and efficiency in automated code repair."}}
{"id": "2507.10182", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10182", "abs": "https://arxiv.org/abs/2507.10182", "authors": ["Gehao Zhang", "Zhenting Wang", "Juan Zhai"], "title": "Breaking the Myth: Can Small Models Infer Postconditions Too?", "comment": null, "summary": "Formal specifications are essential for ensuring software correctness, yet\nmanually writing them is tedious and error-prone. Large Language Models (LLMs)\nhave shown promise in generating such specifications from natural language\nintents, but the giant model size and high computational demands raise a\nfundamental question: Do we really need large models for this task? In this\npaper, we show that a small, fine-tuned language model can achieve high-quality\npostcondition generation with much lower computational costs. We construct a\nspecialized dataset of prompts, reasoning logs, and postconditions, then\nsupervise the fine-tuning of a $7$B-parameter code model. Our approach tackles\nreal-world repository dependencies and preserves pre-state information,\nallowing for expressive and accurate specifications. We evaluate the model on a\nbenchmark of real-world Java bugs (Defects4J) and compare against both\nproprietary giants (e.g., GPT-4o) and open-source large models. Empirical\nresults demonstrate that our compact model matches or outperforms significantly\nlarger counterparts in syntax correctness, semantic correctness, and\nbug-distinguishing capability. These findings highlight that targeted\nfine-tuning on a modest dataset can enable small models to achieve results\nformerly seen only in massive, resource-heavy LLMs, offering a practical and\nefficient path for the real-world adoption of automated specification\ngeneration.", "AI": {"tldr": "A small, fine-tuned code model can match or outperform much larger LLMs in generating correct and expressive software specifications, making automated specification generation more practical and efficient for real-world use.", "motivation": "Generating formal specifications is crucial for software correctness, but manual efforts are tedious and error-prone. While LLMs can automate this task, their large size and high computational requirements pose challenges.", "method": "The authors create a custom dataset of prompts, reasoning logs, and postconditions and supervise the fine-tuning of a 7B-parameter code model. The approach addresses real-world dependencies and ensures pre-state information is preserved for expressive and accurate specifications. The model is evaluated on a Java bugs benchmark (Defects4J) and compared to larger models including GPT-4o.", "result": "The fine-tuned small model matches or exceeds the performance of much larger models in syntax and semantic correctness and bug-distinguishing ability, demonstrating strong performance with lower resource demands.", "conclusion": "Small, fine-tuned language models are capable of high-quality automated specification generation, offering a practical, efficient, and resource-conscious alternative to using large LLMs."}}
{"id": "2507.10228", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10228", "abs": "https://arxiv.org/abs/2507.10228", "authors": ["Hugo Villamizar", "Daniel Mendez", "Marcos Kalinowski"], "title": "Towards a Framework for Operationalizing the Specification of Trustworthy AI Requirements", "comment": "This paper has been accepted for presentation at the 2025 IEEE 33rd\n  International Requirements Engineering Conference Workshops (REW-RETRAI 2025)", "summary": "Growing concerns around the trustworthiness of AI-enabled systems highlight\nthe role of requirements engineering (RE) in addressing emergent,\ncontext-dependent properties that are difficult to specify without structured\napproaches. In this short vision paper, we propose the integration of two\ncomplementary approaches: AMDiRE, an artefact-based approach for RE, and\nPerSpecML, a perspective-based method designed to support the elicitation,\nanalysis, and specification of machine learning (ML)-enabled systems. AMDiRE\nprovides a structured, artefact-centric, process-agnostic methodology and\ntemplates that promote consistency and traceability in the results; however, it\nis primarily oriented toward deterministic systems. PerSpecML, in turn,\nintroduces multi-perspective guidance to uncover concerns arising from the\ndata-driven and non-deterministic behavior of ML-enabled systems. We envision a\npathway to operationalize trustworthiness-related requirements, bridging\nstakeholder-driven concerns and structured artefact models. We conclude by\noutlining key research directions and open challenges to be discussed with the\nRE community.", "AI": {"tldr": "This vision paper proposes combining AMDiRE and PerSpecML to improve requirements engineering for trustworthy AI systems by balancing structure and multi-perspective analysis, and outlines future research topics.", "motivation": "There are increased concerns about the trustworthiness of AI systems, which are often complex, context-dependent, and have properties that are hard to specify. Traditional requirements engineering practices may not suffice for these new challenges.", "method": "The paper proposes integrating two approaches: AMDiRE (an artifact-based requirements engineering methodology focused on deterministic systems) and PerSpecML (a perspective-based method for dealing with concerns in machine learning systems, such as data-driven and non-deterministic behaviors).", "result": "The integration of AMDiRE and PerSpecML is described as a potential way to operationalize trustworthiness requirements in AI-enabled systems, making requirements more consistent and traceable. The authors also identify key research directions and open challenges for further investigation.", "conclusion": "Combining structured artefact-centric approaches with perspective-based guidance can better address the unique requirements and trust issues in ML-enabled systems. This integration provides a new pathway to link stakeholder concerns with systematic requirements models."}}
{"id": "2507.10235", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10235", "abs": "https://arxiv.org/abs/2507.10235", "authors": ["Zhixiang Chen", "Zhuangbin Chen", "Xingjie Cai", "Wei Li", "Zibin Zheng"], "title": "An Empirical Study of Interaction Bugs in ROS-based Software", "comment": null, "summary": "Modern robotic systems integrate multiple independent software and hardware\ncomponents, each responsible for distinct functionalities such as perception,\ndecision-making, and execution. These components interact extensively to\naccomplish complex end-to-end tasks. As a result, the overall system\nreliability depends not only on the correctness of individual components, but\nalso on the correctness of their interactions. Failures often manifest at the\nboundaries between components, yet interaction-related reliability issues in\nrobotics--referred to here as interaction bugs (iBugs)--remain underexplored.\n  This work presents an empirical study of iBugs within robotic systems built\nusing the Robot Operating System (ROS), a widely adopted open-source robotics\nframework. A total of 121 iBugs were analyzed across ten actively maintained\nand representative ROS projects. The identified iBugs are categorized into\nthree major types: intra-system iBugs, hardware iBugs, and environmental iBugs,\ncovering a broad range of interaction scenarios in robotics. The analysis\nincludes an examination of root causes, fixing strategies, and the impact of\nthese bugs. Several findingsa are derived that shed light on the nature of\niBugs and suggest directions for improving their prevention and detection.\nThese insights aim to inform the design of more robust and safer robotic\nsystems.", "AI": {"tldr": "This paper analyzes 121 interaction bugs in ROS-based robotics systems, classifies them, and draws actionable insights to help build more reliable robotic systems by mitigating failures at component interaction points.", "motivation": "Modern robots rely on multiple interacting software and hardware components, and failures often occur at their interaction points. However, reliability issues caused by component interactions\u2014called interaction bugs (iBugs)\u2014are poorly understood and studied.", "method": "The paper presents an empirical study by analyzing 121 interaction bugs (iBugs) across ten representative, actively maintained ROS (Robot Operating System) projects. The study categorizes the iBugs, examines their root causes, fixing approaches, and resulting impact.", "result": "iBugs were classified into three main types: intra-system, hardware, and environmental. The study identified common root causes and effective strategies for resolving these bugs, offering insights into how such interaction bugs arise and propagate.", "conclusion": "The findings highlight the importance of addressing interaction-level reliability in robotics. Improved awareness and systematic handling of iBugs can lead to the design of more robust and safer robotic systems."}}
{"id": "2507.10244", "categories": ["cs.SE", "D.2.2; D.2.11"], "pdf": "https://arxiv.org/pdf/2507.10244", "abs": "https://arxiv.org/abs/2507.10244", "authors": ["Adam \u0160t\u011bp\u00e1nek", "David Ku\u0165\u00e1k", "Barbora Kozl\u00edkov\u00e1", "Jan By\u0161ka"], "title": "Helveg: Diagrams for Software Documentation", "comment": "13 pages, 5 figures, accepted by TVCG", "summary": "Software developers often have to gain an understanding of a codebase. Be it\nprogrammers getting onboarded onto a team project or, for example, developers\nstriving to grasp an external open-source library. In either case, they\nfrequently turn to the project's documentation. However, documentation in its\ntraditional textual form is ill-suited for this kind of high-level exploratory\nanalysis, since it is immutable from the readers' perspective and thus forces\nthem to follow a predefined path. We have designed an approach bringing aspects\nof software architecture visualization to API reference documentation. It\nutilizes a highly interactive node-link diagram with expressive node glyphs and\nflexible filtering capabilities, providing a high-level overview of the\ncodebase as well as details on demand. To test our design, we have implemented\na prototype named Helveg, capable of automatically generating diagrams of C\\#\ncodebases. User testing of Helveg confirmed its potential, but it also revealed\nproblems with the readability, intuitiveness, and user experience of our tool.\nTherefore, in this paper, which is an extended version of our VISSOFT paper\nwith DOI 10.1109/VISSOFT64034.2024.00012, we address many of these problems\nthrough major changes to the glyph design, means of interaction, and user\ninterface of the tool. To assess the improvements, this new version of Helveg\nwas evaluated again with the same group of participants as the previous\nversion.", "AI": {"tldr": "The paper presents Helveg, a tool that turns API documentation into interactive, diagram-based visualizations, making codebase exploration easier for developers. After initial user testing exposed usability problems, major improvements were made and validated with further user evaluation.", "motivation": "Software developers often struggle to understand unfamiliar codebases, especially when onboarding or exploring external libraries. Traditional documentation is static and does not support interactive or high-level exploration, making it less effective for this purpose.", "method": "The paper introduces an approach that integrates software architecture visualization into API reference documentation. The approach uses interactive node-link diagrams with expressive node glyphs and filtering capabilities, implemented in a prototype tool called Helveg that automatically visualizes C# codebases. The paper reports iterative development, with user testing guiding improvements to glyph design, interaction, and the UI.", "result": "User testing of the initial Helveg prototype revealed issues with readability, intuitiveness, and user experience. In response, the tool was significantly improved, and the new version was evaluated again with the same user group to assess these enhancements.", "conclusion": "The improved Helveg tool brings significant benefits to codebase exploration by making documentation more interactive and visually informative. However, iterative user testing remains crucial as initial designs may have usability issues that need to be addressed."}}
{"id": "2507.10305", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10305", "abs": "https://arxiv.org/abs/2507.10305", "authors": ["Linus Ververs", "Trang Linh Lam", "Janina Berger", "Lutz Prechelt"], "title": "A Grounded Theory on the Teacher and Student Roles in Pair Programming", "comment": null, "summary": "Context: Pair programming is an established (agile) practice and is practiced\nthroughout the industry. Objective: Understand under what circumstances\nknowledge transfer can harm a pair programming session. Method: Grounded Theory\nMethodology based on 17 recorded pair programming sessions with 18 developers\nfrom 5 German software companies accompanied, by 6 interviews with different\ndevelopers from 4 other German companies. Results: We define the student and\nteacher roles to help developers deal with a one-sided knowledge gap. We\ndescribe pitfalls to avoid and develop a grounded theory centered around the\nPower Gap in pair programming. Conclusions: Knowledge transfer can be harmful\nwhen developers don't pay attention to their partners needs and desires. If\ndevelopers don't pay attention to the Power Gap and keep it in check, Defensive\nBehavior may arise that leads to a vicious cycle impacting the knowledge\ntransfer, the Togetherness and the code quality in a negative way.", "AI": {"tldr": "Pair programming can suffer when one developer dominates; ignoring power gaps may trigger defensive behaviors and reduce knowledge sharing, teamwork, and code quality.", "motivation": "Pair programming is widely adopted, but the dynamics of knowledge transfer within pairs\u2014especially when there is an imbalance\u2014are not fully understood. The study aims to explore under what circumstances knowledge transfer may negatively impact pair programming.", "method": "The study uses Grounded Theory Methodology involving 17 recorded pair programming sessions with 18 developers from 5 German software companies, complemented by 6 interviews from developers in 4 additional companies.", "result": "The researchers introduce 'student' and 'teacher' roles to address one-sided knowledge gaps and highlight specific pitfalls. They develop a theory focused on the 'Power Gap', showing that unchecked power imbalances lead to defensive behaviors, which harm knowledge transfer, team cohesion, and code quality.", "conclusion": "Knowledge transfer in pair programming can be harmful if power imbalances are ignored and developers do not consider their partners' needs, leading to defensive attitudes and negative cycles affecting collaboration and output quality."}}
{"id": "2507.10321", "categories": ["cs.SE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10321", "abs": "https://arxiv.org/abs/2507.10321", "authors": ["Viktor Sinitsyn", "Nils Schlautmann", "Florian Schwaiger", "Florian Holzapfel"], "title": "Streamlined Airborne Software Development for Large UAVs: From Unified Data Collection to Automated Code Generation", "comment": null, "summary": "The aerospace industry has experienced significant transformations over the\nlast decade, driven by technological advancements and innovative solutions in\ngoods and personal transportation. This evolution has spurred the emergence of\nnumerous start-ups that now face challenges traditionally encountered by\nestablished aerospace companies. Among these challenges is the efficient\nprocessing of digital intra-device communication interfaces for onboard\nequipment - a critical component for ensuring seamless system integration and\nfunctionality. Addressing this challenge requires solutions that emphasize\nclear and consistent interface descriptions, automation of processes, and\nreduced labor-intensive efforts.\n  This paper presents a novel process and toolchain designed to streamline the\ndevelopment of digital interfaces and onboard software, which our team has\nsuccessfully applied in several completed projects. The proposed approach\nfocuses on automation and flexibility while maintaining compliance with design\nassurance requirements.", "AI": {"tldr": "This paper introduces an automated and flexible toolchain for digital interface and onboard software development in aerospace, which has proven effective in real projects and aims to help start-ups handle integration and assurance challenges efficiently.", "motivation": "The motivation of this paper lies in addressing challenges faced by start-ups in the aerospace industry, specifically the need for efficient, automated, and consistent processing of digital intra-device communication interfaces for onboard equipment.", "method": "The authors present a novel process and toolchain focusing on automation, flexibility, and compliance with design assurance requirements for the development of digital interfaces and onboard software.", "result": "The new process and toolchain were successfully applied in several completed projects, demonstrating their effectiveness in streamlining development.", "conclusion": "The proposed solution improves efficiency, clarity, and consistency in digital interface development for aerospace onboard equipment, providing start-ups with modern tools to overcome traditional industry challenges."}}
{"id": "2507.10338", "categories": ["cs.SE", "cs.AR", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.10338", "abs": "https://arxiv.org/abs/2507.10338", "authors": ["Enyuan Tian", "Yiwei Ci", "Qiusong Yang", "Yufeng Li", "Zhichao Lyu"], "title": "AssertCoder: LLM-Based Assertion Generation via Multimodal Specification Extraction", "comment": "7 pages, 3 figures", "summary": "Assertion-Based Verification (ABV) is critical for ensuring functional\ncorrectness in modern hardware systems. However, manually writing high-quality\nSVAs remains labor-intensive and error-prone. To bridge this gap, we propose\nAssertCoder, a novel unified framework that automatically generates\nhigh-quality SVAs directly from multimodal hardware design specifications.\nAssertCoder employs a modality-sensitive preprocessing to parse heterogeneous\nspecification formats (text, tables, diagrams, and formulas), followed by a set\nof dedicated semantic analyzers that extract structured representations aligned\nwith signal-level semantics. These representations are utilized to drive\nassertion synthesis via multi-step chain-of-thought (CoT) prompting. The\nframework incorporates a mutation-based evaluation approach to assess assertion\nquality via model checking and further refine the generated assertions.\nExperimental evaluation across three real-world Register-Transfer Level (RTL)\ndesigns demonstrates AssertCoder's superior performance, achieving an average\nincrease of 8.4% in functional correctness and 5.8% in mutation detection\ncompared to existing state-of-the-art approaches.", "AI": {"tldr": "AssertCoder is an automated framework that transforms various hardware specification formats into high-quality verification assertions, significantly boosting verification accuracy and efficiency over existing tools.", "motivation": "Assertion-Based Verification (ABV) is essential for confirming the functional correctness of modern hardware, but writing SystemVerilog Assertions (SVAs) by hand is time-consuming and prone to errors.", "method": "The paper introduces AssertCoder, a unified framework that generates high-quality SVAs automatically from multimodal hardware design specifications. The process includes modality-sensitive preprocessing to handle different specification formats, semantic analyzers for extracting structured signal-level representations, and multi-step chain-of-thought prompting for assertion synthesis. Generated assertions are evaluated and refined using a mutation-based model checking approach.", "result": "AssertCoder outperforms state-of-the-art methods in experiments across three real-world RTL designs, achieving an average of 8.4% improvement in functional correctness and 5.8% better mutation detection.", "conclusion": "Automating assertion generation with AssertCoder significantly improves both the quality and efficiency of assertion-based verification in hardware design, surpassing existing manual and automated solutions."}}
{"id": "2507.10422", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10422", "abs": "https://arxiv.org/abs/2507.10422", "authors": ["Tao Xiao", "Youmei Fan", "Fabio Calefato", "Christoph Treude", "Raula Gaikovina Kula", "Hideaki Hata", "Sebastian Baltes"], "title": "Self-Admitted GenAI Usage in Open-Source Software", "comment": "17 pages, 8 tables, 1 figures, currently under review", "summary": "The widespread adoption of generative AI (GenAI) tools such as GitHub Copilot\nand ChatGPT is transforming software development. Since generated source code\nis virtually impossible to distinguish from manually written code, their\nreal-world usage and impact on open-source software development remain poorly\nunderstood. In this paper, we introduce the concept of self-admitted GenAI\nusage, that is, developers explicitly referring to the use of GenAI tools for\ncontent creation in software artifacts. Using this concept as a lens to study\nhow GenAI tools are integrated into open-source software projects, we analyze a\ncurated sample of more than 250,000 GitHub repositories, identifying 1,292 such\nself-admissions across 156 repositories in commit messages, code comments, and\nproject documentation. Using a mixed methods approach, we derive a taxonomy of\n32 tasks, 10 content types, and 11 purposes associated with GenAI usage based\non 284 qualitatively coded mentions. We then analyze 13 documents with policies\nand usage guidelines for GenAI tools and conduct a developer survey to uncover\nthe ethical, legal, and practical concerns behind them. Our findings reveal\nthat developers actively manage how GenAI is used in their projects,\nhighlighting the need for project-level transparency, attribution, and quality\ncontrol practices in the new era of AI-assisted software development. Finally,\nwe examine the longitudinal impact of GenAI adoption on code churn in 151\nrepositories with self-admitted GenAI usage and find no general increase,\ncontradicting popular narratives on the impact of GenAI on software\ndevelopment.", "AI": {"tldr": "The paper finds that although generative AI tools are widely acknowledged by developers in open-source projects, their integration is being responsibly managed without causing the code instability or increased churn that is often assumed.", "motivation": "With the rise of generative AI tools like GitHub Copilot and ChatGPT, it's becoming difficult to distinguish between AI-generated and manually written source code. This raises questions about how GenAI tools are actually being used in open-source software projects and the implications for software development.", "method": "The authors introduce the idea of 'self-admitted GenAI usage'\u2014instances where developers explicitly acknowledge using GenAI tools in software artifacts. They analyze over 250,000 GitHub repositories to identify such admissions, conduct qualitative coding to derive a taxonomy, review policy documents, and survey developers to understand concerns and practices related to GenAI. They also examine repository code churn to measure the impact of GenAI usage.", "result": "They found 1,292 self-admitted GenAI usage instances in 156 repositories. The researchers created a taxonomy of tasks, content types, and purposes for GenAI use. Analysis of policies and survey responses revealed significant ethical, legal, and practical concerns, and showed developers are proactively managing GenAI adoption with guidelines and quality control. Importantly, code churn did not generally increase in repositories using GenAI, challenging prevailing assumptions.", "conclusion": "Developers are actively managing the integration of GenAI tools in open-source projects, emphasizing transparency, attribution, and quality control. The anticipated negative impact of GenAI adoption on code churn is not supported by their findings, suggesting that GenAI's integration does not degrade codebase stability as commonly feared."}}
