{"id": "2507.05269", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05269", "abs": "https://arxiv.org/abs/2507.05269", "authors": ["Danning Xie", "Mingwei Zheng", "Xuwei Liu", "Jiannan Wang", "Chengpeng Wang", "Lin Tan", "Xiangyu Zhang"], "title": "CORE: Benchmarking LLMs Code Reasoning Capabilities through Static Analysis Tasks", "comment": null, "summary": "Large language models (LLMs) have been widely adopted across diverse software\nengineering domains, such as code generation, program repair, and vulnerability\ndetection. These applications require understanding beyond surface-level code\npatterns: value propagation, control flow, and interdependence between program\nelements. However, existing benchmarks primarily evaluate end-to-end outcomes,\nsuch as whether code is correctly repaired or generated, leaving the models\nability for program semantic reasoning underexplored. This work presents CoRe,\na high-quality, human-verified benchmark designed to evaluate LLMs on\nfundamental static analysis tasks. CoRe includes 12,553 task instances spanning\ndata dependency, control dependency, and information flow across programs\nwritten in C/C++, Java, and Python. To ensure semantic diversity and reasoning\ncomplexity, we propose a semantics-aware diverse sampling strategy that selects\ntargets and task instances based on structural coverage and dependency depth.\nWe evaluate 10 mainstream LLMs and show that, while they perform well at\nidentifying dependencies, models still struggle with tasks that require deeper\nsemantic understanding and multi-step reasoning. We further conduct qualitative\nanalyses to uncover key challenges, such as complex control structures and\nbackward dependency patterns, offering insights into improving LLMs code\nreasoning capabilities.", "AI": {"tldr": "The paper introduces CoRe, a benchmark for evaluating LLMs on static code analysis tasks. Results show LLMs excel at basic code dependencies but falter on deeper semantic reasoning, highlighting gaps in current model capabilities and suggesting directions for future improvement.", "motivation": "Existing LLM evaluation benchmarks mainly measure end-to-end code correctness but do not adequately assess models' abilities for core program semantic reasoning. There is a need for tasks that probe LLMs\u2019 deeper understanding of code beyond surface-level patterns.", "method": "The authors design and introduce CoRe, a benchmark for static code analysis covering data dependency, control dependency, and information flow in C/C++, Java, and Python. They use a semantics-aware sampling strategy to ensure task diversity, and evaluate 10 mainstream LLMs on 12,553 instances, followed by qualitative challenge analyses.", "result": "LLMs show competence with basic dependency identification but perform poorly on deeper, multi-step semantic code reasoning tasks, particularly in the presence of complex structures. Qualitative findings point to specific reasoning challenges that future LLM improvements should address.", "conclusion": "LLMs perform well on basic dependency identification tasks but struggle with deeper semantic reasoning and multi-step static analysis tasks. Current LLMs have significant limitations in understanding complex control structures and backward dependency patterns."}}
{"id": "2507.05270", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05270", "abs": "https://arxiv.org/abs/2507.05270", "authors": ["Boyuan Li", "Chengwei Liu", "Lingling Fan", "Sen Chen", "Zhenlin Zhang", "Zheli Liu"], "title": "Open Source, Hidden Costs: A Systematic Literature Review on OSS License Management", "comment": null, "summary": "Integrating third-party software components is a common practice in modern\nsoftware development, offering significant advantages in terms of efficiency\nand innovation. However, this practice is fraught with risks related to\nsoftware licensing. A lack of understanding may lead to disputes, which can\npose serious legal and operational challenges. To these ends, both academia and\nindustry have conducted various investigations and proposed solutions and tools\nto deal with these challenges. However, significant limitations still remain.\nMoreover, the rapid evolution of open-source software (OSS) licenses, as well\nas the rapidly incorporated generative software engineering techniques, such as\nlarge language models for code (CodeLLMs), are placing greater demands on the\nsystematic management of software license risks. To unveil the severe\nchallenges and explore possible future directions, we conduct the first\nsystematic literature review (SLR) on 80 carefully selected OSS license-related\npapers, classifying existing research into three key categories, i.e., license\nidentification, license risk assessment, and license risk mitigation. Based on\nthese, we discuss challenges in existing solutions, conclude the opportunities\nto shed light on future research directions and offer practical recommendations\nfor practitioners. We hope this thorough review will help bridge the gaps\nbetween academia and industry and accelerate the ecosystem-wide governance of\nlegitimate software risks within the software engineering community.", "AI": {"tldr": "This paper reviews 80 works on software license risk management, categorizing current solutions, discussing their limitations, and outlining future research needs. It provides insights and recommendations to support better handling of software licensing challenges, especially with emerging OSS and generative AI in software engineering.", "motivation": "Third-party software integration is widespread in software development but brings significant software licensing risks. The evolving landscape of OSS licenses and the rise of technologies like CodeLLMs have increased the complexity and importance of managing these risks systematically.", "method": "The authors conducted the first systematic literature review (SLR) on open-source software license-related papers. They reviewed 80 selected papers and classified research into license identification, risk assessment, and risk mitigation. The paper also analyzes challenges and future research opportunities.", "result": "The review identifies and classifies existing research into three main categories: license identification, risk assessment, and risk mitigation. It highlights significant limitations in current solutions and the need for improved approaches to software license risk management, especially in light of new technologies and evolving license frameworks.", "conclusion": "There remain considerable challenges in managing software license risks. The review outlines opportunities for further research and practical recommendations for software engineers and hopes to foster stronger collaboration between academia and industry while supporting sound license risk governance."}}
{"id": "2507.05272", "categories": ["cs.SE", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.05272", "abs": "https://arxiv.org/abs/2507.05272", "authors": ["Daragh King", "Vasileios Koutavas", "Laura Kovacs"], "title": "FuzzFeed: An Automatic Approach to Weakest Precondition Generation using LLMs and Fuzzing", "comment": null, "summary": "The weakest precondition (WP) of a program describes the largest set of\ninitial states from which all terminating executions of the program satisfy a\ngiven postcondition. The generation of WPs is an important task with practical\napplications in areas ranging from verification to run-time error checking.\n  This paper proposes the combination of Large Language Models (LLMs) and fuzz\ntesting for generating WPs. In pursuit of this goal, we introduce Fuzzing\nGuidance (FG); FG acts as a means of directing LLMs towards correct WPs using\nprogram execution feedback. FG utilises fuzz testing for approximately checking\nthe validity and weakness of candidate WPs, this information is then fed back\nto the LLM as a means of context refinement.\n  We demonstrate the effectiveness of our approach on a comprehensive benchmark\nset of deterministic array programs in Java. Our experiments indicate that LLMs\nare capable of producing viable candidate WPs, and that this ability can be\npractically enhanced through FG.", "AI": {"tldr": "The paper proposes enhancing weakest precondition generation by combining LLMs with fuzz testing through a method called Fuzzing Guidance, which uses execution feedback to improve results. Experiments on Java array programs show that this approach is effective.", "motivation": "Weakest preconditions (WPs) are crucial for program verification and error checking, but generating them can be challenging. The paper aims to improve WP generation using new techniques.", "method": "The authors propose combining Large Language Models (LLMs) with fuzz testing. They introduce a method called Fuzzing Guidance (FG), which provides execution feedback to guide LLMs towards correct WPs. FG uses fuzz testing to evaluate the validity and weakness of candidate WPs and gives this feedback back to the LLM for refining its context.", "result": "On benchmarks of deterministic array programs in Java, LLMs generated viable candidate WPs. The Fuzzing Guidance approach further improved the accuracy and practicality of WP generation.", "conclusion": "Combining LLMs and fuzz testing using the proposed FG method is effective for generating weakest preconditions, and execution feedback can significantly enhance the LLMs' capabilities in this task."}}
{"id": "2507.05279", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.05279", "abs": "https://arxiv.org/abs/2507.05279", "authors": ["Virgile Boraud", "Yannis Bendi-Ouis", "Paul Bernard", "Xavier Hinaut"], "title": "ReservoirChat: Interactive Documentation Enhanced with LLM and Knowledge Graph for ReservoirPy", "comment": null, "summary": "We introduce a tool designed to improve the capabilities of Large Language\nModels (LLMs) in assisting with code development using the ReservoirPy library,\nas well as in answering complex questions in the field of Reservoir Computing.\nBy incorporating external knowledge through Retrieval-Augmented Generation\n(RAG) and knowledge graphs, our approach aims to reduce hallucinations and\nincrease the factual accuracy of generated responses. The system provides an\ninteractive experience similar to ChatGPT, tailored specifically for\nReservoirPy, enabling users to write, debug, and understand Python code while\naccessing reliable domain-specific insights. In our evaluation, while\nproprietary models such as ChatGPT-4o and NotebookLM performed slightly better\non general knowledge questions, our model outperformed them on coding tasks and\nshowed a significant improvement over its base model, Codestral-22B.", "AI": {"tldr": "A specialized LLM tool using RAG and knowledge graphs enhances code generation and question answering for ReservoirPy, outperforming major LLMs in coding tasks and improving accuracy over its base model.", "motivation": "The need to improve the reliability and code-assistance capabilities of LLMs, specifically for working with the ReservoirPy library and answering specialized questions in Reservoir Computing, motivates this research.", "method": "The authors design a tool that integrates Retrieval-Augmented Generation (RAG) and knowledge graphs to enhance LLMs. This system is made interactive with a ChatGPT-like interface focused on ReservoirPy, allowing users to write, debug, and understand Python code with domain-specific support.", "result": "The system enabled better performance on coding tasks related to ReservoirPy compared to proprietary models like ChatGPT-4o and NotebookLM, and it significantly outperformed its own base model (Codestral-22B) in these tasks. Proprietary models still slightly outperformed it on general knowledge questions.", "conclusion": "Integrating RAG and knowledge graphs into LLMs improves factual accuracy and reduces hallucinations for domain-specific code development and Q&A, notably surpassing state-of-the-art proprietary models in targeted tasks."}}
{"id": "2507.05281", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.05281", "abs": "https://arxiv.org/abs/2507.05281", "authors": ["Lingyue Fu", "Hao Guan", "Bolun Zhang", "Haowei Yuan", "Yaoming Zhu", "Jun Xu", "Zongyu Wang", "Lin Qiu", "Xunliang Cai", "Xuezhi Cao", "Weiwen Liu", "Weinan Zhang", "Yong Yu"], "title": "CoreCodeBench: A Configurable Multi-Scenario Repository-Level Benchmark", "comment": null, "summary": "As Large Language Models (LLMs) demonstrate increasingly sophisticated code\nprocessing capabilities, evaluating their performance on engineering-level code\nremains challenging. Existing repository-level benchmarks primarily focus on\nsingle scenarios, such as code generation or bug fixing, without adequately\ncapturing the diversity and complexity of real-world software or project\nengineering workflows. Furthermore, these benchmarks suffer from limited\ncontrollability in question positioning and reliability issues in their\ngenerated test cases. To address these limitations, we present CorePipe, a\nfully automated pipeline that converts repositories into comprehensive test\ncases, and introduce CoreCodeBench, a configurable multi-scenario\nrepository-level benchmark. To simulate real engineering scenarios, CorePipe\ngenerates three types of atomic questions (Development, BugFix, and Test-Driven\nDevelopment) specifically targeting core code segments. These atomic questions\nare further combined into three types of composite questions, with difficulty\nlevels flexibly adjusted through hyperparameter tuning. CoreCodeBench provides\na comprehensive and extensive repository-level benchmark to investigate the\napplicability of LLMs in real-world engineering projects. Experiments with 16\nLLMs across diverse scenarios reveal varying capabilities and offer\nmulti-dimensional insights into LLM performance in engineering contexts. The\ncode for CorePipe is available at\nhttps://github.com/AGI-Eval-Official/CoreCodeBench, and the data for\nCoreCodeBench can be accessed at\nhttps://huggingface.co/collections/tubehhh/corecodebench-68256d2faabf4b1610a08caa.", "AI": {"tldr": "The paper introduces CorePipe and CoreCodeBench\u2014tools for creating and running comprehensive, realistic, and configurable benchmarks to better evaluate LLMs' performance in real-world engineering code contexts, showing existing LLMs have varied strengths but none are universally superior.", "motivation": "Existing benchmarks for evaluating LLMs on code are limited, focusing on single tasks, lacking diversity, and having issues with question control and test reliability. There is a need for more comprehensive and realistic benchmarks to assess LLMs in real engineering contexts.", "method": "The authors introduce CorePipe, an automated pipeline that turns code repositories into test cases. They develop CoreCodeBench, a benchmark suite that includes various scenarios and adjustable difficulty, by generating atomic (e.g., Development, BugFix, Test-Driven Development) and composite questions targeting central code regions.", "result": "Experiments were conducted with 16 LLMs across different scenarios, showing their varied strengths and weaknesses. CoreCodeBench allowed for multi-dimensional insights into LLM capabilities on engineering-level tasks.", "conclusion": "CoreCodeBench, enabled by CorePipe, offers a more realistic, flexible, and comprehensive way to benchmark LLMs on engineering-level repository tasks, better reflecting real-world software development needs."}}
{"id": "2507.05289", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05289", "abs": "https://arxiv.org/abs/2507.05289", "authors": ["Igor Regis da Silva Simoes", "Elaine Venson"], "title": "Measuring how changes in code readability attributes affect code quality evaluation by Large Language Models", "comment": null, "summary": "Code readability is one of the main aspects of code quality, influenced by\nvarious properties like identifier names, comments, code structure, and\nadherence to standards. However, measuring this attribute poses challenges in\nboth industry and academia. While static analysis tools assess attributes such\nas code smells and comment percentage, code reviews introduce an element of\nsubjectivity. This paper explores using Large Language Models (LLMs) to\nevaluate code quality attributes related to its readability in a standardized,\nreproducible, and consistent manner. We conducted a quasi-experiment study to\nmeasure the effects of code changes on Large Language Model (LLM)s\ninterpretation regarding its readability quality attribute. Nine LLMs were\ntested, undergoing three interventions: removing comments, replacing identifier\nnames with obscure names, and refactoring to remove code smells. Each\nintervention involved 10 batch analyses per LLM, collecting data on response\nvariability. We compared the results with a known reference model and tool. The\nresults showed that all LLMs were sensitive to the interventions, with\nagreement with the reference classifier being high for the original and\nrefactored code scenarios. The LLMs demonstrated a strong semantic sensitivity\nthat the reference model did not fully capture. A thematic analysis of the LLMs\nreasoning confirmed their evaluations directly reflected the nature of each\nintervention. The models also exhibited response variability, with 9.37% to\n14.58% of executions showing a standard deviation greater than zero, indicating\nresponse oscillation, though this did not always compromise the statistical\nsignificance of the results. LLMs demonstrated potential for evaluating\nsemantic quality aspects, such as coherence between identifier names, comments,\nand documentation with code purpose.", "AI": {"tldr": "This paper investigates how Large Language Models (LLMs) can be used to assess code readability. Through experiments altering code comments, identifier names, and structure, nine LLMs were tested and compared to a traditional model. Results showed LLMs reliably detected readability changes, matched or exceeded the reference tool in sensitivity, and showed potential for robust code quality evaluation.", "motivation": "Code readability is essential for code quality and is influenced by factors like naming, comments, structure, and standard adherence, but is difficult to measure objectively and consistently. Traditional tools and methods have limitations, prompting the exploration of new approaches.", "method": "The study performed a quasi-experiment with nine Large Language Models (LLMs) subjected to three types of code interventions: removing comments, obfuscating identifier names, and refactoring to remove code smells. Each scenario was tested with 10 analyses per LLM, and results were compared against a known reference tool and model. Thematic analysis was used to assess LLMs' reasoning.", "result": "All LLMs detected the interventions and largely agreed with the reference model in original and refactored scenarios. They showed semantic sensitivity beyond the reference model, and their responses reflected intervention types. Response variability was present but did not consistently affect statistical significance. LLMs demonstrated capability in evaluating semantic aspects like coherence of names, comments, and documentation.", "conclusion": "LLMs show promise for standardized, consistent, and semantically rich evaluation of code readability and related quality attributes, offering advantages over traditional tools and manual reviews."}}
{"id": "2507.05294", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05294", "abs": "https://arxiv.org/abs/2507.05294", "authors": ["William Law"], "title": "zkSDK: Streamlining zero-knowledge proof development through automated trace-driven ZK-backend selection", "comment": "undergrad thesis", "summary": "The rapid advancement of creating Zero-Knowledge (ZK) programs has led to the\ndevelopment of numerous tools designed to support developers. Popular options\ninclude being able to write in general-purpose programming languages like Rust\nfrom Risc Zero. Other languages exist like Circom, Lib-snark, and Cairo.\nHowever, developers entering the ZK space are faced with many different ZK\nbackends to choose from, leading to a steep learning curve and a fragmented\ndeveloper experience across different platforms. As a result, many developers\ntend to select a single ZK backend and remain tied to it. This thesis\nintroduces zkSDK, a modular framework that streamlines ZK application\ndevelopment by abstracting the backend complexities. At the core of zkSDK is\nPresto, a custom Python-like programming language that enables the profiling\nand analysis of a program to assess its computational workload intensity.\nCombined with user-defined criteria, zkSDK employs a dynamic selection\nalgorithm to automatically choose the optimal ZK-proving backend. Through an\nin-depth analysis and evaluation of real-world workloads, we demonstrate that\nzkSDK effectively selects the best-suited backend from a set of supported ZK\nbackends, delivering a seamless and user-friendly development experience.", "AI": {"tldr": "zkSDK streamlines Zero-Knowledge app development by automatically picking the best backend using workload analysis, reducing complexity and improving the developer experience.", "motivation": "Developers in the Zero-Knowledge (ZK) space face a fragmented ecosystem with multiple ZK backend options, resulting in a steep learning curve and a tendency to stick with only one backend. There is a need to simplify and unify the ZK application development experience.", "method": "The authors introduce zkSDK, a modular framework with a custom Python-like language called Presto, which profiles and analyzes program workload. Using user-defined criteria, zkSDK implements a dynamic selection algorithm to automatically determine the most appropriate ZK-proving backend for a given workload.", "result": "Their analysis and evaluation on real-world workloads show that zkSDK successfully selects the optimal backend from several options, improving the developer experience and backend performance.", "conclusion": "zkSDK abstracts the complexity of choosing between multiple ZK backends, enabling developers to build ZK applications more efficiently and user-friendly, regardless of backend fragmentation."}}
{"id": "2507.05307", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05307", "abs": "https://arxiv.org/abs/2507.05307", "authors": ["Xuanqi Gao", "Juan Zhai", "Shiqing Ma", "Siyi Xie", "Chao Shen"], "title": "ASSURE: Metamorphic Testing for AI-powered Browser Extensions", "comment": null, "summary": "The integration of Large Language Models (LLMs) into browser extensions has\nrevolutionized web browsing, enabling sophisticated functionalities like\ncontent summarization, intelligent translation, and context-aware writing\nassistance. However, these AI-powered extensions introduce unprecedented\nchallenges in testing and reliability assurance. Traditional browser extension\ntesting approaches fail to address the non-deterministic behavior,\ncontext-sensitivity, and complex web environment integration inherent to\nLLM-powered extensions. Similarly, existing LLM testing methodologies operate\nin isolation from browser-specific contexts, creating a critical gap in\neffective evaluation frameworks. To bridge this gap, we present ASSURE, a\nmodular automated testing framework specifically designed for AI-powered\nbrowser extensions. ASSURE comprises three principal components: (1) a modular\ntest case generation engine that supports plugin-based extension of testing\nscenarios, (2) an automated execution framework that orchestrates the complex\ninteractions between web content, extension processing, and AI model behavior,\nand (3) a configurable validation pipeline that systematically evaluates\nbehavioral consistency and security invariants rather than relying on exact\noutput matching. Our evaluation across six widely-used AI browser extensions\ndemonstrates ASSURE's effectiveness, identifying 531 distinct issues spanning\nsecurity vulnerabilities, metamorphic relation violations, and content\nalignment problems. ASSURE achieves 6.4x improved testing throughput compared\nto manual approaches, detecting critical security vulnerabilities within 12.4\nminutes on average. This efficiency makes ASSURE practical for integration into\ndevelopment pipelines, offering a comprehensive solution to the unique\nchallenges of testing AI-powered browser extensions.", "AI": {"tldr": "ASSURE is a novel, automated framework for robustly testing LLM-based browser extensions, outperforming manual testing in speed and thoroughness, and uncovering critical vulnerabilities, making it suitable for practical use in development pipelines.", "motivation": "The rapid integration of Large Language Models (LLMs) into browser extensions has enabled advanced features but also introduced new challenges in testing, reliability, and security that existing testing approaches cannot effectively address, particularly due to the complexity and context-sensitivity of LLM-powered extensions.", "method": "The paper proposes ASSURE, a modular automated testing framework designed specifically for AI-powered browser extensions. ASSURE consists of a flexible test case generation engine, an execution framework that manages interactions including web and extension behavior, and a validation pipeline focusing on behavioral and security evaluations rather than exact output matching.", "result": "The evaluation of ASSURE on six popular AI browser extensions revealed 531 distinct issues, such as security risks and content mismatches. ASSURE achieved a 6.4x increase in testing throughput over manual methods and was able to discover critical vulnerabilities in just 12.4 minutes on average.", "conclusion": "ASSURE provides an efficient and comprehensive framework for testing and improving the reliability and security of LLM-powered browser extensions, addressing unique challenges and supporting integration into real-world development workflows."}}
{"id": "2507.05316", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05316", "abs": "https://arxiv.org/abs/2507.05316", "authors": ["Koren Lazar", "Matan Vetzler", "Kiran Kate", "Jason Tsay", "David Boaz Himanshu Gupta", "Avraham Shinnar", "Rohith D Vallam", "David Amid Esther Goldbraich", "Guy Uziel", "Jim Laredo", "Ateret Anaby Tavor"], "title": "OASBuilder: Generating OpenAPI Specifications from Online API Documentation with Large Language Models", "comment": null, "summary": "AI agents and business automation tools interacting with external web\nservices require standardized, machine-readable information about their APIs in\nthe form of API specifications. However, the information about APIs available\nonline is often presented as unstructured, free-form HTML documentation,\nrequiring external users to spend significant time manually converting it into\na structured format. To address this, we introduce OASBuilder, a novel\nframework that transforms long and diverse API documentation pages into\nconsistent, machine-readable API specifications. This is achieved through a\ncarefully crafted pipeline that integrates large language models and rule-based\nalgorithms which are guided by domain knowledge of the structure of\ndocumentation webpages. Our experiments demonstrate that OASBuilder generalizes\nwell across hundreds of APIs, and produces valid OpenAPI specifications that\nencapsulate most of the information from the original documentation. OASBuilder\nhas been successfully implemented in an enterprise environment, saving\nthousands of hours of manual effort and making hundreds of complex enterprise\nAPIs accessible as tools for LLMs.", "AI": {"tldr": "OASBuilder automates the transformation of unstructured API documentation into machine-readable OpenAPI specs, saving significant manual effort and improving API accessibility for AI tools.", "motivation": "Extracting structured, machine-readable API specifications from online documentation is tedious and inefficient, as most API documentation is unstructured. Standardization is necessary for AI agents and automation tools to interact with APIs efficiently.", "method": "The authors designed OASBuilder, a framework that uses a pipeline combining large language models (LLMs) and rule-based algorithms. This system leverages domain knowledge about how documentation webpages are structured to convert unstructured HTML documents into OpenAPI specifications.", "result": "OASBuilder was tested on hundreds of APIs and successfully generalized to diverse documentation. It produced valid OpenAPI specifications covering most information from the original documents. Implementation in an enterprise setting significantly reduced manual effort and enabled better accessibility of APIs for LLM tools.", "conclusion": "OASBuilder automates the conversion of unstructured API documentation into standardized OpenAPI specs, demonstrating robust generalization, practical enterprise value, and notable time savings."}}
{"id": "2507.05325", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05325", "abs": "https://arxiv.org/abs/2507.05325", "authors": ["Lidiany Cerqueira", "Jo\u00e3o Pedro Bastos", "Danilo Neves", "Glauco Carneiro", "Rodrigo Sp\u00ednola", "S\u00e1vio Freire", "Jos\u00e9 Amancio Macedo Santos", "Manoel Mendon\u00e7a"], "title": "Exploring Empathy in Software Engineering: Insights from a Grey Literature Analysis of Practitioners' Perspectives", "comment": "This is the author's version of the paper accepted for publication in\n  ACM Transactions on Software Engineering and Methodology. The final version\n  will be available via the ACM Digital Library. The HTML preview may not\n  render some formatting correctly. Please refer to the PDF version for\n  accurate presentation", "summary": "Context. Empathy, a key social skill, is essential for communication and\ncollaboration in SE but remains an under-researched topic. Aims. This study\ninvestigates empathy in SE from practitioners' perspectives, aiming to\ncharacterize its meaning, identify barriers, discuss practices to overcome\nthem, and explore its effects. Method. A qualitative content analysis was\nconducted on 55 web articles from DEV and Medium, two communities widely used\nby practitioners. To strengthen our findings, we conducted a follow-up survey\nwith empathy experts. Results. The study proposes a definition of empathy in\nSE, identifies barriers such as toxic culture and excessive technical focus,\npractices to foster empathy in teams, and outcomes, including improved\ncollaboration, communication, and reduced anxiety, frustration, and stress.\nThese findings are synthesized into a conceptual framework. Conclusion. Survey\nresults indicate the framework is clear, valuable, and raises empathy\nawareness, with suggestions for improvements and integration into training.\nThis study paves the way for improving team dynamics by addressing barriers and\noffering strategies to cultivate empathy. Future work will explore empathy's\nbroader implications in SE practice.", "AI": {"tldr": "Empathy is vital yet understudied in software engineering. By analyzing practitioner articles and surveying experts, this study defines empathy in SE, pinpoints barriers, suggests fostering practices, and synthesizes these into a practical framework validated by experts. The work highlights empathy\u2019s benefits and lays groundwork for future research and training integration.", "motivation": "Empathy is crucial for communication and collaboration in software engineering (SE) but is underexplored as a research topic. The authors aim to fill this gap by understanding what empathy means to SE practitioners, the barriers they face, and how empathy can be promoted.", "method": "The study used qualitative content analysis on 55 web articles from DEV and Medium, both popular practitioner communities. It also included a follow-up survey with empathy experts to validate and enhance the findings.", "result": "The study defined empathy in SE, identified major barriers (such as toxic culture and excessive technical focus), and recommended practices to foster empathy. Outcomes observed were improved collaboration and communication, as well as reduced anxiety, frustration, and stress. These were synthesized into a conceptual framework.", "conclusion": "The proposed framework is clear and valuable, raising awareness of empathy and providing actionable suggestions for integration into training. The research encourages addressing empathy barriers and adopting strategies to enhance team dynamics. Future research will further investigate empathy's wider impact in SE."}}
{"id": "2507.05504", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05504", "abs": "https://arxiv.org/abs/2507.05504", "authors": ["Alex Kleijwegt", "Sinem Getir Yaman", "Radu Calinescu"], "title": "Tool for Supporting Debugging and Understanding of Normative Requirements Using LLMs", "comment": null, "summary": "Normative requirements specify social, legal, ethical, empathetic, and\ncultural (SLEEC) norms that must be observed by a system. To support the\nidentification of SLEEC requirements, numerous standards and regulations have\nbeen developed. These requirements are typically defined by stakeholders in the\nnon-technical system with diverse expertise (e.g., ethicists, lawyers, social\nscientists). Hence, ensuring their consistency and managing the requirement\nelicitation process are complex and error-prone tasks. Recent research has\naddressed this challenge using domain-specific languages to specify normative\nrequirements as rules, whose consistency can then be analyzed with formal\nmethods. Nevertheless, these approaches often present the results from formal\nverification tools in a way that is inaccessible to non-technical users. This\nhinders understanding and makes the iterative process of eliciting and\nvalidating these requirements inefficient in terms of both time and effort. To\naddress this problem, we introduce SLEEC-LLM, a tool that uses large language\nmodels (LLMs) to provide natural-language interpretations for model-checking\ncounterexamples corresponding to SLEEC rule inconsistencies. SLEEC-LLM improves\nthe efficiency and explainability of normative requirements elicitation and\nconsistency analysis. To demonstrate its effectiveness, we summarise its use in\ntwo real-world case studies involving non-technical stakeholders.", "AI": {"tldr": "SLEEC-LLM uses large language models to translate complex formal analysis of normative (SLEEC) requirements into understandable natural-language feedback for non-technical stakeholders, improving efficiency and clarity in requirements engineering as shown in two real-world case studies.", "motivation": "Normative requirements involving social, legal, ethical, empathetic, and cultural (SLEEC) norms are complex to define and manage due to their multidisciplinary nature. Non-technical stakeholders often struggle to understand the formal results of consistency analyses, making the requirements elicitation process inefficient.", "method": "The paper introduces SLEEC-LLM, a tool that leverages large language models (LLMs) to convert formal model-checking counterexamples related to SLEEC rule inconsistencies into natural-language explanations, making these insights accessible to non-technical stakeholders.", "result": "SLEEC-LLM was used in two real-world case studies with non-technical users and demonstrated improved efficiency and explainability in the elicitation and analysis of normative requirements.", "conclusion": "SLEEC-LLM supports a more understandable and efficient process for non-technical stakeholders in eliciting and validating SLEEC requirements by generating human-friendly explanations of formal analysis results, thereby improving the overall requirements engineering workflow."}}
{"id": "2507.05565", "categories": ["cs.SE", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.05565", "abs": "https://arxiv.org/abs/2507.05565", "authors": ["Sangwon Hyun", "Shaukat Ali", "M. Ali Babar"], "title": "Search-based Selection of Metamorphic Relations for Optimized Robustness Testing of Large Language Models", "comment": null, "summary": "Assessing the trustworthiness of Large Language Models (LLMs), such as\nrobustness, has garnered significant attention. Recently, metamorphic testing\nthat defines Metamorphic Relations (MRs) has been widely applied to evaluate\nthe robustness of LLM executions. However, the MR-based robustness testing\nstill requires a scalable number of MRs, thereby necessitating the optimization\nof selecting MRs. Most extant LLM testing studies are limited to automatically\ngenerating test cases (i.e., MRs) to enhance failure detection. Additionally,\nmost studies only considered a limited test space of single perturbation MRs in\ntheir evaluation of LLMs. In contrast, our paper proposes a search-based\napproach for optimizing the MR groups to maximize failure detection and\nminimize the LLM execution cost. Moreover, our approach covers the\ncombinatorial perturbations in MRs, facilitating the expansion of test space in\nthe robustness assessment. We have developed a search process and implemented\nfour search algorithms: Single-GA, NSGA-II, SPEA2, and MOEA/D with novel\nencoding to solve the MR selection problem in the LLM robustness testing. We\nconducted comparative experiments on the four search algorithms along with a\nrandom search, using two major LLMs with primary Text-to-Text tasks. Our\nstatistical and empirical investigation revealed two key findings: (1) the\nMOEA/D algorithm performed the best in optimizing the MR space for LLM\nrobustness testing, and (2) we identified silver bullet MRs for the LLM\nrobustness testing, which demonstrated dominant capabilities in confusing LLMs\nacross different Text-to-Text tasks. In LLM robustness assessment, our research\nsheds light on the fundamental problem for optimized testing and provides\ninsights into search-based solutions.", "AI": {"tldr": "This paper introduces and evaluates a search-based method, especially using MOEA/D, to optimize the selection of diverse and combinatorial test cases (MRs) for robust and efficient LLM robustness testing, finding 'silver bullet' MRs that reliably reveal model weaknesses.", "motivation": "Current robustness testing of Large Language Models (LLMs) using Metamorphic Relations (MRs) requires selecting a large and diverse set of MRs, which can be inefficient and cost-ineffective. Existing studies mostly focus on generating individual MRs or single perturbations, rather than optimizing MR selection or exploring more complex combinations.", "method": "The paper proposes a search-based approach to optimize groups of Metamorphic Relations (MRs) for robustness testing of LLMs. Four search algorithms (Single-GA, NSGA-II, SPEA2, MOEA/D) with novel encoding are implemented to find optimal combinations of MRs. These methods allow the inclusion of combinatorial perturbations, expanding the testing space. Comparative experiments are conducted using these algorithms and a random search on two major LLMs in Text-to-Text tasks.", "result": "The empirical study found that the MOEA/D search algorithm outperformed others in optimizing MR selection for robustness testing. Additionally, the study discovered certain 'silver bullet' MRs that were particularly effective at confusing LLMs across various tasks.", "conclusion": "The research addresses the key challenge of optimizing metamorphic testing for LLM robustness. It demonstrates the superiority of search-based solutions, especially MOEA/D, for selecting efficient and effective MR sets. The findings provide practical insights for developing scalable and cost-effective LLM testing frameworks."}}
{"id": "2507.05932", "categories": ["cs.SE", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.05932", "abs": "https://arxiv.org/abs/2507.05932", "authors": ["You Lu", "Dingji Wang", "Kaifeng Huang", "Bihuan Chen", "Xin Peng"], "title": "TigAug: Data Augmentation for Testing Traffic Light Detection in Autonomous Driving Systems", "comment": null, "summary": "Autonomous vehicle technology has been developed in the last decades with\nrecent advances in sensing and computing technology. There is an urgent need to\nensure the reliability and robustness of autonomous driving systems (ADSs).\nDespite the recent achievements in testing various ADS modules, little\nattention has been paid on the automated testing of traffic light detection\nmodels in ADSs. A common practice is to manually collect and label traffic\nlight data. However, it is labor-intensive, and even impossible to collect\ndiverse data under different driving environments.\n  To address these problems, we propose and implement TigAug to automatically\naugment labeled traffic light images for testing traffic light detection models\nin ADSs. We construct two families of metamorphic relations and three families\nof transformations based on a systematic understanding of weather environments,\ncamera properties, and traffic light properties. We use augmented images to\ndetect erroneous behaviors of traffic light detection models by\ntransformation-specific metamorphic relations, and to improve the performance\nof traffic light detection models by retraining. Large-scale experiments with\nfour state-of-the-art traffic light detection models and two traffic light\ndatasets have demonstrated that i) TigAug is effective in testing traffic light\ndetection models, ii) TigAug is efficient in synthesizing traffic light images,\nand iii) TigAug generates traffic light images with acceptable naturalness.", "AI": {"tldr": "Manual data collection for testing traffic light detection in autonomous vehicles is limited and laborious. The proposed tool, TigAug, automatically augments labeled images using transformations based on environment and sensor factors. It allows more effective, efficient testing and retraining of detection models, as validated by large-scale experiments.", "motivation": "Ensuring the reliability and robustness of autonomous driving systems (ADSs) is crucial, but automated testing of traffic light detection models in these systems has received little attention. The current practice of manually collecting and labeling traffic light data is labor-intensive and often fails to capture diverse driving environments, making thorough testing impractical.", "method": "The authors propose and implement TigAug, a system that automatically augments labeled traffic light images for testing ADS traffic light detection models. TigAug constructs two families of metamorphic relations and three families of image transformations, which consider weather conditions, camera properties, and traffic light characteristics. These augmented images are used for both detecting faults in detection models (via metamorphic testing) and improving them through retraining.", "result": "Large-scale experiments with four state-of-the-art traffic light detection models and two datasets show that TigAug is effective in testing detection models, efficient in image synthesis, and produces images with satisfactory naturalness. Furthermore, retraining with TigAug-augmented data improves model performance.", "conclusion": "TigAug offers an automated, efficient, and effective approach for generating diverse traffic light images to test and enhance the robustness of traffic light detection models in ADSs. Its use of metamorphic testing and targeted augmentations both improves error detection and model robustness, addressing a significant gap in ADS testing practices."}}
{"id": "2507.05981", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05981", "abs": "https://arxiv.org/abs/2507.05981", "authors": ["Marc Oriol", "Quim Motger", "Jordi Marco", "Xavier Franch"], "title": "Multi-Agent Debate Strategies to Enhance Requirements Engineering with Large Language Models", "comment": null, "summary": "Context: Large Language Model (LLM) agents are becoming widely used for\nvarious Requirements Engineering (RE) tasks. Research on improving their\naccuracy mainly focuses on prompt engineering, model fine-tuning, and retrieval\naugmented generation. However, these methods often treat models as isolated\nblack boxes - relying on single-pass outputs without iterative refinement or\ncollaboration, limiting robustness and adaptability. Objective: We propose\nthat, just as human debates enhance accuracy and reduce bias in RE tasks by\nincorporating diverse perspectives, different LLM agents debating and\ncollaborating may achieve similar improvements. Our goal is to investigate\nwhether Multi-Agent Debate (MAD) strategies can enhance RE performance. Method:\nWe conducted a systematic study of existing MAD strategies across various\ndomains to identify their key characteristics. To assess their applicability in\nRE, we implemented and tested a preliminary MAD-based framework for RE\nclassification. Results: Our study identified and categorized several MAD\nstrategies, leading to a taxonomy outlining their core attributes. Our\npreliminary evaluation demonstrated the feasibility of applying MAD to RE\nclassification. Conclusions: MAD presents a promising approach for improving\nLLM accuracy in RE tasks. This study provides a foundational understanding of\nMAD strategies, offering insights for future research and refinements in RE\napplications.", "AI": {"tldr": "The paper explores using Multi-Agent Debate among LLMs to improve performance in Requirements Engineering tasks, finding that collaborative debate strategies are feasible and potentially valuable. They provide a taxonomy of MAD methods and show initial positive results in RE classification.", "motivation": "Traditional methods for improving LLM agents' accuracy in Requirements Engineering (RE) tasks often view models as isolated entities, producing single-pass outputs. This limits their robustness and adaptability, contrasting with how human debate enhances accuracy and reduces bias by incorporating multiple viewpoints.", "method": "The authors performed a systematic study of Multi-Agent Debate (MAD) strategies across various domains, categorizing their main characteristics and building a taxonomy. They also implemented and tested a preliminary MAD-based framework for RE classification tasks to assess the approach's feasibility.", "result": "The study identified and categorized several MAD strategies and formulated a taxonomy of their core features. Preliminary tests of their MAD framework in RE classification showed the feasibility of the approach.", "conclusion": "Multi-Agent Debate (MAD) holds promise for enhancing LLM agent accuracy in RE tasks. The study lays groundwork for understanding MAD strategies and informs future research and improvements in the field."}}
{"id": "2507.05995", "categories": ["cs.SE", "68Nxx", "D.2.0; D.2.8"], "pdf": "https://arxiv.org/pdf/2507.05995", "abs": "https://arxiv.org/abs/2507.05995", "authors": ["Pengzhou Chen", "Tao Chen"], "title": "PromiseTune: Unveiling Causally Promising and Explainable Configuration Tuning", "comment": "This paper has been accepted by ICSE26", "summary": "The high configurability of modern software systems has made configuration\ntuning a crucial step for assuring system performance, e.g., latency or\nthroughput. However, given the expensive measurements, large configuration\nspace, and rugged configuration landscape, existing tuners suffer\nineffectiveness due to the difficult balance of budget utilization between\nexploring uncertain regions (for escaping from local optima) and exploiting\nguidance of known good configurations (for fast convergence). The root cause is\nthat we lack knowledge of where the promising regions lay, which also causes\nchallenges in the explainability of the results.\n  In this paper, we propose PromiseTune that tunes configuration guided by\ncausally purified rules. PromiseTune is unique in the sense that we learn\nrules, which reflect certain regions in the configuration landscape, and purify\nthem with causal inference. The remaining rules serve as approximated\nreflections of the promising regions, bounding the tuning to emphasize these\nplaces in the landscape. This, as we demonstrate, can effectively mitigate the\nimpact of the exploration and exploitation trade-off. Those purified regions\ncan then be paired with the measured configurations to provide spatial\nexplainability at the landscape level. Comparing with 11 state-of-the-art\ntuners on 12 systems and varying budgets, we show that PromiseTune performs\nsignificantly better than the others with $42\\%$ superior rank to the overall\nsecond best while providing richer information to explain the hidden system\ncharacteristics.", "AI": {"tldr": "PromiseTune uses causally purified rules to guide configuration tuning, outperforming 11 other tuners by 42% and providing better system explainability.", "motivation": "Modern software systems offer vast configuration options, making performance tuning both critical and complex. The challenges arise due to the need to balance exploration (searching for new good configurations) and exploitation (refining known good ones), compounded by costly measurements, large configuration spaces, and unpredictable performance landscapes. Lack of understanding about where promising configurations lie also hampers both effectiveness and explainability in tuning.", "method": "The authors introduce PromiseTune, a new configuration tuner that leverages rules derived from prior knowledge and purifies them using causal inference. These purified rules highlight promising regions in the configuration landscape, guiding the tuner to focus on these areas while offering explainable results. This approach aims to balance exploration and exploitation more effectively.", "result": "PromiseTune was evaluated against 11 state-of-the-art tuners across 12 systems with varying tuning budgets. Results showed that PromiseTune outperformed all competitors, achieving a 42% superior rank compared to the second-best tuner and offering improved explainability of system behaviors.", "conclusion": "PromiseTune provides a more effective and explainable method for configuration tuning by utilizing causally purified rules to guide search in software configuration landscapes, yielding improved performance and richer insights into system behavior compared to previous methods."}}
{"id": "2507.06014", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.06014", "abs": "https://arxiv.org/abs/2507.06014", "authors": ["Tim Puhlf\u00fcr\u00df", "Julia Butzke", "Walid Maalej"], "title": "Model Cards Revisited: Bridging the Gap Between Theory and Practice for Ethical AI Requirements", "comment": "Accepted for publication at the 33rd IEEE International Requirements\n  Engineering 2025 conference", "summary": "Model cards are the primary documentation framework for developers of\nartificial intelligence (AI) models to communicate critical information to\ntheir users. Those users are often developers themselves looking for relevant\ndocumentation to ensure that their AI systems comply with the ethical\nrequirements of existing laws, guidelines, and standards. Recent studies\nindicate inadequate model documentation practices, suggesting a gap between AI\nrequirements and current practices in model documentation. To understand this\ngap and provide actionable guidance to bridge it, we conducted a thematic\nanalysis of 26 guidelines on ethics and AI, three AI documentation frameworks,\nthree quantitative studies of model cards, and ten actual model cards. We\nidentified a total of 43 ethical requirements relevant to model documentation\nand organized them into a taxonomy featuring four themes and twelve sub-themes\nrepresenting ethical principles. Our findings indicate that model developers\npredominantly emphasize model capabilities and reliability in the documentation\nwhile overlooking other ethical aspects, such as explainability, user autonomy,\nand fairness. This underscores the need for enhanced support in documenting\nethical AI considerations. Our taxonomy serves as a foundation for a revised\nmodel card framework that holistically addresses ethical AI requirements.", "AI": {"tldr": "Most AI model documentation misses out on important ethical principles. The authors analyzed guidelines and existing model cards and developed a taxonomy to help improve documentation to better address ethical concerns.", "motivation": "There is a growing concern that current AI model documentation (model cards) does not sufficiently address ethical requirements mandated by laws, guidelines, and standards. This creates a gap between required ethical practices and real-world documentation.", "method": "The authors conducted a thematic analysis of 26 ethics and AI guidelines, three AI documentation frameworks, three quantitative studies of model cards, and ten real model cards to identify ethical requirements and evaluate the current state of model documentation.", "result": "The study identified 43 ethical requirements relevant to model documentation and grouped them into a taxonomy with four themes and twelve sub-themes covering various ethical principles. It found that most model cards focus on model capabilities and reliability but neglect important aspects such as explainability, user autonomy, and fairness.", "conclusion": "Current model documentation predominantly overlooks several key ethical principles. The taxonomy developed in this research can guide the creation of more comprehensive model card frameworks, helping ensure that ethical requirements are systematically addressed in AI documentation."}}
