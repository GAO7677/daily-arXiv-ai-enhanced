<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 11]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [SWE-Effi: Re-Evaluating Software AI Agent System Effectiveness Under Resource Constraints](https://arxiv.org/abs/2509.09853)
*Zhiyu Fan,Kirill Vasilevski,Dayi Lin,Boyuan Chen,Yihao Chen,Zhiqing Zhong,Jie M. Zhang,Pinjia He,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: This paper introduces new metrics (SWE-Effi) to evaluate AI systems for software engineering tasks by considering both accuracy and resource consumption, revealing key trade-offs and system integration challenges that are overlooked by traditional accuracy-centered benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing AI benchmarks for software engineering, such as SWE-bench, evaluate systems mainly on solution accuracy, neglecting considerations of resource efficiency and overall effectiveness. In a resource-limited real-world context, it is crucial for AI systems to be not only accurate but also cost-effective, balancing output quality with resources consumed.

Method: The paper introduces SWE-Effi, a set of new metrics designed to assess AI systems holistically by evaluating both the accuracy of their outcomes and the resources required (such as computational tokens and time). The authors re-rank popular AI systems using these metrics on a subset of SWE-bench, providing a multidimensional analysis of performance.

Result: The study finds that AI effectiveness hinges not only on the system's underlying scaffold but also on its integration with the base model. Major challenges identified include the 'token snowball' effect, where systems consume ever-increasing resources, and so-called 'expensive failures,' where agents waste substantial resources on unsolvable problems. Moreover, the paper reveals a trade-off between efficiency judged by tokens used and efficiency judged by time, both critical for managing budgets and scalable RL training.

Conclusion: SWE-Effi fills a significant gap in AI evaluation for software engineering by incorporating effectiveness as a balance of accuracy and resource consumption. The new metrics expose challenges and trade-offs in current AI designs, informing better system integration and practical deployment strategies. Managing the trade-off between resource types is essential for real-world application and scalable reinforcement learning environments.

Abstract: The advancement of large language models (LLMs) and code agents has
demonstrated significant potential to assist software engineering (SWE) tasks,
such as autonomous issue resolution and feature addition. Existing AI for
software engineering leaderboards (e.g., SWE-bench) focus solely on solution
accuracy, ignoring the crucial factor of effectiveness in a
resource-constrained world. This is a universal problem that also exists beyond
software engineering tasks: any AI system should be more than correct - it must
also be cost-effective. To address this gap, we introduce SWE-Effi, a set of
new metrics to re-evaluate AI systems in terms of holistic effectiveness
scores. We define effectiveness as the balance between the accuracy of outcome
(e.g., issue resolve rate) and the resources consumed (e.g., token and time).
In this paper, we specifically focus on the software engineering scenario by
re-ranking popular AI systems for issue resolution on a subset of the SWE-bench
benchmark using our new multi-dimensional metrics. We found that AI system's
effectiveness depends not just on the scaffold itself, but on how well it
integrates with the base model, which is key to achieving strong performance in
a resource-efficient manner. We also identified systematic challenges such as
the "token snowball" effect and, more significantly, a pattern of "expensive
failures". In these cases, agents consume excessive resources while stuck on
unsolvable tasks - an issue that not only limits practical deployment but also
drives up the cost of failed rollouts during RL training. Lastly, we observed a
clear trade-off between effectiveness under the token budget and effectiveness
under the time budget, which plays a crucial role in managing project budgets
and enabling scalable reinforcement learning, where fast responses are
essential.

</details>


### [2] [From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI Ecosystem](https://arxiv.org/abs/2509.09873)
*James Jewitt,Hao Li,Bram Adams,Gopi Krishnan Rajbahadur,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: This paper reveals widespread license conflicts in open-source AI models and datasets, especially when integrated into software applications. The authors audited millions of resources on Hugging Face and GitHub, finding over a third of model-to-application transitions improperly relax license terms. They introduce a rule engine that can solve most detected conflicts and release both their dataset and the engine to support improved license compliance.


<details>
  <summary>Details</summary>
Motivation: Hidden license conflicts in the open-source AI ecosystem present significant legal and ethical risks which have not been systematically studied. There is a need for a data-driven understanding of the frequency, origins, and community impacts of these conflicts.

Method: Conducted a comprehensive audit of licenses for datasets and models on Hugging Face and their downstream integration into open-source software on GitHub. Analyzed 364k datasets, 1.6M models, 140k GitHub projects, and developed a rule engine encoding 200+ license clauses for automated conflict detection.

Result: Found systemic non-compliance: 35.5% of model-to-application transitions remove restrictive license clauses via relicensing under permissive terms. The prototype rule engine can detect and solve 86.4% of license conflicts in software applications.

Conclusion: License compliance is a major governance issue in open-source AI. The study offers data, a dataset, and tools (rule engine) for enabling automated, scalable, AI-aware license compliance.

Abstract: Hidden license conflicts in the open-source AI ecosystem pose serious legal
and ethical risks, exposing organizations to potential litigation and users to
undisclosed risk. However, the field lacks a data-driven understanding of how
frequently these conflicts occur, where they originate, and which communities
are most affected. We present the first end-to-end audit of licenses for
datasets and models on Hugging Face, as well as their downstream integration
into open-source software applications, covering 364 thousand datasets, 1.6
million models, and 140 thousand GitHub projects. Our empirical analysis
reveals systemic non-compliance in which 35.5% of model-to-application
transitions eliminate restrictive license clauses by relicensing under
permissive terms. In addition, we prototype an extensible rule engine that
encodes almost 200 SPDX and model-specific clauses for detecting license
conflicts, which can solve 86.4% of license conflicts in software applications.
To support future research, we release our dataset and the prototype engine.
Our study highlights license compliance as a critical governance challenge in
open-source AI and provides both the data and tools necessary to enable
automated, AI-aware compliance at scale.

</details>


### [3] [SLD-Spec: Enhancement LLM-assisted Specification Generation for Complex Loop Functions via Program Slicing and Logical Deletion](https://arxiv.org/abs/2509.09917)
*Zehan Chen,Long Zhang,Zhiwei Zhang,JingJing Zhang,Ruoyu Zhou,Yulong Shen,JianFeng Ma,Lin Yang*

Main category: cs.SE

TL;DR: SLD-Spec is an LLM-driven method for generating formal specifications from code with complex loops. By combining program slicing and LLM-based logical deletion, SLD-Spec outperforms existing approaches in correctness, relevance, completeness, and efficiency of specification generation, enabling more reliable automated program verification.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based approaches to generating formal specifications from program code struggle with complex loop structures, resulting in irrelevant, incomplete, or ambiguous specifications. Verification tools' rigorous constraints further exacerbate these issues.

Method: The paper proposes SLD-Spec, an LLM-assisted specification generation method specifically designed for programs with complex loops. SLD-Spec introduces two key phases: program slicing to isolate independent loop structures, and logical deletion, using LLMs to filter out incorrect specifications.

Result: Experiments show that SLD-Spec verifies five more programs and reduces runtime by 23.73% compared to AutoSpec on simple datasets. On a manually constructed complex loop program dataset, SLD-Spec enables 95.1% of assertions and 90.91% of programs to pass verification. Ablation studies indicate both logical deletion and program slicing are crucial for correctness, relevance, and completeness of specifications.

Conclusion: SLD-Spec substantially improves the correctness, relevance, and completeness of formal specification generation for complex loop programs over existing methods, making end-to-end program verification more efficient and reliable.

Abstract: Automatically generating formal specifications from program code can greatly
enhance the efficiency of program verification and enable end-to-end automation
from requirements to reliable software. However, existing LLM-based approaches
often struggle with programs that include complex loop structures, leading to
irrelevant specifications. Moreover, the rigorous proof obligations and design
constraints imposed by verification tools can further result in incomplete and
ambiguous specifications. To address these challenges, we propose SLD-Spec, an
LLM-assisted specification generation method tailored for programs with complex
loop constructs. SLD-Spec introduces two novel phases into the traditional
specification generation framework: (1) A slicing phase, which decomposes each
function into code fragments containing independent loop structures, thereby
reducing the complexity of specification generation; and (2) A logical deletion
phase, which applies LLM-based reasoning to filter out incorrect candidate
specifications--especially those not easily identified by verification
tool--while retaining valid ones. Experimental results show that on the simple
dataset, SLD-Spec successfully verifies five more programs than the
state-of-the-art AutoSpec and reduces runtime by 23.73%. To address the
limitations of existing research, we manually construct a dataset comprising
four categories of complex loop programs. On this dataset, SLD-Spec
significantly improves the correctness, relevance, and completeness of
generated specifications compared to baseline methods, enabling 95.1% of
assertions and 90.91% of programs to pass verification. Ablation studies
further reveal that logical deletion is critical for enhancing specification
correctness and relevance, while program slicing contributes significantly to
specification completeness. Our code and data are publicly available.

</details>


### [4] [WALL: A Web Application for Automated Quality Assurance using Large Language Models](https://arxiv.org/abs/2509.09918)
*Seyed Moein Abtahi,Akramul Azim*

Main category: cs.SE

TL;DR: WALL is a web app that streamlines code issue management using AI and automation. It detects issues, revises code, and evaluates changes automatically, reducing manual work while maintaining quality. Experiments show it is cost-efficient and effective, with future plans to improve automation and cut human involvement entirely.


<details>
  <summary>Details</summary>
Motivation: With the rising complexity of software projects, managing the increasing number and variety of code issues is challenging. There is a need for efficient, automated tools to detect, resolve, and evaluate these issues, minimizing manual effort.

Method: WALL, a web application, integrates SonarQube for issue detection and large language models (GPT-3.5 Turbo, GPT-4o) for automated issue resolution and revision evaluation. It is composed of three modules: issue extraction, code revision, and code comparison. Experiments were conducted on 563 files comprising 7,599 issues.

Result: WALL demonstrated its effectiveness in reducing human effort and maintaining high-quality code revisions. Using a mix of cost-effective and advanced LLMs led to lower costs and higher revision rates.

Conclusion: WALL provides an efficient, automated pipeline for code issue management. Future enhancements will focus on integrating open-source LLMs and achieving fully automated, human-free code quality management.

Abstract: As software projects become increasingly complex, the volume and variety of
issues in code files have grown substantially. Addressing this challenge
requires efficient issue detection, resolution, and evaluation tools. This
paper presents WALL, a web application that integrates SonarQube and large
language models (LLMs) such as GPT-3.5 Turbo and GPT-4o to automate these
tasks. WALL comprises three modules: an issue extraction tool, code issues
reviser, and code comparison tool. Together, they enable a seamless pipeline
for detecting software issues, generating automated code revisions, and
evaluating the accuracy of revisions. Our experiments, conducted on 563 files
with over 7,599 issues, demonstrate WALL's effectiveness in reducing human
effort while maintaining high-quality revisions. Results show that employing a
hybrid approach of cost-effective and advanced LLMs can significantly lower
costs and improve revision rates. Future work aims to enhance WALL's
capabilities by integrating open-source LLMs and eliminating human
intervention, paving the way for fully automated code quality management.

</details>


### [5] [Stencil-Lifting: Hierarchical Recursive Lifting System for Extracting Summary of Stencil Kernel in Legacy Codes](https://arxiv.org/abs/2509.10236)
*Mingyi Li,Junmin Xiao,Siyan Chen,Hui Ma,Xi Chen,Peihua Bao,Liang Yuan,Guangming Tan*

Main category: cs.SE

TL;DR: Stencil-Lifting automates efficient and correct conversion of legacy stencil kernels to DSLs, using novel recursive abstraction and synthesis methods. It greatly outperforms prior systems in speed and scalability, with provable correctness, making legacy code modernization more practical.


<details>
  <summary>Details</summary>
Motivation: Existing verified lifting systems for converting legacy stencil kernels to DSL implementations suffer from efficiency bottlenecks, particularly in scalability and performance. Bridging legacy stencil optimization techniques with modern DSL paradigms is challenging.

Method: Stencil-Lifting introduces two innovations: (1) a hierarchical recursive lifting theory based on invariant subgraphs and predicate-based summaries for nested loop semantics, ensuring correct loop invariants and eliminating external verification; (2) a hierarchical recursive lifting algorithm that converges efficiently and is formally proven to be complete, avoiding search-based synthesis inefficiencies.

Result: Stencil-Lifting outperforms state-of-the-art systems STNG and Dexter with up to 31.62× and 5.8× speedups respectively, while maintaining semantic equivalence. It demonstrates scalable abstraction and efficient translation for diverse benchmarks and real-world applications.

Conclusion: Stencil-Lifting significantly improves the efficiency and scalability of automated stencil kernel translation from low-level languages to DSLs, providing a theoretically sound and experimentally validated solution that maintains correctness and boosts performance.

Abstract: We introduce Stencil-Lifting, a novel system for automatically converting
stencil kernels written in low-level languages in legacy code into semantically
equivalent Domain-Specific Language (DSL) implementations. Targeting the
efficiency bottlenecks of existing verified lifting systems, Stencil-Lifting
achieves scalable stencil kernel abstraction through two key innovations.
First, we propose a hierarchical recursive lifting theory that represents
stencil kernels, structured as nested loops, using invariant subgraphs, which
are customized data dependency graphs that capture loop-carried computation and
structural invariants. Each vertex in the invariant subgraph is associated with
a predicate-based summary, encoding its computational semantics. By enforcing
self-consistency across these summaries, Stencil-Lifting ensures the derivation
of correct loop invariants and postconditions for nested loops, eliminating the
need for external verification. Second, we develop a hierarchical recursive
lifting algorithm that guarantees termination through a convergent recursive
process, avoiding the inefficiencies of search-based synthesis. The algorithm
efficiently derives the valid summaries of stencil kernels, and its
completeness is formally proven. We evaluate Stencil-Lifting on diverse stencil
benchmarks from two different suites and on four real-world applications.
Experimental results demonstrate that Stencil-Lifting achieves 31.62$\times$
and 5.8$\times$ speedups compared to the state-of-the-art verified lifting
systems STNG and Dexter, respectively, while maintaining full semantic
equivalence. Our work significantly enhances the translation efficiency of
low-level stencil kernels to DSL implementations, effectively bridging the gap
between legacy optimization techniques and modern DSL-based paradigms.

</details>


### [6] [Toward Green Code: Prompting Small Language Models for Energy-Efficient Code Generation](https://arxiv.org/abs/2509.09947)
*Humza Ashraf,Syed Muhammad Danish,Zeeshan Sattar*

Main category: cs.SE

TL;DR: Prompt engineering, especially Chain-of-Thought, can make some small code LLMs greener, but results depend on the specific model; not all SLMs benefit equally in energy savings.


<details>
  <summary>Details</summary>
Motivation: There is increasing concern about the environmental impact of large language models (LLMs), especially due to their high energy consumption and carbon emissions in software development. Small Language Models (SLMs) represent a more sustainable alternative, but their effectiveness and energy efficiency, especially aided by prompt engineering, need to be evaluated.

Method: The study evaluates four open-source SLMs (StableCode-Instruct-3B, Qwen2.5-Coder-3B-Instruct, CodeLlama-7B-Instruct, Phi-3-Mini-4K-Instruct) on 150 LeetCode Python problems (spread across three difficulty categories). Each model is tested with four prompting strategies: role prompting, zero-shot, few-shot, and chain-of-thought (CoT), and the energy use, memory, and runtime for the generated solutions are measured and compared to human-written baselines.

Result: CoT prompting reduced energy consumption for Qwen2.5-Coder and StableCode-3B, while CodeLlama-7B and Phi-3-Mini-4K did not outperform human baselines with any prompting strategy. The result indicates model-specific variation in the impact of prompting strategies on energy efficiency.

Conclusion: Prompt engineering, especially with Chain-of-Thought prompting, can improve the energy efficiency of certain SLMs, but the effectiveness is highly model-dependent. Well-chosen prompting strategies have the potential to make SLM-assisted code generation greener, but results will vary across different models.

Abstract: There is a growing concern about the environmental impact of large language
models (LLMs) in software development, particularly due to their high energy
use and carbon footprint. Small Language Models (SLMs) offer a more sustainable
alternative, requiring fewer computational resources while remaining effective
for fundamental programming tasks. In this study, we investigate whether prompt
engineering can improve the energy efficiency of SLMs in code generation. We
evaluate four open-source SLMs, StableCode-Instruct-3B,
Qwen2.5-Coder-3B-Instruct, CodeLlama-7B-Instruct, and Phi-3-Mini-4K-Instruct,
across 150 Python problems from LeetCode, evenly distributed into easy, medium,
and hard categories. Each model is tested under four prompting strategies: role
prompting, zero-shot, few-shot, and chain-of-thought (CoT). For every generated
solution, we measure runtime, memory usage, and energy consumption, comparing
the results with a human-written baseline. Our findings show that CoT prompting
provides consistent energy savings for Qwen2.5-Coder and StableCode-3B, while
CodeLlama-7B and Phi-3-Mini-4K fail to outperform the baseline under any
prompting strategy. These results highlight that the benefits of prompting are
model-dependent and that carefully designed prompts can guide SLMs toward
greener software development.

</details>


### [7] [Development of Automated Software Design Document Review Methods Using Large Language Models](https://arxiv.org/abs/2509.09975)
*Takasaburo Fukuda,Takao Nakagawa,Keisuke Miyazaki,Susumu Tokumoto*

Main category: cs.SE

TL;DR: This paper investigates using LLMs to automate software design document reviews. It identifies which review perspectives LLMs can handle, develops techniques for complex document comprehension, and demonstrates via experiments that LLMs (such as GPT) can detect inconsistencies, suggesting that LLMs can assist or partially automate the review process.


<details>
  <summary>Details</summary>
Motivation: Reviewing software design documents is a time-consuming and error-prone task when done manually. The study aims to explore whether large language models (LLMs) can automate and improve this process.

Method: The authors analyzed common review perspectives for design documents and identified 11 key perspectives. They evaluated the suitability of LLMs for each perspective and developed new techniques for LLMs to process complex design documents, including those with table data. Experiments were performed using GPT to evaluate its ability to detect inconsistencies in real business design documents.

Result: Experiments showed that LLMs can effectively review certain perspectives of design documents and are capable of identifying inconsistencies between design items and their descriptions.

Conclusion: The study concludes that general-purpose LLMs can automate parts of the software design document review process, particularly for detecting inconsistencies, and can supplement human reviewers.

Abstract: In this study, we explored an approach to automate the review process of
software design documents by using LLM. We first analyzed the review methods of
design documents and organized 11 review perspectives. Additionally, we
analyzed the issues of utilizing LLMs for these 11 review perspectives and
determined which perspectives can be reviewed by current general-purpose LLMs
instead of humans. For the reviewable perspectives, we specifically developed
new techniques to enable LLMs to comprehend complex design documents that
include table data. For evaluation, we conducted experiments using GPT to
assess the consistency of design items and descriptions across different design
documents in the design process used in actual business operations. Our results
confirmed that LLMs can be utilized to identify inconsistencies in software
design documents during the review process.

</details>


### [8] [Sustaining Research Software: A Fitness Function Approach](https://arxiv.org/abs/2509.10085)
*Philipp Zech,Irdin Pekaric*

Main category: cs.SE

TL;DR: This paper suggests using automated fitness functions to ensure research software stays maintainable, adaptable, and FAIR over time, helping to transform short-term projects into long-lasting scientific assets.


<details>
  <summary>Details</summary>
Motivation: Research software often becomes unsustainable due to issues like poor maintainability and obsolescence, limiting its long-term usefulness and scientific impact.

Method: The paper introduces the use of fitness functions, inspired by evolutionary architecture, as automated metrics continually evaluated during the software development lifecycle. These functions are specifically tailored to monitor and encourage FAIR (Findability, Accessibility, Interoperability, and Reusability) principles in research software.

Result: Through case studies and experiments, the paper shows that integrating fitness functions improves the sustainability and FAIR qualities of research software.

Conclusion: Applying fitness functions encourages better software practices and supports the long-term sustainability and relevance of research software within the scientific community.

Abstract: The long-term sustainability of research software is a critical challenge, as
it usually suffers from poor maintainability, lack of adaptability, and
eventual obsolescence. This paper proposes a novel approach to addressing this
issue by leveraging the concept of fitness functions from evolutionary
architecture. Fitness functions are automated, continuously evaluated metrics
designed to ensure that software systems meet desired non-functional,
architectural qualities over time. We define a set of fitness functions
tailored to the unique requirements of research software, focusing on
findability, accessibility, interoperability and reusability (FAIR). These
fitness functions act as proactive safeguards, promoting practices such as
modular design, comprehensive documentation, version control, and compatibility
with evolving technological ecosystems. By integrating these metrics into the
development life cycle, we aim to foster a culture of sustainability within the
research community. Case studies and experimental results demonstrate the
potential of this approach to enhance the long-term FAIR of research software,
bridging the gap between ephemeral project-based development and enduring
scientific impact.

</details>


### [9] [Generating Energy-Efficient Code via Large-Language Models -- Where are we now?](https://arxiv.org/abs/2509.10099)
*Radu Apsan,Vincenzo Stoico,Michel Albonico,Rudra Dhar,Karthik Vaidhyanathan,Ivano Malavolta*

Main category: cs.SE

TL;DR: Code from Green software experts is always more energy-efficient than LLM-generated code; human-written code generally beats LLMs except on PCs. Prompts and hardware type affect results, but LLMs still can't fully replace humans for energy-efficient Python yet.


<details>
  <summary>Details</summary>
Motivation: With the increasing use of LLMs in code generation, there is a need to understand their impact on software energy efficiency, especially compared to human developers and experts in Green software.

Method: Empirical evaluation using 363 solutions to 9 coding problems (EvoEval benchmark) generated by 6 LLMs with 4 prompting techniques, compared to human and Green software expert code. Energy use measured on server, PC, and Raspberry Pi over ~881 hours.

Result: Human-written code is generally more energy-efficient than LLM-generated code on most hardware, except for the PC. Green software expert consistently produces the most energy-efficient code on all platforms. Prompting technique does not guarantee energy savings or consistency.

Conclusion: LLM-generated code, while capable, does not surpass the energy efficiency of code written by experienced Green software developers, indicating the continued need for human expertise for energy-efficient Python coding.

Abstract: Context. The rise of Large Language Models (LLMs) has led to their widespread
adoption in development pipelines. Goal. We empirically assess the energy
efficiency of Python code generated by LLMs against human-written code and code
developed by a Green software expert. Method. We test 363 solutions to 9 coding
problems from the EvoEval benchmark using 6 widespread LLMs with 4 prompting
techniques, and comparing them to human-developed solutions. Energy consumption
is measured on three different hardware platforms: a server, a PC, and a
Raspberry Pi for a total of ~881h (36.7 days). Results. Human solutions are 16%
more energy-efficient on the server and 3% on the Raspberry Pi, while LLMs
outperform human developers by 25% on the PC. Prompting does not consistently
lead to energy savings, where the most energy-efficient prompts vary by
hardware platform. The code developed by a Green software expert is
consistently more energy-efficient by at least 17% to 30% against all LLMs on
all hardware platforms. Conclusions. Even though LLMs exhibit relatively good
code generation capabilities, no LLM-generated code was more energy-efficient
than that of an experienced Green software developer, suggesting that as of
today there is still a great need of human expertise for developing
energy-efficient Python code.

</details>


### [10] [Targeted Test Selection Approach in Continuous Integration](https://arxiv.org/abs/2509.10279)
*Pavel Plyusnin,Aleksey Antonov,Vasilii Ermakov,Aleksandr Khaybriev,Margarita Kikot,Ilseyar Alimova,Stanislav Moiseev*

Main category: cs.SE

TL;DR: The paper presents Targeted Test Selection (T-TS), a machine learning method for selecting relevant tests in large software projects without relying on coverage maps. T-TS achieves major reductions in test execution time and pipeline duration while maintaining high fault detection rates, and is publicly released for use and further research.


<details>
  <summary>Details</summary>
Motivation: Change-based testing is vital in modern software development, but managing testing processes is increasingly difficult as codebases and test suites grow, especially with frequent code commits.

Method: The paper introduces Targeted Test Selection (T-TS), a machine learning approach that represents code commits as Bags-of-Words of changed files and uses additional cross-file and predictive features, deliberately excluding coverage maps.

Result: T-TS, when deployed in production, selected only 15% of tests, reduced execution time by 5.9 times, sped up the pipeline by 5.6 times, and detected over 95% of test failures, outperforming industry standards and other methods.

Conclusion: T-TS is an efficient and effective approach for test selection in industrial software development, providing significant speed-ups with high fault detection rates, and is available for public adoption and research.

Abstract: In modern software development change-based testing plays a crucial role.
However, as codebases expand and test suites grow, efficiently managing the
testing process becomes increasingly challenging, especially given the high
frequency of daily code commits. We propose Targeted Test Selection (T-TS), a
machine learning approach for industrial test selection. Our key innovation is
a data representation that represent commits as Bags-of-Words of changed files,
incorporates cross-file and additional predictive features, and notably avoids
the use of coverage maps. Deployed in production, T-TS was comprehensively
evaluated against industry standards and recent methods using both internal and
public datasets, measuring time efficiency and fault detection. On live
industrial data, T-TS selects only 15% of tests, reduces execution time by
$5.9\times$, accelerates the pipeline by $5.6\times$, and detects over 95% of
test failures. The implementation is publicly available to support further
research and practical adoption.

</details>


### [11] [Developer-LLM Conversations: An Empirical Study of Interactions and Generated Code Quality](https://arxiv.org/abs/2509.10402)
*Suzhen Zhong,Ying Zou,Bram Adams*

Main category: cs.SE

TL;DR: The study analyzed over 80,000 real conversations between developers and LLMs, showing that LLM-generated code often has errors that persist unless directly addressed. Explicitly pointing out mistakes helps improve code quality, with certain improvements noticed over multiple turns, especially in documentation and imports.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are widely used in software engineering to assist developers, but there is limited empirical understanding of actual developer-LLM interactions and how these influence code quality and development workflow outcomes.

Method: The authors analyze CodeChat, a large dataset with 82,845 real developer-LLM conversations involving code generation across 20+ programming languages, to examine interaction patterns, conversation evolution, common topics, and the effectiveness of problem resolution during multi-turn dialogue.

Result: LLM responses are much longer than prompts. Most conversations are multi-turn, evolving due to changing requirements or clarification. The most common tasks are web design and neural network training. LLM-generated code contains recurring and language-specific issues. However, in some languages, code quality (e.g., documentation in Java, imports in Python) improves slightly across conversation turns if error corrections are explicitly requested.

Conclusion: LLM-machine conversations are iterative and error-prone, but explicit error signaling in prompts improves code quality. Understanding these dynamics helps improve LLM integration in programming workflows and can guide better tool and prompt design.

Abstract: Large Language Models (LLMs) are becoming integral to modern software
development workflows, assisting developers with code generation, API
explanation, and iterative problem-solving through natural language
conversations. Despite widespread adoption, there is limited understanding of
how developers interact with LLMs in practice and how these conversational
dynamics influence task outcomes, code quality, and software engineering
workflows. To address this, we leverage CodeChat, a large dataset comprising
82,845 real-world developer-LLM conversations, containing 368,506 code snippets
generated across over 20 programming languages, derived from the WildChat
dataset. We find that LLM responses are substantially longer than developer
prompts, with a median token-length ratio of 14:1. Multi-turn conversations
account for 68% of the dataset and often evolve due to shifting requirements,
incomplete prompts, or clarification requests. Topic analysis identifies web
design (9.6% of conversations) and neural network training (8.7% of
conversations) as the most frequent LLM-assisted tasks. Evaluation across five
languages (i.e., Python, JavaScript, C++, Java, and C#) reveals prevalent and
language-specific issues in LLM-generated code: generated Python and JavaScript
code often include undefined variables (83.4% and 75.3% of code snippets,
respectively); Java code lacks required comments (75.9%); C++ code frequently
omits headers (41.1%) and C# code shows unresolved namespaces (49.2%). During a
conversation, syntax and import errors persist across turns; however,
documentation quality in Java improves by up to 14.7%, and import handling in
Python improves by 3.7% over 5 turns. Prompts that point out mistakes in code
generated in prior turns and explicitly request a fix are most effective for
resolving errors.

</details>
