<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 20]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [$\texttt{Droid}$: A Resource Suite for AI-Generated Code Detection](https://arxiv.org/abs/2507.10583)
*Daniil Orel,Indraneil Paul,Iryna Gurevych,Preslav Nakov*

Main category: cs.SE

TL;DR: The paper introduces DroidCollection, a large dataset for machine-generated code detection, and DroidDetect, a set of robust detectors. The study shows that existing detectors lack generalizability but that training with adversarial examples and using advanced learning techniques greatly improves performance and robustness.


<details>
  <summary>Details</summary>
Motivation: There is a lack of extensive, diverse open datasets for training and evaluating detectors that identify machine-generated code, and existing detectors often struggle to generalize across programming languages and coding domains. Adversarial techniques can also evade detection, highlighting the need for robust models.

Method: The authors compiled DroidCollection, a large open dataset containing over a million code samples, across seven programming languages, and from 43 coding models, including adversarial and human-AI co-authored code. They developed DroidDetect, encoder-only detectors using multi-task objectives trained on DroidCollection, and experimented with metric learning and uncertainty-based resampling to handle noisy data and improve robustness.

Result: Experiments showed that current detectors lack generalizability across diverse languages and domains, and are vulnerable to evasion via superficial humanisation techniques. However, adding a small amount of adversarial data in training can substantially improve robustness. Metric learning and uncertainty-based resampling further enhanced detector performance when exposed to noisy data distributions.

Conclusion: DroidCollection and DroidDetect provide strong, diverse resources and methods for building robust machine-generated code detectors that generalize well and resist adversarial evasion, especially when incorporating adversity during training and using advanced training strategies.

Abstract: In this work, we compile $\textbf{$\texttt{DroidCollection}$}$, the most
extensive open data suite for training and evaluating machine-generated code
detectors, comprising over a million code samples, seven programming languages,
outputs from 43 coding models, and over three real-world coding domains.
Alongside fully AI-generated samples, our collection includes human-AI
co-authored code, as well as adversarial samples explicitly crafted to evade
detection. Subsequently, we develop $\textbf{$\texttt{DroidDetect}$}$, a suite
of encoder-only detectors trained using a multi-task objective over
$\texttt{DroidCollection}$. Our experiments show that existing detectors'
performance fails to generalise to diverse coding domains and programming
languages outside of their narrow training data. Additionally, we demonstrate
that while most detectors are easily compromised by humanising the output
distributions using superficial prompting and alignment approaches, this
problem can be easily amended by training on a small amount of adversarial
data. Finally, we demonstrate the effectiveness of metric learning and
uncertainty-based resampling as means to enhance detector training on possibly
noisy distributions.

</details>


### [2] [ARPaCCino: An Agentic-RAG for Policy as Code Compliance](https://arxiv.org/abs/2507.10584)
*Francesco Romeo,Luigi Arena,Francesco Blefari,Francesco Aurelio Pironti,Matteo Lupinacci,Angelo Furfaro*

Main category: cs.SE

TL;DR: The paper presents ARPaCCino, a system combining LLMs and RAG for automated policy generation and enforcement in IaC, showing it can generate correct PaC rules and remediate non-compliance, enhancing automation and reliability in this domain.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenges in adopting Policy as Code (PaC) due to complex policy languages and risks of misconfiguration, which impede automated security and compliance enforcement in Infrastructure as Code (IaC) environments.

Method: The authors propose ARPaCCino, a system that leverages Large Language Models (LLMs), Retrieval-Augmented-Generation (RAG), and tool-based validation to automate the generation and verification of policy rules. It can take natural language descriptions of policies, generate formal rules (specifically Rego), assess compliance of IaC, and iteratively refine configurations for conformance. Its architecture is modular and agentic, supporting integration with external tools and knowledge bases for wide applicability across IaC technologies.

Result: Through experiments on a Terraform-based case study, ARPaCCino shows effectiveness in generating syntactically and semantically correct policy rules, identifying non-compliant infrastructure, and applying corrective modifications, even with smaller, open-weight LLMs.

Conclusion: Agentic RAG architectures like ARPaCCino can significantly automate, improve reliability, and increase accessibility of Policy as Code workflows, potentially advancing secure and compliant IaC adoption.

Abstract: Policy as Code (PaC) is a paradigm that encodes security and compliance
policies into machine-readable formats, enabling automated enforcement in
Infrastructure as Code (IaC) environments. However, its adoption is hindered by
the complexity of policy languages and the risk of misconfigurations. In this
work, we present ARPaCCino, an agentic system that combines Large Language
Models (LLMs), Retrieval-Augmented-Generation (RAG), and tool-based validation
to automate the generation and verification of PaC rules. Given natural
language descriptions of the desired policies, ARPaCCino generates formal Rego
rules, assesses IaC compliance, and iteratively refines the IaC configurations
to ensure conformance. Thanks to its modular agentic architecture and
integration with external tools and knowledge bases, ARPaCCino supports policy
validation across a wide range of technologies, including niche or emerging IaC
frameworks. Experimental evaluation involving a Terraform-based case study
demonstrates ARPaCCino's effectiveness in generating syntactically and
semantically correct policies, identifying non-compliant infrastructures, and
applying corrective modifications, even when using smaller, open-weight LLMs.
Our results highlight the potential of agentic RAG architectures to enhance the
automation, reliability, and accessibility of PaC workflows.

</details>


### [3] [Repairing Language Model Pipelines by Meta Self-Refining Competing Constraints at Runtime](https://arxiv.org/abs/2507.10590)
*Mojtaba Eshghie*

Main category: cs.SE

TL;DR: The paper presents Meta Self-Refining, a framework that enables language model pipelines to escape inefficient failure loops caused by competing constraints. This system uses a meta-repairer language model to analyze and strategically guide the pipeline towards solutions that balance all constraints, significantly improving efficiency and output quality.


<details>
  <summary>Details</summary>
Motivation: Language model (LM) pipelines often fail when they encounter multiple competing soft constraints, resulting in inefficient cycles where addressing one constraint causes another to fail. There is a need to improve LM pipeline efficiency and robustness in such situations.

Method: The authors propose Meta Self-Refining, a framework that adds a meta-corrective layer to LM pipelines. This layer monitors pipeline execution, detects oscillatory failure patterns, and, upon such detection, engages a meta-repairer LM. The meta-repairer analyzes the full history of failed attempts and generates a strategic instruction to guide the original LM towards balancing competing constraints and achieving a successful output.

Result: Meta Self-Refining is shown to successfully break the cycles of failure caused by competing soft constraints, leading to improved efficiency and effectiveness in LM pipeline outputs.

Conclusion: Adding a meta-corrective, self-refining mechanism to LM pipelines helps bypass backtracking failures and enables more efficient and constrained outputs in the presence of competing requirements.

Abstract: Language Model (LM) pipelines can dynamically refine their outputs against
programmatic constraints. However, their effectiveness collapses when faced
with competing soft constraints, leading to inefficient backtracking loops
where satisfying one constraint violates another. We introduce Meta
Self-Refining, a framework that equips LM pipelines with a meta-corrective
layer to repair these competitions at runtime/inference-time. Our approach
monitors the pipeline's execution history to detect oscillatory failures. Upon
detection, it invokes a meta-repairer LM that analyzes the holistic state of
the backtracking attempts and synthesizes a strategic instruction to balance
the competing requirements. This self-repair instruction guides the original LM
out of a failing refining loop towards a successful output. Our results show
Meta Self-Refining can successfully repair these loops, leading to more
efficient LM programs.

</details>


### [4] [ToolRegistry: A Protocol-Agnostic Tool Management Library for Function-Calling LLMs](https://arxiv.org/abs/2507.10593)
*Peng Ding*

Main category: cs.SE

TL;DR: Toolregistry is an open-source library that streamlines integrating external tools with LLMs, cutting code overhead by up to 80% and improving execution speed and maintainability, while being protocol-agnostic and fully compatible with OpenAI standards.


<details>
  <summary>Details</summary>
Motivation: Large Language Model (LLM) applications increasingly need to integrate with external tools to expand their functionality, but current integration methods are fragmented, limited by protocols, and complex to implement, causing high development overhead.

Method: The authors propose Toolregistry, a protocol-agnostic, unified tool management library that streamlines registration, representation, execution, and lifecycle management of tools used by LLM applications.

Result: Evaluation shows that Toolregistry reduces tool integration code by 60-80%, improves performance (up to 3.1x faster via concurrent execution), and maintains 100% compatibility with OpenAI function calling protocols. Real-world case studies indicate significant development efficiency and code maintainability improvements.

Conclusion: Toolregistry efficiently standardizes and simplifies tool integration for LLM-based applications, reducing overhead, boosting performance, and improving maintainability, with open-source availability and detailed documentation.

Abstract: Large Language Model (LLM) applications are increasingly relying on external
tools to extend their capabilities beyond text generation. However, current
tool integration approaches suffer from fragmentation, protocol limitations,
and implementation complexity, leading to substantial development overhead.
This paper presents Toolregistry, a protocol-agnostic tool management library
that simplifies tool registration, representation, execution, and lifecycle
management via a unified interface. Our evaluation demonstrates that
\toolregistry achieves 60-80% reduction in tool integration code, up to 3.1x
performance improvements through concurrent execution, and 100% compatibility
with OpenAI function calling standards. Real-world case studies show
significant improvements in development efficiency and code maintainability
across diverse integration scenarios. \toolregistry is open-source and
available at https://github.com/Oaklight/ToolRegistry, with comprehensive
documentation at https://toolregistry.readthedocs.io/.

</details>


### [5] [SENSOR: An ML-Enhanced Online Annotation Tool to Uncover Privacy Concerns from User Reviews in Social-Media Applications](https://arxiv.org/abs/2507.10640)
*Labiba Farah,Mohammad Ridwan Kabir,Shohel Ahmed,MD Mohaymen Ul Anam,Md. Sakibul Islam*

Main category: cs.SE

TL;DR: The paper presents SENSOR, an annotation tool using the GRACE model (GRU+CBOW+Attention) to classify social media app user reviews into privacy-related feature requests, bug reports, or irrelevant feedback. Achieving high accuracy and reliability, the tool helps developers address privacy concerns efficiently, thereby improving app privacy and user trust.


<details>
  <summary>Details</summary>
Motivation: Social media applications face significant privacy concerns, often highlighted in user reviews. Developers find it challenging to manually identify and address privacy-related concerns due to the massive and nuanced volume of reviews. There is a lack of specialized models that can distinguish between privacy-related feature requests, privacy-related bug reports, and privacy-irrelevant reviews.

Method: The paper introduces SENSOR, an online annotation tool that helps developers annotate and classify user reviews into privacy-related feature requests, privacy-related bug reports, and privacy-irrelevant categories. It presents a novel annotation model called GRACE (GRU-based Attention with CBOW Embedding), using Gated Recurrent Units (GRU) integrated with Continuous Bag of Words (CBOW) embeddings and an Attention mechanism. The model was trained and validated on about 16,000 manually annotated user reviews from seven social media apps. High inter-rater agreement (Cohen's Kappa = 0.87) was achieved in labeling the dataset.

Result: The GRACE model achieved superior classification performance with a macro F1-score of 0.9434, macro ROC-AUC of 0.9934, and accuracy of 95.10%, demonstrating robustness even with class imbalance. SENSOR can reliably help developers distinguish and prioritize privacy-related feedback in user reviews.

Conclusion: SENSOR, powered by the GRACE model, offers an effective, automated solution to annotate and classify user reviews, enabling developers to better extract and respond to privacy-related issues. This leads to improvements in app privacy features and enhances user trust.

Abstract: The widespread use of social media applications has raised significant
privacy concerns, often highlighted in user reviews. These reviews also provide
developers with valuable insights into improving apps by addressing issues and
introducing better features. However, the sheer volume and nuanced nature of
reviews make manual identification and prioritization of privacy-related
concerns challenging for developers. Previous studies have developed software
utilities to automatically classify user reviews as privacy-relevant,
privacy-irrelevant, bug reports, feature requests, etc., using machine
learning. Notably, there is a lack of focus on classifying reviews specifically
as privacy-related feature requests, privacy-related bug reports, or
privacy-irrelevant. This paper introduces SENtinel SORt (SENSOR), an automated
online annotation tool designed to help developers annotate and classify user
reviews into these categories. For automating the annotation of such reviews,
this paper introduces the annotation model, GRACE (GRU-based Attention with
CBOW Embedding), using Gated Recurrent Units (GRU) with Continuous Bag of Words
(CBOW) and Attention mechanism. Approximately 16000 user reviews from seven
popular social media apps on Google Play Store, including Instagram, Facebook,
WhatsApp, Snapchat, X (formerly Twitter), Facebook Lite, and Line were
analyzed. Two annotators manually labelled the reviews, achieving a Cohen's
Kappa value of 0.87, ensuring a labeled dataset with high inter-rater agreement
for training machine learning models. Among the models tested, GRACE
demonstrated the best performance (macro F1-score: 0.9434, macro ROC-AUC:
0.9934, and accuracy: 95.10%) despite class imbalance. SENSOR demonstrates
significant potential to assist developers with extracting and addressing
privacy-related feature requests or bug reports from user reviews, enhancing
user privacy and trust.

</details>


### [6] [A Code Comprehension Benchmark for Large Language Models for Code](https://arxiv.org/abs/2507.10641)
*Jayant Havare,Saurav Chaudhary,Ganesh Ramakrishnan,Kaushik Maharajan,Srikanth Tamilselvam*

Main category: cs.SE

TL;DR: Fine-tuning code LLMs on dedicated code comprehension tasks greatly improves their understanding of code semantics, leading to better performance in tasks that require deeper reasoning beyond mere syntax.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) show strong performance in code generation and completion but struggle with tasks that require deep semantic understanding, such as debugging and optimization. This limitation is likely because LLMs are primarily trained to predict the next token, learning surface-level syntax rather than true code comprehension.

Method: The authors propose fine-tuning existing code models using large-scale datasets specifically curated for code comprehension tasks. They evaluate three different-sized code models on a suite of code comprehension tasks, with a focus on the Subjectivity Grading Task, to measure improvements in semantic understanding.

Result: Fine-tuning leads to significant improvements in code comprehension abilities. For example, the QWQ-32B model's accuracy on the Subjectivity Grading Task increased from 70% to 83.47%. The DPO-fine-tuned Codestral-22B achieved the highest micro-accuracy at 87.66%. Similar trends were observed across other models.

Conclusion: Targeted fine-tuning on code comprehension tasks substantially enhances the semantic understanding capabilities of code LLMs, enabling them to perform better on tasks beyond surface-level syntax, such as debugging and optimization.

Abstract: Large Language Models have shown impressive capabilities in coding tasks like
code generation and code completion, as they have been trained on a large
amount of code data. Also, since one of the core pretraining objectives is Next
Token Prediction, these models tends to learn surface-level syntactic patterns
in code. However, this does not guarantee code comprehension ability i.e. the
ability to capture the semantics of the code. In our opinion, this is the
reason why these models often underperform on tasks that require deeper
semantic understanding, such as code debugging and code optimization. To
address this, we propose fine-tuning these models specifically for code
comprehension tasks using large-scale datasets, enabling them to develop a more
robust understanding of code semantics. We evaluate three code models of
varying sizes on a suite of code comprehension tasks designed to assess
semantic understanding beyond surface-level syntactic pattern matching. In
particular, we analyze performance on the Subjectivity Grading Task and observe
that model performance improves after fine-tuning on relevant downstream tasks.
The most significant improvement is seen in the QWQ-32B model, where accuracy
increases from 70% to 83.47%. A similar or explainable trend is observed across
other models, clearly indicating an enhancement in code comprehension ability.
Among the models studied, the DPO-fine-tuned Codestral-22B achieves the highest
micro-accuracy of 87.66% on the Subjectivity Grading Task.

</details>


### [7] [CodeAssistBench (CAB): Dataset & Benchmarking for Multi-turn Chat-Based Code Assistance](https://arxiv.org/abs/2507.10646)
*Myeongsoo Kim,Shweta Garg,Baishakhi Ray,Varun Kumar,Anoop Deoras*

Main category: cs.SE

TL;DR: CodeAssistBench is a new benchmark that tests LLM programming assistants in realistic, multi-turn, project-specific contexts. Leading models perform well on simple questions but struggle greatly with real-world codebase challenges, revealing a large gap between current evaluation methods and practical programming needs.


<details>
  <summary>Details</summary>
Motivation: Existing programming assistant benchmarks mostly focus on narrow, single-turn code generation tasks and do not adequately reflect the complexity of real-world software development. Newer benchmarks like InfiBench and StackEval try to broaden the scope, but still lack true multi-turn, project-level evaluation and require substantial manual work.

Method: The authors introduce CodeAssistBench (CAB), a benchmark framework designed to automatically generate scalable datasets from GitHub issues linked to real-world codebases, containerize the environments for reproducible evaluation, and simulate multi-turn programming assistance within a natural project setting. They used CAB to construct a diverse test set from thousands of real programming questions.

Result: Experiments show that while current large language models (LLMs) do well on Stack Overflow-style questions (70-83% success), they struggle significantly with the realistic, project-context questions in CAB, achieving only 16.49% resolution on recent issues.

Conclusion: Programming assistants powered by LLMs have a significant capability gap when moving from artificial, standalone questions to realistic, multi-turn, project-based programming assistance. CAB provides a scalable and automated framework to expose and measure this gap.

Abstract: Programming assistants powered by large language models have transformed
software development, yet most benchmarks focus narrowly on code generation
tasks. Recent efforts like InfiBench and StackEval attempt to address this gap
using Stack Overflow data but remain limited to single-turn interactions in
isolated contexts, require significant manual curation, and fail to represent
complete project environments. We introduce CodeAssistBench (CAB), the first
benchmark framework for evaluating multi-turn programming assistance in
realistic settings that address real-world questions about actual codebases.
Unlike existing programming Q&A benchmarks, CAB automatically generates
scalable datasets from question-related GitHub issues using configurable
parameters (e.g., repository creation date, star count, programming languages),
and includes automatic containerization of codebases for evaluation. It then
evaluates models through simulated users in these containerized environments
with full codebase access. Using this framework, we constructed a test set of
3,286 real-world programming questions across 231 repositories, spanning seven
programming languages and diverse problem domains. Our evaluation of leading
LLMs reveals a substantial capability gap: while models perform well on Stack
Overflow questions with success rates of 70-83%, they resolve only up to 16.49%
of CAB's recent issues. This discrepancy highlights the challenges of providing
assistance in complex, project-specific contexts versus answering standalone
questions.

</details>


### [8] [Toward Realistic Evaluations of Just-In-Time Vulnerability Prediction](https://arxiv.org/abs/2507.10729)
*Duong Nguyen,Thanh Le-Cong,Triet Huynh Minh Le,M. Ali Babar,Quyet-Thang Huynh*

Main category: cs.SE

TL;DR: JIT vulnerability prediction models work well only on idealized, balanced datasets, but fail on real-world imbalanced data. Attempts to fix this with standard data imbalance techniques do not work, highlighting the need for better, domain-adapted solutions.


<details>
  <summary>Details</summary>
Motivation: Current JIT vulnerability prediction (JIT-VP) methods are usually evaluated in artificially balanced datasets, which do not reflect the reality of software development, where vulnerable commits are rare. This creates a gap in understanding how well these techniques work in real-world settings.

Method: The authors introduce a large-scale, realistic dataset containing over one million commits from FFmpeg and the Linux kernel, which includes both vulnerability-related and vulnerability-neutral commits. They empirically evaluate eight state-of-the-art JIT-VP techniques on this dataset. They also experiment with common techniques such as customized loss functions, oversampling, and undersampling to address data imbalance.

Result: The results show that JIT-VP techniques perform significantly worse on realistic, imbalanced datasets—illustrated by a dramatic drop in PR-AUC (e.g., from 0.805 to 0.016 on Linux). Popular data imbalance mitigation techniques were found to be ineffective for JIT-VP.

Conclusion: Evaluating JIT-VP techniques in realistic settings is critical, as artificially balanced datasets overestimate their predictive power. There is a significant need for domain-specific solutions to address data imbalance in JIT-VP, as standard approaches are insufficient.

Abstract: Modern software systems are increasingly complex, presenting significant
challenges in quality assurance. Just-in-time vulnerability prediction (JIT-VP)
is a proactive approach to identifying vulnerable commits and providing early
warnings about potential security risks. However, we observe that current
JIT-VP evaluations rely on an idealized setting, where the evaluation datasets
are artificially balanced, consisting exclusively of vulnerability-introducing
and vulnerability-fixing commits.
  To address this limitation, this study assesses the effectiveness of JIT-VP
techniques under a more realistic setting that includes both
vulnerability-related and vulnerability-neutral commits. To enable a reliable
evaluation, we introduce a large-scale public dataset comprising over one
million commits from FFmpeg and the Linux kernel. Our empirical analysis of
eight state-of-the-art JIT-VP techniques reveals a significant decline in
predictive performance when applied to real-world conditions; for example, the
average PR-AUC on Linux drops 98\% from 0.805 to 0.016. This discrepancy is
mainly attributed to the severe class imbalance in real-world datasets, where
vulnerability-introducing commits constitute only a small fraction of all
commits.
  To mitigate this issue, we explore the effectiveness of widely adopted
techniques for handling dataset imbalance, including customized loss functions,
oversampling, and undersampling. Surprisingly, our experimental results
indicate that these techniques are ineffective in addressing the imbalance
problem in JIT-VP. These findings underscore the importance of realistic
evaluations of JIT-VP and the need for domain-specific techniques to address
data imbalance in such scenarios.

</details>


### [9] [GenAI-Enabled Backlog Grooming in Agile Software Projects: An Empirical Study](https://arxiv.org/abs/2507.10753)
*Kasper Lien Oftebro,Anh Nguyen-Duc,Kai-Kristian Kemell*

Main category: cs.SE

TL;DR: The paper shows that using a generative AI-based Jira plug-in can automate backlog grooming in Agile projects, achieving perfect accuracy and nearly halving the time needed compared to manual processes.


<details>
  <summary>Details</summary>
Motivation: Managing backlogs in Agile software projects becomes increasingly difficult as they grow, leading to clutter and complications in prioritization due to redundant, outdated, or poorly defined tasks. There is a need for smarter automation tools to assist with backlog grooming.

Method: The authors used Design Science research cycles to create a Jira plug-in. This tool embeds backlog issues in a vector database, detects duplicates using cosine similarity, and utilizes the GPT-4o model to suggest actions such as merging, deleting, or creating new issues.

Result: The AI-assisted tool achieved 100 percent precision in grooming backlog items and reduced the time required for backlog grooming by 45 percent.

Conclusion: A generative-AI assistant can significantly improve the efficiency and effectiveness of backlog management in Agile software projects, maintaining accuracy and transparency while streamlining the process and enhancing user experience.

Abstract: Effective backlog management is critical for ensuring that development teams
remain aligned with evolving requirements and stakeholder expectations.
However, as product backlogs consistently grow in scale and complexity, they
tend to become cluttered with redundant, outdated, or poorly defined tasks,
complicating prioritization and decision making processes. This study
investigates whether a generative-AI (GenAI) assistant can automate backlog
grooming in Agile software projects without sacrificing accuracy or
transparency. Through Design Science cycles, we developed a Jira plug-in that
embeds backlog issues with the vector database, detects duplicates via cosine
similarity, and leverage the GPT-4o model to propose merges, deletions, or new
issues. We found that AI-assisted backlog grooming achieved 100 percent
precision while reducing the time-to-completion by 45 percent. The findings
demonstrated the tool's potential to streamline backlog refinement processes
while improving user experiences.

</details>


### [10] [Towards a Closer Collaboration Between Practice and Research in Agile Software Development Workshop: A Summary and Research Agenda](https://arxiv.org/abs/2507.10785)
*Michael Neumann,Eva-Maria Schön,Mali Senapathi,Maria Rauschenberger,Tiago Silva da Silva*

Main category: cs.SE

TL;DR: The paper analyzes discussions from a workshop uniting agile research and practice, identifies causes of the research-practice divide, suggests bridging strategies, and points to areas needing more research.


<details>
  <summary>Details</summary>
Motivation: There is a significant gap between research and practical implementation of agile software development, despite its widespread adoption.

Method: This paper reports on the outcomes from the first international workshop aimed at bringing together researchers and practitioners in agile software development.

Result: The workshop identified key themes and factors causing the research-practice gap, discussed strategies for bridging it, and highlighted specific challenges that need more research.

Conclusion: Bridging the gap between agile software development research and practice requires collaboration and continued effort, as outlined by the workshop findings.

Abstract: Agile software development principles and values have been widely adopted
across various industries, influencing products and services globally. Despite
its increasing popularity, a significant gap remains between research and
practical implementation. This paper presents the findings of the first
international workshop designed to foster collaboration between research and
practice in agile software development. We discuss the main themes and factors
identified by the workshop participants that contribute to this gap, strategies
to bridge it, and the challenges that require further research attention.

</details>


### [11] [How Robust are LLM-Generated Library Imports? An Empirical Study using Stack Overflow](https://arxiv.org/abs/2507.10818)
*Jasmine Latendresse,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: The paper shows that LLMs recommend mostly third-party, popular libraries for Python tasks but often fail to support users in library installation and dependency resolution, exposing areas for improvement in model recommendations and usability.


<details>
  <summary>Details</summary>
Motivation: As more developers use Large Language Models (LLMs) to help with programming, it is increasingly important to understand how these models recommend software libraries, because libraries impact code functionality, security, and maintenance.

Method: The authors performed an empirical study with six state-of-the-art LLMs, both open-source and proprietary. They prompted the models to solve real-world Python problems from Stack Overflow and analyzed the types, characteristics, and usability of the recommended libraries.

Result: LLMs tend to favor third-party, mature, popular, and permissively licensed libraries over standard ones. However, 4.6% of suggested libraries could not be automatically resolved due to name mismatches, and only two models provided installation instructions, making users resolve some issues manually.

Conclusion: While LLMs generate technically valid code and generally recommend appropriate libraries, usability issues remain. The process lacks sufficient contextual support, placing additional burdens on users, and further improvements are needed for better reliability and usability of LLM-generated code regarding dependencies.

Abstract: Software libraries are central to the functionality, security, and
maintainability of modern code. As developers increasingly turn to Large
Language Models (LLMs) to assist with programming tasks, understanding how
these models recommend libraries is essential. In this paper, we conduct an
empirical study of six state-of-the-art LLMs, both proprietary and open-source,
by prompting them to solve real-world Python problems sourced from Stack
Overflow. We analyze the types of libraries they import, the characteristics of
those libraries, and the extent to which the recommendations are usable out of
the box. Our results show that LLMs predominantly favour third-party libraries
over standard ones, and often recommend mature, popular, and permissively
licensed dependencies. However, we also identify gaps in usability: 4.6% of the
libraries could not be resolved automatically due to structural mismatches
between import names and installable packages, and only two models (out of six)
provided installation guidance. While the generated code is technically valid,
the lack of contextual support places the burden of manually resolving
dependencies on the user. Our findings offer actionable insights for both
developers and researchers, and highlight opportunities to improve the
reliability and usability of LLM-generated code in the context of software
dependencies.

</details>


### [12] [Past, Present and Future: Exploring Adaptive AI in Software Development Bots](https://arxiv.org/abs/2507.10822)
*Omar Elsisi,Glaucia Melo*

Main category: cs.SE

TL;DR: Adaptive AI chatbots are changing software development by delivering smarter, more personalized support, but issues like privacy and ethics require careful consideration.


<details>
  <summary>Details</summary>
Motivation: Conversational agents are becoming crucial in software development to improve productivity, collaboration, and automate tasks. Traditional systems are limited, so there is a need to explore the benefits and challenges of adaptive AI-powered agents.

Method: The paper reviews the evolution and integration of adaptive AI agents in software development, compares traditional rule-based systems with modern AI-driven tools, and analyzes associated benefits, limitations, and ethical concerns.

Result: Adaptive AI conversational agents like GitHub Copilot and Teams bots provide dynamic, personalized assistance, learn from interactions, and improve workflow. However, challenges such as data privacy and ethical issues remain significant.

Conclusion: Adaptive AI chatbots are poised to transform software development by offering real-time, context-aware support and boosting overall productivity, despite certain challenges that must be addressed.

Abstract: Conversational agents, such as chatbots and virtual assistants, have become
essential in software development, boosting productivity, collaboration, and
automating various tasks. This paper examines the role of adaptive AI-powered
conversational agents in software development, highlighting their ability to
offer dynamic, context-aware assistance to developers. Unlike traditional
rule-based systems, adaptive AI agents use machine learning and natural
language processing to learn from interactions and improve over time, providing
more personalized and responsive help. We look at how these tools have evolved
from simple query-based systems to advanced AI-driven solutions like GitHub
Copilot and Microsoft Teams bots. We also explore the challenges of integrating
adaptive AI into software development processes. The study aims to assess the
benefits and limitations of these systems, address concerns like data privacy
and ethical issues, and offer insights into their future use in the field.
Ultimately, adaptive AI chatbots have great potential to revolutionize software
development by delivering real-time, customized support and enhancing the
efficiency of development cycles.

</details>


### [13] [Evaluating Generated Commit Messages with Large Language Models](https://arxiv.org/abs/2507.10906)
*Qunhong Zeng,Yuxia Zhang,Zexiong Ma,Bo Jiang,Ningyuan Sun,Klaas-Jan Stol,Xingyu Mou,Hui Liu*

Main category: cs.SE

TL;DR: The paper shows that LLMs, when properly prompted, can nearly match human abilities in evaluating the quality of commit messages, significantly outperforming traditional automated metrics and providing a scalable alternative to human evaluation.


<details>
  <summary>Details</summary>
Motivation: Commit messages are crucial for documenting code changes in software development, but their quality is often insufficient. Traditional automated metrics for evaluating the quality of these messages are limited and typically rely on reference-based approaches that do not accurately reflect the diverse nature of high-quality commit messages. As a result, there is a reliance on expensive and time-consuming human evaluation. There is a need for better, scalable evaluation methods.

Method: The paper systematically experiments with different prompt strategies and state-of-the-art Large Language Models (LLMs) to evaluate commit message quality. Specifically, it tests LLMs with Chain-of-Thought reasoning and few-shot demonstrations as evaluators, comparing their performance to traditional reference-based automatic metrics like BLEU, ROUGE-L, and METEOR.

Result: LLMs equipped with Chain-of-Thought reasoning and few-shot demonstrations can evaluate commit message quality with near human-level proficiency. The LLM-based evaluator outperforms traditional automatic metrics and maintains acceptable levels of reproducibility, robustness, and fairness, even accounting for some inherent variability.

Conclusion: LLM-based automated evaluation presents a promising, scalable, and high-quality alternative to human assessment for commit message evaluation, overcoming the limitations of existing automatic metrics. This approach offers a comprehensive preliminary solution that balances accuracy and efficiency.

Abstract: Commit messages are essential in software development as they serve to
document and explain code changes. Yet, their quality often falls short in
practice, with studies showing significant proportions of empty or inadequate
messages. While automated commit message generation has advanced significantly,
particularly with Large Language Models (LLMs), the evaluation of generated
messages remains challenging. Traditional reference-based automatic metrics
like BLEU, ROUGE-L, and METEOR have notable limitations in assessing commit
message quality, as they assume a one-to-one mapping between code changes and
commit messages, leading researchers to rely on resource-intensive human
evaluation. This study investigates the potential of LLMs as automated
evaluators for commit message quality. Through systematic experimentation with
various prompt strategies and state-of-the-art LLMs, we demonstrate that LLMs
combining Chain-of-Thought reasoning with few-shot demonstrations achieve near
human-level evaluation proficiency. Our LLM-based evaluator significantly
outperforms traditional metrics while maintaining acceptable reproducibility,
robustness, and fairness levels despite some inherent variability. This work
conducts a comprehensive preliminary study on using LLMs for commit message
evaluation, offering a scalable alternative to human assessment while
maintaining high-quality evaluation.

</details>


### [14] [SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language Models on Software Engineering Tasks](https://arxiv.org/abs/2507.11059)
*Pavel Adamenko,Mikhail Ivanov,Aidar Valeev,Rodion Levichev,Pavel Zadorozhny,Ivan Lopatin,Dmitry Babayev,Alena Fenogenova,Valentin Malykh*

Main category: cs.SE

TL;DR: The paper introduces SWE-MERA, a new, dynamically-updated benchmark for assessing LLMs in software engineering. It addresses flaws in previous datasets (like SWE-bench), such as data contamination and weak testing. SWE-MERA provides thousands of validated tasks and allows more accurate evaluations of LLMs.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the serious shortcomings of existing software engineering LLM benchmarks, particularly SWE-bench, which suffers from data contamination and poor test cases. The field requires reliable, up-to-date benchmarks to properly assess the abilities of LLMs in software engineering tasks.

Method: The authors introduce SWE-MERA, a new benchmark that automatically collects real-world GitHub issues and applies a rigorous quality validation process. The benchmark is dynamically updated to ensure relevant and uncontaminated test samples. Their workflow includes minimizing contamination risks and establishing a reliable pipeline for benchmark creation.

Result: The paper reports a dataset with approximately 10,000 potential tasks, 300 of which are immediately available. Performance evaluations using the Aider coding agent show that SWE-MERA can distinguish well among state-of-the-art LLMs. Performance results are provided for over a dozen recent LLMs tested on tasks collected from September 2024 to June 2025.

Conclusion: SWE-MERA presents a significant improvement over previous benchmarks by continuously providing high-quality, real-world tasks with minimal data contamination, enabling more accurate and reliable evaluation of LLMs in software engineering.

Abstract: The rapid advancement of Large Language Models (LLMs) in software engineering
has revealed critical limitations in existing benchmarks, particularly the
widely used SWE-bench dataset. Recent studies have uncovered severe data
contamination issues, e.g. SWE-bench reports 32.67% of successful patches
involve direct solution leakage and 31.08\% pass due to inadequate test cases.
We introduce SWE-MERA, a dynamic, continuously updated benchmark designed to
address these fundamental challenges through an automated collection of
real-world GitHub issues and rigorous quality validation. Our approach
implements a reliable pipeline that ensures quality while minimizing
contamination risks, resulting in approximately 10,000 potential tasks with 300
samples currently available. Evaluation using the Aider coding agent
demonstrates strong discriminative power in state-of-the-art models. We report
performance across a dozen recent LLMs evaluated on tasks collected between
September 2024 and June 2025.

</details>


### [15] [MT4DP: Data Poisoning Attack Detection for DL-based Code Search Models via Metamorphic Testing](https://arxiv.org/abs/2507.11092)
*Gong Chen,Wenjie Liu,Xiaoyuan Xie,Xunzhu Tang,Tegawendé F. Bissyandé,Songqiang Chen*

Main category: cs.SE

TL;DR: The paper presents MT4DP, a new framework using metamorphic testing to detect data poisoning in code search models, achieving much higher detection accuracy than prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning-based code search models are vulnerable to data poisoning attacks, where attackers manipulate training data to control model ranking outputs. Current detection methods do not sufficiently address this issue.

Method: The paper proposes MT4DP, a novel framework based on metamorphic testing to detect data poisoning in DL-based code search models. It introduces Semantically Equivalent Metamorphic Relation (SE-MR), uses high-frequency query words to generate source and semantically equivalent follow-up queries, and analyzes ranking variance to detect violations indicative of poisoning.

Result: MT4DP substantially improves the detection of data poisoning attacks compared to the best baseline methods, achieving a 191% increase in average F1 score and a 265% increase in average precision.

Conclusion: MT4DP effectively detects data poisoning attacks in deep learning-based code search models and outperforms existing solutions, contributing to enhanced model security and inspiring further research in this area.

Abstract: Recently, several studies have indicated that data poisoning attacks pose a
severe security threat to deep learning-based (DL-based) code search models.
Attackers inject carefully crafted malicious patterns into the training data,
misleading the code search model to learn these patterns during training.
During the usage of the poisoned code search model for inference, once the
malicious pattern is triggered, the model tends to rank the vulnerability code
higher. However, existing detection methods for data poisoning attacks on
DL-based code search models remain insufficiently effective. To address this
critical security issue, we propose MT4DP, a Data Poisoning Attack Detection
Framework for DL-based Code Search Models via Metamorphic Testing. MT4DP
introduces a novel Semantically Equivalent Metamorphic Relation (SE-MR)
designed to detect data poisoning attacks on DL-based code search models.
Specifically, MT4DP first identifies the high-frequency words from search
queries as potential poisoning targets and takes their corresponding queries as
the source queries. For each source query, MT4DP generates two semantically
equivalent follow-up queries and retrieves its source ranking list. Then, each
source ranking list is re-ranked based on the semantic similarities between its
code snippets and the follow-up queries. Finally, variances between the source
and re-ranked lists are calculated to reveal violations of the SE-MR and warn
the data poisoning attack. Experimental results demonstrate that MT4DP
significantly enhances the detection of data poisoning attacks on DL-based code
search models, outperforming the best baseline by 191% on average F1 score and
265% on average precision. Our work aims to promote further research into
effective techniques for mitigating data poisoning threats on DL-based code
search models.

</details>


### [16] [Automata Models for Effective Bug Description](https://arxiv.org/abs/2507.11146)
*Tom Yaacov,Gera Weiss,Gal Amram,Avi Hayoun*

Main category: cs.SE

TL;DR: The paper introduces automata learning methods to summarize and clarify bug reports, making debugging of complex systems easier and more effective.


<details>
  <summary>Details</summary>
Motivation: Debugging complex systems is challenging and time-consuming, often due to complicated and difficult-to-understand bug reports.

Method: The paper uses automata learning and testing techniques to extract and summarize essential behavior patterns related to bugs, filtering out irrelevant information.

Result: The evaluation on various test patterns and real-world benchmarks shows that the approach generates concise and informative bug descriptions.

Conclusion: Using automata-based techniques for debugging can improve bug detection and understanding by providing clearer and more focused summaries of failures.

Abstract: Debugging complex systems is a crucial yet time-consuming task. This paper
presents the use of automata learning and testing techniques to obtain concise
and informative bug descriptions. We introduce the concepts of Failure
Explanations (FE), Eventual Failure Explanations (EFE), and Early Detection
(ED) to provide meaningful summaries of failing behavior patterns. By factoring
out irrelevant information and focusing on essential test patterns, our
approach aims to enhance bug detection and understanding. We evaluate our
methods using various test patterns and real-world benchmarks, demonstrating
their effectiveness in producing compact and informative bug descriptions.

</details>


### [17] [New Formulation of DNN Statistical Mutation Killing for Ensuring Monotonicity: A Technical Report](https://arxiv.org/abs/2507.11199)
*Jinhan Kim,Nargiz Humbatova,Gunel Jahangirova,Shin Yoo,Paolo Tonella*

Main category: cs.SE

TL;DR: This paper identifies and solves a key limitation in statistical mutation testing for Deep Neural Networks by introducing a Fisher exact test-based approach that ensures monotonicity, thereby making mutant classification more reliable as test sets expand.


<details>
  <summary>Details</summary>
Motivation: The paper addresses a limitation in existing mutation testing methods for Deep Neural Networks, specifically focusing on the non-monotonic nature of the statistical mutant killing criterion in DeepCrime.

Method: The authors propose a new formulation for statistical mutant killing using Fisher's exact test, aiming to maintain statistical rigor while preserving monotonicity.

Result: The new approach ensures that the monotonicity property is upheld, so that expanding a test set does not cause previously killed mutants to be reclassified as alive.

Conclusion: The proposed method successfully overcomes the monotonicity issue in the existing statistical mutant killing techniques, improving the evaluation process for test suites in Deep Neural Networks.

Abstract: Mutation testing has emerged as a powerful technique for evaluating the
effectiveness of test suites for Deep Neural Networks. Among existing
approaches, the statistical mutant killing criterion of DeepCrime has leveraged
statistical testing to determine whether a mutant significantly differs from
the original model. However, it suffers from a critical limitation: it violates
the monotonicity property, meaning that expanding a test set may result in
previously killed mutants no longer being classified as killed. In this
technical report, we propose a new formulation of statistical mutant killing
based on Fisher exact test that preserves the statistical rigour of it while
ensuring monotonicity.

</details>


### [18] [An Empirical Study of Multi-Agent RAG for Real-World University Admissions Counseling](https://arxiv.org/abs/2507.11272)
*Anh Nguyen-Duc,Chien Vu Manh,Bao Anh Tran,Viet Phuong Ngo,Luan Le Chi,Anh Quang Nguyen*

Main category: cs.SE

TL;DR: MARAUS, a conversational AI for university admissions in Vietnam, combines multi-agent coordination and retrieval-augmented LLMs. Real-world deployment showed high accuracy, minimal AI hallucinations, fast responses, and low operation costs, providing a practical blueprint for similar educational AI systems.


<details>
  <summary>Details</summary>
Motivation: While large language models (LLMs) have potential for automating advisory tasks in university admissions, most existing tools remain at the prototype stage or have only been tested on synthetic benchmarks. There is a need for practical, scalable AI-driven solutions tailored to real-world deployments, particularly in low-resource educational settings.

Method: The authors developed MARAUS, a Multi-Agent and Retrieval-Augmented conversational AI system, and deployed it in partnership with the University of Transport Technology (UTT) in Hanoi. They conducted a two-phase study involving system development and real-world evaluation. MARAUS integrates hybrid retrieval, multi-agent orchestration, and LLM-based response generation.

Result: MARAUS processed over 6,000 actual user admissions queries across six categories, achieving an average accuracy of 92%, reducing hallucination rates from 15% to 1.45%, and maintaining average response times under 4 seconds. The system operated cost-effectively, costing $11.58 over two weeks using GPT-4o mini.

Conclusion: MARAUS significantly outperforms LLM-only baselines in real-world university admissions counseling, demonstrating high accuracy, reduced hallucinations, and low operating cost. The system serves as a model for deploying agentic retrieval-augmented generation (RAG) systems in low-resource educational contexts.

Abstract: This paper presents MARAUS (Multi-Agent and Retrieval-Augmented University
Admission System), a real-world deployment of a conversational AI platform for
higher education admissions counseling in Vietnam. While large language models
(LLMs) offer potential for automating advisory tasks, most existing solutions
remain limited to prototypes or synthetic benchmarks. MARAUS addresses this gap
by combining hybrid retrieval, multi-agent orchestration, and LLM-based
generation into a system tailored for real-world university admissions. In
collaboration with the University of Transport Technology (UTT) in Hanoi, we
conducted a two-phase study involving technical development and real-world
evaluation. MARAUS processed over 6,000 actual user interactions, spanning six
categories of queries. Results show substantial improvements over LLM-only
baselines: on average 92 percent accuracy, hallucination rates reduced from 15
precent to 1.45 percent, and average response times below 4 seconds. The system
operated cost-effectively, with a two-week deployment cost of 11.58 USD using
GPT-4o mini. This work provides actionable insights for the deployment of
agentic RAG systems in low-resource educational settings.

</details>


### [19] [RefModel: Detecting Refactorings using Foundation Models](https://arxiv.org/abs/2507.11346)
*Pedro Simões,Rohit Gheyi,Rian Melo,Jonhnanthan Oliveira,Márcio Ribeiro,Wesley K. G. Assunção*

Main category: cs.SE

TL;DR: RefModel uses advanced foundation models for refactoring detection, outperforming traditional tools and generalizing well to multiple languages, with improved flexibility and explanation capabilities.


<details>
  <summary>Details</summary>
Motivation: Traditional refactoring detection tools are difficult to generalize to other programming languages and rely on complex rule-based and static analysis techniques. This limits flexibility and scalability of refactoring detection.

Method: The authors propose and implement RefModel, a tool that applies large foundation models (such as Phi4-14B, Claude 3.5 Sonnet, Gemini 2.5 Pro, and o4-mini-high) to refactoring detection. The performance is evaluated on both synthetic datasets and real-world refactorings, comparing results with traditional static-analysis-based tools.

Result: RefModel, leveraging foundation models, is competitive with or superior to traditional detection tools. Claude 3.5 Sonnet and Gemini 2.5 Pro together identified 97% of real-world refactorings, exceeding current state-of-the-art tools. The models also demonstrated potential to generalize to other languages like Python and Golang and can provide natural language explanations for detected refactorings.

Conclusion: Foundation models offer a viable and potentially superior alternative for automated refactoring detection, enabling easier extension to new refactoring types and programming languages, and providing interpretable outputs.

Abstract: Refactoring is a common software engineering practice that improves code
quality without altering program behavior. Although tools like ReExtractor+,
RefactoringMiner, and RefDiff have been developed to detect refactorings
automatically, they rely on complex rule definitions and static analysis,
making them difficult to extend and generalize to other programming languages.
In this paper, we investigate the viability of using foundation models for
refactoring detection, implemented in a tool named RefModel. We evaluate
Phi4-14B, and Claude 3.5 Sonnet on a dataset of 858 single-operation
transformations applied to artificially generated Java programs, covering
widely-used refactoring types. We also extend our evaluation by including
Gemini 2.5 Pro and o4-mini-high, assessing their performance on 44 real-world
refactorings extracted from four open-source projects. These models are
compared against RefactoringMiner, RefDiff, and ReExtractor+. RefModel is
competitive with, and in some cases outperform, traditional tools. In
real-world settings, Claude 3.5 Sonnet and Gemini 2.5 Pro jointly identified
97% of all refactorings, surpassing the best-performing static-analysis-based
tools. The models showed encouraging generalization to Python and Golang. They
provide natural language explanations and require only a single sentence to
define each refactoring type.

</details>


### [20] [Security Debt in Practice: Nuanced Insights from Practitioners](https://arxiv.org/abs/2507.11362)
*Chaima Boufaied,Taher Ghaleb,Zainab Masood*

Main category: cs.SE

TL;DR: The paper explores how real-world software practitioners perceive and manage 'Security Debts' resulting from insecure coding, revealing inconsistent practices and communication. It calls for stronger, integrated security measures and a better balance between speed, resources, and security across development processes.


<details>
  <summary>Details</summary>
Motivation: As software becomes increasingly vital and complex, development teams often prioritize speed and features over security due to tight deadlines and limited resources. This can result in unresolved security vulnerabilities, or 'Security Debts' (SDs), accumulating over time. Current understanding of how practitioners handle SDs is limited, especially in terms of perception, management, and communication.

Method: A qualitative empirical study was conducted using semi-structured interviews with 22 software practitioners from diverse roles, organizations, and countries. The interview data was analyzed to answer four research questions about knowledge, behaviors, mitigation strategies, and communication regarding security debts.

Result: The study found significant variation in how practitioners perceive and manage security debts. Some prioritize rapid delivery and deprioritize security, while others maintain security as a primary concern. The tools and strategies for managing SDs are inconsistent, and communication about security risks within teams and to decision makers also varies.

Conclusion: There is a need for better integration of security practices throughout the Software Development Life Cycle, more systematic use of mitigation strategies, and improved balancing of deadlines, resources, and security. Emphasizing the Confidentiality, Integrity, and Availability (CIA) triad is crucial to addressing security debts effectively.

Abstract: With the increasing reliance on software and automation nowadays, tight
deadlines, limited resources, and prioritization of functionality over security
can lead to insecure coding practices. When not handled properly, these
constraints cause unaddressed security vulnerabilities to accumulate over time,
forming Security Debts (SDs). Despite their critical importance, there is
limited empirical evidence on how software practitioners perceive, manage, and
communicate SDs in real-world settings. In this paper, we present a qualitative
empirical study based on semi-structured interviews with 22 software
practitioners across various roles, organizations, and countries. We address
four research questions: i) we assess software practitioners' knowledge of SDs
and awareness of associated security risks, ii) we investigate their behavior
towards SDs, iii) we explore common tools and strategies used to mitigate SDs,
and iv) we analyze how security risks are communicated within teams and to
decision makers. We observe variations in how practitioners perceive and manage
SDs, with some prioritizing delivery speed over security, while others
consistently maintain security as a priority. Our findings emphasize the need
for stronger integration of security practices across the Software Development
Life Cycle (SDLC), more consistent use of mitigation strategies, better
balancing of deadlines, resources, and security-related tasks, with attention
to the Confidentiality, Integrity, and Availability (CIA) triad.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [21] [Stream programs are monoid homomorphisms with state](https://arxiv.org/abs/2507.10799)
*Tyler Hou,Michael Arntzenius,Max Willsey*

Main category: cs.PL

TL;DR: A new, simpler theoretical framework using state monoids for deterministic stream functions makes it easier to reason about and optimize complex dataflow programs without sacrificing expressiveness.


<details>
  <summary>Details</summary>
Motivation: To provide a simpler and yet expressive theoretical framework for reasoning about deterministic stream functions used in dataflow and streaming programs.

Method: Defines a broad class of deterministic stream functions and demonstrates that they can be implemented as homomorphisms into a 'state' monoid. The approach simplifies the homomorphism laws in comparison to previous frameworks.

Result: The framework retains support for rich equational reasoning, including features such as sequential and parallel composition and feedback. Effectiveness is shown through examples like partitioned database joins, stratified negation, and a simplified TCP model.

Conclusion: The proposed framework simplifies the semantic foundations of stream functions, making optimization and reasoning about expressive dataflow programs easier while preserving essential features.

Abstract: We define a broad class of deterministic stream functions and show they can
be implemented as homomorphisms into a "state" monoid. The homomorphism laws
are simpler than the conditions of previous semantic frameworks for stream
program optimization, yet retain support for rich equational reasoning over
expressive dataflow programs, including sequential composition, parallel
composition, and feedback. We demonstrate this using examples of partitioned
database joins, stratified negation, and a simplified model of TCP.

</details>


### [22] [The downgrading semantics of memory safety](https://arxiv.org/abs/2507.11282)
*René Rydhof Hansen,Andreas Stenbæk Larsen,Aslan Askarov*

Main category: cs.PL

TL;DR: This paper introduces 'gradual allocator independence' as a principled, semantic way to define memory safety in low-level languages, improving on past work by closely considering real-world programming practices and using state-of-the-art theoretical tools.


<details>
  <summary>Details</summary>
Motivation: Traditionally, memory safety is defined by what bad things should not occur, but this is considered unprincipled and unsatisfactory. There is a need for a better, more principled semantic notion of memory safety, especially in low-level languages where memory-unsafe operations are trivial to express.

Method: The authors propose the notion of gradual allocator independence as a new way to define and capture aspects of memory safety that are specific to memory allocators. They study a low-level language featuring malloc and free, treating pointers as integers. They refine the standard noninterference intuition to handle situations where the allocator runs out of memory and where programs perform pointer-to-integer casts, leveraging information flow theory to formalize these cases.

Result: The proposed definition of gradual allocator independence accurately represents many allocator-specific facets of memory safety, providing a more principled and semantic approach compared to traditional methods.

Conclusion: The paper offers a satisfactory and technically robust semantic notion of memory safety based on gradual allocator independence, improving on previous approaches by addressing practical realities such as out-of-memory conditions and pointer manipulation.

Abstract: Memory safety is traditionally characterized in terms of bad things that
cannot happen, an approach that is often criticized as unprincipled. Prior work
suggest a connection between memory safety and noninterference, but no
satisfactory semantic notion of memory safety is currently known.
  This work proposes a notion of gradual allocator independence that accurately
captures many allocator-specific aspects of memory safety. We consider a
low-level language with access to an allocator that provides malloc and free
primitives in a flat memory model. Pointers are just integers, and as such it
is trivial to write memory-unsafe programs. The basic intuition of gradual
allocator independence is that of noninterference, namely that allocators must
not influence program execution. This intuition is refined in two important
ways to account for the allocators running out-of-memory and for programs to
have pointer-to-integer casts. The key insight of the definition is to treat
these extensions as forms of downgrading and give them satisfactory technical
treatment using the state-of-the-art information flow machinery.

</details>
