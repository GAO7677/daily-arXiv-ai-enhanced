<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 24]
- [cs.PL](#cs.PL) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [TrioXpert: An automated incident management framework for microservice system](https://arxiv.org/abs/2506.10043)
*Yongqian Sun,Yu Luo,Xidao Wen,Yuan Yuan,Xiaohui Nie,Shenglin Zhang,Tong Liu,Xi Luo*

Main category: cs.SE

TL;DR: TrioXpert uses multimodal data and LLMs for more effective and interpretable incident management in microservices, significantly improving results in anomaly detection, failure triage, and root cause localization.


<details>
  <summary>Details</summary>
Motivation: Current automated incident management approaches for microservice systems mainly use single-modal data (like logs, metrics, or traces) and struggle with multitasking and interpretability. There is a need for solutions that can leverage multimodal data and provide clear reasoning.

Method: The proposed framework, TrioXpert, incorporates three independent data processing pipelines, each designed for a specific data modality. It uses large language models (LLMs) for collaborative reasoning, enabling simultaneous handling of anomaly detection, failure triage, and root cause localization tasks, alongside generating reasoning evidence for better interpretability.

Result: Experiments on two well-known microservice datasets show that TrioXpert delivers significant improvements: 4.7%–57.7% in anomaly detection, 2.1%–40.6% in failure triage, and 1.6%–163.1% in root cause localization.

Conclusion: TrioXpert successfully addresses the limitations of current methods by leveraging multimodal data and providing interpretability, achieving outstanding and interpretable performance across multiple incident management tasks in microservice systems.

Abstract: Automated incident management plays a pivotal role in large-scale
microservice systems. However, many existing methods rely solely on
single-modal data (e.g., metrics, logs, and traces) and struggle to
simultaneously address multiple downstream tasks, including anomaly detection
(AD), failure triage (FT), and root cause localization (RCL). Moreover, the
lack of clear reasoning evidence in current techniques often leads to
insufficient interpretability. To address these limitations, we propose
TrioXpert, an end-to-end incident management framework capable of fully
leveraging multimodal data. TrioXpert designs three independent data processing
pipelines based on the inherent characteristics of different modalities,
comprehensively characterizing the operational status of microservice systems
from both numerical and textual dimensions. It employs a collaborative
reasoning mechanism using large language models (LLMs) to simultaneously handle
multiple tasks while providing clear reasoning evidence to ensure strong
interpretability. We conducted extensive evaluations on two popular
microservice system datasets, and the experimental results demonstrate that
TrioXpert achieves outstanding performance in AD (improving by 4.7% to 57.7%),
FT (improving by 2.1% to 40.6%), and RCL (improving by 1.6% to 163.1%) tasks.

</details>


### [2] [Online Discovery of Simulation Models for Evolving Business Processes (Extended Version)](https://arxiv.org/abs/2506.10049)
*Francesco Vinci,Gyunam Park,Wil van der Aalst,Massimiliano de Leoni*

Main category: cs.SE

TL;DR: This paper introduces a novel approach that integrates online machine learning with incremental process discovery, enabling real-time, adaptive business process simulations that effectively balance recent changes and historical knowledge.


<details>
  <summary>Details</summary>
Motivation: Existing business process simulation discovery techniques are not adaptable to real-time operational changes, limiting their usefulness in dynamic business environments where processes are frequently refined.

Method: The paper proposes a streaming process simulation discovery technique that combines Incremental Process Discovery with Online Machine Learning, giving more weight to recent data while preserving historical knowledge.

Result: Experiments on four event logs show that the proposed method generates more stable simulations and is robust to concept drift, by efficiently adapting to evolving process behaviors.

Conclusion: The proposed technique advances process simulation by adapting to real-time changes, balancing recent and historical data, and providing robust and stable simulations even under concept drift.

Abstract: Business Process Simulation (BPS) refers to techniques designed to replicate
the dynamic behavior of a business process. Many approaches have been proposed
to automatically discover simulation models from historical event logs,
reducing the cost and time to manually design them. However, in dynamic
business environments, organizations continuously refine their processes to
enhance efficiency, reduce costs, and improve customer satisfaction. Existing
techniques to process simulation discovery lack adaptability to real-time
operational changes. In this paper, we propose a streaming process simulation
discovery technique that integrates Incremental Process Discovery with Online
Machine Learning methods. This technique prioritizes recent data while
preserving historical information, ensuring adaptation to evolving process
dynamics. Experiments conducted on four different event logs demonstrate the
importance in simulation of giving more weight to recent data while retaining
historical knowledge. Our technique not only produces more stable simulations
but also exhibits robustness in handling concept drift, as highlighted in one
of the use cases.

</details>


### [3] [The Effects of GitHub Copilot on Computing Students' Programming Effectiveness, Efficiency, and Processes in Brownfield Programming Tasks](https://arxiv.org/abs/2506.10051)
*Md Istiak Hossain Shihab,Christopher Hundhausen,Ahsun Tariq,Summit Haque,Yunhan Qiao,Brian Mulanda*

Main category: cs.SE

TL;DR: GitHub Copilot helps students work faster and more efficiently on unfamiliar code, but may hinder their understanding. Teaching methods need to adapt to balance AI benefits with understanding code logic.


<details>
  <summary>Details</summary>
Motivation: Computing students entering the workforce often work with existing code (brownfield development), but the effects of using AI coding assistants like GitHub Copilot in this context are not well understood.

Method: A controlled experiment with 10 undergraduate computer science students who performed similar brownfield programming tasks both with and without GitHub Copilot, using a mixed-methods approach: performance and behavioral analysis plus exit interviews.

Result: With Copilot, students completed tasks 35% faster and achieved 50% more solution progress. They spent 11% less time writing code and 12% less time searching the web. However, students expressed concerns about not fully understanding Copilot's code suggestions.

Conclusion: GenAI coding assistants like Copilot can greatly enhance student productivity and efficiency in brownfield programming tasks, but raise challenges for student understanding. Educators should adapt pedagogy to leverage AI benefits while encouraging deep understanding of AI-generated code.

Abstract: When graduates of computing degree programs enter the software industry, they
will most likely join teams working on legacy code bases developed by people
other than themselves. In these so-called brownfield software development
settings, generative artificial intelligence (GenAI) coding assistants like
GitHub Copilot are rapidly transforming software development practices, yet the
impact of GenAI on student programmers performing brownfield development tasks
remains underexplored. This paper investigates how GitHub Copilot influences
undergraduate students' programming performance, behaviors, and understanding
when completing brownfield programming tasks in which they add new code to an
unfamiliar code base. We conducted a controlled experiment in which 10
undergraduate computer science students completed highly similar brownfield
development tasks with and without Copilot in a legacy web application. Using a
mixed-methods approach combining performance analysis, behavioral analysis, and
exit interviews, we found that students completed tasks 35% faster (p < 0.05)
and made 50% more solution progress p (< 0.05) when using Copilot. Moreover,
our analysis revealed that, when using Copilot, students spent 11% less time
manually writing code (p < 0.05), and 12% less time conducting web searches (p
< 0.05), providing evidence of a fundamental shift in how they engaged in
programming. In exit interviews, students reported concerns about not
understanding how or why Copilot suggestions work. This research suggests the
need for computing educators to develop new pedagogical approaches that
leverage GenAI assistants' benefits while fostering reflection on how and why
GenAI suggestions address brownfield programming tasks. Complete study results
and analysis are presented at https://ghcopilot-icer.github.io/.

</details>


### [4] [Reward Models Enable Scalable Code Verification by Trading Accuracy for Throughput](https://arxiv.org/abs/2506.10056)
*Gabriel Orlanski,Nicholas Roberts,Aws Albarghouthi,Frederic Sala*

Main category: cs.SE

TL;DR: Using a fast, less accurate verifier for early pruning in program ranking can make the verification process over 11 times faster with only a small drop in accuracy, highlighting the practicality of outcome reward models for scalable and efficient coding task solutions with LLMs.


<details>
  <summary>Details</summary>
Motivation: The prevailing approach for coding tasks with LLMs emphasizes comprehensive verifiers, with little attention paid to speed-accuracy trade-offs. The paper seeks to challenge the assumption that full verifiers should always be prioritized over outcome reward models (ORMs).

Method: The authors systematically explore the trade-off between speed and accuracy in coding solutions by comparing ORM-based verification and full test suite verification. They particularly analyze a generate-prune-then-rank strategy, where a fast but less accurate verifier (ORM) is used to prune incorrect solutions before ranking.

Result: The generate-prune-then-rank system, which uses a fast ORM for pruning before a comprehensive verifier for ranking, achieves a speedup of 11.65x while incurring only an 8.33% accuracy loss compared to always using the full test suite.

Conclusion: Outcome reward models offer significant value in scaling verification, making program ranking much more efficient with minimal accuracy loss, especially when combined with comprehensive verifiers in a staged workflow.

Abstract: The standard paradigm for solving coding tasks via large language models
(LLMs) is to generate-then-rank programs, where the latter step uses a verifier
in the ranking process. The growing consensus is that a comprehensive verifier
(e.g., a full test suite) should be prioritized over an outcome reward model
(ORM) whenever possible, with little consideration given to the trade-offs
involved. We aim to challenge this assumption by systematically exploring the
tradeoff between speed and accuracy. We find that ORMs play a crucial role in
scaling verification through trading accuracy for speed, even when a
comprehensive verifier is available. Their value becomes especially apparent
when used in a generate-prune-then-rank approach, where a faster but less
accurate verifier removes incorrect solutions prior to ranking -- leading to a
system that is 11.65x faster while only being 8.33% less accurate than the full
test suite. We analyze the generate-prune-then-rank approach and show that it
works by filtering out incorrect but highly ranked solutions. These findings
enable the design of scalable and accurate program ranking systems.

</details>


### [5] [Prompt Variability Effects On LLM Code Generation](https://arxiv.org/abs/2506.10204)
*Andrei Paleyes,Radzim Sendyka,Diana Robinson,Christian Cabrera,Neil D. Lawrence*

Main category: cs.SE

TL;DR: The paper introduces new, broadly applicable methods for systematically evaluating how LLM-generated code quality changes based on user background and prompt variations, providing both experimental evidence and public code releases.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address how the quality and functionality of code generated by Large Language Models (LLMs) depend on the quality of the prompts, particularly influenced by the user's programming knowledge and background. The authors aim to better understand and quantify LLMs' sensitivity to these variations.

Method: The paper proposes two main methods: (1) a synthetic evaluation pipeline for code generation with LLMs; and (2) a systematic, persona-based evaluation approach that examines how LLM responses qualitatively differ based on variations in prospective user backgrounds. These methods are designed to be independent of both specific programming tasks and particular LLM models, making them broadly applicable.

Result: Experimental results illustrate the practical value of both the synthetic evaluation and persona-based approaches in uncovering sensitivity and qualitative differences in LLM-generated code. The authors also make their code available to the community.

Conclusion: The study concludes that the presented evaluation frameworks are effective at systematically quantifying and exposing how LLM code generation varies depending on prompt and user background. These widely applicable methods can help improve evaluation standards and understanding of LLM behavior in code generation tasks.

Abstract: Code generation is one of the most active areas of application of Large
Language Models (LLMs). While LLMs lower barriers to writing code and
accelerate development process, the overall quality of generated programs
depends on the quality of given prompts. Specifically, functionality and
quality of generated code can be sensitive to user's background and familiarity
with software development. It is therefore important to quantify LLM's
sensitivity to variations in the input. To this end we propose a synthetic
evaluation pipeline for code generation with LLMs, as well as a systematic
persona-based evaluation approach to expose qualitative differences of LLM
responses dependent on prospective user background. Both proposed methods are
completely independent from specific programming tasks and LLMs, and thus are
widely applicable. We provide experimental evidence illustrating utility of our
methods and share our code for the benefit of the community.

</details>


### [6] [AI-Based Software Vulnerability Detection: A Systematic Literature Review](https://arxiv.org/abs/2506.10280)
*Samiha Shimmi,Hamed Okhravi,Mona Rahimi*

Main category: cs.SE

TL;DR: This paper systematically reviews recent AI-based software vulnerability detection methods, finds graph-based models most common, notes limitations like dataset quality, and points to future opportunities in emerging AI technologies.


<details>
  <summary>Details</summary>
Motivation: Traditional software vulnerability detection methods are limited, prompting a need to understand and classify recent AI-driven approaches for improved cybersecurity.

Method: A systematic review of SVD literature from 2018-2023, including taxonomy creation and analysis of methods and trends in the field.

Result: 91% of studies reviewed use AI-based methods, with graph-based models most common. Key limitations and opportunities for less-explored techniques are identified.

Conclusion: The study concludes that AI-based methods dominate software vulnerability detection research, especially graph-based models. However, limitations like dataset quality and interpretability persist, and there are promising future directions in emerging technologies.

Abstract: Software vulnerabilities in source code pose serious cybersecurity risks,
prompting a shift from traditional detection methods (e.g., static analysis,
rule-based matching) to AI-driven approaches. This study presents a systematic
review of software vulnerability detection (SVD) research from 2018 to 2023,
offering a comprehensive taxonomy of techniques, feature representations, and
embedding methods. Our analysis reveals that 91% of studies use AI-based
methods, with graph-based models being the most prevalent. We identify key
limitations, including dataset quality, reproducibility, and interpretability,
and highlight emerging opportunities in underexplored techniques such as
federated learning and quantum neural networks, providing a roadmap for future
research.

</details>


### [7] [Minimizing False Positives in Static Bug Detection via LLM-Enhanced Path Feasibility Analysis](https://arxiv.org/abs/2506.10322)
*Xueying Du,Kai Yu,Chong Wang,Yi Zou,Wentai Deng,Zuoyu Ou,Xin Peng,Lingming Zhang,Yiling Lou*

Main category: cs.SE

TL;DR: LLM4PFA is a new LLM-based static bug analysis framework that dramatically reduces false positives (up to 96%) in large codebases compared to previous methods, with minimal loss in true bug detection.


<details>
  <summary>Details</summary>
Motivation: Existing static bug analyzers struggle with high false positive rates, especially in large codebases with complex control and data dependencies. Current solutions, including LLM-based approaches, are limited by poor constraint analysis and scalability issues.

Method: The paper proposes LLM4PFA, an iterative path feasibility analysis framework that leverages LLM agents for targeted constraint reasoning and context-aware analysis. The agent uses planned, context-driven analysis to better assess path feasibility, aiming to reduce false positives in static bug detection.

Result: LLM4PFA effectively filters out 72% to 96% of false positives in static bug detection, significantly outperforming baseline methods with 41.1% to 105.7% improvements. The system only misses 3 real bugs out of 45 true positives.

Conclusion: LLM4PFA provides a significant advancement in reducing false positives in static bug detection through sophisticated, scalable LLM-driven feasibility analysis, confirming its effectiveness and precision through strong evaluation results.

Abstract: Static bug analyzers play a crucial role in ensuring software quality.
However, existing analyzers for bug detection in large codebases often suffer
from high false positive rates. This is primarily due to the limited
capabilities of analyzers in path feasibility validation with multiple
conditional branches and complex data dependencies. While current LLM-based
approaches attempt to address this issue, their effectiveness remains limited
due to insufficient constraint cascade analysis and scalability challenges in
large projects. To address this challenge, we propose an iterative path
feasibility analysis framework LLM4PFA. By leveraging LLM agent based targeted
constraint reasoning, and key context-aware analysis driven by agent planning,
LLM4PFA effectively enhances complex inter-procedural path feasibility analysis
for minimizing false positives in static bug detection. Evaluation results show
that LLM4PFA precisely filters out 72% to 96% false positives reported during
static bug detection, significantly outperforming all the baselines by 41.1% -
105.7% improvements; meanwhile LLM4PFA only misses 3 real bugs of 45 true
positives.

</details>


### [8] [Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements](https://arxiv.org/abs/2506.10330)
*Seyed Moein Abtahi,Akramul Azim*

Main category: cs.SE

TL;DR: This paper demonstrates that integrating LLMs, static analysis, and RAG enhances automated code issue detection and correction, significantly improving code quality and efficiency while minimizing errors and resource use.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance code quality and streamline software development by automating code issue detection and revision, reducing manual labor and errors.

Method: The method integrates Large Language Models (LLMs), notably GPT-3.5 Turbo and GPT-4o, into static code analysis workflows. The process extracts and structures code issues, employs iterative prompt engineering for LLM-based revisions, uses retrieval-augmented generation (RAG) for better accuracy, and relies on a custom 'Code Comparison App' to detect and correct LLM hallucinations before applying changes.

Result: Static code analysis after applying LLM-based revisions showed a significant reduction in code issues, indicating improved code quality and development efficiency.

Conclusion: Combining LLMs, static analysis, and RAG effectively automates the detection and correction of code issues, reduces hallucinations, and leads to higher code quality and resource savings in software projects.

Abstract: This study examined code issue detection and revision automation by
integrating Large Language Models (LLMs) such as OpenAI's GPT-3.5 Turbo and
GPT-4o into software development workflows. A static code analysis framework
detects issues such as bugs, vulnerabilities, and code smells within a
large-scale software project. Detailed information on each issue was extracted
and organized to facilitate automated code revision using LLMs. An iterative
prompt engineering process is applied to ensure that prompts are structured to
produce accurate and organized outputs aligned with the project requirements.
Retrieval-augmented generation (RAG) is implemented to enhance the relevance
and precision of the revisions, enabling LLM to access and integrate real-time
external knowledge. The issue of LLM hallucinations - where the model generates
plausible but incorrect outputs - is addressed by a custom-built "Code
Comparison App," which identifies and corrects erroneous changes before
applying them to the codebase. Subsequent scans using the static code analysis
framework revealed a significant reduction in code issues, demonstrating the
effectiveness of combining LLMs, static analysis, and RAG to improve code
quality, streamline the software development process, and reduce time and
resource expenditure.

</details>


### [9] [AutoGEEval++: A Multi-Level and Multi-Geospatial-Modality Automated Evaluation Framework for Large Language Models in Geospatial Code Generation on Google Earth Engine](https://arxiv.org/abs/2506.10365)
*Shuyang Hou,Zhangxiao Shen,Huayi Wu,Haoyue Jiao,Ziqi Liu,Lutong Xie,Chang Liu,Jianyuan Liang,Yaxian Qing,Xiaopu Zhang,Dehua Peng,Zhipeng Gui,Xuefeng Guan*

Main category: cs.SE

TL;DR: AutoGEEval++ is a new automated framework and benchmark for evaluating LLM-generated geospatial code on Google Earth Engine, enabling standardized, multi-dimensional, and scalable assessment of 24 top LLMs, and setting the groundwork for future research in this domain.


<details>
  <summary>Details</summary>
Motivation: While geospatial code generation is increasingly important for integrating AI with geo-scientific tasks, there is a lack of standardized and automated evaluation tools to assess large language models (LLMs) in this domain.

Method: The paper introduces AutoGEEval++, an improved framework over AutoGEEval, which systematically evaluates LLMs that generate code for Google Earth Engine (GEE). AutoGEEval++ uses a Python API-based pipeline that includes a benchmark dataset with 6,365 test cases across 26 data types and three levels of task complexity. It offers automated submission, judging, and execution-based validation as well as multi-dimensional evaluation metrics (accuracy, resource usage, run-time, error types).

Result: AutoGEEval++ is used to evaluate 24 cutting-edge LLMs from various categories (general, reasoning, code-focused, geoscience-specific). Testing revealed distinct differences in model performance and error rates based on task type and model design, demonstrating the practical effectiveness and scalability of the framework.

Conclusion: AutoGEEval++ establishes the first standardized and systematic benchmark and evaluation protocol for LLM-generated GEE code, providing a robust foundation and unified methodology for fair performance comparisons and domain-specific code evaluation.

Abstract: Geospatial code generation is becoming a key frontier in integrating
artificial intelligence with geo-scientific analysis, yet standardised
automated evaluation tools for this task remain absent. This study presents
AutoGEEval++, an enhanced framework building on AutoGEEval, and the first
automated assessment system for large language models (LLMs) generating
geospatial code on Google Earth Engine (GEE). It supports diverse data
modalities and varying task complexities. Built on the GEE Python API,
AutoGEEval++ features a benchmark dataset-AutoGEEval++-Bench-with 6,365 test
cases across 26 data types and three task categories: unit, combo, and theme
tests. It includes a submission programme and a judge module to realise an
end-to-end automated evaluation pipeline from code generation to
execution-based validation. The framework adopts multi-dimensional
metrics-accuracy, resource usage, run-time efficiency, and error
types-balancing hallucination control and efficiency, and enabling boundary
testing and error pattern analysis. Using AutoGEEval++, we evaluate 24
state-of-the-art LLMs (as of June 2025), including general-purpose,
reasoning-enhanced, code-centric, and geoscience-specific models. Results
reveal clear performance, stability, and error differences across task types,
model designs, and deployment settings, confirming AutoGEEval++'s practical
value and scalability in vertical-domain code generation. This work establishes
the first standardised evaluation protocol and foundational benchmark for
GEE-based LLM code generation, providing a unified basis for performance
comparison and a methodological framework for systematic, domain-specific code
evaluation.

</details>


### [10] [Solving Package Management via Hypergraph Dependency Resolution](https://arxiv.org/abs/2506.10803)
*Ryan Gibb,Patrick Ferris,David Allsopp,Michael Winston Dales,Mark Elvers,Thomas Gazagnaire,Sadiq Jaffer,Thomas Leonard,Jon Ludlam,Anil Madhavapeddy*

Main category: cs.SE

TL;DR: HyperRes is a new formal system that models and resolves dependencies across different package management ecosystems, enabling cross-language and cross-platform compatibility without changing existing workflows.


<details>
  <summary>Details</summary>
Motivation: Package managers for different languages and operating systems lack interoperability, making it difficult for multi-lingual projects to express and manage dependencies across ecosystems. Additionally, system and hardware dependencies are often implicit and not versioned.

Method: The authors introduce HyperRes, a formal system based on a hypergraph model, to describe and resolve versioned dependencies. They also develop translations from many existing package managers to HyperRes, allowing cross-ecosystem dependency resolution.

Result: The approach is demonstrated across dozens of package management ecosystems. The authors show that it is possible to resolve dependencies across currently distinct systems and ecosystems without requiring users to abandon their preferred package managers.

Conclusion: HyperRes enables cross-ecosystem dependency resolution by translating packaging metadata between various systems. It allows for precise, environment-specific resolution without necessitating a change in how users currently manage their packages.

Abstract: Package managers are everywhere, with seemingly every language and operating
system implementing their own solution. The lack of interoperability between
these systems means that multi-lingual projects are unable to express precise
dependencies across language ecosystems, and external system and hardware
dependencies are typically implicit and unversioned. We define HyperRes, a
formal system for describing versioned dependency resolution using a hypergraph
that is expressive enough to model many ecosystems and solve dependency
constraints across them. We define translations from dozens of existing package
managers to HyperRes and comprehensively demonstrate that dependency resolution
can work across ecosystems that are currently distinct. This does not require
users to shift their choice of package managers; instead, HyperRes allows for
the translation of packaging metadata between ecosystems, and for solving to be
precisely specialised to a particular deployment environment.

</details>


### [11] [MLLM-Based UI2Code Automation Guided by UI Layout Information](https://arxiv.org/abs/2506.10376)
*Fan Wu,Cuiyun Gao,Shuqing Li,Xin-Cheng Wen,Qing Liao*

Main category: cs.SE

TL;DR: LayoutCoder is a new framework using multimodal large language models to automatically generate accurate website code from UI images, substantially improving over existing methods in both standard and new benchmarks.


<details>
  <summary>Details</summary>
Motivation: Automating the process of converting user interfaces (UIs) into code is vital for increasing website development efficiency, but existing deep learning methods require large labeled datasets and struggle to generalize to unseen web designs. Multimodal Large Language Models (MLLMs) could help, but face challenges in comprehending complex layouts and generating accurate, layout-preserving code.

Method: The paper proposes LayoutCoder, a novel framework based on MLLMs for UI code generation from real-world webpage images. LayoutCoder includes: (1) Element Relation Construction (identifies and groups structurally similar components to capture the UI layout); (2) UI Layout Parsing (generates UI layout trees to guide code generation); (3) Layout-Guided Code Fusion (produces accurate code with layout preserved). The authors also built Snap2Code, a new benchmark dataset of 350 real websites, divided into seen and unseen to prevent data leakage, alongside using Design2Code for evaluation.

Result: LayoutCoder outperforms state-of-the-art methods, achieving a 10.14% improvement in BLEU score and a 3.95% gain in CLIP score on average across all datasets compared to the best-performing baseline.

Conclusion: LayoutCoder, with its novel design focused on layout understanding and code generation from website images, effectively improves the accuracy and generalizability of automated UI2Code conversion, outperforming previous methods on benchmark datasets.

Abstract: Converting user interfaces into code (UI2Code) is a crucial step in website
development, which is time-consuming and labor-intensive. The automation of
UI2Code is essential to streamline this task, beneficial for improving the
development efficiency. There exist deep learning-based methods for the task;
however, they heavily rely on a large amount of labeled training data and
struggle with generalizing to real-world, unseen web page designs. The advent
of Multimodal Large Language Models (MLLMs) presents potential for alleviating
the issue, but they are difficult to comprehend the complex layouts in UIs and
generate the accurate code with layout preserved. To address these issues, we
propose LayoutCoder, a novel MLLM-based framework generating UI code from
real-world webpage images, which includes three key modules: (1) Element
Relation Construction, which aims at capturing UI layout by identifying and
grouping components with similar structures; (2) UI Layout Parsing, which aims
at generating UI layout trees for guiding the subsequent code generation
process; and (3) Layout-Guided Code Fusion, which aims at producing the
accurate code with layout preserved. For evaluation, we build a new benchmark
dataset which involves 350 real-world websites named Snap2Code, divided into
seen and unseen parts for mitigating the data leakage issue, besides the
popular dataset Design2Code. Extensive evaluation shows the superior
performance of LayoutCoder over the state-of-the-art approaches. Compared with
the best-performing baseline, LayoutCoder improves 10.14% in the BLEU score and
3.95% in the CLIP score on average across all datasets.

</details>


### [12] [Bug Classification in Quantum Software: A Rule-Based Framework and Its Evaluation](https://arxiv.org/abs/2506.10397)
*Mir Mohammad Yousuf,Shabir Ahmad Sofi*

Main category: cs.SE

TL;DR: A rule-based framework effectively classifies quantum software bugs by type, severity, and impacted qualities, achieving strong accuracy except for severity. Most bugs are classical and low-severity, but quantum-specific defects often involve circuit-level issues.


<details>
  <summary>Details</summary>
Motivation: There is a need for accurate and automated classification of software bugs specifically in quantum software repositories, due to the increasing complexity and specialized nature of quantum computing. Better classification helps improve overall software quality and maintenance.

Method: The paper introduces a rule-based, automated classification framework using keyword and heuristic-based techniques tailored for quantum computing bugs. The framework classifies issues by type, category, severity, and impacted quality attributes, with a special focus on quantum-specific bugs. It was evaluated by manually classifying a stratified sample of issues and comparing automated results to this ground truth using several performance metrics and statistical tests.

Result: The automated framework achieved up to 85.21% accuracy with F1-scores ranging from 0.7075 (severity) to 0.8393 (quality attribute). Cohen's Kappa showed substantial agreement for bug type, category, and quality attribute, but only slight agreement for severity classification. Classical bugs were more prevalent than quantum-specific bugs, and most reported issues were of low severity. Quantum circuit-level problems were the most common in quantum-specific bugs.

Conclusion: The proposed automated framework is reliable for most classification tasks in quantum software repositories, except for severity classification, which requires further improvement. The framework revealed significant trends in bug distribution and defect types, highlighting the dominance of classical bugs and the prevalence of circuit-level problems in quantum-related defects.

Abstract: Accurate classification of software bugs is essential for improving software
quality. This paper presents a rule-based automated framework for classifying
issues in quantum software repositories by bug type, category, severity, and
impacted quality attributes, with additional focus on quantum-specific bug
types. The framework applies keyword and heuristic-based techniques tailored to
quantum computing. To assess its reliability, we manually classified a
stratified sample of 4,984 issues from a dataset of 12,910 issues across 36
Qiskit repositories. Automated classifications were compared with ground truth
using accuracy, precision, recall, and F1-score. The framework achieved up to
85.21% accuracy, with F1-scores ranging from 0.7075 (severity) to 0.8393
(quality attribute). Statistical validation via paired t-tests and Cohen's
Kappa showed substantial to almost perfect agreement for bug type (k = 0.696),
category (k = 0.826), quality attribute (k = 0.818), and quantum-specific bug
type (k = 0.712). Severity classification showed slight agreement (k = 0.162),
suggesting room for improvement. Large-scale analysis revealed that classical
bugs dominate (67.2%), with quantum-specific bugs at 27.3%. Frequent bug
categories included compatibility, functional, and quantum-specific defects,
while usability, maintainability, and interoperability were the most impacted
quality attributes. Most issues (93.7%) were low severity; only 4.3% were
critical. A detailed review of 1,550 quantum-specific bugs showed that over
half involved quantum circuit-level problems, followed by gate errors and
hardware-related issues.

</details>


### [13] [Towards Understanding Bugs in Distributed Training and Inference Frameworks for Large Language Models](https://arxiv.org/abs/2506.10426)
*Xiao Yu,Haoxuan Chen,Feifei Niu,Xing Hu,Jacky Wai Keung,Xin Xia*

Main category: cs.SE

TL;DR: This paper empirically analyzes 308 bugs in popular distributed LLM training/inference frameworks, finding that half of fixes are simple, but diagnosing bugs often remains hard. The results suggest opportunities for automated bug fixing and improved framework reliability using these insights.


<details>
  <summary>Details</summary>
Motivation: Distributed training and inference frameworks are crucial for scaling large language model development, but their increasing software complexity introduces bugs that can degrade performance and waste resources. Understanding these bugs is key to improving software reliability and enabling better debugging tools.

Method: The authors conduct a large-scale empirical analysis of 308 fixed bugs in three major distributed framework—DeepSpeed, Megatron-LM, and Colossal-AI. They examine bug symptoms, root causes, efforts required for identification and fixing, and common strategies applied in bug resolution.

Result: The study finds that distributed frameworks exhibit unique bug causes like allocation strategy and communication errors. Many fixes (48%) are simple (≤10 lines of code), relying on straightforward strategies. However, diagnosing and resolving bugs is often difficult due to the disconnect between symptoms and causes, high effort needed for bug reproduction, and complicated interactions across system components.

Conclusion: The paper highlights opportunities to automate bug fixing in distributed frameworks, thanks to the simplicity of nearly half of observed fixes. Insights from this study suggest directions for building better debugging tools—including those powered by LLMs—and recommend improvements for the reliability of both distributed frameworks and dependent LLM projects.

Abstract: With the rapid development of large language models (LLMs), distributed
training and inference frameworks like DeepSpeed have become essential for
scaling model training and inference across multiple GPUs or nodes. However,
the increasing complexity of these frameworks brings non-trivial software bugs,
which may degrade training performance, cause unexpected failures, and result
in significant resource waste. Understanding framework bugs' characteristics is
fundamental for quality assurance, allowing the design of more effective
debugging and repair methods. Thus, our paper conducts the first large-scale
empirical analysis of 308 fixed bugs across three popular distributed
training/inference frameworks: DeepSpeed, Megatron-LM, and Colossal-AI. We
examine bug symptoms, root causes, bug identification and fixing efforts, and
common low-effort fixing strategies. Additionally, the distributed nature of
these frameworks introduces unique bug root causes, such as allocation strategy
error and distributed communication error. Diagnosing and fixing complex bugs
remains challenging due to factors like the disconnect between symptoms and
root causes, high bug reproduction costs, and low-level or cross-component
interactions. Interestingly, we observe that 48% of bug fixes require minimal
code changes (<=10 LOC) and follow simple strategies such as conditional logic
optimization, parameter handling enhancement, or version compatibility
handling, indicating potential for automation. Based on these insights, we
offer several implications for improving the reliability of both distributed
training and inference frameworks and their dependent LLM projects, while also
identifying opportunities to leverage LLM-based tools for automated debugging
and repair.

</details>


### [14] [EXPEREPAIR: Dual-Memory Enhanced LLM-based Repository-Level Program Repair](https://arxiv.org/abs/2506.10484)
*Fangwen Mu,Junjie Wang,Lin Shi,Song Wang,Shoubin Li,Qing Wang*

Main category: cs.SE

TL;DR: ExpeRepair leverages both concrete and abstract past repair knowledge with context-driven prompting, significantly improving LLM-based code repair over existing approaches.


<details>
  <summary>Details</summary>
Motivation: Automatic software repair is challenging, and current LLM-based methods have two key limitations: they don't learn from previous repairs and use static prompts that restrict adaptability.

Method: The paper introduces ExpeRepair, an LLM-based approach inspired by human memory systems. It uses episodic memory to store concrete past repair instances and semantic memory to store abstract insights. At inference, it retrieves both concrete and high-level memories and composes dynamic, context-driven prompts.

Result: ExpeRepair, evaluated on the SWE-bench Lite benchmark with Claude 3.7 Sonnet, achieves a pass@1 score of 49.3%, outperforming all current open-source methods.

Conclusion: Incorporating dual-memory systems and dynamic prompt construction enables LLMs to better generalize and adapt for software repair, leading to state-of-the-art performance.

Abstract: Automatically repairing software issues remains a fundamental challenge at
the intersection of software engineering and AI. Although recent advancements
in Large Language Models (LLMs) have demonstrated potential for
repository-level repair tasks, current methodologies exhibit two notable
limitations: (1) they often address issues in isolation, neglecting to
incorporate insights from previously resolved issues, and (2) they rely on
static and rigid prompting strategies, which constrain their ability to
generalize across diverse and evolving issue scenarios. Inspired by the dual
memory systems of human cognition, where episodic and semantic memories work
synergistically to support human reasoning and decision-making, we propose
ExpeRepair, a novel LLM-based approach that continuously learns from historical
repair experiences through dual-channel knowledge accumulation. ExpeRepair
organizes historical repair experiences into two complementary memories: an
episodic memory that stores concrete repair demonstrations, and a semantic
memory that encodes abstract reflective insights. At inference time, ExpeRepair
activates both memory systems by retrieving relevant demonstrations from
episodic memory and recalling high-level repair insights from semantic memory.
It further enhances adaptability through dynamic prompt composition,
synergistically integrating both memory types to replace static prompts with
context-aware, experience-driven prompts. Experiments on the SWE-bench Lite
benchmark demonstrate that ExpeRepair achieves a pass@1 score of 49.3% with
Claude 3.7 Sonnet, outperforming all state-of-the-art open-source methods.

</details>


### [15] [BugGen: A Self-Correcting Multi-Agent LLM Pipeline for Realistic RTL Bug Synthesis](https://arxiv.org/abs/2506.10501)
*Surya Jasper,Minh Luu,Evan Pan,Aakash Tyagi,Michael Quinn,Jiang Hu,David Kebo Houngninou*

Main category: cs.SE

TL;DR: BugGen is an autonomous LLM-driven system that quickly and accurately generates realistic hardware bugs, enhancing both coverage and efficiency in hardware verification and ML-based debugging.


<details>
  <summary>Details</summary>
Motivation: Due to increasing hardware complexity, verification resources are under pressure, and machine learning methods for debugging require diverse, scalable bug datasets. Existing ways to create such datasets (manual or automated bug insertion) are inadequate.

Method: The authors introduce BugGen, an autonomous, multi-agent pipeline using Large Language Models (LLMs) to automatically generate, insert, and validate realistic functional bugs in RTL (Register Transfer Level). BugGen partitions hardware modules, selects mutation targets, and uses iterative refinement and rollback processes to ensure correctness and detectability of inserted bugs.

Result: BugGen was tested on five OpenTitan IP blocks and produced 500 unique bugs with 94% functional accuracy, at a throughput over five times faster than manual expert insertion. It uncovered 104 previously undetected bugs and outperformed Certitude in syntactic accuracy, testbench blind spot exposure, and production of complex, meaningful bug scenarios. BugGen-generated datasets also helped ML-based debugging models achieve high classification accuracy (88.1%-93.2%).

Conclusion: BugGen offers a scalable and effective tool for generating realistic hardware bug datasets, improving both verification workflows and the effectiveness of ML-based debugging.

Abstract: Hardware complexity continues to strain verification resources, motivating
the adoption of machine learning (ML) methods to improve debug efficiency.
However, ML-assisted debugging critically depends on diverse and scalable bug
datasets, which existing manual or automated bug insertion methods fail to
reliably produce. We introduce BugGen, a first of its kind, fully autonomous,
multi-agent pipeline leveraging Large Language Models (LLMs) to systematically
generate, insert, and validate realistic functional bugs in RTL. BugGen
partitions modules, selects mutation targets via a closed-loop agentic
architecture, and employs iterative refinement and rollback mechanisms to
ensure syntactic correctness and functional detectability. Evaluated across
five OpenTitan IP blocks, BugGen produced 500 unique bugs with 94% functional
accuracy and achieved a throughput of 17.7 validated bugs per hour-over five
times faster than typical manual expert insertion. Additionally, BugGen
identified 104 previously undetected bugs in OpenTitan regressions,
highlighting its utility in exposing verification coverage gaps. Compared
against Certitude, BugGen demonstrated over twice the syntactic accuracy,
deeper exposure of testbench blind spots, and more functionally meaningful and
complex bug scenarios. Furthermore, when these BugGen-generated datasets were
employed to train ML-based failure triage models, we achieved high
classification accuracy (88.1%-93.2%) across different IP blocks, confirming
the practical utility and realism of generated bugs. BugGen thus provides a
scalable solution for generating high-quality bug datasets, significantly
enhancing verification efficiency and ML-assisted debugging.

</details>


### [16] [AdaptiveLLM: A Framework for Selecting Optimal Cost-Efficient LLM for Code-Generation Based on CoT Length](https://arxiv.org/abs/2506.10525)
*Junhang Cheng,Fang Liu,Chengru Wu,Li Zhang*

Main category: cs.SE

TL;DR: AdaptiveLLM improves code generation by automatically assessing task difficulty and selecting the most suitable LLM, resulting in higher accuracy and much lower resource use compared to baselines and single-model approaches, without needing human-annotated difficulty labels.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are widely used for code generation, but there is a need to balance between computational efficiency (cost) and performance (accuracy) across coding tasks with varying difficulty. Existing methods for selecting the best model are costly and depend on human-created difficulty labels, which are often unavailable and may not reflect the LLMs' real capabilities.

Method: The authors propose AdaptiveLLM, a system that dynamically assesses coding task difficulty using Chain-of-Thought (CoT) lengths, clusters these into three difficulty levels using k-means, embeds difficulty-aware features via fine-tuned CodeBERT, and employs an XGBoost classifier to choose the optimal LLM for each problem.

Result: AdaptiveLLM achieves a 7.86% increase in pass@1 score while reducing resource use by 88.9% compared to the ComplexityNet baseline. Versus a single model, it improves accuracy by about 15% with similar cost. The automatic difficulty assessment using CoT is more reliable than human labels.

Conclusion: AdaptiveLLM enables more efficient and accurate code generation by automatically estimating task difficulty and selecting the best LLM, outperforming baseline and single-model methods both in accuracy and resource use. The framework avoids dependence on human-annotated data and offers better cost-performance trade-offs.

Abstract: While Large Language Models (LLMs) have significantly advanced code
generation efficiency, they face inherent challenges in balancing performance
and inference costs across diverse programming tasks. Dynamically selecting the
optimal LLM based on task difficulty and resource constraints offers a
promising approach to achieve an optimal balance between efficiency and
performance. However, existing model selection methods are resource-intensive
and often neglect cost efficiency. Moreover, these approaches rely on
human-annotated difficulty labels that are frequently inaccessible in
real-world settings and may not align with the LLM's own assessment of task
difficulty. In this paper, we introduce AdaptiveLLM, a framework that
dynamically selects optimal LLMs for a given coding task by automatically
assessing task difficulty. Our framework first estimates task difficulty using
Chain-of-Thought lengths generated by reasoning model, clusters these into
three difficulty levels via k-means, and fine-tunes CodeBERT to embed
difficulty-aware features. A trained XGBoost classifier then selects the best
model for each problem, optimizing the performance-cost trade-off. Experimental
results show that AdaptiveLLM achieves a 7.86% improvement in pass@1 score
while reducing resource consumption by 88.9% compared to baseline method
ComplexityNet. When compared to a single model, AdaptiveLLM demonstrates an
approximately 15% accuracy improvement, while maintaining the same level of
cost consumption. Apart from that, the difficulty assessment using CoT provides
more reliable selection criteria than human evaluation. Our replication package
is available at https://github.com/cjhCoder7/AdaptiveLLM.

</details>


### [17] [Scalable Software Testing in Fast Virtual Platforms: Leveraging SystemC, QEMU and Containerization](https://arxiv.org/abs/2506.10624)
*Lukas Jünger,Jan Henrik Weinstock,Tim Kraus*

Main category: cs.SE

TL;DR: Using containerized, open-source virtual platforms for cloud-based, parallel software testing addresses hardware-software co-development challenges, enabling faster and more scalable verification in safety-critical domains.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of hardware/software (HW/SW) systems, particularly in safety-critical domains like automotive, creates challenges for early software development due to hardware availability lagging behind. Extensive testing is needed, but hardware delays hinder the process.

Method: The paper proposes using containerization to encapsulate Virtual Platforms (VPs) based on SystemC TLM-2.0. This reduces environment dependencies and enables cloud-based, parallelized test execution. Open-source VP technologies (QEMU and VCML) are used to avoid licensing constraints. The approach is demonstrated via a case study involving an Artificial Intelligence (AI) accelerator VP.

Result: The proposed approach enables pre-silicon execution and testing of unmodified target software efficiently. By containerizing VPs and leveraging the cloud, the methodology allows fast, parallel testing without expensive licenses. The case study validates the feasibility and effectiveness of this approach.

Conclusion: Containerized, open-source SystemC TLM-2.0-based VPs solve early-stage HW/SW system testing challenges by enabling scalable, environment-independent, cloud-based testing. This approach supports accelerated HW/SW co-development, especially in domains requiring extensive verification.

Abstract: The ever-increasing complexity of HW/SW systems presents a persistent
challenge, particularly in safety-critical domains like automotive, where
extensive testing is imperative. However, the availability of hardware often
lags behind, hindering early-stage software development. To address this,
Virtual Platforms (VPs) based on the SystemC TLM-2.0 standard have emerged as a
pivotal solution, enabling pre-silicon execution and testing of unmodified
target software. In this study, we propose an approach leveraging
containerization to encapsulate VPs in order to reduce environment dependencies
and enable cloud deployment for fast, parallelized test execution, as well as
open-source VP technologies such as QEMU and VCML to obviate the need for seat
licenses. To demonstrate the efficacy of our approach, we present an Artificial
Intelligence (AI) accelerator VP case study. Through our research, we offer a
robust solution to address the challenges posed by the complexity of HW/SW
systems, with practical implications for accelerating HW/SW co-development.

</details>


### [18] [Not One to Rule Them All: Mining Meaningful Code Review Orders From GitHub](https://arxiv.org/abs/2506.10654)
*Abir Bouraffa,Carolin Brandt,Andy Zaidmann,Walid Maalej*

Main category: cs.SE

TL;DR: Many reviewers do not follow the alphabetical order of files when reviewing pull requests. Instead, they use strategies such as reviewing the largest changes first or prioritizing test files. These strategies are more common in larger pull requests and suggest code review tools should support more flexible navigation options.


<details>
  <summary>Details</summary>
Motivation: Current code review tools like GitHub present changed files in alphabetical order, though this may not align with how reviewers prefer to navigate changes. Understanding preferred navigation orders can help improve review tools and processes.

Method: The researchers mined code review comments from 23,241 pull requests across 100 popular Java and Python GitHub repositories. They analyzed the sequence of file reviews and comments to identify common navigation strategies.

Result: 44.6% of pull requests were reviewed in a non-alphabetical order. Within this group, significant proportions used largest-diff-first, similarity-to-title/description, or test-first strategies. Non-alphabetical orders were associated with a higher proportion of reviewed files, but slightly fewer overall approvals.

Conclusion: The study concludes that developers often adopt complex, meaningful navigation strategies rather than the default alphabetical order, especially for larger pull requests. This suggests a need for better tool support accommodating diverse review workflows.

Abstract: Developers use tools such as GitHub pull requests to review code, discuss
proposed changes, and request modifications. While changed files are commonly
presented in alphabetical order, this does not necessarily coincide with the
reviewer's preferred navigation sequence. This study investigates the different
navigation orders developers follow while commenting on changes submitted in
pull requests. We mined code review comments from 23,241 pull requests in 100
popular Java and Python repositories on GitHub to analyze the order in which
the reviewers commented on the submitted changes. Our analysis shows that for
44.6% of pull requests, the reviewers comment in a non-alphabetical order.
Among these pull requests, we identified traces of alternative meaningful
orders: 20.6% (2,134) followed a largest-diff-first order, 17.6% (1,827) were
commented in the order of the files' similarity to the pull request's title and
description, and 29% (1,188) of pull requests containing changes to both
production and test files adhered to a test-first order. We also observed that
the proportion of reviewed files to total submitted files was significantly
higher in non-alphabetically ordered reviews, which also received slightly
fewer approvals from reviewers, on average. Our findings highlight the need for
additional support during code reviews, particularly for larger pull requests,
where reviewers are more likely to adopt complex strategies rather than
following a single predefined order.

</details>


### [19] [Formalising Software Requirements using Large Language Models](https://arxiv.org/abs/2506.10704)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: VERIFAI is an early-stage project aiming to automate the traceability and verification of natural language software requirements using AI, NLP, ontologies, and LLMs to generate formal specs and track requirements throughout software development.


<details>
  <summary>Details</summary>
Motivation: Traceability and verification of requirements in software systems are difficult because requirements are typically written in natural language, and translating these into formal specifications is complex and error-prone. There is also often a lack of tools supporting automated traceability throughout the system lifecycle.

Method: The project investigates a combination of Natural Language Processing, ontologies to model domains, similarity-based reuse of existing artefacts, and large language models (LLMs) to automate the identification and formalization of requirements. AI technologies guide the traceability and verification processes.

Result: The project is in its early stages; initial findings suggest that leveraging NLP, ontologies, LLMs, and AI can potentially automate and improve the traceability and verification of requirements, though concrete results are yet to be detailed.

Conclusion: Automatic generation of formal specifications and improved traceability are feasible with advances in AI, LLMs, and ontologies. The project lays out foundational methodologies to be developed further for practical application in software engineering.

Abstract: This paper is a brief introduction to our recently initiated project named
VERIFAI: Traceability and verification of natural language requirements. The
project addresses the challenges in the traceability and verification of formal
specifications through providing support for the automatic generation of the
formal specifications and the traceability of the requirements from the initial
software design stage through the systems implementation and verification.
Approaches explored in this project include Natural Language Processing, use of
ontologies to describe the software system domain, reuse of existing software
artefacts from similar systems (i.e. through similarity based reuse) and large
language models to identify and declare the specifications as well as use of
artificial intelligence to guide the process.

</details>


### [20] [From Tea Leaves to System Maps: Context-awareness in Monitoring Operational Machine Learning Models](https://arxiv.org/abs/2506.10770)
*Joran Leest,Claudia Raibulet,Patricia Lago,Ilias Gerostathopoulos*

Main category: cs.SE

TL;DR: This paper reviews and organizes how contextual information should be used in ML monitoring, proposing the C-SAR framework and identifying practical patterns for better system reliability, moving beyond just spotting statistical anomalies.


<details>
  <summary>Details</summary>
Motivation: Current ML models in production often fail not just due to statistical anomalies but because of contextual misalignments—changes in the environment not anticipated during training. Existing monitoring practices lack a unified understanding or effective use of contextual information, though context is crucial for interpreting performance and diagnosing issues.

Method: The paper conducts a systematic review of 94 studies from diverse fields like data mining, databases, software engineering, and ML. It characterizes types of contextual information for ML monitoring and introduces the Contextual System--Aspect--Representation (C-SAR) framework to synthesize findings. Additionally, it identifies 20 reusable patterns linking system, aspect, and representation, mapping them to monitoring activities.

Result: The study introduces C-SAR, a conceptual model to structure contextual information in ML monitoring. It also articulates 20 recurring patterns that can be used across monitoring activities. This work reframes ML monitoring from just detecting statistical anomalies to systematically incorporating context for more reliable and actionable monitoring.

Conclusion: The paper establishes a new theoretical framework and set of patterns for using contextual information in ML monitoring. This advances practice from surface-level anomaly detection to deep, systematic root-cause analysis and reliable system management. It emphasizes that successful monitoring hinges on context-aware approaches rather than pure statistical observation.

Abstract: Machine learning (ML) models in production do not fail due to statistical
anomalies in their input data; they fail due to contextual misalignment -- when
their environment deviates from training assumptions, leading to unreliable
predictions. Effective ML monitoring requires rich contextual information to
move beyond detecting statistical shifts toward meaningful alerts and
systematic root-cause analysis. Yet, surprisingly, despite extensive research
in ML monitoring and related disciplines (drift detection, data validation,
out-of-distribution detection), there is no shared understanding of how to use
contextual information -- striking, given that monitoring involves
interpretation of information in context. In response, this paper presents a
systematic review to characterize and structure the various types of contextual
information in this domain. Our analysis examines 94 primary studies across
data mining, databases, software engineering, and ML. We introduce the
Contextual System--Aspect--Representation (C-SAR) framework, a conceptual model
that synthesizes our findings. We also identify 20 recurring and potentially
reusable patterns of specific system, aspect, and representation combinations,
and map them to the monitoring activities they support. This study provides a
new perspective on ML monitoring: from interpreting "tea leaves" of
observational statistics into constructing and managing "system maps" that
enable systematic and reliable ML monitoring practices.

</details>


### [21] [What Users Value and Critique: Large-Scale Analysis of User Feedback on AI-Powered Mobile Apps](https://arxiv.org/abs/2506.10785)
*Vinaik Chhetri,Krishna Upadhyay,A. B. Siddique,Umar Farooq*

Main category: cs.SE

TL;DR: This paper presents a validated, scalable pipeline using LLMs to extract fine-grained user sentiments from nearly 900K reviews of 292 AI-powered apps. It reveals that users' satisfaction and frustrations hinge on a few key themes, and demonstrates that nuanced feedback—often missed in prior work—can be systematically surfaced to guide app improvement.


<details>
  <summary>Details</summary>
Motivation: The authors observe a lack of understanding about how users truly perceive and critique AI features in mobile apps due to the massive amount of user feedback, which is difficult to analyze manually or with traditional approaches.

Method: They curated a large dataset of 894,000 AI-specific user reviews from 292 AI-enabled apps across 14 categories. They then designed and validated a multi-stage analysis pipeline (including human benchmarking, use of LLMs, and prompting strategies) to classify, extract, and cluster aspect-sentiment pairs for high-precision insights.

Result: Their pipeline extracted over a million aspect-sentiment pairs, sorted into 18 positive and 15 negative user topics. Users consistently focus on productivity, reliability, and personalization positively, whereas they complain most about technical failures, pricing, and language support. Their approach detects nuanced, co-occurring sentiments within single reviews, which traditional methods often miss. Category-wise analysis uncovered both shared and unique user experience drivers.

Conclusion: The study introduces a scalable, high-precision pipeline for mining large-scale user feedback on AI-powered apps, offering richer and more nuanced insights than previous, more superficial methods. This approach yields a more accurate picture of real user experiences and can help developers improve AI features in mobile apps.

Abstract: Artificial Intelligence (AI)-powered features have rapidly proliferated
across mobile apps in various domains, including productivity, education,
entertainment, and creativity. However, how users perceive, evaluate, and
critique these AI features remains largely unexplored, primarily due to the
overwhelming volume of user feedback. In this work, we present the first
comprehensive, large-scale study of user feedback on AI-powered mobile apps,
leveraging a curated dataset of 292 AI-driven apps across 14 categories with
894K AI-specific reviews from Google Play. We develop and validate a
multi-stage analysis pipeline that begins with a human-labeled benchmark and
systematically evaluates large language models (LLMs) and prompting strategies.
Each stage, including review classification, aspect-sentiment extraction, and
clustering, is validated for accuracy and consistency. Our pipeline enables
scalable, high-precision analysis of user feedback, extracting over one million
aspect-sentiment pairs clustered into 18 positive and 15 negative user topics.
Our analysis reveals that users consistently focus on a narrow set of themes:
positive comments emphasize productivity, reliability, and personalized
assistance, while negative feedback highlights technical failures (e.g.,
scanning and recognition), pricing concerns, and limitations in language
support. Our pipeline surfaces both satisfaction with one feature and
frustration with another within the same review. These fine-grained,
co-occurring sentiments are often missed by traditional approaches that treat
positive and negative feedback in isolation or rely on coarse-grained analysis.
To this end, our approach provides a more faithful reflection of the real-world
user experiences with AI-powered apps. Category-aware analysis further uncovers
both universal drivers of satisfaction and domain-specific frustrations.

</details>


### [22] [Evaluating Large Language Models on Non-Code Software Engineering Tasks](https://arxiv.org/abs/2506.10833)
*Fabian C. Peña,Steffen Herbold*

Main category: cs.SE

TL;DR: This paper introduces SELU, a benchmark for testing how well language models perform on non-code software engineering tasks. It finds that moderately-sized decoder-only models work best, and that focusing on code during pre-training offers limited advantages for these tasks.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) have shown strong capabilities in code tasks, but their utility for non-code Software Engineering (SE) tasks has not been thoroughly evaluated. There is a need for a comprehensive benchmark to analyze and compare LLM performance in such tasks.

Method: The authors introduce the 'Software Engineering Language Understanding' (SELU) benchmark covering 17 non-code SE tasks, including classification, regression, NER, and MLM. They draw data from various SE-related sources. They evaluate 22 fine-tuned open-source LLMs, 2 proprietary models (prompted), and 2 baseline models, using diverse performance metrics and statistical tests to compare results.

Result: Moderate-scale decoder-only models achieve the highest and most consistent performance across the tasks, while domain adaptation through code-focused pre-training only provides limited additional benefit.

Conclusion: The SELU benchmark facilitates robust evaluation and comparison of LLMs on non-code SE tasks, guiding model selection for such workflows. The research also underscores the need to expand the benchmark toward generative and design-oriented tasks.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
code understanding and generation; however, their effectiveness on non-code
Software Engineering (SE) tasks remains underexplored. We present the first
comprehensive benchmark, which we name `Software Engineering Language
Understanding' (SELU), for evaluating LLMs on 17 non-code tasks, spanning from
identifying whether a requirement is functional or non-functional to estimating
the effort and complexity of backlog items. SELU covers classification,
regression, Named Entity Recognition (NER), and Masked Language Modeling (MLM)
targets, with data drawn from diverse sources such as code repositories, issue
tracking systems, and developer forums. We fine-tune 22 open-source LLMs,
prompt two proprietary alternatives, and train two baselines. Performance is
measured using metrics such as F1-macro, SMAPE, F1-micro, and accuracy, and
compared via the Bayesian signed-rank test. Our results show that
moderate-scale decoder-only models consistently form a top-tier, exhibiting
high mean performance and low across-task variance, while domain adaptation via
code-focused pre-training might yield only modest improvements. These insights
guide model selection for non-code SE workflows and highlight directions for
expanding SELU to generative and design-oriented scenarios.

</details>


### [23] [MultiCoSim: A Python-based Multi-Fidelity Co-Simulation Framework](https://arxiv.org/abs/2506.10869)
*Quinn Thibeault,Giulia Pedrielli*

Main category: cs.SE

TL;DR: Existing CPS simulation tools are rigid and hard to adapt. MultiCoSim, a Python-based framework, offers programmatic, modular, and flexible co-simulation, supporting both custom and off-the-shelf components, which helps streamline CPS research and development.


<details>
  <summary>Details</summary>
Motivation: As cyber-physical systems (CPS) become more complex, especially in safety-critical and learning-enabled contexts, there is a growing need for simulation tools that can handle heterogeneous components and variable fidelity effectively. Existing simulation frameworks are often rigid, lack automation, and do not support easy integration or modularity, hindering research and development.

Method: The authors propose MultiCoSim, a Python-based simulation framework. MultiCoSim allows users to programmatically define, compose, and configure simulation components. It supports distributed, component-based co-simulation and enables easy substitution and reconfiguration of components. Case studies—including custom automaton-based controllers and integration with PX4 autopilot—are used to demonstrate the framework's flexibility.

Result: MultiCoSim streamlines the process of setting up and managing co-simulations involving diverse components. The provided case studies show successful integration and improved flexibility, usability, and modularity compared to conventional tools. MultiCoSim facilitates comparative and automated evaluation of CPS.

Conclusion: MultiCoSim addresses key limitations in existing CPS simulation platforms by enabling flexible, modular, and automated co-simulation of heterogeneous components, thereby accelerating research and development in the field.

Abstract: Simulation is a foundational tool for the analysis and testing of
cyber-physical systems (CPS), underpinning activities such as algorithm
development, runtime monitoring, and system verification. As CPS grow in
complexity and scale, particularly in safety-critical and learning-enabled
settings, accurate analysis and synthesis increasingly rely on the rapid use of
simulation experiments. Because CPS inherently integrate hardware, software,
and physical processes, simulation platforms must support co-simulation of
heterogeneous components at varying levels of fidelity. Despite recent advances
in high-fidelity modeling of hardware, firmware, and physics, co-simulation in
diverse environments remains challenging. These limitations hinder the
development of reusable benchmarks and impede the use of simulation for
automated and comparative evaluation.
  Existing simulation tools often rely on rigid configurations, lack automation
support, and present obstacles to portability and modularity. Many are
configured through static text files or impose constraints on how simulation
components are represented and connected, making it difficult to flexibly
compose systems or integrate components across platforms.
  To address these challenges, we introduce MultiCoSim, a Python-based
simulation framework that enables users to define, compose, and configure
simulation components programmatically. MultiCoSim supports distributed,
component-based co-simulation and allows seamless substitution and
reconfiguration of components. We demonstrate the flexibility of MultiCoSim
through case studies that include co-simulations involving custom
automaton-based controllers, as well as integration with off-the-shelf
platforms like the PX4 autopilot for aerial robotics. These examples highlight
MultiCoSim's capability to streamline CPS simulation pipelines for research and
development.

</details>


### [24] [SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks](https://arxiv.org/abs/2506.10954)
*Lianghong Guo,Yanlin Wang,Caihua Li,Pengyu Yang,Jiachi Chen,Wei Tao,Yingtian Zou,Duyu Tang,Zibin Zheng*

Main category: cs.SE

TL;DR: SWE-Factory automates the challenging process of building and validating GitHub issue resolution datasets, dramatically reducing manual effort, lowering costs, and achieving highly reliable results for software engineering LLM research.


<details>
  <summary>Details</summary>
Motivation: Creating large-scale datasets for GitHub issue resolution is vital for training and evaluating Large Language Models (LLMs) in software engineering. However, building these benchmarks traditionally requires significant manual effort due to the complexities in evaluation, grading, and validation.

Method: The authors propose SWE-Factory, an automated pipeline composed of three core components: (1) SWE-Builder—a multi-agent system for automating environment construction using collaborative agents and an environment memory pool; (2) a standardized, exit-code-based grading system that replaces manual custom parsers; and (3) automated fail2pass validation leveraging exit code signals.

Result: The pipeline was tested on 671 issues across four programming languages. SWE-Builder (using GPT-4.1-mini) generated 269 valid instances at $0.045 each, and Gemini-2.5-flash performed similarly for $0.024 per instance. The exit-code grading system matched manual inspection with 100% accuracy, and fail2pass validation achieved a precision of 0.92 and a recall of 1.00.

Conclusion: SWE-Factory effectively automates the creation, grading, and validation of issue resolution tasks, making it feasible to scale up the construction of high-quality datasets for evaluating and training LLMs in software engineering. The tools and datasets are publicly available.

Abstract: Constructing large-scale datasets for the GitHub issue resolution task is
crucial for both training and evaluating the software engineering capabilities
of Large Language Models (LLMs). However, the traditional process for creating
such benchmarks is notoriously challenging and labor-intensive, particularly in
the stages of setting up evaluation environments, grading test outcomes, and
validating task instances. In this paper, we propose SWE-Factory, an automated
pipeline designed to address these challenges. To tackle these issues, our
pipeline integrates three core automated components. First, we introduce
SWE-Builder, a multi-agent system that automates evaluation environment
construction, which employs four specialized agents that work in a
collaborative, iterative loop and leverages an environment memory pool to
enhance efficiency. Second, we introduce a standardized, exit-code-based
grading method that eliminates the need for manually writing custom parsers.
Finally, we automate the fail2pass validation process using these reliable exit
code signals. Experiments on 671 issues across four programming languages show
that our pipeline can effectively construct valid task instances; for example,
with GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at $0.045 per
instance, while with Gemini-2.5-flash, it achieves comparable performance at
the lowest cost of $0.024 per instance. We also demonstrate that our
exit-code-based grading achieves 100% accuracy compared to manual inspection,
and our automated fail2pass validation reaches a precision of 0.92 and a recall
of 1.00. We hope our automated pipeline will accelerate the collection of
large-scale, high-quality GitHub issue resolution datasets for both training
and evaluation. Our code and datasets are released at
https://github.com/DeepSoftwareAnalytics/swe-factory.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [25] [From Tool Calling to Symbolic Thinking: LLMs in a Persistent Lisp Metaprogramming Loop](https://arxiv.org/abs/2506.10021)
*Jordi de la Torre*

Main category: cs.PL

TL;DR: A new system bridges LLMs with a live Lisp environment, allowing models to program, create tools, and remember state through symbolic interaction, setting the stage for more interactive and capable AI.


<details>
  <summary>Details</summary>
Motivation: Current large language models (LLMs) are limited in their ability to remember state and dynamically extend their capabilities. Integrating them with a persistent programming environment could overcome these limitations.

Method: The authors embed a large language model within an interactive Lisp environment. By embedding Lisp expressions in LLM outputs and intercepting them with middleware, the system enables programmatic dialogue, stateful memory, and dynamic tool creation via a live REPL.

Result: The system successfully allows LLMs to define, invoke, and evolve custom tools, providing stateful interaction and programmability. The authors also establish a set of architectural principles and a design framework for combining symbolic programming (Lisp) and neural language generation.

Conclusion: This architecture offers a promising approach for enhancing LLMs with external memory, dynamic tool use, and reflective programming by integrating them with a symbolic environment. It lays important groundwork for future AI systems combining symbolic and neural computation.

Abstract: We propose a novel architecture for integrating large language models (LLMs)
with a persistent, interactive Lisp environment. This setup enables LLMs to
define, invoke, and evolve their own tools through programmatic interaction
with a live REPL. By embedding Lisp expressions within generation and
intercepting them via a middleware layer, the system allows for stateful
external memory, reflective programming, and dynamic tool creation. We present
a design framework and architectural principles to guide future implementations
of interactive AI systems that integrate symbolic programming with neural
language generation.

</details>


### [26] [A Language-Agnostic Logical Relation for Message-Passing Protocols](https://arxiv.org/abs/2506.10026)
*Tesla Zhang,Sonya Simkin,Rui Li,Yue Yao,Stephanie Balzer*

Main category: cs.PL

TL;DR: This paper introduces and mechanizes a language-agnostic verification framework for certifying protocol compliance in heterogeneous, message-passing systems, overcoming the limitations of traditional language or type system-based approaches.


<details>
  <summary>Details</summary>
Motivation: Modern distributed and heterogeneous systems, such as those used in cloud computing and IoT, often involve concurrent, message-passing applications interacting with diverse and sometimes foreign components. Traditional verification, which relies on shared language or type system assumptions, is not applicable in these environments, motivating the need for new verification frameworks.

Method: The paper introduces a new framework based on a labelled transition-based semantics to define a language-agnostic logical relation for protocol compliance. This framework is flexible enough to include both typed and untyped components, as well as foreign objects. The authors mechanize their definition and verification scenarios in the Coq theorem prover.

Result: The framework supports certifying protocol compliance in two different ways: (1) verifying compliance for specific application or hardware device instances, and (2) universally verifying all well-typed applications under a given type system. Both approaches are demonstrated and mechanized in Coq.

Conclusion: The work provides the first mechanized language-agnostic logical relation for protocol verification in heterogeneous message-passing systems, enabling protocol compliance certification even when usual language or type system assumptions do not hold.

Abstract: Today's computing landscape has been gradually shifting to applications
targeting distributed and *heterogeneous* systems, such as cloud computing and
Internet of Things (IoT) applications. These applications are predominantly
*concurrent*, employ *message-passing*, and interface with *foreign objects*,
ranging from externally implemented code to actual physical devices such as
sensors. Verifying that the resulting systems adhere to the intended protocol
of interaction is challenging -- the usual assumption of a common
implementation language, let alone a type system, no longer applies, ruling out
any verification method based on them. This paper develops a framework for
certifying *protocol compliance* of heterogeneous message-passing systems. It
contributes the first mechanization of a *language-agnostic logical relation*,
asserting that its inhabitants comply with the protocol specified. This
definition relies entirely on a labelled transition-based semantics,
accommodating arbitrary inhabitants, typed and untyped alike, including foreign
objects. As a case study, the paper considers two scenarios: (1) *per-instance
verification* of a specific application or hardware device, and (2)
*once-and-for-all verification* of well-typed applications for a given type
system. The logical relation and both scenarios are mechanized in the Coq
theorem prover.

</details>


### [27] [Hazel Deriver: A Live Editor for Constructing Rule-Based Derivations](https://arxiv.org/abs/2506.10781)
*Zhiyao Zhong,Cyrus Omar*

Main category: cs.PL

TL;DR: Hazel Deriver is a web-based tool that helps students construct rule-based derivation trees more easily and effectively, providing interactive support and real-time feedback. Initial studies show it lowers difficulty and boosts understanding, but questions remain about the optimal amount of system guidance.


<details>
  <summary>Details</summary>
Motivation: Students in programming languages and formal logic often find it challenging to build rule-based derivation trees due to complex inference rules, insufficient feedback, and the tedious nature of handwritten proofs.

Method: The authors introduce Hazel Deriver, a web-based, live editor that scaffolds user derivation construction through interactive and structured support. Built on the Hazel environment, it offers real-time feedback and encourages exploration. They conducted a preliminary user study with students to evaluate its effectiveness.

Result: The user study indicates that Hazel Deriver reduces the perceived difficulty of derivation tasks and enhances students' conceptual understanding and engagement.

Conclusion: Hazel Deriver effectively assists students in constructing derivation trees by offering multiple layers of support and feedback. The paper also highlights the importance of balancing automated guidance with user independence.

Abstract: Students in programming languages and formal logic courses often struggle
with constructing rule-based derivation trees due to the complexity of applying
inference rules, the lack of immediate feedback, and the manual effort required
for handwritten proofs. We present Hazel Deriver, a live, web-based editor
designed to scaffold derivation construction through multiple layers of
support. Built on the Hazel live programming environment, it provides a
structured, interactive experience that encourages iterative exploration and
real-time feedback. A preliminary user study with former students suggests that
Hazel Deriver reduces the perceived difficulty of derivation tasks while
improving conceptual understanding and engagement. We discuss the design of its
layered scaffolding features and raise questions about balancing system
guidance with learner autonomy.

</details>


### [28] [Choreographic Quick Changes: First-Class Location (Set) Polymorphism](https://arxiv.org/abs/2506.10913)
*Ashley Samuelson,Andrew K. Hirsch,Ethan Cecchetti*

Main category: cs.PL

TL;DR: λ_{QC} is a new choreographic language for concurrent systems that allows dynamic assignment of computations, first class process names, and advanced type features, all within a mechanically verified framework ensuring deadlock freedom.


<details>
  <summary>Details</summary>
Motivation: Existing choreographic programming languages, though useful for concurrent systems, lack support for dynamic assignment of computation (letting a node compute and inform others who should perform which tasks) as well as features important for modern applications like process name handling and polymorphism over locations.

Method: The paper introduces a new choreographic language, called λ_{QC}, which adds first class process names and polymorphism over types and sets of locations. The language supports algebraic and recursive data types and values associated with multiple locations. The work is formalized and mechanically verified in the Rocq system.

Result: λ_{QC} provides greater expressive power than previous choreographic languages. Its features allow a node to dynamically compute and communicate computational assignments. The authors prove standard guarantees, including deadlock freedom, within a verified framework.

Conclusion: The introduction of λ_{QC} significantly enhances choreographic programming for concurrent systems by supporting dynamic process assignment and extending the language with modern features, all with formally verified guarantees.

Abstract: Choreographic programming is a promising new paradigm for programming
concurrent systems where a developer writes a single centralized program that
compiles to individual programs for each node. Existing choreographic
languages, however, lack critical features integral to modern systems, like the
ability of one node to dynamically compute who should perform a computation and
send that decision to others. This work addresses this gap with $\lambda_{QC}$,
the first typed choreographic language with \emph{first class process names}
and polymorphism over both types and (sets of) locations. $\lambda_{QC}$ also
improves expressive power over previous work by supporting algebraic and
recursive data types as well as multiply-located values. We formalize and
mechanically verify our results in Rocq, including the standard choreographic
guarantee of deadlock freedom.

</details>
