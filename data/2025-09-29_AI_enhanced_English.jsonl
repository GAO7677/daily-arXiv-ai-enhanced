{"id": "2509.21629", "categories": ["cs.PL", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21629", "abs": "https://arxiv.org/abs/2509.21629", "authors": ["Anjiang Wei", "Tarun Suresh", "Tianran Sun", "Haoze Wu", "Ke Wang", "Alex Aiken"], "title": "InvBench: Can LLMs Accelerate Program Verification with Invariant Synthesis?", "comment": null, "summary": "Program verification relies on loop invariants, yet automatically discovering\nstrong invariants remains a long-standing challenge. We introduce a principled\nframework for evaluating LLMs on invariant synthesis. Our approach uses a\nverifier-based decision procedure with a formal soundness guarantee and\nassesses not only correctness but also the speedup that invariants provide in\nverification. We evaluate 7 state-of-the-art LLMs, and existing LLM-based\nverifiers against the traditional solver UAutomizer. While LLM-based verifiers\nrepresent a promising direction, they do not yet offer a significant advantage\nover UAutomizer. Model capability also proves critical, as shown by sharp\ndifferences in speedups across models, and our benchmark remains an open\nchallenge for current LLMs. Finally, we show that supervised fine-tuning and\nBest-of-N sampling can improve performance: fine-tuning on 3589 instances\nraises the percentage of speedup cases for Qwen3-Coder-480B from 8% to 29.2%,\nand Best-of-N sampling with N=16 improves Claude-sonnet-4 from 8.8% to 22.1%.", "AI": {"tldr": "The paper presents a new framework to formally evaluate LLMs in discovering program loop invariants. Current LLM-based verifiers are promising but still lag behind traditional tools like UAutomizer. Performance depends on the LLM used, and techniques like fine-tuning and Best-of-N sampling can boost results. The benchmark poses a challenge for current models, suggesting room for further advancement.", "motivation": "Automatically discovering strong loop invariants for program verification is a long-standing, unsolved challenge. With the rise of large language models (LLMs), there is a need to rigorously evaluate their capabilities in invariant synthesis to determine if they can advance the state-of-the-art.", "method": "The paper introduces a principled evaluation framework that leverages a verifier-based decision procedure with formal soundness guarantees. It systematically assesses LLMs and LLM-based verification tools against traditional solvers (such as UAutomizer) on criteria of correctness and verification speedup.", "result": "Evaluations of seven state-of-the-art LLMs and existing LLM-based verifiers reveal that, although promising, these approaches currently do not surpass UAutomizer in effectiveness. Model capability significantly affects performance, demonstrated by substantial differences in speedup achievements between models. Supervised fine-tuning and Best-of-N sampling both improve outcomes: fine-tuning notably increases the success rate for Qwen3-Coder-480B, and Best-of-N sampling yields higher speedup cases for Claude-sonnet-4.", "conclusion": "While LLM-based approaches point to a promising future in invariant synthesis, they have not yet consistently outperformed traditional solvers. Performance disparities highlight the importance of model choice, and improvement can be achieved through methods like fine-tuning and sampling. Their benchmark establishes a challenge for the current generation of LLMs and a basis for future research."}}
{"id": "2509.21793", "categories": ["cs.PL", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21793", "abs": "https://arxiv.org/abs/2509.21793", "authors": ["Jianhong Zhao", "Everett Hildenbrandt", "Juan Conejero", "Yongwang Zhao"], "title": "Compiling by Proving: Language-Agnostic Automatic Optimization from Formal Semantics", "comment": null, "summary": "Verification proofs encode complete program behavior, yet we discard them\nafter checking correctness. We present compiling by proving, a paradigm that\ntransforms these proofs into optimized execution rules. By constructing\nAll-Path Reachability Proofs through symbolic execution and compiling their\ngraph structure, we consolidate many semantic rewrites into single rules while\npreserving correctness by construction. We implement this as a\nlanguage-agnostic extension to the K framework. Evaluation demonstrates\nperformance improvements across different compilation scopes: opcode-level\noptimizations show consistent speedups, while whole-program compilation\nachieves orders of magnitude greater performance gains.", "AI": {"tldr": "The paper introduces a method to turn program verification proofs into efficient execution rules using symbolic execution and the K framework, resulting in major performance improvements while preserving correctness.", "motivation": "Existing verification proofs are discarded after checking program correctness, even though they encode the complete program behavior. Harnessing these proofs for further optimization is a missed opportunity.", "method": "The authors propose 'compiling by proving,' a paradigm that transforms verification proofs into optimized execution rules. They utilize symbolic execution to construct All-Path Reachability Proofs and compile their graph structures to merge many semantic rewrites into single rules. This approach is implemented as a language-agnostic extension to the K framework.", "result": "Performance evaluations show significant improvements: consistent speedups at the opcode-level and dramatic, orders-of-magnitude gains with whole-program compilation.", "conclusion": "Compiling by proving enables the transformation of verification proofs into highly optimized execution rules, maintaining correctness and achieving substantial performance benefits across various compilation levels."}}
{"id": "2509.22614", "categories": ["cs.PL", "D.3.1; F.3.2; D.3.2; D.3.3"], "pdf": "https://arxiv.org/pdf/2509.22614", "abs": "https://arxiv.org/abs/2509.22614", "authors": ["Dmitri Volkov", "Yafei Yang", "Chung-chieh Shan"], "title": "Committing to the bit: Relational programming with semiring arrays and SAT solving", "comment": "12 pages, for associated repo see\n  https://github.com/sporkl/semiringkanren", "summary": "We propose semiringKanren, a relational programming language where each\nrelation expression denotes a semiring array. We formalize a type system that\nrestricts the arrays to finite size. We then define a semantics that is\nparameterized by the semiring that the arrays draw their elements from. We\ncompile semiringKanren types to bitstring representations. For the Boolean\nsemiring, this compilation enables us to use an SAT solver to run\nsemiringKanren programs efficiently. We compare the performance of\nsemiringKanren and faster miniKanren for solving Sudoku puzzles. Our experiment\nshows that semiringKanren can be a more efficient variant of miniKanren.", "AI": {"tldr": "semiringKanren is a new relational programming language using semiring arrays, with type-based SAT compilation for the Boolean semiring. Experiments show it's often faster than miniKanren for Sudoku, demonstrating improved efficiency for certain constraints.", "motivation": "The motivation is to enhance relational programming by introducing a language and framework capable of leveraging semiring structures, with the goal of improving efficiency, especially in constraint-solving scenarios.", "method": "The paper proposes semiringKanren, a new relational programming language. It formalizes a type system that restricts relations to finite semiring arrays and defines a semantics parameterized by the underlying semiring. The approach compiles program types to bitstring representations, allowing the use of SAT solvers in the case of the Boolean semiring.", "result": "Experiments conducted using Sudoku puzzles compare semiringKanren to miniKanren. The results show that semiringKanren can be more efficient than miniKanren in these tasks.", "conclusion": "semiringKanren offers a general and efficient approach to relational programming, especially when utilizing SAT solvers for the Boolean case. It can outperform existing miniKanren implementations in specific problem domains."}}
{"id": "2509.21427", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21427", "abs": "https://arxiv.org/abs/2509.21427", "authors": ["Ying Wang", "Wenjun Mao", "Chong Wang", "Zhenhao Zhou", "Yicheng Zhou", "Wenyun Zhao", "Yiling Lou", "Xin Peng"], "title": "Extracting Conceptual Knowledge to Locate Software Issues", "comment": null, "summary": "Issue localization, which identifies faulty code elements such as files or\nfunctions, is critical for effective bug fixing. While recent LLM-based and\nLLM-agent-based approaches improve accuracy, they struggle in large-scale\nrepositories due to concern mixing, where relevant logic is buried in large\nfunctions, and concern scattering, where related logic is dispersed across\nfiles.\n  To address these challenges, we propose RepoLens, a novel approach that\nabstracts and leverages conceptual knowledge from code repositories. RepoLens\ndecomposes fine-grained functionalities and recomposes them into high-level\nconcerns, semantically coherent clusters of functionalities that guide LLMs. It\noperates in two stages: an offline stage that extracts and enriches conceptual\nknowledge into a repository-wide knowledge base, and an online stage that\nretrieves issue-specific terms, clusters and ranks concerns by relevance, and\nintegrates them into localization workflows via minimally intrusive prompt\nenhancements. We evaluate RepoLens on SWE-Lancer-Loc, a benchmark of 216 tasks\nderived from SWE-Lancer. RepoLens consistently improves three state-of-the-art\ntools, namely AgentLess, OpenHands, and mini-SWE-agent, achieving average gains\nof over 22% in Hit@k and 46% in Recall@k for file- and function-level\nlocalization. It generalizes across models (GPT-4o, GPT-4o-mini, GPT-4.1) with\nHit@1 and Recall@10 gains up to 504% and 376%, respectively. Ablation studies\nand manual evaluation confirm the effectiveness and reliability of the\nconstructed concerns.", "AI": {"tldr": "RepoLens improves LLM-based issue localization in large code repositories by abstracting conceptual knowledge into semantically meaningful clusters, boosting localization performance by up to 504% on rigorous benchmarks and models.", "motivation": "Recent LLM-based and LLM-agent-based issue localization approaches, while accurate, face challenges in large code repositories due to concern mixing (relevant logic buried in large functions) and concern scattering (related logic spread across files). Addressing these challenges is necessary for scalable and effective bug localization.", "method": "RepoLens is proposed as a novel approach that creates and leverages a repository-wide conceptual knowledge base. RepoLens operates in two stages: (1) offline extraction and enrichment of conceptual knowledge into a knowledge base, and (2) online retrieval of issue-specific terms, clustering and ranking of concerns, and integration into localization workflows through prompt enhancements.", "result": "RepoLens was evaluated on the SWE-Lancer-Loc benchmark (216 tasks) and consistently improved the performance of three state-of-the-art localization tools. It achieved average gains of over 22% in Hit@k and 46% in Recall@k for file- and function-level localization, with maximum observed gains up to 504% (Hit@1) and 376% (Recall@10) across three different language models.", "conclusion": "RepoLens addresses the challenges of concern mixing and scattering by abstracting and organizing conceptual knowledge, significantly enhancing issue localization accuracy and reliability in large code repositories. It demonstrates both strong empirical improvements and generalizability across different models."}}
{"id": "2509.21533", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21533", "abs": "https://arxiv.org/abs/2509.21533", "authors": ["Shalini Chakraborty", "Sebastian Baltes"], "title": "Lost in Transition: The Struggle of Women Returning to Software Engineering Research after Career Breaks", "comment": "3 pages, published in the Proceedings of the 18th International\n  Conference on Cooperative and Human Aspects of Software Engineering (CHASE\n  2025)", "summary": "The IT industry provides supportive pathways such as returnship programs,\ncoding boot camps, and buddy systems for women re-entering their job after a\ncareer break. Academia, however, offers limited opportunities to motivate women\nto return. We propose a diverse multicultural research project investigating\nthe challenges faced by women with software engineering (SE) backgrounds\nre-entering academia or related research roles after a career break. Career\ndisruptions due to pregnancy, immigration status, or lack of flexible work\noptions can significantly impact women's career progress, creating barriers for\nreturning as lecturers, professors, or senior researchers. Although many\ncompanies promote gender diversity policies, such measures are less prominent\nand often under-recognized within academic institutions. Our goal is to explore\nthe specific challenges women encounter when re-entering academic roles\ncompared to industry roles; to understand the institutional perspective,\nincluding a comparative analysis of existing policies and opportunities in\ndifferent countries for women to return to the field; and finally, to provide\nrecommendations that support transparent hiring practices. The research project\nwill be carried out in multiple universities and in multiple countries to\ncapture the diverse challenges and policies that vary by location.", "AI": {"tldr": "This paper proposes a multi-country study on the challenges faced by women with software engineering backgrounds returning to academia after career breaks. It seeks to understand the existing barriers, compare returnship policies in different countries, and provide recommendations to improve hiring transparency and support for women in academic research roles.", "motivation": "The paper aims to address the lack of supportive pathways in academia for women with software engineering backgrounds who are returning after a career break, compared to more established support systems in the IT industry. The authors are motivated by observed gender inequities and the limited recognition of returnship-friendly policies in academic settings.", "method": "The proposed study involves a diverse, multicultural research project conducted across multiple universities and countries. It will investigate challenges faced by women re-entering academia or research roles after a career break, and undertake a comparative analysis of institutional policies and opportunities for women returning to research roles in different countries.", "result": "The expected outcome is a deeper understanding of the specific challenges women face re-entering academic roles relative to industry, insights into institutional perspectives, and a comparative overview of related policies. The study also aims to produce concrete recommendations for academic institutions to support more transparent and accessible hiring practices for returning women.", "conclusion": "There is a clear gap in academic support structures for women with software engineering backgrounds returning after career breaks, and this research will offer valuable recommendations and evidence to inform policy improvements and transparent practices."}}
{"id": "2509.21816", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21816", "abs": "https://arxiv.org/abs/2509.21816", "authors": ["Yuhang Xie", "Jian Mu", "Xiaojun Ma", "Chaoyun Zhang", "Lu Wang", "Mengyu Zhou", "Mugeng Liu", "Si Qin", "Qingwei Lin", "Saravan Rajmohan", "Shi Han", "Dongmei Zhang"], "title": "No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials", "comment": null, "summary": "Excel is one of the most widely used productivity tools across domains,\noffering rich functionality but also overwhelming users with its complexity.\nThis creates a persistent demand for tutorials to support effective usage.\nHowever, existing tutorials are manually authored by experts, require frequent\nupdates after each software release, and incur substantial labor costs. Prior\nwork has not achieved fully automated tutorial generation, since existing\nmethods still depend on handcrafted operation sequences or example materials.\nIn this paper, we present the first framework for automatically generating\nExcel tutorials directly from natural language task descriptions. Our framework\nfirst instantiates the task. Then a central component of this framework,\nExecution Agent, plans and executes the solution in Excel, and collects the\nintermediate artifacts required for tutorial construction. These artifacts are\nthen transformed into both structured Excel documents and video demonstrations.\nTo build a comprehensive tutorial corpus, we collected 1,559 task descriptions\nfrom real-world scenarios. In addition, we designed a systematic evaluation\nframework that integrates assessments from both large language models (LLMs)\nand human reviewers. Experimental results show that our framework improves task\nexecution success rates by 8.5% over state-of-the-art baselines. Moreover, the\ngenerated tutorials demonstrate superior readability and instructional\neffectiveness, often approaching or surpassing expert-authored materials.\nImportantly, the automated pipeline eliminates manual labor and reduces time\ncosts to 1/20 of expert authoring, making scalable and high-quality tutorial\ngeneration practical for the first time.", "AI": {"tldr": "This paper presents the first fully automated system for generating Excel tutorials from user-described tasks. It uses an agent to execute and document tasks, producing readable documents and videos. The system improves success rates and quality over previous methods, drastically lowers time and labor costs, and is validated on over 1,500 real-world scenarios.", "motivation": "Excel is extremely versatile but also complex, leading to a strong demand for tutorials. Existing tutorials require manual authoring, frequent updates, and significant labor, while previous automated methods still rely on handcrafted materials.", "method": "The paper introduces an automated framework that generates Excel tutorials from natural language task descriptions. It uses an Execution Agent that translates tasks into actions within Excel, gathers artifacts, and outputs both structured documents and video demonstrations. Evaluation involves both LLMs and human reviewers.", "result": "The framework generated tutorials for 1,559 real-world tasks, improving task execution success rates by 8.5% compared to baselines. The tutorials were found to have better readability and instructional quality, matching or exceeding expert-authored content. The automated process dramatically reduces labor and time costs (20x faster than manual authoring).", "conclusion": "The proposed framework achieves fully automated, scalable, and high-quality tutorial generation for Excel, outperforming prior approaches and making wide-scale tutorial creation practical."}}
{"id": "2509.21881", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21881", "abs": "https://arxiv.org/abs/2509.21881", "authors": ["Chaman Wijesiriwardana", "Prasad Wimalaratne"], "title": "Software Engineering Data Analytics: A Framework Based on a Multi-Layered Abstraction Mechanism", "comment": null, "summary": "This paper presents a concept of a domain-specific framework for software\nanalytics by enabling querying, modeling, and integration of heterogeneous\nsoftware repositories. The framework adheres to a multi-layered abstraction\nmechanism that consists of domain-specific operators. We showcased the\npotential of this approach by employing a case study.", "AI": {"tldr": "This paper proposes a domain-specific, multi-layered framework for integrating and analyzing heterogeneous software repositories, demonstrated via a case study, to improve software analytics.", "motivation": "There is a need to efficiently analyze and integrate data from heterogeneous software repositories for improved software analytics.", "method": "The authors propose a domain-specific framework with multi-layered abstraction and domain-specific operators for querying, modeling, and integrating software repositories. The effectiveness is demonstrated via a case study.", "result": "The proposed framework enables advanced querying, modeling, and integration of various software repositories, validated through a case study.", "conclusion": "A domain-specific multi-layered framework can facilitate better software analytics by enabling effective integration and analysis of heterogeneous repositories."}}
{"id": "2509.21891", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21891", "abs": "https://arxiv.org/abs/2509.21891", "authors": ["Yangtian Zi", "Zixuan Wu", "Aleksander Boruch-Gruszecki", "Jonathan Bell", "Arjun Guha"], "title": "AgentPack: A Dataset of Code Changes, Co-Authored by Agents and Humans", "comment": null, "summary": "Fine-tuning large language models for code editing has typically relied on\nmining commits and pull requests. The working hypothesis has been that commit\nmessages describe human intent in natural language, and patches to code\ndescribe the changes that implement that intent. However, much of the\npreviously collected data is noisy: commit messages are terse, human-written\ncommits commingle several unrelated edits, and many commits come from simple,\nrule-based bots.\n  The recent adoption of software engineering agents changes this landscape.\nCode changes co-authored by humans and agents tend to be more narrowly scoped\nand focused on clearer goals. Their commit messages, generated by LLMs,\narticulate intent and rationale in much greater detail. Moreover, when these\nchanges land in public repositories, they are implicitly filtered by humans:\nmaintainers discard low-quality commits to their projects.\n  We present AgentPack, a corpus of 1.3M code edits co-authored by Claude Code,\nOpenAI Codex, and Cursor Agent across public GitHub projects up to mid-August\n2025. We describe the identification and curation pipeline, quantify adoption\ntrends of these agents, and analyze the structural properties of the edits.\nFinally, we show that models fine-tuned on AgentPack can outperform models\ntrained on prior human-only commit corpora, highlighting the potential of using\npublic data from software engineering agents to train future code-editing\nmodels.", "AI": {"tldr": "The paper presents AgentPack, a new dataset of code edits co-authored by human and AI agents. This more focused, high-quality data leads to better-performing code-editing models compared to those trained on older human-only commit datasets.", "motivation": "Traditional methods for fine-tuning language models for code editing use data from commits and pull requests, but these are often noisy and unfocused, leading to suboptimal training datasets for code-editing models.", "method": "The authors introduce AgentPack, a large curated corpus of 1.3 million code edits co-authored by both humans and AI agents (such as Claude Code, OpenAI Codex, and Cursor Agent) from public GitHub repositories. They describe their process for identifying and curating this data, analyze adoption trends of such agents, and structurally evaluate the properties of the edits.", "result": "Models fine-tuned using the AgentPack dataset outperform those trained on traditional, human-only commit datasets.", "conclusion": "Using code changes co-authored by AI agents and humans, as well as LLM-generated commit messages, produces a higher-quality training resource for code-editing language models than traditional sources. This demonstrates the value of leveraging software engineering agents for future advancements in code-editing model training."}}
{"id": "2509.21945", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21945", "abs": "https://arxiv.org/abs/2509.21945", "authors": ["Pengzhou Chen", "Hongyuan Liang", "Tao Chen"], "title": "Unveiling Many Faces of Surrogate Models for Configuration Tuning: A Fitness Landscape Analysis Perspective", "comment": "This paper is under review", "summary": "To efficiently tune configuration for better system performance (e.g.,\nlatency), many tuners have leveraged a surrogate model to expedite the process\ninstead of solely relying on the profoundly expensive system measurement. As\nsuch, it is naturally believed that we need more accurate models. However, the\nfact of accuracy can lie-a somewhat surprising finding from prior work-has left\nus many unanswered questions regarding what role the surrogate model plays in\nconfiguration tuning. This paper provides the very first systematic exploration\nand discussion, together with a resolution proposal, to disclose the many faces\nof surrogate models for configuration tuning, through the novel perspective of\nfitness landscape analysis. We present a theory as an alternative to accuracy\nfor assessing the model usefulness in tuning, based on which we conduct an\nextensive empirical study involving up to 27,000 cases. Drawing on the above,\nwe propose Model4Tune, an automated predictive tool that estimates which\nmodel-tuner pairs are the best for an unforeseen system without expensive tuner\nprofiling. Our results suggest that Moldel4Tune, as one of the first of its\nkind, performs significantly better than random guessing in 79%-82% of the\ncases. Our results not only shed light on the possible future research\ndirections but also offer a practical resolution that can assist practitioners\nin evaluating the most useful model for configuration tuning.", "AI": {"tldr": "Surrogate model accuracy isn\u2019t enough for effective system performance tuning. This paper uses fitness landscape analysis to evaluate model usefulness, conducts a large-scale empirical study, and introduces Model4Tune\u2014a tool that reliably helps choose the best models for configuration tuning without expensive profiling.", "motivation": "Traditional system configuration tuning is slow and expensive due to reliance on direct system measurements. While surrogate models are used to accelerate tuning, prior research reveals that just improving surrogate model accuracy does not necessarily yield better performance, raising questions about their actual role in the process.", "method": "The paper introduces a systematic study using fitness landscape analysis to evaluate surrogate models for configuration tuning. It develops and tests a theoretical framework that assesses model usefulness beyond accuracy. An extensive empirical study (27,000 cases) is conducted to validate the approach. The paper also proposes Model4Tune, an automated predictive tool to estimate the suitability of model-tuner pairs for new systems without expensive profiling.", "result": "The empirical study shows that using Model4Tune allows practitioners to select effective model-tuner pairs significantly better than random guessing in 79%-82% of testing cases. The tool provides a practical method for evaluating and choosing surrogate models for configuration tuning.", "conclusion": "Accuracy is not the only, or best, measure of a surrogate model\u2019s usefulness for system configuration tuning. Fitness landscape analysis offers a better perspective, allowing practitioners to more effectively select models without costly profiling. The work opens up new research and practical avenues for surrogate-based performance tuning."}}
{"id": "2509.22097", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22097", "abs": "https://arxiv.org/abs/2509.22097", "authors": ["Junkai Chen", "Huihui Huang", "Yunbo Lyu", "Junwen An", "Jieke Shi", "Chengran Yang", "Ting Zhang", "Haoye Tian", "Yikun Li", "Zhenhao Li", "Xin Zhou", "Xing Hu", "David Lo"], "title": "SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios", "comment": null, "summary": "Large language model (LLM) powered code agents are rapidly transforming\nsoftware engineering by automating tasks such as testing, debugging, and\nrepairing, yet the security risks of their generated code have become a\ncritical concern. Existing benchmarks have offered valuable insights but remain\ninsufficient: they often overlook the genuine context in which vulnerabilities\nwere introduced or adopt narrow evaluation protocols that fail to capture\neither functional correctness or newly introduced vulnerabilities. We therefore\nintroduce SecureAgentBench, a benchmark of 105 coding tasks designed to\nrigorously evaluate code agents' capabilities in secure code generation. Each\ntask includes (i) realistic task settings that require multi-file edits in\nlarge repositories, (ii) aligned contexts based on real-world open-source\nvulnerabilities with precisely identified introduction points, and (iii)\ncomprehensive evaluation that combines functionality testing, vulnerability\nchecking through proof-of-concept exploits, and detection of newly introduced\nvulnerabilities using static analysis. We evaluate three representative agents\n(SWE-agent, OpenHands, and Aider) with three state-of-the-art LLMs (Claude 3.7\nSonnet, GPT-4.1, and DeepSeek-V3.1). Results show that (i) current agents\nstruggle to produce secure code, as even the best-performing one, SWE-agent\nsupported by DeepSeek-V3.1, achieves merely 15.2% correct-and-secure solutions,\n(ii) some agents produce functionally correct code but still introduce\nvulnerabilities, including new ones not previously recorded, and (iii) adding\nexplicit security instructions for agents does not significantly improve secure\ncoding, underscoring the need for further research. These findings establish\nSecureAgentBench as a rigorous benchmark for secure code generation and a step\ntoward more reliable software development with LLMs.", "AI": {"tldr": "SecureAgentBench reveals significant security shortcomings in LLM-powered code agents, with most failing to reliably generate secure code despite being functionally correct. This new benchmark highlights the pressing need for research into safe automation in software development.", "motivation": "Automated code generation by LLM-powered agents is transforming software engineering, but there are rising concerns over the security vulnerabilities these agents may introduce. Existing benchmarks are inadequate because they lack real-world context and comprehensive evaluation of security and functionality.", "method": "The authors introduce SecureAgentBench, a new benchmark of 105 coding tasks that replicate real software engineering scenarios, leveraging realistic multi-file tasks and actual vulnerability contexts. The evaluation involves functional testing, proof-of-concept exploits, and static analysis for new vulnerabilities. They assess multiple agents and LLMs.", "result": "Current code agents perform poorly in secure code generation, with the best combination achieving only 15.2% secure solutions. Agents often produce functionally correct code that is still insecure, sometimes introducing entirely new vulnerabilities. Adding explicit security instructions shows limited improvement.", "conclusion": "SecureAgentBench offers a rigorous way to evaluate LLM-powered code agents' ability to generate secure code. The disappointing results underscore the urgent need for more research and development of secure, reliable code agents. Explicit security prompts alone do not suffice to produce safe code."}}
{"id": "2509.22114", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22114", "abs": "https://arxiv.org/abs/2509.22114", "authors": ["Hanzhuo Tan", "Weihao Li", "Xiaolong Tian", "Siyi Wang", "Jiaming Liu", "Jing Li", "Yuqun Zhang"], "title": "SK2Decompile: LLM-based Two-Phase Binary Decompilation from Skeleton to Skin", "comment": null, "summary": "Large Language Models (LLMs) have emerged as a promising approach for binary\ndecompilation. However, the existing LLM-based decompilers still are somewhat\nlimited in effectively presenting a program's source-level structure with its\noriginal identifiers. To mitigate this, we introduce SK2Decompile, a novel\ntwo-phase approach to decompile from the skeleton (semantic structure) to the\nskin (identifier) of programs. Specifically, we first apply a Structure\nRecovery model to translate a program's binary code to an Intermediate\nRepresentation (IR) as deriving the program's \"skeleton\", i.e., preserving\ncontrol flow and data structures while obfuscating all identifiers with generic\nplaceholders. We also apply reinforcement learning to reward the model for\nproducing program structures that adhere to the syntactic and semantic rules\nexpected by compilers. Second, we apply an Identifier Naming model to produce\nmeaningful identifiers which reflect actual program semantics as deriving the\nprogram's \"skin\". We train the Identifier Naming model with a separate\nreinforcement learning objective that rewards the semantic similarity between\nits predictions and the reference code. Such a two-phase decompilation process\nfacilitates advancing the correctness and readability of decompilation\nindependently. Our evaluations indicate that SK2Decompile, significantly\noutperforms the SOTA baselines, achieving 21.6% average re-executability rate\ngain over GPT-5-mini on the HumanEval dataset and 29.4% average R2I improvement\nover Idioms on the GitHub2025 benchmark.", "AI": {"tldr": "SK2Decompile introduces a two-stage system that first recovers program structure and then assigns meaningful variable names, outperforming previous methods in accuracy and readability on key benchmarks.", "motivation": "Current LLM-based binary decompilers struggle to recover both the source-level structure and original identifiers of programs, resulting in output that is hard to understand and less usable.", "method": "SK2Decompile introduces a two-phase decompilation process. First, a Structure Recovery model translates binary code into an Intermediate Representation (IR) that maintains the program's control flow and data structures but uses generic placeholders for identifiers. Reinforcement learning rewards the model for generating syntactically and semantically valid structures. Second, an Identifier Naming model assigns meaningful names to the generic identifiers, again trained with reinforcement learning to maximize semantic similarity with reference code.", "result": "SK2Decompile significantly surpasses state-of-the-art baselines, with a 21.6% higher average re-executability rate than GPT-5-mini on the HumanEval dataset and a 29.4% improvement in R2I over Idioms on the GitHub2025 benchmark.", "conclusion": "The novel two-phase approach improves both the correctness and readability of binary decompilation, offering superior performance compared to existing models on multiple benchmarks."}}
{"id": "2509.22170", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22170", "abs": "https://arxiv.org/abs/2509.22170", "authors": ["Chengjia Wang", "Lanling Tang", "Ming Yuan", "Jiongchi Yu", "Xiaofei Xie", "Jiajun Bu"], "title": "Leveraging LLM Agents for Automated Video Game Testing", "comment": "17 pages", "summary": "Testing MMORPGs (Massively Multiplayer Online Role-Playing Games) is a\ncritical yet labor-intensive task in game development due to their complexity\nand frequent updating nature. Traditional automated game testing approaches\nstruggle to achieve high state coverage and efficiency in these rich,\nopen-ended environments, while existing LLM-based game-playing approaches are\nlimited to shallow reasoning ability in understanding complex game state-action\nspaces and long-complex tasks. To address these challenges, we propose TITAN,\nan effective LLM-driven agent framework for intelligent MMORPG testing. TITAN\nincorporates four key components to: (1) perceive and abstract high-dimensional\ngame states, (2) proactively optimize and prioritize available actions, (3)\nenable long-horizon reasoning with action trace memory and reflective\nself-correction, and (4) employ LLM-based oracles to detect potential\nfunctional and logic bugs with diagnostic reports.\n  We implement the prototype of TITAN and evaluate it on two large-scale\ncommercial MMORPGs spanning both PC and mobile platforms. In our experiments,\nTITAN achieves significantly higher task completion rates (95%) and bug\ndetection performance compared to existing automated game testing approaches.\nAn ablation study further demonstrates that each core component of TITAN\ncontributes substantially to its overall performance. Notably, TITAN detects\nfour previously unknown bugs that prior testing approaches fail to identify. We\nprovide an in-depth discussion of these results, which offer guidance for new\navenues of advancing intelligent, general-purpose testing systems. Moreover,\nTITAN has been deployed in eight real-world game QA pipelines, underscoring its\npractical impact as an LLM-driven game testing framework.", "AI": {"tldr": "TITAN is a new LLM-driven game testing framework for MMORPGs that outperforms existing methods in task completion and bug detection, and is already being used in industry QA pipelines.", "motivation": "Testing MMORPGs is challenging due to their complex, frequently updated nature, and the limitations of traditional automated and LLM-based game testing approaches. There is a need for more effective techniques that offer deep reasoning, high state coverage, and efficient bug detection.", "method": "The paper proposes TITAN, an LLM-driven agent framework for intelligent MMORPG testing. TITAN includes four components: (1) perception and abstraction of high-dimensional game states, (2) action prioritization, (3) long-horizon reasoning with action trace memory and self-correction, and (4) LLM-based oracles for bug detection and diagnostics. TITAN is implemented and tested on commercial MMORPGs across PC and mobile.", "result": "TITAN achieves higher task completion rates (95%), superior bug detection, and finds bugs missed by previous methods. Ablation studies show each component's importance. TITAN has also been deployed in real-world QA pipelines, highlighting its effectiveness.", "conclusion": "TITAN advances MMORPG testing with intelligent, LLM-supported techniques, improving efficiency and depth over prior approaches, and has practical industry impact."}}
{"id": "2509.22202", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22202", "abs": "https://arxiv.org/abs/2509.22202", "authors": ["Lukas Twist", "Jie M. Zhang", "Mark Harman", "Helen Yannakoudakis"], "title": "Library Hallucinations in LLMs: Risk Analysis Grounded in Developer Queries", "comment": "23 pages, 5 tables", "summary": "Large language models (LLMs) are increasingly used to generate code, yet they\ncontinue to hallucinate, often inventing non-existent libraries. Such library\nhallucinations are not just benign errors: they can mislead developers, break\nbuilds, and expose systems to supply chain threats such as slopsquatting.\nDespite increasing awareness of these risks, little is known about how\nreal-world prompt variations affect hallucination rates. Therefore, we present\nthe first systematic study of how user-level prompt variations impact library\nhallucinations in LLM-generated code. We evaluate six diverse LLMs across two\nhallucination types: library name hallucinations (invalid imports) and library\nmember hallucinations (invalid calls from valid libraries). We investigate how\nrealistic user language extracted from developer forums and how user errors of\nvarying degrees (one- or multi-character misspellings and completely fake\nnames/members) affect LLM hallucination rates. Our findings reveal systemic\nvulnerabilities: one-character misspellings in library names trigger\nhallucinations in up to 26% of tasks, fake library names are accepted in up to\n99% of tasks, and time-related prompts lead to hallucinations in up to 84% of\ntasks. Prompt engineering shows promise for mitigating hallucinations, but\nremains inconsistent and LLM-dependent. Our results underscore the fragility of\nLLMs to natural prompt variation and highlight the urgent need for safeguards\nagainst library-related hallucinations and their potential exploitation.", "AI": {"tldr": "This paper systematically shows that LLMs generating code often hallucinate fake libraries and members, especially when faced with natural user prompt errors. These hallucinations are a major risk and demand urgent safeguards.", "motivation": "LLMs increasingly generate code but often hallucinate, particularly by inventing non-existent libraries. These errors can mislead developers, break builds, and introduce supply chain threats. Despite awareness of these risks, the impact of real-world prompt variations on hallucination rates is largely unknown.", "method": "The authors systematically study how user-level prompt variations affect library hallucinations in code produced by six diverse LLMs. They analyze two types of hallucinations: invalid imports (library name hallucinations) and invalid calls from valid libraries (library member hallucinations). They use realistic prompt variations extracted from developer forums, including user errors like misspellings and fake names, to measure the effects on LLM hallucination rates.", "result": "The study finds systemic vulnerabilities in LLMs: one-character misspellings cause hallucinations in up to 26% of tasks, fake library names are accepted in up to 99% of tasks, and time-related prompts induce hallucinations in up to 84% of cases. While prompt engineering can reduce hallucinations, its effectiveness is inconsistent and depends on the specific LLM.", "conclusion": "LLMs are fragile in the face of natural prompt variations, frequently hallucinating libraries and members with significant risks for developers and systems. The work stresses the need for better safeguards against such hallucinations due to their potential for exploitation."}}
{"id": "2509.22320", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22320", "abs": "https://arxiv.org/abs/2509.22320", "authors": ["Vincenzo De Martino", "Mohammad Amin Zadenoori", "Xavier Franch", "Alessio Ferrari"], "title": "Green Prompt Engineering: Investigating the Energy Impact of Prompt Design in Software Engineering", "comment": null, "summary": "Language Models are increasingly applied in software engineering, yet their\ninference raises growing environmental concerns. Prior work has examined\nhardware choices and prompt length, but little attention has been paid to\nlinguistic complexity as a sustainability factor. This paper introduces Green\nPrompt Engineering, framing linguistic complexity as a design dimension that\ncan influence energy consumption and performance. We conduct an empirical study\non requirement classification using open-source Small Language Models, varying\nthe readability of prompts. Our results reveal that readability affects\nenvironmental sustainability and performance, exposing trade-offs between them.\nFor practitioners, simpler prompts can reduce energy costs without a\nsignificant F1-score loss; for researchers, it opens a path toward guidelines\nand studies on sustainable prompt design within the Green AI agenda.", "AI": {"tldr": "Simpler, more readable prompts for language models can save energy and maintain good performance, making prompt design an important factor in sustainable AI practices.", "motivation": "The motivation is to explore sustainability aspects in using language models for software engineering, specifically focusing on the understudied factor of linguistic complexity in prompts, beyond typical concerns like hardware or prompt length.", "method": "The authors conducted an empirical study using open-source Small Language Models, where they varied the readability (linguistic complexity) of prompts for a requirement classification task, and measured both energy consumption and performance (F1-score).", "result": "The study found that the readability of prompts significantly affects both the environmental sustainability (energy usage) and the performance (F1-score) of language models. Simple prompts reduce energy use without significantly decreasing F1-score, but there are trade-offs.", "conclusion": "The paper concludes that linguistic complexity of prompts is a key design variable affecting language model sustainability and performance. Simple, readable prompts can be both energy-efficient and effective, encouraging the development of guidelines for sustainable prompt design in Green AI."}}
{"id": "2509.22337", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22337", "abs": "https://arxiv.org/abs/2509.22337", "authors": ["Haoyu Feng", "Xin Zhang"], "title": "GPU-Accelerated Loopy Belief Propagation for Program Analysis", "comment": null, "summary": "Loopy Belief Propagation (LBP) is a widely used approximate inference\nalgorithm in probabilistic graphical models, with applications in computer\nvision, error correction codes, protein folding, program analysis, etc.\nHowever, LBP faces significant computational challenges when applied to\nlarge-scale program analysis. While GPU (Graphics Processing Unit) parallel\ncomputing provides a promising solution, existing approaches lack support for\nflexible update strategies and have yet to integrate logical constraints with\nGPU acceleration, leading to suboptimal practical performance.\n  This paper presents a GPU-accelerated LBP algorithm for program analysis. To\nsupport the diverse update strategies required by users, we propose a unified\nrepresentation for specifying arbitrary user-defined update strategies, along\nwith a dependency analysis algorithm. Furthermore, building on previous work\nthat leverages the local structure of Horn clauses to simplify message passing,\nwe group messages to minimize warp divergence and better utilize GPU resources.\nExperimental results on datarace analysis over eight real-world Java programs\nshow that our approach achieves an average speedup of $2.14\\times$ over the\nstate-of-the-art sequential approach and $5.56\\times$ over the state-of-the-art\nGPU-based approach, while maintaining high accuracy.", "AI": {"tldr": "This paper introduces a highly efficient, flexible GPU-accelerated LBP algorithm for program analysis, achieving major speedups and maintaining accuracy compared to current leading methods.", "motivation": "Loopy Belief Propagation (LBP) is widely used in probabilistic graphical models for various applications, including program analysis. However, it's computationally intensive for large-scale tasks, and existing GPU-based methods do not effectively support flexible update strategies or integrate logical constraints, resulting in poor performance.", "method": "The paper proposes a GPU-accelerated LBP algorithm specifically designed for program analysis. It introduces a unified representation for user-defined update strategies, a dependency analysis algorithm, and an optimized message grouping method that minimizes GPU warp divergence while leveraging the structure of Horn clauses.", "result": "The experimental results, focusing on datarace analysis in eight real-world Java programs, demonstrate significant performance improvements: an average speedup of 2.14x over state-of-the-art sequential methods and 5.56x over existing GPU-based methods, with high accuracy maintained.", "conclusion": "The proposed GPU-accelerated LBP algorithm substantially improves computational efficiency for program analysis tasks while supporting flexible update strategies and logical constraint integration, outperforming both sequential and previous GPU-based methods."}}
{"id": "2509.22379", "categories": ["cs.SE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.22379", "abs": "https://arxiv.org/abs/2509.22379", "authors": ["Stefano Carlo Lambertenghi", "Mirena Flores Valdez", "Andrea Stocco"], "title": "A Multi-Modality Evaluation of the Reality Gap in Autonomous Driving Systems", "comment": "In proceedings of the 40th IEEE/ACM International Conference on\n  Automated Software Engineering (ASE '25)", "summary": "Simulation-based testing is a cornerstone of Autonomous Driving System (ADS)\ndevelopment, offering safe and scalable evaluation across diverse driving\nscenarios. However, discrepancies between simulated and real-world behavior,\nknown as the reality gap, challenge the transferability of test results to\ndeployed systems. In this paper, we present a comprehensive empirical study\ncomparing four representative testing modalities: Software-in-the-Loop (SiL),\nVehicle-in-the-Loop (ViL), Mixed-Reality (MR), and full real-world testing.\nUsing a small-scale physical vehicle equipped with real sensors (camera and\nLiDAR) and its digital twin, we implement each setup and evaluate two ADS\narchitectures (modular and end-to-end) across diverse indoor driving scenarios\ninvolving real obstacles, road topologies, and indoor environments. We\nsystematically assess the impact of each testing modality along three\ndimensions of the reality gap: actuation, perception, and behavioral fidelity.\nOur results show that while SiL and ViL setups simplify critical aspects of\nreal-world dynamics and sensing, MR testing improves perceptual realism without\ncompromising safety or control. Importantly, we identify the conditions under\nwhich failures do not transfer across testing modalities and isolate the\nunderlying dimensions of the gap responsible for these discrepancies. Our\nfindings offer actionable insights into the respective strengths and\nlimitations of each modality and outline a path toward more robust and\ntransferable validation of autonomous driving systems.", "AI": {"tldr": "Simulation-based testing for autonomous vehicles is challenged by the 'reality gap.' This study empirically compares SiL, ViL, MR, and real-world testing, finding MR best balances realism and safety. The analysis clarifies each approach's limitations and guides future robust testing of autonomous driving systems.", "motivation": "Simulation-based testing is essential for the development of autonomous driving systems due to its safety and scalability. However, there is a significant challenge called the 'reality gap'\u2014the discrepancies between simulation and real-world performance\u2014which hinders the transferability of test results to real deployments.", "method": "The authors conducted an empirical study comparing four testing modalities: Software-in-the-Loop (SiL), Vehicle-in-the-Loop (ViL), Mixed-Reality (MR), and full real-world testing. They used a small-scale physical vehicle equipped with real sensors and its digital twin. Two ADS architectures (modular and end-to-end) were evaluated across diverse indoor driving scenarios. They systematically assessed the impact of each modality on actuation, perception, and behavioral fidelity\u2014the three dimensions of the reality gap.", "result": "The results reveal that SiL and ViL oversimplify crucial real-world dynamics and sensing. Mixed-Reality testing enhances perceptual realism while maintaining safety and control. The authors pinpoint specific conditions where failures do not translate between modalities and link these inconsistencies to certain dimensions of the reality gap.", "conclusion": "Each testing modality has clear strengths and limitations: SiL and ViL are less representative of real-world challenges, while MR bridges the gap between simulation and reality effectively. The study provides practical insights for choosing appropriate testing approaches, and maps a pathway toward robust and transferable validation of autonomous driving systems."}}
{"id": "2509.22420", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.22420", "abs": "https://arxiv.org/abs/2509.22420", "authors": ["Ziyi Zhang", "Devjeet Roy", "Venera Arnaoudova"], "title": "Context-Specific Instruction: A Longitudinal Study on Debugging Skill Acquisition and Retention for Novice Programmers", "comment": "31 pages (25 pages for the paper, rest pages are references and\n  appendix). 4 tables, 7 figures", "summary": "Bug localization is a critical skill, yet novices often lack systematic\napproaches. Prior work tested abstract guidelines and general concrete steps;\nthe impact of context-specific instruction is unclear. We ran an eight-week\nlongitudinal study with four conditions: no instruction (G1), abstract\nguidelines (G2), concrete steps (G3), and our context-specific instruction that\npairs concrete bug-localization steps with problem-specific details (G4).\nForty-four undergraduates participated; 41 completed all five sessions (S1-S5).\nEach session included 2-3 debugging tasks to identify the minimal code element\ncontaining a seeded logical fault. We measured correctness (binary), time to\ncompletion, self-perceived scores (stress, difficulty, satisfaction, and\nstrategy adherence). G4 achieved higher correctness and shorter time to\ncompletion: it reached 80% correctness after one session (vs. 20-44% for other\ngroups) and maintained 80% after three weeks, outperforming all groups (p <\n0.05); its time to completion stabilized at 13-15 minutes in S1, whereas other\ngroups took 2-3 sessions to stabilize at 22-27 minutes. Qualitative responses\nshowed lower stress and higher satisfaction in G4, with participants\ninternalizing strategies via contextual examples. We conclude that\ncontext-specific instruction yields faster skill acquisition and stronger\nretention than abstract guidelines or context-agnostic steps. Even 1-2 sessions\nproduced significant gains, while extended practice optimized and stabilized\nperformance. Integrating contextual examples with abstract principles may\nbridge theory-practice gaps in bug-localization education and provide a more\nequitable path for novices.", "AI": {"tldr": "Context-specific bug localization instruction significantly outperforms abstract/generic methods for novices, leading to higher accuracy, faster completion, less stress, and better strategy retention. Even 1-2 sessions have strong effects; integrating examples with principles bridges theory and practice.", "motivation": "Novices struggle with bug localization due to lack of systematic approaches. Existing instructional methods involve abstract guidelines or generic steps, but it is unclear if context-specific instruction provides additional benefits.", "method": "An eight-week longitudinal study with four groups: no instruction (G1), abstract guidelines (G2), concrete steps (G3), and context-specific instruction (G4). 44 undergraduates participated; 41 completed all five sessions. Each session had 2-3 debugging tasks. Measures included correctness, time to completion, and self-reported stress, difficulty, satisfaction, and strategy adherence.", "result": "Participants who received context-specific instruction (G4) achieved 80% correctness after one session (versus 20-44% for other groups) and maintained this over three weeks. Their completion time stabilized quicker at 13-15 minutes, compared to 22-27 minutes for others. G4 participants also reported lower stress, higher satisfaction, and better strategy internalization.", "conclusion": "Context-specific instruction yields faster skill acquisition and stronger retention in bug localization than abstract or context-agnostic approaches. Even brief exposure brings significant gains, and practice stabilizes performance. Combining contextual examples and abstract principles may better equip novices and make education more equitable."}}
{"id": "2509.22431", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22431", "abs": "https://arxiv.org/abs/2509.22431", "authors": ["Zhengyu Chen", "Zhaoyi Meng", "Wenxiang Zhao", "Wansen Wang", "Haoyang Zhao", "Jiahao Zhan", "Jie Cui", "Hong Zhong"], "title": "TreeMind: Automatically Reproducing Android Bug Reports via LLM-empowered Monte Carlo Tree Search", "comment": null, "summary": "Automatically reproducing Android app crashes from textual bug reports is\nchallenging, particularly when the reports are incomplete and the modern UI\nexhibits high combinatorial complexity. Existing approaches based on\nreinforcement learning or large language models (LLMs) exhibit limitations in\nsuch scenarios. They struggle to infer unobserved steps and reconstruct the\nunderlying user action sequences to navigate the vast UI interaction space,\nprimarily due to limited goal-directed reasoning and planning. We present\nTreeMind, a novel technique that integrates LLMs with a customized Monte Carlo\nTree Search (MCTS) algorithm to achieve strategic UI exploration in bug\nreproduction. To the best of our knowledge, this is the first work to combine\nexternal decision-making with LLM semantic reasoning for reliable bug\nreproduction. We formulate the reproduction task as a target-driven search\nproblem, leveraging MCTS as the core planning mechanism to iteratively refine\naction sequences. To enhance MCTS with semantic reasoning, we introduce two\nLLM-guided agents with distinct roles: Expander generates top-k promising\nactions based on the current UI state and exploration history, while Simulator\nestimates the likelihood that each action leads toward successful reproduction.\nBy incorporating multi-modal UI inputs and advanced prompting techniques,\nTreeMind conducts feedback-aware navigation that identifies missing but\nessential user actions and incrementally reconstructs the reproduction paths.\nWe evaluate TreeMind on a dataset of 93 real-world Android bug reports from\nthree widely-used benchmarks. Experimental results show that it significantly\noutperforms four state-of-the-art baselines in reproduction success rate. A\nreal-world case study indicates that integrating LLM reasoning with MCTS-based\nplanning is a compelling direction for automated bug reproduction.", "AI": {"tldr": "TreeMind is a new technique that combines LLMs and Monte Carlo Tree Search for reproducing Android app crashes from incomplete bug reports, significantly outperforming existing methods in both controlled experiments and a real-world case study.", "motivation": "Automatically reproducing Android app crashes from textual bug reports is difficult due to incomplete reports and complex modern user interfaces. Existing methods, such as reinforcement learning and LLMs, struggle to reconstruct the necessary user actions for bug reproduction due to limited reasoning and planning abilities.", "method": "TreeMind combines Large Language Models (LLMs) with a custom Monte Carlo Tree Search (MCTS) algorithm. It treats bug reproduction as a target-driven search. Two LLM-guided agents are created: Expander (to generate promising next actions) and Simulator (to estimate action success probability). TreeMind uses multi-modal UI inputs and feedback-aware navigation for strategic UI exploration.", "result": "TreeMind was evaluated on 93 real-world Android bug reports, outperforming four state-of-the-art baseline techniques in terms of reproduction success rate. The method was further validated with a real-world case study highlighting its effectiveness.", "conclusion": "Integrating LLMs for semantic reasoning with MCTS-based external decision-making enables more effective and strategic UI exploration for automated Android bug reproduction, especially where reports are incomplete and the interaction space is complex."}}
{"id": "2509.22530", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22530", "abs": "https://arxiv.org/abs/2509.22530", "authors": ["Baijun Cheng", "Kailong Wang", "Ling Shi", "Haoyu Wang", "Peng Di", "Yao Guo", "Ding Li", "Xiangqun Chen"], "title": "Boosting Pointer Analysis With Large Language Model-Enhanced Allocation Function Detection", "comment": null, "summary": "Pointer analysis is foundational for many static analysis tasks, yet its\neffectiveness is often hindered by imprecise modeling of heap allocations,\nparticularly in C/C++ programs where user-defined allocation functions (AFs)\nare pervasive. Existing approaches largely overlook these custom allocators,\nleading to coarse aliasing and reduced analysis precision. In this paper, we\npresent AFD, a novel technique that enhances pointer analysis by automatically\nidentifying and modeling custom allocation functions. AFD employs a hybrid\napproach: it uses value-flow analysis to detect straightforward wrappers and\nleverages Large Language Models (LLMs) to reason about more complex allocation\npatterns with side effects. This targeted enhancement enables precise modeling\nof heap objects at each call site, achieving context-sensitivity-like benefits\nwithout the associated overhead. We evaluate AFD on 15 real-world C projects,\nidentifying over 600 custom AFs. Integrating AFD into a baseline pointer\nanalysis yields a 26x increase in modeled heap objects and a 39% reduction in\nalias set sizes, with only 1.4x runtime overhead. Furthermore, our enhanced\nanalysis improves indirect call resolution and uncovers 17 previously\nundetected memory bugs. These results demonstrate that precise modeling of\ncustom allocation functions offers a scalable and practical path to improving\npointer analysis in large software systems.", "AI": {"tldr": "AFD automatically identifies and models custom memory allocators in C/C++ code using analysis and LLMs, leading to much more precise and scalable pointer analysis and finding bugs missed by prior approaches, with acceptable overhead.", "motivation": "Many static analysis tasks rely on pointer analysis, but its effectiveness is limited in C/C++ programs due to imprecise modeling of heap allocations from user-defined allocation functions, which are common and often ignored by previous tools.", "method": "The authors introduce AFD, a hybrid technique combining value-flow analysis for simple cases and Large Language Models (LLMs) for more complex patterns to automatically identify and accurately model custom allocation functions.", "result": "AFD was evaluated on 15 real-world C projects, successfully identifying over 600 custom allocation functions. Integrating AFD led to a 26x increase in modeled heap objects, a 39% reduction in alias set sizes, and improved memory bug detection with only moderate runtime overhead.", "conclusion": "Precise, automated handling of custom allocation functions dramatically improves the precision and practicality of pointer analysis, revealing many more heap objects and memory bugs in large codebases."}}
