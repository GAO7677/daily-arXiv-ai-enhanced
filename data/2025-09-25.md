<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 14]
- [cs.PL](#cs.PL) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Automated Insertion of Flushes and Fences for Persistency](https://arxiv.org/abs/2509.19459)
*Yutong Guo,Weiyu Luo,Brian Demsky*

Main category: cs.SE

TL;DR: PMRobust automatically inserts persistence-enforcing flush/fence operations in code, guaranteeing safety from missing flush bugs and incurring minimal overhead, making persistent memory usage more reliable and developer-friendly.


<details>
  <summary>Details</summary>
Motivation: Using persistent memory (like CXL shared memory) requires developers to manually flush cache lines to ensure data is truly persistent, which is error-prone and hard to get right. Existing tools detect some bugs but require special test cases and cannot guarantee the absence of missing flush instructions.

Method: PMRobust is a compiler tool that automatically inserts flush and fence operations. It applies novel static analysis techniques and optimizations, especially focusing on newly allocated objects to ensure bug-free persistence guarantees without developer intervention.

Result: PMRobust was tested on libraries and data structures using persistent memory and showed very low performance overhead (0.26% geometric mean) compared to manual flush/fence placement.

Conclusion: PMRobust enables reliable bug-free usage of persistent memory by automating the insertion of flush and fence operations, eliminating developer errors and need for special test cases, with minimal overhead.

Abstract: CXL shared memory and persistent memory allow the contents of memory to
persist beyond crashes. Stores to persistent or CXL memory are typically not
immediately made persistent; developers must manually flush the corresponding
cache lines to force the data to be written to the underlying storage.
Correctly using flush and fence operations is known to be challenging. While
state-of-the-art tools can find missing flush instructions, they often require
bug-revealing test cases. No existing tools can ensure the absence of missing
flush bugs.
  In this paper, we present PMRobust, a compiler that automatically inserts
flush and fence operations to ensure that code using persistent memory is free
from missing flush and fence bugs. PMRobust employs a novel static analysis
with optimizations that target newly allocated objects. We have evaluated
PMRobust on persistent memory libraries and several persistent memory data
structures and measured a geometric mean overhead of 0.26% relative to the
original benchmarks with hand-placed flush and fence operations.

</details>


### [2] [Semantic-Aware Fuzzing: An Empirical Framework for LLM-Guided, Reasoning-Driven Input Mutation](https://arxiv.org/abs/2509.19533)
*Mengdi Lu,Steven Ding,Furkan Alaca,Philippe Charland*

Main category: cs.SE

TL;DR: The paper introduces a framework that combines AFL++ fuzzing and large language models to improve software vulnerability discovery by generating smarter mutations. The study shows that model selection and prompt design are more important than the number of examples shown, with throughput bottlenecks remaining a challenge.


<details>
  <summary>Details</summary>
Motivation: Traditional mutation-based fuzzers lack semantic reasoning and often miss deep vulnerabilities in IoT, mobile, and autonomous systems due to shallow input mutation strategies. Domain-specific constraints and protocol logic are typically not well handled, and supervised learning for correct mutation reasoning is impractical.

Method: The authors present an open-source microservices framework that integrates reasoning-capable large language models (LLMs) with AFL++ fuzzer on FuzzBench. They focus on asynchronous execution and hardware differences, and evaluate the integration of LLMs via few-shot and zero-shot prompt engineering for mutation generation.

Result: Among the tested models, Deepseek-r1-Distill-Llama-70B showed the most promising results. The effectiveness of LLM-based mutations is more influenced by prompt complexity and model choice than by the number of shots. Latency and throughput remain significant challenges.

Conclusion: Integrating reasoning LLMs with state-of-the-art fuzzers can yield more effective mutation strategies by incorporating semantic and protocol knowledge. However, practical adoption requires overcoming hardware and performance bottlenecks, and selecting suitable LLMs and prompt strategies.

Abstract: Security vulnerabilities in Internet-of-Things devices, mobile platforms, and
autonomous systems remain critical. Traditional mutation-based fuzzers -- while
effectively explore code paths -- primarily perform byte- or bit-level edits
without semantic reasoning. Coverage-guided tools such as AFL++ use
dictionaries, grammars, and splicing heuristics to impose shallow structural
constraints, leaving deeper protocol logic, inter-field dependencies, and
domain-specific semantics unaddressed. Conversely, reasoning-capable large
language models (LLMs) can leverage pretraining knowledge to understand input
formats, respect complex constraints, and propose targeted mutations, much like
an experienced reverse engineer or testing expert. However, lacking ground
truth for "correct" mutation reasoning makes supervised fine-tuning
impractical, motivating explorations of off-the-shelf LLMs via prompt-based
few-shot learning. To bridge this gap, we present an open-source microservices
framework that integrates reasoning LLMs with AFL++ on Google's FuzzBench,
tackling asynchronous execution and divergent hardware demands (GPU- vs.
CPU-intensive) of LLMs and fuzzers. We evaluate four research questions: (R1)
How can reasoning LLMs be integrated into the fuzzing mutation loop? (R2) Do
few-shot prompts yield higher-quality mutations than zero-shot? (R3) Can prompt
engineering with off-the-shelf models improve fuzzing directly? and (R4) Which
open-source reasoning LLMs perform best under prompt-only conditions?
Experiments with Llama3.3, Deepseek-r1-Distill-Llama-70B, QwQ-32B, and Gemma3
highlight Deepseek as the most promising. Mutation effectiveness depends more
on prompt complexity and model choice than shot count. Response latency and
throughput bottlenecks remain key obstacles, offering directions for future
work.

</details>


### [3] [Reverse Engineering User Stories from Code using Large Language Models](https://arxiv.org/abs/2509.19587)
*Mohamed Ouf,Haoyu Li,Michael Zhang,Mariam Guizani*

Main category: cs.SE

TL;DR: LLMs can accurately recover user stories from C++ source code, with simple prompt design (such as providing an example) often sufficient even for smaller models. Chain-of-Thought only slightly improves results, primarily in larger models.


<details>
  <summary>Details</summary>
Motivation: User stories are often crucial in agile software development, but are frequently missing or outdated in legacy code bases lacking documentation. This paper explores whether large language models (LLMs) can automatically recover user stories from source code, potentially aiding maintainers and developers.

Method: The authors use 1,750 annotated C++ code snippets of varying complexity to assess five state-of-the-art LLMs (ranging in size) over six different prompt design strategies, including illustrative examples and structured reasoning (Chain-of-Thought).

Result: All tested LLMs achieved an average F1 score of 0.8 for code snippets up to 200 NLOC. Notably, providing a single example enables the smallest model (8B) to reach the performance of a much larger model (70B). Chain-of-Thought strategies only provide minor improvements, mainly for the larger models.

Conclusion: LLMs are effective at recovering user stories from source code, even in complex legacy systems, with simple prompt designs (e.g., single illustrative examples) being highly efficient, especially for smaller models. More complex prompting strategies offer limited additional benefits.

Abstract: User stories are essential in agile development, yet often missing or
outdated in legacy and poorly documented systems. We investigate whether large
language models (LLMs) can automatically recover user stories directly from
source code and how prompt design impacts output quality. Using 1,750 annotated
C++ snippets of varying complexity, we evaluate five state-of-the-art LLMs
across six prompting strategies. Results show that all models achieve, on
average, an F1 score of 0.8 for code up to 200 NLOC. Our findings show that a
single illustrative example enables the smallest model (8B) to match the
performance of a much larger 70B model. In contrast, structured reasoning via
Chain-of-Thought offers only marginal gains, primarily for larger models.

</details>


### [4] [Assertion Messages with Large Language Models (LLMs) for Code](https://arxiv.org/abs/2509.19673)
*Ahmed Aljohani,Anamul Haque Mollah,Hyunsook Do*

Main category: cs.SE

TL;DR: LLMs can generate assertion messages for unit tests, though their performance lags behind human-written messages. More context leads to better quality, and the field needs improved evaluation methods and modeling. Codestral-22B currently leads among tested LLMs, especially when given descriptive comments.


<details>
  <summary>Details</summary>
Motivation: Assertion messages are crucial in unit tests for clarifying reasons behind test failures, but are often neglected by developers and automated tools. LLMs have not been systematically evaluated for their capability to generate assertion messages.

Method: The paper evaluates four advanced Fill-in-the-Middle LLMs (Qwen2.5-Coder-32B, Codestral-22B, CodeLlama-13B, StarCoder) using a dataset of 216 Java test methods with developer-written assertion messages. Scores are determined through human-like evaluation and ablation studies are performed to test the effect of adding descriptive test comments.

Result: Codestral-22B achieved the highest LLM-generated score (2.76/5) compared to human-written messages (3.24/5). Adding descriptive test comments further improved Codestral's score to 2.97/5. All models frequently mirrored developer linguistic patterns. The study found limitations in current models and evaluation metrics.

Conclusion: Context, such as descriptive comments, significantly improves LLM-generated assertion messages. The paper provides a foundation for further research in automated, context-aware generation of informative assertion messages for unit tests.

Abstract: Assertion messages significantly enhance unit tests by clearly explaining the
reasons behind test failures, yet they are frequently omitted by developers and
automated test-generation tools. Despite recent advancements, Large Language
Models (LLMs) have not been systematically evaluated for their ability to
generate informative assertion messages. In this paper, we introduce an
evaluation of four state-of-the-art Fill-in-the-Middle (FIM) LLMs -
Qwen2.5-Coder-32B, Codestral-22B, CodeLlama-13B, and StarCoder - on a dataset
of 216 Java test methods containing developer-written assertion messages. We
find that Codestral-22B achieves the highest quality score of 2.76 out of 5
using a human-like evaluation approach, compared to 3.24 for manually written
messages. Our ablation study shows that including descriptive test comments
further improves Codestral's performance to 2.97, highlighting the critical
role of context in generating clear assertion messages. Structural analysis
demonstrates that all models frequently replicate developers' preferred
linguistic patterns. We discuss the limitations of the selected models and
conventional text evaluation metrics in capturing diverse assertion message
structures. Our benchmark, evaluation results, and discussions provide an
essential foundation for advancing automated, context-aware generation of
assertion messages in test code. A replication package is available at
https://doi.org/10.5281/zenodo.15293133

</details>


### [5] [Intuition to Evidence: Measuring AI's True Impact on Developer Productivity](https://arxiv.org/abs/2509.19708)
*Anand Kumar,Vishal Khare,Deepak Sharma,Satyam Kumar,Vijay Saini,Anshul Yadav,Sachendra Jain,Ankit Rana,Pratham Verma,Vaibhav Meena,Avinash Edubilli*

Main category: cs.SE

TL;DR: A year-long, real-world study of an AI tool (DeputyDev) used by 300 engineers showed major gains: faster code reviews, more code shipped, and high satisfaction. Empirical results underline both the potential and the challenges of embedding AI in enterprise software development workflows.


<details>
  <summary>Details</summary>
Motivation: AI-assisted software development tools are rapidly gaining attention, but most studies rely on controlled benchmarks rather than real-world, large-scale deployments. There is a need to empirically assess productivity, adoption, and challenges when such tools are actually used in enterprise environments.

Method: A longitudinal, cohort-based empirical study was conducted over one year with 300 engineers using an in-house AI platform called DeputyDev. Productivity metrics, adoption rates, satisfaction scores, and code shipment volume were carefully tracked and analyzed as the platform was integrated into daily workflows.

Result: The tool achieved a 31.8% reduction in PR review cycle time, an 85% satisfaction rate for review features, and a 93% desire to continue usage. Usage ramped to 83% engagement by month 6, later stabilizing at 60%. Top adopters saw a 61% increase in code volume, and the tool contributed to a 28% overall increase in shipped code.

Conclusion: Deploying AI-assisted development tools at scale produces measurable productivity gains and high developer satisfaction, but also reveals unique challenges not observed in lab settings. Real-world adoption is feasible and impactful, provided practical deployment barriers are addressed.

Abstract: We present a comprehensive real-world evaluation of AI-assisted software
development tools deployed at enterprise scale. Over one year, 300 engineers
across multiple teams integrated an in-house AI platform (DeputyDev) that
combines code generation and automated review capabilities into their daily
workflows. Through rigorous cohort analysis, our study demonstrates
statistically significant productivity improvements, including an overall 31.8%
reduction in PR review cycle time.
  Developer adoption was strong, with 85% satisfaction for code review features
and 93% expressing a desire to continue using the platform. Adoption patterns
showed systematic scaling from 4% engagement in month 1 to 83% peak usage by
month 6, stabilizing at 60% active engagement. Top adopters achieved a 61%
increase in code volume pushed to production, contributing to approximately 30
to 40% of code shipped to production through this tool, accounting for an
overall 28% increase in code shipment volume.
  Unlike controlled benchmark evaluations, our longitudinal analysis provides
empirical evidence from production environments, revealing both the
transformative potential and practical deployment challenges of integrating AI
into enterprise software development workflows.

</details>


### [6] [Beyond Language Barriers: Multi-Agent Coordination for Multi-Language Code Generation](https://arxiv.org/abs/2509.19918)
*Micheline Bénédicte Moumoula,Serge Lionel Nikiema,Albérick Euraste Djire,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.SE

TL;DR: XL-CoGen introduces a multi-agent system for code generation across diverse languages, using data-driven bridging and cross-language knowledge transfer to achieve significantly better results over existing methods.


<details>
  <summary>Details</summary>
Motivation: Ensuring high-quality code generation across various programming languages is crucial due to the heterogeneous nature of modern software stacks. Existing LLM-based solutions struggle with languages that have limited training data and often treat each language separately, missing opportunities to transfer knowledge or leverage patterns across languages.

Method: XL-CoGen uses a coordinated multi-agent architecture combining intermediate representation, code generation, translation, and automated code repair. A key innovation is its data-driven selection of bridging languages using transfer matrices, enabling effective cross-language translation and validating outputs early with iterative correction and contextual scaffolding.

Result: XL-CoGen delivers substantial improvements, achieving up to 13 percentage points gain over the best fine-tuned baseline and up to 30 percentage points over single-language multi-agent systems. Ablation studies show that transfer-matrix-driven bridging outperforms heuristics-based approaches, underscoring the advantage of cumulative cross-language knowledge transfer.

Conclusion: XL-CoGen establishes a new state-of-the-art in multi-language code generation, especially for languages with limited training data, by leveraging data-driven cross-language knowledge sharing and coordinated agent collaboration.

Abstract: Producing high-quality code across multiple programming languages is
increasingly important as today's software systems are built on heterogeneous
stacks. Large language models (LLMs) have advanced the state of automated
programming, yet their proficiency varies sharply between languages, especially
those with limited training data such as Rust, Perl, OCaml, and Erlang. Many
current solutions including language-specific fine-tuning, multi-agent
orchestration, transfer learning, and intermediate-representation pipelines
still approach each target language in isolation, missing opportunities to
share knowledge or exploit recurring cross-language patterns.
  XL-CoGen tackles this challenge with a coordinated multi-agent architecture
that integrates intermediate representation, code generation, translation, and
automated repair. Its distinguishing feature is a data-driven mechanism for
selecting bridging languages: empirically derived transfer matrices identify
the best intermediate languages based on demonstrated translation success
rather than raw generation accuracy. The system performs early output
validation, iteratively corrects errors, and reuses intermediate artifacts as
contextual scaffolds for subsequent translations.
  Extensive experiments show that XL-CoGen yields notable improvements with 13
percentage-point gains over the strongest fine-tuned baseline and as much as 30
percentage points over existing single-language multi-agent methods. Ablation
studies further demonstrate that compatibility-guided bridging significantly
outperforms LLM-based heuristics, confirming the value of cumulative
cross-language knowledge transfer.

</details>


### [7] [Demystifying the Evolution of Neural Networks with BOM Analysis: Insights from a Large-Scale Study of 55,997 GitHub Repositories](https://arxiv.org/abs/2509.20010)
*Xiaoning Ren,Yuhang Ye,Xiongfei Wu,Yueming Wu,Yinxing Xue*

Main category: cs.SE

TL;DR: This paper introduces NNBOM, a dedicated Bill of Materials dataset for neural networks, constructed from nearly 56K PyTorch GitHub repositories. It provides empirical insights into NN software evolution and demonstrates practical utility through prototype analysis and recommendation tools.


<details>
  <summary>Details</summary>
Motivation: Existing SBOMs and conceptual AIBOMs do not meet the evolving needs of neural network software analysis because of their unique reliance on modules and pre-trained models. There is a lack of practical tools for large-scale evolutionary analysis in this domain, necessitating an NN-specific solution.

Method: Construction of a large-scale NNBOM from 55,997 PyTorch GitHub repositories. Empirical analysis of neural network software evolution regarding scale, reuse, and dependencies. Development of supporting prototype applications.

Result: Creation of a large-scale, curated NNBOM database. Comprehensive analysis showing trends in NN software evolution. Development of two prototype applications that validate the usefulness of NNBOM-based insights for repository maintenance and component recommendation.

Conclusion: The authors have established Neural Network Bill of Materials (NNBOM) as a formal dataset structure for analyzing the evolution of neural network repositories, showing its effectiveness with prototype tools.

Abstract: Neural networks have become integral to many fields due to their exceptional
performance. The open-source community has witnessed a rapid influx of neural
network (NN) repositories with fast-paced iterations, making it crucial for
practitioners to analyze their evolution to guide development and stay ahead of
trends. While extensive research has explored traditional software evolution
using Software Bill of Materials (SBOMs), these are ill-suited for NN software,
which relies on pre-defined modules and pre-trained models (PTMs) with distinct
component structures and reuse patterns. Conceptual AI Bills of Materials
(AIBOMs) also lack practical implementations for large-scale evolutionary
analysis. To fill this gap, we introduce the Neural Network Bill of Material
(NNBOM), a comprehensive dataset construct tailored for NN software. We create
a large-scale NNBOM database from 55,997 curated PyTorch GitHub repositories,
cataloging their TPLs, PTMs, and modules. Leveraging this database, we conduct
a comprehensive empirical study of neural network software evolution across
software scale, component reuse, and inter-domain dependency, providing
maintainers and developers with a holistic view of its long-term trends.
Building on these findings, we develop two prototype applications,
\textit{Multi repository Evolution Analyzer} and \textit{Single repository
Component Assessor and Recommender}, to demonstrate the practical value of our
analysis.

</details>


### [8] [V-GameGym: Visual Game Generation for Code Large Language Models](https://arxiv.org/abs/2509.20136)
*Wei Zhang,Jack Yang,Renshuai Tao,Lingzheng Chai,Shawn Guo,Jiajun Wu,Xiaoming Chen,Ganqu Cui,Ning Ding,Xander Xu,Hu Wei,Bowen Zhou*

Main category: cs.SE

TL;DR: Current code LLM benchmarks miss key aspects of visual game development. V-GameGym is a new, diverse benchmark and evaluation framework that assesses LLMs on tasks like playability and visual appeal, supporting better real-world use in game development.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for code large language models (LLMs) focus on basic programming tasks and single modalities, neglecting important aspects of game development like playability, visual appeal, and user interaction. This limits the real-world applicability of LLMs in game development.

Method: The paper introduces V-GameGym, a new benchmark containing 2,219 curated game development samples from 100 clusters, using a novel clustering-based selection method. It also proposes a multimodal evaluation framework with an automated pipeline for assessing visual code synthesis within UI sandbox environments.

Result: V-GameGym enables robust evaluation of code LLMs on comprehensive game development tasks. The benchmark provides diversity and completeness, and the evaluation framework delivers quantifiable metrics for both the quality of visual programming and interactive elements.

Conclusion: V-GameGym bridges the gap between current code LLM benchmarks and practical game development demands by focusing on visual and interactive metrics, supporting better assessment of LLMs for real-world game creation tasks.

Abstract: Code large language models have demonstrated remarkable capabilities in
programming tasks, yet current benchmarks primarily focus on single modality
rather than visual game development. Most existing code-related benchmarks
evaluate syntax correctness and execution accuracy, overlooking critical
game-specific metrics such as playability, visual aesthetics, and user
engagement that are essential for real-world deployment. To address the gap
between current LLM capabilities in algorithmic problem-solving and competitive
programming versus the comprehensive requirements of practical game
development, we present V-GameGym, a comprehensive benchmark comprising 2,219
high-quality samples across 100 thematic clusters derived from real-world
repositories, adopting a novel clustering-based curation methodology to ensure
both diversity and structural completeness. Further, we introduce a multimodal
evaluation framework with an automated LLM-driven pipeline for visual code
synthesis using complete UI sandbox environments. Our extensive analysis
reveals that V-GameGym effectively bridges the gap between code generation
accuracy and practical game development workflows, providing quantifiable
quality metrics for visual programming and interactive element generation.

</details>


### [9] [Enhancing Requirement Traceability through Data Augmentation Using Large Language Models](https://arxiv.org/abs/2509.20149)
*Jianzhang Zhang,Jialong Zhou,Nan Niu,Chuang Liu*

Main category: cs.SE

TL;DR: This paper tackles the data scarcity issue in automated requirements traceability using large language models for data augmentation. Leveraging different prompt-based templates and optimizing the model's encoder, the approach greatly improves performance, highlighting the practical value of LLMs for trace link generation.


<details>
  <summary>Details</summary>
Motivation: Automated requirements traceability is essential in software engineering but is currently limited by a lack of sufficient training data and a persistent semantic gap between requirements and code.

Method: The paper uses large language models (LLMs) for prompt-based data augmentation, applying zero-shot and few-shot templates with four different LLMs (Gemini 1.5 Pro, Claude 3, GPT-3.5, GPT-4) to generate augmented requirement-to-code trace links. The encoder of the tracing model is also optimized for efficiency and adaptability to the augmented data.

Result: Experimental results demonstrate that the proposed approach significantly boosts model performance, with an F1 score improvement of up to 28.59%.

Conclusion: The use of LLM-based data augmentation and encoder optimization provides a promising solution to data scarcity challenges in requirements traceability, leading to substantial improvements in traceability automation.

Abstract: Requirements traceability is crucial in software engineering to ensure
consistency between requirements and code. However, existing automated
traceability methods are constrained by the scarcity of training data and
challenges in bridging the semantic gap between artifacts. This study aims to
address the data scarcity problem in requirements traceability by employing
large language models (LLMs) for data augmentation. We propose a novel approach
that utilizes prompt-based techniques with LLMs to generate augmented
requirement-to-code trace links, thereby enhancing the training dataset. Four
LLMs (Gemini 1.5 Pro, Claude 3, GPT-3.5, and GPT-4) were used, employing both
zero-shot and few-shot templates. Moreover, we optimized the encoder component
of the tracing model to improve its efficiency and adaptability to augmented
data. The key contributions of this paper are: (1) proposing and evaluating
four prompt templates for data augmentation; (2) providing a comparative
analysis of four LLMs for generating trace links; (3) enhancing the model's
encoder for improved adaptability to augmented datasets. Experimental results
show that our approach significantly enhances model performance, achieving an
F1 score improvement of up to 28.59%, thus demonstrating its effectiveness and
potential for practical application.

</details>


### [10] [Benchmarking Web API Integration Code Generation](https://arxiv.org/abs/2509.20172)
*Daniel Maninger,Leon Chemnitz,Amir Molzam Sharifloo,Jannis Brugger,Mira Mezini*

Main category: cs.SE

TL;DR: Open-source LLMs have trouble accurately generating web API code, with less than 40% task success in tests. API invocation remains a major challenge for today's models.


<details>
  <summary>Details</summary>
Motivation: API integration is essential in digital infrastructure, yet generating correct code for invoking web APIs is often difficult and error-prone. With the rise of large language models (LLMs) in software development, it's necessary to assess their ability to automate API code generation.

Method: The authors introduce a dataset and an evaluation pipeline specifically designed to test and analyze the performance of LLMs in generating web API invocation code. They conduct experiments using various open-source LLMs.

Result: Experimental results show that current open-source LLMs frequently make mistakes in generating API invocation code, such as hallucinating endpoints and misusing arguments. No tested model correctly generated solutions for more than 40% of the tasks.

Conclusion: LLMs, despite their popularity in software development, currently struggle with automating web API integration code generation. Significant improvements are needed before LLMs can reliably perform this task.

Abstract: API integration is a cornerstone of our digital infrastructure, enabling
software systems to connect and interact. However, as shown by many studies,
writing or generating correct code to invoke APIs, particularly web APIs, is
challenging. Although large language models~(LLMs) have become popular in
software development, their effectiveness in automating the generation of web
API integration code remains unexplored. In order to address this, we present a
dataset and evaluation pipeline designed to assess the ability of LLMs to
generate web API invocation code. Our experiments with several open-source LLMs
reveal that generating API invocations poses a significant challenge, resulting
in hallucinated endpoints, incorrect argument usage, and other errors. None of
the evaluated open-source models were able to solve more than 40% of the tasks.

</details>


### [11] [The Cream Rises to the Top: Efficient Reranking Method for Verilog Code Generation](https://arxiv.org/abs/2509.20215)
*Guang Yang,Wei Zheng,Xiang Chen,Yifan Sun,Fengji Zhang,Terry Yue Zhuo*

Main category: cs.SE

TL;DR: LLMs aren't reliable for Verilog code generation due to specialized knowledge gaps. The paper introduces VCD-RNK, a domain-aware model that reranks code by simulating expert reasoning, improving code reliability without expensive testing.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle to generate reliable Verilog code due to lack of specialized domain knowledge. Hardware engineers require a single trustworthy solution, not multiple uncertain code samples. Existing evaluation methods are computationally expensive and don't guarantee semantic correctness.

Method: The paper formulates code generation as a semantic alignment issue and introduces VCD-RNK, a discriminator model designed to rerank Verilog code candidates. VCD-RNK simulates expert reasoning (code analysis, test generation, correctness assessment) via distilled knowledge to evaluate code quality without expensive test executions.

Result: VCD-RNK effectively reranks Verilog code candidates by leveraging domain-specific reasoning, enabling efficient and trustworthy selection of correct code. It avoids the need for costly computational tests required by previous approaches.

Conclusion: By incorporating expert-level reasoning dimensions and semantic alignment techniques, VCD-RNK enables efficient and reliable selection of correct Verilog code, addressing key limitations of LLMs in hardware code generation.

Abstract: LLMs face significant challenges in Verilog generation due to limited
domain-specific knowledge. While sampling techniques improve pass@k metrics,
hardware engineers need one trustworthy solution rather than uncertain
candidates. To bridge this gap, we formulate it as a semantic alignment problem
between requirements and Verilog implementations, and propose VCD-RNK, a
discriminator model tailored for efficient Verilog code reranking.
Specifically, VCD-RNKincorporates Verilog-specific reasoning by distilling
expert knowledge across three dimensions: code semantic analysis, test case
generation, and functional correctness assessment. By explicitly simulating the
above reasoning processes during inference, VCD-RNK effectively avoids
computationally intensive test execution in existing methods.

</details>


### [12] [Confidentiality-Preserving Verifiable Business Processes through Zero-Knowledge Proofs](https://arxiv.org/abs/2509.20300)
*Jannis Kiesel,Jonathan Heiss*

Main category: cs.SE

TL;DR: This paper proposes integrating zero-knowledge proofs with business process management to securely verify process integrity across organizations, automatically and efficiently, while keeping sensitive business data private. The technique is validated with a product carbon footprinting case and shows promise for practical BPM lifecycle integration.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of ensuring business process integrity across organizations without disclosing confidential information. In inter-organizational processes, sensitive data sharing is a barrier to transparency and trust.

Method: It introduces a zero-knowledge proof (ZKP)-based approach, integrating ZK virtual machines (zkVMs) into business process management engines. The method includes a comprehensive system architecture and a prototype implementation that enables chained verifiable computations using proof compositions.

Result: The approach is tested with product carbon footprinting as an example, demonstrating organizations can verify business process integrity without exposing sensitive data. Multiple ZKP proving variants are assessed for their efficiency, and practical BPM lifecycle integration is discussed. Experimental results show successful automation of process verification under confidentiality constraints.

Conclusion: Zero-knowledge proofs can effectively ensure business process integrity and automation within confidentiality limits, making process verification secure and practical for real-world applications.

Abstract: Ensuring the integrity of business processes without disclosing confidential
business information is a major challenge in inter-organizational processes.
This paper introduces a zero-knowledge proof (ZKP)-based approach for the
verifiable execution of business processes while preserving confidentiality. We
integrate ZK virtual machines (zkVMs) into business process management engines
through a comprehensive system architecture and a prototypical implementation.
Our approach supports chained verifiable computations through proof
compositions. On the example of product carbon footprinting, we model
sequential footprinting activities and demonstrate how organizations can prove
and verify the integrity of verifiable processes without exposing sensitive
information. We assess different ZKP proving variants within process models for
their efficiency in proving and verifying, and discuss the practical
integration of ZKPs throughout the Business Process Management (BPM) lifecycle.
Our experiment-driven evaluation demonstrates the automation of process
verification under given confidentiality constraints.

</details>


### [13] [Protocol Testing with I/O Grammars](https://arxiv.org/abs/2509.20308)
*Alexander Liggesmeyer,José Antonio Zamudio Amaya,Andreas Zeller*

Main category: cs.SE

TL;DR: This paper presents I/O grammars and the FANDANGO framework, offering an effective unified solution for protocol test generation and validation, enabling quick and comprehensive coverage versus traditional methods.


<details>
  <summary>Details</summary>
Motivation: Software test generation, especially for protocols, faces two major challenges: generating syntactically/semantically correct and diverse inputs, and constructing oracles to check the correctness of outputs. Existing tools do not fully address both in an integrated, flexible, and comprehensive manner.

Method: The paper introduces 'I/O grammars', a novel formalism that allows the complete specification of protocol syntax and semantics across messages, states, and interactions. The FANDANGO framework uses these grammars to function as a test generator, mock object, and oracle for multiple protocol parties. The approach features user-defined constraints and k-path guidance for focused, systematic coverage.

Result: I/O grammars were implemented for several protocols including DNS, FTP, and SMTP, demonstrating the ability to specify protocols completely and enable precise output validation. Empirical results show that this approach achieves significantly faster and more thorough coverage of protocol behaviors than random-based approaches.

Conclusion: The proposed approach, using I/O grammars and the FANDANGO framework, provides a complete, integrated, and flexible solution for protocol testing, combining both input generation and output checking and outperforming existing state-of-the-art methods in coverage.

Abstract: Generating software tests faces two fundamental problems. First, one needs to
_generate inputs_ that are syntactically and semantically correct, yet
sufficiently diverse to cover behavior. Second, one needs an _oracle_ to _check
outputs_ whether a test case is correct or not. Both problems become apparent
in _protocol testing_, where inputs are messages exchanged between parties, and
outputs are the responses of these parties.
  In this paper, we propose a novel approach to protocol testing that combines
input generation and output checking in a single framework. We introduce _I/O
grammars_ as the first means to _completely_ specify the syntax and semantics
of protocols, including messages, states, and interactions. Our implementation,
based on the FANDANGO framework, takes a single I/O grammar, and can act as a
_test generator_, as a _mock object_, and as an _oracle_ for a _client_, a
_server_, or both (or actually any number of parties), a versatility not found
in any existing tool or formalism. User-defined _constraints}_can have the
generator focus on arbitrary protocol features; $k$-path guidance
systematically covers states, messages, responses, and value alternatives in a
unified fashion.
  We evaluate the effectiveness of our approach by applying it to several
protocols, including DNS, FTP, and SMTP. We demonstrate that I/O grammars can
specify advanced protocol features correctly and completely, while also
enabling output validation of the programs under test. In its evaluation, we
find that systematic coverage of the I/O grammar results in much quicker
coverage of the input and response spaces (and thus functionality) compared to
the random-based state-of-the-art approaches.

</details>


### [14] [Developer Productivity With and Without GitHub Copilot: A Longitudinal Mixed-Methods Case Study](https://arxiv.org/abs/2509.20353)
*Viktoria Stray,Elias Goldmann Brandtzæg,Viggo Tellefsen Wivestad,Astri Barbala,Nils Brede Moe*

Main category: cs.SE

TL;DR: GitHub Copilot users feel more productive, but their commit activity does not show significant change after adopting the tool; subjective perceptions of productivity may differ from objective measures.


<details>
  <summary>Details</summary>
Motivation: To understand the actual impact of generative AI tools like GitHub Copilot on developer productivity and activity in a real organizational setting.

Method: Mixed-methods case study: quantitative analysis of 26,317 non-merge commits from 703 repositories comparing Copilot users vs. non-users, complemented by surveys and 13 interviews.

Result: Copilot users were consistently more active than non-users even before adopting Copilot. After adoption, no statistically significant changes in commit-based activity were identified, although small increases were seen.

Conclusion: There is a discrepancy between objective commit-based metrics and developers' subjective experience of increased productivity with Copilot. The tool may not show measurable activity changes, but users perceive themselves as more productive.

Abstract: This study investigates the real-world impact of the generative AI (GenAI)
tool GitHub Copilot on developer activity and perceived productivity. We
conducted a mixed-methods case study in NAV IT, a large public sector agile
organization. We analyzed 26,317 unique non-merge commits from 703 of NAV IT's
GitHub repositories over a two-year period, focusing on commit-based activity
metrics from 25 Copilot users and 14 non-users. The analysis was complemented
by survey responses on their roles and perceived productivity, as well as 13
interviews. Our analysis of activity metrics revealed that individuals who used
Copilot were consistently more active than non-users, even prior to Copilot's
introduction. We did not find any statistically significant changes in
commit-based activity for Copilot users after they adopted the tool, although
minor increases were observed. This suggests a discrepancy between changes in
commit-based metrics and the subjective experience of productivity.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [15] [Macro-embedding Compiler Intermediate Languages in Racket](https://arxiv.org/abs/2509.19607)
*William J. Bowman*

Main category: cs.PL

TL;DR: This paper introduces a macro-embedding framework in Racket for a family of compiler intermediate languages, supporting both high- and low-level features. The approach promotes code reuse, modularity, and flexible extension of language features, streamlining interpreter generation and enhancing compiler course infrastructure.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to improve the process of implementing and testing compiler intermediate languages by enabling more code reuse, modularity, and easier interoperability between high-level and low-level language features. Traditional approaches require separate interpreter implementations for each language, which is inefficient and less flexible.

Method: The authors designed and implemented a macro-embedding approach in Racket. This method involves embedding a family of intermediate languages—ranging from a Scheme-like language to x86-64 assembly—into the host language Racket, primarily via local macro expansion. This allows abstraction and composition of language features, safe and unsafe, at different compiler stages.

Result: The result is a flexible framework that allows interpreters for all these languages to be derived easily, facilitates code reuse, and supports interoperability and extensibility. It simplifies semantic development, enables multiple interfaces, and allows individual features of intermediate languages to be extended or redefined.

Conclusion: The paper concludes that macro-embedding for compiler intermediate languages enables much greater code reuse, modularity, extensibility, and interoperability, compared to separately implemented interpreters. It offers a powerful and practical approach for language-oriented compiler design and education.

Abstract: We present the design and implementation of a macro-embedding of a family of
compiler intermediate languages, from a Scheme-like language to x86-64, into
Racket. This embedding is used as part of a testing framework for a compilers
course to derive interpreters for all the intermediate languages. The embedding
implements features including safe, functional abstractions as well as unsafe
assembly features, and the interactions between the two at various intermediate
stages.
  This paper aims to demonstrate language-oriented techniques and abstractions
for implementing (1) a large family of languages and (2) interoperability
between low- and high-level languages. The primary strength of this approach is
the high degree of code reuse and interoperability compared to implementing
each interpreter separately. The design emphasizes modularity and
compositionality of an open set of language features by local macro expansion
into a single host language, rather than implementing a language pre-defined by
a closed set of features. This enables reuse from both the host language
(Racket) and between intermediate languages, and enables interoperability
between high- and low-level features, simplifying development of the
intermediate language semantics. It also facilitates extending or redefining
individual language features in intermediate languages, and exposing multiple
interfaces to the embedded languages.

</details>


### [16] [Compilation as Multi-Language Semantics](https://arxiv.org/abs/2509.19613)
*William J. Bowman*

Main category: cs.PL

TL;DR: The paper proposes modeling compilers and interoperability as a unified reduction system, reducing duplication and strengthening semantic guarantees like correctness and type-preservation for secure multi-language compilation.


<details>
  <summary>Details</summary>
Motivation: Modeling interoperability between programs written in different languages is challenging, especially for verified and secure compilation. Traditional approaches using multi-language semantics require duplicating work by having separate models for compilation and interoperability.

Method: The paper presents a work-in-progress approach to model the entire compiler as a reduction system on open terms within multi-language semantics, instead of using syntactic translation. This uniform method simultaneously defines both the compiler and the semantics of interoperability.

Result: The approach reduces duplication and provides new semantic insights. It covers both ahead-of-time (AOT) and just-in-time (JIT) compilation within the model. Key properties like confluence and subject reduction in the multi-language reduction system imply compiler correctness and type-preservation, helping to concentrate proof efforts on the harder aspects.

Conclusion: This unified reduction-based approach to modeling verified and secure compilation in multi-language semantics avoids duplication, clarifies compiler correctness and secure compilation proofs, and type-preservation, streamlining interoperability modeling.

Abstract: Modeling interoperability between programs in different languages is a key
problem when modeling verified and secure compilation, which has been
successfully addressed using multi-language semantics. Unfortunately, existing
models of compilation using multi-language semantics define two variants of
each compiler pass: a syntactic translation on open terms to model compilation,
and a run-time translation of closed terms at multi-language boundaries to
model interoperability.
  In this talk, I discuss work-in-progress approach to uniformly model a
compiler entirely as a reduction system on open term in a multi-language
semantics, rather than as a syntactic translation. This simultaneously defines
the compiler and the interoperability semantics, reducing duplication. It also
provides interesting semantic insights. Normalization of the cross-language
redexes performs ahead-of-time (AOT) compilation. Evaluation in the
multi-language models just-in-time (JIT) compilation. Confluence of
multi-language reduction implies compiler correctness, and part of the secure
compilation proof (full abstraction), enabling focus on the difficult part of
the proof. Subject reduction of the multi-language reduction implies
type-preservation of the compiler.

</details>


### [17] [The Syntax and Semantics of einsum](https://arxiv.org/abs/2509.20020)
*Maurice Wenig,Paul G. Rump,Mark Blacher,Joachim Giesen*

Main category: cs.PL

TL;DR: This paper builds a formal theoretical foundation for the widely-used einsum tensor notation, proving equivalence rules and fostering better interoperability and optimization in machine learning and scientific computing frameworks.


<details>
  <summary>Details</summary>
Motivation: The einsum notation is widely used for tensor expressions in various fields, but it lacks a consistent theoretical framework and is implemented differently across major libraries. This fragmentation limits formal reasoning, verification, and optimization.

Method: The paper discusses existing terminology related to tensor expressions and develops a formal definition of the einsum language. It then establishes and proves key equivalence rules based on this formalization, connecting these rules to relevant practical applications.

Result: The authors provide a rigorous formal foundation for the einsum notation, prove important equivalence rules, and demonstrate their practical value for systematic reasoning and optimization in computational frameworks.

Conclusion: A unified and formal definition of einsum enables systematic reasoning, verification, and more effective implementation and optimization across different frameworks.

Abstract: In 2011, einsum was introduced to NumPy as a practical and convenient
notation for tensor expressions in machine learning, quantum circuit
simulation, and other fields. It has since been implemented in additional
Python frameworks such as PyTorch and TensorFlow, as well as in other
programming languages such as Julia. Despite its practical success, the einsum
notation still lacks a solid theoretical basis, and is not unified across the
different frameworks, limiting opportunities for formal reasoning and
systematic optimization. In this work, we discuss the terminology of tensor
expressions and provide a formal definition of the einsum language. Based on
this definition, we formalize and prove important equivalence rules for tensor
expressions and highlight their relevance in practical applications.

</details>
