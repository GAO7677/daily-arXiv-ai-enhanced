{"id": "2507.17930", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.17930", "abs": "https://arxiv.org/abs/2507.17930", "authors": ["Vahid Garousi", "Zafar Jafarov"], "title": "How Software Engineers Engage with AI: A Pragmatic Process Model and Decision Framework Grounded in Industry Observations", "comment": null, "summary": "Artificial Intelligence (AI) has the potential to transform Software\nEngineering (SE) by enhancing productivity, efficiency, and decision support.\nTools like GitHub Copilot and ChatGPT have given rise to \"vibe coding\"-an\nexploratory, prompt-driven development style. Yet, how software engineers\nengage with these tools in daily tasks, especially in deciding whether to\ntrust, refine, or reject AI-generated outputs, remains underexplored. This\npaper presents two complementary contributions. First, a pragmatic process\nmodel capturing real-world AI-assisted SE activities, including prompt design,\ninspection, fallback, and refinement. Second, a 2D decision framework that\ncould help developers reason about trade-offs between effort saved and output\nquality. Grounded in practitioner reports and direct observations in three\nindustry settings across Turkiye and Azerbaijan, our work illustrates how\nengineers navigate AI use with human oversight. These models offer structured,\nlightweight guidance to support more deliberate and effective use of AI tools\nin SE, contributing to ongoing discussions on practical human-AI collaboration.", "AI": {"tldr": "The paper studies how engineers engage with AI tools in real-world software engineering, presenting practical models for structured, effective collaboration between humans and AI, informed by industry observations.", "motivation": "The authors are motivated by the increasing use of Artificial Intelligence (AI) in Software Engineering (SE), particularly with tools like GitHub Copilot and ChatGPT, which enable new development approaches. However, how engineers interact with these tools in daily practice\u2014including trust and refinement decisions\u2014remains insufficiently studied.", "method": "The paper employs qualitative research, grounding its findings in practitioner reports and direct observations across three industry settings in Turkiye and Azerbaijan. The study synthesizes these insights into models and frameworks for guiding AI-assisted SE activities.", "result": "The authors introduced a pragmatic process model covering key AI-assisted SE activities: prompt design, inspection, fallback, and refinement. Additionally, they proposed a 2D decision framework to help developers weigh the trade-offs between effort saved and the quality of AI-generated output.", "conclusion": "The study concludes that the presented models provide structured, lightweight guidance that supports more effective, deliberative use of AI tools in SE, addressing a gap in practical human-AI collaboration understanding."}}
{"id": "2507.17991", "categories": ["cs.SE", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.17991", "abs": "https://arxiv.org/abs/2507.17991", "authors": ["Peter Eckmann", "Adrian Barnett", "Alexandra Bannach-Brown", "Elisa Pilar Bascunan Atria", "Guillaume Cabanac", "Louise Delwen Owen Franzen", "Ma\u0142gorzata Anna Gazda", "Kaitlyn Hair", "James Howison", "Halil Kilicoglu", "Cyril Labbe", "Sarah McCann", "Vladislav Nachev", "Martijn Roelandse", "Maia Salholz-Hillel", "Robert Schulz", "Gerben ter Riet", "Colby Vorland", "Anita Bandrowski", "Tracey Weissgerber"], "title": "Use as Directed? A Comparison of Software Tools Intended to Check Rigor and Transparency of Published Work", "comment": null, "summary": "The causes of the reproducibility crisis include lack of standardization and\ntransparency in scientific reporting. Checklists such as ARRIVE and CONSORT\nseek to improve transparency, but they are not always followed by authors and\npeer review often fails to identify missing items. To address these issues,\nthere are several automated tools that have been designed to check different\nrigor criteria. We have conducted a broad comparison of 11 automated tools\nacross 9 different rigor criteria from the ScreenIT group. We found some\ncriteria, including detecting open data, where the combination of tools showed\na clear winner, a tool which performed much better than other tools. In other\ncases, including detection of inclusion and exclusion criteria, the combination\nof tools exceeded the performance of any one tool. We also identified key areas\nwhere tool developers should focus their effort to make their tool maximally\nuseful. We conclude with a set of insights and recommendations for stakeholders\nin the development of rigor and transparency detection tools. The code and data\nfor the study is available at https://github.com/PeterEckmann1/tool-comparison.", "AI": {"tldr": "A study compared 11 automated tools for checking transparency and rigor in scientific reporting. Results showed some tools excel at specific tasks, while tool combinations work best for others. Recommendations for future tool development are given, with open access to study resources.", "motivation": "The reproducibility crisis in science is partly due to inadequate standardization and transparency in scientific reporting. While checklists like ARRIVE and CONSORT aim to improve these aspects, they are often underutilized, and peer review may not catch omissions. Automated tools have therefore been developed to help check for rigor and transparency.", "method": "The authors compared 11 different automated tools designed to assess 9 rigor criteria, as part of the ScreenIT group. This comparison evaluated individual and combined tool performance across these criteria.", "result": "For some criteria, such as open data detection, one tool outperformed the rest. For other criteria, like inclusion and exclusion criteria detection, combining multiple tools yielded better performance than any single tool. The study also highlighted areas where further tool development is needed.", "conclusion": "Key insights and recommendations are presented for those developing rigor and transparency detection tools. The results also emphasize the value of tool combinations and identify high-priority areas for improvement. All code and data from the study is openly available."}}
{"id": "2507.18029", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18029", "abs": "https://arxiv.org/abs/2507.18029", "authors": ["Xiang Echo Chen", "Wenhan Zhu", "Guoshuai Albert Shi", "Michael W. Godfrey"], "title": "An Empirical Study of GenAI Adoption in Open-Source Game Development: Tools, Tasks, and Developer Challenges", "comment": null, "summary": "The growing capabilities of generative AI (GenAI) have begun to reshape how\ngames are designed and developed, offering new tools for content creation,\ngameplay simulation, and design ideation. While prior research has explored\ntraditional uses of AI in games, such as controlling agents or generating\nprocedural content. There is limited empirical understanding of how GenAI is\nadopted by developers in real-world contexts, especially within the open-source\ncommunity. This study aims to explore how GenAI technologies are discussed,\nadopted, and integrated into open-source game development by analyzing issue\ndiscussions on GitHub. We investigate the tools, tasks, and challenges\nassociated with GenAI by comparing GenAI-related issues to those involving\ntraditional AI (TradAI) and NonAI topics. Our goal is to uncover how GenAI\ndiffers from other approaches in terms of usage patterns, developer concerns,\nand integration practices. To address this objective, we construct a dataset of\nopen-source game repositories that discuss AI-related topics. We apply open\ncard sorting and thematic analysis to a stratified sample of GitHub issues,\nlabelling each by type and content. These annotations enable comparative\nanalysis across GenAI, TradAI, and NonAI groups, and provide insight into how\nGenAI is shaping the workflows and pain points of open-source game developers.", "AI": {"tldr": "This paper studies how generative AI is being used in open-source game development by analyzing GitHub issue discussions, comparing it to traditional AI and non-AI topics to reveal differences in adoption, challenges, and integration practices.", "motivation": "Generative AI (GenAI) is transforming game design and development, but there is limited empirical research on how open-source game developers are actually adopting and integrating GenAI compared to traditional AI (TradAI) or NonAI approaches. The motivation is to fill this research gap by exploring real-world usage patterns, challenges, and integration practices of GenAI in open-source gaming.", "method": "The study constructs a dataset of open-source game repositories from GitHub discussing AI topics. It uses open card sorting and thematic analysis on a stratified sample of GitHub issues, labeling each by type and content to facilitate comparative analysis between GenAI, TradAI, and NonAI issues.", "result": "The comparative analysis provides insights into the tools, tasks, and challenges unique to GenAI versus TradAI and NonAI topics. The annotation and analysis offer an understanding of GenAI's impact on developer workflows and highlight specific pain points encountered by open-source game developers.", "conclusion": "GenAI is being distinctly adopted and integrated within open-source game development, with unique usage patterns, developer concerns, and integration methods compared to traditional AI and non-AI methods. Understanding these differences can inform better tool development and support for open-source game communities."}}
{"id": "2507.18037", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.18037", "abs": "https://arxiv.org/abs/2507.18037", "authors": ["Sivana Hamer", "Jacob Bowen", "Md Nazmul Haque", "Chris Madden", "Laurie Williams"], "title": "Your ATs to Ts: MITRE ATT&CK Attack Technique to P-SSCRM Task Mapping", "comment": "Mapping generated from: arXiv:2503.12192", "summary": "The MITRE Adversarial Tactics, Techniques and Common Knowledge (MITRE ATT&CK)\nAttack Technique to Proactive Software Supply Chain Risk Management Framework\n(P-SSCRM) Task mapping described in this document helps software organizations\nto determine how different tasks mitigate the attack techniques of software\nsupply chain attacks. The mapping was created through four independent\nstrategies to find agreed-upon mappings. Because each P-SSCRM task is mapped to\none or more tasks from the 10 frameworks, the mapping we provide is also a\nmapping between MITRE ATT&CK and other prominent government and industry\nframeworks.", "AI": {"tldr": "This paper presents a consensus-based mapping connecting MITRE ATT&CK supply chain attack techniques, P-SSCRM tasks, and ten major security frameworks, helping organizations strategically mitigate software supply chain risks.", "motivation": "Software supply chain attacks are a growing threat, and organizations need structured ways to mitigate these risks. The motivation behind this paper is to help software organizations understand how to use the MITRE ATT&CK framework in conjunction with other established frameworks for proactive supply chain risk management.", "method": "The paper utilizes four independent strategies to establish consensus-based mappings between the MITRE ATT&CK Software Supply Chain Attack Techniques and tasks from the Proactive Software Supply Chain Risk Management Framework (P-SSCRM). Each task was carefully analyzed and cross-referenced with ten different prominent frameworks.", "result": "The study produces a mapping between specific P-SSCRM tasks and MITRE ATT&CK attack techniques, as well as a broader mapping that connects MITRE ATT&CK to various other well-known government and industry frameworks. This aids organizations in aligning their security practices across multiple frameworks.", "conclusion": "The paper concludes that this mapping enables software organizations to better understand and mitigate software supply chain threats by leveraging consensus-based connections across several major security frameworks, thus enhancing their proactive risk management strategies."}}
{"id": "2507.18509", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.18509", "abs": "https://arxiv.org/abs/2507.18509", "authors": ["Henning Urbat"], "title": "Higher-Order Behavioural Conformances via Fibrations", "comment": null, "summary": "Coinduction is a widely used technique for establishing behavioural\nequivalence of programs in higher-order languages. In recent years, the rise of\nlanguages with quantitative (e.g.~probabilistic) features has led to extensions\nof coinductive methods to more refined types of behavioural conformances, most\nnotably notions of behavioural distance. To guarantee soundness of coinductive\nreasoning, one needs to show that the behavioural conformance at hand forms a\nprogram congruence, i.e. it is suitably compatible with the operations of the\nlanguage. This is usually achieved by a complex proof technique known as\n\\emph{Howe's method}, which needs to be carefully adapted to both the specific\nlanguage and the targeted notion of behavioural conformance. We develop a\nuniform categorical approach to Howe's method that features two orthogonal\ndimensions of abstraction: (1) the underlying higher-order language is modelled\nby an \\emph{abstract higher-order specification} (AHOS), a novel and very\ngeneral categorical account of operational semantics, and (2) notions of\nbehavioural conformance (such as relations or metrics) are modelled via\nfibrations over the base category of an AHOS. Our main result is a fundamental\ncongruence theorem at this level of generality: Under natural conditions on the\ncategorical ingredients and the operational rules of a language modelled by an\nAHOS, the greatest behavioural (bi)conformance on its operational model forms a\ncongruence. We illustrate our theory by deriving congruence of bisimilarity and\nbehavioural pseudometrics for probabilistic higher-order languages.", "AI": {"tldr": "The paper introduces a categorical and highly general framework for establishing program congruence in higher-order languages\u2014including quantitative (probabilistic) ones\u2014by generalizing Howe's method via abstract operational semantics and behavioral fibrations, with successful application to important examples like bisimilarity and behavioral metrics.", "motivation": "Coinductive reasoning is essential for verifying behavioral equivalence in higher-order programming languages. The rise of quantitative language features (e.g., probabilistic constructs) requires more sophisticated notions of behavioral conformance, such as behavioral distance. A critical challenge is ensuring that these conformances respect the structure of the language, demanding robust and adaptable proof techniques.", "method": "The authors propose a uniform, categorical approach to Howe's method using two abstraction layers: (1) model the higher-order language with Abstract Higher-Order Specifications (AHOS), providing a general categorical operational semantic framework; (2) represent behavioral conformances (relations or metrics) as fibrations over the AHOS's base category. They establish soundness through a general congruence theorem under natural conditions.", "result": "The main result is a fundamental congruence theorem: for any language modeled by AHOS and appropriate categorical conditions, the greatest behavioral (bi)conformance induces a program congruence. The framework unifies treatment across different languages and notions of conformance. The approach is validated by deriving congruence results for bisimilarity and behavioral pseudometrics in probabilistic higher-order languages.", "conclusion": "This work generalizes and abstracts Howe's method, allowing for broader and more uniform proofs of program congruence in both classical and quantitative higher-order languages. The categorical methodology enables systematic congruence theorems for various behavioral conformances."}}
{"id": "2507.18039", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18039", "abs": "https://arxiv.org/abs/2507.18039", "authors": ["Ahmad D. Suleiman", "Yiming Tang", "Daqing Hou"], "title": "Factors Impacting Faculty Adoption of Project-Based Learning in Computing Education: a Survey", "comment": "Accepted at IEEE Frontiers in Education (FIE) 2025. This work has\n  been submitted to the IEEE for possible publication", "summary": "This research full paper investigates the factors influencing computing\neducators' adoption of project-based learning (PjBL) in software engineering\nand computing curricula. Recognized as a student-centered pedagogical approach,\nPjBL has the potential to enhance student motivation, engagement, critical\nthinking, collaboration, and problem-solving skills. Despite these benefits,\nfaculty adoption remains inconsistent due to challenges such as insufficient\ninstitutional support, time constraints, limited training opportunities,\ndesigning or sourcing projects, and aligning them with course objectives. This\nresearch explores these barriers and investigates the strategies and resources\nthat facilitate a successful adoption. Using a mixed-methods approach, data\nfrom 80 computing faculty were collected through an online survey comprising\nclosed-ended questions to quantify barriers, enablers, and resource needs,\nalong with an open-ended question to gather qualitative insights. Quantitative\ndata were analyzed using statistical methods, while qualitative responses\nunderwent thematic analysis. Results reveal that while PjBL is widely valued,\nits adoption is often selective and impacted by challenges in planning and\nmanaging the learning process, designing suitable projects, and a lack of\ninstitutional support, such as time, funding, and teaching assistants. Faculty\nare more likely to adopt or sustain PjBL when they have access to peer\ncollaboration, professional development, and institutional incentives. In\naddition, sourcing projects from research, industry partnerships, and borrowing\nfrom peers emerged as key facilitators for new projects. These findings\nunderscore the need for systemic support structures to empower faculty to\nexperiment with and scale PjBL practices.", "AI": {"tldr": "Although project-based learning is seen as highly beneficial in computing education, faculty uptake is limited by support and resource barriers. This study finds that professional development, collaboration, and institutional incentives are key to broader adoption, underscoring the need for systemic changes to enable and encourage PjBL in curricula.", "motivation": "Project-based learning (PjBL) is known to enhance student outcomes such as motivation, engagement, and problem-solving skills in computing education. However, the adoption of PjBL by faculty remains inconsistent due to various practical barriers.", "method": "The study utilized a mixed-methods approach, collecting quantitative data through an online survey with closed-ended questions, and qualitative data via an open-ended question, from 80 computing faculty. Quantitative data was statistically analyzed and qualitative data underwent thematic analysis.", "result": "Findings indicate that while PjBL is highly valued, its adoption is inconsistent due to obstacles like insufficient institutional support, time, funding, lack of training, and challenges in project design. Faculty are more likely to adopt PjBL with peer collaboration, professional development, institutional incentives, and accessible project sources (from research, industry, and peers).", "conclusion": "To improve and scale PjBL adoption among computing educators, comprehensive systemic support, professional development opportunities, and better resource access are necessary. Institutional structures that promote collaboration and incentives can empower faculty to implement and sustain PjBL."}}
{"id": "2507.18062", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18062", "abs": "https://arxiv.org/abs/2507.18062", "authors": ["Edward Abrokwah", "Taher A. Ghaleb"], "title": "An Empirical Study of Complexity, Heterogeneity, and Compliance of GitHub Actions Workflows", "comment": "Registered Report Accepted at the 41st IEEE International Conference\n  on Software Maintenance and Evolution 2025 (ICSME'25)", "summary": "Continuous Integration (CI) has evolved from a tooling strategy to a\nfundamental mindset in modern CI engineering. It enables teams to develop,\ntest, and deliver software rapidly and collaboratively. Among CI services,\nGitHub Actions (GHA) has emerged as a dominant service due to its deep\nintegration with GitHub and a vast ecosystem of reusable workflow actions.\nAlthough GHA provides official documentation and community-supported best\npractices, there appears to be limited empirical understanding of how\nopen-source real-world CI workflows align with such practices. Many workflows\nmight be unnecessarily complex and not aligned with the simplicity goals of CI\npractices. This study will investigate the structure, complexity,\nheterogeneity, and compliance of GHA workflows in open-source software\nrepositories. Using a large dataset of GHA workflows from Java, Python, and C++\nrepositories, our goal is to (a) identify workflow complexities, (b) analyze\nrecurring and heterogeneous structuring patterns, (c) assess compliance with\nGHA best practices, and (d) uncover differences in CI pipeline design across\nprogramming languages. Our findings are expected to reveal both areas of strong\nadherence to best practices and areas for improvement where needed. These\ninsights will also have implications for CI services, as they will highlight\nthe need for clearer guidelines and comprehensive examples in CI documentation.", "AI": {"tldr": "This paper analyzes thousands of GitHub Actions workflows in open-source projects to assess complexity, structure, and compliance with best practices, finding both strong and weak points in current usage and recommending clearer guidelines for developers.", "motivation": "There is a lack of empirical understanding regarding how real-world open-source CI workflows, specifically using GitHub Actions (GHA), align with recommended best practices. Many existing workflows might be unnecessarily complex and fall short of CI simplicity goals.", "method": "The study analyzes a large dataset of GHA workflows from open-source repositories written in Java, Python, and C++. It examines the structure, complexity, heterogeneity, and compliance of these workflows with GHA best practices.", "result": "The analysis identifies workflow complexities, recurring and heterogeneous structuring patterns, evaluates compliance with best practices, and reveals differences in CI pipeline design across programming languages.", "conclusion": "There are both strong areas of adherence to GHA best practices and notable areas for improvement. The study suggests the need for clearer guidelines and more comprehensive documentation to enhance the quality and simplicity of CI workflows."}}
{"id": "2507.18081", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18081", "abs": "https://arxiv.org/abs/2507.18081", "authors": ["Carol Wong", "Mai Abe", "Silvia De Benedictis", "Marissa Halim", "Anthony Peruma"], "title": "Identifier Name Similarities: An Exploratory Study", "comment": "The 19th ACM/IEEE International Symposium on Empirical Software\n  Engineering and Measurement - Emerging Results and Vision Track", "summary": "Identifier names, which comprise a significant portion of the codebase, are\nthe cornerstone of effective program comprehension. However, research has shown\nthat poorly chosen names can significantly increase cognitive load and hinder\ncollaboration. Even names that appear readable in isolation may lead to\nmisunderstandings in contexts when they closely resemble other names in either\nstructure or functionality. In this exploratory study, we present our\npreliminary findings on the occurrence of identifier name similarity in\nsoftware projects through the development of a taxonomy that categorizes\ndifferent forms of identifier name similarity. We envision our initial taxonomy\nproviding researchers with a platform to analyze and evaluate the impact of\nidentifier name similarity on code comprehension, maintainability, and\ncollaboration among developers, while also allowing for further refinement and\nexpansion of the taxonomy.", "AI": {"tldr": "The paper introduces a preliminary taxonomy for categorizing types of identifier name similarity in software projects to support further research on code comprehension and collaboration.", "motivation": "Identifier names are critical for program comprehension, yet poorly chosen or highly similar identifier names can hinder understanding and collaboration. There is a need to systematically study and categorize the different types of identifier name similarity.", "method": "The study involves the development of a taxonomy categorizing various forms of identifier name similarity, supported by an exploratory analysis of their occurrence in software projects.", "result": "The researchers present a preliminary taxonomy that classifies different kinds of identifier name similarity observed in codebases.", "conclusion": "The initial taxonomy will help researchers analyze how identifier name similarity affects code comprehension, maintainability, and developer collaboration, while serving as a foundation for further refinement and study."}}
{"id": "2507.18105", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.18105", "abs": "https://arxiv.org/abs/2507.18105", "authors": ["Yujie Ma", "Lili Quan", "Xiaofei Xie", "Qiang Hu", "Jiongchi Yu", "Yao Zhang", "Sen Chen"], "title": "Understanding the Supply Chain and Risks of Large Language Model Applications", "comment": "26 pages", "summary": "The rise of Large Language Models (LLMs) has led to the widespread deployment\nof LLM-based systems across diverse domains. As these systems proliferate,\nunderstanding the risks associated with their complex supply chains is\nincreasingly important. LLM-based systems are not standalone as they rely on\ninterconnected supply chains involving pretrained models, third-party\nlibraries, datasets, and infrastructure. Yet, most risk assessments narrowly\nfocus on model or data level, overlooking broader supply chain vulnerabilities.\nWhile recent studies have begun to address LLM supply chain risks, there\nremains a lack of benchmarks for systematic research.\n  To address this gap, we introduce the first comprehensive dataset for\nanalyzing and benchmarking LLM supply chain security. We collect 3,859\nreal-world LLM applications and perform interdependency analysis, identifying\n109,211 models, 2,474 datasets, and 9,862 libraries. We extract model\nfine-tuning paths, dataset reuse, and library reliance, mapping the ecosystem's\nstructure. To evaluate security, we gather 1,555 risk-related issues-50 for\napplications, 325 for models, 18 for datasets, and 1,229 for libraries from\npublic vulnerability databases.\n  Using this dataset, we empirically analyze component dependencies and risks.\nOur findings reveal deeply nested dependencies in LLM applications and\nsignificant vulnerabilities across the supply chain, underscoring the need for\ncomprehensive security analysis. We conclude with practical recommendations to\nguide researchers and developers toward safer, more trustworthy LLM-enabled\nsystems.", "AI": {"tldr": "LLM systems have complex supply chains with significant, often-overlooked vulnerabilities. The authors provide a first-of-its-kind dataset to map these risks, revealing widespread dependencies and security issues. They offer recommendations for improving supply chain security in LLM applications.", "motivation": "The motivation of this paper is to address the growing risks associated with the complex supply chains of Large Language Model (LLM)-based systems, which rely on interconnected pretrained models, datasets, third-party libraries, and infrastructure. Existing risk assessments often neglect these broader supply chain vulnerabilities, creating a need for systematic benchmarks to evaluate and improve the security of LLM systems.", "method": "The authors introduce the first comprehensive dataset for analyzing and benchmarking LLM supply chain security. They collect 3,859 real-world LLM applications and perform interdependency analysis to identify models, datasets, and libraries involved. The study extracts model fine-tuning paths, dataset reuse, and library reliance, and gathers risk-related issues from public vulnerability databases for empirical analysis.", "result": "The empirical analysis shows that LLM applications have deeply nested dependencies and face significant vulnerabilities across their supply chain components, including models, datasets, and especially third-party libraries.", "conclusion": "The paper highlights the pervasive security risks present in LLM supply chains and emphasizes the importance of comprehensive security analysis. Practical recommendations are provided to guide researchers and developers toward building safer and more trustworthy LLM-enabled systems."}}
{"id": "2507.18130", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18130", "abs": "https://arxiv.org/abs/2507.18130", "authors": ["Le Deng", "Zhonghao Jiang", "Jialun Cao", "Michael Pradel", "Zhongxin Liu"], "title": "NoCode-bench: A Benchmark for Evaluating Natural Language-Driven Feature Addition", "comment": null, "summary": "Natural language-driven no-code development allows users to specify software\nfunctionality using natural language (NL) instead of editing source code,\npromising increased productivity and democratized development. Large language\nmodels (LLMs) show potential in enabling this paradigm. In this context,\nsoftware documentation acts as an NL specification for functionality. This work\nintroduces NoCode-bench, a benchmark designed to evaluate LLMs on real-world\nNL-driven feature addition tasks, consisting of 634 tasks across 10 projects\nand 114k code changes. Each task pairs documentation updates with corresponding\ncode implementations, validated by developer-written test cases. A subset of\n114 high-quality, human-verified instances, NoCode-bench Verified, ensures\nreliable evaluation. Our experiments reveal that, despite high token usage, the\nbest LLMs achieve a task success rate of only 15.79%, highlighting challenges\nin cross-file editing, codebase understanding, and tool calling. These findings\nindicate that LLMs are not yet ready for fully NL-driven no-code development.\nNoCode-bench lays the foundation for future advances in this area.", "AI": {"tldr": "LLMs struggle with real-world natural language-driven no-code development (15.79% success). NoCode-bench offers a standard to measure and improve on this task, revealing that major technical challenges remain.", "motivation": "Enable software development with natural language (no-code) specifications using powerful LLMs, aiming for higher productivity and wider accessibility. Assess LLMs' real capability to handle real-world NL-driven software feature additions, as documentation often serves as NL specifications.", "method": "The authors introduce NoCode-bench, a benchmark dataset featuring 634 tasks pulled from 10 real-world projects, linking natural language documentation updates with corresponding code implementations and validated by developer-written test cases. A smaller, highly curated subset (NoCode-bench Verified, 114 tasks) ensures reliable evaluation. Multiple LLMs are evaluated for their ability to edit codebases following NL specifications.", "result": "State-of-the-art LLMs could only achieve a task success rate of 15.79%, even with high token usage. The primary challenges identified: difficulties with cross-file editing, comprehensive codebase understanding, and correct tool invocation.", "conclusion": "Despite the promise of natural language-driven no-code development, current LLMs are not yet ready for such real-world tasks. NoCode-bench provides a foundation and benchmark for future improvements in NL-driven feature addition capabilities."}}
{"id": "2507.18159", "categories": ["cs.SE", "cs.DL"], "pdf": "https://arxiv.org/pdf/2507.18159", "abs": "https://arxiv.org/abs/2507.18159", "authors": ["Stephan Ferenz", "Aida Jafarbigloo", "Oliver Werth", "Astrid Nie\u00dfe"], "title": "SMECS: A Software Metadata Extraction and Curation Software", "comment": null, "summary": "Metadata play a crucial role in adopting the FAIR principles for research\nsoftware and enables findability and reusability. However, creating\nhigh-quality metadata can be resource-intensive for researchers and research\nsoftware engineers. To address this challenge, we developed the Software\nMetadata Extraction and Curation Software (SMECS) which integrates the\nextraction of metadata from existing sources together with a user-friendly\ninterface for metadata curation. SMECS extracts metadata from online\nrepositories such as GitHub and presents it to researchers through an\ninteractive interface for further curation and export as a CodeMeta file. The\nusability of SMECS was evaluated through usability experiments which confirmed\nthat SMECS provides a satisfactory user experience. SMECS supports the\nFAIRification of research software by simplifying metadata creation.", "AI": {"tldr": "SMECS automates and streamlines the creation of high-quality research software metadata, supporting FAIR principles and providing a user-friendly interface, as confirmed by usability testing.", "motivation": "Creating high-quality metadata is essential for implementing the FAIR (Findable, Accessible, Interoperable, Reusable) principles in research software. However, generating such metadata is often resource-intensive and challenging for researchers and software engineers.", "method": "The authors developed the Software Metadata Extraction and Curation Software (SMECS), which automatically extracts metadata from platforms like GitHub and offers an interactive, user-friendly interface for metadata curation and export as a CodeMeta file. They evaluated the tool through usability experiments.", "result": "Experiments showed that SMECS provides a satisfactory user experience in creating and curating research software metadata.", "conclusion": "SMECS facilitates the FAIRification process of research software by making metadata extraction and curation easier and more accessible for users."}}
{"id": "2507.18223", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18223", "abs": "https://arxiv.org/abs/2507.18223", "authors": ["Nenad Petrovic", "Fengjunjie Pan", "Vahid Zolfaghari", "Krzysztof Lebioda", "Andre Schamschurko", "Alois Knoll"], "title": "GenAI for Automotive Software Development: From Requirements to Wheels", "comment": null, "summary": "This paper introduces a GenAI-empowered approach to automated development of\nautomotive software, with emphasis on autonomous and Advanced Driver Assistance\nSystems (ADAS) capabilities. The process starts with requirements as input,\nwhile the main generated outputs are test scenario code for simulation\nenvironment, together with implementation of desired ADAS capabilities\ntargeting hardware platform of the vehicle connected to testbench. Moreover, we\nintroduce additional steps for requirements consistency checking leveraging\nModel-Driven Engineering (MDE). In the proposed workflow, Large Language Models\n(LLMs) are used for model-based summarization of requirements (Ecore metamodel,\nXMI model instance and OCL constraint creation), test scenario generation,\nsimulation code (Python) and target platform code generation (C++).\nAdditionally, Retrieval Augmented Generation (RAG) is adopted to enhance test\nscenario generation from autonomous driving regulations-related documents. Our\napproach aims shorter compliance and re-engineering cycles, as well as reduced\ndevelopment and testing time when it comes to ADAS-related capabilities.", "AI": {"tldr": "The paper presents a GenAI-powered method using LLMs and RAG to automate code and test scenario generation for ADAS, enabling consistent, regulation-aware, and efficient automotive software development.", "motivation": "Development for ADAS systems is complex, requires consistency, regulatory compliance, and fast iteration. Automating code and test scenario generation from requirements addresses the need for efficiency, reliability, and compliance in automotive software development.", "method": "The methodology incorporates Large Language Models for model-based summarization, code generation (Python and C++), and requirements validation. It also uses Retrieval Augmented Generation to enhance scenario generation from regulatory documents, all embedded in a workflow based on Model-Driven Engineering.", "result": "The approach demonstrates that requirements can be consistently checked and automatically translated into simulation scenarios and hardware-targeted implementation, potentially achieving shorter development and testing times for ADAS.", "conclusion": "The paper concludes that integrating GenAI, LLMs, and RAG into the software development workflow of automotive systems can streamline code and test generation, enhance requirement consistency, and potentially shorten development, compliance, and re-engineering cycles for ADAS capabilities."}}
{"id": "2507.18267", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18267", "abs": "https://arxiv.org/abs/2507.18267", "authors": ["Zeqin Liao", "Zibin Zheng", "Peifan Reng", "Henglong Liang", "Zixu Gao", "Zhixiang Chen", "Wei Li", "Yuhong Nan"], "title": "An Empirical Study on Embodied Artificial Intelligence Robot (EAIR) Software Bugs", "comment": null, "summary": "Embodied Artificial Intelligence Robots (EAIR) is an emerging and rapidly\nevolving technological domain. Ensuring their program correctness is\nfundamental to their successful deployment. However, a general and in-depth\nunderstanding of EAIR system bugs remains lacking, which hinders the\ndevelopment of practices and techniques to tackle EAIR system bugs.\n  To bridge this gap, we conducted the first systematic study of 885 EAIR\nsystem bugs collected from 80 EAIR system projects to investigate their\nsymptoms, underlying causes, and module distribution. Our analysis takes\nconsiderable effort, which classifies these bugs into 18 underlying causes, 15\ndistinct symptoms, and identifies 13 affected modules. It reveals several new\ninteresting findings and implications which help shed light on future research\non tackling or repairing EAIR system bugs. First, among the 15 identified\nsymptoms, our findings highlight 8 symptoms specific to EAIR systems, which is\ncharacterized by severe functional failures and potential physical hazards.\nSecond, within the 18 underlying causes, we define 8 EAIR-specific causes, the\nmajority of which stem from the intricate issues of AI- agent reasoning and\ndecision making. Finally, to facilitate precise and efficient bug prediction,\ndetection, and repair, we constructed a mapping between underlying causes and\nthe modules in which they most frequently occur, which enables researchers to\nfocus diagnostic efforts on the modules most susceptible to specific bug types.", "AI": {"tldr": "The paper systematically analyzes 885 bugs in Embodied AI Robots, identifying symptoms, causes, and affected modules. It reveals several EAIR-specific issues, especially linked to AI-reasoning, and provides a mapping to help researchers diagnose and repair such bugs efficiently.", "motivation": "There is a lack of in-depth understanding of bugs in Embodied Artificial Intelligence Robots (EAIR) systems, which impedes the development of effective practices and tools for bug diagnosis and repair in this rapidly evolving domain.", "method": "The authors conducted a systematic empirical study by collecting and manually analyzing 885 EAIR system bugs from 80 different EAIR projects. They classified the bugs according to their symptoms, underlying causes, and associated modules.", "result": "The study classified EAIR bugs into 18 causes, 15 symptoms, and 13 modules. It discovered 8 symptoms and 8 underlying causes unique to EAIR, mostly related to AI-agent reasoning and decision making, and found these symptoms are characterized by severe functional failures and potential hazards. The mapping of causes to modules enables more targeted bug diagnostics.", "conclusion": "This study provides the first comprehensive taxonomy and mapping of EAIR bugs, symptoms, and causes, identifying several EAIR-specific aspects. The findings can inform more precise bug prediction, detection, and repair, and help focus future research and engineering efforts on the most vulnerable modules and causes in EAIR systems."}}
{"id": "2507.18289", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.18289", "abs": "https://arxiv.org/abs/2507.18289", "authors": ["Yan Li", "Wenzhang Yang", "Yuekun Wang", "Jian Gao", "Shaohua Wang", "Yinxing Xue", "Lijun Zhang"], "title": "Scheduzz: Constraint-based Fuzz Driver Generation with Dual Scheduling", "comment": "15 pages, 12 figures, 5 tables", "summary": "Fuzzing a library requires experts to understand the library usage well and\ncraft high-quality fuzz drivers, which is tricky and tedious. Therefore, many\ntechniques have been proposed to automatically generate fuzz drivers. However,\nthey fail to generate rational fuzz drivers due to the lack of adherence to\nproper library usage conventions, such as ensuring a resource is closed after\nbeing opened. To make things worse, existing library fuzzing techniques\nunconditionally execute each driver, resulting in numerous irrational drivers\nthat waste computational resources while contributing little coverage and\ngenerating false positive bug reports.\n  To tackle these challenges, we propose a novel automatic library fuzzing\ntechnique, Scheduzz, an LLM-based library fuzzing technique. It leverages LLMs\nto understand rational usage of libraries and extract API combination\nconstraints. To optimize computational resource utilization, a dual scheduling\nframework is implemented to efficiently manage API combinations and fuzz\ndrivers. The framework models driver generation and the corresponding fuzzing\ncampaign as an online optimization problem. Within the scheduling loop,\nmultiple API combinations are selected to generate fuzz drivers, while\nsimultaneously, various optimized fuzz drivers are scheduled for execution or\nsuspension.\n  We implemented Scheduzz and evaluated it in 33 real-world libraries. Compared\nto baseline approaches, Scheduzz significantly reduces computational overhead\nand outperforms UTopia on 16 out of 21 libraries. It achieves 1.62x, 1.50x, and\n1.89x higher overall coverage than the state-of-the-art techniques CKGFuzzer,\nPromptfuzz, and the handcrafted project OSS-Fuzz, respectively. In addition,\nScheduzz discovered 33 previously unknown bugs in these well-tested libraries,\n3 of which have been assigned CVEs.", "AI": {"tldr": "Scheduzz is an LLM-driven library fuzzing tool that intelligently generates and schedules fuzz drivers based on real usage patterns, significantly improving efficiency and bug detection over existing solutions.", "motivation": "Fuzzing libraries is currently a manual and expertise-driven process, requiring extensive knowledge to create effective fuzz drivers. Existing methods for automatic driver generation often produce irrational drivers and waste computational resources, as they fail to adhere to proper usage conventions and execute all generated drivers indiscriminately.", "method": "The authors propose Scheduzz, an LLM-based fuzzing tool that uses large language models to infer rational API usage patterns and generate appropriate fuzz drivers. Scheduzz utilizes a dual scheduling framework that treats driver generation and fuzzing as an online optimization problem, selectively generating and executing drivers to maximize resource efficiency and coverage.", "result": "Scheduzz was evaluated on 33 real-world libraries, where it reduced computational overhead compared to existing methods and achieved higher code coverage (1.62x, 1.50x, and 1.89x more than CKGFuzzer, Promptfuzz, and OSS-Fuzz respectively). It also discovered 33 previously unknown bugs, including 3 that were assigned security CVEs.", "conclusion": "Scheduzz effectively addresses the limitations of prior automatic fuzz driver generation techniques by using LLMs to understand and adhere to library usage conventions and by employing an efficient driver execution framework. It demonstrates superior coverage and bug-finding abilities while reducing wasted computational resources."}}
{"id": "2507.18316", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18316", "abs": "https://arxiv.org/abs/2507.18316", "authors": ["Michael Konstantinou", "Renzo Degiovanni", "Jie M. Zhang", "Mark Harman", "Mike Papadakis"], "title": "YATE: The Role of Test Repair in LLM-Based Unit Test Generation", "comment": "12 pages, 4 figures", "summary": "Recent advances in automated test generation utilises language models to\nproduce unit tests. While effective, language models tend to generate many\nincorrect tests with respect to both syntax and semantics. Although such\nincorrect tests can be easily detected and discarded, they constitute a \"missed\nopportunity\" -- if fixed, they are often valuable as they directly add testing\nvalue (they effectively target the underlying program logic to be tested) and\nindirectly form good seeds for generating additional tests. To this end, we\npropose a simple technique for repairing some of these incorrect tests through\na combination of rule-based static analysis and re-prompting. We evaluate this\nsimple approach, named YATE, on a set of 6 open-source projects and show that\nit can effectively produce tests that cover on average 32.06% more lines and\nkill 21.77% more mutants than a plain LLM-based method. We also compare YATE\nwith four other LLM-based methods, namely HITS, SYMPROMPT, TESTSPARK and\nCOVERUP and show that it produces tests that cover substantially more code.\nYATE achieves 22% higher line coverage, 20% higher branch coverage and kill 20%\nmore mutants at a comparable cost (number of calls to LLMs).", "AI": {"tldr": "The paper presents YATE, a technique to repair incorrect tests generated by language models using static analysis and re-prompting, resulting in significantly improved code coverage and fault detection over existing methods, at similar computational cost.", "motivation": "Automated test generation using language models produces many incorrect tests, which are often easily detected and discarded. However, if these tests are repaired, they can provide significant testing value and serve as useful seeds for further test generation. Hence, leveraging and fixing these incorrect tests would enhance the overall effectiveness of automated test generation.", "method": "The paper proposes a technique called YATE that combines rule-based static analysis and re-prompting to repair some of the incorrect tests generated by language models. The method is evaluated on six open-source projects.", "result": "YATE can generate tests that achieve, on average, 32.06% more line coverage and kill 21.77% more mutants than standard LLM-based methods. When compared to other LLM-based test generation methods (HITS, SYMPROMPT, TESTSPARK, and COVERUP), YATE achieves 22% higher line coverage, 20% higher branch coverage, and kills 20% more mutants, with a similar cost in the number of LLM calls.", "conclusion": "Repairing incorrect unit tests generated by language models is a promising approach. The proposed technique YATE effectively improves test coverage and fault detection compared to existing LLM-based test generation techniques, providing more value at a comparable computational cost."}}
{"id": "2507.18319", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18319", "abs": "https://arxiv.org/abs/2507.18319", "authors": ["Jesse Maarleveld", "Jiapan Guo", "Daniel Feitosa"], "title": "Gotta catch 'em all! Towards File Localisation from Issues at Large", "comment": "12 pages, 6 figures", "summary": "Bug localisation, the study of developing methods to localise the files\nrequiring changes to resolve bugs, has been researched for a long time to\ndevelop methods capable of saving developers' time. Recently, researchers are\nstarting to consider issues outside of bugs. Nevertheless, most existing\nresearch into file localisation from issues focusses on bugs or uses other\nselection methods to ensure only certain types of issues are considered as part\nof the focus of the work. Our goal is to work on all issues at large, without\nany specific selection.\n  In this work, we provide a data pipeline for the creation of issue file\nlocalisation datasets, capable of dealing with arbitrary branching and merging\npractices. We provide a baseline performance evaluation for the file\nlocalisation problem using traditional information retrieval approaches.\nFinally, we use statistical analysis to investigate the influence of biases\nknown in the bug localisation community on our dataset.\n  Our results show that methods designed using bug-specific heuristics perform\npoorly on general issue types, indicating a need for research into general\npurpose models. Furthermore, we find that there are small, but statistically\nsignificant differences in performance between different issue types. Finally,\nwe find that the presence of identifiers have a small effect on performance for\nmost issue types. Many results are project-dependent, encouraging the\ndevelopment of methods which can be tuned to project-specific characteristics.", "AI": {"tldr": "The paper presents a flexible dataset creation pipeline for issue file localisation and shows that bug-focussed methods are inadequate for general issue types. Differences in localisation performance exist across issue types and projects, indicating the need for adaptable models.", "motivation": "There is a growing need to localize files for all issue types\u2014not just bugs\u2014in software projects, as traditional research has narrowly focused on bug reports. The motivation is to address the gap by creating approaches and datasets that apply broadly to diverse issue types to better aid developer productivity.", "method": "The authors developed a data pipeline to produce issue file localisation datasets robust to varying code branching and merging practices. They conducted baseline evaluations using traditional information retrieval techniques and performed statistical analyses to examine biases and the effects of specific factors on performance.", "result": "Methods reliant on bug-specific heuristics were found to perform poorly on broader issue types. There are small but statistically significant variations in localisation performance depending on the issue type, and the presence of identifiers only minimally affects most types. Results also showed significant project-dependent variation, suggesting the need for project-specific approaches.", "conclusion": "Existing bug-localisation approaches do not generalize well to all issue types. There is a need for new, general-purpose or project-tunable models to improve file localisation across various issue categories."}}
{"id": "2507.18339", "categories": ["cs.SE", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.18339", "abs": "https://arxiv.org/abs/2507.18339", "authors": ["Nils Bosbach", "Meik Schmidt", "Lukas J\u00fcnger", "Matthias Berthold", "Rainer Leupers"], "title": "FMI Meets SystemC: A Framework for Cross-Tool Virtual Prototyping", "comment": "PREPRINT - accepted by the 16th International Modelica and FMI\n  Conference 2025", "summary": "As systems become more complex, the demand for thorough testing and virtual\nprototyping grows. To simulate whole systems, multiple tools are usually needed\nto cover different parts. These parts include the hardware of a system and the\nenvironment with which the system interacts. The Functional Mock-up Interface\n(FMI) standard for co-simulation can be used to connect these tools.\n  The control part of modern systems is usually a computing unit, such as a\nSystem-on-a-Chip (SoC) or Microcontroller Unit (MCU), which executes software\nfrom a connected memory and interacts with peripherals. To develop software\nwithout requiring access to physical hardware, full-system simulators, the\nso-called Virtual Platforms (VPs), are commonly used. The IEEE-standardized\nframework for VP development is SystemC TLM. SystemC provides interfaces and\nconcepts that enable modular design and model exchange. However, SystemC lacks\nnative FMI support, which limits the integration into broader co-simulation\nenvironments.\n  This paper presents a novel framework to control and interact with\nSystemC-based VPs using the FMI. We present a case study showing how a\nsimulated temperature sensor in a SystemC simulation can obtain temperature\nvalues from an external tool via FMI. This approach allows the unmodified\ntarget software to run on the VP and receive realistic environmental input data\nsuch as temperature, velocity, or acceleration values from other tools. Thus,\nextensive software testing and verification is enabled. By having tests ready\nand the software pre-tested using a VP once the physical hardware is available,\ncertifications like ISO 26262 can be done earlier.", "AI": {"tldr": "The paper introduces a framework that merges SystemC virtual platforms with FMI-based simulation tools, allowing realistic environmental data input for software testing. This enables more comprehensive and earlier verification, benefiting certification processes such as ISO 26262.", "motivation": "As system complexity increases, thorough testing and prototyping require integrating multiple simulation tools, especially to verify interactions between hardware and their operating environments. SystemC is a key technology for virtual platforms but lacks easy co-simulation with other tools due to missing FMI support. There is a need to bridge this gap for improved testing, verification, and accelerated certification processes.", "method": "The paper proposes a novel framework that enables SystemC-based virtual platforms (VPs) to communicate with other simulation tools using the Functional Mock-up Interface (FMI) standard. This is demonstrated through a case study where a SystemC simulation of a temperature sensor obtains real-time environmental data from an external FMI-compliant tool.", "result": "The framework successfully allows the unmodified target software in the SystemC VP to receive realistic input from external tools, facilitating more extensive and accurate software testing and verification. The case study shows practical integration between SystemC and FMI, enabling a simulated system to interact with realistic environmental inputs like temperature, velocity, or acceleration.", "conclusion": "By linking SystemC-based VPs with external simulation tools via the FMI standard, this framework enables more robust and realistic virtual prototyping. This advancement allows early, thorough software validation and testing, which expedites processes such as ISO 26262 certification, as tests and verifications can be completed before physical hardware becomes available."}}
{"id": "2507.18476", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18476", "abs": "https://arxiv.org/abs/2507.18476", "authors": ["Busra Icoz", "Goksel Biricik"], "title": "Automated Code Review Using Large Language Models with Symbolic Reasoning", "comment": null, "summary": "Code review is one of the key processes in the software development lifecycle\nand is essential to maintain code quality. However, manual code review is\nsubjective and time consuming. Given its rule-based nature, code review is well\nsuited for automation. In recent years, significant efforts have been made to\nautomate this process with the help of artificial intelligence. Recent\ndevelopments in Large Language Models (LLMs) have also emerged as a promising\ntool in this area, but these models often lack the logical reasoning\ncapabilities needed to fully understand and evaluate code. To overcome this\nlimitation, this study proposes a hybrid approach that integrates symbolic\nreasoning techniques with LLMs to automate the code review process. We tested\nour approach using the CodexGlue dataset, comparing several models, including\nCodeT5, CodeBERT, and GraphCodeBERT, to assess the effectiveness of combining\nsymbolic reasoning and prompting techniques with LLMs. Our results show that\nthis approach improves the accuracy and efficiency of automated code review.", "AI": {"tldr": "The paper introduces a hybrid method using symbolic reasoning and LLMs for automated code review, showing improved accuracy and efficiency over LLMs alone through evaluation on the CodexGlue dataset.", "motivation": "Manual code review is essential but subjective and time-consuming, making it a prime candidate for automation. Although LLMs have promise for automating code review, they lack strong logical reasoning abilities.", "method": "The study proposes a hybrid approach that integrates symbolic reasoning techniques with Large Language Models (LLMs) to automate code reviews. The approach was tested using the CodexGlue dataset and compared models such as CodeT5, CodeBERT, and GraphCodeBERT with and without symbolic reasoning enhancements.", "result": "The hybrid approach, combining symbolic reasoning and LLM prompting, led to improved accuracy and efficiency in automated code review compared to using LLMs alone.", "conclusion": "Integrating symbolic reasoning with LLM-based approaches enhances the effectiveness of automated code review, addressing the limitations of LLMs in logical reasoning and evaluation tasks."}}
{"id": "2507.18515", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18515", "abs": "https://arxiv.org/abs/2507.18515", "authors": ["Zezhou Yang", "Ting Peng", "Cuiyun Gao", "Chaozheng Wang", "Hailiang Huang", "Yuetang Deng"], "title": "A Deep Dive into Retrieval-Augmented Generation for Code Completion: Experience on WeChat", "comment": "Accepted in ICSME 25 Industry Track", "summary": "Code completion, a crucial task in software engineering that enhances\ndeveloper productivity, has seen substantial improvements with the rapid\nadvancement of large language models (LLMs). In recent years,\nretrieval-augmented generation (RAG) has emerged as a promising method to\nenhance the code completion capabilities of LLMs, which leverages relevant\ncontext from codebases without requiring model retraining. While existing\nstudies have demonstrated the effectiveness of RAG on public repositories and\nbenchmarks, the potential distribution shift between open-source and\nclosed-source codebases presents unique challenges that remain unexplored. To\nmitigate the gap, we conduct an empirical study to investigate the performance\nof widely-used RAG methods for code completion in the industrial-scale codebase\nof WeChat, one of the largest proprietary software systems. Specifically, we\nextensively explore two main types of RAG methods, namely identifier-based RAG\nand similarity-based RAG, across 26 open-source LLMs ranging from 0.5B to 671B\nparameters. For a more comprehensive analysis, we employ different retrieval\ntechniques for similarity-based RAG, including lexical and semantic retrieval.\nBased on 1,669 internal repositories, we achieve several key findings: (1) both\nRAG methods demonstrate effectiveness in closed-source repositories, with\nsimilarity-based RAG showing superior performance, (2) the effectiveness of\nsimilarity-based RAG improves with more advanced retrieval techniques, where\nBM25 (lexical retrieval) and GTE-Qwen (semantic retrieval) achieve superior\nperformance, and (3) the combination of lexical and semantic retrieval\ntechniques yields optimal results, demonstrating complementary strengths.\nFurthermore, we conduct a developer survey to validate the practical utility of\nRAG methods in real-world development environments.", "AI": {"tldr": "This paper investigates RAG methods for code completion on WeChat\u2019s massive closed-source codebase. It finds that similarity-based RAG\u2014especially with combined lexical and semantic retrieval\u2014significantly boosts LLM code completion performance in industrial settings, validated by both quantitative results and developer feedback.", "motivation": "The motivation is to address a gap in knowledge: while retrieval-augmented generation (RAG) methods have shown promise in improving code completion with LLMs on open-source code, their performance on closed-source, industrial-scale codebases remains unexplored. Since closed-source environments may differ from open-source ones, understanding RAG's effectiveness there is important for real-world applications.", "method": "The authors perform an empirical study using WeChat\u2019s large, proprietary codebase. They examine two types of RAG methods for code completion: identifier-based and similarity-based RAG. They test 26 open-source LLMs (ranging from 0.5B to 671B parameters) and explore different retrieval strategies, including lexical (BM25) and semantic (GTE-Qwen) approaches. The study also includes a developer survey for real-world validation.", "result": "Both RAG methods work effectively in the closed-source codebase, with similarity-based RAG outperforming identifier-based RAG. Advanced retrieval methods further enhance similarity-based RAG, with BM25 (lexical) and GTE-Qwen (semantic) being particularly effective. Combining lexical and semantic retrieval provides the best results. Developer feedback validates the utility of RAG in practice.", "conclusion": "RAG methods, particularly those combining advanced similarity-based techniques, significantly improve code completion in industrial, closed-source settings and provide real-world value, bridging the gap between public benchmarks and proprietary software contexts."}}
