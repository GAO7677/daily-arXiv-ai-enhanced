<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Evaluating the Limitations of Local LLMs in Solving Complex Programming Challenges](https://arxiv.org/abs/2509.15283)
*Kadin Matotek,Heather Cassel,Md Amiruzzaman,Linh B. Ngo*

Main category: cs.SE

TL;DR: The paper presents an improved, offline benchmarking framework for code-generation LLMs and finds that locally hosted open-source models currently lag well behind proprietary solutions but are improving, with practical evaluation now more accessible on in-house hardware.


<details>
  <summary>Details</summary>
Motivation: To investigate if modern open-source, locally hosted LLMs are capable of solving complex competitive programming tasks when compared to proprietary models, especially under realistic problem settings and with a more robust, offline evaluation pipeline.

Method: The authors retrofit the FACE code-generation evaluation pipeline to operate entirely offline, simplifying data management and introducing checkpointing for stability. They apply this system to evaluate eight open-source LLMs (6.7-9B parameters) by generating and submitting solutions to 3,589 programming problems from Kattis, comparing results with proprietary models like Gemini 1.5 and ChatGPT-4.

Result: Open-source local models achieve only about half the pass@1 accuracy of proprietary models on complex programming challenges. However, the results highlight the steady progress of open models and demonstrate the feasibility of robust, fully offline benchmarking on local infrastructure.

Conclusion: There remains a significant performance gap between local open-source LLMs and leading proprietary ones in solving advanced programming problems. Nevertheless, the improved evaluation framework enables reproducible, practical benchmarking on private hardware—supporting further research and real-world adoption.

Abstract: This study examines the performance of today's open-source, locally hosted
large-language models (LLMs) in handling complex competitive programming tasks
with extended problem descriptions and contexts. Building on the original
Framework for AI-driven Code Generation Evaluation (FACE), the authors retrofit
the pipeline to work entirely offline through the Ollama runtime, collapsing
FACE's sprawling per-problem directory tree into a handful of consolidated JSON
files, and adding robust checkpointing so multi-day runs can resume after
failures. The enhanced framework generates, submits, and records solutions for
the full Kattis corpus of 3,589 problems across eight code-oriented models
ranging from 6.7-9 billion parameters. The submission results show that the
overall pass@1 accuracy is modest for the local models, with the best models
performing at approximately half the acceptance rate of the proprietary models,
Gemini 1.5 and ChatGPT-4. These findings expose a persistent gap between
private, cost-controlled LLM deployments and state-of-the-art proprietary
services, yet also highlight the rapid progress of open models and the
practical benefits of an evaluation workflow that organizations can replicate
on in-house hardware.

</details>


### [2] [LoCaL: Countering Surface Bias in Code Evaluation Metrics](https://arxiv.org/abs/2509.15397)
*Simantika Bhattacharjee Dristi,Matthew B. Dwyer*

Main category: cs.SE

TL;DR: Current code evaluation metrics for LLMs are biased toward superficial code similarity and struggle to gauge true functional equivalence. The LoCaL benchmark exposes these weaknesses, showing that all evaluated metrics perform poorly when surface and functional similarities diverge. Using such challenging datasets can guide the creation of more reliable metrics focused on functionality, not just appearance.


<details>
  <summary>Details</summary>
Motivation: Current code evaluation metrics (CEMs) used for assessing code generated by large language models are often biased towards surface-level similarity rather than true functional similarity. Additionally, existing benchmarks do not adequately challenge CEMs with functionally distinct but surface-similar code, leaving their weaknesses untested and unresolved.

Method: The authors critically evaluated four state-of-the-art reference-based CEMs by designing a new benchmark, LoCaL (Looks Can Lie), containing 3117 code pairs at both the method and program levels. Each code pair in the dataset received a functional similarity score, determined with differential fuzzing rather than conventional test cases (which are expensive to create), allowing for more comprehensive and reliable testing.

Result: All four state-of-the-art CEMs exhibited significant performance declines on the LoCaL benchmark, indicating their strong surface bias and lack of robustness when distinguishing between functionally similar and dissimilar code pairs, especially when surface features are misleading.

Conclusion: The study demonstrates that current reference-based CEMs are not reliable indicators of functional similarity. The authors suggest that exposure to LoCaL-like data, which includes challenging code pairs, may help in the development of more robust and functionally aware code evaluation metrics.

Abstract: With the increasing popularity of large language models (LLMs) and LLM-based
agents, reliable and effective code evaluation metrics (CEMs) have become
crucial for progress across several software engineering tasks. While popular
benchmarks often provide test cases to assess the correctness of generated
code, crafting and executing test cases is expensive. Reference-based CEMs
provide a cheaper alternative by scoring a candidate program based on its
functional similarity to a reference. Although prior research has focused on
reporting the weak correlation between these CEMs and functional correctness,
the causes are only assumed, and plausible solutions remain unexplored. In this
work, we critically evaluate four state-of-the-art reference-based CEMs,
revealing their strong bias towards surface-level features rather than code
functionality. Despite this surface bias, current evaluation datasets for these
CEMs rarely include code pairs that are surface-similar yet functionally
dissimilar, or functionally similar yet surface-dissimilar. To mitigate this
gap, we propose LoCaL (Looks Can Lie), a CEM evaluation benchmark, with 3117
code pairs at both the method and program levels. Each pair is labeled with a
functional similarity score and aims to target regions where CEMs are likely to
perform poorly. The functional similarity scores are calculated through
differential fuzzing, which eliminates the need for predefined test cases and,
at the same time, improves the reliability of the scores by executing an order
of magnitude more tests than prior work. We find that all four CEMs show
significant performance degradation on LoCaL, compared to the baselines.
Finally, based on our findings, we draw the implication that exposing CEMs to
LoCaL-like data might facilitate the development of metrics that are robust to
surface bias.

</details>


### [3] [Brevity is the Soul of Wit: Condensing Code Changes to Improve Commit Message Generation](https://arxiv.org/abs/2509.15567)
*Hongyu Kuang,Ning Zhang,Hui Gao,Xin Zhou,Wesley K. G. Assunção,Xiaoxing Ma,Dong Shao,Guoping Rong,He Zhang*

Main category: cs.SE

TL;DR: The paper introduces a template-based method for generating commit messages using CodeLlama-7B, outperforming current state-of-the-art methods in both automated metrics and human evaluation.


<details>
  <summary>Details</summary>
Motivation: Developers often fail to write high-quality commit messages, making it hard for others to understand code changes and maintain software. Current automatic generation methods focus on code representation but overlook the potential of concise templates.

Method: The authors propose text templates to structure code changes into three parts: summarized code changes, elicited comments, and emphasized code identifiers. They use a heuristic tool, ChangeScribe, to create template pairs with commit messages and fine-tune CodeLlama-7B on these pairs.

Result: Their approach, leveraging these structured templates, significantly outperforms six baseline models in BLEU-Norm, METEOR, and ROUGE-L by 51.7%, 78.7%, and 62.5% on average, respectively. Additional ablation studies and human evaluation confirm the method's effectiveness.

Conclusion: The proposed template-based approach for condensing code changes enhances automated commit message generation. It results in more readable and informative messages, showing notable improvements over existing baselines and strong empirical performance.

Abstract: Commit messages are valuable resources for describing why code changes are
committed to repositories in version control systems (e.g., Git). They
effectively help developers understand code changes and better perform software
maintenance tasks. Unfortunately, developers often neglect to write
high-quality commit messages in practice. Therefore, a growing body of work is
proposed to generate commit messages automatically. These works all
demonstrated that how to organize and represent code changes is vital in
generating good commit messages, including the use of fine-grained graphs or
embeddings to better represent code changes. In this study, we choose an
alternative way to condense code changes before generation, i.e., proposing
brief yet concise text templates consisting of the following three parts: (1)
summarized code changes, (2) elicited comments, and (3) emphasized code
identifiers. Specifically, we first condense code changes by using our proposed
templates with the help of a heuristic-based tool named ChangeScribe, and then
fine-tune CodeLlama-7B on the pairs of our proposed templates and corresponding
commit messages. Our proposed templates better utilize pre-trained language
models, while being naturally brief and readable to complement generated commit
messages for developers. Our evaluation based on a widely used dataset showed
that our approach can outperform six baselines in terms of BLEU-Norm, METEOR,
and ROUGE-L, with average improvements of 51.7%, 78.7%, and 62.5%,
respectively. The ablation study and human evaluation also provide further
insights into the effectiveness of our approach.

</details>


### [4] [How Far Are We? An Empirical Analysis of Current Vulnerability Localization Approaches](https://arxiv.org/abs/2509.15777)
*Haoran Xu,Zhi Chen,Junxiao Han,Xinkui Zhao,Jianwei Yin,Shuiguang Deng*

Main category: cs.SE

TL;DR: Traditional vulnerability patch detection methods do not scale and are inaccurate. This paper introduces a two-stage framework using version filtering and language models, notably improving patch detection in extensive experiments.


<details>
  <summary>Details</summary>
Motivation: Manual detection of vulnerability patches in open-source software is inefficient, error-prone, and cannot scale to large codebases. Existing automated methods have poor accuracy and generalization, making them impractical for real-world use.

Method: The paper presents a comprehensive empirical study on current vulnerability patch detection methods, identifying key challenges and success factors. It then proposes a novel two-stage framework: (1) version-driven candidate filtering to reduce the search space, and (2) large language model-based multi-round dialogue voting to select accurate patches.

Result: Experiments conducted on a dataset of 750 real vulnerabilities show that the proposed method significantly improves detection accuracy and efficiency over existing approaches.

Conclusion: Leveraging search space reduction and large language model-based dialogue yields a more effective and scalable solution for vulnerability patch detection in open-source software.

Abstract: Open-source software vulnerability patch detection is a critical component
for maintaining software security and ensuring software supply chain integrity.
Traditional manual detection methods face significant scalability challenges
when processing large volumes of commit histories, while being prone to human
errors and omissions. Existing automated approaches, including heuristic-based
methods and pre-trained model solutions, suffer from limited accuracy, poor
generalization capabilities, and inherent methodological constraints that
hinder their practical deployment. To address these fundamental challenges,
this paper conducts a comprehensive empirical study of existing vulnerability
patch detection methods, revealing four key insights that guide the design of
effective solutions: the critical impact of search space reduction, the
superiority of pre-trained semantic understanding over architectural
complexity, the temporal limitations of web crawling approaches, and the
advantages of knowledge-driven methods. Based on these insights, we propose a
novel two-stage framework that combines version-driven candidate filtering with
large language model-based multi-round dialogue voting to achieve accurate and
efficient vulnerability patch identification. Extensive experiments on a
dataset containing 750 real vulnerabilities demonstrate that our method
outperforms current approaches.

</details>


### [5] [Failure Modes and Effects Analysis: An Experience from the E-Bike Domain](https://arxiv.org/abs/2509.15893)
*Andrea Bombarda,Federico Conti,Marcello Minervini,Aurora Zanenga,Claudio Menghi*

Main category: cs.SE

TL;DR: This paper evaluates simulation-driven FMEA for safety analysis of e-Bike CPS using Simulink Fault Analyzer. Modeling 13 faults and incorporating expert feedback showed that simulation helped uncover unexpected fault effects and improved model quality, suggesting strong industrial relevance for similar tools and approaches.


<details>
  <summary>Details</summary>
Motivation: Software failures in Cyber-Physical Systems (CPS) can lead to severe consequences, necessitating effective methods to identify and analyze such failures. The motivation is to provide practical evidence of the value of simulation-driven Functional Failure Mode and Effects Analysis (FMEA) for industrial adoption, specifically within the e-Bike domain using commercial tools.

Method: The authors performed FMEA on an industrial CPS (e-Bike domain) using Simulink Fault Analyzer. They identified and modeled 13 realistic faults in the system, then analyzed their effects through simulation. Feedback from domain experts was sought to evaluate model accuracy and effectiveness in detecting safety issues.

Result: The study found that the fault models were mostly accurate, with only minor corrections required after expert feedback. In 38.4% of the cases, simulation-driven FMEA revealed fault effects that did not match engineers’ expectations, thus highlighting unexpected safety issues. The process helped improve model quality and provided valuable lessons for practitioners.

Conclusion: Simulation-driven support for FMEA, using tools like Simulink Fault Analyzer, is beneficial for safety analysis in CPS. It improves model accuracy, helps engineers identify unexpected faults, and increases confidence in safety assessments. The paper delivers actionable lessons for engineers and safety analysts working with similar approaches.

Abstract: Software failures can have catastrophic and costly consequences. Functional
Failure Mode and Effects Analysis (FMEA) is a standard technique used within
Cyber-Physical Systems (CPS) to identify software failures and assess their
consequences. Simulation-driven approaches have recently been shown to be
effective in supporting FMEA. However, industries need evidence of the
effectiveness of these approaches to increase practical adoption. This
industrial paper presents our experience with using FMEA to analyze the safety
of a CPS from the e-Bike domain. We used Simulink Fault Analyzer, an industrial
tool that supports engineers with FMEA. We identified 13 realistic faults,
modeled them, and analyzed their effects. We sought expert feedback to analyze
the appropriateness of our models and the effectiveness of the faults in
detecting safety breaches. Our results reveal that for the faults we
identified, our models were accurate or contained minor imprecision that we
subsequently corrected. They also confirm that FMEA helps engineers improve
their models. Specifically, the output provided by the simulation-driven
support for 38.4% (5 out of 13) of the faults did not match the engineers'
expectations, helping them discover unexpected effects of the faults. We
present a thorough discussion of our results and ten lessons learned. Our
findings are useful for software engineers who work as Simulink engineers, use
the Simulink Fault Analyzer, or work as safety analysts.

</details>


### [6] [LeakageDetector 2.0: Analyzing Data Leakage in Jupyter-Driven Machine Learning Pipelines](https://arxiv.org/abs/2509.15971)
*Owen Truong,Terrence Zhang,Arnav Marchareddy,Ryan Lee,Jeffery Busold,Michael Socas,Eman Abdullah AlOmar*

Main category: cs.SE

TL;DR: The paper introduces LeakageDetector, a VS Code extension that automatically spots and helps fix data leakage issues in ML Jupyter Notebook code, using both manual and AI-guided methods to improve code quality and reliability.


<details>
  <summary>Details</summary>
Motivation: Data Leakage is a common and serious issue in machine learning model development, leading to inaccurate performance evaluations. Many ML engineers may unknowingly introduce leakage due to improper data separation, especially while using interactive tools like Jupyter Notebooks.

Method: This paper develops a Visual Studio Code extension named LeakageDetector, capable of detecting various types of Data Leakage (Overlap, Preprocessing, and Multi-test) in Jupyter Notebook files. The extension offers two correction mechanisms: a quick fix that manually repairs detected leakage, and an LLM-guided approach that advises ML developers on best practices.

Result: The authors built and integrated LeakageDetector into the VS Code environment, enabling automatic detection and correction (either manual or LLM-assisted) of data leakage issues in ML code within Jupyter Notebooks.

Conclusion: LeakageDetector supports ML engineers by detecting and helping correct Data Leakage in their code, thus promoting more reliable evaluation and development of machine learning models.

Abstract: In software development environments, code quality is crucial. This study
aims to assist Machine Learning (ML) engineers in enhancing their code by
identifying and correcting Data Leakage issues within their models. Data
Leakage occurs when information from the test dataset is inadvertently included
in the training data when preparing a data science model, resulting in
misleading performance evaluations. ML developers must carefully separate their
data into training, evaluation, and test sets to avoid introducing Data Leakage
into their code. In this paper, we develop a new Visual Studio Code (VS Code)
extension, called LeakageDetector, that detects Data Leakage, mainly Overlap,
Preprocessing and Multi-test leakage, from Jupyter Notebook files. Beyond
detection, we included two correction mechanisms: a conventional approach,
known as a quick fix, which manually fixes the leakage, and an LLM-driven
approach that guides ML developers toward best practices for building ML
pipelines.

</details>


### [7] [Software Development Aspects of Integrating Linear Algebra Libraries](https://arxiv.org/abs/2509.16081)
*Marcel Koch,Tobias Ribizel,Pratik Nayak,Fritz Göbel,Gregor Olenik,Terry Cojean*

Main category: cs.SE

TL;DR: The paper examines the integration of the Ginkgo numerical linear algebra library into various simulation applications, highlighting its performance benefits, software engineering challenges, and its support for sustainable development practices.


<details>
  <summary>Details</summary>
Motivation: Application scientists often rely on simulation software that integrates external libraries for specific functionalities, such as numerical linear algebra, but these libraries are frequently from domains outside their expertise. This creates challenges in software integration, performance, and sustainability.

Method: The paper analyzes the adoption of the Ginkgo library—a platform-independent sparse numerical linear algebra package—across various application domains. It presents real-world examples in CFD, power grid simulation, and electro-cardiophysiology. The analysis addresses software engineering implications and software integration strategies.

Result: The integration of Ginkgo into scientific applications allowed for faster numerical computations and easier adaptation to modern computing systems. The analysis revealed both challenges and benefits, particularly with respect to sustainable software development and the engineering approaches used by both Ginkgo and the application developers.

Conclusion: Ginkgo serves as a valuable tool for application software, improving performance and maintainability by providing efficient linear algebra routines. The software’s engineering practices and integration strategies support sustainable development across different scientific domains.

Abstract: Many scientific discoveries are made through, or aided by, the use of
simulation software. These sophisticated software applications are not built
from the ground up, instead they rely on smaller parts for specific use cases,
usually from domains unfamiliar to the application scientists. The software
library Ginkgo is one of these building blocks to handle sparse numerical
linear algebra on different platforms. By using Ginkgo, applications are able
to ease the transition to modern systems, and speed up their simulations
through faster numerical linear algebra routines. This paper discusses the
challenges and benefits for application software in adopting Ginkgo. It will
present examples from different domains, such as CFD, power grid simulation, as
well as electro-cardiophysiology. For these cases, the impact of the
integrations on the application code is discussed from a software engineering
standpoint, and in particular, the approaches taken by Ginkgo and the
applications to enable sustainable software development are highlighted.

</details>


### [8] [When Bugs Linger: A Study of Anomalous Resolution Time Outliers and Their Themes](https://arxiv.org/abs/2509.16140)
*Avinash Patil*

Main category: cs.SE

TL;DR: The paper analyzes bug resolution anomalies in major open-source projects using statistical and text analysis techniques, revealing that long-standing bugs often cluster around recurring themes like test failures and UI issues, helping maintainers target common underlying problems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to help software projects maintain quality and user satisfaction by understanding why certain bug reports take longer to resolve, potentially due to process inefficiencies or complex underlying problems.

Method: The research uses statistical methods (Z-score and IQR) to detect anomalies in bug resolution durations. It then applies TF-IDF for feature extraction from bug report text, followed by KMeans clustering to group bug reports with similar themes.

Result: The results show that anomalies in bug resolution times consistently appear across different projects and are thematically grouped, making persistent problems easier to identify and prioritize.

Conclusion: The study concludes that anomalies in bug resolution times are commonly linked to specific recurring themes like test failures, enhancement requests, and UI issues. Identifying and analyzing these patterns offers actionable advice for maintainers to efficiently target and resolve persistent bugs.

Abstract: Efficient bug resolution is critical for maintaining software quality and
user satisfaction. However, specific bug reports experience unusually long
resolution times, which may indicate underlying process inefficiencies or
complex issues. This study presents a comprehensive analysis of bug resolution
anomalies across seven prominent open-source repositories: Cassandra, Firefox,
Hadoop, HBase, SeaMonkey, Spark, and Thunderbird. Utilizing statistical methods
such as Z-score and Interquartile Range (IQR), we identify anomalies in bug
resolution durations. To understand the thematic nature of these anomalies, we
apply Term Frequency-Inverse Document Frequency (TF-IDF) for textual feature
extraction and KMeans clustering to group similar bug summaries. Our findings
reveal consistent patterns across projects, with anomalies often clustering
around test failures, enhancement requests, and user interface issues. This
approach provides actionable insights for project maintainers to prioritize and
effectively address long-standing bugs.

</details>


### [9] [MatchFixAgent: Language-Agnostic Autonomous Repository-Level Code Translation Validation and Repair](https://arxiv.org/abs/2509.16187)
*Ali Reza Ibrahimzada,Brandon Paulsen,Reyhaneh Jabbarvand,Joey Dodds,Daniel Kroening*

Main category: cs.SE

TL;DR: MatchFixAgent uses a language-model-powered, multi-agent system to thoroughly validate and repair code translation between different programming languages, achieving higher accuracy and broader applicability than past solutions.


<details>
  <summary>Details</summary>
Motivation: Code translation between programming languages requires reliable validation of functional equivalence and effective repair of incorrect translations, but current automated approaches are difficult to generalize and rely on inadequate test suites.

Method: The authors introduce MatchFixAgent, a programming language-agnostic framework based on large language models with a multi-agent architecture. The system divides equivalence validation into sub-tasks, generates and executes tests, repairs failed translations, and delivers a final verdict using collaborative agents.

Result: MatchFixAgent achieved (in)equivalence verdicts for 99.2% of translation pairs, matched previous validation results in 72.8% of cases, and when differing, was found correct 60.7% of the time. It repaired 50.6% of inequivalent translations, outperforming the prior approach's 18.5%.

Conclusion: MatchFixAgent is highly adaptable across many programming language pairs, offering superior and accurate functional equivalence validation and repair outcomes compared to previous methods.

Abstract: Code translation transforms source code from one programming language (PL) to
another. Validating the functional equivalence of translation and repairing, if
necessary, are critical steps in code translation. Existing automated
validation and repair approaches struggle to generalize to many PLs due to high
engineering overhead, and they rely on existing and often inadequate test
suites, which results in false claims of equivalence and ineffective
translation repair. We develop MatchFixAgent, a large language model
(LLM)-based, PL-agnostic framework for equivalence validation and repair of
translations. MatchFixAgent features a multi-agent architecture that divides
equivalence validation into several sub-tasks to ensure thorough and consistent
semantic analysis of the translation. Then it feeds this analysis to test agent
to write and execute tests. Upon observing a test failure, the repair agent
attempts to fix the translation bug. The final (in)equivalence decision is made
by the verdict agent, considering semantic analyses and test execution results.
  We compare MatchFixAgent's validation and repair results with four
repository-level code translation techniques. We use 2,219 translation pairs
from their artifacts, which cover 6 PL pairs, and are collected from 24 GitHub
projects totaling over 900K lines of code. Our results demonstrate that
MatchFixAgent produces (in)equivalence verdicts for 99.2% of translation pairs,
with the same equivalence validation result as prior work on 72.8% of them.
When MatchFixAgent's result disagrees with prior work, we find that 60.7% of
the time MatchFixAgent's result is actually correct. In addition, we show that
MatchFixAgent can repair 50.6% of inequivalent translation, compared to prior
work's 18.5%. This demonstrates that MatchFixAgent is far more adaptable to
many PL pairs than prior work, while producing highly accurate validation
results.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [10] [Automatic layout of railroad diagrams](https://arxiv.org/abs/2509.15834)
*Shardul Chiplunkar,Clément Pit-Claudel*

Main category: cs.PL

TL;DR: This paper introduces a formal layout model and compiler for railroad diagrams, enabling automatic, high-quality visualization of grammars and making these diagrams more practical and standardized than traditional, hand-drawn versions.


<details>
  <summary>Details</summary>
Motivation: Railroad diagrams are an intuitive way to visualize grammars, but currently, they are mainly hand-drawn due to insufficient tooling and lack of formal study on their layout. This limits their practicality and broader adoption.

Method: The paper provides the first formal framework for the layout of railroad diagrams. It conceptualizes the design process as a compilation from a high-level diagram language to a low-level layout language. The authors built a compiler that introduces features like line wrapping (to fit a target width), alignment, and justification, guided by user preferences. Line wrapping is treated as an optimization problem, with defined optimality criteria and implemented heuristics. The approach is evaluated on both the front-end (showing suitability for describing regular expressions and Backus-Naur form) and the back-end (comparing generated diagrams to manually created and tool-generated diagrams).

Result: The results show that the proposed diagram language effectively models typical use cases, such as regular expressions and Backus-Naur form. The compiler creates layouts comparable in quality to those produced by hand and alternative tools, demonstrating practicality and effectiveness.

Conclusion: The paper establishes a formal foundation for railroad diagram layout and provides a practical implementation, advancing the state of tooling and usability for grammar visualization.

Abstract: Railroad diagrams (also called "syntax diagrams") are a common, intuitive
visualization of grammars, but limited tooling and a lack of formal attention
to their layout mostly confines them to hand-drawn documentation. We present
the first formal treatment of railroad diagram layout along with a principled,
practical implementation. We characterize the problem as compiling a *diagram
language* (specifying conceptual components and how they connect and compose)
to a *layout language* (specifying basic graphical shapes and their sizes and
positions). We then implement a compiler that performs *line wrapping* to meet
a target width, as well as vertical *alignment* and horizontal *justification*
per user-specified policies. We frame line wrapping as an optimization problem,
where we describe principled dimensions of optimality and implement
corresponding heuristics. For front-end evaluation, we show that our diagram
language is well-suited for common applications by describing how regular
expressions and Backus-Naur form can be compiled to it. For back-end
evaluation, we argue that our compiler is practical by comparing its output to
diagrams laid out by hand and by other tools.

</details>
