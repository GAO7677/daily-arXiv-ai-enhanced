{"id": "2508.08322", "categories": ["cs.SE", "cs.AI", "68T07, 68N01", "D.2.2; I.2.6; D.2.5; I.2.8"], "pdf": "https://arxiv.org/pdf/2508.08322", "abs": "https://arxiv.org/abs/2508.08322", "authors": ["Muhammad Haseeb"], "title": "Context Engineering for Multi-Agent LLM Code Assistants Using Elicit, NotebookLM, ChatGPT, and Claude Code", "comment": "15 pages, 5 figures, research paper on multi-agent LLM systems for\n  code generation", "summary": "Large Language Models (LLMs) have shown promise in automating code generation\nand software engineering tasks, yet they often struggle with complex,\nmulti-file projects due to context limitations and knowledge gaps. We propose a\nnovel context engineering workflow that combines multiple AI components: an\nIntent Translator (GPT-5) for clarifying user requirements, an Elicit-powered\nsemantic literature retrieval for injecting domain knowledge, NotebookLM-based\ndocument synthesis for contextual understanding, and a Claude Code multi-agent\nsystem for code generation and validation. Our integrated approach leverages\nintent clarification, retrieval-augmented generation, and specialized\nsub-agents orchestrated via Claude's agent framework. We demonstrate that this\nmethod significantly improves the accuracy and reliability of code assistants\nin real-world repositories, yielding higher single-shot success rates and\nbetter adherence to project context than baseline single-agent approaches.\nQualitative results on a large Next.js codebase show the multi-agent system\neffectively plans, edits, and tests complex features with minimal human\nintervention. We compare our system with recent frameworks like CodePlan,\nMASAI, and HyperAgent, highlighting how targeted context injection and agent\nrole decomposition lead to state-of-the-art performance. Finally, we discuss\nthe implications for deploying LLM-based coding assistants in production, along\nwith lessons learned on context management and future research directions.", "AI": {"tldr": "This paper proposes a multi-agent workflow for LLM-based code generation, leveraging intent clarification, semantic retrieval, and agent orchestration to address context limitations. The system markedly improves code assistant performance on complex software projects, surpassing existing frameworks and offering practical guidance for production deployment.", "motivation": "LLMs struggle with complex, multi-file software projects due to context limitations and knowledge gaps. This limits their practical usefulness for real-world code generation and engineering tasks.", "method": "The paper proposes a workflow that integrates several specialized AI components: a GPT-5 Intent Translator for requirement clarification, Elicit-derived semantic retrieval for domain knowledge, NotebookLM for document synthesis, and a Claude Code multi-agent system for code generation and validation. These components are orchestrated via Claude's agent framework using intent clarification, context injection, and agent role decomposition.", "result": "Experiments on a large Next.js codebase show the multi-agent system can accurately plan, edit, and test complex features, requiring minimal human input. The approach demonstrates higher single-shot success rates and better context adherence compared to single-agent baselines, and outperforms recent systems like CodePlan, MASAI, and HyperAgent.", "conclusion": "Combining multiple specialized AI agents for context engineering in code generation leads to significant improvements in the reliability and accuracy of LLM-based coding assistants. The system sets new standards for single-shot code generation in complex projects, providing practical insights for future deployment of LLM assistants."}}
{"id": "2508.08332", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08332", "abs": "https://arxiv.org/abs/2508.08332", "authors": ["Humza Ashraf", "Syed Muhammad Danish", "Aris Leivadeas", "Yazan Otoum", "Zeeshan Sattar"], "title": "Energy-Aware Code Generation with LLMs: Benchmarking Small vs. Large Language Models for Sustainable AI Programming", "comment": null, "summary": "Large Language Models (LLMs) are widely used for code generation. However,\ncommercial models like ChatGPT require significant computing power, which leads\nto high energy use and carbon emissions. This has raised concerns about their\nenvironmental impact. In this study, we evaluate open-source Small Language\nModels (SLMs) trained explicitly for code generation and compare their\nperformance and energy efficiency against large LLMs and efficient\nhuman-written Python code. The goal is to investigate whether SLMs can match\nthe performance of LLMs on certain types of programming problems while\nproducing more energy-efficient code. We evaluate 150 coding problems from\nLeetCode, evenly distributed across three difficulty levels: easy, medium, and\nhard. Our comparison includes three small open-source models, StableCode-3B,\nStarCoderBase-3B, and Qwen2.5-Coder-3B-Instruct, and two large commercial\nmodels, GPT-4.0 and DeepSeek-Reasoner. The generated code is evaluated using\nfour key metrics: run-time, memory usage, energy consumption, and correctness.\nWe use human-written solutions as a baseline to assess the quality and\nefficiency of the model-generated code. Results indicate that LLMs achieve the\nhighest correctness across all difficulty levels, but SLMs are often more\nenergy-efficient when their outputs are correct. In over 52% of the evaluated\nproblems, SLMs consumed the same or less energy than LLMs.", "AI": {"tldr": "SLMs are generally more energy-efficient than LLMs for code generation when outputs are correct, though LLMs are more accurate overall.", "motivation": "Large Language Models (LLMs) such as ChatGPT are popular for code generation but are resource-intensive, leading to high energy consumption and environmental concerns. The motivation behind this paper is to explore whether smaller, open-source language models (SLMs) can perform code generation tasks effectively while being more energy-efficient, thus reducing the environmental impact.", "method": "The authors evaluated three open-source SLMs and two large commercial LLMs on 150 LeetCode programming problems of varying difficulty. They tested StableCode-3B, StarCoderBase-3B, and Qwen2.5-Coder-3B-Instruct (SLMs) versus GPT-4.0 and DeepSeek-Reasoner (LLMs). The generated Python code was compared to human-written solutions using four metrics: run-time, memory usage, energy consumption, and correctness.", "result": "LLMs achieved the highest correctness across all problem difficulties. However, SLMs often generated more energy-efficient code, and in over 52% of the problems, the SLMs used the same or less energy than LLMs when producing correct solutions.", "conclusion": "Open-source SLMs can generate programming solutions to LeetCode problems with competitive energy efficiency compared to large commercial LLMs. While LLMs remain more accurate, SLMs offer a viable trade-off when energy consumption is a priority and correctness can be maintained."}}
{"id": "2508.08342", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.08342", "abs": "https://arxiv.org/abs/2508.08342", "authors": ["Maximilian Jungwirth", "Martin Gruber", "Gordon Fraser"], "title": "Improving Merge Pipeline Throughput in Continuous Integration via Pull Request Prioritization", "comment": "This paper is accepted on the Industry Track of the 41st\n  International Conference on Software Maintenance and Evolution (ICSME 2025)", "summary": "Integrating changes into large monolithic software repositories is a critical\nstep in modern software development that substantially impacts the speed of\nfeature delivery, the stability of the codebase, and the overall productivity\nof development teams. To ensure the stability of the main branch, many\norganizations use merge pipelines that test software versions before the\nchanges are permanently integrated. However, the load on merge pipelines is\noften so high that they become bottlenecks, despite the use of parallelization.\nExisting optimizations frequently rely on specific build systems, limiting\ntheir generalizability and applicability. In this paper we propose to optimize\nthe order of PRs in merge pipelines using practical build predictions utilizing\nonly historical build data, PR metadata, and contextual information to estimate\nthe likelihood of successful builds in the merge pipeline. By dynamically\nprioritizing likely passing PRs during peak hours, this approach maximizes\nthroughput when it matters most. Experiments conducted on a real-world,\nlarge-scale project demonstrate that predictive ordering significantly\noutperforms traditional first-in-first-out (FIFO), as well as\nnon-learning-based ordering strategies. Unlike alternative optimizations, this\napproach is agnostic to the underlying build system and thus easily integrable\ninto existing automated merge pipelines.", "AI": {"tldr": "This paper introduces a predictive, build-system-independent approach to prioritize pull requests in software merge pipelines. By using historical and contextual data, it improves throughput over naive ordering strategies, providing scalable and easily integrated optimization for large-scale development teams.", "motivation": "Merge pipelines in large software repositories are essential for maintaining code stability but can become bottlenecks during high load, slowing down feature delivery and reducing development productivity. Existing optimizations are often tied to specific build systems, limiting their broader applicability.", "method": "The paper proposes optimizing the order of pull requests (PRs) in merge pipelines using predictions based on historical build data, PR metadata, and contextual information. It prioritizes PRs likely to pass builds during peak hours, aiming for maximum throughput. The method is build-system agnostic and relies on practical data-driven predictions.", "result": "Experiments on a real-world, large-scale project show that the predictive PR ordering method significantly outperforms FIFO and non-learning ordering strategies, resulting in higher throughput and easier integration into existing systems due to build-system independence.", "conclusion": "Predictive ordering of PRs in merge pipelines, using practical build data and metadata, improves throughput and stability without dependence on specific build systems, making it an effective and easily adoptable optimization for organizations."}}
{"id": "2508.08545", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08545", "abs": "https://arxiv.org/abs/2508.08545", "authors": ["Youssef Esseddiq Ouatiti", "Mohammed Sayagh", "Bram Adams", "Ahmed E. Hassan"], "title": "OmniLLP: Enhancing LLM-based Log Level Prediction with Context-Aware Retrieval", "comment": null, "summary": "Developers insert logging statements in source code to capture relevant\nruntime information essential for maintenance and debugging activities. Log\nlevel choice is an integral, yet tricky part of the logging activity as it\ncontrols log verbosity and therefore influences systems' observability and\nperformance. Recent advances in ML-based log level prediction have leveraged\nlarge language models (LLMs) to propose log level predictors (LLPs) that\ndemonstrated promising performance improvements (AUC between 0.64 and 0.8).\nNevertheless, current LLM-based LLPs rely on randomly selected in-context\nexamples, overlooking the structure and the diverse logging practices within\nmodern software projects. In this paper, we propose OmniLLP, a novel LLP\nenhancement framework that clusters source files based on (1) semantic\nsimilarity reflecting the code's functional purpose, and (2) developer\nownership cohesion. By retrieving in-context learning examples exclusively from\nthese semantic and ownership aware clusters, we aim to provide more coherent\nprompts to LLPs leveraging LLMs, thereby improving their predictive accuracy.\nOur results show that both semantic and ownership-aware clusterings\nstatistically significantly improve the accuracy (by up to 8\\% AUC) of the\nevaluated LLM-based LLPs compared to random predictors (i.e., leveraging\nrandomly selected in-context examples from the whole project). Additionally,\nour approach that combines the semantic and ownership signal for in-context\nprediction achieves an impressive 0.88 to 0.96 AUC across our evaluated\nprojects. Our findings highlight the value of integrating software\nengineering-specific context, such as code semantic and developer ownership\nsignals into LLM-LLPs, offering developers a more accurate, contextually-aware\napproach to logging and therefore, enhancing system maintainability and\nobservability.", "AI": {"tldr": "OmniLLP improves LLM-based log level prediction by pulling learning examples from clusters based on code semantics and developer ownership, boosting accuracy by up to 8% AUC and providing better, context-aware logging recommendations.", "motivation": "Log level choices in logging statements greatly impact software observability and performance, but are difficult to automate accurately. Recent ML approaches use large language models but ignore code structure and developer practices, limiting their effectiveness.", "method": "OmniLLP clusters source files by semantic similarity and developer ownership. In-context examples for LLM-based log level predictors are then selected from these clusters to improve the coherence and relevance of prompts, thus enhancing prediction accuracy.", "result": "Semantic and ownership-aware clustering improves predictor accuracy by up to 8% AUC compared to random in-context examples. Combining both signals achieves between 0.88 and 0.96 AUC, outperforming previous methods on multiple projects.", "conclusion": "Incorporating code semantic and developer ownership context into LLM-based log level prediction provides significantly more accurate and contextually relevant predictions, improving software maintainability and observability."}}
{"id": "2508.08661", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08661", "abs": "https://arxiv.org/abs/2508.08661", "authors": ["Chunhua Liu", "Hong Yi Lin", "Patanamon Thongtanunam"], "title": "Hallucinations in Code Change to Natural Language Generation: Prevalence and Evaluation of Detection Metrics", "comment": "8 main pages, 5 figures", "summary": "Language models have shown strong capabilities across a wide range of tasks\nin software engineering, such as code generation, yet they suffer from\nhallucinations. While hallucinations have been studied independently in natural\nlanguage and code generation, their occurrence in tasks involving code changes\nwhich have a structurally complex and context-dependent format of code remains\nlargely unexplored. This paper presents the first comprehensive analysis of\nhallucinations in two critical tasks involving code change to natural language\ngeneration: commit message generation and code review comment generation. We\nquantify the prevalence of hallucinations in recent language models and explore\na range of metric-based approaches to automatically detect them. Our findings\nreveal that approximately 50\\% of generated code reviews and 20\\% of generated\ncommit messages contain hallucinations. Whilst commonly used metrics are weak\ndetectors on their own, combining multiple metrics substantially improves\nperformance. Notably, model confidence and feature attribution metrics\neffectively contribute to hallucination detection, showing promise for\ninference-time detection.\\footnote{All code and data will be released upon\nacceptance.", "AI": {"tldr": "The paper uncovers widespread hallucinations in LLM-generated code reviews and commit messages (up to 50%), and finds that combining multiple detection metrics\u2014especially model confidence and feature attribution\u2014significantly improves hallucination detection rates.", "motivation": "While language models have demonstrated strong performance in software engineering tasks like code generation, their tendency to generate 'hallucinations'\u2014i.e., incorrect or fabricated information\u2014remains a significant problem. Specifically, there has been little exploration of hallucinations in code change tasks where the input-output structures are complex and context-dependent.", "method": "This paper conducts the first in-depth analysis of hallucinations in two important code change tasks: commit message generation and code review comment generation. The authors quantify hallucination prevalence in outputs from recent language models and evaluate various metric-based approaches to automatically detect these hallucinations. They test individual metrics and combinations, and pay particular attention to model confidence and feature attribution.", "result": "The study reveals that about 50% of generated code reviews and 20% of commit messages contain hallucinations. Individually, common detection metrics are not very effective, but combining metrics\u2014especially utilizing model confidence and feature attribution\u2014significantly enhances hallucination detection.", "conclusion": "Hallucinations are highly prevalent in code-change-related natural language tasks performed by large language models. Automatic detection is challenging with standalone metrics, but approaches that combine multiple indicators, particularly model confidence and feature attribution, offer a promising path to effective, inference-time hallucination identification."}}
{"id": "2508.08868", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.08868", "abs": "https://arxiv.org/abs/2508.08868", "authors": ["Henning Femmer", "Frank Houdek", "Max Unterbusch", "Andreas Vogelsang"], "title": "Description and Comparative Analysis of QuRE: A New Industrial Requirements Quality Dataset", "comment": null, "summary": "Requirements quality is central to successful software and systems\nengineering. Empirical research on quality defects in natural language\nrequirements relies heavily on datasets, ideally as realistic and\nrepresentative as possible. However, such datasets are often inaccessible,\nsmall, or lack sufficient detail. This paper introduces QuRE (Quality in\nRequirements), a new dataset comprising 2,111 industrial requirements that have\nbeen annotated through a real-world review process. Previously used for over\nfive years as part of an industrial contract, this dataset is now being\nreleased to the research community. In this work, we furthermore provide\ndescriptive statistics on the dataset, including measures such as lexical\ndiversity and readability, and compare it to existing requirements datasets and\nsynthetically generated requirements. In contrast to synthetic datasets, QuRE\nis linguistically similar to existing ones. However, this dataset comes with a\ndetailed context description, and its labels have been created and used\nsystematically and extensively in an industrial context over a period of close\nto a decade. Our goal is to foster transparency, comparability, and empirical\nrigor by supporting the development of a common gold standard for requirements\nquality datasets. This, in turn, will enable more sound and collaborative\nresearch efforts in the field.", "AI": {"tldr": "The paper introduces and releases QuRE, a large, annotated industrial requirements dataset, addressing the lack of realistic and accessible data in requirements quality research. QuRE is detailed, systematically labeled, and comparable to existing datasets, making it a strong candidate for a community gold standard to advance empirical studies and collaboration in the field.", "motivation": "Empirical research on requirements quality defects is hampered by the lack of large, detailed, accessible, and realistic datasets. Existing datasets are often small, inaccessible, or lack vital context, which reduces the effectiveness and generalizability of research.", "method": "The authors introduce QuRE, a new dataset of 2,111 industrial requirements annotated through a real-world review process during five years of industrial use. They release this dataset to the research community and provide descriptive statistics, such as lexical diversity and readability, comparing QuRE to both existing and synthetic datasets.", "result": "QuRE is linguistically similar to other existing datasets, but stands out through its detailed context information and systematically created labels from industrial practice. It presents more realistic and representative requirements data than synthetic datasets, with high-quality annotations developed over nearly a decade.", "conclusion": "QuRE aims to serve as a common gold standard for requirements quality datasets, fostering transparency, comparability, and empirical rigor, and enabling more sound and collaborative research efforts in software requirements quality."}}
{"id": "2508.08872", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2508.08872", "abs": "https://arxiv.org/abs/2508.08872", "authors": ["Dylan Callaghan", "Alexandra van der Spuy", "Bernd Fischer"], "title": "Empirical Analysis of Temporal and Spatial Fault Characteristics in Multi-Fault Bug Repositories", "comment": null, "summary": "Fixing software faults contributes significantly to the cost of software\nmaintenance and evolution. Techniques for reducing these costs require datasets\nof software faults, as well as an understanding of the faults, for optimal\ntesting and evaluation. In this paper, we present an empirical analysis of the\ntemporal and spatial characteristics of faults existing in 16 open-source Java\nand Python projects, which form part of the Defects4J and BugsInPy datasets,\nrespectively. Our findings show that many faults in these software systems are\nlong-lived, leading to the majority of software versions having multiple\ncoexisting faults. This is in contrast to the assumptions of the original\ndatasets, where the majority of versions only identify a single fault. In\naddition, we show that although the faults are found in only a small subset of\nthe systems, these faults are often evenly distributed amongst this subset,\nleading to relatively few bug hotspots.", "AI": {"tldr": "This paper analyzes how software faults persist and cluster in real-world projects, revealing that multiple, long-lived faults often coexist and are spread out, challenging the assumptions made by popular datasets. This highlights the need to adjust current approaches for software testing and maintenance.", "motivation": "Software maintenance is expensive largely due to the cost of fixing software faults. Existing techniques for reducing these costs require accurate datasets and an understanding of software fault characteristics, yet assumptions about these faults may not hold in real-world projects.", "method": "The paper empirically analyzes the temporal (time-based) and spatial (location-based) characteristics of software faults in 16 open-source Java and Python projects, using the Defects4J and BugsInPy datasets.", "result": "The study finds that many faults are long-lived and that most software versions have multiple coexisting faults, which contrasts with original dataset assumptions of primarily single-fault versions. Furthermore, faults are present in only a small subset of the system but are fairly evenly distributed among these parts, resulting in few concentrated bug hotspots.", "conclusion": "The results suggest that existing assumptions about software faults in popular datasets underestimate both the longevity and the frequency of coexisting faults, indicating a need to revise current methodologies and datasets for more effective testing and evaluation."}}
{"id": "2508.08952", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.08952", "abs": "https://arxiv.org/abs/2508.08952", "authors": ["Hyunwoo Kim", "Jaeseong Lee", "Sunpyo Hong", "Changmin Han"], "title": "Toward Automated Hypervisor Scenario Generation Based on VM Workload Profiling for Resource-Constrained Environments", "comment": null, "summary": "In the automotive industry, the rise of software-defined vehicles (SDVs) has\n  driven a shift toward virtualization-based architectures that consolidate\n  diverse automotive workloads on a shared hardware platform. To support this\n  evolution, chipset vendors provide board support packages (BSPs), hypervisor\n  setups, and resource allocation guidelines. However, adapting these static\n  configurations to varying system requirements and workloads remain a\n  significant challenge for Tier 1 integrators.\n  This paper presents an automated scenario generation framework, which helps\n  automotive vendors to allocate hardware resources efficiently across multiple\n  VMs. By profiling runtime behavior and integrating both theoretical models\nand\n  vendor heuristics, the proposed tool generates optimized hypervisor\n  configurations tailored to system constraints.\n  We compare two main approaches for modeling target QoS based on profiled data\n  and resource allocation: domain-guided parametric modeling and deep\n  learning-based modeling. We further describe our optimization strategy using\n  the selected QoS model to derive efficient resource allocations. Finally, we\n  report on real-world deployments to demonstrate the effectiveness of our\n  framework in improving integration efficiency and reducing development time\nin\n  resource-constrained environments.", "AI": {"tldr": "This paper presents an automated tool that optimizes hardware resource allocation for software-defined vehicles using advanced modeling techniques, showing real-world improvements in efficiency and development speed.", "motivation": "The shift towards software-defined vehicles (SDVs) in the automotive industry requires more flexible and efficient resource allocation on shared hardware platforms. However, adapting static vendor configurations to diverse and dynamic workloads is challenging for Tier 1 integrators.", "method": "The paper introduces an automated scenario generation framework that profiles the runtime behavior of systems. It incorporates both theoretical models and vendor heuristics to generate optimized hypervisor configurations. Two modeling approaches are compared: domain-guided parametric modeling and deep learning-based modeling, and these models are used as the basis for a resource allocation optimization strategy.", "result": "The proposed framework demonstrates its effectiveness in real-world deployments by improving hardware integration efficiency and reducing development time in environments where resources are limited.", "conclusion": "The automated framework provides a practical solution for Tier 1 automotive vendors, enabling efficient hardware resource allocation across multiple VMs and addressing the challenges of adapting to evolving SDV requirements."}}
