{"id": "2508.16671", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16671", "abs": "https://arxiv.org/abs/2508.16671", "authors": ["Mingyang Zhou", "Quanming Yao", "Lun Du", "Lanning Wei", "Da Zheng"], "title": "Reflective Paper-to-Code Reproduction Enabled by Fine-Grained Verification", "comment": null, "summary": "Reproducing machine learning papers is essential for scientific progress but\nremains challenging for both humans and automated agents. Existing agent-based\nmethods often struggle to fully and accurately reproduce implementation details\nsuch as mathematical formulas and algorithmic logic. Previous studies show that\nreflection with explicit feedback improves agent performance. However, current\npaper reproduction methods fail to effectively adopt this strategy. This gap\nmainly arises from the diverse paper patterns, complex method modules, and\nvaried configurations encountered in research papers. Motivated by how humans\nuse systematic checklists to efficiently debug complex code, we propose\n\\textbf{RePro}, a \\textbf{Re}flective Paper-to-Code \\textbf{Repro}duction\nframework that automatically extracts a paper's fingerprint, referring to a\ncomprehensive set of accurate and atomic criteria serving as high-quality\nsupervisory signals. The framework first generates code based on the extracted\ninformation, and then leverages the fingerprint within iterative verification\nand refinement loop. This approach systematically detects discrepancies and\nproduces targeted revisions to align generated code with the paper's\nimplementation details. Extensive experiments on the PaperBench Code-Dev\nbenchmark have been conducted, RePro achieves 13.0\\% performance gap over\nbaselines, and it correctly revises complex logical and mathematical criteria\nin reflecting, on which the effectiveness is obvious.", "AI": {"tldr": "RePro is a new framework that uses detailed checklists to guide iterative code generation and correction, resulting in a 13% improvement over existing methods for reproducing machine learning papers from text.", "motivation": "Reproducing machine learning papers is crucial for scientific progress but is difficult due to diverse paper structures, complex methods, and varied configurations. Existing automated methods struggle to accurately reproduce implementation details, especially mathematical formulas and algorithmic logic. Previous work shows feedback and reflection improve agent performance, but current solutions do not effectively leverage these strategies.", "method": "The authors propose RePro, a Reflective Paper-to-Code Reproduction framework. RePro automatically extracts a 'fingerprint' from a paper, which consists of detailed and atomic criteria that provide high-quality supervisory signals. The system then generates code based on this fingerprint and iteratively verifies and refines the code using the fingerprint, systematically detecting and correcting discrepancies to ensure alignment with the paper's true implementation.", "result": "RePro was evaluated on the PaperBench Code-Dev benchmark. It achieved a 13.0% improvement over baseline methods and was especially effective at revising complex logical and mathematical implementation details.", "conclusion": "RePro provides a systematic and effective framework for reproducing machine learning papers by leveraging targeted, reflective iteration and high-quality criteria extracted directly from research papers. This addresses key challenges in paper reproduction and significantly improves the accuracy of automatic code generation from papers."}}
{"id": "2508.16678", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.16678", "abs": "https://arxiv.org/abs/2508.16678", "authors": ["Konrad Cinkusz", "Jaros\u0142aw A. Chudziak", "Ewa Niewiadomska-Szynkiewicz"], "title": "Cognitive Agents Powered by Large Language Models for Agile Software Project Management", "comment": null, "summary": "This paper investigates the integration of cognitive agents powered by Large\nLanguage Models (LLMs) within the Scaled Agile Framework (SAFe) to reinforce\nsoftware project management. By deploying virtual agents in simulated software\nenvironments, this study explores their potential to fulfill fundamental roles\nin IT project development, thereby optimizing project outcomes through\nintelligent automation. Particular emphasis is placed on the adaptability of\nthese agents to Agile methodologies and their transformative impact on\ndecision-making, problem-solving, and collaboration dynamics. The research\nleverages the CogniSim ecosystem, a platform designed to simulate real-world\nsoftware engineering challenges, such as aligning technical capabilities with\nbusiness objectives, managing interdependencies, and maintaining project\nagility. Through iterative simulations, cognitive agents demonstrate advanced\ncapabilities in task delegation, inter-agent communication, and project\nlifecycle management. By employing natural language processing to facilitate\nmeaningful dialogues, these agents emulate human roles and improve the\nefficiency and precision of Agile practices. Key findings from this\ninvestigation highlight the ability of LLM-powered cognitive agents to deliver\nmeasurable improvements in various metrics, including task completion times,\nquality of deliverables, and communication coherence. These agents exhibit\nscalability and adaptability, ensuring their applicability across diverse and\ncomplex project environments. This study underscores the potential of\nintegrating LLM-powered agents into Agile project management frameworks as a\nmeans of advancing software engineering practices. This integration not only\nrefines the execution of project management tasks but also sets the stage for a\nparadigm shift in how teams collaborate and address emerging challenges.", "AI": {"tldr": "This paper simulates using LLM-powered agents in Agile project management, finding that these agents measurably improve efficiency, communication, and quality, suggesting a transformative potential for software engineering teams.", "motivation": "To explore how Large Language Model-powered cognitive agents can enhance software project management in Agile environments by automating fundamental roles and improving team collaboration and decision-making.", "method": "The study uses the CogniSim ecosystem to deploy and simulate virtual cognitive agents in software project scenarios aligned with the Scaled Agile Framework (SAFe). Iterative simulations measure agents\u2019 performance in tasks such as delegation, inter-agent communication, and project lifecycle management.", "result": "Cognitive agents showed advanced task delegation, effective communication, and better project management. They improved metrics such as task completion time, deliverable quality, and communication clarity. The agents demonstrated adaptability and scalability across different and complex project types.", "conclusion": "Integrating LLM-powered cognitive agents into Agile project management enhances efficiency, collaboration, and decision-making. This approach refines project task execution and suggests a paradigm shift for modern software development teams."}}
{"id": "2508.16684", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.16684", "abs": "https://arxiv.org/abs/2508.16684", "authors": ["Vikranth Udandarao", "Nipun Misra"], "title": "Democratizing AI Development: Local LLM Deployment for India's Developer Ecosystem in the Era of Tokenized APIs", "comment": "for survey results, check\n  https://docs.google.com/spreadsheets/d/1t0eV9oURaiu2HfARWo6sriBO0eC8bHUyZNN7CgK2NAk/edit?usp=sharing", "summary": "India's developer community faces significant barriers to sustained\nexperimentation and learning with commercial Large Language Model (LLM) APIs,\nprimarily due to economic and infrastructural constraints. This study\nempirically evaluates local LLM deployment using Ollama as an alternative to\ncommercial cloud-based services for developer-focused applications. Through a\nmixed-methods analysis involving 180 Indian developers, students, and AI\nenthusiasts, we find that local deployment enables substantially greater\nhands-on development and experimentation, while reducing costs by 33% compared\nto commercial solutions. Developers using local LLMs completed over twice as\nmany experimental iterations and reported deeper understanding of advanced AI\narchitectures. Our results highlight local deployment as a critical enabler for\ninclusive and accessible AI development, demonstrating how technological\naccessibility can enhance learning outcomes and innovation capacity in\nresource-constrained environments.", "AI": {"tldr": "Deploying LLMs locally, instead of using expensive commercial APIs, greatly boosts hands-on AI learning and experimentation for Indian developers, significantly lowering costs and increasing innovation.", "motivation": "Indian developers often struggle to continuously learn and experiment with commercial LLM APIs due to significant economic and resource barriers.", "method": "The study conducted a mixed-methods analysis involving 180 Indian developers, students, and AI enthusiasts, comparing local LLM deployment using Ollama to commercial cloud-based services.", "result": "Local LLM deployment reduced costs by 33% and enabled developers to complete over twice as many experimental iterations. It also led to a deeper understanding of advanced AI architectures.", "conclusion": "Local deployment of LLMs is crucial for accessible and inclusive AI development in India, dramatically enhancing experimentation, learning, and innovation in resource-limited settings."}}
{"id": "2508.16688", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16688", "abs": "https://arxiv.org/abs/2508.16688", "authors": ["Ankur Tomar", "Hengyue Liang", "Indranil Bhattacharya", "Natalia Larios", "Francesco Carbone"], "title": "Cybernaut: Towards Reliable Web Automation", "comment": null, "summary": "The emergence of AI-driven web automation through Large Language Models\n(LLMs) offers unprecedented opportunities for optimizing digital workflows.\nHowever, deploying such systems within industry's real-world environments\npresents four core challenges: (1) ensuring consistent execution, (2)\naccurately identifying critical HTML elements, (3) meeting human-like accuracy\nin order to automate operations at scale and (4) the lack of comprehensive\nbenchmarking data on internal web applications. Existing solutions are\nprimarily tailored for well-designed, consumer-facing websites (e.g.,\nAmazon.com, Apple.com) and fall short in addressing the complexity of\npoorly-designed internal web interfaces. To address these limitations, we\npresent Cybernaut, a novel framework to ensure high execution consistency in\nweb automation agents designed for robust enterprise use. Our contributions are\nthreefold: (1) a Standard Operating Procedure (SOP) generator that converts\nuser demonstrations into reliable automation instructions for linear browsing\ntasks, (2) a high-precision HTML DOM element recognition system tailored for\nthe challenge of complex web interfaces, and (3) a quantitative metric to\nassess execution consistency. The empirical evaluation on our internal\nbenchmark demonstrates that using our framework enables a 23.2% improvement\n(from 72% to 88.68%) in task execution success rate over the browser_use.\nCybernaut identifies consistent execution patterns with 84.7% accuracy,\nenabling reliable confidence assessment and adaptive guidance during task\nexecution in real-world systems. These results highlight Cybernaut's\neffectiveness in enterprise-scale web automation and lay a foundation for\nfuture advancements in web automation.", "AI": {"tldr": "Cybernaut is a new AI framework that boosts the reliability and accuracy of web automation in tough enterprise environments, significantly outperforming previous methods and enabling more robust automated workflows.", "motivation": "AI-driven web automation using large language models (LLMs) presents exciting potential in optimizing digital workflows. However, current solutions struggle with real-world deployment, especially in complex, poorly designed internal web applications. Key challenges include consistent execution, accurately identifying HTML elements, achieving human-like accuracy, and a lack of benchmarking data.", "method": "The authors introduce 'Cybernaut,' a new framework aimed at enterprise web automation. Its method involves: (1) an SOP generator that translates user demonstrations into automation steps, (2) a specialized HTML DOM recognition system to handle complex interfaces, and (3) a new metric for measuring execution consistency.", "result": "Empirical evaluation on internal benchmarks shows Cybernaut improves task execution success rate from 72% to 88.68% over previous methods. It also reliably identifies consistent execution patterns with 84.7% accuracy, enabling better confidence assessment and adaptive guidance.", "conclusion": "Cybernaut effectively addresses web automation challenges in enterprise environments, demonstrating significant improvements in consistency and reliability. The work lays groundwork for further advancements in AI-powered automation of complex workflows."}}
{"id": "2508.16746", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.16746", "abs": "https://arxiv.org/abs/2508.16746", "authors": ["Karuna Grewal", "P. Brighten Godfrey", "Justin Hsu"], "title": "SafeTree: Expressive Tree Policies for Microservices", "comment": null, "summary": "A microservice-based application is composed of multiple self-contained\ncomponents called microservices, and controlling inter-service communication is\nimportant for enforcing safety properties. Presently, inter-service\ncommunication is configured using microservice deployment tools. However, such\ntools only support a limited class of single-hop policies, which can be overly\npermissive because they ignore the rich service tree structure of microservice\ncalls. Policies that can express the service tree structure can offer\ndevelopment and security teams more fine-grained control over communication\npatterns.\n  To this end, we design an expressive policy language to specify service tree\nstructures, and we develop a visibly pushdown automata-based dynamic\nenforcement mechanism to enforce service tree policies. Our technique is\nnon-invasive: it does not require any changes to service implementations, and\ndoes not require access to microservice code. To realize our method, we build a\nruntime monitor on top of a service mesh, an emerging network infrastructure\nlayer that can control inter-service communication during deployment. In\nparticular, we employ the programmable network traffic filtering capabilities\nof Istio, a popular service mesh implementation, to implement an online and\ndistributed monitor. Our experiments show that our monitor can enforce rich\nsafety properties while adding minimal latency overhead on the order of\nmilliseconds.", "AI": {"tldr": "The paper introduces a new policy language and enforcement mechanism for microservices, allowing fine-grained, service tree-aware control of inter-service communication. Their solution, built on Istio, requires no code changes and adds only minimal latency, enhancing security beyond current deployment tools.", "motivation": "Current microservice deployment tools only allow for simple, single-hop communication policies, which can be overly permissive and do not take into account the complex hierarchical structure of microservice calls. This lack restricts security and control over inter-service communications.", "method": "The authors design a novel policy language that can express service tree structures and develop a dynamic enforcement mechanism based on visibly pushdown automata. They implement a runtime monitor on top of Istio, a popular service mesh, enabling programmable, distributed online enforcement of policies without requiring changes to the microservices themselves.", "result": "The implemented monitor is able to enforce sophisticated safety properties reflecting service tree communication patterns, while incurring only minimal latency overhead, measured in milliseconds.", "conclusion": "The approach offers an expressive, practical, and minimally invasive solution for fine-grained control over microservice communication, improving security and flexibility without significant performance penalties."}}
{"id": "2508.16708", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16708", "abs": "https://arxiv.org/abs/2508.16708", "authors": ["Shufeng Chen", "Halima El Badaoui", "Mariat James Elizebeth", "Takuya Nakashima", "Siddartha Khastgir", "Paul Jennings"], "title": "A Scalable Framework for the Management of STPA Requirements: a Case Study on eVTOL Operations", "comment": null, "summary": "System-Theoretic Process Analysis (STPA) is a recommended method for\nanalysing complex systems, capable of identifying thousands of safety\nrequirements often missed by traditional techniques such as Failure Mode and\nEffects Analysis (FMEA) and Fault Tree Analysis (FTA). However, the absence of\na structured framework for managing and prioritising these requirements\npresents challenges, particularly in fast-paced development environments. This\npaper introduces a scalable framework for prioritising STPA-derived\nrequirements. The framework integrates outputs from each STPA step and\nincorporates expert evaluations based on four key factors: implementation time,\ncost, requirement type, and regulatory coverage. To reduce subjectivity,\nMonte-Carlo Simulation (MCS) is employed to calculate and stabilise requirement\nrankings. An automation toolchain supports the framework, enabling dynamic\nmapping of prioritised requirements in a scaling matrix. This visualisation\naids decision-making and ensures traceability across development phases. The\nframework is applicable from early conceptualisation to more advanced stages,\nenhancing its utility in iterative system development. The framework was\nvalidated through a real-world case study focused on Electric Vertical Take-off\nand Landing (eVTOL) operations, conducted in collaboration with the UK Civil\nAviation Authority. The findings contributed directly to CAP3141, a Civil\nAviation Publication that identifies systemic operational risks and safety\nmitigations for regulators, operators, and vertiports. The prioritisation\nprocess supported decision-making by helping stakeholders identify and manage\nhigh-impact requirements efficiently. This work contributes a practical\nsolution for managing STPA outputs, bridging gaps in requirement prioritisation\nand supporting safety-critical development in emerging technologies.", "AI": {"tldr": "The paper proposes and validates a new framework and toolchain for efficiently prioritising and managing safety requirements produced by STPA, using expert input and Monte-Carlo Simulation to ensure objectivity and traceability, with successful application in the eVTOL aviation sector.", "motivation": "STPA can generate thousands of safety requirements for complex systems, but lacks a structured and practical approach for prioritising and managing these requirements, especially in fast-paced or iterative development environments.", "method": "The paper presents a scalable framework for prioritising requirements identified through STPA. It integrates outputs from each STPA step and uses expert evaluation across four factors: implementation time, cost, requirement type, and regulatory coverage. To reduce subjectivity, Monte-Carlo Simulation (MCS) is used to stabilise and calculate requirement rankings. The framework is supported by an automation toolchain that visualises prioritised requirements and enables traceability. Validation was performed via a real-world case study in collaboration with the UK Civil Aviation Authority for eVTOL operations.", "result": "The framework was shown to support efficient decision-making and management of high-impact requirements in safety-critical systems. It was validated in a real-world eVTOL case and its findings were incorporated into an official Civil Aviation Authority publication, helping regulators and operators manage operational risks and mitigations.", "conclusion": "A practical, scalable framework for managing and prioritising STPA-derived safety requirements was developed and validated, improving requirement traceability, stability, and decision-support for emerging technological systems."}}
{"id": "2508.16848", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.16848", "abs": "https://arxiv.org/abs/2508.16848", "authors": ["David Moon", "Andrew Blinn", "Thomas J. Porter", "Cyrus Omar"], "title": "Syntactic Completions with Material Obligations", "comment": null, "summary": "Code editors provide essential services that help developers understand,\nnavigate, and modify programs. However, these services often fail in the\npresence of syntax errors. Existing syntax error recovery techniques, like\npanic mode and multi-option repairs, are either too coarse, e.g. in deleting\nlarge swathes of code, or lead to a proliferation of possible completions. This\npaper introduces $\\texttt{tylr}$, a parser and editor generator that completes\narbitrarily malformed code by inserting obligations, which generalize holes to\ncover missing operands, operators, mixfix keywords, and sort transitions.\n$\\texttt{tylr}$ is backed by a novel theory of tile-based parsing, which\nextends operator-precedence parsing in two ways. First, traditional token\nprecedence comparisons are replaced by a notion of grammar walks, which form\nthe basis for generating obligations. Second, a distinct \"molding\" system based\non grammar zippers expand grammar expressivity by allowing the system to\ndisambiguate between possible parses and completions based on an obligation\nminimization criterion. In addition to serving as a novel approach to error\ncorrection, $\\texttt{tylr}$'s design enables the development of an editor that\nvisually materializes obligations to the human user, serving as a novel hybrid\nbetween a text editor and a structure editor. We introduce $\\texttt{tylr}$ by\nexample, then formalize its key ideas. Finally, we conduct a human subjects\nstudy to evaluate the extent to which an editor like $\\texttt{tylr}$ that\nmaterializes syntactic obligations might be usable and useful, finding both\npoints of positivity and interesting new avenues for future work.", "AI": {"tldr": "The paper proposes tylr, a novel code editor and parser that helps users deal with syntax errors by representing them as 'obligations,' improving error correction and offering a user-friendly editing experience, with positive results shown in user studies.", "motivation": "Code editors often struggle to provide helpful services\u2014like navigation and code understanding\u2014when the user's code contains syntax errors. Existing error recovery strategies either lose significant amounts of code or result in too many repair options, making them impractical for effective developer support.", "method": "The authors introduce 'tylr,' a parser and editor generator using a new tile-based parsing theory. This approach generalizes the notion of 'holes' (places in code that need fixing) to 'obligations' that can represent a greater variety of syntax gaps. The parsing strategy extends operator-precedence parsing with 'grammar walks' and a grammar zipper-based molding system. tylr visually presents these obligations to the user within the editor.", "result": "The paper presents tylr through examples, formally describes the underlying mechanisms, and evaluates it via a human subjects study. The results show that visualizing syntactic obligations can be both usable and useful for developers, highlighting positive user feedback along with areas for further research.", "conclusion": "tylr provides a new approach to real-time syntax error recovery and code editing by representing syntax issues as obligations directly in the editor UI, blending features of text and structure editors. The study indicates that this method can assist developers effectively when dealing with malformed code."}}
{"id": "2508.16713", "categories": ["cs.SE", "cs.AI", "hep-ex"], "pdf": "https://arxiv.org/pdf/2508.16713", "abs": "https://arxiv.org/abs/2508.16713", "authors": ["Mohammad Atif", "Kriti Chopra", "Ozgur Kilic", "Tianle Wang", "Zhihua Dong", "Charles Leggett", "Meifeng Lin", "Paolo Calafiura", "Salman Habib"], "title": "CelloAI: Leveraging Large Language Models for HPC Software Development in High Energy Physics", "comment": "12 pages, 2 figures", "summary": "Next-generation High Energy Physics (HEP) experiments will generate\nunprecedented data volumes, necessitating High Performance Computing (HPC)\nintegration alongside traditional high-throughput computing. However, HPC\nadoption in HEP is hindered by the challenge of porting legacy software to\nheterogeneous architectures and the sparse documentation of these complex\nscientific codebases. We present CelloAI, a locally hosted coding assistant\nthat leverages Large Language Models (LLMs) with retrieval-augmented generation\n(RAG) to support HEP code documentation and generation. This local deployment\nensures data privacy, eliminates recurring costs and provides access to large\ncontext windows without external dependencies. CelloAI addresses two primary\nuse cases, code documentation and code generation, through specialized\ncomponents. For code documentation, the assistant provides: (a) Doxygen style\ncomment generation for all functions and classes by retrieving relevant\ninformation from RAG sources (papers, posters, presentations), (b) file-level\nsummary generation, and (c) an interactive chatbot for code comprehension\nqueries. For code generation, CelloAI employs syntax-aware chunking strategies\nthat preserve syntactic boundaries during embedding, improving retrieval\naccuracy in large codebases. The system integrates callgraph knowledge to\nmaintain dependency awareness during code modifications and provides\nAI-generated suggestions for performance optimization and accurate refactoring.\nWe evaluate CelloAI using real-world HEP applications from ATLAS, CMS, and DUNE\nexperiments, comparing different embedding models for code retrieval\neffectiveness. Our results demonstrate the AI assistant's capability to enhance\ncode understanding and support reliable code generation while maintaining the\ntransparency and safety requirements essential for scientific computing\nenvironments.", "AI": {"tldr": "CelloAI is a locally hosted AI assistant that uses large language models and enhanced retrieval strategies to help physicists document and generate code for complex scientific software, addressing HPC integration challenges in HEP. It outperforms prior tools in effectiveness while meeting scientific standards for privacy and transparency.", "motivation": "The motivation is the difficulty of adopting High Performance Computing in High Energy Physics (HEP) due to legacy software complexities, heterogeneous architectures, and poor documentation. There is a need for tools to aid code documentation and generation for efficient and reliable adaptation.", "method": "CelloAI is a locally hosted coding assistant based on Large Language Models enhanced with retrieval-augmented generation. It features specialized documentation (Doxygen comments, summaries, chatbot) and code generation (syntax-aware chunking, callgraph integration) components. Evaluation uses real-world data from major HEP experiments and compares embedding models.", "result": "CelloAI supports code documentation and generation across large, complex legacy scientific codebases, enhancing code comprehension and reliable code modification. The AI assistant preserves data privacy and offers transparency and safety necessary for scientific computing, outperforming traditional code retrieval approaches in effectiveness.", "conclusion": "CelloAI is a practical solution for modernizing and maintaining HEP scientific codes, facilitating documentation and code adaptation within HPC environments, while ensuring privacy and reliability crucial for scientific research."}}
{"id": "2508.17344", "categories": ["cs.SE", "cs.LG", "cs.PF", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.17344", "abs": "https://arxiv.org/abs/2508.17344", "authors": ["Rajrupa Chattaraj", "Sridhar Chimalakonda", "Vibhu Saujanya Sharma", "Vikrant Kaulgud"], "title": "Who Wins the Race? (R Vs Python) - An Exploratory Study on Energy Consumption of Machine Learning Algorithms", "comment": "18 pages including references, 5 figures", "summary": "The utilization of Machine Learning (ML) in contemporary software systems is\nextensive and continually expanding. However, its usage is energy-intensive,\ncontributing to increased carbon emissions and demanding significant resources.\nWhile numerous studies examine the performance and accuracy of ML, only a\nlimited few focus on its environmental aspects, particularly energy\nconsumption. In addition, despite emerging efforts to compare energy\nconsumption across various programming languages for specific algorithms and\ntasks, there remains a gap specifically in comparing these languages for\nML-based tasks. This paper aims to raise awareness of the energy costs\nassociated with employing different programming languages for ML model training\nand inference. Through this empirical study, we measure and compare the energy\nconsumption along with run-time performance of five regression and five\nclassification tasks implemented in Python and R, the two most popular\nprogramming languages in this context. Our study results reveal a statistically\nsignificant difference in costs between the two languages in 95% of the cases\nexamined. Furthermore, our analysis demonstrates that the choice of programming\nlanguage can influence energy efficiency significantly, up to 99.16% during\nmodel training and up to 99.8% during inferences, for a given ML task.", "AI": {"tldr": "The paper investigates the energy impact of Python vs. R for ML tasks, showing major differences and highlighting the importance of language choice for greener machine learning.", "motivation": "To address the lack of studies on energy consumption associated with ML model training and inference across different programming languages, especially given the environmental concerns of energy usage in ML.", "method": "Empirical measurement and comparison of energy consumption and run-time performance in five regression and five classification ML tasks using Python and R.", "result": "The study found statistically significant differences in energy costs between Python and R in 95% of cases, with language choice affecting energy efficiency by up to 99.16% for model training and 99.8% for inference tasks.", "conclusion": "There are significant differences in energy consumption and performance between Python and R for ML tasks; language choice can greatly impact energy efficiency."}}
{"id": "2508.16771", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.16771", "abs": "https://arxiv.org/abs/2508.16771", "authors": ["Yifan Zhang", "Chen Huang", "Yueke Zhang", "Jiahao Zhang", "Toby Jia-Jun Li", "Collin McMillan", "Kevin Leach", "Yu Huang"], "title": "EyeMulator: Improving Code Language Models by Mimicking Human Visual Attention", "comment": null, "summary": "Code language models (so-called CodeLLMs) are now commonplace in software\ndevelopment. As a general rule, CodeLLMs are trained by dividing training\nexamples into input tokens and then learn importance of those tokens in a\nprocess called machine attention. Machine attention is based solely on input\ntoken salience to output token examples during training. Human software\ndevelopers are different, as humans intuitively know that some tokens are more\nsalient than others. While intuition itself is ineffable and a subject of\nphilosophy, clues about salience are present in human visual attention, since\npeople tend to look at more salient words more often. In this paper, we present\nEyeMulator, a technique for training CodeLLMs to mimic human visual attention\nwhile training for various software development tasks. We add special weights\nfor each token in each input example to the loss function used during LLM\nfine-tuning. We draw these weights from observations of human visual attention\nderived from a previously-collected publicly-available dataset of eye-tracking\nexperiments in software engineering tasks. These new weights ultimately induce\nchanges in the attention of the subject LLM during training, resulting in a\nmodel that does not need eye-tracking data during inference. Our evaluation\nshows that EyeMulator outperforms strong LLM baselines on several tasks such as\ncode translation, completion and summarization. We further show an ablation\nstudy that demonstrates the improvement is due to subject models learning to\nmimic human attention.", "AI": {"tldr": "EyeMulator uses eye-tracking data to adjust the way CodeLLMs learn token importance, helping them mimic human attention. This approach yields better results on coding tasks compared to traditional models trained without human attention data.", "motivation": "CodeLLMs use machine attention based solely on token salience in training data, which differs from human developers, who intuitively prioritize tokens based on visual attention. The motivation is to reduce this gap and make CodeLLMs better align with human cognitive patterns.", "method": "The authors introduce EyeMulator, a technique that adds special weights to each input token in the loss function during LLM fine-tuning. These weights are derived from eye-tracking data collected from software engineering tasks, reflecting human visual attention patterns.", "result": "Models trained with EyeMulator learn to mimic human visual attention and outperform strong CodeLLM baselines on tasks like code translation, completion, and summarization. Ablation studies confirm that the gains are due to the integration of human attention data.", "conclusion": "Incorporating human-derived visual attention weights into CodeLLM training produces models that better emulate human judgment and attention during software development tasks, leading to measurable improvements in performance."}}
{"id": "2508.16853", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16853", "abs": "https://arxiv.org/abs/2508.16853", "authors": ["Pratyush Nidhi Sharma", "Lauren Wright", "Anne Herfurth", "Munsif Sokiyna", "Pratyaksh Nidhi Sharma", "Sethu Das", "Mikko Siponen"], "title": "DevLicOps: A Framework for Mitigating Licensing Risks in AI-Generated Code", "comment": "18 pages, 1 figure, 2 Tables", "summary": "Generative AI coding assistants (ACAs) are widely adopted yet pose serious\nlegal and compliance risks. ACAs can generate code governed by restrictive\nopen-source licenses (e.g., GPL), potentially exposing companies to litigation\nor forced open-sourcing. Few developers are trained in these risks, and legal\nstandards vary globally, especially with outsourcing. Our article introduces\nDevLicOps, a practical framework that helps IT leaders manage ACA-related\nlicensing risks through governance, incident response, and informed tradeoffs.\nAs ACA adoption grows and legal frameworks evolve, proactive license compliance\nis essential for responsible, risk-aware software development in the AI era.", "AI": {"tldr": "As AI coding assistants become common, they bring major legal risks by generating potentially non-compliant code. The paper proposes DevLicOps, a management framework to help organizations handle these risks with governance and incident response strategies, emphasizing the importance of proactive license compliance.", "motivation": "The rapid adoption of AI coding assistants (ACAs) introduces significant legal and compliance risks due to code generation under restrictive licenses, while many developers lack awareness and training in these legal aspects.", "method": "The paper introduces DevLicOps, a practical management framework for IT leaders focused on governance, incident response, and balancing tradeoffs regarding ACA-related licensing risks.", "result": "The DevLicOps framework supports responsible and risk-aware AI-driven software development by guiding leaders to manage the evolving legal risks of ACA-generated code.", "conclusion": "Proactive license compliance, enabled by a structured framework like DevLicOps, is critical as ACA adoption grows and legal standards shift; organizations must address these risks to avoid legal exposure and ensure responsible development."}}
{"id": "2508.16860", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.16860", "abs": "https://arxiv.org/abs/2508.16860", "authors": ["Md Afif Al Mamun", "Gias Uddin", "Lan Xia", "Longyu Zhang"], "title": "TriagerX: Dual Transformers for Bug Triaging Tasks with Content and Interaction Based Rankings", "comment": "This work is currently under review at IEEE Transactions on Software\n  Engineering. The replication package will be made publicly available upon\n  acceptance", "summary": "Pretrained Language Models or PLMs are transformer-based architectures that\ncan be used in bug triaging tasks. PLMs can better capture token semantics than\ntraditional Machine Learning (ML) models that rely on statistical features\n(e.g., TF-IDF, bag of words). However, PLMs may still attend to less relevant\ntokens in a bug report, which can impact their effectiveness. In addition, the\nmodel can be sub-optimal with its recommendations when the interaction history\nof developers around similar bugs is not taken into account. We designed\nTriagerX to address these limitations. First, to assess token semantics more\nreliably, we leverage a dual-transformer architecture. Unlike current\nstate-of-the-art (SOTA) baselines that employ a single transformer\narchitecture, TriagerX collects recommendations from two transformers with each\noffering recommendations via its last three layers. This setup generates a\nrobust content-based ranking of candidate developers. TriagerX then refines\nthis ranking by employing a novel interaction-based ranking methodology, which\nconsiders developers' historical interactions with similar fixed bugs. Across\nfive datasets, TriagerX surpasses all nine transformer-based methods, including\nSOTA baselines, often improving Top-1 and Top-3 developer recommendation\naccuracy by over 10%. We worked with our large industry partner to successfully\ndeploy TriagerX in their development environment. The partner required both\ndeveloper and component recommendations, with components acting as proxies for\nteam assignments-particularly useful in cases of developer turnover or team\nchanges. We trained TriagerX on the partner's dataset for both tasks, and it\noutperformed SOTA baselines by up to 10% for component recommendations and 54%\nfor developer recommendations.", "AI": {"tldr": "TriagerX is a dual-transformer-based bug triaging system that considers developer interaction history, outperforming state-of-the-art methods by substantial margins and proving effective in real-world industry deployment.", "motivation": "Pretrained Language Models (PLMs) have shown promise for bug triaging by capturing token semantics better than traditional ML models, but they still focus on less relevant tokens and do not take developer interaction history into account. These limitations hinder their performance and practical adoption for bug triage automation.", "method": "The authors propose TriagerX, a dual-transformer architecture. Unlike single-transformer SOTA baselines, TriagerX combines recommendations from the last three layers of two separate transformers for robust content-based candidate developer ranking. It then uses a novel interaction-based ranking that incorporates developers' historical interactions with similar bug fixes.", "result": "TriagerX significantly outperforms nine transformer-based SOTA methods on five datasets, with Top-1 and Top-3 developer recommendation accuracy increases of over 10%. Deployment with a large industry partner showed up to 10% improvement for component recommendations and up to 54% for developer recommendations compared to SOTA baselines.", "conclusion": "TriagerX effectively addresses the token relevance and interaction history limitations of PLMs, resulting in superior bug triage performance and successful industry deployment."}}
{"id": "2508.16903", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16903", "abs": "https://arxiv.org/abs/2508.16903", "authors": ["Yijun Lu", "Hironori Washizaki", "Naoyasu Ubayashi", "Nobukazu Yoshioka", "Chenhao Wu", "Masanari Kondo", "Yuyin Ma", "Jiong Dong", "Jianjin Zhao", "Dongqi Han"], "title": "Mind the Gap: A Decade-Scale Empirical Study of Multi-Stakeholder Dynamics in VR Ecosystem", "comment": null, "summary": "In the development and evolution of VR ecosystem, platform stakeholders\ncontinuously adapt their products in response to user and technical feedback,\noften reflected in subtle shifts in discussion topics or system updates. A\ncomprehensive understanding of these changes is essential for identifying gaps\nbetween user expectations and developer actions, which can guide more effective\nquality assurance and user-centered innovation. While previous studies have\nanalyzed either user reviews or developer discussions in isolation, such\napproaches typically fail to reveal how specific user concerns are (or are not)\naddressed by corresponding technical activities. To address this limitation,\nour study introduces a multi-view empirical framework that systematically\ncompares and aligns stakeholder perspectives. By applying topic modeling and\nquantitative impact analysis to 944,320 user reviews and 389,477 developer\nposts, we identify not only the overlap in concerns (e.g., performance, input\nmethods), but also clear gaps in areas like inclusivity and community safety\n(e.g., LGBTQ+ representation, child-friendly content). Our findings show that\nwhile users repeatedly raise such issues, they are rarely discussed in\ndeveloper forums. These insights enable data-driven recommendations for closing\nthe user-developer gap in VR ecosystems, offering practical implications for\nplatform governance and the design of next-generation VR systems.", "AI": {"tldr": "By analyzing user reviews and developer discussions together, the study uncovers mismatches between user concerns (notably around inclusivity and safety) and developer priorities, offering practical guidance to bridge these gaps in VR platforms.", "motivation": "Current research often analyzes user or developer feedback separately in VR ecosystems, making it difficult to understand how user concerns are addressed by developers. This paper aims to bridge the gap by examining both perspectives together.", "method": "The paper introduces a multi-view empirical framework that uses topic modeling and quantitative impact analysis on a large dataset of 944,320 user reviews and 389,477 developer posts.", "result": "The study identified overlaps in concerns between users and developers (like performance and input methods), but also found significant gaps, especially regarding inclusivity and community safety (such as LGBTQ+ representation and child-friendly content), which are often ignored in developer forums despite user emphasis.", "conclusion": "Aligning user and developer perspectives reveals actionable gaps in VR ecosystem development, informing targeted improvements for platform governance and the user experience."}}
{"id": "2508.17161", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.17161", "abs": "https://arxiv.org/abs/2508.17161", "authors": ["Julyanara R. Silva", "Carlos Eduardo C. Dantas", "Marcelo A. Maia"], "title": "What Developers Ask to ChatGPT in GitHub Pull Requests? an Exploratory Study", "comment": "12 pages, 3 figures", "summary": "The emergence of Large Language Models (LLMs), such as ChatGPT, has\nintroduced a new set of tools to support software developers in solving pro-\ngramming tasks. However, our understanding of the interactions (i.e., prompts)\nbetween developers and ChatGPT that result in contributions to the codebase\nremains limited. To explore this limitation, we conducted a manual evaluation\nof 155 valid ChatGPT share links extracted from 139 merged Pull Requests (PRs),\nrevealing the interactions between developers and reviewers with ChatGPT that\nled to merges into the main codebase. Our results produced a catalog of 14\ntypes of ChatGPT requests categorized into four main groups. We found a\nsignificant number of requests involving code review and the implementation of\ncode snippets based on specific tasks. Developers also sought to clarify doubts\nby requesting technical explanations or by asking for text refinements for\ntheir web pages. Furthermore, we verified that prompts involving code\ngeneration generally required more interactions to produce the desired answer\ncompared to prompts requesting text review or technical information.", "AI": {"tldr": "This paper manually analyzes developer interactions with ChatGPT in merged GitHub Pull Requests, categorizing 14 types of prompts into four groups. It finds that code generation tasks require more back-and-forth than text reviews or technical questions, thereby offering new insights into how LLMs are used in real-world software development workflows.", "motivation": "There is a lack of understanding about how developers interact with Large Language Models (LLMs), such as ChatGPT, in the context of contributions to software codebases. The study is motivated by the need to explore the nature of these interactions and how they influence software development processes.", "method": "The authors conducted a manual evaluation of 155 ChatGPT share links extracted from 139 merged Pull Requests (PRs). They analyzed these links to uncover patterns in interactions between developers, reviewers, and ChatGPT that resulted in changes merged into the main codebase.", "result": "The study produced a catalog of 14 types of ChatGPT requests, grouped into four broad categories. Common requests involved code review and generating code snippets for specific tasks, as well as seeking technical explanations or refinements for webpage texts. The study also found that code generation prompts typically required more interactions to reach satisfactory answers, compared to prompts for text review or technical information.", "conclusion": "The interaction patterns between developers and ChatGPT are varied, with certain types of requests\u2014especially those related to code generation\u2014being more complex and requiring iterative communication. The research provides a categorization of interaction types, enhancing our understanding of how developers leverage LLMs like ChatGPT in real software development scenarios."}}
{"id": "2508.17343", "categories": ["cs.SE", "cs.AI", "D.2"], "pdf": "https://arxiv.org/pdf/2508.17343", "abs": "https://arxiv.org/abs/2508.17343", "authors": ["Abhik Roychoudhury"], "title": "Agentic AI for Software: thoughts from Software Engineering community", "comment": "4 pages", "summary": "AI agents have recently shown significant promise in software engineering.\nMuch public attention has been transfixed on the topic of code generation from\nLarge Language Models (LLMs) via a prompt. However, software engineering is\nmuch more than programming, and AI agents go far beyond instructions given by a\nprompt.\n  At the code level, common software tasks include code generation, testing,\nand program repair. Design level software tasks may include architecture\nexploration, requirements understanding, and requirements enforcement at the\ncode level. Each of these software tasks involves micro-decisions which can be\ntaken autonomously by an AI agent, aided by program analysis tools. This\ncreates the vision of an AI software engineer, where the AI agent can be seen\nas a member of a development team.\n  Conceptually, the key to successfully developing trustworthy agentic AI-based\nsoftware workflows will be to resolve the core difficulty in software\nengineering - the deciphering and clarification of developer intent.\nSpecification inference, or deciphering the intent, thus lies at the heart of\nmany software tasks, including software maintenance and program repair. A\nsuccessful deployment of agentic technology into software engineering would\ninvolve making conceptual progress in such intent inference via agents.\n  Trusting the AI agent becomes a key aspect, as software engineering becomes\nmore automated. Higher automation also leads to higher volume of code being\nautomatically generated, and then integrated into code-bases. Thus to deal with\nthis explosion, an emerging direction is AI-based verification and validation\n(V & V) of AI generated code. We posit that agentic software workflows in\nfuture will include such AIbased V&V.", "AI": {"tldr": "AI agents are evolving to perform more sophisticated roles in software engineering, beyond code generation, by automating tasks and inferring developer intent. The future of software engineering will rely on trustworthy agentic workflows, including AI-based verification and validation to ensure quality as automation increases.", "motivation": "The motivation is to explore the growing role of AI agents in software engineering, moving beyond prompt-based code generation to autonomous participation in a wider range of software tasks, and addressing the core challenge of understanding developer intent.", "method": "The method is a conceptual and visionary analysis, discussing how AI agents could autonomously perform a variety of software development tasks, leveraging program analysis tools, and the importance of specification inference.", "result": "The paper highlights the necessity for progress in intent inference and proposes that trustworthy, agentic AI software workflows should include AI-based verification and validation to manage the increasing volume of AI-generated code.", "conclusion": "Successfully integrating AI agents into software engineering requires resolving the challenge of inferring developer intent, alongside developing robust AI-based verification and validation systems for automated code."}}
{"id": "2508.17713", "categories": ["cs.SE", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.17713", "abs": "https://arxiv.org/abs/2508.17713", "authors": ["Zhihao Xu", "Shikai Guo", "Guilin Zhao", "Peiyu Zou", "Siwen Wang", "Qian Ma", "Hui Li", "Furui Zhan"], "title": "Code Difference Guided Fuzzing for FPGA Logic Synthesis Compilers via Bayesian Optimization", "comment": null, "summary": "Field Programmable Gate Arrays (FPGAs) play a crucial role in Electronic\nDesign Automation (EDA) applications, which have been widely used in\nsafety-critical environments, including aerospace, chip manufacturing, and\nmedical devices. A critical step in FPGA development is logic synthesis, which\nenables developers to translate their software designs into hardware net lists,\nwhich facilitates the physical implementation of the chip, detailed timing and\npower analysis, gate-level simulation, test vector generation, and optimization\nand consistency checking. However, bugs or incorrect implementations in FPGA\nlogic synthesis compilers may lead to unexpected behaviors in target\nwapplications, posing security risks. Therefore, it is crucial to eliminate\nsuch bugs in FPGA logic synthesis compilers. The effectiveness of existing\nworks is still limited by its simple, blind mutation strategy. To address this\nchallenge, we propose a guided mutation strategy based on Bayesian optimization\ncalled LSC-Fuzz to detect bugs in FPGA logic synthesis compilers. Specifically,\nLSC-Fuzz consists of three components: the test-program generation component,\nthe Bayesian diversity selection component, and the equivalent check component.\nBy performing test-program generation and Bayesian diversity selection,\nLSC-Fuzz generates diverse and complex HDL code, thoroughly testing the FPGA\nlogic synthesis compilers using equivalent check to detect bugs. Through three\nmonths, LSC-Fuzz has found 16 bugs, 12 of these has been confirmed by official\ntechnical support.", "AI": {"tldr": "LSC-Fuzz uses Bayesian optimization to guide mutation and generate diverse HDL test cases, resulting in more effective bug detection in FPGA logic synthesis compilers compared to previous fuzzing strategies.", "motivation": "Bugs in FPGA logic synthesis compilers can cause unexpected behaviors and security risks in safety-critical applications. Existing methods for fuzzing these compilers use simple, unguided mutation strategies, which limits their effectiveness in finding bugs.", "method": "The authors propose LSC-Fuzz, a guided mutation testing strategy based on Bayesian optimization. It consists of a test-program generation component, a Bayesian diversity selection component for generating varied HDL code, and an equivalent check component to detect bugs in FPGA logic synthesis compilers.", "result": "Over three months, LSC-Fuzz discovered 16 bugs in FPGA logic synthesis compilers, and 12 of these bugs were confirmed by official technical support.", "conclusion": "LSC-Fuzz, with its Bayesian-guided mutation and diversity selection, is an effective approach for detecting bugs in FPGA logic synthesis compilers, surpassing previous methods."}}
{"id": "2508.17719", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.17719", "abs": "https://arxiv.org/abs/2508.17719", "authors": ["Akhila Sri Manasa Venigalla", "Sridhar Chimalakonda"], "title": "DocFetch - Towards Generating Software Documentation from Multiple Software Artifacts", "comment": "12 pages, 7 Figures, 4 Tables", "summary": "Software Documentation plays a major role in the usage and development of a\nproject. Widespread adoption of open source software projects contributes to\nlarger and faster development of the projects, making it difficult to maintain\nthe associated documentation. Existing automated approaches to generate\ndocumentation largely focus on source code. However, information useful for\ndocumentation is observed to be scattered across various artifacts that\nco-evolve with the source code. Leveraging this information across multiple\nartifacts can reduce the effort involved in maintaining documentation. Hence,\nwe propose DocFetch, to generate different types of documentation from multiple\nsoftware artifacts. We employ a multi-layer prompt based LLM and generate\nstructured documentation corresponding to different documentation types for the\ndata consolidated in DocMine dataset. We evaluate the performance of DocFetch\nusing a manually curated groundtruth dataset by analysing the artifacts in\nDocMine. The evaluation yields a highest BLEU-4 score of 43.24% and ROUGE-L\nscore of 0.39 for generation of api-related and file-related information from\nfive documentation sources. The generation of other documentation type related\ninformation also reported BLEU-4 scores close to 30% indicating good\nperformance of the approach. Thus,DocFetch can be employed to\nsemi-automatically generate documentation, and helps in comprehending the\nprojects with minimal effort in maintaining the documentation.", "AI": {"tldr": "DocFetch uses a prompt-based LLM to generate accurate software documentation from diverse artifacts, achieving strong BLEU and ROUGE scores, and helps maintain documentation with less manual effort.", "motivation": "Maintaining software documentation is increasingly difficult due to rapid project development, especially in open-source projects. Existing automated documentation methods focus mainly on source code, missing out on valuable information scattered in other artifacts.", "method": "The authors propose DocFetch, a system using a multi-layer prompt-based large language model (LLM) to generate structured documentation from multiple software artifacts. The DocMine dataset was used to consolidate data, and DocFetch's outputs were evaluated with BLEU-4 and ROUGE-L scores against a manually curated ground truth.", "result": "DocFetch achieved a highest BLEU-4 score of 43.24% and a ROUGE-L score of 0.39 in generating API-related and file-related documentation information. Other documentation types also reached BLEU-4 scores close to 30%, demonstrating good overall performance.", "conclusion": "DocFetch effectively semi-automates the generation of various documentation types by leveraging information from multiple artifacts, easing documentation maintenance and improving project comprehension with minimal effort."}}
{"id": "2508.17720", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.17720", "abs": "https://arxiv.org/abs/2508.17720", "authors": ["Ziqi Guan", "Xin Yin", "Zhiyuan Peng", "Chao Ni"], "title": "RepoTransAgent: Multi-Agent LLM Framework for Repository-Aware Code Translation", "comment": null, "summary": "Repository-aware code translation is critical for modernizing legacy systems,\nenhancing maintainability, and enabling interoperability across diverse\nprogramming languages. While recent advances in large language models (LLMs)\nhave improved code translation quality, existing approaches face significant\nchallenges in practical scenarios: insufficient contextual understanding,\ninflexible prompt designs, and inadequate error correction mechanisms. These\nlimitations severely hinder accurate and efficient translation of complex,\nreal-world code repositories. To address these challenges, we propose\nRepoTransAgent, a novel multi-agent LLM framework for repository-aware code\ntranslation. RepoTransAgent systematically decomposes the translation process\ninto specialized subtasks-context retrieval, dynamic prompt construction, and\niterative code refinement-each handled by dedicated agents. Our approach\nleverages retrieval-augmented generation (RAG) for contextual information\ngathering, employs adaptive prompts tailored to varying repository scenarios,\nand introduces a reflection-based mechanism for systematic error correction. We\nevaluate RepoTransAgent on hundreds of Java-C# translation pairs from six\npopular open-source projects. Experimental results demonstrate that\nRepoTransAgent significantly outperforms state-of-the-art baselines in both\ncompile and pass rates. Specifically, RepoTransAgent achieves up to 55.34%\ncompile rate and 45.84% pass rate. Comprehensive analysis confirms the\nrobustness and generalizability of RepoTransAgent across different LLMs,\nestablishing its effectiveness for real-world repository-aware code\ntranslation.", "AI": {"tldr": "RepoTransAgent introduces a multi-agent LLM framework that systematically improves repository-aware code translation by handling context, prompting, and error refinement. It significantly beats existing methods on Java-C# tasks, showing strong generalizability and robustness.", "motivation": "Current code translation tools using large language models (LLMs) struggle with repository-level translation due to lack of contextual understanding, rigid prompt construction, and poor error correction. These issues limit the effectiveness of translating complex, real-world code.", "method": "RepoTransAgent is proposed, a multi-agent LLM framework that divides the code translation process into three specialized subtasks: (1) context retrieval with retrieval-augmented generation, (2) dynamic and adaptive prompt construction, and (3) iterative code refinement with reflection-based error correction. Each subtask is handled by a dedicated agent.", "result": "RepoTransAgent was evaluated on hundreds of Java-C# translation pairs from six open-source projects and showed significant improvement over existing methods, achieving up to 55.34% compile rate and 45.84% pass rate.", "conclusion": "RepoTransAgent is effective, robust, and generalizable for repository-aware code translation, outperforming current state-of-the-art methods on real-world data."}}
{"id": "2508.17851", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.17851", "abs": "https://arxiv.org/abs/2508.17851", "authors": ["Patrick Loic Foalem", "Leuson Da Silva", "Foutse Khomh", "Heng Li", "Ettore Merlo"], "title": "Logging Requirement for Continuous Auditing of Responsible Machine Learning-based Applications", "comment": null, "summary": "Machine learning (ML) is increasingly applied across industries to automate\ndecision-making, but concerns about ethical and legal compliance remain due to\nlimited transparency, fairness, and accountability. Monitoring through logging\na long-standing practice in traditional software offers a potential means for\nauditing ML applications, as logs provide traceable records of system behavior\nuseful for debugging, performance analysis, and continuous auditing.\nsystematically auditing models for compliance or accountability. The findings\nunderscore the need for enhanced logging practices and tooling that\nsystematically integrate responsible AI metrics. Such practices would support\nthe development of auditable, transparent, and ethically responsible ML\nsystems, aligning with growing regulatory requirements and societal\nexpectations. By highlighting specific deficiencies and opportunities, this\nwork provides actionable guidance for both practitioners and tool developers\nseeking to strengthen the accountability and trustworthiness of ML\napplications.", "AI": {"tldr": "The paper reviews logging as a tool for auditing machine learning systems, finds major gaps, and recommends integrating responsible AI metrics into logging practices to enhance transparency, accountability, and compliance.", "motivation": "Machine learning is being used more widely in decision-making systems, but there are persistent concerns about ethical and legal compliance due to lack of transparency, fairness, and accountability. Traditional software often uses logging for auditing, which could be applied to ML systems.", "method": "The study explores the use of logging practices as a way to systematically audit ML systems for compliance and accountability. It analyzes current methods, identifying deficiencies and opportunities, and emphasizes the integration of responsible AI metrics into logging.", "result": "The paper finds significant shortcomings in current logging practices and highlights the need for better tools and standards. Enhanced logging that includes responsible AI metrics would support transparency, accountability, and help meet regulatory and societal demands.", "conclusion": "The work provides actionable recommendations for practitioners and tool developers on improving logging in ML systems to achieve better auditability and responsible AI, thereby increasing accountability and trustworthiness."}}
{"id": "2508.17882", "categories": ["cs.SE", "cs.SC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.17882", "abs": "https://arxiv.org/abs/2508.17882", "authors": ["Izudin Dzafic", "Rabih A. Jabr"], "title": "modelSolver: A Symbolic Model-Driven Solver for Power Network Simulation and Monitoring", "comment": null, "summary": "The development of advanced software tools for power system analysis requires\nextensive programming expertise. Even when using open-source tools, programming\nskills are essential to modify built-in models. This can be particularly\nchallenging for domain experts who lack coding proficiency. This paper\nintroduces modelSolver, a software solution with a new framework centered\naround symbolic mathematical modeling. The proposed paradigm facilitates\ndefining models through intuitive mathematical expressions, thus eliminating\nthe need for traditional programming constructs such as arrays, loops, and\nsparse matrix computations. The modelSolver focuses on power flow and state\nestimation using an open-box approach, which allows users to specify custom\nmodels using either real or complex variables. Unlike existing tools that rely\non hard-coded models, modelSolver enables the representation of a wide range of\nadvanced functionalities, including power flow with voltage regulators and load\ntap changers, continuation power flow, and Gauss-Newton state estimation with\nequality constraints. Compatibility with MATPOWER is ensured via a converter\nthat automates importing data files. The framework prioritizes model-driven\ndevelopment and empowers domain experts to focus on power system modeling\nwithout programming barriers. It aims to simplify power system computations,\nmaking them more accessible to students, scientists, and practitioners.", "AI": {"tldr": "modelSolver introduces a programming-free framework for power system analysis, allowing easy and intuitive model creation and modification with mathematical expressions, making advanced analysis accessible to non-programmer domain experts.", "motivation": "Developing advanced power system analysis tools currently demands significant programming skills, making customization and model definition challenging for domain experts who lack coding experience.", "method": "The paper introduces 'modelSolver,' a framework based on symbolic mathematical modeling, allowing users to define models through mathematical expressions rather than traditional programming constructs. It supports power flow and state estimation, flexible model specification, and automatic data conversion for compatibility with MATPOWER.", "result": "modelSolver enables intuitive custom model creation for power systems, supports advanced functions like voltage regulators and state estimation, and removes programming barriers for users. It is accessible for students and professionals and simplifies the workflow of power system analysis.", "conclusion": "modelSolver democratizes power system modeling by allowing domain experts to create and modify advanced power system analysis models without needing programming skills, thereby making these analyses more accessible and efficient."}}
{"id": "2508.17900", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17900", "abs": "https://arxiv.org/abs/2508.17900", "authors": ["Mohammed O. Alannsary"], "title": "A Defect Classification Framework for AI-Based Software Systems (AI-ODC)", "comment": "Article, 19 pages, 6 figures, 8 tables,", "summary": "Artificial Intelligence has gained a lot of attention recently, it has been\nutilized in several fields ranging from daily life activities, such as\nresponding to emails and scheduling appointments, to manufacturing and\nautomating work activities. Artificial Intelligence systems are mainly\nimplemented as software solutions, and it is essential to discover and remove\nsoftware defects to assure its quality using defect analysis which is one of\nthe major activities that contribute to software quality. Despite the\nproliferation of AI-based systems, current defect analysis models fail to\ncapture their unique attributes. This paper proposes a framework inspired by\nthe Orthogonal Defect Classification (ODC) paradigm and enables defect analysis\nof Artificial Intelligence systems while recognizing its special attributes and\ncharacteristics. This study demonstrated the feasibility of modifying ODC for\nAI systems to classify its defects. The ODC was adjusted to accommodate the\nData, Learning, and Thinking aspects of AI systems which are newly introduced\nclassification dimensions. This adjustment involved the introduction of an\nadditional attribute to the ODC attributes, the incorporation of a new severity\nlevel, and the substitution of impact areas with characteristics pertinent to\nAI systems. The framework was showcased by applying it to a publicly available\nMachine Learning bug dataset, with results analyzed through one-way and two-way\nanalysis. The case study indicated that defects occurring during the Learning\nphase were the most prevalent and were significantly linked to high-severity\nclassifications. In contrast, defects identified in the Thinking phase had a\ndisproportionate effect on trustworthiness and accuracy. These findings\nillustrate AIODC's capability to identify high-risk defect categories and\ninform focused quality assurance measures.", "AI": {"tldr": "AI defect analysis models don't fit unique AI challenges. This paper introduces a modified ODC approach (AIODC), focusing on Data, Learning, and Thinking defects. Applied to ML bugs, Learning phase defects are most common/severe; Thinking defects harm trust/accuracy. AIODC helps identify high-risk issues and improve AI quality assurance.", "motivation": "Current defect analysis models are inadequate for AI systems because they do not account for their unique characteristics. With AI systems proliferating, it is essential to have models that can effectively classify and analyze defects specific to AI.", "method": "The paper proposes a framework based on Orthogonal Defect Classification (ODC), modified for AI systems by introducing new classification dimensions (Data, Learning, Thinking), an added attribute, a new severity level, and replacing impact areas with those relevant to AI. It applies this framework to a public Machine Learning bug dataset and analyzes the results statistically.", "result": "Defects in the Learning phase were the most common and most associated with high-severity ratings. Defects in the Thinking phase had a strong impact on trustworthiness and accuracy. The framework (AIODC) could effectively pinpoint high-risk defect categories.", "conclusion": "Modifying ODC for AI systems is feasible and effective. The AIODC framework provides valuable insights into defect categorization and can guide targeted quality assurance in AI system development."}}
{"id": "2508.17912", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.17912", "abs": "https://arxiv.org/abs/2508.17912", "authors": ["Mohammed O. Alannsary"], "title": "Evaluating Citizen Satisfaction with Saudi Arabia's E-Government Services: A Standards-Based, Theory-Informed Approach", "comment": "38 pages, 1 figure, 16 tables, journal research paper", "summary": "As digital government platforms become central to public service delivery,\nunderstanding citizen assessment is crucial for enhancing usability, trust, and\ninclusivity. This study investigates citizen satisfaction with the e-government\nservices in Saudi Arabia through a quality-in-use framework based on ISO/IEC\n25010 and ISO/IEC 25022 standards, interpreted through the lens of the Unified\nTheory of Acceptance and Use of Technology (UTAUT). A structured questionnaire\nwas administered to 500 citizens, yielding 276 valid responses. Satisfaction\nwas evaluated across four dimensions: overall satisfaction, feature\nsatisfaction, trust, and emotional engagement (pleasure). The findings\ndemonstrate consistently high levels of satisfaction regarding usability and\ntrust, aligning with Saudi Arabia's top-tier global ranking in e-government\ndevelopment. However, the results also highlight persistent challenges related\nto service clarity and system responsiveness. Emotional engagement was limited,\nindicating that users perceive these services primarily as functional tools\nrather than as engaging digital experiences. The study offers valuable insights\nfor policymakers and contributes to the theoretical integration of\nstandards-based and behavioral adoption models in the context of citizenship.", "AI": {"tldr": "Saudi citizens report high satisfaction with e-government usability and trust, but service clarity and emotional engagement need improvement. The findings can guide policymakers in refining platforms, and contribute to theory by integrating technical standards with adoption models.", "motivation": "With increasing reliance on digital government platforms for public service delivery, it is critical to understand how citizens assess these services to improve usability, trust, and inclusiveness.", "method": "The study utilizes a quality-in-use framework based on ISO/IEC 25010 and ISO/IEC 25022 standards, analyzed using the Unified Theory of Acceptance and Use of Technology (UTAUT). Data were collected via a structured questionnaire administered to 500 citizens, yielding 276 valid responses.", "result": "The study found high levels of citizen satisfaction in terms of usability and trust. Despite Saudi Arabia's strong global e-government rankings, issues remain in service clarity and system responsiveness. Emotional engagement was low, suggesting users view the platforms as practical tools rather than engaging experiences.", "conclusion": "Citizen satisfaction with Saudi e-government services is high, particularly for usability and trust. However, service clarity and responsiveness still need improvement, and digital platforms are seen as functional rather than emotionally engaging. The research provides important insights for policymakers and advances theoretical integration of standards-based and behavioral adoption models."}}
{"id": "2508.17988", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.17988", "abs": "https://arxiv.org/abs/2508.17988", "authors": ["Eduardo de Conto", "Blaise Genest", "Arvind Easwaran", "Nicholas Ng", "Shweta Menon"], "title": "DesCartes Builder: A Tool to Develop Machine-Learning Based Digital Twins", "comment": "5 pages, 4 figures. Accepted at EDTconf 2025", "summary": "Digital twins (DTs) are increasingly utilized to monitor, manage, and\noptimize complex systems across various domains, including civil engineering. A\ncore requirement for an effective DT is to act as a fast, accurate, and\nmaintainable surrogate of its physical counterpart, the physical twin (PT). To\nthis end, machine learning (ML) is frequently employed to (i) construct\nreal-time DT prototypes using efficient reduced-order models (ROMs) derived\nfrom high-fidelity simulations of the PT's nominal behavior, and (ii)\nspecialize these prototypes into DT instances by leveraging historical sensor\ndata from the target PT. Despite the broad applicability of ML, its use in DT\nengineering remains largely ad hoc. Indeed, while conventional ML pipelines\noften train a single model for a specific task, DTs typically require multiple,\ntask- and domain-dependent models. Thus, a more structured approach is required\nto design DTs.\n  In this paper, we introduce DesCartes Builder, an open-source tool to enable\nthe systematic engineering of ML-based pipelines for real-time DT prototypes\nand DT instances. The tool leverages an open and flexible visual data flow\nparadigm to facilitate the specification, composition, and reuse of ML models.\nIt also integrates a library of parameterizable core operations and ML\nalgorithms tailored for DT design. We demonstrate the effectiveness and\nusability of DesCartes Builder through a civil engineering use case involving\nthe design of a real-time DT prototype to predict the plastic strain of a\nstructure.", "AI": {"tldr": "Digital twin engineering in civil engineering needs structured ML pipelines. DesCartes Builder is an open-source tool that visually and modularly streamlines ML-based DT design. Its usefulness is demonstrated for predicting plastic strain in structural systems.", "motivation": "Digital twins are widely used in civil engineering but lack structured, systematic engineering practices when leveraging machine learning. Typical ML approaches, which train a single model for a task, do not align with the multi-model, multi-domain requirements of DTs.", "method": "The authors introduce DesCartes Builder, an open-source tool that employs a flexible visual data flow paradigm along with a library of parameterizable core operations and tailored ML algorithms for building and managing ML-based digital twin pipelines. The tool enables specification, composition, and reuse of models for real-time DT prototypes and instances.", "result": "DesCartes Builder's effectiveness and usability are demonstrated via a civil engineering use case, specifically in creating a real-time DT prototype that accurately predicts plastic strain in structures.", "conclusion": "A systematic, visual, and modular ML engineering approach is necessary for the robust development of digital twins. DesCartes Builder offers a practical solution, improving structure, reusability, and real-time efficiency in DT creation."}}
{"id": "2508.18003", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18003", "abs": "https://arxiv.org/abs/2508.18003", "authors": ["Robert Heum\u00fcller", "Frank Ortmeier"], "title": "Previously on... Automating Code Review", "comment": "Preprint currently under review", "summary": "Modern Code Review (MCR) is a standard practice in software engineering, yet\nit demands substantial time and resource investments. Recent research has\nincreasingly explored automating core review tasks using machine learning (ML)\nand deep learning (DL). As a result, there is substantial variability in task\ndefinitions, datasets, and evaluation procedures. This study provides the first\ncomprehensive analysis of MCR automation research, aiming to characterize the\nfield's evolution, formalize learning tasks, highlight methodological\nchallenges, and offer actionable recommendations to guide future research.\nFocusing on the primary code review tasks, we systematically surveyed 691\npublications and identified 24 relevant studies published between May 2015 and\nApril 2024. Each study was analyzed in terms of tasks, models, metrics,\nbaselines, results, validity concerns, and artifact availability. In\nparticular, our analysis reveals significant potential for standardization,\nincluding 48 task metric combinations, 22 of which were unique to their\noriginal paper, and limited dataset reuse. We highlight challenges and derive\nconcrete recommendations for examples such as the temporal bias threat, which\nare rarely addressed so far. Our work contributes to a clearer overview of the\nfield, supports the framing of new research, helps to avoid pitfalls, and\npromotes greater standardization in evaluation practices.", "AI": {"tldr": "A systematic review of ML/DL-based Modern Code Review automation shows widespread inconsistencies in tasks, metrics, and datasets. The authors call for deeper standardization, clearer evaluation, and provide guidance for future research.", "motivation": "Modern Code Review (MCR), essential in software engineering, requires significant time and resources. There is growing interest in automating these tasks via ML/DL, but inconsistencies exist in how tasks are defined, evaluated, and benchmarked. Thus, the field needs comprehensive analysis and recommendations.", "method": "A systematic survey of 691 publications, narrowing to 24 relevant studies on MCR automation published between May 2015 and April 2024. Each study was evaluated based on tasks, models, metrics, baselines, results, validity issues, and availability of artifacts.", "result": "The analysis found significant variability and lack of standardization in task definitions and metrics (with 48 task-metric combinations, 22 unique to a paper). Dataset reuse is limited. Methodological challenges, like temporal bias, are under-addressed. Recommendations for improved future research are provided.", "conclusion": "This study clarifies MCR automation research, highlights pitfalls, and calls for better standardization and rigorous evaluation, supporting future advances in the field."}}
{"id": "2508.18070", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18070", "abs": "https://arxiv.org/abs/2508.18070", "authors": ["Karolina M. Milano", "Wesley K. G. Assun\u00e7\u00e3o", "Bruno B. P. Cafeo"], "title": "A Large-Scale Study on Developer Engagement and Expertise in Configurable Software System Projects", "comment": null, "summary": "Modern systems operate in multiple contexts making variability a fundamental\naspect of Configurable Software Systems (CSSs). Variability, implemented via\npre-processor directives (e.g., #ifdef blocks) interleaved with other code and\nspread across files, complicates maintenance and increases error risk. Despite\nits importance, little is known about how variable code is distributed among\ndevelopers or whether conventional expertise metrics adequately capture\nvariable code proficiency. This study investigates developers' engagement with\nvariable versus mandatory code, the concentration of variable code workload,\nand the effectiveness of expertise metrics in CSS projects. We mined\nrepositories of 25 CSS projects, analyzing 450,255 commits from 9,678\ndevelopers. Results show that 59% of developers never modified variable code,\nwhile about 17% were responsible for developing and maintaining 83% of it. This\nindicates a high concentration of variable code expertise among a few\ndevelopers, suggesting that task assignments should prioritize these\nspecialists. Moreover, conventional expertise metrics performed\npoorly--achieving only around 55% precision and 50% recall in identifying\ndevelopers engaged with variable code. Our findings highlight an unbalanced\ndistribution of variable code responsibilities and underscore the need to\nrefine expertise metrics to better support task assignments in CSS projects,\nthereby promoting a more equitable workload distribution.", "AI": {"tldr": "The paper finds that a small group of developers handle most of the variable code in configurable software systems. Current expertise metrics do not accurately identify who these specialists are, pointing to a need for better metrics to improve workload distribution and task assignment.", "motivation": "Variability in configurable software systems (CSSs) is essential, but its implementation using pre-processor directives complicates maintenance and increases error risk. There is limited understanding of how variable code is distributed among developers and whether current expertise metrics can identify proficiency in handling this type of code.", "method": "The authors mined repositories of 25 CSS projects, analyzing 450,255 commits from 9,678 developers. They evaluated developers' involvement with variable vs. mandatory code, measured workload concentration, and assessed the accuracy of conventional expertise metrics in this context.", "result": "The results indicate that 59% of developers never changed variable code, while 17% handled 83% of it, highlighting a concentration of expertise. Conventional expertise metrics demonstrated only around 55% precision and 50% recall in identifying developers engaged with variable code.", "conclusion": "Variable code maintenance is concentrated among a small subset of developers, and existing expertise metrics are insufficient for distinguishing proficiency in variable code. More refined metrics are needed to better support task assignment and distribute workload more equitably in CSS projects."}}
{"id": "2508.18073", "categories": ["cs.SE", "cs.DL"], "pdf": "https://arxiv.org/pdf/2508.18073", "abs": "https://arxiv.org/abs/2508.18073", "authors": ["Joenio Marques da Costa", "Christina von Flach"], "title": "Debian in the Research Software Ecosystem: A Bibliometric Analysis", "comment": "5 pages; 3 figures; 2 tables; to be published in DebConf25 Academic\n  Track https://www.diverse-team.fr/debconf25-academictrack", "summary": "Context: The Debian system has historically participated in academic works\nand scientific projects, with well-known examples including NeuroDebian, Debian\nMed, Debsources, Debian Science, and Debian GIS, where the scientific relevance\nof Debian and its contribution to the Research Software ecosystem are evident.\n  Objective: The objective of this study is to investigate the Debian system\nthrough academic publications, with the aim of classifying articles, mapping\nresearch, identifying trends, and finding opportunities.\n  Method: The study is based on a bibliometric analysis starting with an\ninitial search for the term \"Debian\" in the titles, abstracts, or keywords of\nacademic publications, using the Scopus database. This analysis calculates\nmetrics of co-citation, co-authorship, and word co-occurrence, and is guided by\na set of research questions and criteria for inclusion and exclusion to conduct\nthe bibliometric analysis.\n  Results: The study includes a set of articles published across various fields\nof knowledge, providing a map of the academic publication space about Debian.\nThe study's data will be available in a public repository, reporting\ndemographic and bibliometric trends, including the most cited articles, active\ncountries, researchers, and popular conferences.\n  Conclusion: Results includes a bibliometric and demographic analysis\nidentified in publications about Debian, shedding light on the intellectual\nstructure of academic research. The results of the analyses can help\nresearchers gain an overview of existing trends in publications about Debian\nand identify areas that require more attention from the scientific community.", "AI": {"tldr": "The paper presents a bibliometric analysis of academic publications mentioning Debian, mapping key research trends, influential works, demographics, and areas needing further attention in Debian-related science.", "motivation": "Debian has played a significant role in scientific and academic projects, but there has not been a comprehensive study mapping this involvement through academic publications. The authors seek to systematically chart and analyze the academic landscape around Debian.", "method": "The study conducts a bibliometric analysis based on publications found by searching for 'Debian' in titles, abstracts, or keywords in the Scopus database. The analysis calculates metrics such as co-citation, co-authorship, and word co-occurrence, underpinned by predefined research questions and inclusion/exclusion criteria.", "result": "A categorized collection of articles from various scientific fields referring to Debian was compiled. The study presents a mapped overview of the relevant academic literature and demographic/bibliometric trends (e.g., influential articles, key researchers and countries, and leading conferences), with data made publicly available.", "conclusion": "This bibliometric and demographic analysis reveals the intellectual structure of Debian-related academic research, provides insights into publication trends, and highlights opportunities and underexplored areas for future research."}}
{"id": "2508.18089", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18089", "abs": "https://arxiv.org/abs/2508.18089", "authors": ["Karine Even-Mendoza", "Alexander Brownlee", "Alina Geiger", "Carol Hanna", "Justyna Petke", "Federica Sarro", "Dominik Sobania"], "title": "LLM-Guided Genetic Improvement: Envisioning Semantic Aware Automated Software Evolution", "comment": null, "summary": "Genetic Improvement (GI) of software automatically creates alternative\nsoftware versions that are improved according to certain properties of\ninterests (e.g., running-time). Search-based GI excels at navigating large\nprogram spaces, but operates primarily at the syntactic level. In contrast,\nLarge Language Models (LLMs) offer semantic-aware edits, yet lack goal-directed\nfeedback and control (which is instead a strength of GI). As such, we propose\nthe investigation of a new research line on AI-powered GI aimed at\nincorporating semantic aware search. We take a first step at it by augmenting\nGI with the use of automated clustering of LLM edits. We provide initial\nempirical evidence that our proposal, dubbed PatchCat, allows us to\nautomatically and effectively categorize LLM-suggested patches. PatchCat\nidentified 18 different types of software patches and categorized newly\nsuggested patches with high accuracy. It also enabled detecting NoOp edits in\nadvance and, prospectively, to skip test suite execution to save resources in\nmany cases. These results, coupled with the fact that PatchCat works with\nsmall, local LLMs, are a promising step toward interpretable, efficient, and\ngreen GI. We outline a rich agenda of future work and call for the community to\njoin our vision of building a principled understanding of LLM-driven mutations,\nguiding the GI search process with semantic signals.", "AI": {"tldr": "The paper introduces PatchCat, which uses automated clustering to organize and assess LLM-generated software edits in Genetic Improvement. PatchCat improves interpretability and efficiency, identifies patch types, detects ineffective edits early, and saves computational resources. This marks an important step toward goal-driven, semantic-aware software improvement using local LLMs.", "motivation": "Traditional Genetic Improvement (GI) of software is effective for optimizing performance properties but works primarily at the syntactic level. GI lacks the semantic-aware editing capabilities of Large Language Models (LLMs), which, however, are not goal-directed or easily steered for specific improvements.", "method": "The paper augments GI by incorporating automated clustering of software patches proposed by LLMs. This approach, called PatchCat, categorizes LLM-suggested patches using clustering techniques and assesses their effectiveness.", "result": "PatchCat was able to identify 18 distinct types of LLM-generated software patches and categorize newly suggested patches with high accuracy. It can detect 'NoOp' edits in advance, allowing for the possibility of skipping unnecessary test suite executions, thereby saving resources. It also works with small, local LLMs.", "conclusion": "The integration of semantic clustering of LLM edits into GI is a promising step toward more interpretable, efficient, and resource-friendly software improvement. PatchCat demonstrates initial success and opens up opportunities for deeper, principled understanding and direction of LLM-driven GI processes."}}
{"id": "2508.18106", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18106", "abs": "https://arxiv.org/abs/2508.18106", "authors": ["Keke Lian", "Bin Wang", "Lei Zhang", "Libo Chen", "Junjie Wang", "Ziming Zhao", "Yujiu Yang", "Haotong Duan", "Haoran Zhao", "Shuang Liao", "Mingda Guo", "Jiazheng Quan", "Yilu Zhong", "Chenhao He", "Zichuan Chen", "Jie Wu", "Haoling Li", "Zhaoxuan Li", "Jiongchi Yu", "Hui Li", "Dong Zhang"], "title": "A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code", "comment": null, "summary": "The increasing adoption of large language models (LLMs) in software\nengineering necessitates rigorous security evaluation of their generated code.\nHowever, existing benchmarks are inadequate, as they focus on isolated code\nsnippets, employ unstable evaluation methods that lack reproducibility, and\nfail to connect the quality of input context with the security of the output.\nTo address these gaps, we introduce A.S.E (AI Code Generation Security\nEvaluation), a benchmark for repository-level secure code generation. A.S.E\nconstructs tasks from real-world repositories with documented CVEs, preserving\nfull repository context like build systems and cross-file dependencies. Its\nreproducible, containerized evaluation framework uses expert-defined rules to\nprovide stable, auditable assessments of security, build quality, and\ngeneration stability. Our evaluation of leading LLMs on A.S.E reveals three key\nfindings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The\nsecurity gap between proprietary and open-source models is narrow;\nQwen3-235B-A22B-Instruct attains the top security score. (3) Concise,\n``fast-thinking'' decoding strategies consistently outperform complex,\n``slow-thinking'' reasoning for security patching.", "AI": {"tldr": "A.S.E is a new, realistic benchmark for evaluating LLM-generated code security at the repository level. It reveals strong performance by Claude-3.7-Sonnet and top security scores by Qwen3-235B-A22B-Instruct, with concise decoding strategies outperforming complex reasoning for patching security flaws.", "motivation": "Existing benchmarks for evaluating the security of code generated by large language models (LLMs) are inadequate. They focus only on isolated code snippets, use unstable evaluation methods that lack reproducibility, and do not consider how the quality of input context affects the security of the output code.", "method": "The authors propose A.S.E (AI Code Generation Security Evaluation), a benchmark that uses real-world repositories with known CVEs. It preserves complete repository context (including build systems and dependencies) and employs a reproducible, containerized framework with expert-defined rules for stable and auditable assessment of security, build quality, and generation stability.", "result": "Testing leading LLMs on A.S.E led to three major findings: (1) Claude-3.7-Sonnet provided the best overall performance; (2) The security differences between proprietary and open-source models were small, with Qwen3-235B-A22B-Instruct achieving the highest security score; (3) Simple, \"fast-thinking\" decoding strategies outperformed more complex reasoning methods in security patching.", "conclusion": "A.S.E offers a more realistic and robust benchmark for assessing the security of LLM-generated code at the repository level, revealing gaps and differences among current LLMs and highlighting the strengths of concise decoding strategies."}}
