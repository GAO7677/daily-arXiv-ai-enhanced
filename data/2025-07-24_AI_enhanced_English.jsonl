{"id": "2507.17049", "categories": ["cs.SE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17049", "abs": "https://arxiv.org/abs/2507.17049", "authors": ["Pablo Valle", "Chengjie Lu", "Shaukat Ali", "Aitor Arrieta"], "title": "Evaluating Uncertainty and Quality of Visual Language Action-enabled Robots", "comment": null, "summary": "Visual Language Action (VLA) models are a multi-modal class of Artificial\nIntelligence (AI) systems that integrate visual perception, natural language\nunderstanding, and action planning to enable agents to interpret their\nenvironment, comprehend instructions, and perform embodied tasks autonomously.\nRecently, significant progress has been made to advance this field. These kinds\nof models are typically evaluated through task success rates, which fail to\ncapture the quality of task execution and the mode's confidence in its\ndecisions. In this paper, we propose eight uncertainty metrics and five quality\nmetrics specifically designed for VLA models for robotic manipulation tasks. We\nassess their effectiveness through a large-scale empirical study involving 908\nsuccessful task executions from three state-of-the-art VLA models across four\nrepresentative robotic manipulation tasks. Human domain experts manually\nlabeled task quality, allowing us to analyze the correlation between our\nproposed metrics and expert judgments. The results reveal that several metrics\nshow moderate to strong correlation with human assessments, highlighting their\nutility for evaluating task quality and model confidence. Furthermore, we found\nthat some of the metrics can discriminate between high-, medium-, and\nlow-quality executions from unsuccessful tasks, which can be interesting when\ntest oracles are not available. Our findings challenge the adequacy of current\nevaluation practices that rely solely on binary success rates and pave the way\nfor improved real-time monitoring and adaptive enhancement of VLA-enabled\nrobotic systems.", "AI": {"tldr": "The paper introduces new uncertainty and quality metrics for VLA models in robotics, demonstrating that these offer a much better evaluation of task execution and model confidence than simple success rates, and suggests such metrics should supplement or replace conventional evaluation methods.", "motivation": "Current evaluation methods for Visual Language Action (VLA) models focus mainly on binary task success rates. These measures do not adequately capture how well tasks are executed or how confident the model is in its decisions. There is a need for better evaluation metrics that provide a richer understanding of model performance, especially for complex robotic manipulation tasks.", "method": "The authors propose eight uncertainty metrics and five quality metrics specifically designed to evaluate VLA models in robotic manipulation contexts. They conduct a large-scale empirical study, evaluating 908 successful task executions from three state-of-the-art VLA models across four types of manipulation tasks. Human experts manually label the quality of each task execution, enabling correlation analysis between the proposed metrics and expert judgments.", "result": "Several of the proposed metrics show moderate to strong correlation with expert human judgments of task quality. Some of these metrics can effectively distinguish between high-, medium-, and low-quality task executions, even in the absence of a traditional success/failure test oracle.", "conclusion": "Reliance solely on task success rates is inadequate for evaluating VLA models. The proposed metrics enable more nuanced assessment of both task quality and model confidence, supporting improved real-time monitoring and adaptive improvements in VLA-based robotic systems."}}
{"id": "2507.17093", "categories": ["cs.SE", "68N30", "D.2.4; D.2.5; D.2.8"], "pdf": "https://arxiv.org/pdf/2507.17093", "abs": "https://arxiv.org/abs/2507.17093", "authors": ["Danushka Liyanage", "Nelum Attanayake", "Zijian Luo", "Rahul Gopinath"], "title": "Assessing Reliability of Statistical Maximum Coverage Estimators in Fuzzing", "comment": "ICSME'25 Registered Report", "summary": "Background: Fuzzers are often guided by coverage, making the estimation of\nmaximum achievable coverage a key concern in fuzzing. However, achieving 100%\ncoverage is infeasible for most real-world software systems, regardless of\neffort. While static reachability analysis can provide an upper bound, it is\noften highly inaccurate. Recently, statistical estimation methods based on\nspecies richness estimators from biostatistics have been proposed as a\npotential solution. Yet, the lack of reliable benchmarks with labeled ground\ntruth has limited rigorous evaluation of their accuracy.\n  Objective: This work examines the reliability of reachability estimators from\ntwo axes: addressing the lack of labeled ground truth and evaluating their\nreliability on real-world programs.\n  Methods: (1) To address the challenge of labeled ground truth, we propose an\nevaluation framework that synthetically generates large programs with complex\ncontrol flows, ensuring well-defined reachability and providing ground truth\nfor evaluation. (2) To address the criticism from use of synthetic benchmarks,\nwe adapt a reliability check for reachability estimators on real-world\nbenchmarks without labeled ground truth -- by varying the size of sampling\nunits, which, in theory, should not affect the estimate.\n  Results: These two studies together will help answer the question of whether\ncurrent reachability estimators are reliable, and defines a protocol to\nevaluate future improvements in reachability estimation.", "AI": {"tldr": "This paper addresses the problem of estimating maximum achievable coverage in fuzz testing, introduces a synthetic benchmark framework to provide ground truth, and applies a new evaluation protocol to gauge the reliability of reachability estimators both on synthetic and on real-world programs.", "motivation": "In fuzz testing, coverage-guided strategies are central, but accurately estimating the maximum possible code coverage is a challenge, especially since 100% coverage is unrealistic in complex real-world software. Existing static reachability analysis is not accurate enough, and while statistical methods have been proposed, their evaluation is limited by the lack of ground truth benchmarks.", "method": "The paper proposes two main methods: (1) Developing a synthetic evaluation framework that generates large, complex programs with exactly known reachability, thus providing labeled ground truth for evaluating reachability estimators; (2) Applying a new reliability check on real-world programs by varying sampling unit sizes to see whether reachability estimates remain stable, circumventing the lack of ground truth in these scenarios.", "result": "These methods together assess the reliability of current reachability estimators and establish an evaluation protocol for future developments. The study reveals whether these estimators are dependable both on synthetic benchmarks with ground truth and on real-world software without it.", "conclusion": "The research clarifies the strengths and limitations of current reachability estimation approaches, delivers useful tools and protocols for their assessment, and lays a foundation to improve estimation methods in fuzzing practice."}}
{"id": "2507.17165", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.17165", "abs": "https://arxiv.org/abs/2507.17165", "authors": ["Taher A. Ghaleb", "Dulina Rathnayake"], "title": "Can LLMs Write CI? A Study on Automatic Generation of GitHub Actions Configurations", "comment": "Accepted at the 41st IEEE International Conference on Software\n  Maintenance and Evolution 2025 (ICSME'25)", "summary": "Continuous Integration (CI) services, such as GitHub Actions, require\ndevelopers to write YAML-based configurations, which can be tedious and\nerror-prone. Despite the increasing use of Large Language Models (LLMs) to\nautomate software engineering tasks, their ability to generate CI\nconfigurations remains underexplored. This paper presents a preliminary study\nevaluating six LLMs for generating GitHub Actions configurations from natural\nlanguage descriptions. We assess three general-purpose foundation models\n(GPT-4o, Llama, and Gemma) and three code-pretrained models (GPT-4.1, Code\nLlama, and CodeGemma). We also introduce the first labeled dataset of its kind,\nconstructed from GitHub Actions documentation, pairing descriptions with\ncorresponding best-practice YAML configurations. Zero-shot prompting achieves\nup to 69% similarity with the ground truth, with only 3% perfect matches.\nCode-pretrained models slightly underperform compared to general-purpose ones\nin YAML-based CI tasks, revealing LLM limitations for CI configuration\ngeneration. Analyzing GPT-4o outputs reveals issues like missing or renamed\nsteps, misinterpreted descriptions, and unnecessary additions that may affect\nstructural and contextual correctness, indicating a gap between generation\nquality and the precision required for executable CI configurations. Our\nresearch offers insights for improving LLM alignment with configuration\nlanguages and guiding future efforts on CI automation and tooling support.", "AI": {"tldr": "LLMs struggle to generate precise GitHub Actions CI YAML from natural language; code-pretrained models don't outperform generalist ones, and accuracy remains limited, indicating the need for more targeted model improvements for CI automation.", "motivation": "Writing YAML-based continuous integration (CI) configurations for services like GitHub Actions is tedious and error-prone. Though large language models (LLMs) are widely used in software engineering automation, their performance in generating CI configurations has not been thoroughly studied.", "method": "The authors conducted a preliminary evaluation of six LLMs\u2014including three general-purpose models (GPT-4o, Llama, Gemma) and three code-pretrained models (GPT-4.1, Code Llama, CodeGemma)\u2014for generating GitHub Actions YAML configurations based on natural language descriptions. They created a unique labeled dataset pairing textual descriptions and best-practice YAML, and used zero-shot prompting to assess model outputs against ground truth.", "result": "Zero-shot prompting produced up to 69% similarity with ground truth YAML, but only 3% perfect matches. Code-pretrained models performed slightly worse than general-purpose models for this task. Common issues in generated configurations included missing or renamed steps, incorrect interpretation of descriptions, and unnecessary additions.", "conclusion": "Current LLMs, especially code-pretrained ones, have notable limitations for generating accurate CI configurations, with frequent structural and contextual errors. There is a significant gap between generated quality and the precision needed for working CI setups. This suggests a need for better LLM alignment with configuration languages and improved CI automation tools."}}
{"id": "2507.17235", "categories": ["cs.SE", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.17235", "abs": "https://arxiv.org/abs/2507.17235", "authors": ["Andriy Miranskyy", "Jos\u00e9 Campos", "Anila Mjeda", "Lei Zhang", "Ignacio Garc\u00eda Rodr\u00edguez de Guzm\u00e1n"], "title": "On the Feasibility of Quantum Unit Testing", "comment": null, "summary": "The increasing complexity of quantum software presents significant challenges\nfor software verification and validation, particularly in the context of unit\ntesting. This work presents a comprehensive study on quantum-centric unit\ntests, comparing traditional statistical approaches with tests specifically\ndesigned for quantum circuits. These include tests that run only on a classical\ncomputer, such as the Statevector test, as well as those executable on quantum\nhardware, such as the Swap test and the novel Inverse test. Through an\nempirical study and detailed analysis on 1,796,880 mutated quantum circuits, we\ninvestigate (a) each test's ability to detect subtle discrepancies between the\nexpected and actual states of a quantum circuit, and (b) the number of\nmeasurements required to achieve high reliability. The results demonstrate that\nquantum-centric tests, particularly the Statevector test and the Inverse test,\nprovide clear advantages in terms of precision and efficiency, reducing both\nfalse positives and false negatives compared to statistical tests. This work\ncontributes to the development of more robust and scalable strategies for\ntesting quantum software, supporting the future adoption of fault-tolerant\nquantum computers and promoting more reliable practices in quantum software\nengineering.", "AI": {"tldr": "Quantum-centric unit tests (Statevector and Inverse tests) offer significant improvements over traditional statistical approaches in testing quantum software, delivering greater accuracy and efficiency for both classical and quantum hardware environments.", "motivation": "Quantum software is becoming more complex, making verification and validation\u2014especially unit testing\u2014more challenging. Traditional statistical testing methods may not be well-suited for quantum circuits, necessitating new and more effective testing approaches.", "method": "The paper conducts a comprehensive empirical study, analyzing 1,796,880 mutated quantum circuits with different types of unit tests: traditional statistical methods and quantum-centric techniques (Statevector, Swap, and a novel Inverse test). The analysis focuses on each test's ability to detect discrepancies between expected and actual quantum states and evaluates the number of measurements needed for reliable results.", "result": "Quantum-centric tests, particularly the Statevector and Inverse tests, outperform traditional statistical tests. They offer higher precision and efficiency, with lower rates of false positives and false negatives. These tests require fewer measurements to reach high reliability.", "conclusion": "Quantum-centric unit tests are more effective than classical statistical tests in terms of both accuracy and efficiency for quantum software validation. These results pave the way for more robust, scalable testing strategies that support the future of reliable quantum software and hardware."}}
{"id": "2507.17233", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.17233", "abs": "https://arxiv.org/abs/2507.17233", "authors": ["Marco Ciccal\u00e8", "Daniel Jurjo-Rivas", "Jose F. Morales", "Pedro L\u00f3pez-Garc\u00eda", "Manuel V. Hermenegildo"], "title": "Hiord: An Approach to the Specification and Verification of Higher-Order (C)LP Programs", "comment": "Accepted for publication in Theory and Practice of Logic Programming\n  (TPLP)", "summary": "Higher-order constructs enable more expressive and concise code by allowing\nprocedures to be parameterized by other procedures. Assertions allow expressing\npartial program specifications, which can be verified either at compile time\n(statically) or run time (dynamically). In higher-order programs, assertions\ncan also describe higher-order arguments. While in the context of (C)LP,\nrun-time verification of higher-order assertions has received some attention,\ncompile-time verification remains relatively unexplored. We propose a novel\napproach for statically verifying higher-order (C)LP programs with higher-order\nassertions. Although we use the Ciao assertion language for illustration, our\napproach is quite general and we believe is applicable to similar contexts.\nHigher-order arguments are described using predicate properties -- a special\nkind of property which exploits the (Ciao) assertion language. We refine the\nsyntax and semantics of these properties and introduce an abstract criterion to\ndetermine conformance to a predicate property at compile time, based on a\nsemantic order relation comparing the predicate property with the predicate\nassertions. We then show how to handle these properties using an abstract\ninterpretation-based static analyzer for programs with first-order assertions\nby reducing predicate properties to first-order properties. Finally, we report\non a prototype implementation and evaluate it through various examples within\nthe Ciao system.", "AI": {"tldr": "This paper introduces a static analysis technique for verifying higher-order assertions in (constraint) logic programming. By refining and reducing these properties, the method allows compile-time checking, demonstrated effectively with a prototype in the Ciao system.", "motivation": "Higher-order programming enables more expressive code, while assertions add specification and verification support. However, static (compile-time) verification of higher-order assertions\u2014especially in constraint logic programming ((C)LP)\u2014remains underdeveloped. The paper addresses this gap to improve program correctness and reliability.", "method": "The paper introduces a new approach for statically verifying higher-order (C)LP programs with higher-order assertions. The method uses the Ciao assertion language and extends its predicate properties. The approach refines the syntax and semantics of predicate properties and devises an abstract semantic criterion to check conformance at compile time. It employs an abstract interpretation-based static analyzer where higher-order predicates are reduced to first-order properties, making verification tractable.", "result": "The authors present a prototype implementation of their method within the Ciao system and demonstrate its effectiveness using several example programs. The evaluation shows that higher-order predicate properties can be soundly reduced and checked in a static analyzer for first-order assertions.", "conclusion": "The proposed approach effectively enables compile-time verification of higher-order assertions in (C)LP, leveraging and extending the existing Ciao assertion language. The technique is general enough to be applicable to similar assertion languages and environments that support higher-order programming."}}
{"id": "2507.17264", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.17264", "abs": "https://arxiv.org/abs/2507.17264", "authors": ["Jenny T. Liang", "Chenyang Yang", "Agnia Sergeyuk", "Travis D. Breaux", "Brad A. Myers"], "title": "Understanding Prompt Programming Tasks and Questions", "comment": null, "summary": "Prompting foundation models (FMs) like large language models (LLMs) have\nenabled new AI-powered software features (e.g., text summarization) that\npreviously were only possible by fine-tuning FMs. Now, developers are embedding\nprompts in software, known as prompt programs. The process of prompt\nprogramming requires the developer to make many changes to their prompt. Yet,\nthe questions developers ask to update their prompt is unknown, despite the\nanswers to these questions affecting how developers plan their changes. With\nthe growing number of research and commercial prompt programming tools, it is\nunclear whether prompt programmers' needs are being adequately addressed. We\naddress these challenges by developing a taxonomy of 25 tasks prompt\nprogrammers do and 51 questions they ask, measuring the importance of each task\nand question. We interview 16 prompt programmers, observe 8 developers make\nprompt changes, and survey 50 developers. We then compare the taxonomy with 48\nresearch and commercial tools. We find that prompt programming is not\nwell-supported: all tasks are done manually, and 16 of the 51 questions --\nincluding a majority of the most important ones -- remain unanswered. Based on\nthis, we outline important opportunities for prompt programming tools.", "AI": {"tldr": "Prompt programming for AI models involves many complex, unsupported manual tasks. Developers face unanswered key questions, and current tools do not meet their needs, revealing substantial opportunities for better tool development.", "motivation": "The motivation of the paper is to understand the process and challenges of prompt programming for foundation models (FMs), as the practice becomes increasingly widespread with the advent of new AI-powered software features. The authors aim to uncover the specific tasks developers undertake and the questions they consider when updating prompts, to identify whether current tools support these needs.", "method": "The authors develop a taxonomy of 25 tasks and 51 questions that prompt programmers encounter by interviewing 16 prompt programmers, observing 8 developers as they make prompt changes, and surveying 50 developers. They also compare the identified taxonomy against 48 existing research and commercial prompt programming tools to assess the adequacy of current tool support.", "result": "The study finds that all prompt programming tasks are carried out manually by developers, and 16 out of 51 questions, including many of the most important ones, remain unanswered and unsupported by current tooling. This demonstrates significant gaps in the existing landscape of prompt programming tools.", "conclusion": "Prompt programming as it currently stands is not well-supported by existing research and commercial tools. There are significant manual burdens for developers, and critical needs are not being addressed. The authors highlight key opportunities for improving prompt programming tools to better serve developers\u2019 needs."}}
{"id": "2507.17691", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.17691", "abs": "https://arxiv.org/abs/2507.17691", "authors": ["Shan Jiang", "Pranoy Kovuri", "David Tao", "Zhixun Tan"], "title": "CASCADE: LLM-Powered JavaScript Deobfuscator at Google", "comment": null, "summary": "Software obfuscation, particularly prevalent in JavaScript, hinders code\ncomprehension and analysis, posing significant challenges to software testing,\nstatic analysis, and malware detection. This paper introduces CASCADE, a novel\nhybrid approach that integrates the advanced coding capabilities of Gemini with\nthe deterministic transformation capabilities of a compiler Intermediate\nRepresentation (IR), specifically JavaScript IR (JSIR). By employing Gemini to\nidentify critical prelude functions, the foundational components underlying the\nmost prevalent obfuscation techniques, and leveraging JSIR for subsequent code\ntransformations, CASCADE effectively recovers semantic elements like original\nstrings and API names, and reveals original program behaviors. This method\novercomes limitations of existing static and dynamic deobfuscation techniques,\neliminating hundreds to thousands of hardcoded rules while achieving\nreliability and flexibility. CASCADE is already deployed in Google's production\nenvironment, demonstrating substantial improvements in JavaScript deobfuscation\nefficiency and reducing reverse engineering efforts.", "AI": {"tldr": "CASCADE is a new hybrid system that uses AI and compiler techniques to automatically deobfuscate JavaScript code, already improving real-world workflows at Google by making this process more efficient and less dependent on manual rule-writing.", "motivation": "Software obfuscation, especially in JavaScript, makes understanding and analyzing code extremely difficult, which affects software testing, static analysis, and malware detection processes.", "method": "The authors propose CASCADE, a hybrid approach that combines the AI-based capabilities of Gemini to identify key functions used in obfuscation, with the deterministic transformation capabilities of the JavaScript Intermediate Representation (JSIR) to transform and deobfuscate code. This integration allows for automated and robust code semantic recovery.", "result": "CASCADE is shown to effectively recover original semantic elements from obfuscated JavaScript code, such as strings and API names, and to reveal original program behaviors. It eliminates the need for extensive hardcoded rules and demonstrates increased efficiency and flexibility. CASCADE has been successfully deployed in a production environment at Google, resulting in substantial improvements in deobfuscation efficiency and decreased reverse engineering effort.", "conclusion": "CASCADE advances the field of JavaScript deobfuscation by offering a robust, flexible, and scalable solution that overcomes key limitations in static and dynamic approaches. Its deployment in production shows real-world impact, enhancing both efficiency and effectiveness for deobfuscation tasks."}}
{"id": "2507.17270", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.17270", "abs": "https://arxiv.org/abs/2507.17270", "authors": ["Alessandro Aneggi", "Andrea Janes"], "title": "Lessons from a Big-Bang Integration: Challenges in Edge Computing and Machine Learning", "comment": "Accepted @ XP2025 Poster session", "summary": "This experience report analyses a one year project focused on building a\ndistributed real-time analytics system using edge computing and machine\nlearning. The project faced critical setbacks due to a big-bang integration\napproach, where all components developed by multiple geographically dispersed\npartners were merged at the final stage. The integration effort resulted in\nonly six minutes of system functionality, far below the expected 40 minutes.\nThrough root cause analysis, the study identifies technical and organisational\nbarriers, including poor communication, lack of early integration testing, and\nresistance to topdown planning. It also considers psychological factors such as\na bias toward fully developed components over mockups. The paper advocates for\nearly mock based deployment, robust communication infrastructures, and the\nadoption of topdown thinking to manage complexity and reduce risk in reactive,\ndistributed projects. These findings underscore the limitations of traditional\nAgile methods in such contexts and propose simulation-driven engineering and\nstructured integration cycles as key enablers for future success.", "AI": {"tldr": "A distributed analytics project failed due to a big-bang integration and lack of early testing, yielding only 6 out of 40 expected functional minutes. The analysis highlights the limits of Agile in complex settings, stressing the need for early mockups, better communication, and structured integration for future success.", "motivation": "The motivation behind this paper is to examine the challenges and failures encountered during the development of a distributed real-time analytics system using edge computing and machine learning, specifically focusing on issues arising from a big-bang integration approach.", "method": "The paper employs an experience report and root cause analysis to investigate the technical, organisational, and psychological factors leading to the project's setbacks.", "result": "The project resulted in only six minutes of system functionality, instead of the expected 40 minutes. The root cause analysis revealed barriers such as poor communication, lack of early integration testing, and resistance to topdown planning. Psychological bias towards fully developed components over mockups also contributed to the problems.", "conclusion": "The study concludes that traditional Agile methods have limitations in complex, reactive distributed projects. It advocates for early mock-based deployment, robust communication, topdown planning, simulation-driven engineering, and structured integration cycles as solutions to reduce risk and manage complexity."}}
{"id": "2507.17271", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.17271", "abs": "https://arxiv.org/abs/2507.17271", "authors": ["Shuaiyu Zhou", "Zhengran Zeng", "Xiaoling Zhou", "Rui Xie", "Shikun Zhang", "Wei Ye"], "title": "Seed&Steer: Guiding Large Language Models with Compilable Prefix and Branch Signals for Unit Test Generation", "comment": null, "summary": "Unit tests play a vital role in the software development lifecycle. Recent\nadvances in Large Language Model (LLM)-based approaches have significantly\nimproved automated test generation, garnering attention from both academia and\nindustry. We revisit LLM-based unit test generation from a novel perspective by\ndecoupling prefix generation and assertion generation. To characterize their\nrespective challenges, we define Initialization Complexity and adopt Cyclomatic\nComplexity to measure the difficulty of prefix and assertion generation,\nrevealing that the former primarily affects compilation success, while the\nlatter influences test coverage. To address these challenges, we propose\nSeed&Steer, a two-step approach that combines traditional unit testing\ntechniques with the capabilities of large language models. Seed&Steer leverages\nconventional unit testing tools (e.g., EvoSuite) to generate method invocations\nwith high compilation success rates, which serve as seeds to guide LLMs in\nconstructing effective test contexts. It then introduces branching cues to help\nLLMs explore diverse execution paths (e.g., normal, boundary, and exception\ncases) and generate assertions with high coverage. We evaluate Seed&Steer on\nfive real-world Java projects against state-of-the-art baselines. Results show\nthat Seed&Steer improves the compilation pass rate by approximately 7%,\nsuccessfully compiling 792 and 887 previously failing cases on two LLMs. It\nalso achieves up to ~73% branch and line coverage across focal methods of\nvarying complexity, with coverage improvements ranging from 1.09* to 1.26*. Our\ncode, dataset, and experimental scripts will be publicly released to support\nfuture research and reproducibility.", "AI": {"tldr": "Seed&Steer, a hybrid approach combining traditional testing tools and LLMs, significantly boosts the success and coverage of automated unit test generation by decoupling and refining prefix and assertion creation.", "motivation": "Unit tests are crucial for software quality, but current LLM-based test generation approaches face challenges in generating both the invocation setup (prefix) and assertions within test cases. The authors aim to better understand and overcome these challenges to improve effectiveness and reliability of automated unit test generation.", "method": "The authors decouple unit test generation into prefix (method invocation setup) and assertion generation, analyzing their difficulties via Initialization Complexity (for prefixes) and Cyclomatic Complexity (for assertions). They introduce Seed&Steer\u2014a two-step process that first uses traditional tools like EvoSuite to generate high-compilation-rate prefixes as seeds, then leverages LLMs guided through branching cues to generate diverse, high-coverage assertions.", "result": "Seed&Steer was evaluated on five real-world Java projects using two LLMs against strong baselines. The approach achieved a 7% higher compilation pass rate and succeeded in compiling 792 and 887 previously failed tests. It also reached up to approximately 73% branch and line coverage, with relative improvements between 1.09x and 1.26x over baselines.", "conclusion": "Decoupling prefix and assertion generation, and combining traditional tools with LLMs in the proposed Seed&Steer framework, addresses key difficulties in automated unit test generation, leading to more compilable and high-coverage tests. The work advances automated software testing by providing open-source tools and data for reproducibility."}}
{"id": "2507.17293", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17293", "abs": "https://arxiv.org/abs/2507.17293", "authors": ["Saiful Khan", "Joyraj Chakraborty", "Philip Beaucamp", "Niraj Bhujel", "Min Chen"], "title": "Data Virtualization for Machine Learning", "comment": null, "summary": "Nowadays, machine learning (ML) teams have multiple concurrent ML workflows\nfor different applications. Each workflow typically involves many experiments,\niterations, and collaborative activities and commonly takes months and\nsometimes years from initial data wrangling to model deployment.\nOrganizationally, there is a large amount of intermediate data to be stored,\nprocessed, and maintained. \\emph{Data virtualization} becomes a critical\ntechnology in an infrastructure to serve ML workflows. In this paper, we\npresent the design and implementation of a data virtualization service,\nfocusing on its service architecture and service operations. The infrastructure\ncurrently supports six ML applications, each with more than one ML workflow.\nThe data virtualization service allows the number of applications and workflows\nto grow in the coming years.", "AI": {"tldr": "This paper introduces a scalable data virtualization service designed to manage and streamline large amounts of intermediate data from concurrent ML workflows, demonstrating its effectiveness across several applications and promising future growth.", "motivation": "Managing multiple concurrent ML workflows for different applications results in significant intermediate data storage, processing, and maintenance challenges. Data virtualization technology is necessary to address these organizational needs and support efficient collaboration and iteration within ML teams.", "method": "The paper presents the design and implementation of a data virtualization service, detailing its service architecture and operational aspects. The infrastructure is evaluated across six ML applications, each featuring multiple workflows.", "result": "The data virtualization service successfully supports six ML applications, each with several workflows. The service is scalable, enabling the addition of more applications and workflows in the future.", "conclusion": "The data virtualization service proves essential for managing large, complex ML workflows, facilitating data management, and meeting the growing needs of ML teams by providing a scalable and efficient infrastructure."}}
