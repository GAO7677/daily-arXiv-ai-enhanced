{"id": "2508.15109", "categories": ["cs.PL", "D.3.0; F.3.1"], "pdf": "https://arxiv.org/pdf/2508.15109", "abs": "https://arxiv.org/abs/2508.15109", "authors": ["Ziteng Wang", "Ruijie Fang", "Linus Zheng", "Dixin Tang", "Isil Dillig"], "title": "Homomorphism Calculus for User-Defined Aggregations", "comment": null, "summary": "Data processing frameworks like Apache Spark and Flink provide built-in\nsupport for user-defined aggregation functions (UDAFs), enabling the\nintegration of domain-specific logic. However, for these frameworks to support\n\\emph{efficient} UDAF execution, the function needs to satisfy a\n\\emph{homomorphism property}, which ensures that partial results from\nindependent computations can be merged correctly. Motivated by this problem,\nthis paper introduces a novel \\emph{homomorphism calculus} that can both verify\nand refute whether a UDAF is a dataframe homomorphism. If so, our calculus also\nenables the construction of a corresponding merge operator which can be used\nfor incremental computation and parallel execution. We have implemented an\nalgorithm based on our proposed calculus and evaluate it on real-world UDAFs,\ndemonstrating that our approach significantly outperforms two leading\nsynthesizers.", "AI": {"tldr": "This paper introduces a homomorphism calculus to automatically verify if user-defined aggregation functions (UDAFs) used in frameworks like Spark and Flink can be executed efficiently in parallel. Their algorithm not only identifies suitable UDAFs but also constructs merge operators, and achieves better performance than current leading solutions.", "motivation": "Efficient execution of user-defined aggregation functions (UDAFs) in data processing frameworks requires satisfying the homomorphism property, which is essential for merging partial results in parallel and incremental computations. Ensuring and automating the detection of this property is challenging.", "method": "The paper introduces a novel homomorphism calculus that can verify or refute if a UDAF is a dataframe homomorphism, and constructs corresponding merge operators for qualifying UDAFs. An algorithm based on this calculus is implemented and evaluated.", "result": "The proposed calculus-based algorithm was tested on real-world UDAFs and shown to significantly outperform two leading synthesizers.", "conclusion": "The homomorphism calculus enables more effective and automated verification of UDAFs for homomorphism properties, thus facilitating their efficient use in data processing frameworks."}}
{"id": "2508.15137", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15137", "abs": "https://arxiv.org/abs/2508.15137", "authors": ["Ruijie Fang", "Zachary Kincaid", "Thomas Reps"], "title": "Software Model Checking via Summary-Guided Search (Extended Version)", "comment": "Preliminary manuscript of extended version of paper that will appear\n  in OOPSLA 2025. 36 pages", "summary": "In this work, we describe a new software model-checking algorithm called GPS.\nGPS treats the task of model checking a program as a directed search of the\nprogram states, guided by a compositional, summary-based static analysis. The\nsummaries produced by static analysis are used both to prune away infeasible\npaths and to drive test generation to reach new, unexplored program states. GPS\ncan find both proofs of safety and counter-examples to safety (i.e., inputs\nthat trigger bugs), and features a novel two-layered search strategy that\nrenders it particularly efficient at finding bugs in programs featuring long,\ninput-dependent error paths. To make GPS refutationally complete (in the sense\nthat it will find an error if one exists, if it is allotted enough time), we\nintroduce an instrumentation technique and show that it helps GPS achieve\nrefutation-completeness without sacrificing overall performance. We benchmarked\nGPS on a suite of benchmarks including both programs from the Software\nVerification Competition (SV-COMP) and from prior literature, and found that\nour implementation of GPS outperforms state-of-the-art software model checkers\n(including the top performers in SV-COMP ReachSafety-Loops category), both in\nterms of the number of benchmarks solved and in terms of running time.", "AI": {"tldr": "GPS is a new software model checker using static analysis summaries for guided search and test generation. It efficiently finds bugs, especially in complex programs, and outperforms leading tools in accuracy and speed.", "motivation": "Traditional software model checking can be inefficient, especially for programs with complex, input-dependent error paths. There is a need for more scalable and effective model-checking algorithms that can efficiently prove safety or find bugs in complex software.", "method": "The paper introduces GPS, a novel software model-checking algorithm. GPS employs a compositional, summary-based static analysis to guide its search of program states. It uses these summaries for pruning infeasible paths and for generating new test cases, thereby exploring new program states. GPS features a two-layered search strategy and introduces an instrumentation technique to ensure refutational completeness.", "result": "GPS was benchmarked on a suite of programs from SV-COMP and past literature. It outperformed state-of-the-art model checkers (including top SV-COMP performers), solving more benchmarks and doing so faster.", "conclusion": "GPS offers an efficient and effective approach to software model checking, excelling at finding deep, input-dependent bugs while also capable of proving safety. Its two-layered search and static analysis summaries enable superior performance and completeness compared to existing tools."}}
{"id": "2508.15157", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.15157", "abs": "https://arxiv.org/abs/2508.15157", "authors": ["David M Kahn", "Jan Hoffmann", "Runming Li"], "title": "Big-Stop Semantics: A Simple Way to Get the Benefits of Small-Step Semantics in a Big-Step Judgment", "comment": "26 pages, 27 figures", "summary": "As evident in the programming language literature, many practitioners favor\nspecifying dynamic program behavior using big-step over small-step semantics.\nUnlike small-step semantics, which must dwell on every intermediate program\nstate, big-step semantics conveniently jump directly to the ever-important\nresult of the computation. Big-step semantics also typically involve fewer\ninference rules than their small-step counterparts. However, in exchange for\nergonomics, big-step semantics give up power: Small-step semantics describes\nprogram behaviors that are outside the grasp of big-step semantics, notably\ndivergence. This work presents a little-known extension of big-step semantics\nwith inductive definitions that captures diverging computations without\nintroducing error states. This big-stop semantics is illustrated for typed,\nuntyped, and effectful variants of PCF, as well as a while-loop-based\nimperative language. Big-stop semantics extends the standard big-step inference\nrules with a few additional rules to define an evaluation judgment that is\nequivalent to the reflexive-transitive closure of small-step transitions. This\nsimple extension contrasts with other solutions in the literature which\nsacrifice ergonomics by introducing many additional inference rules, global\nstate, and/or less-commonly-understood reasoning principles like coinduction.", "AI": {"tldr": "The paper presents big-stop semantics, a simple extension to big-step semantics that can describe diverging computations like small-step semantics, without sacrificing its usual simplicity.", "motivation": "Practitioners prefer big-step semantics for their ergonomic advantages, but these semantics traditionally lack the ability to capture certain program behaviors, especially divergence, which small-step semantics can describe.", "method": "The paper introduces an extension called big-stop semantics, which adds a few inductive rules to the standard big-step inference rules, allowing the semantics to capture diverging computations without error states. This method is applied to various forms of PCF and an imperative language.", "result": "Big-stop semantics successfully extends big-step semantics to account for divergent computations, making it equivalent to the reflexive-transitive closure of small-step transitions. The approach achieves this with minimal changes, preserving ergonomics and avoiding complex alternatives like coinduction or excessive additional rules.", "conclusion": "Big-stop semantics provides an elegant and ergonomic solution to capturing divergence in big-step semantics, addressing a known limitation of traditional big-step approaches and simplifying the specification of dynamic program behaviors."}}
{"id": "2508.15166", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.15166", "abs": "https://arxiv.org/abs/2508.15166", "authors": ["Jingbo Wang", "Shashin Halalingaiah", "Weiyi Chen", "Chao Wang", "Isil Dillig"], "title": "Probabilistic Inference for Datalog with Correlated Inputs", "comment": "Accepted for publication at OOPSLA 2025 (R2)", "summary": "Probabilistic extensions of logic programming languages, such as ProbLog,\nintegrate logical reasoning with probabilistic inference to evaluate\nprobabilities of output relations; however, prior work does not account for\npotential statistical correlations among input facts. This paper introduces\nPraline, a new extension to Datalog designed for precise probabilistic\ninference in the presence of (partially known) input correlations. We formulate\nthe inference task as a constrained optimization problem, where the solution\nyields sound and precise probability bounds for output facts. However, due to\nthe complexity of the resulting optimization problem, this approach alone often\ndoes not scale to large programs. To address scalability, we propose a more\nefficient $\\delta$-exact inference algorithm that leverages constraint solving,\nstatic analysis, and iterative refinement. Our empirical evaluation on\nchallenging real-world benchmarks, including side-channel analysis,\ndemonstrates that our method not only scales effectively but also delivers\ntight probability bounds.", "AI": {"tldr": "This paper presents Praline, an extension to Datalog designed for accurate probabilistic inference with correlated input facts. It frames inference as a constrained optimization problem and introduces a scalable \u03b4-exact algorithm. Empirical results show improved scalability and precision on complex, real-world tasks.", "motivation": "Existing probabilistic logic programming languages cannot handle statistical correlations among input facts, limiting the precision of inference in many real-world scenarios.", "method": "The paper introduces Praline, an extension to Datalog, and formulates the inference task as a constrained optimization problem. It further proposes a scalable \u03b4-exact inference algorithm using constraint solving, static analysis, and iterative refinement.", "result": "The proposed algorithms scale effectively to large programs and provide tight probability bounds, as demonstrated in real-world benchmarks including side-channel analysis.", "conclusion": "Praline enables precise probabilistic inference under correlated inputs, and the methods introduced achieve both scalability and accuracy in practical applications."}}
{"id": "2508.15135", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15135", "abs": "https://arxiv.org/abs/2508.15135", "authors": ["Sumudu Liyanage", "Sherlock A. Licorish", "Markus Wagner", "Stephen G. MacDonell"], "title": "On the need to perform comprehensive evaluations of automated program repair benchmarks: Sorald case study", "comment": null, "summary": "In supporting the development of high-quality software, especially necessary\nin the era of LLMs, automated program repair (APR) tools aim to improve code\nquality by automatically addressing violations detected by static analysis\nprofilers. Previous research tends to evaluate APR tools only for their ability\nto clear violations, neglecting their potential introduction of new (sometimes\nsevere) violations, changes to code functionality and degrading of code\nstructure. There is thus a need for research to develop and assess\ncomprehensive evaluation frameworks for APR tools. This study addresses this\nresearch gap, and evaluates Sorald (a state-of-the-art APR tool) as a proof of\nconcept. Sorald's effectiveness was evaluated in repairing 3,529 SonarQube\nviolations across 30 rules within 2,393 Java code snippets extracted from Stack\nOverflow. Outcomes show that while Sorald fixes specific rule violations, it\nintroduced 2,120 new faults (32 bugs, 2088 code smells), reduced code\nfunctional correctness--as evidenced by a 24% unit test failure rate--and\ndegraded code structure, demonstrating the utility of our framework. Findings\nemphasize the need for evaluation methodologies that capture the full spectrum\nof APR tool effects, including side effects, to ensure their safe and effective\nadoption.", "AI": {"tldr": "APR tools like Sorald can fix code violations but may introduce new faults and degrade code quality. Comprehensive evaluation frameworks are needed to identify both benefits and risks of APR tools.", "motivation": "Automated program repair (APR) tools are increasingly important for improving code quality, especially in the era of large language models (LLMs). However, current evaluations often focus only on whether APR tools remove violations, without considering if the tools introduce new problems or negatively affect the code.", "method": "This study develops and applies a comprehensive evaluation framework to assess APR tools, using Sorald as a proof of concept. The framework examines not only the removal of violations but also the introduction of new faults, impact on code correctness, and code structure. The evaluation involved fixing 3,529 SonarQube violations in 2,393 Java code snippets from Stack Overflow.", "result": "Sorald was effective at fixing the targeted rule violations, but also introduced 2,120 new faults (32 bugs and 2,088 code smells), caused a 24% unit test failure rate, and degraded code structure.", "conclusion": "Evaluation of APR tools should not focus solely on violation clearance but must also consider side effects such as new faults, reduced functional correctness, and code structure degradation to ensure safe and beneficial use."}}
{"id": "2508.15264", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.15264", "abs": "https://arxiv.org/abs/2508.15264", "authors": ["Patrick Redmond", "Jonathan Castello", "Jos\u00e9 Manuel Calder\u00f3n Trilla", "Lindsey Kuper"], "title": "Exploring the Theory and Practice of Concurrency in the Entity-Component-System Pattern", "comment": "This is an extended version (with appendices) of the OOPSLA 2025\n  paper", "summary": "The Entity-Component-System (ECS) software design pattern, long used in game\ndevelopment, encourages a clean separation of identity (entities), data\nproperties (components), and computational behaviors (systems). Programs\nwritten using the ECS pattern are naturally concurrent, and the pattern offers\nmodularity, flexibility, and performance benefits that have led to a\nproliferation of ECS frameworks. Nevertheless, the ECS pattern is little-known\nand not well understood outside of a few domains. Existing explanations of the\nECS pattern tend to be mired in the concrete details of particular ECS\nframeworks, or they explain the pattern in terms of imperfect metaphors or in\nterms of what it is not. We seek a rigorous understanding of the ECS pattern\nvia the design of a formal model, Core ECS, that abstracts away the details of\nspecific implementations to reveal the essence of software using the ECS\npattern. We identify a class of Core ECS programs that behave deterministically\nregardless of scheduling, enabling use of the ECS pattern as a\ndeterministic-by-construction concurrent programming model. With Core ECS as a\npoint of comparison, we then survey several real-world ECS frameworks and find\nthat they all leave opportunities for deterministic concurrency unexploited.\nOur findings point out a space for new ECS implementation techniques that\nbetter leverage such opportunities.", "AI": {"tldr": "The paper formally models the Entity-Component-System (ECS) design pattern, showing that it supports deterministic concurrency, unlike most real-world implementations. The findings suggest opportunities for new ECS frameworks to better exploit concurrency guarantees.", "motivation": "The ECS pattern is widely used in game development for its modularity, flexibility, and performance. However, it remains under-explained and poorly understood outside of specific domains, and explanations are often too implementation-specific or metaphorical. The authors aim to provide a more formal, abstract understanding of ECS.", "method": "The authors design a formal model called Core ECS to abstract and generalize the key ideas of the ECS pattern, stripping away implementation specifics. They use this model to analyze program behavior, focusing on deterministic concurrency, and survey real-world ECS frameworks for comparison.", "result": "The study identifies a class of Core ECS programs that are deterministic regardless of scheduling, demonstrating that ECS can be used for deterministic concurrent programming. The analysis of real-world ECS frameworks reveals that current implementations do not fully exploit these deterministic concurrency opportunities.", "conclusion": "There is significant potential for new ECS frameworks to provide improved deterministic concurrency based on the formal principles identified in Core ECS. Existing frameworks do not yet take full advantage of these possibilities."}}
{"id": "2508.15411", "categories": ["cs.SE", "cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.15411", "abs": "https://arxiv.org/abs/2508.15411", "authors": ["Frederik Vandeputte"], "title": "Foundational Design Principles and Patterns for Building Robust and Adaptive GenAI-Native Systems", "comment": null, "summary": "Generative AI (GenAI) has emerged as a transformative technology,\ndemonstrating remarkable capabilities across diverse application domains.\nHowever, GenAI faces several major challenges in developing reliable and\nefficient GenAI-empowered systems due to its unpredictability and inefficiency.\nThis paper advocates for a paradigm shift: future GenAI-native systems should\nintegrate GenAI's cognitive capabilities with traditional software engineering\nprinciples to create robust, adaptive, and efficient systems.\n  We introduce foundational GenAI-native design principles centered around five\nkey pillars -- reliability, excellence, evolvability, self-reliance, and\nassurance -- and propose architectural patterns such as GenAI-native cells,\norganic substrates, and programmable routers to guide the creation of resilient\nand self-evolving systems. Additionally, we outline the key ingredients of a\nGenAI-native software stack and discuss the impact of these systems from\ntechnical, user adoption, economic, and legal perspectives, underscoring the\nneed for further validation and experimentation. Our work aims to inspire\nfuture research and encourage relevant communities to implement and refine this\nconceptual framework.", "AI": {"tldr": "This paper advocates combining GenAI abilities with software engineering principles to build robust systems, introduces core design pillars and architecture patterns, and emphasizes further research and validation.", "motivation": "Generative AI (GenAI) offers transformative potential but faces challenges regarding reliability, efficiency, and unpredictability in practical system development.", "method": "The paper proposes a paradigm shift combining GenAI's cognitive abilities with traditional software engineering principles. It introduces five design pillars and new architectural patterns (GenAI-native cells, organic substrates, programmable routers) as a framework.", "result": "A conceptual framework of GenAI-native systems with design principles and architectural patterns is presented, along with a discussion of technical, user, economic, and legal impacts. The framework's effectiveness remains to be validated and refined.", "conclusion": "The paper emphasizes the need for future research and community involvement to implement, experiment with, and improve GenAI-native systems based on the proposed principles."}}
{"id": "2508.15333", "categories": ["cs.PL", "F.3.3"], "pdf": "https://arxiv.org/pdf/2508.15333", "abs": "https://arxiv.org/abs/2508.15333", "authors": ["Francesco Dagnino", "Paola Giannini", "Violet Ka I Pun", "Ulises Torrella"], "title": "Fair Termination for Resource-Aware Active Objects", "comment": "18 pages, 12 pages of appendix, 12 figures, APLAS 2025", "summary": "Active object systems are a model of distributed computation that has been\nadopted for modelling distributed systems and business process workflows. This\nfield of modelling is, in essence, concurrent and resource-aware, motivating\nthe development of resource-aware formalisations on the active object model.\nThe contributions of this work are the development of a core calculus for\nresource-aware active objects together with a type system ensuring that\nwell-typed programs are fairly terminating, i.e., they can always eventually\nterminate. To achieve this, we combine techniques from graded semantics and\ntype systems, which are quite well understood for sequential programs, with\nthose for fair termination, which have been developed for synchronous~sessions.", "AI": {"tldr": "The paper introduces a formal calculus and type system for active object models in distributed systems, ensuring resource-awareness and fair termination by combining existing semantics and session-based techniques.", "motivation": "Active object systems are essential for modeling distributed and concurrent systems, which require careful consideration of resource management and termination guarantees. Existing models lack robust resource-aware formalizations that ensure fair termination.", "method": "The authors develop a core calculus tailored to resource-aware active objects. They integrate graded semantics and type system techniques (common in sequential programming) with fair termination approaches from synchronous session theory.", "result": "The proposed type system soundly guarantees that well-typed programs in this model will fairly terminate; that is, every computation will eventually reach termination under the constraints specified.", "conclusion": "This work advances the modeling of distributed computation by providing a formal framework that enforces resource-awareness and fair termination in active object systems. The type system bridges the gap between termination guarantees and resource management."}}
{"id": "2508.15423", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15423", "abs": "https://arxiv.org/abs/2508.15423", "authors": ["Ruiqi Wang", "Zezhou Yang", "Cuiyun Gao", "Xin Xia", "Qing Liao"], "title": "An Empirical Study of Knowledge Distillation for Code Understanding Tasks", "comment": "Accepted by ICSE 2026 (Cycle 1)", "summary": "Pre-trained language models (PLMs) have emerged as powerful tools for code\nunderstanding. However, deploying these PLMs in large-scale applications faces\npractical challenges due to their computational intensity and inference\nlatency. Knowledge distillation (KD), a promising model compression and\nacceleration technique, addresses these limitations by transferring knowledge\nfrom large teacher models to compact student models, enabling efficient\ninference while preserving most of the teacher models' capabilities. While this\ntechnique has shown remarkable success in natural language processing and\ncomputer vision domains, its potential for code understanding tasks remains\nlargely underexplored.\n  In this paper, we systematically investigate the effectiveness and usage of\nKD in code understanding tasks. Our study encompasses two popular types of KD\nmethods, i.e., logit-based and feature-based KD methods, experimenting across\neight student models and two teacher PLMs from different domains on three\ndownstream tasks. The experimental results indicate that KD consistently offers\nnotable performance boosts across student models with different sizes compared\nwith standard fine-tuning. Notably, code-specific PLM demonstrates better\neffectiveness as the teacher model. Among all KD methods, the latest\nfeature-based KD methods exhibit superior performance, enabling student models\nto retain up to 98% teacher performance with merely 5% parameters. Regarding\nstudent architecture, our experiments reveal that similarity with teacher\narchitecture does not necessarily lead to better performance. We further\ndiscuss the efficiency and behaviors in the KD process and inference, summarize\nthe implications of findings, and identify promising future directions.", "AI": {"tldr": "Knowledge distillation, especially feature-based methods with code-specific teacher models, can significantly compress and accelerate code-understanding PLMs, allowing student models to match up to 98% of the teacher's performance while using much fewer parameters.", "motivation": "Deploying pre-trained language models (PLMs) for code understanding faces challenges due to computational demands and latency. Knowledge distillation (KD) promises to make inference more efficient but its potential in code-related tasks is underexplored.", "method": "The paper systematically investigates knowledge distillation for code understanding tasks, analyzing both logit-based and feature-based KD methods. Experiments are conducted using eight student models and two teacher PLMs on three downstream tasks, comparing performance across different approaches and architectures.", "result": "KD consistently provides significant performance improvements over standard fine-tuning for student models of various sizes. Code-specific teacher PLMs are more effective. Feature-based KD methods enable student models to achieve up to 98% of the teacher performance with only 5% of the parameters. Architectural similarity between teacher and student models does not guarantee better results.", "conclusion": "Knowledge distillation is highly effective for compressing and accelerating code-understanding PLMs, with feature-based methods and code-specific teachers offering the best outcomes. Future work should explore deeper aspects of KD efficiency and performance for code tasks."}}
{"id": "2508.15576", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.15576", "abs": "https://arxiv.org/abs/2508.15576", "authors": ["Andreas L\u00f6\u00f6w", "Seung Hoon Park", "Daniele Nantes-Sobrinho", "Sacha-\u00c9lie Ayoun", "Opale Sj\u00f6stedt", "Philippa Gardner"], "title": "Compositional Symbolic Execution for the Next 700 Memory Models (Extended Version)", "comment": null, "summary": "Multiple successful compositional symbolic execution (CSE) tools and\nplatforms exploit separation logic (SL) for compositional verification and/or\nincorrectness separation logic (ISL) for compositional bug-finding, including\nVeriFast, Viper, Gillian, CN, and Infer-Pulse. Previous work on the Gillian\nplatform, the only CSE platform that is parametric on the memory model, meaning\nthat it can be instantiated to different memory models, suggests that the\nability to use custom memory models allows for more flexibility in supporting\nanalysis of a wide range of programming languages, for implementing custom\nautomation, and for improving performance. However, the literature lacks a\nsatisfactory formal foundation for memory-model-parametric CSE platforms.\n  In this paper, inspired by Gillian, we provide a new formal foundation for\nmemory-model-parametric CSE platforms. Our foundation advances the state of the\nart in four ways. First, we mechanise our foundation (in the interactive\ntheorem prover Rocq). Second, we validate our foundation by instantiating it to\na broad range of memory models, including models for C and CHERI. Third,\nwhereas previous memory-model-parametric work has only covered SL analyses, we\ncover both SL and ISL analyses. Fourth, our foundation is based on standard\ndefinitions of SL and ISL (including definitions of function specification\nvalidity, to ensure sound interoperation with other tools and platforms also\nbased on standard definitions).", "AI": {"tldr": "The paper introduces and mechanizes a new formal foundation for compositional symbolic execution tools that are parameterized by different memory models, validated against real models (like C and CHERI), supporting both verification and bug-finding logics, and ensuring standards-based interoperability.", "motivation": "Compositional symbolic execution (CSE) platforms use separation logic (SL) and incorrectness separation logic (ISL) for verification and bug-finding across programming languages. Previous work, especially Gillian, highlights that custom memory models offer flexibility and better support for various languages and analyses. However, there is no formal theoretical foundation for CSE platforms that can be parameterized by memory models.", "method": "The authors present a new, mechanized formal foundation for memory-model-parametric CSE platforms. They implement this in the interactive theorem prover Rocq, validate it by instantiating various memory models (including C and CHERI), and ensure their approach supports both SL and ISL based analyses. They ground their formalism in standard definitions to ensure cross-tool compatibility.", "result": "The new foundation is mechanized, validated with several concrete memory models, supports both types of analyses (SL and ISL), and is compatible with existing tools and definitions. This proffers a robust, formal backbone for future and existing CSE platforms that require memory model flexibility.", "conclusion": "This work fills an important gap by providing a robust, mechanized, and validated formal foundation for memory-model-parametric CSE platforms, supporting a wider range of analyses and ensuing better interoperability and verification reliability."}}
{"id": "2508.15495", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15495", "abs": "https://arxiv.org/abs/2508.15495", "authors": ["Dongjun Yu", "Xiao Yan", "Zhenrui Li", "Jipeng Xiao", "Haochuan He", "Yongda Yu", "Hao Zhang", "Guoping Rong", "Xiaobo Huang"], "title": "SynthCoder: A Synthetical Strategy to Tune LLMs for Code Completion", "comment": null, "summary": "Code completion is a prominent application of Large Language Models (LLMs) in\nsoftware engineering. Due to the near real-time response requirements of this\ntask, base models with small to medium-sized parameters are typically employed,\nsupplemented by various optimization and post-training techniques. However,\nthese optimization methods often have trade-offs, leading to a seesaw effect\nwhere performance improvements on certain datasets or metrics are accompanied\nby degradations on others -- sometimes even falling below the baseline model's\nperformance. This paper proposes SynthCoder, a model that integrates leading\nindustry practices to achieve state-of-the-art performance on the\nFill-in-the-Middle (FIM) code completion task. In specific, we first construct\na diverse dataset by combining Abstract Syntax Tree (AST) node extraction with\nheuristics that simulate developer behavior. Then we enrich our training corpus\nwith cross-file contextual information using the BM25 algorithm and call\ngraphs, enhancing the model's ability to perform code completion in both\nfile-level and repository-level scenarios. As the last step, we employ a\ntwo-stage training process using the Seed-Coder-8B-Base as the base model.\nFirst, we fine-tune the model using Curriculum Learning technology. Following\nthis, we perform alignment using Direct Preference Optimization (DPO) with\npreference pairs generated through Rejection Sampling. Experimental results\ndemonstrate that our final model excels on mainstream repository-level code\ncompletion benchmarks, including aiXcoder, ExecRepoBench, CrossCodeEval, and\nCoLT. Furthermore, our carefully curated training set effectively mitigates the\nmodel's tendency to just repeat existing code, a common issue existing in\nvarious code completion models.", "AI": {"tldr": "SynthCoder is a new code completion model that uses advanced data construction, contextual enrichment, and a two-stage training process to outperform existing solutions on multiple benchmarks and addresses common issues like code repetition.", "motivation": "Current optimization methods for code completion models often involve trade-offs, improving performance on some datasets while degrading it on others. There is a need for a solution that achieves state-of-the-art performance without sacrificing consistency across benchmarks.", "method": "SynthCoder integrates industry best practices for code completion by constructing a diverse dataset using AST extraction and developer heuristics. It incorporates cross-file context with BM25 and call graph analysis. The training follows a two-stage process: fine-tuning with Curriculum Learning and final alignment using Direct Preference Optimization with preference pairs via Rejection Sampling.", "result": "SynthCoder outperforms previous models on repository-level code completion benchmarks (aiXcoder, ExecRepoBench, CrossCodeEval, CoLT). The custom training set also reduces the problem of models repeating existing code.", "conclusion": "SynthCoder achieves state-of-the-art code completion performance across metrics and reduces undesirable model tendencies like repetition, demonstrating the effectiveness of its integrated methodology."}}
{"id": "2508.15750", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.15750", "abs": "https://arxiv.org/abs/2508.15750", "authors": ["Celeste Barnaby", "Qiaochu Chen", "Ramya Ramalingam", "Osbert Bastani", "Isil Dillig"], "title": "Active Learning for Neurosymbolic Program Synthesis", "comment": null, "summary": "The goal of active learning for program synthesis is to synthesize the\ndesired program by asking targeted questions that minimize user interaction.\nWhile prior work has explored active learning in the purely symbolic setting,\nsuch techniques are inadequate for the increasingly popular paradigm of\nneurosymbolic program synthesis, where the synthesized program incorporates\nneural components. When applied to the neurosymbolic setting, such techniques\ncan -- and, in practice, do -- return an unintended program due to\nmispredictions of neural components. This paper proposes a new active learning\ntechnique that can handle the unique challenges posed by neural network\nmispredictions. Our approach is based upon a new evaluation strategy called\nconstrained conformal evaluation (CCE), which accounts for neural\nmispredictions while taking into account user-provided feedback. Our proposed\nmethod iteratively makes CCE more precise until all remaining programs are\nguaranteed to be observationally equivalent. We have implemented this method in\na tool called SmartLabel and experimentally evaluated it on three neurosymbolic\ndomains. Our results demonstrate that SmartLabel identifies the ground truth\nprogram for 98% of the benchmarks, requiring under 5 rounds of user interaction\non average. In contrast, prior techniques for active learning are only able to\nconverge to the ground truth program for at most 65% of the benchmarks.", "AI": {"tldr": "This paper introduces SmartLabel, an active learning tool for neurosymbolic program synthesis that uses a new evaluation method (CCE) to effectively handle neural mispredictions. SmartLabel identifies correct programs in 98% of test cases with minimal user interaction, outperforming previous approaches.", "motivation": "Existing active learning techniques for program synthesis struggle when neural components are involved, as their mispredictions can lead to unintended program outputs. There's a need for methods that can robustly handle these neural mispredictions in neurosymbolic synthesis.", "method": "The method introduces 'constrained conformal evaluation' (CCE), which iteratively refines the set of candidate programs by accounting for neural network mispredictions and user feedback, aiming for observational equivalence.", "result": "SmartLabel was tested on three domains and achieved correct synthesis (ground truth program) for 98% of benchmarks, requiring fewer than 5 user interactions on average, while prior methods succeeded on no more than 65% of benchmarks.", "conclusion": "The proposed active learning method, implemented in SmartLabel, substantially outperforms prior techniques in neurosymbolic program synthesis by identifying the correct program in 98% of cases, with fewer user interactions."}}
{"id": "2508.15496", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15496", "abs": "https://arxiv.org/abs/2508.15496", "authors": ["Elena Masserini", "Diego Clerissi", "Daniela Micucci", "Jo\u00e3o R. Campos", "Leonardo Mariani"], "title": "Towards the Assessment of Task-based Chatbots: From the TOFU-R Snapshot to the BRASATO Curated Dataset", "comment": "10 pages, 10 figure, Accepted at IEEE International Symposium on\n  Software Reliability Engineering (ISSRE) 2025", "summary": "Task-based chatbots are increasingly being used to deliver real services, yet\nassessing their reliability, security, and robustness remains underexplored,\nalso due to the lack of large-scale, high-quality datasets. The emerging\nautomated quality assessment techniques targeting chatbots often rely on\nlimited pools of subjects, such as custom-made toy examples, or outdated, no\nlonger available, or scarcely popular agents, complicating the evaluation of\nsuch techniques. In this paper, we present two datasets and the tool support\nnecessary to create and maintain these datasets. The first dataset is RASA\nTASK-BASED CHATBOTS FROM GITHUB (TOFU-R), which is a snapshot of the Rasa\nchatbots available on GitHub, representing the state of the practice in\nopen-source chatbot development with Rasa. The second dataset is BOT RASA\nCOLLECTION (BRASATO), a curated selection of the most relevant chatbots for\ndialogue complexity, functional complexity, and utility, whose goal is to ease\nreproducibility and facilitate research on chatbot reliability.", "AI": {"tldr": "The paper introduces two datasets of Rasa-based chatbots from GitHub to address the lack of resources in chatbot reliability research, offering curated collections and tools to support further study.", "motivation": "Reliability, security, and robustness of chatbots is underexplored due to absence of large-scale, high-quality datasets.", "method": "Presented two datasets (TOFU-R and BRASATO) and tool support for their creation and maintenance.", "result": "TOFU-R: Snapshot of Rasa-based chatbots from GitHub; BRASATO: Curated selection of chatbots chosen for dialogue complexity, functional complexity, and utility.", "conclusion": "These datasets and the supporting tools ease reproducibility and enable research into chatbot reliability."}}
{"id": "2508.15503", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15503", "abs": "https://arxiv.org/abs/2508.15503", "authors": ["Sebastian Baltes", "Florian Angermeir", "Chetan Arora", "Marvin Mu\u00f1oz Bar\u00f3n", "Chunyang Chen", "Lukas B\u00f6hme", "Fabio Calefato", "Neil Ernst", "Davide Falessi", "Brian Fitzgerald", "Davide Fucci", "Marcos Kalinowski", "Stefano Lambiase", "Daniel Russo", "Mircea Lungu", "Lutz Prechelt", "Paul Ralph", "Christoph Treude", "Stefan Wagner"], "title": "Evaluation Guidelines for Empirical Studies in Software Engineering involving LLMs", "comment": "Draft of evaluation guidelines for empirical studies in software\n  engineering involving LLMs (see also llm-guidelines.org)", "summary": "Large language models (LLMs) are increasingly being integrated into software\nengineering (SE) research and practice, yet their non-determinism, opaque\ntraining data, and evolving architectures complicate the reproduction and\nreplication of empirical studies. We present a community effort to scope this\nspace, introducing a taxonomy of LLM-based study types together with eight\nguidelines for designing and reporting empirical studies involving LLMs. The\nguidelines present essential (must) criteria as well as desired (should)\ncriteria and target transparency throughout the research process. Our\nrecommendations, contextualized by our study types, are: (1) to declare LLM\nusage and role; (2) to report model versions, configurations, and fine-tuning;\n(3) to document tool architectures; (4) to disclose prompts and interaction\nlogs; (5) to use human validation; (6) to employ an open LLM as a baseline; (7)\nto report suitable baselines, benchmarks, and metrics; and (8) to openly\narticulate limitations and mitigations. Our goal is to enable reproducibility\nand replicability despite LLM-specific barriers to open science. We maintain\nthe study types and guidelines online as a living resource for the community to\nuse and shape (llm-guidelines.org).", "AI": {"tldr": "This paper proposes a taxonomy and eight guidelines to increase transparency and reproducibility in software engineering studies involving LLMs, helping researchers navigate and address the unique challenges of working with these models.", "motivation": "Reproducibility and replicability are increasingly difficult for software engineering studies involving LLMs due to factors like model non-determinism, lack of transparency in training data, and constantly evolving architectures.", "method": "The paper introduces a taxonomy of LLM-based study types and formulates eight concrete guidelines for designing and reporting empirical studies that integrate LLMs. The guidelines focus on transparency and clear reporting, and have criteria categorized as essential and desired.", "result": "A comprehensive set of eight guidelines is provided for empirical SE studies using LLMs, covering aspects like LLM disclosure, version/configuration reporting, prompt/log disclosure, human validation, baselines, benchmarks, and open sharing of limitations. The resources are made available online for community input and ongoing updates.", "conclusion": "Adhering to the proposed guidelines and taxonomy will help the research community overcome LLM-specific challenges for reproducibility and replicability, promoting open and transparent science in the context of LLM-based software engineering studies. The guidelines serve as a living resource continually shaped by the community."}}
{"id": "2508.15512", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15512", "abs": "https://arxiv.org/abs/2508.15512", "authors": ["Markus Borg", "Martin Larsson", "Philip Breid", "Nadim Hagatulah"], "title": "QUPER-MAn: Benchmark-Guided Target Setting for Maintainability Requirements", "comment": "Accepted at the 1st International Workshop on Responsible Software\n  Engineering", "summary": "Maintainable source code is essential for sustainable development in any\nsoftware organization. Unfortunately, many studies show that maintainability\noften receives less attention than its importance warrants. We argue that\nrequirements engineering can address this gap the problem by fostering\ndiscussions and setting appropriate targets in a responsible manner. In this\npreliminary work, we conducted an exploratory study of industry practices\nrelated to requirements engineering for maintainability. Our findings confirm\nprevious studies: maintainability remains a second-class quality concern.\nExplicit requirements often make sweeping references to coding conventions.\nTools providing maintainability proxies are common but typically only used in\nimplicit requirements related to engineering practices. To address this, we\npropose QUPER-MAn, a maintainability adaption of the QUPER model, which was\noriginally developed to help organizations set targets for performance\nrequirements. Developed using a design science approach, QUPER-MAn, integrates\nmaintainability benchmarks and supports target setting. We posit that it can\nshift maintainability from an overlooked development consequence to an actively\nmanaged goal driven by informed and responsible engineering decisions.", "AI": {"tldr": "Source code maintainability is vital but often overlooked. The authors study industry practices and propose QUPER-MAn, a model for setting maintainability targets, aiming to make maintainability a primary engineering goal.", "motivation": "Maintainability of source code is crucial for long-term software development, yet it often does not get the attention it deserves. The authors are motivated by the need to address this gap and improve how maintainability is handled within requirements engineering.", "method": "The authors conduct an exploratory study of industry practices concerning requirements engineering for maintainability. They confirm previous findings that maintainability is often undervalued. To address this, they introduce QUPER-MAn, an adaptation of the QUPER model, developed using a design science approach.", "result": "The study finds that maintainability is still a secondary concern in practice. Explicit requirements focus on coding conventions, and maintainability assessment tools are mainly used for implicit requirements. QUPER-MAn is proposed to help organizations set clear, actionable maintainability goals.", "conclusion": "QUPER-MAn could help organizations move from neglecting maintainability to actively managing and improving it through informed, responsible engineering decisions."}}
{"id": "2508.15536", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15536", "abs": "https://arxiv.org/abs/2508.15536", "authors": ["Yi Zhang", "He Jiang", "Xiaochen Li", "Shikai Guo", "Peiyu Zou", "Zun Wang"], "title": "A Novel Mutation Based Method for Detecting FPGA Logic Synthesis Tool Bugs", "comment": null, "summary": "FPGA (Field-Programmable Gate Array) logic synthesis tools are key components\nin the EDA (Electronic Design Automation) toolchain. They convert hardware\ndesigns written in description languages such as Verilog into gate-level\nrepresentations for FPGAs. However, defects in these tools may lead to\nunexpected behaviors and pose security risks. Therefore, it is crucial to\nharden these tools through testing. Although several methods have been proposed\nto automatically test FPGA logic synthesis tools, the challenge remains of\ninsufficient semantic and logical complexity in test programs. In this paper,\nwe propose VERMEI, a new method for testing FPGA logic synthesis tools. VERMEI\nconsists of three modules: preprocessing, equivalent mutation, and bug\nidentification. The preprocessing module identifies zombie logic (inactive code\nwith no impact on the circuit output) in seed programs through simulation and\ncoverage analysis. The equivalent mutation module generates equivalent variants\nof seed programs by pruning or inserting logic fragments in zombie areas. It\nuses Bayesian sampling to extract logic fragments from historical Verilog\ndesigns, making the generated variants have complex control flows and\nstructures. The bug identification module, based on differential testing,\ncompares the synthesized outputs of seed and variant programs to identify bugs.\nExperiments on Yosys, Vivado, and Quartus demonstrate that VERMEI outperforms\nthe state-of-the-art methods. Within five months, VERMEI reported 15 bugs to\nvendors, 9 of which were confirmed as new.", "AI": {"tldr": "VERMEI enhances the testing of FPGA logic synthesis tools, using innovative techniques to generate complex test cases. It outperforms existing methods and has uncovered several new bugs in widely used EDA tools.", "motivation": "Defects in FPGA logic synthesis tools can lead to unpredictable behaviors and security risks, making robust testing essential. Existing automatic testing methods often fail to generate test programs with sufficient semantic and logical complexity.", "method": "VERMEI is a new testing method composed of three modules: preprocessing (which identifies inactive code through simulation and coverage analysis), equivalent mutation (which generates complex program variants by modifying zombie logic using Bayesian sampling from historical designs), and bug identification (using differential testing to compare outcomes).", "result": "VERMEI was empirically evaluated on popular synthesis tools (Yosys, Vivado, Quartus), outperforming prior approaches. Over five months, it reported 15 bugs, out of which vendors confirmed 9 as new.", "conclusion": "VERMEI is a more effective approach for testing FPGA logic synthesis tools, able to uncover complex bugs missed by existing methods, and has real-world impact with several newly identified bugs."}}
{"id": "2508.15570", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15570", "abs": "https://arxiv.org/abs/2508.15570", "authors": ["Marion Wiese", "Kamila Serwa", "Anastasia Besier", "Ariane S. Marion-Jetten", "Eva Bittner"], "title": "Establishing Technical Debt Management -- A Five-Step Workshop Approach and an Action Research Study", "comment": "Accepted for publication by the Journal of Systems and Software --\n  Special Issue on Managing Technical Debt in Software-Intensive Products and\n  Services", "summary": "Context. Technical debt (TD) items are constructs in a software system\nproviding short-term benefits but hindering future changes. TD management (TDM)\nis frequently researched but rarely adopted in practice. Goal. This study aimed\nto establish a TDM process in an IT company based on a predefined workshop\nconcept. We analyzed which research approaches practitioners adopted for each\nTD activity and the TDM's long-term effect on TD awareness. Method. We used\naction research (five action cycles in 16 months) with an IT team that creates\nIT solutions for signal processing. To examine TD awareness, we (1) analyzed\nquestionnaires completed during each workshop, (2) observed team meetings, (3)\nadopted a method from psychology for measuring awareness in decision-making\nsituations called TD-SAGAT, and (4) evaluated the backlog data. Results.\nPractitioners preferred TD repayment and prioritization based on the system's\nevolution and cost calculations, i.e., repayment of so-called low-hanging\nfruits. Reminders in the backlog items, such as checkboxes or text templates,\nled to a sustainable rise in TD awareness. Conclusions. We showed that a\nworkshop-based approach is feasible and leads to sustainable process changes.\nNew ideas for TDM applicable to other IT teams emerged, e.g., using a\nre-submission date, using a Talked about TD checkbox, and using visualizations\nfor TD prioritization.", "AI": {"tldr": "The paper demonstrates that a hands-on, workshop-based TD management process can boost and sustain awareness of technical debt in development teams, offering actionable techniques for broader application.", "motivation": "Technical debt (TD) management is often researched but seldom adopted in industry, leading to a gap between research findings and practical implementation. The authors aim to bridge this gap by establishing an effective TD management process that can be practically applied in an IT company.", "method": "The authors implemented action research involving five action cycles over 16 months with an IT team developing signal processing solutions. They assessed TD awareness through questionnaires, observation of team meetings, a psychology-based method (TD-SAGAT) for measuring awareness, and backlog data evaluation.", "result": "Practitioners favored addressing TD items that are easy to fix (low-hanging fruits), and prioritization was based on system evolution and cost calculations. Addition of simple reminders in backlog items (like checkboxes or templates) resulted in sustained growth in TD awareness.", "conclusion": "A workshop-based, practical approach to technical debt management is feasible and leads to sustainable changes in team behavior and processes. The study also introduced new, transferable ideas for TDM, such as re-submission dates, dedicated checkboxes, and visual tools for prioritization."}}
{"id": "2508.15584", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15584", "abs": "https://arxiv.org/abs/2508.15584", "authors": ["Maria Teresa Rossi", "Leonardo Mariani", "Oliviero Riganelli"], "title": "From PREVENTion to REACTion: Enhancing Failure Resolution in Naval Systems", "comment": null, "summary": "Complex and large industrial systems often misbehave, for instance, due to\nwear, misuse, or faults. To cope with these incidents, it is important to\ntimely detect their occurrences, localize the sources of the problems, and\nimplement the appropriate countermeasures. This paper reports our experience\nwith a state-of-the-art failure prediction method, PREVENT, and its extension\nwith a troubleshooting module, REACT, applied to naval systems developed by\nFincantieri. Our results show how to integrate anomaly detection with\ntroubleshooting procedures. We conclude by discussing a lesson learned, which\nmay help deploy and extend these analyses to other industrial products.", "AI": {"tldr": "This paper explores applying PREVENT (failure prediction) and REACT (troubleshooting) to naval industrial systems, showing successful integration and offering insights for deploying such methods in other industries.", "motivation": "Industrial systems frequently experience failures due to factors like wear, misuse, or faults. Early detection and troubleshooting are crucial to minimize downtime and maintain production efficiency.", "method": "The authors utilize a failure prediction method called PREVENT and introduce a troubleshooting module named REACT. They apply these methods to naval systems developed by Fincantieri, integrating anomaly detection with troubleshooting procedures.", "result": "The integration of anomaly detection and troubleshooting procedures was successfully demonstrated on naval systems. The study provided practical insights and lessons learned from the deployment of the methods.", "conclusion": "The combination of PREVENT and REACT offers a practical approach for early failure prediction and troubleshooting, which can be extended to other industrial systems. The paper discusses lessons learned to facilitate broader adoption."}}
