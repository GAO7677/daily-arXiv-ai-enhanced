<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Evaluating Uncertainty and Quality of Visual Language Action-enabled Robots](https://arxiv.org/abs/2507.17049)
*Pablo Valle,Chengjie Lu,Shaukat Ali,Aitor Arrieta*

Main category: cs.SE

TL;DR: The paper introduces new uncertainty and quality metrics for VLA models in robotics, demonstrating that these offer a much better evaluation of task execution and model confidence than simple success rates, and suggests such metrics should supplement or replace conventional evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for Visual Language Action (VLA) models focus mainly on binary task success rates. These measures do not adequately capture how well tasks are executed or how confident the model is in its decisions. There is a need for better evaluation metrics that provide a richer understanding of model performance, especially for complex robotic manipulation tasks.

Method: The authors propose eight uncertainty metrics and five quality metrics specifically designed to evaluate VLA models in robotic manipulation contexts. They conduct a large-scale empirical study, evaluating 908 successful task executions from three state-of-the-art VLA models across four types of manipulation tasks. Human experts manually label the quality of each task execution, enabling correlation analysis between the proposed metrics and expert judgments.

Result: Several of the proposed metrics show moderate to strong correlation with expert human judgments of task quality. Some of these metrics can effectively distinguish between high-, medium-, and low-quality task executions, even in the absence of a traditional success/failure test oracle.

Conclusion: Reliance solely on task success rates is inadequate for evaluating VLA models. The proposed metrics enable more nuanced assessment of both task quality and model confidence, supporting improved real-time monitoring and adaptive improvements in VLA-based robotic systems.

Abstract: Visual Language Action (VLA) models are a multi-modal class of Artificial
Intelligence (AI) systems that integrate visual perception, natural language
understanding, and action planning to enable agents to interpret their
environment, comprehend instructions, and perform embodied tasks autonomously.
Recently, significant progress has been made to advance this field. These kinds
of models are typically evaluated through task success rates, which fail to
capture the quality of task execution and the mode's confidence in its
decisions. In this paper, we propose eight uncertainty metrics and five quality
metrics specifically designed for VLA models for robotic manipulation tasks. We
assess their effectiveness through a large-scale empirical study involving 908
successful task executions from three state-of-the-art VLA models across four
representative robotic manipulation tasks. Human domain experts manually
labeled task quality, allowing us to analyze the correlation between our
proposed metrics and expert judgments. The results reveal that several metrics
show moderate to strong correlation with human assessments, highlighting their
utility for evaluating task quality and model confidence. Furthermore, we found
that some of the metrics can discriminate between high-, medium-, and
low-quality executions from unsuccessful tasks, which can be interesting when
test oracles are not available. Our findings challenge the adequacy of current
evaluation practices that rely solely on binary success rates and pave the way
for improved real-time monitoring and adaptive enhancement of VLA-enabled
robotic systems.

</details>


### [2] [Assessing Reliability of Statistical Maximum Coverage Estimators in Fuzzing](https://arxiv.org/abs/2507.17093)
*Danushka Liyanage,Nelum Attanayake,Zijian Luo,Rahul Gopinath*

Main category: cs.SE

TL;DR: This paper addresses the problem of estimating maximum achievable coverage in fuzz testing, introduces a synthetic benchmark framework to provide ground truth, and applies a new evaluation protocol to gauge the reliability of reachability estimators both on synthetic and on real-world programs.


<details>
  <summary>Details</summary>
Motivation: In fuzz testing, coverage-guided strategies are central, but accurately estimating the maximum possible code coverage is a challenge, especially since 100% coverage is unrealistic in complex real-world software. Existing static reachability analysis is not accurate enough, and while statistical methods have been proposed, their evaluation is limited by the lack of ground truth benchmarks.

Method: The paper proposes two main methods: (1) Developing a synthetic evaluation framework that generates large, complex programs with exactly known reachability, thus providing labeled ground truth for evaluating reachability estimators; (2) Applying a new reliability check on real-world programs by varying sampling unit sizes to see whether reachability estimates remain stable, circumventing the lack of ground truth in these scenarios.

Result: These methods together assess the reliability of current reachability estimators and establish an evaluation protocol for future developments. The study reveals whether these estimators are dependable both on synthetic benchmarks with ground truth and on real-world software without it.

Conclusion: The research clarifies the strengths and limitations of current reachability estimation approaches, delivers useful tools and protocols for their assessment, and lays a foundation to improve estimation methods in fuzzing practice.

Abstract: Background: Fuzzers are often guided by coverage, making the estimation of
maximum achievable coverage a key concern in fuzzing. However, achieving 100%
coverage is infeasible for most real-world software systems, regardless of
effort. While static reachability analysis can provide an upper bound, it is
often highly inaccurate. Recently, statistical estimation methods based on
species richness estimators from biostatistics have been proposed as a
potential solution. Yet, the lack of reliable benchmarks with labeled ground
truth has limited rigorous evaluation of their accuracy.
  Objective: This work examines the reliability of reachability estimators from
two axes: addressing the lack of labeled ground truth and evaluating their
reliability on real-world programs.
  Methods: (1) To address the challenge of labeled ground truth, we propose an
evaluation framework that synthetically generates large programs with complex
control flows, ensuring well-defined reachability and providing ground truth
for evaluation. (2) To address the criticism from use of synthetic benchmarks,
we adapt a reliability check for reachability estimators on real-world
benchmarks without labeled ground truth -- by varying the size of sampling
units, which, in theory, should not affect the estimate.
  Results: These two studies together will help answer the question of whether
current reachability estimators are reliable, and defines a protocol to
evaluate future improvements in reachability estimation.

</details>


### [3] [Can LLMs Write CI? A Study on Automatic Generation of GitHub Actions Configurations](https://arxiv.org/abs/2507.17165)
*Taher A. Ghaleb,Dulina Rathnayake*

Main category: cs.SE

TL;DR: LLMs struggle to generate precise GitHub Actions CI YAML from natural language; code-pretrained models don't outperform generalist ones, and accuracy remains limited, indicating the need for more targeted model improvements for CI automation.


<details>
  <summary>Details</summary>
Motivation: Writing YAML-based continuous integration (CI) configurations for services like GitHub Actions is tedious and error-prone. Though large language models (LLMs) are widely used in software engineering automation, their performance in generating CI configurations has not been thoroughly studied.

Method: The authors conducted a preliminary evaluation of six LLMs—including three general-purpose models (GPT-4o, Llama, Gemma) and three code-pretrained models (GPT-4.1, Code Llama, CodeGemma)—for generating GitHub Actions YAML configurations based on natural language descriptions. They created a unique labeled dataset pairing textual descriptions and best-practice YAML, and used zero-shot prompting to assess model outputs against ground truth.

Result: Zero-shot prompting produced up to 69% similarity with ground truth YAML, but only 3% perfect matches. Code-pretrained models performed slightly worse than general-purpose models for this task. Common issues in generated configurations included missing or renamed steps, incorrect interpretation of descriptions, and unnecessary additions.

Conclusion: Current LLMs, especially code-pretrained ones, have notable limitations for generating accurate CI configurations, with frequent structural and contextual errors. There is a significant gap between generated quality and the precision needed for working CI setups. This suggests a need for better LLM alignment with configuration languages and improved CI automation tools.

Abstract: Continuous Integration (CI) services, such as GitHub Actions, require
developers to write YAML-based configurations, which can be tedious and
error-prone. Despite the increasing use of Large Language Models (LLMs) to
automate software engineering tasks, their ability to generate CI
configurations remains underexplored. This paper presents a preliminary study
evaluating six LLMs for generating GitHub Actions configurations from natural
language descriptions. We assess three general-purpose foundation models
(GPT-4o, Llama, and Gemma) and three code-pretrained models (GPT-4.1, Code
Llama, and CodeGemma). We also introduce the first labeled dataset of its kind,
constructed from GitHub Actions documentation, pairing descriptions with
corresponding best-practice YAML configurations. Zero-shot prompting achieves
up to 69% similarity with the ground truth, with only 3% perfect matches.
Code-pretrained models slightly underperform compared to general-purpose ones
in YAML-based CI tasks, revealing LLM limitations for CI configuration
generation. Analyzing GPT-4o outputs reveals issues like missing or renamed
steps, misinterpreted descriptions, and unnecessary additions that may affect
structural and contextual correctness, indicating a gap between generation
quality and the precision required for executable CI configurations. Our
research offers insights for improving LLM alignment with configuration
languages and guiding future efforts on CI automation and tooling support.

</details>


### [4] [On the Feasibility of Quantum Unit Testing](https://arxiv.org/abs/2507.17235)
*Andriy Miranskyy,José Campos,Anila Mjeda,Lei Zhang,Ignacio García Rodríguez de Guzmán*

Main category: cs.SE

TL;DR: Quantum-centric unit tests (Statevector and Inverse tests) offer significant improvements over traditional statistical approaches in testing quantum software, delivering greater accuracy and efficiency for both classical and quantum hardware environments.


<details>
  <summary>Details</summary>
Motivation: Quantum software is becoming more complex, making verification and validation—especially unit testing—more challenging. Traditional statistical testing methods may not be well-suited for quantum circuits, necessitating new and more effective testing approaches.

Method: The paper conducts a comprehensive empirical study, analyzing 1,796,880 mutated quantum circuits with different types of unit tests: traditional statistical methods and quantum-centric techniques (Statevector, Swap, and a novel Inverse test). The analysis focuses on each test's ability to detect discrepancies between expected and actual quantum states and evaluates the number of measurements needed for reliable results.

Result: Quantum-centric tests, particularly the Statevector and Inverse tests, outperform traditional statistical tests. They offer higher precision and efficiency, with lower rates of false positives and false negatives. These tests require fewer measurements to reach high reliability.

Conclusion: Quantum-centric unit tests are more effective than classical statistical tests in terms of both accuracy and efficiency for quantum software validation. These results pave the way for more robust, scalable testing strategies that support the future of reliable quantum software and hardware.

Abstract: The increasing complexity of quantum software presents significant challenges
for software verification and validation, particularly in the context of unit
testing. This work presents a comprehensive study on quantum-centric unit
tests, comparing traditional statistical approaches with tests specifically
designed for quantum circuits. These include tests that run only on a classical
computer, such as the Statevector test, as well as those executable on quantum
hardware, such as the Swap test and the novel Inverse test. Through an
empirical study and detailed analysis on 1,796,880 mutated quantum circuits, we
investigate (a) each test's ability to detect subtle discrepancies between the
expected and actual states of a quantum circuit, and (b) the number of
measurements required to achieve high reliability. The results demonstrate that
quantum-centric tests, particularly the Statevector test and the Inverse test,
provide clear advantages in terms of precision and efficiency, reducing both
false positives and false negatives compared to statistical tests. This work
contributes to the development of more robust and scalable strategies for
testing quantum software, supporting the future adoption of fault-tolerant
quantum computers and promoting more reliable practices in quantum software
engineering.

</details>


### [5] [Understanding Prompt Programming Tasks and Questions](https://arxiv.org/abs/2507.17264)
*Jenny T. Liang,Chenyang Yang,Agnia Sergeyuk,Travis D. Breaux,Brad A. Myers*

Main category: cs.SE

TL;DR: Prompt programming for AI models involves many complex, unsupported manual tasks. Developers face unanswered key questions, and current tools do not meet their needs, revealing substantial opportunities for better tool development.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to understand the process and challenges of prompt programming for foundation models (FMs), as the practice becomes increasingly widespread with the advent of new AI-powered software features. The authors aim to uncover the specific tasks developers undertake and the questions they consider when updating prompts, to identify whether current tools support these needs.

Method: The authors develop a taxonomy of 25 tasks and 51 questions that prompt programmers encounter by interviewing 16 prompt programmers, observing 8 developers as they make prompt changes, and surveying 50 developers. They also compare the identified taxonomy against 48 existing research and commercial prompt programming tools to assess the adequacy of current tool support.

Result: The study finds that all prompt programming tasks are carried out manually by developers, and 16 out of 51 questions, including many of the most important ones, remain unanswered and unsupported by current tooling. This demonstrates significant gaps in the existing landscape of prompt programming tools.

Conclusion: Prompt programming as it currently stands is not well-supported by existing research and commercial tools. There are significant manual burdens for developers, and critical needs are not being addressed. The authors highlight key opportunities for improving prompt programming tools to better serve developers’ needs.

Abstract: Prompting foundation models (FMs) like large language models (LLMs) have
enabled new AI-powered software features (e.g., text summarization) that
previously were only possible by fine-tuning FMs. Now, developers are embedding
prompts in software, known as prompt programs. The process of prompt
programming requires the developer to make many changes to their prompt. Yet,
the questions developers ask to update their prompt is unknown, despite the
answers to these questions affecting how developers plan their changes. With
the growing number of research and commercial prompt programming tools, it is
unclear whether prompt programmers' needs are being adequately addressed. We
address these challenges by developing a taxonomy of 25 tasks prompt
programmers do and 51 questions they ask, measuring the importance of each task
and question. We interview 16 prompt programmers, observe 8 developers make
prompt changes, and survey 50 developers. We then compare the taxonomy with 48
research and commercial tools. We find that prompt programming is not
well-supported: all tasks are done manually, and 16 of the 51 questions --
including a majority of the most important ones -- remain unanswered. Based on
this, we outline important opportunities for prompt programming tools.

</details>


### [6] [CASCADE: LLM-Powered JavaScript Deobfuscator at Google](https://arxiv.org/abs/2507.17691)
*Shan Jiang,Pranoy Kovuri,David Tao,Zhixun Tan*

Main category: cs.SE

TL;DR: CASCADE is a new hybrid system that uses AI and compiler techniques to automatically deobfuscate JavaScript code, already improving real-world workflows at Google by making this process more efficient and less dependent on manual rule-writing.


<details>
  <summary>Details</summary>
Motivation: Software obfuscation, especially in JavaScript, makes understanding and analyzing code extremely difficult, which affects software testing, static analysis, and malware detection processes.

Method: The authors propose CASCADE, a hybrid approach that combines the AI-based capabilities of Gemini to identify key functions used in obfuscation, with the deterministic transformation capabilities of the JavaScript Intermediate Representation (JSIR) to transform and deobfuscate code. This integration allows for automated and robust code semantic recovery.

Result: CASCADE is shown to effectively recover original semantic elements from obfuscated JavaScript code, such as strings and API names, and to reveal original program behaviors. It eliminates the need for extensive hardcoded rules and demonstrates increased efficiency and flexibility. CASCADE has been successfully deployed in a production environment at Google, resulting in substantial improvements in deobfuscation efficiency and decreased reverse engineering effort.

Conclusion: CASCADE advances the field of JavaScript deobfuscation by offering a robust, flexible, and scalable solution that overcomes key limitations in static and dynamic approaches. Its deployment in production shows real-world impact, enhancing both efficiency and effectiveness for deobfuscation tasks.

Abstract: Software obfuscation, particularly prevalent in JavaScript, hinders code
comprehension and analysis, posing significant challenges to software testing,
static analysis, and malware detection. This paper introduces CASCADE, a novel
hybrid approach that integrates the advanced coding capabilities of Gemini with
the deterministic transformation capabilities of a compiler Intermediate
Representation (IR), specifically JavaScript IR (JSIR). By employing Gemini to
identify critical prelude functions, the foundational components underlying the
most prevalent obfuscation techniques, and leveraging JSIR for subsequent code
transformations, CASCADE effectively recovers semantic elements like original
strings and API names, and reveals original program behaviors. This method
overcomes limitations of existing static and dynamic deobfuscation techniques,
eliminating hundreds to thousands of hardcoded rules while achieving
reliability and flexibility. CASCADE is already deployed in Google's production
environment, demonstrating substantial improvements in JavaScript deobfuscation
efficiency and reducing reverse engineering efforts.

</details>


### [7] [Lessons from a Big-Bang Integration: Challenges in Edge Computing and Machine Learning](https://arxiv.org/abs/2507.17270)
*Alessandro Aneggi,Andrea Janes*

Main category: cs.SE

TL;DR: A distributed analytics project failed due to a big-bang integration and lack of early testing, yielding only 6 out of 40 expected functional minutes. The analysis highlights the limits of Agile in complex settings, stressing the need for early mockups, better communication, and structured integration for future success.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to examine the challenges and failures encountered during the development of a distributed real-time analytics system using edge computing and machine learning, specifically focusing on issues arising from a big-bang integration approach.

Method: The paper employs an experience report and root cause analysis to investigate the technical, organisational, and psychological factors leading to the project's setbacks.

Result: The project resulted in only six minutes of system functionality, instead of the expected 40 minutes. The root cause analysis revealed barriers such as poor communication, lack of early integration testing, and resistance to topdown planning. Psychological bias towards fully developed components over mockups also contributed to the problems.

Conclusion: The study concludes that traditional Agile methods have limitations in complex, reactive distributed projects. It advocates for early mock-based deployment, robust communication, topdown planning, simulation-driven engineering, and structured integration cycles as solutions to reduce risk and manage complexity.

Abstract: This experience report analyses a one year project focused on building a
distributed real-time analytics system using edge computing and machine
learning. The project faced critical setbacks due to a big-bang integration
approach, where all components developed by multiple geographically dispersed
partners were merged at the final stage. The integration effort resulted in
only six minutes of system functionality, far below the expected 40 minutes.
Through root cause analysis, the study identifies technical and organisational
barriers, including poor communication, lack of early integration testing, and
resistance to topdown planning. It also considers psychological factors such as
a bias toward fully developed components over mockups. The paper advocates for
early mock based deployment, robust communication infrastructures, and the
adoption of topdown thinking to manage complexity and reduce risk in reactive,
distributed projects. These findings underscore the limitations of traditional
Agile methods in such contexts and propose simulation-driven engineering and
structured integration cycles as key enablers for future success.

</details>


### [8] [Seed&Steer: Guiding Large Language Models with Compilable Prefix and Branch Signals for Unit Test Generation](https://arxiv.org/abs/2507.17271)
*Shuaiyu Zhou,Zhengran Zeng,Xiaoling Zhou,Rui Xie,Shikun Zhang,Wei Ye*

Main category: cs.SE

TL;DR: Seed&Steer, a hybrid approach combining traditional testing tools and LLMs, significantly boosts the success and coverage of automated unit test generation by decoupling and refining prefix and assertion creation.


<details>
  <summary>Details</summary>
Motivation: Unit tests are crucial for software quality, but current LLM-based test generation approaches face challenges in generating both the invocation setup (prefix) and assertions within test cases. The authors aim to better understand and overcome these challenges to improve effectiveness and reliability of automated unit test generation.

Method: The authors decouple unit test generation into prefix (method invocation setup) and assertion generation, analyzing their difficulties via Initialization Complexity (for prefixes) and Cyclomatic Complexity (for assertions). They introduce Seed&Steer—a two-step process that first uses traditional tools like EvoSuite to generate high-compilation-rate prefixes as seeds, then leverages LLMs guided through branching cues to generate diverse, high-coverage assertions.

Result: Seed&Steer was evaluated on five real-world Java projects using two LLMs against strong baselines. The approach achieved a 7% higher compilation pass rate and succeeded in compiling 792 and 887 previously failed tests. It also reached up to approximately 73% branch and line coverage, with relative improvements between 1.09x and 1.26x over baselines.

Conclusion: Decoupling prefix and assertion generation, and combining traditional tools with LLMs in the proposed Seed&Steer framework, addresses key difficulties in automated unit test generation, leading to more compilable and high-coverage tests. The work advances automated software testing by providing open-source tools and data for reproducibility.

Abstract: Unit tests play a vital role in the software development lifecycle. Recent
advances in Large Language Model (LLM)-based approaches have significantly
improved automated test generation, garnering attention from both academia and
industry. We revisit LLM-based unit test generation from a novel perspective by
decoupling prefix generation and assertion generation. To characterize their
respective challenges, we define Initialization Complexity and adopt Cyclomatic
Complexity to measure the difficulty of prefix and assertion generation,
revealing that the former primarily affects compilation success, while the
latter influences test coverage. To address these challenges, we propose
Seed&Steer, a two-step approach that combines traditional unit testing
techniques with the capabilities of large language models. Seed&Steer leverages
conventional unit testing tools (e.g., EvoSuite) to generate method invocations
with high compilation success rates, which serve as seeds to guide LLMs in
constructing effective test contexts. It then introduces branching cues to help
LLMs explore diverse execution paths (e.g., normal, boundary, and exception
cases) and generate assertions with high coverage. We evaluate Seed&Steer on
five real-world Java projects against state-of-the-art baselines. Results show
that Seed&Steer improves the compilation pass rate by approximately 7%,
successfully compiling 792 and 887 previously failing cases on two LLMs. It
also achieves up to ~73% branch and line coverage across focal methods of
varying complexity, with coverage improvements ranging from 1.09* to 1.26*. Our
code, dataset, and experimental scripts will be publicly released to support
future research and reproducibility.

</details>


### [9] [Data Virtualization for Machine Learning](https://arxiv.org/abs/2507.17293)
*Saiful Khan,Joyraj Chakraborty,Philip Beaucamp,Niraj Bhujel,Min Chen*

Main category: cs.SE

TL;DR: This paper introduces a scalable data virtualization service designed to manage and streamline large amounts of intermediate data from concurrent ML workflows, demonstrating its effectiveness across several applications and promising future growth.


<details>
  <summary>Details</summary>
Motivation: Managing multiple concurrent ML workflows for different applications results in significant intermediate data storage, processing, and maintenance challenges. Data virtualization technology is necessary to address these organizational needs and support efficient collaboration and iteration within ML teams.

Method: The paper presents the design and implementation of a data virtualization service, detailing its service architecture and operational aspects. The infrastructure is evaluated across six ML applications, each featuring multiple workflows.

Result: The data virtualization service successfully supports six ML applications, each with several workflows. The service is scalable, enabling the addition of more applications and workflows in the future.

Conclusion: The data virtualization service proves essential for managing large, complex ML workflows, facilitating data management, and meeting the growing needs of ML teams by providing a scalable and efficient infrastructure.

Abstract: Nowadays, machine learning (ML) teams have multiple concurrent ML workflows
for different applications. Each workflow typically involves many experiments,
iterations, and collaborative activities and commonly takes months and
sometimes years from initial data wrangling to model deployment.
Organizationally, there is a large amount of intermediate data to be stored,
processed, and maintained. \emph{Data virtualization} becomes a critical
technology in an infrastructure to serve ML workflows. In this paper, we
present the design and implementation of a data virtualization service,
focusing on its service architecture and service operations. The infrastructure
currently supports six ML applications, each with more than one ML workflow.
The data virtualization service allows the number of applications and workflows
to grow in the coming years.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [10] [Hiord: An Approach to the Specification and Verification of Higher-Order (C)LP Programs](https://arxiv.org/abs/2507.17233)
*Marco Ciccalè,Daniel Jurjo-Rivas,Jose F. Morales,Pedro López-García,Manuel V. Hermenegildo*

Main category: cs.PL

TL;DR: This paper introduces a static analysis technique for verifying higher-order assertions in (constraint) logic programming. By refining and reducing these properties, the method allows compile-time checking, demonstrated effectively with a prototype in the Ciao system.


<details>
  <summary>Details</summary>
Motivation: Higher-order programming enables more expressive code, while assertions add specification and verification support. However, static (compile-time) verification of higher-order assertions—especially in constraint logic programming ((C)LP)—remains underdeveloped. The paper addresses this gap to improve program correctness and reliability.

Method: The paper introduces a new approach for statically verifying higher-order (C)LP programs with higher-order assertions. The method uses the Ciao assertion language and extends its predicate properties. The approach refines the syntax and semantics of predicate properties and devises an abstract semantic criterion to check conformance at compile time. It employs an abstract interpretation-based static analyzer where higher-order predicates are reduced to first-order properties, making verification tractable.

Result: The authors present a prototype implementation of their method within the Ciao system and demonstrate its effectiveness using several example programs. The evaluation shows that higher-order predicate properties can be soundly reduced and checked in a static analyzer for first-order assertions.

Conclusion: The proposed approach effectively enables compile-time verification of higher-order assertions in (C)LP, leveraging and extending the existing Ciao assertion language. The technique is general enough to be applicable to similar assertion languages and environments that support higher-order programming.

Abstract: Higher-order constructs enable more expressive and concise code by allowing
procedures to be parameterized by other procedures. Assertions allow expressing
partial program specifications, which can be verified either at compile time
(statically) or run time (dynamically). In higher-order programs, assertions
can also describe higher-order arguments. While in the context of (C)LP,
run-time verification of higher-order assertions has received some attention,
compile-time verification remains relatively unexplored. We propose a novel
approach for statically verifying higher-order (C)LP programs with higher-order
assertions. Although we use the Ciao assertion language for illustration, our
approach is quite general and we believe is applicable to similar contexts.
Higher-order arguments are described using predicate properties -- a special
kind of property which exploits the (Ciao) assertion language. We refine the
syntax and semantics of these properties and introduce an abstract criterion to
determine conformance to a predicate property at compile time, based on a
semantic order relation comparing the predicate property with the predicate
assertions. We then show how to handle these properties using an abstract
interpretation-based static analyzer for programs with first-order assertions
by reducing predicate properties to first-order properties. Finally, we report
on a prototype implementation and evaluate it through various examples within
the Ciao system.

</details>
