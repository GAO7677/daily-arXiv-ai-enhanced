<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 10]
- [cs.PL](#cs.PL) [Total: 8]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [On the need to perform comprehensive evaluations of automated program repair benchmarks: Sorald case study](https://arxiv.org/abs/2508.15135)
*Sumudu Liyanage,Sherlock A. Licorish,Markus Wagner,Stephen G. MacDonell*

Main category: cs.SE

TL;DR: This paper shows that automated program repair tools like Sorald can fix code issues but may also introduce new faults and degrade code quality, suggesting the need for more thorough evaluation frameworks.


<details>
  <summary>Details</summary>
Motivation: Automated program repair (APR) tools are increasingly important for improving software quality, especially with the rise of LLMs. However, current assessments of APR tools often overlook negative side effects such as new violations and reduced code quality.

Method: The paper develops a comprehensive evaluation framework for APR tools and applies it to Sorald, evaluating its repair of 3,529 SonarQube rule violations across 2,393 Java snippets from Stack Overflow.

Result: Sorald successfully fixed specific rule violations but introduced 2,120 new faults (32 bugs, 2,088 code smells), led to a 24% unit test failure rate, and degraded code structure.

Conclusion: Current APR tool evaluations are insufficient and need to include a broader range of effects, including potential harm, to ensure safe adoption. The presented framework highlights these issues and advocates for more comprehensive evaluation methodologies.

Abstract: In supporting the development of high-quality software, especially necessary
in the era of LLMs, automated program repair (APR) tools aim to improve code
quality by automatically addressing violations detected by static analysis
profilers. Previous research tends to evaluate APR tools only for their ability
to clear violations, neglecting their potential introduction of new (sometimes
severe) violations, changes to code functionality and degrading of code
structure. There is thus a need for research to develop and assess
comprehensive evaluation frameworks for APR tools. This study addresses this
research gap, and evaluates Sorald (a state-of-the-art APR tool) as a proof of
concept. Sorald's effectiveness was evaluated in repairing 3,529 SonarQube
violations across 30 rules within 2,393 Java code snippets extracted from Stack
Overflow. Outcomes show that while Sorald fixes specific rule violations, it
introduced 2,120 new faults (32 bugs, 2088 code smells), reduced code
functional correctness--as evidenced by a 24% unit test failure rate--and
degraded code structure, demonstrating the utility of our framework. Findings
emphasize the need for evaluation methodologies that capture the full spectrum
of APR tool effects, including side effects, to ensure their safe and effective
adoption.

</details>


### [2] [Foundational Design Principles and Patterns for Building Robust and Adaptive GenAI-Native Systems](https://arxiv.org/abs/2508.15411)
*Frederik Vandeputte*

Main category: cs.SE

TL;DR: This paper proposes foundational principles and new architectural patterns for building reliable, efficient, and adaptive GenAI-native systems, advocating for integration of AI with traditional software engineering to address current GenAI challenges.


<details>
  <summary>Details</summary>
Motivation: Generative AI (GenAI) has shown significant promise, but its adoption in systems is hindered by unpredictability and inefficiency, motivating the search for more reliable and efficient ways to build GenAI-powered systems.

Method: The paper introduces five foundational design principles (reliability, excellence, evolvability, self-reliance, and assurance) and proposes architectural patterns such as GenAI-native cells, organic substrates, and programmable routers. It also specifies key components for a GenAI-native software stack and discusses multi-perspective impacts.

Result: A conceptual framework for building GenAI-native systems that integrates AI capabilities with classical software engineering, guiding the design of resilient, adaptive, and efficient systems, and encouraging validation and iteration by the community.

Conclusion: Combining AI's cognitive strengths with established software engineering through new design principles and architectural patterns will foster the development of robust and self-evolving GenAI-native systems.

Abstract: Generative AI (GenAI) has emerged as a transformative technology,
demonstrating remarkable capabilities across diverse application domains.
However, GenAI faces several major challenges in developing reliable and
efficient GenAI-empowered systems due to its unpredictability and inefficiency.
This paper advocates for a paradigm shift: future GenAI-native systems should
integrate GenAI's cognitive capabilities with traditional software engineering
principles to create robust, adaptive, and efficient systems.
  We introduce foundational GenAI-native design principles centered around five
key pillars -- reliability, excellence, evolvability, self-reliance, and
assurance -- and propose architectural patterns such as GenAI-native cells,
organic substrates, and programmable routers to guide the creation of resilient
and self-evolving systems. Additionally, we outline the key ingredients of a
GenAI-native software stack and discuss the impact of these systems from
technical, user adoption, economic, and legal perspectives, underscoring the
need for further validation and experimentation. Our work aims to inspire
future research and encourage relevant communities to implement and refine this
conceptual framework.

</details>


### [3] [An Empirical Study of Knowledge Distillation for Code Understanding Tasks](https://arxiv.org/abs/2508.15423)
*Ruiqi Wang,Zezhou Yang,Cuiyun Gao,Xin Xia,Qing Liao*

Main category: cs.SE

TL;DR: Knowledge distillation greatly improves small code-understanding models, with new methods allowing them to perform almost as well as much larger models using far fewer resources; using code-specialized teacher models helps, and matching architectures does not always yield better results.


<details>
  <summary>Details</summary>
Motivation: Deploying large pre-trained language models (PLMs) for code understanding is hampered by their high computational demands and slow inference. Although model compression via knowledge distillation (KD) offers a way to address this, its potential in code understanding tasks has yet to be comprehensively explored.

Method: The paper systematically evaluates knowledge distillation techniques for code understanding. It compares logit-based and feature-based KD methods, using eight student models and two teacher PLMs across three code understanding tasks. The study examines performance retention, parameter efficiency, and the influence of architectural similarity.

Result: Knowledge distillation significantly enhances the performance of compact student models compared to standard fine-tuning, with feature-based methods enabling student models to retain up to 98% of the teacher modelsâ€™ performance while using only 5% of the parameters. Code-specific PLMs work better as teachers, and student-teacher architectural similarity does not guarantee better outcomes.

Conclusion: Knowledge distillation, especially feature-based methods, is highly effective for compressing PLMs in code understanding tasks, enabling efficient deployment without substantial loss in performance. Architectural similarity between student and teacher is not a decisive factor for success. The paper also highlights efficiency considerations and proposes future research directions.

Abstract: Pre-trained language models (PLMs) have emerged as powerful tools for code
understanding. However, deploying these PLMs in large-scale applications faces
practical challenges due to their computational intensity and inference
latency. Knowledge distillation (KD), a promising model compression and
acceleration technique, addresses these limitations by transferring knowledge
from large teacher models to compact student models, enabling efficient
inference while preserving most of the teacher models' capabilities. While this
technique has shown remarkable success in natural language processing and
computer vision domains, its potential for code understanding tasks remains
largely underexplored.
  In this paper, we systematically investigate the effectiveness and usage of
KD in code understanding tasks. Our study encompasses two popular types of KD
methods, i.e., logit-based and feature-based KD methods, experimenting across
eight student models and two teacher PLMs from different domains on three
downstream tasks. The experimental results indicate that KD consistently offers
notable performance boosts across student models with different sizes compared
with standard fine-tuning. Notably, code-specific PLM demonstrates better
effectiveness as the teacher model. Among all KD methods, the latest
feature-based KD methods exhibit superior performance, enabling student models
to retain up to 98% teacher performance with merely 5% parameters. Regarding
student architecture, our experiments reveal that similarity with teacher
architecture does not necessarily lead to better performance. We further
discuss the efficiency and behaviors in the KD process and inference, summarize
the implications of findings, and identify promising future directions.

</details>


### [4] [SynthCoder: A Synthetical Strategy to Tune LLMs for Code Completion](https://arxiv.org/abs/2508.15495)
*Dongjun Yu,Xiao Yan,Zhenrui Li,Jipeng Xiao,Haochuan He,Yongda Yu,Hao Zhang,Guoping Rong,Xiaobo Huang*

Main category: cs.SE

TL;DR: SynthCoder is a code completion model that uses an enriched and diverse training set combined with advanced training strategies to outperform prior models on major benchmarks, while also avoiding issues like excessive code repetition.


<details>
  <summary>Details</summary>
Motivation: Code completion with LLMs is important for software engineering, but optimizing models for this task often results in a trade-off between performance on different datasets or metrics. Existing methods can even degrade performance below the baseline in some cases. The motivation is to address these trade-offs and build a model that consistently excels in code completion.

Method: The authors construct a diverse dataset by combining AST node extraction with heuristics simulating developer behavior and enrich it with cross-file context using the BM25 algorithm and call graphs. Their system, SynthCoder, uses a two-stage training pipeline: first, Curriculum Learning is applied to fine-tune the base model (Seed-Coder-8B-Base), and then Direct Preference Optimization (DPO) is used, employing preference pairs generated via Rejection Sampling for alignment.

Result: SynthCoder achieves state-of-the-art results on mainstream repository-level code completion benchmarks like aiXcoder, ExecRepoBench, CrossCodeEval, and CoLT. The novel training set also reduces the model's tendency to simply repeat existing code, addressing a key limitation of previous approaches.

Conclusion: SynthCoder successfully integrates industry best practices and novel data enrichment strategies to overcome common trade-offs in LLM-based code completion, establishing new performance standards on multiple benchmarks and improving result diversity.

Abstract: Code completion is a prominent application of Large Language Models (LLMs) in
software engineering. Due to the near real-time response requirements of this
task, base models with small to medium-sized parameters are typically employed,
supplemented by various optimization and post-training techniques. However,
these optimization methods often have trade-offs, leading to a seesaw effect
where performance improvements on certain datasets or metrics are accompanied
by degradations on others -- sometimes even falling below the baseline model's
performance. This paper proposes SynthCoder, a model that integrates leading
industry practices to achieve state-of-the-art performance on the
Fill-in-the-Middle (FIM) code completion task. In specific, we first construct
a diverse dataset by combining Abstract Syntax Tree (AST) node extraction with
heuristics that simulate developer behavior. Then we enrich our training corpus
with cross-file contextual information using the BM25 algorithm and call
graphs, enhancing the model's ability to perform code completion in both
file-level and repository-level scenarios. As the last step, we employ a
two-stage training process using the Seed-Coder-8B-Base as the base model.
First, we fine-tune the model using Curriculum Learning technology. Following
this, we perform alignment using Direct Preference Optimization (DPO) with
preference pairs generated through Rejection Sampling. Experimental results
demonstrate that our final model excels on mainstream repository-level code
completion benchmarks, including aiXcoder, ExecRepoBench, CrossCodeEval, and
CoLT. Furthermore, our carefully curated training set effectively mitigates the
model's tendency to just repeat existing code, a common issue existing in
various code completion models.

</details>


### [5] [Towards the Assessment of Task-based Chatbots: From the TOFU-R Snapshot to the BRASATO Curated Dataset](https://arxiv.org/abs/2508.15496)
*Elena Masserini,Diego Clerissi,Daniela Micucci,JoÃ£o R. Campos,Leonardo Mariani*

Main category: cs.SE

TL;DR: This paper introduces two comprehensive datasets, TOFU-R and BRASATO, alongside supporting tools, to advance research in evaluating task-based chatbot reliability by overcoming existing barriers like poor dataset availability and reproducibility.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of high-quality, large-scale datasets for evaluating the reliability, security, and robustness of task-based chatbots. Existing automated quality assessment methods are hampered by limited, outdated, or unavailable chatbot sources.

Method: The paper presents and maintains two new datasets: TOFU-R (a snapshot of Rasa chatbots from GitHub) and BRASATO (a curated collection of notable chatbots), along with supporting tools and documentation.

Result: The authors provide the TOFU-R and BRASATO datasets, enabling reproducibility and more effective research on chatbot reliability, with diverse examples representing real-world and functionally complex agents.

Conclusion: These contributions significantly facilitate research in chatbot quality assessment by offering accessible, updated, and relevant datasets which overcome previous limitations in the field.

Abstract: Task-based chatbots are increasingly being used to deliver real services, yet
assessing their reliability, security, and robustness remains underexplored,
also due to the lack of large-scale, high-quality datasets. The emerging
automated quality assessment techniques targeting chatbots often rely on
limited pools of subjects, such as custom-made toy examples, or outdated, no
longer available, or scarcely popular agents, complicating the evaluation of
such techniques. In this paper, we present two datasets and the tool support
necessary to create and maintain these datasets. The first dataset is RASA
TASK-BASED CHATBOTS FROM GITHUB (TOFU-R), which is a snapshot of the Rasa
chatbots available on GitHub, representing the state of the practice in
open-source chatbot development with Rasa. The second dataset is BOT RASA
COLLECTION (BRASATO), a curated selection of the most relevant chatbots for
dialogue complexity, functional complexity, and utility, whose goal is to ease
reproducibility and facilitate research on chatbot reliability.

</details>


### [6] [Evaluation Guidelines for Empirical Studies in Software Engineering involving LLMs](https://arxiv.org/abs/2508.15503)
*Sebastian Baltes,Florian Angermeir,Chetan Arora,Marvin MuÃ±oz BarÃ³n,Chunyang Chen,Lukas BÃ¶hme,Fabio Calefato,Neil Ernst,Davide Falessi,Brian Fitzgerald,Davide Fucci,Marcos Kalinowski,Stefano Lambiase,Daniel Russo,Mircea Lungu,Lutz Prechelt,Paul Ralph,Christoph Treude,Stefan Wagner*

Main category: cs.SE

TL;DR: The paper identifies key reproducibility issues with LLMs in software engineering and offers a detailed taxonomy and eight practical guidelines to improve study transparency, reproducibility, and replicability. These resources are available online for the community to refine and adopt.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the challenges of applying large language models (LLMs) in software engineering (SE) research, especially due to LLMs' non-determinism, hidden training data, and ever-changing architectures which hamper reproducibility and replication in empirical studies.

Method: The authors conducted a community-driven effort to map the field, establishing a taxonomy for LLM-based studies and crafting eight practical guidelines for the design and reporting of SE experiments using LLMs.

Result: The result is the introduction of a taxonomy for study types and a set of eight clear guidelines, categorized by essential and desirable transparency criteria. These guidelines aim to improve reproducibility and replicability in LLM-focused SE research and are hosted online as an evolving community resource.

Conclusion: The study concludes that following these guidelines will help overcome LLM-specific obstacles to open science, facilitating more transparent, reproducible, and replicable empirical research in SE. The guidelines and study types are intended to be living resources that can adapt and improve with community input.

Abstract: Large language models (LLMs) are increasingly being integrated into software
engineering (SE) research and practice, yet their non-determinism, opaque
training data, and evolving architectures complicate the reproduction and
replication of empirical studies. We present a community effort to scope this
space, introducing a taxonomy of LLM-based study types together with eight
guidelines for designing and reporting empirical studies involving LLMs. The
guidelines present essential (must) criteria as well as desired (should)
criteria and target transparency throughout the research process. Our
recommendations, contextualized by our study types, are: (1) to declare LLM
usage and role; (2) to report model versions, configurations, and fine-tuning;
(3) to document tool architectures; (4) to disclose prompts and interaction
logs; (5) to use human validation; (6) to employ an open LLM as a baseline; (7)
to report suitable baselines, benchmarks, and metrics; and (8) to openly
articulate limitations and mitigations. Our goal is to enable reproducibility
and replicability despite LLM-specific barriers to open science. We maintain
the study types and guidelines online as a living resource for the community to
use and shape (llm-guidelines.org).

</details>


### [7] [QUPER-MAn: Benchmark-Guided Target Setting for Maintainability Requirements](https://arxiv.org/abs/2508.15512)
*Markus Borg,Martin Larsson,Philip Breid,Nadim Hagatulah*

Main category: cs.SE

TL;DR: The study confirms that maintainability is often neglected in software development, typically handled by generic coding guidelines and minimally used tools. The authors suggest that using their QUPER-MAn model can help organizations set clear, measurable maintainability targets, making it a central concern in engineering processes.


<details>
  <summary>Details</summary>
Motivation: Maintainability is critical for sustainable software development, yet often underemphasized in practice. The motivation is to address this gap by using requirements engineering to promote proper attention and goal-setting for maintainable code.

Method: An exploratory industry study examining current practices in requirements engineering concerning maintainability. The authors then propose QUPER-MAn, a maintainability-focused adaptation of the QUPER model, using a design science approach.

Result: Findings confirm previous studies: maintainability is under-prioritized, mostly referenced in broad terms like coding conventions. Maintainability tools are present but used only implicitly. The proposed QUPER-MAn model incorporates benchmarks and supports explicit target-setting for maintainability.

Conclusion: QUPER-MAn can transform maintainability from an afterthought to a managed, goal-oriented quality attribute in software engineering, based on informed decisions.

Abstract: Maintainable source code is essential for sustainable development in any
software organization. Unfortunately, many studies show that maintainability
often receives less attention than its importance warrants. We argue that
requirements engineering can address this gap the problem by fostering
discussions and setting appropriate targets in a responsible manner. In this
preliminary work, we conducted an exploratory study of industry practices
related to requirements engineering for maintainability. Our findings confirm
previous studies: maintainability remains a second-class quality concern.
Explicit requirements often make sweeping references to coding conventions.
Tools providing maintainability proxies are common but typically only used in
implicit requirements related to engineering practices. To address this, we
propose QUPER-MAn, a maintainability adaption of the QUPER model, which was
originally developed to help organizations set targets for performance
requirements. Developed using a design science approach, QUPER-MAn, integrates
maintainability benchmarks and supports target setting. We posit that it can
shift maintainability from an overlooked development consequence to an actively
managed goal driven by informed and responsible engineering decisions.

</details>


### [8] [A Novel Mutation Based Method for Detecting FPGA Logic Synthesis Tool Bugs](https://arxiv.org/abs/2508.15536)
*Yi Zhang,He Jiang,Xiaochen Li,Shikai Guo,Peiyu Zou,Zun Wang*

Main category: cs.SE

TL;DR: The paper proposes VERMEI, a novel automated testing method for FPGA logic synthesis tools, which identifies and exploits inactive code (zombie logic) to generate complex test cases, leading to superior bug discovery compared to state-of-the-art approaches.


<details>
  <summary>Details</summary>
Motivation: FPGA logic synthesis tools are essential but can have defects that pose functional and security risks. Existing automated testing methods struggle to generate test programs with sufficient semantic and logical complexity to robustly evaluate these tools.

Method: The proposed method, VERMEI, includes three modules: 1) Preprocessing, which finds zombie logic through simulation and coverage analysis; 2) Equivalent mutation, which generates complex program variants by manipulating zombie logic using fragments sampled from historical Verilog designs; 3) Bug identification via differential testing, comparing outputs to identify discrepancies.

Result: In extensive experiments on popular synthesis tools (Yosys, Vivado, Quartus), VERMEI was able to identify more bugs than previous methods. Over a five-month period, VERMEI discovered 15 bugs, 9 of which were confirmed as new by vendors.

Conclusion: VERMEI significantly improves the effectiveness of automated testing for FPGA logic synthesis tools by generating complex and diverse test cases that reveal more bugs compared to existing techniques.

Abstract: FPGA (Field-Programmable Gate Array) logic synthesis tools are key components
in the EDA (Electronic Design Automation) toolchain. They convert hardware
designs written in description languages such as Verilog into gate-level
representations for FPGAs. However, defects in these tools may lead to
unexpected behaviors and pose security risks. Therefore, it is crucial to
harden these tools through testing. Although several methods have been proposed
to automatically test FPGA logic synthesis tools, the challenge remains of
insufficient semantic and logical complexity in test programs. In this paper,
we propose VERMEI, a new method for testing FPGA logic synthesis tools. VERMEI
consists of three modules: preprocessing, equivalent mutation, and bug
identification. The preprocessing module identifies zombie logic (inactive code
with no impact on the circuit output) in seed programs through simulation and
coverage analysis. The equivalent mutation module generates equivalent variants
of seed programs by pruning or inserting logic fragments in zombie areas. It
uses Bayesian sampling to extract logic fragments from historical Verilog
designs, making the generated variants have complex control flows and
structures. The bug identification module, based on differential testing,
compares the synthesized outputs of seed and variant programs to identify bugs.
Experiments on Yosys, Vivado, and Quartus demonstrate that VERMEI outperforms
the state-of-the-art methods. Within five months, VERMEI reported 15 bugs to
vendors, 9 of which were confirmed as new.

</details>


### [9] [Establishing Technical Debt Management -- A Five-Step Workshop Approach and an Action Research Study](https://arxiv.org/abs/2508.15570)
*Marion Wiese,Kamila Serwa,Anastasia Besier,Ariane S. Marion-Jetten,Eva Bittner*

Main category: cs.SE

TL;DR: Implementing workshop-based technical debt management processes in an IT team led to greater and sustained TD awareness, with practical approaches such as backlog reminders and new prioritization methods proving effective.


<details>
  <summary>Details</summary>
Motivation: Technical debt (TD) management is commonly studied in research but seldom implemented effectively in practice. The paper seeks to bridge the gap between theoretical approaches and practical adoption in software teams.

Method: The study used action research over five action cycles spanning 16 months with an IT team specialized in signal processing. Data was collected through workshop questionnaires, meeting observations, a psychology-based awareness measurement tool (TD-SAGAT), and backlog data analysis.

Result: Practitioners mainly focus TD repayment and prioritization on system evolution and cost, often addressing the so-called 'low-hanging fruits.' Backlog reminders such as checkboxes or templates significantly and sustainably increased TD awareness among the team.

Conclusion: A workshop-based approach for technical debt management is feasible in real-world IT teams and results in sustainable changes. Innovative TDM practices emerged from this process, such as re-submission dates, discussion checkboxes, and visual tools for prioritization, which could be applied elsewhere.

Abstract: Context. Technical debt (TD) items are constructs in a software system
providing short-term benefits but hindering future changes. TD management (TDM)
is frequently researched but rarely adopted in practice. Goal. This study aimed
to establish a TDM process in an IT company based on a predefined workshop
concept. We analyzed which research approaches practitioners adopted for each
TD activity and the TDM's long-term effect on TD awareness. Method. We used
action research (five action cycles in 16 months) with an IT team that creates
IT solutions for signal processing. To examine TD awareness, we (1) analyzed
questionnaires completed during each workshop, (2) observed team meetings, (3)
adopted a method from psychology for measuring awareness in decision-making
situations called TD-SAGAT, and (4) evaluated the backlog data. Results.
Practitioners preferred TD repayment and prioritization based on the system's
evolution and cost calculations, i.e., repayment of so-called low-hanging
fruits. Reminders in the backlog items, such as checkboxes or text templates,
led to a sustainable rise in TD awareness. Conclusions. We showed that a
workshop-based approach is feasible and leads to sustainable process changes.
New ideas for TDM applicable to other IT teams emerged, e.g., using a
re-submission date, using a Talked about TD checkbox, and using visualizations
for TD prioritization.

</details>


### [10] [From PREVENTion to REACTion: Enhancing Failure Resolution in Naval Systems](https://arxiv.org/abs/2508.15584)
*Maria Teresa Rossi,Leonardo Mariani,Oliviero Riganelli*

Main category: cs.SE

TL;DR: The paper presents an integrated approach to predicting and troubleshooting failures in industrial systems using PREVENT and REACT, showing successful application in naval systems and offering insights for wider use.


<details>
  <summary>Details</summary>
Motivation: Industrial systems often experience failures due to factors like wear, misuse, or faults. Detecting and managing these failures quickly is critical to maintaining system integrity and reliability.

Method: The paper applies PREVENT, a failure prediction method, and extends it with a troubleshooting module called REACT. Both techniques are evaluated in the context of naval systems to demonstrate their effectiveness in integrating anomaly detection with practical troubleshooting.

Result: Experiments show successful integration of anomaly detection and troubleshooting procedures in naval systems. The authors share practical lessons that can aid in deploying such approaches in other industrial settings.

Conclusion: The research demonstrates that combining state-of-the-art failure prediction (PREVENT) with a troubleshooting module (REACT) enhances detection and management of faults in industrial systems. The integrated approach is effective and holds promise for broader adoption.

Abstract: Complex and large industrial systems often misbehave, for instance, due to
wear, misuse, or faults. To cope with these incidents, it is important to
timely detect their occurrences, localize the sources of the problems, and
implement the appropriate countermeasures. This paper reports our experience
with a state-of-the-art failure prediction method, PREVENT, and its extension
with a troubleshooting module, REACT, applied to naval systems developed by
Fincantieri. Our results show how to integrate anomaly detection with
troubleshooting procedures. We conclude by discussing a lesson learned, which
may help deploy and extend these analyses to other industrial products.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [11] [Homomorphism Calculus for User-Defined Aggregations](https://arxiv.org/abs/2508.15109)
*Ziteng Wang,Ruijie Fang,Linus Zheng,Dixin Tang,Isil Dillig*

Main category: cs.PL

TL;DR: The paper presents a homomorphism calculus that automates checking and enabling efficient parallel execution of user-defined aggregation functions in data frameworks, and demonstrates clear performance improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: Efficient execution of user-defined aggregation functions (UDAFs) in data processing frameworks requires that these functions possess the homomorphism property, which allows for correct merging of partial results. However, verifying this property and constructing the necessary merge operators can be challenging.

Method: The paper introduces a novel 'homomorphism calculus' capable of verifying and refuting whether a UDAF satisfies the dataframe homomorphism property. If the property is verified, the calculus also facilitates the automatic construction of an appropriate merge operator for incremental and parallel computation.

Result: An algorithm implementing the homomorphism calculus was developed and tested on real-world UDAFs. The evaluation shows that this approach significantly outperforms two leading synthesizers in terms of efficiency.

Conclusion: The proposed homomorphism calculus provides a practical solution for verifying homomorphism properties of UDAFs, as well as constructing merge operators, enhancing the efficiency of user-defined aggregation in data processing frameworks like Spark and Flink.

Abstract: Data processing frameworks like Apache Spark and Flink provide built-in
support for user-defined aggregation functions (UDAFs), enabling the
integration of domain-specific logic. However, for these frameworks to support
\emph{efficient} UDAF execution, the function needs to satisfy a
\emph{homomorphism property}, which ensures that partial results from
independent computations can be merged correctly. Motivated by this problem,
this paper introduces a novel \emph{homomorphism calculus} that can both verify
and refute whether a UDAF is a dataframe homomorphism. If so, our calculus also
enables the construction of a corresponding merge operator which can be used
for incremental computation and parallel execution. We have implemented an
algorithm based on our proposed calculus and evaluate it on real-world UDAFs,
demonstrating that our approach significantly outperforms two leading
synthesizers.

</details>


### [12] [Software Model Checking via Summary-Guided Search (Extended Version)](https://arxiv.org/abs/2508.15137)
*Ruijie Fang,Zachary Kincaid,Thomas Reps*

Main category: cs.PL

TL;DR: GPS is a new software model-checking algorithm that efficiently discovers bugs and proofs, outperforming leading tools in both benchmarks solved and speed, through guided search and static analysis summaries.


<details>
  <summary>Details</summary>
Motivation: Traditional software model checkers can struggle to efficiently find bugs, especially in programs with long, input-dependent error paths, and often balance between completeness and performance.

Method: They propose GPS, a new model-checking algorithm that uses directed search of program states guided by compositional, summary-based static analysis. The algorithm prunes infeasible paths, generates tests to explore new program states, and uses a novel two-layered search strategy. An additional instrumentation technique is introduced for refutational completeness.

Result: GPS is shown to be refutationally complete (capable of finding errors if enough time is given). Empirical benchmarks on SV-COMP and literature programs show GPS outperforms state-of-the-art model checkers both in number of benchmarks solved and runtime.

Conclusion: GPS provides an efficient, refutationally complete model checker that effectively finds bugs and proofs of safety, with superior performance compared to current best tools.

Abstract: In this work, we describe a new software model-checking algorithm called GPS.
GPS treats the task of model checking a program as a directed search of the
program states, guided by a compositional, summary-based static analysis. The
summaries produced by static analysis are used both to prune away infeasible
paths and to drive test generation to reach new, unexplored program states. GPS
can find both proofs of safety and counter-examples to safety (i.e., inputs
that trigger bugs), and features a novel two-layered search strategy that
renders it particularly efficient at finding bugs in programs featuring long,
input-dependent error paths. To make GPS refutationally complete (in the sense
that it will find an error if one exists, if it is allotted enough time), we
introduce an instrumentation technique and show that it helps GPS achieve
refutation-completeness without sacrificing overall performance. We benchmarked
GPS on a suite of benchmarks including both programs from the Software
Verification Competition (SV-COMP) and from prior literature, and found that
our implementation of GPS outperforms state-of-the-art software model checkers
(including the top performers in SV-COMP ReachSafety-Loops category), both in
terms of the number of benchmarks solved and in terms of running time.

</details>


### [13] [Big-Stop Semantics: A Simple Way to Get the Benefits of Small-Step Semantics in a Big-Step Judgment](https://arxiv.org/abs/2508.15157)
*David M Kahn,Jan Hoffmann,Runming Li*

Main category: cs.PL

TL;DR: This paper presents a straightforward extension to big-step semanticsâ€”called 'big-stop semantics'â€”using a handful of new rules, letting it describe diverging computations (like infinite loops) efficiently. Unlike previous approaches, it preserves simplicity, making it practical for language designers and theorists.


<details>
  <summary>Details</summary>
Motivation: Big-step semantics, while more ergonomic and requiring fewer inference rules than small-step semantics, traditionally cannot describe divergence (non-terminating behavior). The motivation is to extend big-step semantics to capture such behavior without resorting to complex or less-practical techniques used in previous literature.

Method: The authors extend traditional big-step semantics by adding a few inductive rules, allowing the semantics to describe not only terminating but also diverging computations, and illustrate this approach across several language variants, including PCF and imperative languages.

Result: The proposed 'big-stop semantics' provides a semantics that is equivalent to the reflexive-transitive closure of small-step semantics, enables direct reasoning about divergence, and maintains the simplicity and practicality of traditional big-step style, avoiding error states or complicated transformers like coinduction.

Conclusion: The paper introduces 'big-stop semantics', an extension to big-step semantics, which elegantly captures both normal and diverging computations without sacrificing the ergonomic advantages of big-step semantics.

Abstract: As evident in the programming language literature, many practitioners favor
specifying dynamic program behavior using big-step over small-step semantics.
Unlike small-step semantics, which must dwell on every intermediate program
state, big-step semantics conveniently jump directly to the ever-important
result of the computation. Big-step semantics also typically involve fewer
inference rules than their small-step counterparts. However, in exchange for
ergonomics, big-step semantics give up power: Small-step semantics describes
program behaviors that are outside the grasp of big-step semantics, notably
divergence. This work presents a little-known extension of big-step semantics
with inductive definitions that captures diverging computations without
introducing error states. This big-stop semantics is illustrated for typed,
untyped, and effectful variants of PCF, as well as a while-loop-based
imperative language. Big-stop semantics extends the standard big-step inference
rules with a few additional rules to define an evaluation judgment that is
equivalent to the reflexive-transitive closure of small-step transitions. This
simple extension contrasts with other solutions in the literature which
sacrifice ergonomics by introducing many additional inference rules, global
state, and/or less-commonly-understood reasoning principles like coinduction.

</details>


### [14] [Probabilistic Inference for Datalog with Correlated Inputs](https://arxiv.org/abs/2508.15166)
*Jingbo Wang,Shashin Halalingaiah,Weiyi Chen,Chao Wang,Isil Dillig*

Main category: cs.PL

TL;DR: This paper introduces Praline, improving probabilistic logic programming with a scalable approach to handle correlated inputs, achieving precise and scalable probabilistic inference in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing probabilistic logic programming languages like ProbLog do not handle statistical correlations among input facts, limiting the precision of probabilistic inference.

Method: Praline, a new Datalog extension, models the inference task as a constrained optimization problem to capture input correlations and uses a scalable delta-exact inference algorithm combining constraint solving, static analysis, and iterative refinement.

Result: Empirical results on real-world benchmarks, such as side-channel analysis, show that Praline scales well and provides precise (tight) probability bounds for output facts.

Conclusion: Praline advances probabilistic logic programming by enabling sound and precise probabilistic inference even in the presence of correlated input facts, overcoming scalability limitations of earlier approaches.

Abstract: Probabilistic extensions of logic programming languages, such as ProbLog,
integrate logical reasoning with probabilistic inference to evaluate
probabilities of output relations; however, prior work does not account for
potential statistical correlations among input facts. This paper introduces
Praline, a new extension to Datalog designed for precise probabilistic
inference in the presence of (partially known) input correlations. We formulate
the inference task as a constrained optimization problem, where the solution
yields sound and precise probability bounds for output facts. However, due to
the complexity of the resulting optimization problem, this approach alone often
does not scale to large programs. To address scalability, we propose a more
efficient $\delta$-exact inference algorithm that leverages constraint solving,
static analysis, and iterative refinement. Our empirical evaluation on
challenging real-world benchmarks, including side-channel analysis,
demonstrates that our method not only scales effectively but also delivers
tight probability bounds.

</details>


### [15] [Exploring the Theory and Practice of Concurrency in the Entity-Component-System Pattern](https://arxiv.org/abs/2508.15264)
*Patrick Redmond,Jonathan Castello,JosÃ© Manuel CalderÃ³n Trilla,Lindsey Kuper*

Main category: cs.PL

TL;DR: This paper formalizes the ECS design pattern with Core ECS, highlights unexplored opportunities for deterministic concurrency in current frameworks, and calls for improved implementations based on this deeper understanding.


<details>
  <summary>Details</summary>
Motivation: Although the Entity-Component-System (ECS) pattern offers modularity, flexibility, and performance, its rigorous understanding is limited, and its usage outside of game development and similar domains is minimal. Existing explanations often focus too much on specific frameworks or use metaphors, rather than providing a clear, abstract definition.

Method: The authors design a formal model called Core ECS to abstractly define the ECS pattern, removing implementation details. They identify classes of programs within this model that are deterministic regardless of scheduling. The authors also survey several existing ECS frameworks, comparing them to their formal model.

Result: They found that real-world ECS frameworks do not fully exploit opportunities for deterministic concurrency. Their formal Core ECS model demonstrates such capabilities, indicating possibilities for new implementations that provide better deterministic concurrent behavior.

Conclusion: A formal, abstract model (Core ECS) clarifies the essence and power of the ECS pattern, especially with respect to deterministic concurrency, revealing limitations and untapped potential in modern ECS frameworks.

Abstract: The Entity-Component-System (ECS) software design pattern, long used in game
development, encourages a clean separation of identity (entities), data
properties (components), and computational behaviors (systems). Programs
written using the ECS pattern are naturally concurrent, and the pattern offers
modularity, flexibility, and performance benefits that have led to a
proliferation of ECS frameworks. Nevertheless, the ECS pattern is little-known
and not well understood outside of a few domains. Existing explanations of the
ECS pattern tend to be mired in the concrete details of particular ECS
frameworks, or they explain the pattern in terms of imperfect metaphors or in
terms of what it is not. We seek a rigorous understanding of the ECS pattern
via the design of a formal model, Core ECS, that abstracts away the details of
specific implementations to reveal the essence of software using the ECS
pattern. We identify a class of Core ECS programs that behave deterministically
regardless of scheduling, enabling use of the ECS pattern as a
deterministic-by-construction concurrent programming model. With Core ECS as a
point of comparison, we then survey several real-world ECS frameworks and find
that they all leave opportunities for deterministic concurrency unexploited.
Our findings point out a space for new ECS implementation techniques that
better leverage such opportunities.

</details>


### [16] [Fair Termination for Resource-Aware Active Objects](https://arxiv.org/abs/2508.15333)
*Francesco Dagnino,Paola Giannini,Violet Ka I Pun,Ulises Torrella*

Main category: cs.PL

TL;DR: This paper introduces a core calculus and type system for resource-aware active object systems, ensuring that well-typed programs always fairly terminate by integrating techniques from graded semantics and fair termination.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a formal model for distributed and concurrent systems that are resource-aware, to improve modelling of distributed computations such as business workflows. Ensuring fairness and termination is a key concern in such systems.

Method: The paper develops a core calculus for resource-aware active objects, together with a type system. It combines graded semantics and type system techniques from sequential programming with fair termination methods from synchronous sessions.

Result: The result is a formal framework and type system where any well-typed program is guaranteed to be fairly terminating, ensuring programs will eventually terminate under fair scheduling.

Conclusion: The proposed calculus and type system ensure fair termination in resource-aware active object systems, advancing concurrency modelling with termination guarantees.

Abstract: Active object systems are a model of distributed computation that has been
adopted for modelling distributed systems and business process workflows. This
field of modelling is, in essence, concurrent and resource-aware, motivating
the development of resource-aware formalisations on the active object model.
The contributions of this work are the development of a core calculus for
resource-aware active objects together with a type system ensuring that
well-typed programs are fairly terminating, i.e., they can always eventually
terminate. To achieve this, we combine techniques from graded semantics and
type systems, which are quite well understood for sequential programs, with
those for fair termination, which have been developed for synchronous~sessions.

</details>


### [17] [Compositional Symbolic Execution for the Next 700 Memory Models (Extended Version)](https://arxiv.org/abs/2508.15576)
*Andreas LÃ¶Ã¶w,Seung Hoon Park,Daniele Nantes-Sobrinho,Sacha-Ã‰lie Ayoun,Opale SjÃ¶stedt,Philippa Gardner*

Main category: cs.PL

TL;DR: This paper introduces a formal foundation for symbolic execution platforms that support multiple memory models, enabling broader language and analysis coverage with assured compatibility and rigor.


<details>
  <summary>Details</summary>
Motivation: Existing compositional symbolic execution (CSE) platforms benefit from customizable memory models for verification and bug-finding. While tools like Gillian allow memory model parametricity, there is no satisfactory formal foundation for such parametric CSE platforms.

Method: The authors introduce a novel formal foundation for memory-model-parametric CSE platforms, mechanizing it in the Rocq interactive theorem prover. They validate their approach by instantiating it with various memory models, including those for C and CHERI. Their foundation accommodates both separation logic (SL) and incorrectness separation logic (ISL) analyses, built upon standard definitions.

Result: The proposed formal foundation is mechanized in Rocq, supports a wide array of memory models, accommodates both SL and ISL, and uses standard definitions to ensure compatibility with existing tools.

Conclusion: The new foundation provides the necessary formal underpinnings for memory-model-parametric CSE platforms, enhancing flexibility, usability for different languages and analyses, and facilitating sound interoperability with other tools.

Abstract: Multiple successful compositional symbolic execution (CSE) tools and
platforms exploit separation logic (SL) for compositional verification and/or
incorrectness separation logic (ISL) for compositional bug-finding, including
VeriFast, Viper, Gillian, CN, and Infer-Pulse. Previous work on the Gillian
platform, the only CSE platform that is parametric on the memory model, meaning
that it can be instantiated to different memory models, suggests that the
ability to use custom memory models allows for more flexibility in supporting
analysis of a wide range of programming languages, for implementing custom
automation, and for improving performance. However, the literature lacks a
satisfactory formal foundation for memory-model-parametric CSE platforms.
  In this paper, inspired by Gillian, we provide a new formal foundation for
memory-model-parametric CSE platforms. Our foundation advances the state of the
art in four ways. First, we mechanise our foundation (in the interactive
theorem prover Rocq). Second, we validate our foundation by instantiating it to
a broad range of memory models, including models for C and CHERI. Third,
whereas previous memory-model-parametric work has only covered SL analyses, we
cover both SL and ISL analyses. Fourth, our foundation is based on standard
definitions of SL and ISL (including definitions of function specification
validity, to ensure sound interoperation with other tools and platforms also
based on standard definitions).

</details>


### [18] [Active Learning for Neurosymbolic Program Synthesis](https://arxiv.org/abs/2508.15750)
*Celeste Barnaby,Qiaochu Chen,Ramya Ramalingam,Osbert Bastani,Isil Dillig*

Main category: cs.PL

TL;DR: This paper introduces a new active learning method for neurosymbolic program synthesis, addressing neural mispredictions by using constrained conformal evaluation (CCE). The implemented tool, SmartLabel, outperforms previous approaches, achieving near-perfect accuracy and fewer user interactions in experiments.


<details>
  <summary>Details</summary>
Motivation: Active learning in program synthesis aims to minimize user interaction by asking targeted questions. Traditional methods work well in symbolic settings but struggle with neurosymbolic program synthesis due to neural component mispredictions.

Method: The paper proposes a new active learning approach, introducing constrained conformal evaluation (CCE), which considers neural mispredictions and integrates user feedback. The method iteratively refines CCE until only observationally equivalent programs remain. Implementation is done in a tool called SmartLabel, tested in three neurosymbolic domains.

Result: SmartLabel identifies the ground truth program in 98% of benchmarks, requiring less than 5 rounds of user interaction on average. Previous methods only achieve up to 65% accuracy in finding the correct program.

Conclusion: The presented active learning technique, based on CCE, substantially improves the effectiveness and efficiency of program synthesis in neurosymbolic settings by better handling neural mispredictions and reducing user interaction.

Abstract: The goal of active learning for program synthesis is to synthesize the
desired program by asking targeted questions that minimize user interaction.
While prior work has explored active learning in the purely symbolic setting,
such techniques are inadequate for the increasingly popular paradigm of
neurosymbolic program synthesis, where the synthesized program incorporates
neural components. When applied to the neurosymbolic setting, such techniques
can -- and, in practice, do -- return an unintended program due to
mispredictions of neural components. This paper proposes a new active learning
technique that can handle the unique challenges posed by neural network
mispredictions. Our approach is based upon a new evaluation strategy called
constrained conformal evaluation (CCE), which accounts for neural
mispredictions while taking into account user-provided feedback. Our proposed
method iteratively makes CCE more precise until all remaining programs are
guaranteed to be observationally equivalent. We have implemented this method in
a tool called SmartLabel and experimentally evaluated it on three neurosymbolic
domains. Our results demonstrate that SmartLabel identifies the ground truth
program for 98% of the benchmarks, requiring under 5 rounds of user interaction
on average. In contrast, prior techniques for active learning are only able to
converge to the ground truth program for at most 65% of the benchmarks.

</details>
