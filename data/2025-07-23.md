<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.PL](#cs.PL) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?](https://arxiv.org/abs/2507.15887)
*Ori Press,Brandon Amos,Haoyu Zhao,Yikai Wu,Samuel K. Ainsworth,Dominik Krupke,Patrick Kidger,Touqir Sajed,Bartolomeo Stellato,Jisun Park,Nathanael Bosch,Eli Meril,Albert Steppi,Arman Zharmagambetov,Fangzhao Zhang,David Perez-Pineiro,Alberto Mercurio,Ni Zhan,Talor Abramovich,Kilian Lieret,Hanlin Zhang,Shirley Huang,Matthias Bethge,Ofir Press*

Main category: cs.SE

TL;DR: AlgoTune is a new benchmark for evaluating language models' ability to create efficient algorithms for challenging problems. While current models, including the AlgoTuner agent, can speed up existing solutions, they show little true innovation. The benchmark aims to inspire advancements in models that can surpass human creative problem-solving skills.


<details>
  <summary>Details</summary>
Motivation: Most current evaluations of language models (LMs) focus on tasks already solved by humans, especially in programming and mathematics. There is a lack of benchmarks testing models' ability to develop novel, efficient algorithms for computationally hard problems.

Method: The authors introduce AlgoTune, an open-ended benchmark comprising 155 coding tasks collected from domain experts in computer science, physics, and mathematics. This includes a framework for validating and timing LM-generated solutions and comparing them to reference implementations from open-source libraries. They also develop a baseline LM agent, AlgoTuner, to evaluate current frontier models.

Result: AlgoTuner demonstrates an average 1.72x speedup compared to reference solvers using standard libraries. However, existing models typically perform only superficial optimizations and fail to generate novel algorithmic ideas.

Conclusion: Current language models, as assessed with AlgoTune, tend to optimize existing solutions rather than invent new algorithms. The benchmark provides a platform to drive progress toward LMs with more advanced, creative problem-solving abilities.

Abstract: Despite progress in language model (LM) capabilities, evaluations have thus
far focused on models' performance on tasks that humans have previously solved,
including in programming (Jimenez et al., 2024) and mathematics (Glazer et al.,
2024). We therefore propose testing models' ability to design and implement
algorithms in an open-ended benchmark: We task LMs with writing code that
efficiently solves computationally challenging problems in computer science,
physics, and mathematics. Our AlgoTune benchmark consists of 155 coding tasks
collected from domain experts and a framework for validating and timing
LM-synthesized solution code, which is compared to reference implementations
from popular open-source packages. In addition, we develop a baseline LM agent,
AlgoTuner, and evaluate its performance across a suite of frontier models.
AlgoTuner achieves an average 1.72x speedup against our reference solvers,
which use libraries such as SciPy, sk-learn and CVXPY. However, we find that
current models fail to discover algorithmic innovations, instead preferring
surface-level optimizations. We hope that AlgoTune catalyzes the development of
LM agents exhibiting creative problem solving beyond state-of-the-art human
performance.

</details>


### [2] [Dr. Boot: Bootstrapping Program Synthesis Language Models to Perform Repairing](https://arxiv.org/abs/2507.15889)
*Noah van der Vleuten*

Main category: cs.SE

TL;DR: The paper proposes a bootstrapping approach that teaches language models to repair code during training, improving program synthesis performance beyond traditional methodsâ€”even with smaller models. The method also reveals issues with commonly used datasets, which may affect future research.


<details>
  <summary>Details</summary>
Motivation: Current program synthesis language models are limited by the small and low-quality datasets and are misaligned with human programming workflows. Humans use iterative development and repair code using compilers, but models generate code in a single step. This motivates developing better training techniques to close this gap.

Method: The paper introduces a bootstrapping algorithm specifically designed for program synthesis, which teaches models not just to generate code but also to repair it. The approach involves training models to iteratively fix their code using compiler feedback, more closely mimicking human coding practices.

Result: Bootstrapping resulted in consistently better performance than regular fine-tuning. The bootstrapped models match the performance of much larger models trained with traditional fine-tuning. Moreover, teaching models to repair code also improved their regular code generation abilities. However, directly repairing during inference was less effective than generating multiple samples and choosing the best one. Additionally, the paper identifies flaws in the training test cases of the APPS dataset, which could impact methods relying on them.

Conclusion: Bootstrapping and teaching models how to repair code leads to more robust program synthesis models, especially with limited data. This method narrows the gap between human and model coding behaviors, and can achieve competitive performance with smaller model sizes. Problems with existing benchmarks (like APPS) need to be addressed for trustworthy further research.

Abstract: Language models for program synthesis are usually trained and evaluated on
programming competition datasets (MBPP, APPS). However, these datasets are
limited in size and quality, while these language models are extremely data
hungry. Additionally, the language models have a misaligned program synthesis
process compared to humans. While humans iteratively develop code with the help
of a compiler, most program synthesis models currently produce code in one go.
To solve these issues, we introduce a bootstrapping algorithm for program
synthesis, that supports teaching models how to repair. We show that
bootstrapping consistently outperforms regular fine-tuning. Compared to other
work, our bootstrapped model performs on par with fine-tuned models that are
68\% larger. Notably, bootstrapping with repairing also improves non-repairing
performance compared to regular bootstrapping during inference. However, on our
models, repairing during inference is likely inferior to simply sampling the
same number of solutions. Furthermore, we find that there are issues with the
example test cases in the training portion of the APPS dataset that are
valuable to the community, as many repairing and reinforcement learning methods
rely on them.

</details>


### [3] [StaAgent: An Agentic Framework for Testing Static Analyzers](https://arxiv.org/abs/2507.15892)
*Elijah Nnorom,Md Basim Uddin Ahmed,Jiho Shin,Hung Viet Pham,Song Wang*

Main category: cs.SE

TL;DR: This paper presents StaAgent, a novel framework utilizing LLMs to systematically test and uncover flaws in static analyzer rules. The approach outperformed baselines by detecting dozens of previously missed bugs, showing the value of LLM-driven, agent-based testing in software engineering.


<details>
  <summary>Details</summary>
Motivation: Static analyzers are essential for early bug detection in software development, but the detection rules they rely on are often under-tested, which can lead to missed bugs and inconsistencies. Improving the reliability and coverage of these rules is important for developing more robust software.

Method: The paper introduces StaAgent, an agentic framework that leverages large language models (LLMs) in a multi-agent approach to systematically test static analyzer rules. It involves four agents: Seed Generation, Code Validation, Mutation Generation, and Analyzer Evaluation. These agents work together to generate and validate bug-inducing programs, create mutants, and compare analyzer behaviors to reveal inconsistencies using metamorphic testing.

Result: The authors evaluated StaAgent using five LLMs and five industry-standard static analyzers. StaAgent successfully identified 64 problematic rules across different analyzers, of which 53 were missed by the current state-of-the-art baseline methods. Several bugs were confirmed or fixed based on their reports, evidencing practical value.

Conclusion: StaAgent, an LLM-driven and multi-agent system, provides an effective, scalable, and adaptable solution for uncovering flaws in static analyzer rule implementations. Experimental validation demonstrates significant improvement over existing approaches, suggesting a promising direction for combining LLMs with software engineering tools.

Abstract: Static analyzers play a critical role in identifying bugs early in the
software development lifecycle, but their rule implementations are often
under-tested and prone to inconsistencies. To address this, we propose
StaAgent, an agentic framework that harnesses the generative capabilities of
Large Language Models (LLMs) to systematically evaluate static analyzer rules.
StaAgent comprises four specialized agents: a Seed Generation Agent that
translates bug detection rules into concrete, bug-inducing seed programs; a
Code Validation Agent that ensures the correctness of these seeds; a Mutation
Generation Agent that produces semantically equivalent mutants; and an Analyzer
Evaluation Agent that performs metamorphic testing by comparing the static
analyzer's behavior on seeds and their corresponding mutants. By revealing
inconsistent behaviors, StaAgent helps uncover flaws in rule implementations.
This LLM-driven, multi-agent framework offers a scalable and adaptable solution
to improve the reliability of static analyzers. We evaluated StaAgent with five
state-of-the-art LLMs (CodeL-lama, DeepSeek, Codestral, Qwen, and GPT-4o)
across five widely used static analyzers (SpotBugs, SonarQube, ErrorProne,
Infer, and PMD). The experimental results show that our approach can help
reveal 64 problematic rules in the latest versions of these five static
analyzers (i.e., 28 in SpotBugs, 18 in SonarQube, 6 in ErrorProne, 4 in Infer,
and 8 in PMD). In addition, 53 out of the 64 bugs cannot be detected by the
SOTA baseline. We have reported all the bugs to developers, with two of them
already fixed. Three more have been confirmed by developers, while the rest are
awaiting response. These results demonstrate the effectiveness of our approach
and underscore the promise of agentic, LLM-driven data synthesis to advance
software engineering.

</details>


### [4] [A Pilot Study on LLM-Based Agentic Translation from Android to iOS: Pitfalls and Insights](https://arxiv.org/abs/2507.16037)
*Zhili Zeng,Kimya Khakzad Shahandashti,Alvine Boaye Belle,Song Wang,Zhen Ming,Jiang*

Main category: cs.SE

TL;DR: This paper investigates using LLM-driven agent chains for translating Android apps to iOS, systematically assessing their successes and failures, and providing practical guidelines for improvement.


<details>
  <summary>Details</summary>
Motivation: The growing demand for effective cross-platform mobile application translation (especially between Android and iOS) and the shortcomings of both manual, rule-based, and existing machine learning approaches motivate this work. There is a need for automated solutions that account for contextual understanding and adaptability in translation.

Method: The paper develops a chain of LLM-based agents to handle Android-to-iOS application translation, explicitly considering dependencies, program specifications, structure, and control flow. Manual examination of translated code is performed to assess syntactic, semantic, and functional accuracy, with root cause analysis for translation failures.

Result: The study identifies key failure points and limitations in the LLM-based agentic translation process, provides an evaluation of their performance, and proposes guidelines to improve future translations.

Conclusion: LLM-based agentic approaches show promise for automating cross-platform mobile application translation but currently face limitations in fully capturing context, dependencies, and complex control flows. Detailed evaluation and root cause analysis can inform guidelines for improved future designs.

Abstract: The rapid advancement of mobile applications has led to a significant demand
for cross-platform compatibility, particularly between the Android and iOS
platforms. Traditional approaches to mobile application translation often rely
on manual intervention or rule-based systems, which are labor-intensive and
time-consuming. While recent advancements in machine learning have introduced
automated methods, they often lack contextual understanding and adaptability,
resulting in suboptimal translations. Large Language Models (LLMs) were
recently leveraged to enhance code translation at different granularities,
including the method, class, and repository levels. Researchers have
investigated common errors, limitations, and potential strategies to improve
these tasks. However, LLM-based application translation across different
platforms, such as migrating mobile applications between Android and iOS or
adapting software across diverse frameworks, remains underexplored.
Understanding the performance, strengths, and limitations of LLMs in
cross-platform application translation is critical for advancing software
engineering automation. This study aims to fill this gap by evaluating
LLM-based agentic approaches for mobile application translation, identifying
key failure points, and proposing guidelines to improve translation
performance. We developed a chain of agents that account for dependencies,
specifications, program structure, and program control flow when translating
applications from Android to iOS. To evaluate the performance, we manually
examined the translated code for syntactic correctness, semantic accuracy, and
functional completeness. For translation failures, we further conducted a
detailed root cause analysis to understand the underlying limitations of the
agentic translation process and identify opportunities for improvement.

</details>


### [5] [Making REST APIs Agent-Ready: From OpenAPI to Model Context Protocol Servers for Tool-Augmented LLMs](https://arxiv.org/abs/2507.16044)
*Meriem Mastouri,Emna Ksontini,Wael Kessentini*

Main category: cs.SE

TL;DR: Constructing MCP servers for integrating tools with LLMs is currently a manual, repetitive process slowing broader adoption. This paper presents AutoMCP, a compiler that generates complete MCP servers from OpenAPI specs. Tested on 50 APIs, it succeeded in 76.5% of cases without intervention and 99.9% after small spec fixes. Automating this process is not only possible but highly effective, as long as minor API specification errors are corrected.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the manual and repetitive process involved in constructing Model Context Protocol (MCP) servers, which are crucial for integrating tools with Large Language Models (LLMs). Despite MCP's goal of simplifying integration, current adoption is low and mostly consists of small, repetitive projects. Automating MCP server creation could significantly reduce developer effort and accelerate the adoption and utility of LLM-connected tools.

Method: The authors analyzed adoption trends of MCP on GitHub, reviewed over 22,000 repositories, and identified the need for automation in server construction. To solve this, they introduced AutoMCP, a compiler that generates MCP servers automatically from OpenAPI 2.0/3.0 specifications by parsing REST API definitions and handling schema registration and authentication. The tool was evaluated on 50 real-world APIs, making 1,023 tool calls to assess its efficacy and diagnosing issues when failures occurred.

Result: AutoMCP succeeded in 76.5% of tool calls on first attempt. Upon minor manual fixes to OpenAPI specs (averaging 19 additional lines per API), the success rate increased dramatically to 99.9%. The majority of the failures were due to inconsistencies or omissions in the OpenAPI contracts rather than flaws in AutoMCP itself. The study also produced a corpus of 5,066 callable tools and provided insights into common specification repair needs.

Conclusion: OpenAPI specifications, even with quality issues, can be leveraged to automate nearly all aspects of MCP server construction, substantially reducing the manual labor previously required. Widespread adoption of automation tools like AutoMCP is feasible and beneficial, provided there is minor intervention to fix recurrent specification issues.

Abstract: Large Language Models (LLMs) are evolving from passive text generators into
active agents that invoke external tools. To support this shift, scalable
protocols for tool integration are essential. The Model Context Protocol (MCP),
introduced by Anthropic in 2024, offers a schema-driven standard for dynamic
tool discovery and invocation. Yet, building MCP servers remains manual and
repetitive, requiring developers to write glue code, handle authentication, and
configure schemas by hand-replicating much of the integration effort MCP aims
to eliminate.
  This paper investigates whether MCP server construction can be meaningfully
automated. We begin by analyzing adoption trends: among 22,000+ MCP-tagged
GitHub repositories created within six months of release, fewer than 5% include
servers, typically small, single-maintainer projects dominated by repetitive
scaffolding. To address this gap, we present AutoMCP, a compiler that generates
MCP servers from OpenAPI 2.0/3.0 specifications. AutoMCP parses REST API
definitions and produces complete server implementations, including schema
registration and authentication handling.
  We evaluate AutoMCP on 50 real-world APIs spanning 5,066 endpoints across
over 10 domains. From a stratified sample of 1,023 tool calls, 76.5% succeeded
out of the box. Manual failure analysis revealed five recurring issues, all
attributable to inconsistencies or omissions in the OpenAPI contracts. After
minor fixes, averaging 19 lines of spec changes per API, AutoMCP achieved 99.9%
success.
  Our findings (i) analyze MCP adoption and quantify the cost of manual server
development, (ii) demonstrate that OpenAPI specifications, despite quality
issues, enable near-complete MCP server automation, and (iii) contribute a
corpus of 5,066 callable tools along with insights on repairing common
specification flaws.

</details>


### [6] [AI-Powered Commit Explorer (APCE)](https://arxiv.org/abs/2507.16063)
*Yousab Grees,Polina Iaremchuk,Ramtin Ehsani,Esteban Parra,Preetha Chatterjee,Sonia Haiduc*

Main category: cs.SE

TL;DR: The paper presents APCE, a tool that helps generate, enhance, store, and evaluate commit messages produced by large language models, addressing the common neglect of high-quality commit documentation.


<details>
  <summary>Details</summary>
Motivation: Commit messages are important for understanding code changes, but high-quality messages are often neglected. LLMs can generate such messages, but tools to support and evaluate them are lacking.

Method: The authors introduce APCE, a tool for using, managing prompts, and evaluating LLM-generated commit messages. The tool enables storing prompts, using evaluation prompts, and supports both automated and human evaluations.

Result: APCE allows users to generate, enhance, store, and evaluate LLM-generated commit messages efficiently.

Conclusion: APCE can help developers and researchers more effectively leverage and assess LLM-generated commit messages, improving the quality and utility of software documentation.

Abstract: Commit messages in a version control system provide valuable information for
developers regarding code changes in software systems. Commit messages can be
the only source of information left for future developers describing what was
changed and why. However, writing high-quality commit messages is often
neglected in practice. Large Language Model (LLM) generated commit messages
have emerged as a way to mitigate this issue. We introduce the AI-Powered
Commit Explorer (APCE), a tool to support developers and researchers in the use
and study of LLM-generated commit messages. APCE gives researchers the option
to store different prompts for LLMs and provides an additional evaluation
prompt that can further enhance the commit message provided by LLMs. APCE also
provides researchers with a straightforward mechanism for automated and human
evaluation of LLM-generated messages. Demo link https://youtu.be/zYrJ9s6sZvo

</details>


### [7] [Ten Essential Guidelines for Building High-Quality Research Software](https://arxiv.org/abs/2507.16166)
*Nasir U. Eisty,David E. Bernholdt,Alex Koufos,David J. Luet,Miranda Mundt*

Main category: cs.SE

TL;DR: This paper provides ten practical guidelines to help researchers develop high-quality and sustainable research software that advances science through reproducibility, usability, and community engagement.


<details>
  <summary>Details</summary>
Motivation: Scientific advances increasingly depend on high-quality research software, but many researchers lack guidance on best practices for developing and maintaining such software.

Method: The paper proposes and explains ten guidelines, covering all stages of the software development lifecycle, for creating robust, usable, and sustainable research software.

Result: The authors describe specific best practices including planning, clean code, version control, testing, modular design, reproducibility, performance, maintenance, documentation, and community engagement, which collectively improve research software quality.

Conclusion: Following these ten guidelines enables researchers to produce software that is robust, reusable, and impactful, thereby advancing both individual research goals and the broader scientific ecosystem.

Abstract: High-quality research software is a cornerstone of modern scientific
progress, enabling researchers to analyze complex data, simulate phenomena, and
share reproducible results. However, creating such software requires adherence
to best practices that ensure robustness, usability, and sustainability. This
paper presents ten guidelines for producing high-quality research software,
covering every stage of the development lifecycle. These guidelines emphasize
the importance of planning, writing clean and readable code, using version
control, and implementing thorough testing strategies. Additionally, they
address key principles such as modular design, reproducibility, performance
optimization, and long-term maintenance. The paper also highlights the role of
documentation and community engagement in enhancing software usability and
impact. By following these guidelines, researchers can create software that
advances their scientific objectives and contributes to a broader ecosystem of
reliable and reusable research tools. This work serves as a practical resource
for researchers and developers aiming to elevate the quality and impact of
their research software.

</details>


### [8] [LOCOFY Large Design Models -- Design to code conversion solution](https://arxiv.org/abs/2507.16208)
*Sohaib Muhammad,Ashwati Vipin,Karan Shetti,Honey Mittal*

Main category: cs.SE

TL;DR: The paper introduces Large Design Models (LDMs), specialized for design-to-code tasks, which outperform general LLMs in accuracy and reliability by using a custom data pipeline focused on UI optimization, feature detection, and component modularity.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in Large Language Models (LLMs) and Multimodal LLMs, there are persistent challenges in interpretability, scalability, resource usage, and repeatability, particularly in automating the transformation from design to code. This motivates the search for more specialized models for design-to-code conversion.

Method: The authors developed a new paradigm called Large Design Models (LDMs), trained specifically on designs and webpages. They created a specialized pipeline with data engineering and model architecture modifications, which involves three main steps: (1) A proprietary-based Design Optimiser to fix sub-optimal designs; (2) Tagging and feature detection using pre-trained/fine-tuned models to classify UI elements; (3) Auto Components to extract reusable UI structures. The inference pipeline converts real designs into interpretable code-generation instructions.

Result: The LDMs achieve high end-to-end accuracy in design-to-code conversion, measured by a new preview match score. Comparative experiments show LDMs outperform LLMs on accuracy of node positioning, responsiveness, and reproducibility. The tagging and feature detection models also demonstrate high precision and consistency with UI element identification.

Conclusion: LDMs provide a reliable and superior approach for automating design-to-code conversion, delivering more accurate, interpretable, and reusable code compared to conventional LLMs. They offer improved precision and reliability for code generation from real-world designs.

Abstract: Despite rapid advances in Large Language Models and Multimodal Large Language
Models (LLMs), numerous challenges related to interpretability, scalability,
resource requirements and repeatability remain, related to their application in
the design-to-code space. To address this, we introduce the Large Design Models
(LDMs) paradigm specifically trained on designs and webpages to enable seamless
conversion from design-to-code. We have developed a training and inference
pipeline by incorporating data engineering and appropriate model architecture
modification. The training pipeline consists of the following: 1)Design
Optimiser: developed using a proprietary ground truth dataset and addresses
sub-optimal designs; 2)Tagging and feature detection: using pre-trained and
fine-tuned models, this enables the accurate detection and classification of UI
elements; and 3)Auto Components: extracts repeated UI structures into reusable
components to enable creation of modular code, thus reducing redundancy while
enhancing code reusability. In this manner, each model addresses distinct but
key issues for design-to-code conversion. Separately, our inference pipeline
processes real-world designs to produce precise and interpretable instructions
for code generation and ensures reliability. Additionally, our models
illustrated exceptional end-to-end design-to-code conversion accuracy using a
novel preview match score metric. Comparative experiments indicated superior
performance of LDMs against LLMs on accuracy of node positioning,
responsiveness and reproducibility. Moreover, our custom-trained tagging and
feature detection model demonstrated high precision and consistency in
identifying UI elements across a wide sample of test designs. Thus, our
proposed LDMs are a reliable and superior solution to understanding designs
that subsequently enable the generation of efficient and reliable
production-ready code.

</details>


### [9] [Search-based Generation of Waypoints for Triggering Self-Adaptations in Maritime Autonomous Vessels](https://arxiv.org/abs/2507.16327)
*Karoline NylÃ¦nder,Aitor Arrieta,Shaukat Ali,Paolo Arcaini*

Main category: cs.SE

TL;DR: WPgen is a search-based tool that tweaks waypoints for autonomous vessels to test their adaptability. By causing subtle navigation errors, it helps validate adaptation mechanisms, and its effectiveness varies by vessel type.


<details>
  <summary>Details</summary>
Motivation: Maritime autonomous vessels (AVs) require self-adaptation to handle unexpected situations while maintaining reliable operation. To ensure their correct behavior, it is important to identify critical settings that should trigger adaptations during design and validation of AV navigation software.

Method: The paper introduces WPgen, a multi-objective search-based approach using NSGA-II as the core search algorithm, to generate slight modifications to predefined waypoints. These modifications are designed to remain close to the original but cause the AV to behave inappropriately, enabling the testing and validation of adaptation mechanisms. WPgen is tested with three different seeding strategies, leading to three variations, compared against Random Search as a baseline.

Result: The experimental evaluation was performed on three AVs: one overwater tanker and two underwater vehicles. The effectiveness of the three WPgen variations differed depending on the specific AV, showing that no single strategy is universally superior. The comparison with Random Search demonstrates WPgen's capability to generate valuable test cases for adaptation validation.

Conclusion: WPgen is an effective approach for generating test scenarios that challenge the adaptability of AV navigation systems. The research highlights that different AV types may benefit from different WPgen variants, and implications for both research and practical validation of AV systems are discussed.

Abstract: Self-adaptation in maritime autonomous vessels (AVs) enables them to adapt
their behaviors to address unexpected situations while maintaining
dependability requirements. During the design of such AVs, it is crucial to
understand and identify the settings that should trigger adaptations, enabling
validation of their implementation. To this end, we focus on the navigation
software of AVs, which must adapt their behavior during operation through
adaptations. AVs often rely on predefined waypoints to guide them along
designated routes, ensuring safe navigation. We propose a multiobjective
search-based approach, called WPgen, to generate minor modifications to the
predefined set of waypoints, keeping them as close as possible to the original
waypoints, while causing the AV to navigate inappropriately when navigating
with the generated waypoints. WPgen uses NSGA-II as the multi-objective search
algorithm with three seeding strategies for its initial population, resulting
in three variations of WPgen. We evaluated these variations on three AVs (one
overwater tanker and two underwater). We compared the three variations of WPgen
with Random Search as the baseline and with each other. Experimental results
showed that the effectiveness of these variations varied depending on the AV.
Based on the results, we present the research and practical implications of
WPgen.

</details>


### [10] [Improving Code LLM Robustness to Prompt Perturbations via Layer-Aware Model Editing](https://arxiv.org/abs/2507.16407)
*Shuhan Liu,Xing Hu,Kerui Huang,Xiaohu Yang,David Lo,Xin Xia*

Main category: cs.SE

TL;DR: Code generation LLMs are fragile to small prompt changes. CREME selectively edits model layers to boost robustness, greatly improving code accuracy on perturbed prompts while keeping normal accuracy high.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) excel at code generation but are highly sensitive to small changes in prompts, which can seriously reduce code quality. In real-world applications, such prompt variations are common, so improving robustness is crucial.

Method: The introduced method, CREME (Code Robustness Enhancement via Model Editing), enhances model robustness by (1) identifying network layers most sensitive to prompt perturbations (by comparing hidden states for original vs. altered prompts), and (2) editing parameters specifically at those layers to reduce impact on code quality.

Result: CREME improves Pass@1 accuracy by 63% on perturbed prompts (compared to baseline) and maintains stable performance on unperturbed inputs in major code generation benchmarks (HumanEval and MBPP). It also finds that robustness-sensitive layers are often in the middle to deeper network layers, varying by model architecture.

Conclusion: CREME offers an effective way to make code generation LLMs more robust against prompt variations, with minimal compromise on normal performance. Identifying robustness-sensitive layers is key, and the provided insights help inform future improvements.

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
code generation, where the natural language prompt plays a crucial role in
conveying user intent to the model. However, prior studies have shown that LLMs
are highly sensitive to prompt perturbations. Minor modifications in wording,
syntax, or formatting can significantly reduce the functional correctness of
generated code. As perturbations frequently occur in real-world scenarios,
improving the robustness of LLMs to prompt perturbations is essential for
ensuring reliable performance in practical code generation. In this paper, we
introduce CREME (Code Robustness Enhancement via Model Editing), a novel
approach that enhances LLM robustness through targeted parameter updates. CREME
first identifies robustness-sensitive layers by comparing hidden states between
an original prompt and its perturbed variant. Then, it performs lightweight
parameter editing at the identified layer to reduce performance degradation. We
evaluate CREME on two widely used code generation benchmarks (HumanEval and
MBPP) along with their perturbed counterparts. Experimental results show that
CREME improves Pass@1 accuracy by 63% on perturbed prompts while maintaining
stable performance on clean inputs, with accuracy deviations within 1%. Further
analysis reveals that robustness-sensitive layers are primarily concentrated in
the middle and deeper layers of the network, and their locations vary across
different model architectures. These insights provide a valuable foundation for
developing future robustness-oriented editing strategies.

</details>


### [11] [Exploring Large Language Models for Analyzing and Improving Method Names in Scientific Code](https://arxiv.org/abs/2507.16439)
*Gunnar Larsen,Carol Wong,Anthony Peruma*

Main category: cs.SE

TL;DR: LLMs can assess and suggest improvements for method names in scientific code, generally following best practices. However, their suggestions for domain-specific names are inconsistent and need human oversight, highlighting both the potential and limitations of using AI for scientific code quality.


<details>
  <summary>Details</summary>
Motivation: Research scientists depend more on software, but little is known about how good identifier names affect comprehension in scientific code, especially method names. Recent advancements in LLMs offer a way to automate code analysis, opening new possibilities for improving research code quality.

Method: The authors evaluated four leading Large Language Models on their capability to analyze and suggest improvements to 496 method names taken from Python-based Jupyter Notebooks. They assessed how well LLMs recognize good grammatical patterns and naming conventions, and compared LLM suggestions to human annotation.

Result: LLMs are somewhat effective at analyzing method names and usually adhere to best practices, such as beginning method names with verbs. Nonetheless, LLMs are inconsistent with domain-specific terms and only moderately agree with human annotations.

Conclusion: Automated LLM suggestions can be helpful for naming methods in scientific code, but human review is still necessary due to LLMs' inconsistencies, particularly with specialized terminology.

Abstract: Research scientists increasingly rely on implementing software to support
their research. While previous research has examined the impact of identifier
names on program comprehension in traditional programming environments, limited
work has explored this area in scientific software, especially regarding the
quality of method names in the code. The recent advances in Large Language
Models (LLMs) present new opportunities for automating code analysis tasks,
such as identifier name appraisals and recommendations. Our study evaluates
four popular LLMs on their ability to analyze grammatical patterns and suggest
improvements for 496 method names extracted from Python-based Jupyter
Notebooks. Our findings show that the LLMs are somewhat effective in analyzing
these method names and generally follow good naming practices, like starting
method names with verbs. However, their inconsistent handling of
domain-specific terminology and only moderate agreement with human annotations
indicate that automated suggestions require human evaluation. This work
provides foundational insights for improving the quality of scientific code
through AI automation.

</details>


### [12] [On the Effectiveness of LLM-as-a-judge for Code Generation and Summarization](https://arxiv.org/abs/2507.16587)
*Giuseppe Crupi,Rosalia Tufano,Alejandro Velasco,Antonio Mastropaolo,Denys Poshyvanyk,Gabriele Bavota*

Main category: cs.SE

TL;DR: LLMs can help automate evaluation for code tasks where traditional metrics fail, but even top models like GPT-4-turbo are not consistently reliable judges. Current LLMs are helpful, but not yet a substitute for humans in assessing code quality or summaries.


<details>
  <summary>Details</summary>
Motivation: Traditional quantitative metrics (like BLEU) are inadequate for assessing complex NLP tasks such as code generation and summarization, and large-scale human evaluations are prohibitively expensive. This motivates exploring large language models (LLMs) as automated judges for these tasks.

Method: The authors examined the judgment capabilities of several LLMs (including GPT-4-turbo and smaller models) on the quality of outputs for code generation and code summarization tasks. They compared LLM judgments against human evaluations on thousands of Java and Python code samples and summaries.

Result: GPT-4-turbo outperformed other LLMs in judging code correctness and summary quality. However, even the best LLM often misjudged outputs, and smaller LLMs struggled significantly with judgment tasks.

Conclusion: LLMs, particularly advanced ones like GPT-4-turbo, offer potential for automating the evaluation of code-related tasks, but they are not yet reliable enough to replace human judgment. Improvements are needed before trusting LLMs as autonomous judges for quality control in code generation and summarization.

Abstract: Large Language Models have been recently exploited as judges for complex
natural language processing tasks, such as Q&A. The basic idea is to delegate
to an LLM the assessment of the "quality" of the output provided by an
automated technique for tasks for which: (i) quantitative metrics would only
tell part of the story, and; (ii) a large-scale human-based evaluation would be
too expensive. LLMs-as-a-judge, if proven effective for a specific task, can
also unlock new possibilities for automation, with several LLMs proposing a
solution for a given instance of the task and others judging and deciding what
is the best output to show the user. We study the effectiveness of
LLMs-as-a-judge for two code-related tasks, namely code generation and code
summarization. The rationale for choosing these tasks is two-fold. First,
quantitative metrics are usually not enough for the assessment of code
summarizers/generators. For example, it is well documented that metrics such as
BLEU are quite weak proxies for the quality of the generated summaries. Second,
even state-of-the-art techniques still struggle with handling complex instances
of these tasks, making them good candidates for benefiting from more advanced
solutions envisioning collaboration among LLMs. For code generation, we check
whether eight LLMs are able to judge the correctness of 1,405 Java methods and
1,281 Python functions generated by the same LLMs or implemented by humans. For
code summarization, we compare the judgment of five LLMs to those provided by
nine humans for ~1.2k summaries, related to both Java and Python functions. Our
findings show that GPT-4-turbo is the best LLM in terms of judging capabilities
for both tasks, with "smaller" LLMs featuring tens of billions parameters not
being able to cope with judging tasks. However, even the best-performing LLM
frequently misjudges the correctness of the code and summary quality.

</details>


### [13] [VulCoCo: A Simple Yet Effective Method for Detecting Vulnerable Code Clones](https://arxiv.org/abs/2507.16661)
*Tan Bui,Yan Naing Tun,Thanh Phuc Nguyen,Yindu Su,Ferdian Thung,Yikun Li,Han Wei Ang,Yide Yin,Frank Liauw,Lwin Khin Shar,Eng Lieh Ouh,Ting Zhang,David Lo*

Main category: cs.SE

TL;DR: VulCoCo combines code embedding retrieval with LLM validation to detect vulnerable code clones more accurately than existing tools, demonstrated both on a new benchmark and in real-world open-source projects.


<details>
  <summary>Details</summary>
Motivation: Code reuse can cause the spread of software vulnerabilities due to the unintentional copying of risky code. Detecting these 'vulnerable code clones' (VCCs) is important for software security, but current detection tools are limited by their reliance on syntactic similarity and lack of interpretability.

Method: The paper introduces VulCoCo, a system that first uses embedding-based retrieval to find candidate functions similar to known vulnerable code, and then applies a large language model (LLM) to validate whether the extracted candidates retain the actual vulnerability. The authors also build a synthetic benchmark for reproducible evaluation.

Result: VulCoCo outperformed previous state-of-the-art methods on key metrics such as Precision@k and mean average precision (MAP) on the constructed benchmark. In real-world applications, VulCoCo led to 400 pull requests, with 75 merged and 15 causing new CVEs.

Conclusion: VulCoCo is an effective and scalable tool for detecting vulnerable code clones in large codebases, improving upon existing methods both in experimental and real project settings. It advances the field of automated vulnerability detection and sets a foundation for future work on increasing precision.

Abstract: Code reuse is common in modern software development, but it can also spread
vulnerabilities when developers unknowingly copy risky code. The code fragments
that preserve the logic of known vulnerabilities are known as vulnerable code
clones (VCCs). Detecting those VCCs is a critical but challenging task.
Existing VCC detection tools often rely on syntactic similarity or produce
coarse vulnerability predictions without clear explanations, limiting their
practical utility. In this paper, we propose VulCoCo, a lightweight and
scalable approach that combines embedding-based retrieval with large language
model (LLM) validation. Starting from a set of known vulnerable functions, we
retrieve syntactically or semantically similar candidate functions from a large
corpus and use an LLM to assess whether the candidates retain the
vulnerability. Given that there is a lack of reproducible vulnerable code clone
benchmarks, we first construct a synthetic benchmark that spans various clone
types.
  Our experiments on the benchmark show that VulCoCo outperforms prior
state-of-the-art methods in terms of Precision@k and mean average precision
(MAP). In addition, we also demonstrate VulCoCo's effectiveness in real-world
projects by submitting 400 pull requests (PRs) to 284 open-source projects.
Among them, 75 PRs were merged, and 15 resulted in newly published CVEs. We
also provide insights to inspire future work to further improve the precision
of vulnerable code clone detection.

</details>


### [14] [VulGuard: An Unified Tool for Evaluating Just-In-Time Vulnerability Prediction Models](https://arxiv.org/abs/2507.16685)
*Duong Nguyen,Manh Tran-Duc,Thanh Le-Cong,Triet Huynh Minh Le,M. Ali Babar,Quyet-Thang Huynh*

Main category: cs.SE

TL;DR: VulGuard is an automated tool that streamlines mining, processing, and benchmarking for Just-In-Time vulnerability prediction research. It handles data extraction from GitHub, supports state-of-the-art models, and can be integrated into development pipelines, enhancing reproducibility, scalability, and standardized evaluation.


<details>
  <summary>Details</summary>
Motivation: Just-In-Time vulnerability prediction (JIT-VP) research lacks standardization and is often hindered by challenges in reproducibility, scalability, and integration with modern software development pipelines. There is a need for an automated and unified tool to mine data and benchmark models efficiently.

Method: The authors introduce VulGuard, an automated tool that mines commit histories from GitHub repositories, extracts code changes and relevant metadata, and prepares them for analysis. The tool integrates state-of-the-art vulnerability prediction models, enabling streamlined training, evaluation, and benchmarking. It also supports repository-scale mining and model-level experimentation, and can be incorporated into CI/CD workflows.

Result: VulGuard was tested on two popular open-source projects, FFmpeg and the Linux kernel, demonstrating its effectiveness in automating data extraction, modeling, and benchmarking. VulGuard improved the reproducibility and scalability of JIT-VP experiments.

Conclusion: VulGuard addresses key hurdles in JIT-VP research by offering a unified, automated pipeline for data extraction, model benchmarking, and experimentation. Its integration capabilities and practical application on large projects validate its utility and potential to standardize future JIT-VP studies.

Abstract: We present VulGuard, an automated tool designed to streamline the extraction,
processing, and analysis of commits from GitHub repositories for Just-In-Time
vulnerability prediction (JIT-VP) research. VulGuard automatically mines commit
histories, extracts fine-grained code changes, commit messages, and software
engineering metrics, and formats them for downstream analysis. In addition, it
integrates several state-of-the-art vulnerability prediction models, allowing
researchers to train, evaluate, and compare models with minimal setup. By
supporting both repository-scale mining and model-level experimentation within
a unified framework, VulGuard addresses key challenges in reproducibility and
scalability in software security research. VulGuard can also be easily
integrated into the CI/CD pipeline. We demonstrate the effectiveness of the
tool in two influential open-source projects, FFmpeg and the Linux kernel,
highlighting its potential to accelerate real-world JIT-VP research and promote
standardized benchmarking. A demo video is available at:
https://youtu.be/j96096-pxbs

</details>


### [15] [Never Come Up Empty: Adaptive HyDE Retrieval for Improving LLM Developer Support](https://arxiv.org/abs/2507.16754)
*Fangjian Lei,Mariam El Mezouar,Shayan Noei,Ying Zou*

Main category: cs.SE

TL;DR: The paper develops and evaluates multiple RAG pipeline designs using a large Stack Overflow dataset to improve LLM-generated answers for developer questions. Their optimal RAG pipeline, especially when incorporating HyDE and full context, outperforms standard LLM responses for both old and new questions in terms of correctness and detail.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are helpful for developer questions but can provide unreliable (hallucinated) answers. Retrieval-Augmented Generation (RAG) can help, but its design is complex and not well-established.

Method: The authors built a retrieval corpus from over 3 million Stack Overflow posts in Java and Python. They designed and evaluated 7 RAG pipelines and 63 variants. For new questions without similar historical matches, the pipelines lower the similarity threshold to find partially relevant context. The performance was tested across different LLMs and compared to zero-shot responses.

Result: The best performance was achieved using a RAG pipeline that combines hypothetical-documentation-embedding (HyDE) and full-answer context. This approach outperformed zero-shot baselines across multiple LLMs on helpfulness, correctness, and detail, as judged by LLMs themselves.

Conclusion: Optimally designed RAG pipelines significantly improve answer quality for developer questions across LLMs, making responses more accurate, helpful, and detailed for both familiar and novel queries.

Abstract: Large Language Models (LLMs) have shown promise in assisting developers with
code-related questions; however, LLMs carry the risk of generating unreliable
answers. To address this, Retrieval-Augmented Generation (RAG) has been
proposed to reduce the unreliability (i.e., hallucinations) of LLMs. However,
designing effective pipelines remains challenging due to numerous design
choices. In this paper, we construct a retrieval corpus of over 3 million Java
and Python related Stack Overflow posts with accepted answers, and explore
various RAG pipeline designs to answer developer questions, evaluating their
effectiveness in generating accurate and reliable responses. More specifically,
we (1) design and evaluate 7 different RAG pipelines and 63 pipeline variants
to answer questions that have historically similar matches, and (2) address new
questions without any close prior matches by automatically lowering the
similarity threshold during retrieval, thereby increasing the chance of finding
partially relevant context and improving coverage for unseen cases. We find
that implementing a RAG pipeline combining hypothetical-documentation-embedding
(HyDE) with the full-answer context performs best in retrieving and answering
similarcontent for Stack Overflow questions. Finally, we apply our optimal RAG
pipeline to 4 open-source LLMs and compare the results to their zero-shot
performance. Our findings show that RAG with our optimal RAG pipeline
consistently outperforms zero-shot baselines across models, achieving higher
scores for helpfulness, correctness, and detail with LLM-as-a-judge. These
findings demonstrate that our optimal RAG pipelines robustly enhance answer
quality for a wide range of developer queries including both previously seen
and novel questions across different LLMs

</details>


### [16] [Rethinking LLM-Based RTL Code Optimization Via Timing Logic Metamorphosis](https://arxiv.org/abs/2507.16808)
*Zhihao Xu,Bixin Li,Lulu Wang*

Main category: cs.SE

TL;DR: LLMs are promising for RTL code logic optimization but struggle with complex timing logic due to limited timing reasoning. New benchmarks and methodologies reveal strengths and weaknesses, guiding future research for better LLM-based RTL optimization.


<details>
  <summary>Details</summary>
Motivation: RTL code optimization is important for high-performance, low-power digital circuit design, but current methods are manual, slow, and error-prone. Recent approaches use Large Language Models (LLMs) for automated RTL code optimization, but their effectiveness, especially for complex timing logic, isn't well understood.

Method: The study introduces a new benchmark with four subsets targeting different RTL optimization areas. The authors use a metamorphosis-based method to systematically evaluate LLM-based RTL code optimization methods, focusing on their consistency and effectiveness with increasingly complex code that remains functionally equivalent.

Result: Experiments show LLM-based methods outperform compiler-based approaches in optimizing logic operations, but fall short for complex timing logic, such as timing control flow and clock domain optimization. The main limitation is LLMs' difficulty understanding RTL timing logic.

Conclusion: LLM-based RTL optimization is effective for logic operations but not for more complex timing logic areas. The findings highlight challenges for LLMs and suggest future research directions to improve their capability for RTL code optimization.

Abstract: Register Transfer Level(RTL) code optimization is crucial for achieving high
performance and low power consumption in digital circuit design. However,
traditional optimization methods often rely on manual tuning and heuristics,
which can be time-consuming and error-prone. Recent studies proposed to
leverage Large Language Models(LLMs) to assist in RTL code optimization. LLMs
can generate optimized code snippets based on natural language descriptions,
potentially speeding up the optimization process. However, existing approaches
have not thoroughly evaluated the effectiveness of LLM-Based code optimization
methods for RTL code with complex timing logic. To address this gap, we
conducted a comprehensive empirical investigation to assess the capability of
LLM-Based RTL code optimization methods in handling RTL code with complex
timing logic. In this study, we first propose a new benchmark for RTL
optimization evaluation. It comprises four subsets, each corresponding to a
specific area of RTL code optimization. Then we introduce a method based on
metamorphosis to systematically evaluate the effectiveness of LLM-Based RTL
code optimization methods.Our key insight is that the optimization
effectiveness should remain consistent for semantically equivalent but more
complex code. After intensive experiments, we revealed several key findings.
(1) LLM-Based RTL optimization methods can effectively optimize logic
operations and outperform existing compiler-based methods. (2) LLM-Based RTL
optimization methods do not perform better than existing compiler-based methods
on RTL code with complex timing logic, particularly in timing control flow
optimization and clock domain optimization. This is primarily attributed to the
challenges LLMs face in understanding timing logic in RTL code. Based on these
findings, we provide insights for further research in leveraging LLMs for RTL
code optimization.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [17] [RightTyper: Effective and Efficient Type Annotation for Python](https://arxiv.org/abs/2507.16051)
*Juan Altmayer Pizzorno,Emery D. Berger*

Main category: cs.PL

TL;DR: RightTyper is a fast, efficient tool that generates precise Python type annotations using sampling and statistical filtering, and also helps developers detect bugs by turning type checking into anomaly detection.


<details>
  <summary>Details</summary>
Motivation: Writing type annotations in Python is tedious and time-consuming, and existing automated annotation methods have significant limitations: static analysis struggles with dynamic code, AI-based methods can be unsound, and dynamic methods are slow and may be inaccurate. Furthermore, previous approaches assume code is correct, which is not always true for untyped codebases.

Method: The paper proposes RightTyper, a dynamic type annotation tool for Python. RightTyper leverages sampling guided by self-profiling, statistical filtering, and careful resolution and aggregation of type information. It observes actual program behavior with minimal performance overhead to generate precise annotations and identify anomalous cases that may indicate bugs.

Result: RightTyper generates more precise type annotations and improves the recall of type checking over prior approaches. It has an average runtime overhead of just 30%, is space-efficient, and turns type checking into anomaly detection, enabling discovery of edge cases for further auditing.

Conclusion: RightTyper overcomes major limitations of previous type annotation techniques in Python by balancing runtime efficiency, accuracy, and the ability to detect anomalies. It is a practical, effective tool for improving type safety in real-world Python codebases.

Abstract: Python type annotations bring the benefits of static type checking to the
language. However, manually writing annotations can be time-consuming and
tedious. The result is that most real-world Python code remains largely
untyped. Past approaches to annotating types in Python code fall short in a
number of ways. Static approaches struggle with dynamic features and infer
overly broad types. AI-based methods are inherently unsound and can miss rare
or user-defined types. Dynamic methods can impose extreme runtime overheads,
degrading performance by up to 270x, abort execution as they exhaust resources,
and even infer incorrect types that lead to runtime errors. Crucially, all
prior work assumes implicitly that the code to be annotated is already correct.
This assumption is generally unwarranted, especially for large codebases that
have been untyped.
  This paper presents RightTyper, a novel approach for Python that overcomes
these disadvantages. RightTyper not only generates precise type annotations
based on actual program behavior, improving recall in type checking relative to
prior approaches. It also turns type checking into anomaly detection, allowing
the type checker to identify corner cases that the programmer can audit for
unintended behavior. RightTyper is also fast and space-efficient, imposing just
30% performance overhead on average. RightTyper achieves these characteristics
by a principled yet pervasive use of sampling--guided by self-profiling--along
with statistical filtering and careful resolution and aggregation of type
information.

</details>


### [18] [Understanding Haskell-style Overloading via Open Data and Open Functions](https://arxiv.org/abs/2507.16086)
*Andrew Marmaduke,Apoorv Ingle,J. Garrett Morris*

Main category: cs.PL

TL;DR: The paper introduces System F$_\mathrm{D}$, a new core language with a uniform semantics for Haskell-style overloading, mechanized in Lean4, that better captures advanced type class features without extra axioms.


<details>
  <summary>Details</summary>
Motivation: Haskell's type class system is powerful but lacks a uniform and well-mechanized semantics, particularly for advanced features. Existing approaches often rely on additional type equality axioms and do not capture the full expressiveness required.

Method: The authors introduce a new core language, System F$_\mathrm{D}$, characterized by open data types and open functions defined via collections of instances. They formalize the language's metatheory using the Lean4 interactive theorem prover.

Result: System F$_\mathrm{D}$ successfully models advanced aspects of Haskell's type class features and is more expressive compared to prior semantic frameworks. It does this without relying on extra axioms regarding type equality.

Conclusion: System F$_\mathrm{D}$ offers a precise and broadly applicable semantics for Haskell-style overloading, improving both expressiveness and formal rigor over existing approaches.

Abstract: We present a new, uniform semantics for Haskell-style overloading. We realize
our approach in a new core language, System F$_\mathrm{D}$, whose metatheory we
mechanize in the Lean4 interactive theorem prover. System F$_\mathrm{D}$ is
distinguished by its open data types and open functions, each given by a
collection of instances rather than by a single definition. We show that System
F$_\mathrm{D}$ can encode advanced features of Haskell's of type class systems,
more expressively than current semantics of these features, and without
assuming additional type equality axioms.

</details>


### [19] [Querying Graph-Relational Data](https://arxiv.org/abs/2507.16089)
*Michael J. Sullivan,Zhibo Chen,Elvis Pranskevichus,Robert J. Simmons,Victor Petrovykh,AljaÅ¾ Mur ErÅ¾en,Yury Selivanov*

Main category: cs.PL

TL;DR: This paper introduces the graph-relational database model, which aligns relational database storage with application-level object structures. Through EdgeQL and the Gel system, it enables expressive object-shaped queries with performance close to custom SQL, overcoming object-relational mismatch issues.


<details>
  <summary>Details</summary>
Motivation: There is a fundamental mismatch between how relational databases store data (in flat tables) and how modern applications expect to handle data (in deeply nested or object-shaped structures), commonly known as the object-relational mismatch. Existing solutions like ORMs are often inefficient or cumbersome.

Method: The authors propose a new graph-relational database model that is flexible, compositional, and strongly typed. They formally define the model, develop static and dynamic semantics for queries, and implement the approach in the EdgeQL query language and the Gel system, which compiles EdgeQL to efficient PostgreSQL queries.

Result: The graph-relational model supports object-shaped data manipulation natively, addressing limitations of traditional ORMs. The Gel system achieves much of the efficiency of hand-written SQL while enabling expressive and convenient object-style queries.

Conclusion: The graph-relational database model and its implementation in EdgeQL and Gel bridge the gap between application data needs and relational storage. They provide both flexibility and efficiency, offering an improved alternative over conventional ORM technologies.

Abstract: For applications that store structured data in relational databases, there is
an impedance mismatch between the flat representations encouraged by relational
data models and the deeply nested information that applications expect to
receive. In this work, we present the graph-relational database model, which
provides a flexible, compositional, and strongly-typed solution to this
"object-relational mismatch." We formally define the graph-relational database
model and present a static and dynamic semantics for queries. In addition, we
discuss the realization of the graph-relational database model in EdgeQL, a
general-purpose SQL-style query language, and the Gel system, which compiles
EdgeQL schemas and queries into PostgreSQL queries. Gel facilitates the kind of
object-shaped data manipulation that is frequently provided inefficiently by
object-relational mapping (ORM) technologies, while achieving most of the
efficiency that comes from require writing complex PostgreSQL queries directly.

</details>


### [20] [Enhancing Compiler Optimization Efficiency through Grammatical Decompositions of Control-Flow Graphs](https://arxiv.org/abs/2507.16660)
*Xuran Cai*

Main category: cs.PL

TL;DR: This thesis introduces SPL decomposition, a new framework for compiler optimization that efficiently solves key problems like register allocation and redundancy elimination. The approach leverages graph sparsity more effectively than previous methods, resulting in significant performance improvements across a range of benchmarks.


<details>
  <summary>Details</summary>
Motivation: Traditional tree decomposition algorithms for compiler optimizations, such as register allocation and LOSPRE, often neglect important sparsity properties of Control Flow Graphs (CFGs), resulting in high computational costs and suboptimal performance.

Method: The paper proposes SPL (Series-Parallel-Loop) decomposition, a new framework for handling compiler optimization problems. It provides a general solution for Partial Constraint Satisfaction Problems (PCSPs) within graphs, and applies this framework to three specific compiler optimization tasks: register allocation, lifetime-optimal speculative partial redundancy elimination (LOSPRE), and bank selection instruction placement.

Result: SPL decomposition enables more accurate modeling of variable interference for efficient register allocation, more effective elimination of redundancies in program execution for LOSPRE, and improved bank selection instruction placement for better data retrieval and reduced latency. Experiments demonstrate that SPL decomposition significantly outperforms existing methods across benchmarks.

Conclusion: SPL decomposition is a powerful and generalizable framework for addressing complex compiler optimization problems, yielding substantial empirical performance gains in register allocation, LOSPRE, and bank selection.

Abstract: This thesis addresses the complexities of compiler optimizations, such as
register allocation and Lifetime-optimal Speculative Partial Redundancy
Elimination (LOSPRE), which are often handled using tree decomposition
algorithms. However, these methods frequently overlook important sparsity
aspects of Control Flow Graphs (CFGs) and result in high computational costs.
We introduce the SPL (Series-Parallel-Loop) decomposition, a novel framework
that offers optimal solutions to these challenges. A key contribution is the
formulation of a general solution for Partial Constraint Satisfaction Problems
(PCSPs) within graph structures, applied to three optimization problems. First,
SPL decomposition enhances register allocation by accurately modeling variable
interference graphs, leading to efficient register assignments and improved
performance across benchmarks. Second, it optimizes LOSPRE by effectively
identifying and eliminating redundancies in program execution. Finally, the
thesis focuses on optimizing the placement of bank selection instructions to
enhance data retrieval efficiency and reduce latency. Extensive experimentation
demonstrates significant performance improvements over existing methods,
establishing SPL decomposition as a powerful tool for complex compiler
optimizations, including register allocation, LOSPRE, and bank selection.

</details>
