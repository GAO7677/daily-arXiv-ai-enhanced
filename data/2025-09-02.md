<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 8]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Model-Driven Quantum Code Generation Using Large Language Models and Retrieval-Augmented Generation](https://arxiv.org/abs/2508.21097)
*Nazanin Siavash,Armin Moin*

Main category: cs.SE

TL;DR: This paper explores the use of LLMs with a RAG pipeline to automatically generate quantum code from software models, achieving significant improvements in code quality and highlighting future expansion opportunities.


<details>
  <summary>Details</summary>
Motivation: Quantum and hybrid quantum-classical software systems are complex due to heterogeneous platforms and a shortage of skilled developers. Model-driven approaches can help address these challenges.

Method: The authors validate a model-to-code generation technique using Large Language Models (LLMs) enhanced with a Retrieval-Augmented Generation (RAG) pipeline. Specifically, they transform UML model instances into Python code utilizing the Qiskit library, with RAG incorporating example code from public GitHub repositories.

Result: Experimental results demonstrate that well-crafted prompts in the RAG-augmented LLM pipeline can improve CodeBLEU scores by up to four times, resulting in more accurate and consistent quantum code.

Conclusion: The integration of LLMs with RAG for model-to-code transformations is promising for quantum software development. Further research could extend these techniques to use system model instances as RAG sources or for code-to-code transformations, including transpilation.

Abstract: This paper introduces a novel research direction for model-to-text/code
transformations by leveraging Large Language Models (LLMs) that can be enhanced
with Retrieval-Augmented Generation (RAG) pipelines. The focus is on quantum
and hybrid quantum-classical software systems, where model-driven approaches
can help reduce the costs and mitigate the risks associated with the
heterogeneous platform landscape and lack of developers' skills. We validate
one of the proposed ideas regarding generating code out of UML model instances
of software systems. This Python code uses a well-established library, called
Qiskit, to execute on gate-based or circuit-based quantum computers. The RAG
pipeline that we deploy incorporates sample Qiskit code from public GitHub
repositories. Experimental results show that well-engineered prompts can
improve CodeBLEU scores by up to a factor of four, yielding more accurate and
consistent quantum code. However, the proposed research direction can go beyond
this through further investigation in the future by conducting experiments to
address our other research questions and ideas proposed here, such as deploying
software system model instances as the source of information in the RAG
pipelines, or deploying LLMs for code-to-code transformations, for instance,
for transpilation use cases.

</details>


### [2] [Learning to Generate Unit Test via Adversarial Reinforcement Learning](https://arxiv.org/abs/2508.21107)
*Dongjun Lee,Changho Hwang,Kimin Lee*

Main category: cs.SE

TL;DR: UTRL is a reinforcement learning-based framework that adversarially trains LLMs to generate better unit tests. It outperforms standard approaches and even top models like GPT-4.1 in test quality.


<details>
  <summary>Details</summary>
Motivation: Writing comprehensive unit tests for programs is challenging, and while LLMs are used to automate test generation, there is limited exploration into training methods for LLMs to produce high-quality tests. This work addresses the need for better approaches to train LLMs for effective unit test generation.

Method: UTRL is a novel reinforcement learning framework where two LLMs—a unit test generator and a code generator—are iteratively trained in an adversarial way. The test generator maximizes a reward for finding faults in code produced by the code generator, while the code generator maximizes a reward for passing the generated tests.

Result: Experiments show that Qwen3-4B trained via UTRL produces higher-quality unit tests than standard supervised fine-tuning on human-written tests. Additionally, Qwen3-4B with UTRL surpasses even advanced models like GPT-4.1 in generating effective unit tests.

Conclusion: UTRL demonstrates superior performance in training LLMs to generate comprehensive unit tests, both compared to traditional supervised methods and leading models like GPT-4.1.

Abstract: Unit testing is a core practice in programming, enabling systematic
evaluation of programs produced by human developers or large language models
(LLMs). Given the challenges in writing comprehensive unit tests, LLMs have
been employed to automate test generation, yet methods for training LLMs to
produce high-quality tests remain underexplored. In this work, we propose UTRL,
a novel reinforcement learning framework that trains an LLM to generate
high-quality unit tests given a programming instruction. Our key idea is to
iteratively train two LLMs, the unit test generator and the code generator, in
an adversarial manner via reinforcement learning. The unit test generator is
trained to maximize a discrimination reward, which reflects its ability to
produce tests that expose faults in the code generator's solutions, and the
code generator is trained to maximize a code reward, which reflects its ability
to produce solutions that pass the unit tests generated by the test generator.
In our experiments, we demonstrate that unit tests generated by Qwen3-4B
trained via UTRL show higher quality compared to unit tests generated by the
same model trained via supervised fine-tuning on human-written ground-truth
unit tests, yielding code evaluations that more closely align with those
induced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL
outperforms frontier models such as GPT-4.1 in generating high-quality unit
tests, highlighting the effectiveness of UTRL in training LLMs for this task.

</details>


### [3] [Automated Bug Triaging using Instruction-Tuned Large Language Models](https://arxiv.org/abs/2508.21156)
*Kiana Kiashemshaki,Arsham Khosravani,Alireza Hosseinpour,Arshia Akhavan*

Main category: cs.SE

TL;DR: A new framework using instruction-tuned LLMs and LoRA adapters improves bug triaging efficiency and accuracy, providing a strong alternative to traditional, resource-intensive methods.


<details>
  <summary>Details</summary>
Motivation: Bug triaging is slow and inconsistent in large software projects, making it difficult to efficiently assign new issues to developers. Traditional approaches involve costly feature engineering or complex graph-based methods.

Method: The paper proposes a lightweight framework that uses an instruction-tuned large language model (LLM) with LoRA adapters and employs candidate-constrained decoding to ensure valid developer assignments.

Result: The framework demonstrates strong shortlist quality on EclipseJDT and Mozilla datasets (Hit at 10 up to 0.753), although exact Top-1 accuracy remains modest. However, accuracy improves significantly on recent data snapshots, highlighting its adaptability.

Conclusion: Instruction-tuned LLMs with LoRA adapters represent a viable and practical alternative to traditional bug triaging approaches, especially when used in real-world, human-in-the-loop scenarios.

Abstract: Bug triaging, the task of assigning new issues to developers, is often slow
and inconsistent in large projects. We present a lightweight framework that
instruction-tuned large language model (LLM) with LoRA adapters and uses
candidate-constrained decoding to ensure valid assignments. Tested on
EclipseJDT and Mozilla datasets, the model achieves strong shortlist quality
(Hit at 10 up to 0.753) despite modest exact Top-1 accuracy. On recent
snapshots, accuracy rises sharply, showing the framework's potential for
real-world, human-in-the-loop triaging. Our results suggest that
instruction-tuned LLMs offer a practical alternative to costly feature
engineering and graph-based methods.

</details>


### [4] [The Complexity Trap: Simple Observation Masking Is as Efficient as LLM Summarization for Agent Context Management](https://arxiv.org/abs/2508.21433)
*Tobias Lindenbauer,Igor Slinko,Ludwig Felder,Egor Bogomolov,Yaroslav Zharov*

Main category: cs.SE

TL;DR: Simple observation-masking as a context management strategy in LLM-based Software Engineering agents matches or slightly exceeds the performance of more complex LLM summarization approaches, while significantly reducing computational cost. 


<details>
  <summary>Details</summary>
Motivation: LLM-based agents generate long context histories during complex tasks, leading to high computational cost. Existing SE agents use summarization as a mitigation, but it's unclear if this added complexity is justified over simpler approaches.

Method: Systematic comparison of context management strategies—specifically, LLM-based summarization versus simple observation-masking—within the SWE-agent framework on the SWE-bench Verified benchmark, across five model configurations.

Result: Observation-masking halves computational cost relative to using raw agent context, while matching or sometimes slightly outperforming LLM summarization in solve rate. Example: Qwen3-Coder 480B achieves 54.8% solve rate with masking (vs. 53.8% raw, competitive with summarization) at a lower cost.

Conclusion: In the SWE-agent context, simple observation-masking provides equally (or slightly more) effective and considerably more efficient context management than more complex LLM summarization strategies. Code and data are released for reproducibility.

Abstract: Large Language Model (LLM)-based agents solve complex tasks through iterative
reasoning, exploration, and tool-use, a process that can result in long,
expensive context histories. While state-of-the-art Software Engineering ( SE)
agents like OpenHands or Cursor use LLM-based summarization to tackle this
issue, it is unclear whether the increased complexity offers tangible
performance benefits compared to simply omitting older observations. We present
a systematic comparison of these strategies within SWE-agent on SWE-bench
Verified across five diverse model configurations. We find that a simple
observation-masking strategy halves cost relative to a raw agent while
matching, and sometimes slightly exceeding, the solve rate of LLM
summarization. For example, with Qwen3-Coder 480B, masking improves solve rate
from 53.8% (raw agent) to 54.8%, while remaining competitive with summarization
at a lower cost. These results suggest that, at least within SWE-agent on
SWE-bench Verified, the most effective and efficient context management can be
the simplest. We release code and data for reproducibility

</details>


### [5] [Enhancing Semantic Understanding in Pointer Analysis using Large Language Models](https://arxiv.org/abs/2508.21454)
*Baijun Cheng,Kailong Wang,Ling Shi,Haoyu Wang,Yao Guo,Ding Li,Xiangqun Chen*

Main category: cs.SE

TL;DR: This paper proposes LMPA, a framework leveraging LLMs to improve pointer analysis by better handling user-defined functions, increasing accuracy and scalability, and outlines future challenges.


<details>
  <summary>Details</summary>
Motivation: Existing pointer analysis frameworks are limited due to their lack of semantic understanding of code, resulting in imprecise and conservative analysis of user-defined functions.

Method: The authors propose LMPA (LLM-enhanced Pointer Analysis), a framework that incorporates large language models (LLMs) to identify and model user-defined functions similar to system APIs, and to enhance summary-based analysis via initial points-to set inference and a natural-language-augmented summary strategy.

Result: LMPA mitigates erroneous propagation of facts across calling contexts and improves both the precision and scalability of pointer analysis. The challenges of implementing this vision are also discussed.

Conclusion: Incorporating LLMs into pointer analysis addresses key limitations of existing methods, especially with respect to user-defined functions, but challenges remain in practical realization.

Abstract: Pointer analysis has been studied for over four decades. However, existing
frameworks continue to suffer from the propagation of incorrect facts. A major
limitation stems from their insufficient semantic understanding of code,
resulting in overly conservative treatment of user-defined functions. Recent
advances in large language models (LLMs) present new opportunities to bridge
this gap. In this paper, we propose LMPA (LLM-enhanced Pointer Analysis), a
vision that integrates LLMs into pointer analysis to enhance both precision and
scalability. LMPA identifies user-defined functions that resemble system APIs
and models them accordingly, thereby mitigating erroneous cross-calling-context
propagation. Furthermore, it enhances summary-based analysis by inferring
initial points-to sets and introducing a novel summary strategy augmented with
natural language. Finally, we discuss the key challenges involved in realizing
this vision.

</details>


### [6] [Reusable Test Suites for Reinforcement Learning](https://arxiv.org/abs/2508.21553)
*Jørn Eirik Betten,Quentin Mazouni,Dennis Gross,Pedro Lind,Helge Spieker*

Main category: cs.SE

TL;DR: MPTCS is a new framework for creating automated, reusable, and diverse test suites for RL agents, leveraging multiple policies and difficulty scoring. It improves reliability testing by finding agent flaws more effectively and generalizing test cases beyond single-policy approaches.


<details>
  <summary>Details</summary>
Motivation: Validating the reliability and performance of reinforcement learning agents before deployment is challenging, especially because existing test suites are typically tailored to specific policies and may not generalize to others.

Method: The paper proposes Multi-Policy Test Case Selection (MPTCS), an automated method for selecting test suites in RL environments. MPTCS identifies test cases based on their solvability, diversity, and general difficulty, regardless of the policy used to generate them. It uses a pool of policies to extract diverse, reusable, policy-agnostic test cases using a difficulty score, and incorporates a test case descriptor surface inspired by quality-diversity algorithms to ensure state space coverage and trigger policy faults.

Result: The study demonstrates the effectiveness of the difficulty score for test case selection and analyzes how the number of policies used affects both the effectiveness and cost of MPTCS. The proposed diversity-enhancing method successfully increases coverage of the state space and identifies faulty policy behaviors across different agents.

Conclusion: MPTCS provides a robust mechanism for generating reusable, diverse, and policy-agnostic test suites, improving the reliability assessment of a wide range of RL agents and facilitating broader, more reliable testing for deployment.

Abstract: Reinforcement learning (RL) agents show great promise in solving sequential
decision-making tasks. However, validating the reliability and performance of
the agent policies' behavior for deployment remains challenging. Most
reinforcement learning policy testing methods produce test suites tailored to
the agent policy being tested, and their relevance to other policies is
unclear. This work presents Multi-Policy Test Case Selection (MPTCS), a novel
automated test suite selection method for RL environments, designed to extract
test cases generated by any policy testing framework based on their
solvability, diversity, and general difficulty. MPTCS uses a set of policies to
select a diverse collection of reusable policy-agnostic test cases that reveal
typical flaws in the agents' behavior. The set of policies selects test cases
from a candidate pool, which can be generated by any policy testing method,
based on a difficulty score. We assess the effectiveness of the difficulty
score and how the method's effectiveness and cost depend on the number of
policies in the set. Additionally, a method for promoting diversity in the test
suite, a discretized general test case descriptor surface inspired by
quality-diversity algorithms, is examined to determine how it covers the state
space and which policies it triggers to produce faulty behaviors.

</details>


### [7] [Human-Written vs. AI-Generated Code: A Large-Scale Study of Defects, Vulnerabilities, and Complexity](https://arxiv.org/abs/2508.21634)
*Domenico Cotroneo,Cristina Improta,Pietro Liguori*

Main category: cs.SE

TL;DR: AI code assistants produce simpler, more repetitive code but are more prone to security vulnerabilities compared to human-written code. Each has unique quality issues, requiring tailored QA practices for safe, effective use of AI in programming.


<details>
  <summary>Details</summary>
Motivation: With the widespread adoption of AI code assistants in software development, there's a critical need to understand how AI-generated code compares to human-written code, particularly regarding software quality, reliability, maintainability, and security.

Method: The paper conducts a large-scale comparison of over 500,000 code samples written by human developers and three leading large language models (ChatGPT, DeepSeek-Coder, Qwen-Coder) in Python and Java. It evaluates code defects (via Orthogonal Defect Classification), security vulnerabilities (using the Common Weakness Enumeration), and structural complexity.

Result: AI-generated code is overall simpler and more repetitive, with more unused constructs and hardcoded debugging information. Human-written code, on the other hand, is structurally more complex and tends to have more maintainability issues. Notably, AI-generated code contains a higher rate of high-risk security vulnerabilities.

Conclusion: There are clear and distinct differences in the software quality profiles between AI-generated and human-written code. AI code tends to be simpler but presents specific risks, especially concerning security vulnerabilities, thus necessitating specialized quality assurance approaches for AI-assisted programming.

Abstract: As AI code assistants become increasingly integrated into software
development workflows, understanding how their code compares to human-written
programs is critical for ensuring reliability, maintainability, and security.
In this paper, we present a large-scale comparison of code authored by human
developers and three state-of-the-art LLMs, i.e., ChatGPT, DeepSeek-Coder, and
Qwen-Coder, on multiple dimensions of software quality: code defects, security
vulnerabilities, and structural complexity. Our evaluation spans over 500k code
samples in two widely used languages, Python and Java, classifying defects via
Orthogonal Defect Classification and security vulnerabilities using the Common
Weakness Enumeration. We find that AI-generated code is generally simpler and
more repetitive, yet more prone to unused constructs and hardcoded debugging,
while human-written code exhibits greater structural complexity and a higher
concentration of maintainability issues. Notably, AI-generated code also
contains more high-risk security vulnerabilities. These findings highlight the
distinct defect profiles of AI- and human-authored code and underscore the need
for specialized quality assurance practices in AI-assisted programming.

</details>


### [8] [The Integration of Agile Methodologies in DevOps Practices within the Information Technology Industry](https://arxiv.org/abs/2508.21811)
*Ashley Hourigan,Ridewaan Hanslo*

Main category: cs.SE

TL;DR: Growing software delivery speed expectations have driven a transition from Waterfall to Agile, and further to DevOps practices in IT. Through interviews and thematic analysis, the paper maps how Agile methods are implemented in DevOps, revealing a nuanced interrelationship. The study provides actionable insights into merging Agile and DevOps to better meet industry demands.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the need for faster software delivery in the IT industry to meet heightened customer expectations for rapid, feature-rich product releases. This trend has led to a shift from traditional software development approaches like Waterfall to Agile methodologies and further towards DevOps, which promises increased collaboration and continuous delivery.

Method: The paper conducted eleven semi-structured interviews with practitioners of Agile and DevOps from various IT sectors. Thematic analysis was employed, extracting 51 unique codes, which were synthesized into 19 key themes reflecting the phases of the DevOps lifecycle and the integration of Agile methods.

Result: The study identified nuanced themes related to the feasibility and applicability of Agile methods within DevOps practices. It provided a synthesized understanding of how Agile methods are integrated and implemented throughout the DevOps lifecycle, meeting the research objectives.

Conclusion: The paper concludes with a new understanding of the interrelationship between Agile methods and DevOps practices, providing insights into the effective integration of Agile into DevOps within the IT industry, thus responding better to industry needs for rapid, high-quality software delivery.

Abstract: The demand for rapid software delivery in the Information Technology (IT)
industry has significantly intensified, emphasising the need for faster
software products and service releases with enhanced features to meet customer
expectations. Agile methodologies are replacing traditional approaches such as
Waterfall, where flexibility, iterative development and adaptation to change
are favoured over rigid planning and execution. DevOps, a subsequent evolution
from Agile, emphasises collaborative efforts in development and operations
teams, focusing on continuous integration and deployment to deliver resilient
and high-quality software products and services. This study aims to critically
assess both Agile and DevOps practices in the IT industry to identify the
feasibility and applicability of Agile methods in DevOps practices. Eleven
semi-structured interviews were conducted with Agile and DevOps practitioners
in varying capacities across several sectors within the IT industry. Through
thematic analysis, 51 unique codes were extracted and synthesised into 19
themes that reported on each phase of the DevOps lifecycle, specifically
regarding the integration and implementation of Agile methods into DevOps
practices. Based on the findings, a new understanding detailing the
interrelationship of Agile methods in DevOps practices was discussed that met
the research objectives.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [9] [CrossTL: A Universal Programming Language Translator with Unified Intermediate Representation](https://arxiv.org/abs/2508.21256)
*Nripesh Niketan,Vaatsalya Shrivastva*

Main category: cs.PL

TL;DR: CrossTL is a universal programming language translator that uses a unified intermediate representation (CrossGL) to enable easy, bidirectional code translation among various languages (GPU, graphics, systems). Its modular design greatly simplifies supporting new languages and achieves successful compilation and execution across domains.


<details>
  <summary>Details</summary>
Motivation: Current code translation approaches require a separate tool for every language pair, creating exponential complexity when adding support for new languages. The field lacks a universal, scalable solution for translating code between multiple, diverse programming languages efficiently.

Method: The authors introduce CrossTL, which uses a single, unified intermediate representation (CrossGL). Their system employs language-specific lexers and parsers to translate source code into ASTs, a set of ToCrossGLConverter classes to import code into the CrossGL IR, and CodeGen classes for code generation targeting various languages. This modular pipeline makes translation between any supported language pair possible via the universal IR, and adding new languages requires only creating corresponding frontend/backend modules.

Result: CrossTL successfully translates and compiles code across a wide variety of programming domains and languages (e.g., CUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan SPIR-V, Rust, Mojo). Tests confirm that translated code runs as intended across all backends. The IR-based design proves extensible and practical for supporting new languages and paradigms with minimal extra work.

Conclusion: CrossTL demonstrates that a universal IR-based, modular architecture can enable practical, extensible, and accurate translation between diverse programming languages. This paves the way for language-agnostic, write-once, deploy-anywhere development across platforms and paradigms.

Abstract: We present CrossTL, a universal programming language translator enabling
bidirectional translation between multiple languages through a unified
intermediate representation called CrossGL. Traditional approaches require
separate translators for each language pair, leading to exponential complexity
growth. CrossTL uses a single universal IR to facilitate translations between
CUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan SPIR-V, Rust, and Mojo,
with Slang support in development. Our system consists of: language-specific
lexers/parsers converting source code to ASTs, bidirectional CrossGL
translation modules implementing ToCrossGLConverter classes for importing code
and CodeGen classes for target generation, and comprehensive backend
implementations handling full translation pipelines. We demonstrate
effectiveness through comprehensive evaluation across programming domains,
achieving successful compilation and execution across all supported backends.
The universal IR design enables adding new languages with minimal effort,
requiring only language-specific frontend/backend components. Our contributions
include: (1) a unified IR capturing semantics of multiple programming
paradigms, (2) a modular architecture enabling extensibility, (3) a
comprehensive framework supporting GPU compute, graphics programming, and
systems languages, and (4) empirical validation demonstrating practical
viability of universal code translation. CrossTL represents a significant step
toward language-agnostic programming, enabling write-once, deploy-everywhere
development.

</details>


### [10] [Growing Mathlib: maintenance of a large scale mathematical library](https://arxiv.org/abs/2508.21593)
*Anne Baanen,Matthew Robert Ballard,Johan Commelin,Bryan Gin-ge Chen,Michael Rothgang,Damiano Testa*

Main category: cs.PL

TL;DR: This paper discusses how the Mathlib community manages rapid library growth by implementing deprecation controls, quality checks, design optimizations, debt management, and custom contribution tools to keep maintenance efficient and sustainable.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges posed by the rapid growth of the Lean mathematical library Mathlib, particularly focusing on how to manage this expansion without overwhelming maintainers and while supporting ongoing changes.

Method: The approach includes implementing deprecation systems to manage breaking changes, deploying code quality tools (linters) for immediate feedback, optimizing (re-)design to enhance compilation speeds, tackling technical debt, and developing custom review and triage tooling.

Result: By using these strategies, the Mathlib community has managed to effectively cope with rapid growth, maintained code quality, and reduced maintainer burden.

Conclusion: The combination of systematic project management, technical enhancements, and custom tooling offers a scalable way to handle growth and change in large formal mathematics libraries like Mathlib.

Abstract: The Lean mathematical library Mathlib is one of the fastest-growing libraries
of formalised mathematics. We describe various strategies to manage this
growth, while allowing for change and avoiding maintainer overload. This
includes dealing with breaking changes via a deprecation system, using code
quality analysis tools (linters) to provide direct user feedback about common
pitfalls, speeding up compilation times through conscious library (re-)design,
dealing with technical debt as well as writing custom tooling to help with the
review and triage of new contributions.

</details>
