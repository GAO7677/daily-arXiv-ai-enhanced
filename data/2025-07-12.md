<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 6]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A German Gold-Standard Dataset for Sentiment Analysis in Software Engineering](https://arxiv.org/abs/2507.07325)
*Martin Obaidi,Marc Herrmann,Elisa Schmid,Raymond Ochsner,Kurt Schneider,Jil Klünder*

Main category: cs.SE

TL;DR: The paper introduces a robust German dataset for developer sentiment analysis, demonstrates its validity, and highlights the scarcity of specialized tools for sentiment analysis in German software engineering.


<details>
  <summary>Details</summary>
Motivation: Existing sentiment analysis tools for software engineering are mainly based on English or non-German datasets, leaving a gap for robust German-language resources to analyze developer sentiment.

Method: The authors created a dataset of 5,949 unique German-language developer statements from the forum Android-Hilfe.de. Each statement was annotated with one of six basic emotions, based on Shaver et al.'s emotion model, by four German-speaking computer science students. The annotation process was evaluated for interrater agreement and reliability.

Result: The annotation process showed high interrater agreement and reliability, confirming the validity and robustness of the dataset. Testing with current German sentiment analysis tools revealed a lack of domain-specific solutions for software engineering.

Conclusion: The new German-language dataset provides a valuable resource for sentiment analysis in software engineering, validated by strong annotation reliability. Existing sentiment tools are insufficient for this domain, and the dataset enables further research and optimization in annotation strategies.

Abstract: Sentiment analysis is an essential technique for investigating the emotional
climate within developer teams, contributing to both team productivity and
project success. Existing sentiment analysis tools in software engineering
primarily rely on English or non-German gold-standard datasets. To address this
gap, our work introduces a German dataset of 5,949 unique developer statements,
extracted from the German developer forum Android-Hilfe.de. Each statement was
annotated with one of six basic emotions, based on the emotion model by Shaver
et al., by four German-speaking computer science students. Evaluation of the
annotation process showed high interrater agreement and reliability. These
results indicate that the dataset is sufficiently valid and robust to support
sentiment analysis in the German-speaking software engineering community.
Evaluation with existing German sentiment analysis tools confirms the lack of
domain-specific solutions for software engineering. We also discuss approaches
to optimize annotation and present further use cases for the dataset.

</details>


### [2] [Automatic Generation of Explainability Requirements and Software Explanations From User Reviews](https://arxiv.org/abs/2507.07344)
*Martin Obaidi,Jannik Fischbach,Jakob Droste,Hannah Deters,Marc Herrmann,Jil Klünder,Steffen Krätzig,Hugo Villamizar,Kurt Schneider*

Main category: cs.SE

TL;DR: This paper presents a tool that automatically extracts and explains explainability requirements from user reviews. While AI-generated explanations are usually preferred for clear communication, accuracy is still an issue, emphasizing the importance of human validation. The study also provides a new annotated dataset for future research.


<details>
  <summary>Details</summary>
Motivation: Explainability is increasingly important for software systems to enhance transparency, user trust, and regulatory compliance. However, it is difficult to translate user feedback on explainability into structured requirements and linked explanations, and there is a lack of established, systematic approaches for this task.

Method: The authors developed a tool-supported, automated process for extracting explainability requirements and generating corresponding explanations from user reviews. This was evaluated through a case study using 58 user reviews from an industrial automation manufacturer, with manual annotation of requirements and explanations for comparison.

Result: AI-generated requirements often lack relevance and correctness compared to human-crafted ones, but the generated explanations are usually preferred for their clarity and style. However, there are correctness issues in both, underlining the need for human oversight.

Conclusion: The paper introduces an automated method for deriving explainability requirements from user reviews and creating explanations, offers empirical insights into the strengths and weaknesses of automatic approaches, and releases an annotated dataset to further research in this area.

Abstract: Explainability has become a crucial non-functional requirement to enhance
transparency, build user trust, and ensure regulatory compliance. However,
translating explanation needs expressed in user feedback into structured
requirements and corresponding explanations remains challenging. While existing
methods can identify explanation-related concerns in user reviews, there is no
established approach for systematically deriving requirements and generating
aligned explanations. To contribute toward addressing this gap, we introduce a
tool-supported approach that automates this process. To evaluate its
effectiveness, we collaborated with an industrial automation manufacturer to
create a dataset of 58 user reviews, each annotated with manually crafted
explainability requirements and explanations. Our evaluation shows that while
AI-generated requirements often lack relevance and correctness compared to
human-created ones, the AI-generated explanations are frequently preferred for
their clarity and style. Nonetheless, correctness remains an issue,
highlighting the importance of human validation. This work contributes to the
advancement of explainability requirements in software systems by (1)
introducing an automated approach to derive requirements from user reviews and
generate corresponding explanations, (2) providing empirical insights into the
strengths and limitations of automatically generated artifacts, and (3)
releasing a curated dataset to support future research on the automatic
generation of explainability requirements.

</details>


### [3] [Towards an Engineering Workflow Management System for Asset Administration Shells using BPMN](https://arxiv.org/abs/2507.07468)
*Sten Grüner,Nafise Eskandani*

Main category: cs.SE

TL;DR: This paper proposes integrating AAS and BPMN for automated engineering workflows, introducing a secure and scalable distributed infrastructure, and demonstrates improved efficiency through a novel management prototype.


<details>
  <summary>Details</summary>
Motivation: There is a need to automate and optimize engineering workflows in the context of Industry 4.0, especially to enhance cross-organizational collaboration, security, and scalability.

Method: The paper explores the application of Asset Administration Shell (AAS) in conjunction with Business Process Model and Notation (BPMN). It proposes a distributed AAS copy-on-write infrastructure and presents a workflow management prototype for automating AAS operations.

Result: The proposed distributed AAS infrastructure enables secure, scalable, and seamless collaboration across organizations. The workflow management prototype successfully automates engineering workflows and AAS operations, resulting in improved efficiency and traceability.

Conclusion: Integrating AAS and BPMN within engineering workflows using a distributed copy-on-write approach significantly enhances automation, collaboration, and data management, making engineering processes more efficient and secure.

Abstract: The integration of Industry 4.0 technologies into engineering workflows is an
essential step toward automating and optimizing plant and process engineering
processes. The Asset Administration Shell (AAS) serves as a key enabler for
creating interoperable Digital Twins that facilitate engineering data exchange
and automation. This paper explores the use of AAS within engineering
workflows, particularly in combination with Business Process Model and Notation
(BPMN) to define structured and automated processes. We propose a distributed
AAS copy-on-write infrastructure that enhances security and scalability while
enabling seamless cross organizational collaboration. We also introduce a
workflow management prototype automating AAS operations and engineering
workflows, improving efficiency and traceability.

</details>


### [4] [From Requirements to Code: Understanding Developer Practices in LLM-Assisted Software Engineering](https://arxiv.org/abs/2507.07548)
*Jonathan Ullrich,Matthias Koch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: Despite advances in LLM code generation, developers must still manually convert and enrich requirements into concrete tasks for LLM prompts. Basic requirements engineering remains crucial, and the dream of fully automated requirements-to-code generation is not yet feasible.


<details>
  <summary>Details</summary>
Motivation: With the advancement of generative LLMs for code generation, there is speculation that traditional software engineering could become obsolete. However, it is unclear how developers incorporate requirements when using these LLMs—a critical and underexplored area.

Method: The researchers interviewed 18 practitioners from 14 companies to explore how they leverage requirements and design artifacts in prompts for LLM-based code generation. The data from these interviews was analyzed to build a grounded theory.

Result: The study found that requirements as typically documented are too abstract for direct use in LLM prompts. Developers must decompose requirements into programming tasks and enrich them with design and architectural constraints before inputting them into LLMs. The process is not fully automated and requires significant manual effort.

Conclusion: Traditional requirements engineering (RE) work remains necessary even with LLM-powered code generation. Developers still need to translate and adapt requirements into forms suitable for LLM consumption, meaning automation of requirements-centric tasks is not yet fully realized.

Abstract: With the advent of generative LLMs and their advanced code generation
capabilities, some people already envision the end of traditional software
engineering, as LLMs may be able to produce high-quality code based solely on
the requirements a domain expert feeds into the system. The feasibility of this
vision can be assessed by understanding how developers currently incorporate
requirements when using LLMs for code generation-a topic that remains largely
unexplored. We interviewed 18 practitioners from 14 companies to understand how
they (re)use information from requirements and other design artifacts to feed
LLMs when generating code. Based on our findings, we propose a theory that
explains the processes developers employ and the artifacts they rely on. Our
theory suggests that requirements, as typically documented, are too abstract
for direct input into LLMs. Instead, they must first be manually decomposed
into programming tasks, which are then enriched with design decisions and
architectural constraints before being used in prompts. Our study highlights
that fundamental RE work is still necessary when LLMs are used to generate
code. Our theory is important for contextualizing scientific approaches to
automating requirements-centric SE tasks.

</details>


### [5] [Prompt Engineering for Requirements Engineering: A Literature Review and Roadmap](https://arxiv.org/abs/2507.07682)
*Kaicheng Huang,Fanyu Wang,Yutan Huang,Chetan Arora*

Main category: cs.SE

TL;DR: This paper systematically reviews and structures the field of prompt engineering for requirements engineering, proposing a taxonomy and a stepwise roadmap to make LLM applications in RE more trustworthy and practical.


<details>
  <summary>Details</summary>
Motivation: Large language models have unlocked new possibilities in requirements engineering through prompt engineering, but uncertainty and lack of controllability hinder reliable implementation. Clear, evidence-based guidance is needed to leverage LLMs effectively.

Method: The authors performed a systematic literature review using Kitchenham’s and Petersen’s protocol, searching six digital libraries, screening 867 records, and analyzing 35 selected primary studies in the field of prompt engineering for requirements engineering.

Result: The review produced a hybrid taxonomy connecting prompt engineering techniques to requirements engineering roles and mapped out the tasks, LLM families, and prompt types reported in the literature. It revealed persistent limitations and gaps in current knowledge.

Conclusion: A roadmap was outlined, guiding the transition from current ad-hoc prototypes to reproducible and user-friendly workflows, providing structure for future research and more reliable PE adoption in RE.

Abstract: Advancements in large language models (LLMs) have led to a surge of prompt
engineering (PE) techniques that can enhance various requirements engineering
(RE) tasks. However, current LLMs are often characterized by significant
uncertainty and a lack of controllability. This absence of clear guidance on
how to effectively prompt LLMs acts as a barrier to their trustworthy
implementation in the RE field. We present the first roadmap-oriented
systematic literature review of Prompt Engineering for RE (PE4RE). Following
Kitchenham's and Petersen's secondary-study protocol, we searched six digital
libraries, screened 867 records, and analyzed 35 primary studies. To bring
order to a fragmented landscape, we propose a hybrid taxonomy that links
technique-oriented patterns (e.g., few-shot, Chain-of-Thought) to task-oriented
RE roles (elicitation, validation, traceability). Two research questions, with
five sub-questions, map the tasks addressed, LLM families used, and prompt
types adopted, and expose current limitations and research gaps. Finally, we
outline a step-by-step roadmap showing how today's ad-hoc PE prototypes can
evolve into reproducible, practitioner-friendly workflows.

</details>


### [6] [From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry](https://arxiv.org/abs/2507.07689)
*Chetan Arora,Fanyu Wang,Chakkrit Tantithamthavorn,Aldeida Aleti,Shaun Kenyon*

Main category: cs.SE

TL;DR: AI-driven RAG models can automate and improve requirements engineering for the space industry, helping small organizations more easily generate and comply with mission requirements.


<details>
  <summary>Details</summary>
Motivation: Requirements engineering (RE) in the space industry is challenging, especially for small organizations, due to the complexity, demand for precision, and need to adhere to strict standards when deriving actionable requirements from unstructured documents.

Method: The paper proposes a modular, AI-driven approach leveraging Retrieval-Augmented Generation (RAG) models. It preprocesses mission documents, classifies them semantically, retrieves domain-standard content, and synthesizes requirements drafts via large language models (LLMs), demonstrated on a real-world case study with industry partnership.

Result: Preliminary results show the approach reduces manual effort, improves requirements coverage, and aids compliance, making RE more accessible for smaller organizations.

Conclusion: Integrating AI-based RAG models into the requirements engineering process can significantly benefit small space organizations by automating and streamlining requirement generation, thus enabling broader participation in critical space missions.

Abstract: Requirements engineering (RE) in the space industry is inherently complex,
demanding high precision, alignment with rigorous standards, and adaptability
to mission-specific constraints. Smaller space organisations and new entrants
often struggle to derive actionable requirements from extensive, unstructured
documents such as mission briefs, interface specifications, and regulatory
standards. In this innovation opportunity paper, we explore the potential of
Retrieval-Augmented Generation (RAG) models to support and (semi-)automate
requirements generation in the space domain. We present a modular, AI-driven
approach that preprocesses raw space mission documents, classifies them into
semantically meaningful categories, retrieves contextually relevant content
from domain standards, and synthesises draft requirements using large language
models (LLMs). We apply the approach to a real-world mission document from the
space domain to demonstrate feasibility and assess early outcomes in
collaboration with our industry partner, Starbound Space Solutions. Our
preliminary results indicate that the approach can reduce manual effort,
improve coverage of relevant requirements, and support lightweight compliance
alignment. We outline a roadmap toward broader integration of AI in RE
workflows, intending to lower barriers for smaller organisations to participate
in large-scale, safety-critical missions.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [7] [On Propositional Program Equivalence (extended abstract)](https://arxiv.org/abs/2507.07480)
*Tobias Kappé*

Main category: cs.PL

TL;DR: By abstracting program semantics and focusing on propositional equivalence, program equivalence problems become decidable and applicable in practice. Recent advances leverage (G)KAT as a solid foundation for this analysis.


<details>
  <summary>Details</summary>
Motivation: Since general program equivalence is undecidable, the motivation is to find a practical, decidable approach by abstracting away from detailed semantics, leading to the study of propositional equivalence. This has practical relevance for verification and optimization of programs.

Method: The paper uses the framework of (Guarded) Kleene Algebra with Tests, or (G)KAT, to analyze and formalize propositional program equivalence.

Result: Recent progress has been made in defining and determining propositional program equivalence within the (G)KAT framework, making equivalence checking feasible for practical purposes.

Conclusion: Propositional program equivalence, when ignoring detailed statement semantics, becomes a decidable and practical problem. (Guarded) Kleene Algebra with Tests ((G)KAT) provides a valuable framework for analyzing such equivalence.

Abstract: General program equivalence is undecidable. However, if we abstract away the
semantics of statements, then this problem becomes not just decidable, but
practically feasible. For instance, a program of the form "if $b$ then $e$ else
$f$" should be equivalent to "if not $b$ then $f$ else $e$" - no matter what
$b$, $e$ and $f$ are. This kind of equivalence is known as propositional
equivalence. In this extended abstract, we discuss recent developments in
propositional program equivalence from the perspective of (Guarded) Kleene
Algebra with Tests, or (G)KAT.

</details>
