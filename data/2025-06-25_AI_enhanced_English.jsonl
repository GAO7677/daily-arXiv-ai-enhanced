{"id": "2506.19045", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.19045", "abs": "https://arxiv.org/abs/2506.19045", "authors": ["Ahmadreza Saboor Yaraghi", "Golnaz Gharachorlu", "Sakina Fatima", "Lionel C. Briand", "Ruiyuan Wan", "Ruifeng Gao"], "title": "Black-Box Test Code Fault Localization Driven by Large Language Models and Execution Estimation", "comment": null, "summary": "Fault localization (FL) is a critical step in debugging which typically\nrelies on repeated executions to pinpoint faulty code regions. However,\nrepeated executions can be impractical in the presence of non-deterministic\nfailures or high execution costs. While recent efforts have leveraged Large\nLanguage Models (LLMs) to aid execution-free FL, these have primarily focused\non identifying faults in the system under test (SUT) rather than in the often\ncomplex system test code. However, the latter is also important as, in\npractice, many failures are triggered by faulty test code. To overcome these\nchallenges, we introduce a fully static, LLM-driven approach for system test\ncode fault localization (TCFL) that does not require executing the test case.\nOur method uses a single failure execution log to estimate the test's execution\ntrace through three novel algorithms that identify only code statements likely\ninvolved in the failure. This pruned trace, combined with the error message, is\nused to prompt the LLM to rank potential faulty locations. Our black-box,\nsystem-level approach requires no access to the SUT source code and is\napplicable to large test scripts that assess full system behavior. We evaluate\nour technique at function, block, and line levels using an industrial dataset\nof faulty test cases not previously used in pre-training LLMs. Results show\nthat our best estimated trace closely match actual traces, with an F1 score of\naround 90%. Additionally, pruning the complex system test code reduces the\nLLM's inference time by up to 34% without any loss in FL performance. Our\nresults further suggest that block-level TCFL offers a practical balance,\nnarrowing the search space while preserving useful context, achieving an 81%\nhit rate at top-3 (Hit@3).", "AI": {"tldr": "This paper introduces a novel static approach using LLMs to localize faults in test scripts based on a single failure log, showing high accuracy and efficiency without needing repeated executions or system code access.", "motivation": "Traditional fault localization requires repeated executions, which is infeasible for non-deterministic failures or expensive executions. Existing LLM-based methods typically focus on the system under test, neglecting fault localization in complex test code, which often triggers failures in practice.", "method": "The authors propose a fully static, LLM-driven approach for localizing faults in system test code, without running the test case. They estimate likely failure-related execution traces from a single failure log using three novel algorithms to prune irrelevant code, then use LLMs prompted by this trace and the error message to rank possible faulty code locations. This black-box method works without requiring system under test source code.", "result": "Evaluations on an industrial dataset show that their estimated traces achieve an F1 score of about 90% compared to actual traces. Further, their code-pruning significantly reduces LLM inference time (up to 34%) without harming localization performance. Block-level analysis offers a strong practical trade-off, getting an 81% top-3 hit rate while keeping sufficient context.", "conclusion": "A new static, LLM-based method enables effective fault localization in system test code without execution or access to SUT code, achieving high accuracy, efficiency, and practical trade-offs in real-world test case datasets."}}
{"id": "2506.19153", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.19153", "abs": "https://arxiv.org/abs/2506.19153", "authors": ["Krzysztof Fonal"], "title": "Dataset of Yul Contracts to Support Solidity Compiler Research", "comment": "4 pages", "summary": "The YulCode dataset presents a comprehensive collection of 348,840 Yul-based\nsmart contract instances, comprising approximately 135,013 unique contracts.\nThese contracts were generated through the compilation of Solidity source files\nthat have been deployed on the Ethereum mainnet, making the dataset directly\nrepresentative of real-world decentralized applications. YulCode provides a\nrich foundation for a variety of research and development tasks, including but\nnot limited to machine learning applications, formal verification, optimization\nanalysis, and software engineering tool evaluation in the context of low-level\nsmart contract code. To the best of our knowledge at the time of writing,\nYulCode is the first and only publicly available dataset that focuses\nspecifically on Yul, an intermediate language designed for the Ethereum Virtual\nMachine (EVM). As such, it fills a critical gap in the current ecosystem of\nsmart contract datasets and opens new avenues for research and tooling aimed at\nlow-level contract analysis and generation.", "AI": {"tldr": "YulCode is a pioneering and extensive dataset of Yul-based smart contracts, filling a major gap for low-level Ethereum research, and enabling advancements in contract analysis, verification, and tools.", "motivation": "There was a lack of publicly available datasets focused specifically on Yul, the Ethereum intermediate language, hampering research and tool development for low-level smart contract analysis.", "method": "The authors compiled Solidity source files deployed on the Ethereum mainnet to generate Yul-based contract instances and constructed a large dataset (YulCode) of these instances.", "result": "The YulCode dataset consists of 348,840 Yul-based smart contract instances, representing about 135,013 unique contracts derived from real-world decentralized applications deployed on Ethereum.", "conclusion": "YulCode is the first public and comprehensive Yul-focused dataset, providing valuable resources for machine learning, formal verification, optimization, and software engineering research on Ethereum smart contracts."}}
{"id": "2506.19287", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.19287", "abs": "https://arxiv.org/abs/2506.19287", "authors": ["Yaoxuan Wu", "Xiaojie Zhou", "Ahmad Humayun", "Muhammad Ali Gulzar", "Miryung Kim"], "title": "Generating and Understanding Tests via Path-Aware Symbolic Execution with LLMs", "comment": null, "summary": "Symbolic execution is a widely used technique for test generation, offering\nsystematic exploration of program paths through constraint solving. However, it\nis fundamentally constrained by the capability to model the target code\nincluding library functions in terms of symbolic constraint and the capability\nof underlying constraint solvers. As a result, many paths involving complex\nfeatures remain unanalyzed or insufficiently modeled. Recent advances in large\nlanguage models (LLMs) have shown promise in generating diverse and valid test\ninputs. Yet, LLMs lack mechanisms for systematically enumerating program paths\nand often fail to cover subtle corner cases. We observe that directly prompting\nan LLM with the full program leads to missed coverage of interesting paths. In\nthis paper, we present PALM, a test generation system that combines symbolic\npath enumeration with LLM-assisted test generation. PALM statically enumerates\npossible paths through AST-level analysis and transforms each into an\nexecutable variant with embedded assertions that specify the target path. This\navoids the need to translate path constraints into SMT formulae, by instead\nconstructing program variants that LLM can interpret. Importantly, PALM is the\nfirst to provide an interactive frontend that visualizes path coverage\nalongside generated tests, assembling tests based on the specific paths they\nexercise. A user study with 12 participants demonstrates that PALM's frontend\nhelps users better understand path coverage and identify which paths are\nactually exercised by PALM-generated tests, through verification and\nvisualization of their path profiles.", "AI": {"tldr": "PALM is a test generation tool that combines symbolic path enumeration with LLM-driven test input generation, transforming program paths into variants that LLMs can readily understand and generate tests for. Its interactive coverage visualization aids users in verifying and exploring test coverage, outperforming traditional symbolic execution and LLM-only test generation.", "motivation": "Symbolic execution is effective for test generation but struggles with complex code, library functions, and constraint solving limits. Large Language Models (LLMs) can generate diverse tests but lack systematic path exploration and miss edge cases. There's a need to combine the strengths of both approaches while overcoming their individual limitations.", "method": "The authors propose PALM, a system that uses static AST-level analysis to enumerate program paths and transform each path into an executable program variant with embedded assertions. This bypasses the need for SMT-based constraint solving, allowing the LLM to generate tests for each path variant. PALM also includes an interactive frontend for users to visualize path coverage and understand the relationship between generated tests and code paths.", "result": "PALM improves the coverage of interesting and complex code paths by combining symbolic path enumeration with LLM-based test generation. The interactive frontend enhances user understanding of which paths have been tested. A user study with 12 participants showed that the PALM interface aided in recognizing path coverage and verifying which code paths were exercised by generated tests.", "conclusion": "PALM provides a novel and effective approach to automated test generation by marrying symbolic path enumeration with LLM-assistance, and offering intuitive path coverage visualization. It successfully overcomes key limitations of both symbolic execution and LLM-only approaches, helping users understand and verify which program paths are covered by generated tests."}}
{"id": "2506.19425", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.19425", "abs": "https://arxiv.org/abs/2506.19425", "authors": ["Ang Jia", "He Jiang", "Zhilei Ren", "Xiaochen Li", "Ming Fan", "Ting Liu"], "title": "What Makes the Best Decomposition? Investigating Binary Decomposition Under FCG Variance", "comment": null, "summary": "Binary decomposition, which decomposes binary files into modules, plays a\ncritical role in binary reuse detection. Existing binary decomposition works\neither apply anchor-based methods by extending anchor functions to generate\nmodules, or apply clustering-based methods by using clustering algorithms to\ngroup binary functions, which all rely on that reused code shares similar\nfunction call relationships. However, we find that function call graphs (FCGs)\nvary a lot when using different compilation settings, especially with diverse\nfunction inlining decisions.\n  In this work, we conduct the first systematic empirical study on the variance\nof FCGs compiled by various compilation settings and explore its effect on\nbinary decomposition methods. We first construct a dataset compiled by 17\ncompilers, using 6 optimizations to 4 architectures and analyze the changes and\nmappings of the FCGs. We find that the size of FCGs changes dramatically, while\nthe FCGs are still linked by three different kinds of mappings. Then we\nevaluate the existing works under the FCG variance, and results show that\nexisting works are facing great challenges when conducting cross-compiler\nevaluation with diverse optimization settings. Finally, we propose a method to\nidentify the optimal decomposition and compare the existing decomposition works\nwith the optimal decomposition. Existing works either suffer from low coverage\nor cannot generate stable community similarities.", "AI": {"tldr": "The paper shows that binary decomposition methods for reuse detection struggle due to highly variable function call graphs across compiler settings. By systematically studying this variance, the authors demonstrate that current methods lack stability and coverage, and propose a way to identify optimal decompositions for improved evaluations.", "motivation": "Binary decomposition is vital for binary reuse detection, but existing methods depend on function call graphs (FCGs), which may differ greatly due to various compilation settings and function inlining, potentially compromising decomposition effectiveness.", "method": "The authors conducted the first systematic empirical study analyzing the variance of FCGs caused by different compilers, optimizations, and architectures. They built a dataset compiled by 17 compilers, 6 optimization settings, and 4 architectures, assessing how FCGs change. They further evaluated the performance of existing anchor-based and clustering-based binary decomposition methods under these varying FCGs, and proposed a method to identify the optimal binary decomposition.", "result": "They found that FCGs change size dramatically with compilation settings, although certain mappings persist. Existing decomposition methods perform poorly in cross-compiler evaluations with diverse optimizations, showing low coverage or unstable results. The proposed optimal decomposition method enables better performance assessment by comparison.", "conclusion": "FCGs are highly variable across compilation settings, challenging the effectiveness and reliability of existing binary decomposition methods for reuse detection. The study highlights weaknesses in current techniques and provides a new way to define and evaluate optimal decomposition outcomes under such variability."}}
{"id": "2506.18923", "categories": ["cs.PL", "cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18923", "abs": "https://arxiv.org/abs/2506.18923", "authors": ["Yifan Zong", "Yuntian Deng", "Pengyu Nie"], "title": "Mix-of-Language-Experts Architecture for Multilingual Programming", "comment": "Accepted at LLM4Code @ ICSE 2025", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\naiding developers with tasks like code comprehension, generation, and\ntranslation. Supporting multilingual programming -- i.e., coding tasks across\nmultiple programming languages -- typically requires either (1) finetuning a\nsingle LLM across all programming languages, which is cost-efficient but\nsacrifices language-specific specialization and performance, or (2) finetuning\nseparate LLMs for each programming language, which allows for specialization\nbut is computationally expensive and storage-intensive due to the duplication\nof parameters. This paper introduces MoLE (Mix-of-Language-Experts), a novel\narchitecture that balances efficiency and specialization for multilingual\nprogramming. MoLE is composed of a base model, a shared LoRA (low-rank\nadaptation) module, and a collection of language-specific LoRA modules. These\nmodules are jointly optimized during the finetuning process, enabling effective\nknowledge sharing and specialization across programming languages. During\ninference, MoLE automatically routes to the language-specific LoRA module\ncorresponding to the programming language of the code token being generated.\nOur experiments demonstrate that MoLE achieves greater parameter efficiency\ncompared to training separate language-specific LoRAs, while outperforming a\nsingle shared LLM finetuned for all programming languages in terms of accuracy.", "AI": {"tldr": "MoLE is a modular LLM setup that combines shared and language-specific expertise for code tasks, achieving both efficiency and high performance across multiple programming languages.", "motivation": "Existing methods to support multilingual programming with LLMs either lack specialization for individual languages or are computationally expensive due to duplicated models. There is a need for an efficient approach that doesn't compromise on performance.", "method": "The authors propose MoLE (Mix-of-Language-Experts), a new LLM architecture featuring a base model, a shared LoRA module, and several language-specific LoRA modules. These are collectively finetuned to balance shared knowledge and language-specific expertise. During inference, MoLE dynamically uses the appropriate language-specific module based on the programming language.", "result": "MoLE offers better parameter efficiency than having separate LoRA models for each language and achieves higher accuracy than a naively shared LLM finetuned for all languages.", "conclusion": "MoLE architecture effectively balances efficiency and specialization, offering a scalable solution for multilingual programming with LLMs."}}
{"id": "2506.19481", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.19481", "abs": "https://arxiv.org/abs/2506.19481", "authors": ["Shahbaz Siddeeq", "Muhammad Waseem", "Zeeshan Rasheed", "Md Mahade Hasan", "Jussi Rasku", "Mika Saari", "Henri Terho", "Kalle Makela", "Kai-Kristian Kemell", "Pekka Abrahamsson"], "title": "LLM-based Multi-Agent System for Intelligent Refactoring of Haskell Code", "comment": "arXiv admin note: text overlap with arXiv:2502.07928", "summary": "Refactoring is a constant activity in software development and maintenance.\nScale and maintain software systems are based on code refactoring. However,\nthis process is still labor intensive, as it requires programmers to analyze\nthe codebases in detail to avoid introducing new defects. In this research, we\nput forward a large language model (LLM)-based multi-agent system to automate\nthe refactoring process on Haskell code. The objective of this research is to\nevaluate the effect of LLM-based agents in performing structured and\nsemantically accurate refactoring on Haskell code. Our proposed multi-agent\nsystem based on specialized agents with distinct roles, including code\nanalysis, refactoring execution, verification, and debugging. To test the\neffectiveness and practical applicability of the multi-agent system, we\nconducted evaluations using different open-source Haskell codebases. The\nresults of the experiments carried out showed that the proposed LLM-based\nmulti-agent system could average 11.03% decreased complexity in code, an\nimprovement of 22.46% in overall code quality, and increase performance\nefficiency by an average of 13.27%. Furthermore, memory allocation was\noptimized by up to 14.57%. These results highlight the ability of LLM-based\nmulti-agent in managing refactoring tasks targeted toward functional\nprogramming paradigms. Our findings hint that LLM-based multi-agent systems\nintegration into the refactoring of functional programming languages can\nenhance maintainability and support automated development workflows.", "AI": {"tldr": "This paper presents a multi-agent system based on large language models to automate refactoring of Haskell code, achieving measurable improvements in code complexity, quality, efficiency, and memory use. The approach supports more maintainable and automated development for functional programming.", "motivation": "Refactoring is essential in software development for maintaining and scaling systems, but it remains labor-intensive due to the need to closely analyze code and avoid introducing new defects. The motivation is to reduce manual effort and the risk of errors through automation.", "method": "The authors propose a large language model (LLM)-based multi-agent system for automating refactoring of Haskell code. The system uses specialized agents with different roles: code analysis, refactoring execution, verification, and debugging. They tested this approach on open-source Haskell codebases.", "result": "The LLM-based multi-agent system led to an average of 11.03% decrease in code complexity, 22.46% improvement in code quality, 13.27% increased performance efficiency, and up to 14.57% optimization in memory allocation.", "conclusion": "LLM-based multi-agent systems can effectively automate and improve the refactoring process for functional programming languages, enhancing maintainability and supporting development workflows."}}
{"id": "2506.19457", "categories": ["cs.PL", "cs.DC", "D.3.1; F.3.1; F.3.2"], "pdf": "https://arxiv.org/pdf/2506.19457", "abs": "https://arxiv.org/abs/2506.19457", "authors": ["Tom T. P. Franken", "Thomas Neele", "Jan Friso Groote"], "title": "The Autonomous Data Language -- Concepts, Design and Formal Verification", "comment": "48 pages, preprint submitted to Elsevier", "summary": "Nowadays, the main advances in computational power are due to parallelism.\nHowever, most parallel languages have been designed with a focus on processors\nand threads. This makes dealing with data and memory in programs hard, which\ndistances the implementation from its original algorithm. We propose a new\nparadigm for parallel programming, the data-autonomous paradigm, where\ncomputation is performed by autonomous data elements. Programs in this paradigm\nare focused on making the data collaborate in a highly parallel fashion. We\nfurthermore present AuDaLa, the first data autonomous programming language, and\nprovide a full formalisation that includes a type system and operational\nsemantics. Programming in AuDaLa is very natural, as illustrated by examples,\nalbeit in a style very different from sequential and contemporary parallel\nprogramming. Additionally, it lends itself for the formal verification of\nparallel programs, which we demonstrate.", "AI": {"tldr": "This paper introduces a new parallel programming paradigm\u2014data-autonomous computation\u2014and presents the AuDaLa language, which allows data elements to autonomously drive parallel processes, resulting in more natural programming and easier verification of parallel programs.", "motivation": "Current parallel programming languages prioritize processors and threads, making data and memory management difficult and creating a disconnect between implementations and original algorithms.", "method": "The authors introduce a new parallel programming paradigm called the data-autonomous paradigm, where autonomous data elements drive computation. They also present AuDaLa, the first programming language based on this paradigm, including its type system and operational semantics. They illustrate its use with programming examples and demonstrate formal program verification.", "result": "The data-autonomous paradigm enables highly parallel collaboration among data, resulting in a natural programming style distinct from existing approaches. AuDaLa supports this paradigm and facilitates formal verification of parallel programs.", "conclusion": "The data-autonomous paradigm, realized through the AuDaLa language, offers a new, natural way to program parallel systems by focusing on autonomous data elements and enables better formal verification."}}
{"id": "2506.19511", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.19511", "abs": "https://arxiv.org/abs/2506.19511", "authors": ["Nina Haugland Andersen", "Anastasiia Tkalich", "Nils Brede Moe", "Darja Smite", "Asgaut Mj\u00f8lne S\u00f6derbom", "Ola Hast", "Viktoria Stray"], "title": "Integrating Pair Programming as a Work Practice", "comment": "The pre-print is submitted to the Journal of Systems and Software", "summary": "Context: Pair programming (PP) is more relevant than ever. As modern systems\ngrow in complexity, knowledge sharing and collaboration across teams have\nbecome essential. However, despite well-documented benefits of PP, its adoption\nremains inconsistent across software teams. Objective: This study aims to\nunderstand the factors that facilitate or hinder team members' adoption as well\nas lasting engagement in PP. Method: We have conducted an exploratory\nsingle-case study in a mature agile company in Norway. We collected data\nthrough two rounds of interviews with team members in different roles and\nperformed a thematic analysis of the interviews. Results: Our key finding is\nthat multiple factors, related to the perceptions of how PP contributes to\ndaily work, efforts associated with engaging in PP sessions, company and team\nattitudes, resources, infrastructure, and task characteristics, affect PP\nengagement. Conclusion: Long-term engagement in PP requires expected benefits\nwith the practice being confirmed in firsthand experiences. Adapting the\npractice to each unique team, with insights drawn from collective learning, is\nalso beneficial. Our findings will be beneficial for software practitioners\nseeking to make PP an integrated part of their team's workflow.", "AI": {"tldr": "The paper explores why pair programming isn't consistently adopted despite its benefits. Through interviews and analysis at an agile company, the authors identified factors influencing engagement such as team attitudes, effort, and infrastructure. They conclude that sustained use relies on direct, confirmed benefits and team-driven adaptations.", "motivation": "The motivation behind this paper is to understand why, despite the well-known advantages of pair programming (PP), its widespread and sustained adoption remains inconsistent in software development teams\u2014especially as software complexity and the necessity for knowledge sharing have increased.", "method": "The authors used an exploratory single-case study approach at a mature agile company in Norway. They collected qualitative data via two rounds of interviews with team members filling various roles and analyzed the data using thematic analysis.", "result": "The study found that a range of factors influence engagement with PP. These include perceptions of PP's contributions to daily work, the effort required for effective participation, team and company attitudes, the availability of resources and infrastructure, and the specific characteristics of tasks.", "conclusion": "Long-term engagement in pair programming hinges on users experiencing tangible, confirmed benefits. Customizing and adapting the practice based on collective team learning further supports sustained adoption. The results offer guidance to practitioners looking to integrate PP into their team's workflow."}}
{"id": "2506.19539", "categories": ["cs.SE", "cs.AI", "D.2.7"], "pdf": "https://arxiv.org/pdf/2506.19539", "abs": "https://arxiv.org/abs/2506.19539", "authors": ["Julian Fragner", "Christian Macho", "Bernhard Dieber", "Martin Pinzger"], "title": "Lost in Translation? Converting RegExes for Log Parsing into Dynatrace Pattern Language", "comment": "18 pages, 7 tables, 18 figures", "summary": "Log files provide valuable information for detecting and diagnosing problems\nin enterprise software applications and data centers. Several log analytics\ntools and platforms were developed to help filter and extract information from\nlogs, typically using regular expressions (RegExes). Recent commercial log\nanalytics platforms provide domain-specific languages specifically designed for\nlog parsing, such as Grok or the Dynatrace Pattern Language (DPL). However,\nusers who want to migrate to these platforms must manually convert their\nRegExes into the new pattern language, which is costly and error-prone. In this\nwork, we present Reptile, which combines a rule-based approach for converting\nRegExes into DPL patterns with a best-effort approach for cases where a full\nconversion is impossible. Furthermore, it integrates GPT-4 to optimize the\nobtained DPL patterns. The evaluation with 946 RegExes collected from a large\ncompany shows that Reptile safely converted 73.7% of them. The evaluation of\nReptile's pattern optimization with 23 real-world RegExes showed an F1-score\nand MCC above 0.91. These results are promising and have ample practical\nimplications for companies that migrate to a modern log analytics platform,\nsuch as Dynatrace.", "AI": {"tldr": "Reptile automates the conversion of regular expressions to the Dynatrace Pattern Language for log analytics, achieving strong accuracy and reducing manual migration work for companies adopting new platforms.", "motivation": "Companies increasingly rely on commercial log analytics platforms with specialized pattern languages for log parsing. Migrating existing workflows requires converting numerous regular expressions (RegExes) to these new languages, a process that is labor-intensive and error-prone.", "method": "The authors propose Reptile, a tool that combines rule-based conversion from RegExes to Dynatrace Pattern Language (DPL) with best-effort strategies for partial conversions. Additionally, Reptile leverages GPT-4 to further optimize the generated patterns.", "result": "In evaluations using 946 real-world RegExes, Reptile achieved safe conversion for 73.7% of them. Further assessments using 23 RegExes demonstrated that Reptile's GPT-4 optimization achieved F1-scores and MCC values above 0.91, indicating strong performance.", "conclusion": "Reptile effectively automates much of the RegEx-to-DPL conversion process, reducing migration effort and error rates. Its success suggests practical value for enterprises transitioning to modern log analytics platforms like Dynatrace."}}
{"id": "2506.19653", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.19653", "abs": "https://arxiv.org/abs/2506.19653", "authors": ["Antonios Saravanos"], "title": "Simulating the Waterfall Model: A Systematic Review", "comment": null, "summary": "This systematic mapping study examines how the Waterfall Model has been\nrepresented in computational simulations within peer-reviewed literature. While\nAgile methodologies dominate contemporary software design practices, the\nWaterfall Model persists, particularly, within hybrid approaches that fuse\nstructured, sequential workflows with the adaptability of agile practices.\nDespite its continued presence, little attention has been given to how the\nWaterfall Model is simulated in research contexts. A structured search of major\nacademic databases identified 68 peer-reviewed studies published between 2000\nand 2024. After applying inclusion criteria, selected studies were analyzed\nacross four dimensions: (1) simulation methodologies (e.g., discrete-event\nsimulation, system dynamics), (2) platforms and tools (e.g., Simphony.NET,\nSimPy), (3) geographic and temporal trends, and (4) fidelity to Royce's\noriginal seven-phase model. Discrete-event simulation was most commonly used,\nreflecting the model's sequential nature. Early work relied on proprietary\nplatforms, while recent studies increasingly use open-source, Python-based\ntools. No studies fully implemented Royce's original formulation, most employed\nadaptations. These findings suggest that although niche, simulation of the\nWaterfall Model is present in academic discourse. This work highlights the need\nfor accessible modeling tools and calls for future research that integrates the\nwaterfall software process model with modern hybrid practices.", "AI": {"tldr": "This paper mapped how the Waterfall Model is simulated in academic research. It found mostly adapted (not original) versions are used, with discrete-event simulation and a shift to open-source tools. The paper suggests more accessible tools and further study into integrating the Waterfall Model with contemporary hybrid approaches.", "motivation": "The study is motivated by the ongoing use of the Waterfall Model in software development, particularly in hybrid approaches, despite Agile methodologies being more prevalent. Additionally, there is a lack of research on how the Waterfall Model is actually simulated within academic contexts.", "method": "The research employs a systematic mapping study. It involved a structured search across major academic databases to identify and select relevant peer-reviewed papers (published 2000-2024), followed by analysis of these studies across four dimensions: simulation methodologies, tools/platforms, trends (geographic and temporal), and fidelity to Royce's original model.", "result": "Discrete-event simulation is the most commonly used methodology, aligning with the sequential nature of the Waterfall Model. Earlier studies used proprietary tools, but there is a trend toward open-source, Python-based tools. No identified study has implemented Royce's original seven-phase model fully; most use adaptations.", "conclusion": "While the simulation of the Waterfall Model remains a niche research area, it is present in academic literature. The findings indicate a need for more accessible modeling tools and encourage future research to further explore integration of the Waterfall Model with modern hybrid software development practices."}}
{"id": "2506.19677", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.19677", "abs": "https://arxiv.org/abs/2506.19677", "authors": ["Shi Chang", "Boyuan Chen", "Kishanthan Thangarajah", "Hanan Lutfiyya", "Ahmed E. Hassan"], "title": "Adaptive Request Scheduling for CodeLLM Serving with SLA Guarantees", "comment": null, "summary": "Code Large Language Models (CodeLLMs) are increasingly integrated into modern\nsoftware development workflows, yet efficiently serving them in\nresource-constrained, self-hosted environments remains a significant challenge.\nExisting LLM serving systems employs Continuous Batching for throughput\nimprovement. However, they rely on static batch size configurations that cannot\nadapt to fluctuating request rates or heterogeneous workloads, leading to\nfrequent SLA (Service Level Agreement) violations and unstable performance. In\nthis study, We propose SABER, a dynamic batching strategy that predicts\nper-request SLA feasibility and adjusts decisions in real time. SABER improves\ngoodput by up to 26% over the best static configurations and reduces latency\nvariability by up to 45%, all without manual tuning or service restarts. Our\nresults demonstrate that SLA-aware, adaptive scheduling is key to robust,\nhigh-performance CodeLLM serving.", "AI": {"tldr": "Static batching strategies for serving CodeLLMs struggle with fluctuating workloads. SABER, a new dynamic batching system, predicts per-request SLA feasibility and adjusts in real time, boosting throughput and stability without manual tuning.", "motivation": "Efficiently serving Code Large Language Models (CodeLLMs) in resource-constrained, self-hosted environments is challenging due to fluctuating request rates and varying workloads. Existing systems with static batching cannot maintain stable, high-performance service, leading to frequent SLA violations.", "method": "The study proposes SABER, a dynamic batching strategy that predicts whether each incoming request can meet its SLA and dynamically adjusts batching decisions in real time. This approach does not require manual tuning or restarting the service.", "result": "SABER improves goodput by up to 26% compared with the best static batching configurations and reduces latency variability by up to 45%.", "conclusion": "SLA-aware, adaptive scheduling like SABER is essential for robust, high-performance CodeLLM serving in practical, self-hosted scenarios."}}
{"id": "2506.19757", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.19757", "abs": "https://arxiv.org/abs/2506.19757", "authors": ["Rodrigo Oliveira Zacarias", "L\u00e9o Carvalho Ramos Antunes", "M\u00e1rcio de Oliveira Barros", "Rodrigo Pereira dos Santos", "Patricia Lago"], "title": "Exploring Developer Experience Factors in Software Ecosystems", "comment": "58 pages", "summary": "Context: Developer experience (DX) plays a key role in developers'\nperformance and their continued involvement in a software ecosystem (SECO)\nplatform. While researchers and practitioners have recognized several factors\naffecting DX in SECO platforms, a clear roadmap of the most influential factors\nis still missing. This is particularly important given the direct impact on\ndevelopers' interest in SECO and their ongoing engagement with the common\ntechnological platform. Goal: This work aims to identify key DX factors and\nunderstand how they influence third-party developers' decisions to adopt and\nkeep contributing to a SECO. Methods: We conducted a systematic mapping study\n(SMS), analyzing 29 studies to assess the state-of-the-art of DX in SECO.\nAdditionally, we conducted a Delphi study to evaluate the influence of 27 DX\nfactors (identified in our SMS) from the perspective of 21 third-party\ndevelopers to adopt and keep contributing to a SECO. Results: The factors that\nmost strongly influence developers' adoption and ongoing contributions to a\nSECO are: financial costs for using the platform, desired technical resources\nfor development, low barriers to entry into the applications market, and more\nfinancial gains. Conclusion: DX is essential for the success and sustainability\nof SECO. Our set of DX factors provides valuable insights and recommendations\nfor researchers and practitioners to address key DX concerns from the\nperspective of third-party developers.", "AI": {"tldr": "This paper identifies and prioritizes key developer experience (DX) factors influencing third-party developer adoption and continued involvement in software ecosystem (SECO) platforms. Using a systematic mapping and Delphi study, it finds that financial concerns, technical resources, entry barriers, and financial incentives are the strongest factors in shaping developer engagement. The results offer clear guidance to improve DX and SECO sustainability.", "motivation": "Developer experience (DX) significantly affects developer performance and long-term engagement in software ecosystem (SECO) platforms. However, there has been a lack of a clear, prioritized roadmap of the most influential DX factors within SECOs, despite recognized importance. Addressing this gap is critical for sustaining developer interest and contributions.", "method": "The authors conducted a systematic mapping study (SMS) of 29 previous studies to compile state-of-the-art DX factors in SECOs. They further employed a Delphi study methodology with 21 third-party developers to evaluate the influence of 27 identified DX factors on developers\u2019 decisions to adopt and continuously contribute to SECO platforms.", "result": "The most influential DX factors for adoption and ongoing contribution to SECOs are: financial costs of platform usage, availability of desired technical resources, low barriers to entry in the applications marketplace, and potential for increased financial gains.", "conclusion": "DX is crucial to the growth and sustainability of SECOs. The identified set of DX factors provides practical insights and actionable recommendations for both researchers and practitioners to enhance third-party developer engagement in SECOs."}}
