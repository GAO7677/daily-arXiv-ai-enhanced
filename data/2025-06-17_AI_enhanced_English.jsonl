{"id": "2506.12084", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.FL", "cs.NE"], "pdf": "https://arxiv.org/pdf/2506.12084", "abs": "https://arxiv.org/abs/2506.12084", "authors": ["Michele Alberti", "Fran\u00e7ois Bobot", "Julien Girard-Satabin", "Alban Grastien", "Aymeric Varasse", "Zakaria Chihani"], "title": "The CAISAR Platform: Extending the Reach of Machine Learning Specification and Verification", "comment": null, "summary": "The formal specification and verification of machine learning programs saw\nremarkable progress in less than a decade, leading to a profusion of tools.\nHowever, diversity may lead to fragmentation, resulting in tools that are\ndifficult to compare, except for very specific benchmarks. Furthermore, this\nprogress is heavily geared towards the specification and verification of a\ncertain class of property, that is, local robustness properties. But while\nprovers are becoming more and more efficient at solving local robustness\nproperties, even slightly more complex properties, involving multiple neural\nnetworks for example, cannot be expressed in the input languages of winners of\nthe International Competition of Verification of Neural Networks VNN-Comp. In\nthis tool paper, we present CAISAR, an open-source platform dedicated to\nmachine learning specification and verification. We present its specification\nlanguage, suitable for modelling complex properties on neural networks, support\nvector machines and boosted trees. We show on concrete use-cases how\nspecifications written in this language are automatically translated to queries\nto state-of-the-art provers, notably by using automated graph editing\ntechniques, making it possible to use their off-the-shelf versions. The\nartifact to reproduce the paper claims is available at the following DOI:\nhttps://doi.org/10.5281/zenodo.15209510", "AI": {"tldr": "CAISAR is a new open-source platform that allows the formal specification and verification of complex properties in machine learning models (including neural networks, SVMs, and boosted trees) by automatically translating specifications into queries for existing verification tools, thus overcoming current limitations in expressiveness and tool fragmentation.", "motivation": "The motivation behind this paper is to address the fragmentation in the field of formal specification and verification of machine learning programs, where a variety of tools have emerged that are mostly limited to verifying simple local robustness properties. Existing tools are difficult to compare and cannot handle more complex properties, especially those involving multiple neural networks or various model types.", "method": "The authors present CAISAR, an open-source platform that introduces a new specification language capable of modeling complex properties in machine learning systems, such as those involving neural networks, support vector machines, and boosted trees. The platform automatically translates these specifications into queries compatible with existing state-of-the-art verification tools using automated graph editing.", "result": "The paper demonstrates, through concrete use-cases, that CAISAR can effectively model and specify complex properties, and successfully leverage existing provers by automatically translating its specifications, thus enabling the use of unmodified, off-the-shelf verification tools.", "conclusion": "CAISAR streamlines the specification and verification process for complex machine learning systems, bridging the gap caused by fragmented and narrowly focused tools. It enables automatic, tool-agnostic verification of more sophisticated properties across various machine learning models."}}
{"id": "2506.12111", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.12111", "abs": "https://arxiv.org/abs/2506.12111", "authors": ["Oscar Boullosa Dapena"], "title": "Quantum-Inspired Differentiable Integral Neural Networks (QIDINNs): A Feynman-Based Architecture for Continuous Learning Over Streaming Data", "comment": null, "summary": "Real-time continuous learning over streaming data remains a central challenge\nin deep learning and AI systems. Traditional gradient-based models such as\nbackpropagation through time (BPTT) face computational and stability\nlimitations when dealing with temporally unbounded data. In this paper, we\nintroduce a novel architecture, Quantum-Inspired Differentiable Integral Neural\nNetworks (QIDINNs), which leverages the Feynman technique of differentiation\nunder the integral sign to formulate neural updates as integrals over\nhistorical data. This reformulation allows for smoother, more stable learning\ndynamics that are both physically interpretable and computationally tractable.\nInspired by Feynman's path integral formalism and compatible with quantum\ngradient estimation frameworks, QIDINNs open a path toward hybrid\nclassical-quantum neural computation. We demonstrate our model's effectiveness\non synthetic and real-world streaming tasks, and we propose directions for\nquantum extensions and scalable implementations.", "AI": {"tldr": "The paper proposes QIDINNs\u2014a new neural architecture using integral-based learning inspired by quantum mechanics\u2014to achieve stable, efficient, and interpretable real-time learning on streaming data, showing promising results and paving the way for quantum-classical hybrid models.", "motivation": "Real-time continuous learning on streaming, temporally-unbounded data is limited by the computational cost and stability challenges of traditional gradient-based models like BPTT.", "method": "The paper introduces Quantum-Inspired Differentiable Integral Neural Networks (QIDINNs), which use differentiation under the integral sign (a Feynman technique) to express neural updates as integrals over historical data, enabling smoother and more stable updates. This approach is inspired by Feynman's path integral, making it compatible with quantum gradient estimation and allowing hybrid classical-quantum computation.", "result": "The QIDINN model is evaluated on both synthetic and real-world streaming tasks, demonstrating improved stability and effectiveness over previous methods. Potential for quantum extensions and scalable implementations is discussed.", "conclusion": "QIDINNs provide a computationally efficient, stable, and physically interpretable framework for continuous learning on streaming data, with promising avenues for quantum integration and broader scalability."}}
{"id": "2506.12278", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.12278", "abs": "https://arxiv.org/abs/2506.12278", "authors": ["Zheyuan Yang", "Zexi Kuang", "Xue Xia", "Yilun Zhao"], "title": "Can LLMs Generate High-Quality Test Cases for Algorithm Problems? TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure", "comment": "ACL 2025", "summary": "We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs\nin test-case generation. TestCase-Eval includes 500 algorithm problems and\n100,000 human-crafted solutions from the Codeforces platform. It focuses on two\npivotal tasks: (1) Fault Coverage, which measures how well LLM-generated test\nsets probe diverse input scenarios and cover a wide range of potential failure\nmodes. (2) Fault Exposure, which evaluates whether LLMs can craft a tailored\ntest input that reveals a specific incorrect code implementation. We provide a\ncomprehensive assessment of 19 state-of-the-art open-source and proprietary\nLLMs on TestCase-Eval, offering insights into their strengths and limitations\nin generating effective test cases for algorithm problems.", "AI": {"tldr": "TestCase-Eval is a new benchmark for evaluating LLMs' ability to generate test cases for algorithm problems. It assesses both coverage of diverse failures and the capacity to expose specific faults, analyzing 19 LLMs and uncovering valuable insights into their real-world testing capabilities.", "motivation": "Evaluating LLMs on code generation typically focuses on generating correct code, but systematic test-case generation is an under-explored capability with real-world importance. Current benchmarks lack comprehensive and structured evaluation for test-case generation in competitive programming.", "method": "Developed TestCase-Eval, a benchmark containing 500 algorithm problems and 100,000 human-written solutions from Codeforces. Introduces two tasks: Fault Coverage (coverage of diverse inputs and failure types) and Fault Exposure (ability to generate inputs revealing specific bugs). 19 leading LLMs were comprehensively evaluated using this benchmark.", "result": "The benchmark highlighted differences in the strengths and weaknesses of leading LLMs for generating effective test cases. The systematic evaluation revealed variability in LLMs' ability to cover diverse failure modes and expose specific faults in coding problems.", "conclusion": "TestCase-Eval offers a rigorous, large-scale benchmark for the evaluation of LLMs in test-case generation, revealing nuanced insights into their capabilities and limitations in supporting robust software engineering tasks."}}
{"id": "2506.12320", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12320", "abs": "https://arxiv.org/abs/2506.12320", "authors": ["Weipeng Jiang", "Xiaoyu Zhang", "Xiaofei Xie", "Jiongchi Yu", "Yuhan Zhi", "Shiqing Ma", "Chao Shen"], "title": "The Foundation Cracks: A Comprehensive Study on Bugs and Testing Practices in LLM Libraries", "comment": null, "summary": "Large Language Model (LLM) libraries have emerged as the foundational\ninfrastructure powering today's AI revolution, serving as the backbone for LLM\ndeployment, inference optimization, fine-tuning, and production serving across\ndiverse applications. Despite their critical role in the LLM ecosystem, these\nlibraries face frequent quality issues and bugs that threaten the reliability\nof AI systems built upon them. To address this knowledge gap, we present the\nfirst comprehensive empirical investigation into bug characteristics and\ntesting practices in modern LLM libraries. We examine 313 bug-fixing commits\nextracted across two widely-adopted LLM libraries: HuggingFace Transformers and\nvLLM.Through rigorous manual analysis, we establish comprehensive taxonomies\ncategorizing bug symptoms into 5 types and root causes into 14 distinct\ncategories.Our primary discovery shows that API misuse has emerged as the\npredominant root cause (32.17%-48.19%), representing a notable transition from\nalgorithm-focused defects in conventional deep learning frameworks toward\ninterface-oriented problems. Additionally, we examine 7,748 test functions to\nidentify 7 distinct test oracle categories employed in current testing\napproaches, with predefined expected outputs (such as specific tensors and text\nstrings) being the most common strategy. Our assessment of existing testing\neffectiveness demonstrates that the majority of bugs escape detection due to\ninadequate test cases (41.73%), lack of test drivers (32.37%), and weak test\noracles (25.90%). Drawing from these findings, we offer some recommendations\nfor enhancing LLM library quality assurance.", "AI": {"tldr": "LLM libraries face many interface-related bugs and poor bug detection due to weak testing practices. This study classifies bug/root cause types and test strategies, recommending improvements for better AI library reliability.", "motivation": "Large Language Model (LLM) libraries are crucial for today's AI systems, but frequent bugs and quality issues threaten their reliability. There is a lack of comprehensive understanding about the nature of these bugs and the effectiveness of testing in such libraries.", "method": "The authors conducted a comprehensive empirical investigation, analyzing 313 bug-fixing commits from two leading LLM libraries (HuggingFace Transformers and vLLM). They manually classified bug symptoms and root causes, and evaluated 7,748 test functions to categorize test oracle strategies and assess the effectiveness of current testing practices.", "result": "API misuse is the most frequent root cause of bugs (32.17%-48.19%), marking a shift from traditional algorithm-focused defects to more interface-oriented issues. Predefined expected outputs are the most common test oracle. Many bugs remain undetected due to inadequate test cases, lack of proper test drivers, and weak oracles.", "conclusion": "The study exposes a strong prevalence of interface/API-related bugs and inadequate testing effectiveness in LLM libraries. The findings suggest specific avenues for improving quality assurance, such as strengthening test cases, drivers, and oracles."}}
{"id": "2506.12202", "categories": ["cs.PL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.12202", "abs": "https://arxiv.org/abs/2506.12202", "authors": ["Stephen Mell", "Botong Zhang", "David Mell", "Shuo Li", "Ramya Ramalingam", "Nathan Yu", "Steve Zdancewic", "Osbert Bastani"], "title": "A Fast, Reliable, and Secure Programming Language for LLM Agents with Code Actions", "comment": null, "summary": "Modern large language models (LLMs) are often deployed as agents, calling\nexternal tools adaptively to solve tasks. Rather than directly calling tools,\nit can be more effective for LLMs to write code to perform the tool calls,\nenabling them to automatically generate complex control flow such as\nconditionals and loops. Such code actions are typically provided as Python\ncode, since LLMs are quite proficient at it; however, Python may not be the\nideal language due to limited built-in support for performance, security, and\nreliability. We propose a novel programming language for code actions, called\nQuasar, which has several benefits: (1) automated parallelization to improve\nperformance, (2) uncertainty quantification to improve reliability and mitigate\nhallucinations, and (3) security features enabling the user to validate\nactions. LLMs can write code in a subset of Python, which is automatically\ntranspiled to Quasar. We evaluate our approach on the ViperGPT visual question\nanswering agent, applied to the GQA dataset, demonstrating that LLMs with\nQuasar actions instead of Python actions retain strong performance, while\nreducing execution time when possible by 42%, improving security by reducing\nuser approval interactions when possible by 52%, and improving reliability by\napplying conformal prediction to achieve a desired target coverage level.", "AI": {"tldr": "The paper introduces Quasar, a new language for code actions in LLM agents, which provides improved performance, security, and reliability compared to Python, as shown in VQA experiments.", "motivation": "Existing LLMs often use Python for tool-calling code actions, but Python has limitations in performance, security, and reliability for this purpose.", "method": "The authors propose a new programming language, Quasar, specifically designed for LLM-driven code actions. Key features of Quasar include automated parallelization, uncertainty quantification, and enhanced security mechanisms. The approach allows LLMs to initially write in a subset of Python, which is then transpiled to Quasar. The method is evaluated by comparing Quasar-based code actions with Python-based actions in a VQA (Visual Question Answering) agent called ViperGPT on the GQA dataset.", "result": "Using Quasar enables LLM agents to maintain high task performance, reduces execution time by up to 42%, decreases the frequency of user approval interactions by up to 52% (enhancing security), and increases reliability through uncertainty quantification to meet desired confidence thresholds.", "conclusion": "Quasar is a promising language for LLM-driven code actions, offering meaningful improvements in performance, security, and reliability over Python, and is validated on real-world agent tasks."}}
{"id": "2506.12347", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.12347", "abs": "https://arxiv.org/abs/2506.12347", "authors": ["Aayush Kumar", "Yasharth Bajpai", "Sumit Gulwani", "Gustavo Soares", "Emerson Murphy-Hill"], "title": "How Developers Use AI Agents: When They Work, When They Don't, and Why", "comment": null, "summary": "Software Engineering Agents (SWE agents) can autonomously perform development\ntasks on benchmarks like SWE Bench, but still face challenges when tackling\ncomplex and ambiguous real-world tasks. Consequently, SWE agents are often\ndesigned to allow interactivity with developers, enabling collaborative\nproblem-solving. To understand how developers collaborate with SWE agents and\nthe communication challenges that arise in such interactions, we observed 19\ndevelopers using an in-IDE agent to resolve 33 open issues in repositories to\nwhich they had previously contributed. Participants successfully resolved about\nhalf of these issues, with participants solving issues incrementally having\ngreater success than those using a one-shot approach. Participants who actively\ncollaborated with the agent and iterated on its outputs were also more\nsuccessful, though they faced challenges in trusting the agent's responses and\ncollaborating on debugging and testing. These results have implications for\nsuccessful developer-agent collaborations, and for the design of more effective\nSWE agents.", "AI": {"tldr": "Developer collaboration with SWE agents is more successful when iterative and interactive, but trust and debugging challenges remain; design improvements are needed for better real-world performance.", "motivation": "Software Engineering Agents (SWE agents) can perform development tasks autonomously but struggle with complex, ambiguous real-world problems. Understanding how developers interact and collaborate with such agents is important for improving their usefulness.", "method": "The study involved observing 19 developers as they used an in-IDE SWE agent to try to solve 33 open issues in repositories to which the developers had previously contributed.", "result": "Participants resolved about half of the issues. Success was higher among those who took incremental, interactive, and iterative approaches rather than a single-shot attempt. Challenges included trust, debugging, and testing collaborations.", "conclusion": "Active, iterative collaboration with SWE agents leads to greater developer success, but significant communication and trust issues must be addressed to enhance agent effectiveness and collaboration."}}
{"id": "2506.12212", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2506.12212", "abs": "https://arxiv.org/abs/2506.12212", "authors": ["Grant VanDomelen", "Gan Shen", "Lindsey Kuper", "Yao Li"], "title": "Freer Arrows and Why You Need Them in Haskell", "comment": "In submission to the Haskell Symposium 2025", "summary": "Freer monads are a useful structure commonly used in various domains due to\ntheir expressiveness. However, a known issue with freer monads is that they are\nnot amenable to static analysis. This paper explores freer arrows, a relatively\nexpressive structure that is amenable to static analysis. We propose several\nvariants of freer arrows. We conduct a case study on choreographic programming\nto demonstrate the usefulness of freer arrows in Haskell.", "AI": {"tldr": "Freer arrows fix a known limitation of freer monads (poor static analysis) and are shown\u2014via Haskell case studies\u2014to be practical and expressive, making them useful for domains needing static verification.", "motivation": "Freer monads are widely used for their expressiveness but are difficult to analyze statically, which limits certain compiler optimizations and error checking.", "method": "The paper investigates alternative structures called freer arrows. Several variants of freer arrows are proposed, and their applicability is demonstrated through a case study focused on choreographic programming in Haskell.", "result": "Freer arrows prove to be more amenable to static analysis than freer monads. The case study shows that they can be effectively used in Haskell for choreographic programming.", "conclusion": "Freer arrows present a viable alternative to freer monads when static analysis is required. The proposed variants expand their usability, and their effectiveness is validated in a real-world programming scenario."}}
{"id": "2506.12590", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.12590", "abs": "https://arxiv.org/abs/2506.12590", "authors": ["Breno Alves de Andrade", "Rodrigo Siqueira", "Lidiane Gomes", "Antonio Oliveira", "Danilo Monteiro Ribeiro"], "title": "A Mapping Study About Training in Industry Context in Software Engineering", "comment": null, "summary": "Context: Corporate training plays a strategic role in the continuous\ndevelopment of professionals in the software engineering industry. However,\nthere is a lack of systematized understanding of how training initiatives are\ndesigned, implemented, and evaluated within this domain.\n  Objective: This study aims to map the current state of research on corporate\ntraining in software engineering in industry settings, using Eduardo Salas'\ntraining framework as an analytical lens.\n  Method: A systematic mapping study was conducted involving the selection and\nanalysis of 26 primary studies published in the field. Each study was\ncategorized according to Salas' four key areas: Training Needs Analysis,\nAntecedent Training Conditions, Training Methods and Instructional Strategies,\nand Post-Training Conditions.\n  Results: The findings show a predominance of studies focusing on Training\nMethods and Instructional Strategies. Significant gaps were identified in other\nareas, particularly regarding Job/Task Analysis and Simulation-based Training\nand Games. Most studies were experience reports, lacking methodological rigor\nand longitudinal assessment.\n  Conclusions: The study offers a structured overview of how corporate training\nis approached in software engineering, revealing underexplored areas and\nproposing directions for future research. It contributes to both academic and\npractical communities by highlighting challenges, methodological trends, and\nopportunities for designing more effective training programs in industry.", "AI": {"tldr": "The paper systematically maps research on corporate training in software engineering, finding a focus on instructional strategies and identifying gaps in needs analysis and simulation-based training. It calls for more rigorous and diverse research to improve training effectiveness in the industry.", "motivation": "There is a lack of systematized understanding of how corporate training initiatives in software engineering are designed, implemented, and evaluated. This paper aims to address this gap and provide a structured overview of the current state of research in this area.", "method": "A systematic mapping study was conducted by selecting and analyzing 26 primary studies related to corporate training in software engineering. The studies were categorized according to Eduardo Salas' framework, covering four areas: Training Needs Analysis, Antecedent Training Conditions, Training Methods and Instructional Strategies, and Post-Training Conditions.", "result": "The study found that most research focuses on Training Methods and Instructional Strategies, with significant gaps identified in Job/Task Analysis and Simulation-based Training and Games. The majority of studies reviewed were experience reports and many lacked methodological rigor and longitudinal assessment.", "conclusion": "This work provides a structured overview of corporate training research in software engineering, identifying underexplored areas and suggesting future research directions. It highlights challenges and current trends, offering insights for academics and practitioners to design more effective training programs."}}
{"id": "2506.13383", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2506.13383", "abs": "https://arxiv.org/abs/2506.13383", "authors": ["Jules Jacobs", "Nate Foster", "Tobias Kapp\u00e9", "Dexter Kozen", "Lily Saada", "Alexandra Silva", "Jana Wagemaker"], "title": "StacKAT: Infinite State Network Verification", "comment": null, "summary": "We develop StacKAT, a network verification language featuring loops, finite\nstate variables, nondeterminism, and - most importantly - access to a stack\nwith accompanying push and pop operations. By viewing the variables and stack\nas the (parsed) headers and (to-be-parsed) contents of a network packet,\nStacKAT can express a wide range of network behaviors including parsing, source\nrouting, and telemetry. These behaviors are difficult or impossible to model\nusing existing languages like NetKAT. We develop a decision procedure for\nStacKAT program equivalence, based on finite automata. This decision procedure\nprovides the theoretical basis for verifying network-wide properties and is\nable to provide counterexamples for inequivalent programs. Finally, we provide\nan axiomatization of StacKAT equivalence and establish its completeness.", "AI": {"tldr": "StacKAT is a new network verification language that introduces stack operations for richer expressiveness, enabling analysis of complex behaviors like parsing and source routing. It supports formal equivalence checking and has a complete axiomatization, addressing key gaps in prior solutions.", "motivation": "Existing network verification languages like NetKAT cannot express a wide range of network behaviors such as parsing, source routing, and telemetry, due to limitations like lack of stack operations and more complex state handling.", "method": "The authors develop StacKAT, a network verification language that extends expressiveness by incorporating loops, finite state variables, nondeterminism, and a stack with push/pop operations. They interpret variables and the stack as analogous to parsed headers and packet contents, enabling more complex modeling. They also propose a decision procedure based on finite automata to check program equivalence and provide a formal axiomatization of the language's equivalence, establishing completeness.", "result": "StacKAT is able to express complex network behaviors that are out of reach for previous languages like NetKAT. The proposed decision procedure supports equivalence checking and provides counterexamples for non-equivalence. The axiomatization is shown to be complete.", "conclusion": "StacKAT extends the capabilities of network verification languages to cover behaviors like parsing, source routing, and telemetry, which previous languages struggled to represent. Its decision procedure and axiomatization enable rigorous verification and analysis of network programs."}}
{"id": "2506.12616", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.12616", "abs": "https://arxiv.org/abs/2506.12616", "authors": ["Debasish Jana", "Pinakpani Pal", "Pawan Kumar"], "title": "Real-Time Agile Software Management for Edge and Fog Computing Based Smart City Infrastructure", "comment": "The paper has been published at the Fifth International Conference on\n  Computing and Communication Networks (ICCCN 2025), Volume 1", "summary": "The evolution of smart cities demands scalable, secure, and energy-efficient\narchitectures for real-time data processing. With the number of IoT devices\nexpected to exceed 40 billion by 2030, traditional cloud-based systems are\nincreasingly constrained by bandwidth, latency, and energy limitations. This\npaper leverages the ROOF (Real-time Onsite Operations Facilitation) framework\nwith decentralized computing at intermediary fog and peripheral edge network\nlayers to reduce latency by processing data near its point of origin. ROOF\nfeatures fog caching to avoid redundancy, ultra-low-power wireless transmission\nfor energy savings, and AI-driven resource allocation for efficiency. Security\nis enhanced through TLS encryption, blockchain-based authentication, and\nedge-level access control. Case studies from Bhubaneswar, Barcelona and\nCopenhagen validate the use of ROOF in traffic systems and environmental\nmonitoring. The paper concludes by outlining key challenges and prospects of\nAI-driven analytics in smart urban infrastructure.", "AI": {"tldr": "This paper introduces the ROOF framework to decentralize data processing in smart cities via fog and edge computing. It demonstrates improved latency, energy savings, redundancy reduction, and strong security through various technologies, validated by international case studies. Key challenges and the role of AI in future urban analytics are also explored.", "motivation": "The demand for smart cities is rapidly increasing, necessitating new solutions for scalable, secure, and energy-efficient real-time data processing due to the predicted surge of IoT devices (over 40 billion by 2030). Existing cloud-based systems are unable to sufficiently address bandwidth, latency, and energy constraints.", "method": "The paper implements the ROOF (Real-time Onsite Operations Facilitation) framework, which decentralizes computing by introducing intermediary fog and edge networks for local data processing. Key techniques include fog caching, ultra-low-power wireless transmission, AI-driven resource allocation, TLS encryption, blockchain authentication, and edge-level access control. Case studies from Bhubaneswar, Barcelona, and Copenhagen are used for validation.", "result": "ROOF reduces latency, avoids redundancy, saves energy, and improves resource efficiency in smart city scenarios. Enhanced security is achieved with blockchain authentication and edge-level controls. The case studies affirm the framework's benefits in real-world traffic and environmental monitoring applications.", "conclusion": "ROOF effectively addresses existing limitations of cloud-based IoT management architectures in smart cities. The framework offers scalable and secure real-time processing with reduced latency and energy consumption. Future challenges and the growing importance of AI-driven analytics in urban infrastructure are discussed."}}
{"id": "2506.12643", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.12643", "abs": "https://arxiv.org/abs/2506.12643", "authors": ["Prachnachai Meakpaiboonwattana", "Warittha Tarntong", "Thai Mekratanavorakul", "Chaiyong Ragkhitwetsagul", "Pattaraporn Sangaroonsilp", "Raula Kula", "Morakot Choetkiertikul", "Kenichi Matsumoto", "Thanwadee Sunetnanta"], "title": "Social Media Reactions to Open Source Promotions: AI-Powered GitHub Projects on Hacker News", "comment": null, "summary": "Social media platforms have become more influential than traditional news\nsources, shaping public discourse and accelerating the spread of information.\nWith the rapid advancement of artificial intelligence (AI), open-source\nsoftware (OSS) projects can leverage these platforms to gain visibility and\nattract contributors. In this study, we investigate the relationship between\nHacker News, a social news site focused on computer science and\nentrepreneurship, and the extent to which it influences developer activity on\nthe promoted GitHub AI projects.\n  We analyzed 2,195 Hacker News (HN) stories and their corresponding comments\nover a two-year period. Our findings reveal that at least 19\\% of AI developers\npromoted their GitHub projects on Hacker News, often receiving positive\nengagement from the community. By tracking activity on the associated 1,814\nGitHub repositories after they were shared on Hacker News, we observed a\nsignificant increase in forks, stars, and contributors. These results suggest\nthat Hacker News serves as a viable platform for AI-powered OSS projects, with\nthe potential to gain attention, foster community engagement, and accelerate\nsoftware development.", "AI": {"tldr": "Sharing AI-related open-source projects on Hacker News can significantly boost community engagement and development activity on GitHub, making the platform valuable for project promotion and growth.", "motivation": "The paper is motivated by the increasing influence of social media, especially in the context of the AI and open-source software (OSS) ecosystem. The authors aim to understand how platforms like Hacker News can impact the visibility and developmental growth of AI-based OSS projects.", "method": "The authors analyzed 2,195 stories on Hacker News and their comments over two years. They tracked the activity on 1,814 associated GitHub repositories\u2014specifically monitoring metrics like forks, stars, and contributors\u2014after the projects were shared on Hacker News to assess the impact.", "result": "At least 19% of AI developers promoted their GitHub projects on Hacker News, frequently receiving positive engagement. The data showed a significant increase in developer activity, as seen in the rise of forks, stars, and contributors on these repositories after being shared on the platform.", "conclusion": "Hacker News is an effective platform for promoting AI OSS projects, leading to increased visibility, engagement, and accelerated software development activity."}}
{"id": "2506.12669", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.12669", "abs": "https://arxiv.org/abs/2506.12669", "authors": ["Anrafel Fernandes Pereira", "Marcos Kalinowski", "Maria Teresa Baldassarre", "J\u00fcrgen B\u00f6rstler", "Nauman bin Ali", "Daniel Mendez"], "title": "Towards Lean Research Inception: Assessing Practical Relevance of Formulated Research Problems", "comment": "Accepted for publication at EASE 2025", "summary": "[Context] The lack of practical relevance in many Software Engineering (SE)\nresearch contributions is often rooted in oversimplified views of industrial\npractice, weak industry connections, and poorly defined research problems.\nClear criteria for evaluating SE research problems can help align their value,\nfeasibility, and applicability with industrial needs. [Goal] In this paper, we\nintroduce the Lean Research Inception (LRI) framework, designed to support the\nformulation and assessment of practically relevant research problems in SE. We\ndescribe its initial evaluation strategy conducted in a workshop with a network\nof SE researchers experienced in industry-academia collaboration and report the\nevaluation of its three assessment criteria (valuable, feasible, and\napplicable) regarding their importance in assessing practical relevance.\n[Method] We applied LRI retroactively to a published research paper, engaging\nworkshop participants in discussing and assessing the research problem by\napplying the proposed criteria using a semantic differential scale.\nParticipants provided feedback on the criteria's importance and completeness,\ndrawn from their own experiences in industry-academia collaboration. [Results]\nThe findings reveal an overall agreement on the importance of the three\ncriteria - valuable (83.3%), feasible (76.2%), and applicable (73.8%) - for\naligning research problems with industrial needs. Qualitative feedback\nsuggested adjustments in terminology with a clearer distinction between\nfeasible and applicable, and refinements for valuable by more clearly\nconsidering business value, ROI, and originality. [Conclusion] While LRI\nconstitutes ongoing research and requires further evaluation, our results\nstrengthen our confidence that the three criteria applied using the semantic\ndifferential scale can already help the community assess the practical\nrelevance of SE research problems.", "AI": {"tldr": "The paper presents and evaluates the Lean Research Inception (LRI) framework for assessing practical relevance in SE research. It identifies 'valuable', 'feasible', and 'applicable' as key criteria, validated through expert feedback. The approach appears promising but needs further evaluation.", "motivation": "Many Software Engineering (SE) research contributions lack practical relevance due to oversimplified industrial perspectives, weak connections to industry, and poorly-defined research problems. There is a need for clear criteria that align research with industrial needs.", "method": "The authors introduce the Lean Research Inception (LRI) framework and evaluate it during a workshop with SE researchers experienced in industry-academia collaboration. They retroactively applied LRI to an existing paper, facilitating discussion on three criteria (valuable, feasible, applicable) using a semantic differential scale and gathering participant feedback.", "result": "The study found strong agreement among participants regarding the importance of the three criteria: valuable (83.3%), feasible (76.2%), and applicable (73.8%) in making research relevant to industry needs. Feedback suggested refinement of the criteria, especially in clarifying terminology and expanding the concept of 'valuable' to include business value, ROI, and originality.", "conclusion": "While further evaluation is required, the LRI framework and its three assessment criteria (valuable, feasible, applicable) show promise in helping assess the practical relevance of SE research problems, contributing confidence to their utility within the research and practitioner community."}}
{"id": "2506.12691", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12691", "abs": "https://arxiv.org/abs/2506.12691", "authors": ["Bianca Trinkenreich", "Fabio Calefato", "Geir Hanssen", "Kelly Blincoe", "Marcos Kalinowski", "Mauro Pezz\u00e8", "Paolo Tell", "Margaret-Anne Storey"], "title": "Get on the Train or be Left on the Station: Using LLMs for Software Engineering Research", "comment": "Accepted for publication at the 1st Workshop on Human-Centered AI for\n  SE (Human AISE) held at the 33rd ACM International Conference on the\n  Foundations of Software Engineering (FSE Companion '25), June 23-28, 2025,\n  Trondheim, Norway", "summary": "The adoption of Large Language Models (LLMs) is not only transforming\nsoftware engineering (SE) practice but is also poised to fundamentally disrupt\nhow research is conducted in the field. While perspectives on this\ntransformation range from viewing LLMs as mere productivity tools to\nconsidering them revolutionary forces, we argue that the SE research community\nmust proactively engage with and shape the integration of LLMs into research\npractices, emphasizing human agency in this transformation. As LLMs rapidly\nbecome integral to SE research - both as tools that support investigations and\nas subjects of study - a human-centric perspective is essential. Ensuring human\noversight and interpretability is necessary for upholding scientific rigor,\nfostering ethical responsibility, and driving advancements in the field.\nDrawing from discussions at the 2nd Copenhagen Symposium on Human-Centered AI\nin SE, this position paper employs McLuhan's Tetrad of Media Laws to analyze\nthe impact of LLMs on SE research. Through this theoretical lens, we examine\nhow LLMs enhance research capabilities through accelerated ideation and\nautomated processes, make some traditional research practices obsolete,\nretrieve valuable aspects of historical research approaches, and risk reversal\neffects when taken to extremes. Our analysis reveals opportunities for\ninnovation and potential pitfalls that require careful consideration. We\nconclude with a call to action for the SE research community to proactively\nharness the benefits of LLMs while developing frameworks and guidelines to\nmitigate their risks, to ensure continued rigor and impact of research in an\nAI-augmented future.", "AI": {"tldr": "LLMs are set to transform SE research, offering significant benefits and risks. The community must ensure human oversight and develop guidelines to balance innovation with scientific rigor and ethical responsibility.", "motivation": "Large Language Models (LLMs) are rapidly becoming central to software engineering (SE) research, with the potential to revolutionize practices. This paper is motivated by the need for the SE research community to proactively address the challenges and opportunities brought by LLMs, emphasizing the importance of maintaining human agency and scientific rigor as these technologies are integrated into research.", "method": "The paper employs McLuhan's Tetrad of Media Laws as a theoretical framework. Through this lens and supported by discussions at the 2nd Copenhagen Symposium on Human-Centered AI in SE, the authors analyze the multifaceted impact of LLMs on SE research, considering aspects like enhancement, obsolescence, retrieval, and reversal.", "result": "The analysis shows that LLMs can boost research through faster ideation and automation, but may also obsolete some traditional practices and introduce new risks if over-relied upon. Key findings include identifying both innovative opportunities and potential hazards tied to LLM adoption in SE research.", "conclusion": "SE researchers should not passively accept LLMs but actively shape their adoption, ensuring human oversight, interpretability, and ethical responsibility. Proactive community efforts are needed to realize the benefits of LLMs while establishing guidelines and frameworks to mitigate associated risks, thus maintaining rigor and impact in AI-augmented research."}}
{"id": "2506.12713", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.12713", "abs": "https://arxiv.org/abs/2506.12713", "authors": ["Xiangyang Li", "Xiaopeng Li", "Kuicai Dong", "Quanhu Zhang", "Rongju Ruan", "Xinyi Dai", "Xiaoshuang Liu", "Shengchun Xu", "Yasheng Wang", "Ruiming Tang"], "title": "Humanity's Last Code Exam: Can Advanced LLMs Conquer Human's Hardest Code Competition?", "comment": null, "summary": "Code generation is a core capability of large language models (LLMs), yet\nmainstream benchmarks (e.g., APPs and LiveCodeBench) contain questions with\nmedium-level difficulty and pose no challenge to advanced LLMs. To better\nreflected the advanced reasoning and code generation ability, We introduce\nHumanity's Last Code Exam (HLCE), comprising 235 most challenging problems from\nthe International Collegiate Programming Contest (ICPC World Finals) and the\nInternational Olympiad in Informatics (IOI) spanning 2010 - 2024. As part of\nHLCE, we design a harmonized online-offline sandbox that guarantees fully\nreproducible evaluation. Through our comprehensive evaluation, we observe that\neven the strongest reasoning LLMs: o4-mini(high) and Gemini-2.5 Pro, achieve\npass@1 rates of only 15.9% and 11.4%, respectively. Meanwhile, we propose a\nnovel \"self-recognition\" task to measure LLMs' awareness of their own\ncapabilities. Results indicate that LLMs' self-recognition abilities are not\nproportionally correlated with their code generation performance. Finally, our\nempirical validation of test-time scaling laws reveals that current advanced\nLLMs have substantial room for improvement on complex programming tasks. We\nexpect HLCE to become a milestone challenge for code generation and to catalyze\nadvances in high-performance reasoning and human-AI collaborative programming.\nOur code and dataset are also public\navailable(https://github.com/Humanity-s-Last-Code-Exam/HLCE).", "AI": {"tldr": "The paper introduces HLCE, a new, much harder code generation benchmark. Even top LLMs perform poorly, highlighting significant weaknesses and the need for further progress in advanced LLM coding and reasoning.", "motivation": "Current benchmarks for LLM code generation, such as APPs and LiveCodeBench, are of medium difficulty and no longer challenge state-of-the-art models. There is a need for more difficult and representative benchmarks to properly assess advanced LLM reasoning and code generation capabilities.", "method": "The authors introduce Humanity's Last Code Exam (HLCE), a new benchmark consisting of 235 highly challenging programming problems from the ICPC World Finals and International Olympiad in Informatics (2010-2024). They design an integrated sandbox for reproducible evaluation, assess top LLMs on HLCE, and propose a 'self-recognition' task to measure LLMs' ability to predict their own code generation performance.", "result": "Even the best LLMs, such as o4-mini(high) and Gemini-2.5 Pro, achieve pass@1 rates of only 15.9% and 11.4% respectively. LLMs' self-recognition ability is not well correlated with actual code generation performance. Test-time scaling law analysis shows there is substantial room for improvement on difficult programming tasks.", "conclusion": "HLCE poses a significantly higher challenge for LLM code generation than existing benchmarks, revealing that advanced LLMs still struggle with complex tasks. This benchmark is likely to become an important standard for evaluating progress in LLM reasoning and human-AI programming collaboration."}}
{"id": "2506.12728", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.12728", "abs": "https://arxiv.org/abs/2506.12728", "authors": ["Yibo Wang", "Zhihao Peng", "Ying Wang", "Zhao Wei", "Hai Yu", "Zhiliang Zhu"], "title": "MCTS-Refined CoT: High-Quality Fine-Tuning Data for LLM-Based Repository Issue Resolution", "comment": null, "summary": "LLMs demonstrate strong performance in auto-mated software engineering,\nparticularly for code generation and issue resolution. While proprietary models\nlike GPT-4o achieve high benchmarks scores on SWE-bench, their API dependence,\ncost, and privacy concerns limit adoption. Open-source alternatives offer\ntransparency but underperform in complex tasks, especially sub-100B parameter\nmodels. Although quality Chain-of-Thought (CoT) data can enhance reasoning,\ncurrent methods face two critical flaws: (1) weak rejection sampling reduces\ndata quality, and (2) inadequate step validation causes error accumulation.\nThese limitations lead to flawed reasoning chains that impair LLMs'ability to\nlearn reliable issue resolution. The paper proposes MCTS-REFINE, an enhanced\nMonte Carlo Tree Search (MCTS)-based algorithm that dynamically validates and\noptimizes intermediate reasoning steps through a rigorous rejection sampling\nstrategy, generating high-quality CoT data to improve LLM performance in issue\nresolution tasks. Key innovations include: (1) augmenting MCTS with a\nreflection mechanism that corrects errors via rejection sampling and\nrefinement, (2) decomposing issue resolution into three subtasks-File\nLocalization, Fault Localization, and Patch Generation-each with clear\nground-truth criteria, and (3) enforcing a strict sampling protocol where\nintermediate outputs must exactly match verified developer patches, ensuring\ncorrectness across reasoning paths. Experiments on SWE-bench Lite and SWE-bench\nVerified demonstrate that LLMs fine-tuned with our CoT dataset achieve\nsubstantial improvements over baselines.Notably, Qwen2.5-72B- Instruct achieves\n28.3%(Lite) and 35.0%(Verified) resolution rates, surpassing SOTA baseline\nSWE-Fixer-Qwen-72B with the same parameter scale, which only reached\n24.7%(Lite) and 32.8%(Verified).", "AI": {"tldr": "Existing open-source LLMs struggle with software issue resolution due to poor CoT training data. This paper proposes MCTS-REFINE, which rigorously validates and refines reasoning steps using enhanced rejection sampling, producing higher-quality training data and resulting in substantial performance gains above previous methods.", "motivation": "The motivation is to address the limitations in current methods for generating high-quality Chain-of-Thought (CoT) data used for training LLMs in automated software engineering, especially for issue resolution tasks. Existing approaches suffer from weak rejection sampling and inadequate step validation, leading to poor learning outcomes and flawed reasoning chains in open-source models, which underperform compared to proprietary solutions due to these data quality issues.", "method": "The paper introduces MCTS-REFINE, an improved Monte Carlo Tree Search algorithm enhanced with a reflection mechanism for correcting reasoning errors via rigorous rejection sampling and refinement. The method decomposes issue resolution into three subtasks with ground-truth criteria and applies strict intermediate output validation, requiring alignment with developer-generated patches at each step.", "result": "LLMs fine-tuned with the CoT data generated through MCTS-REFINE show notable performance gains on issue resolution benchmarks (SWE-bench Lite and SWE-bench Verified), with Qwen2.5-72B-Instruct surpassing prior state-of-the-art results at the same parameter scale, achieving 28.3% and 35.0% resolution rates, compared to the baselines' 24.7% and 32.8%.", "conclusion": "MCTS-REFINE significantly enhances the quality of CoT data for LLM fine-tuning by robustly validating intermediate reasoning, enabling open-source models to achieve superior performance in software issue resolution, reducing the gap with proprietary solutions."}}
{"id": "2506.12760", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.12760", "abs": "https://arxiv.org/abs/2506.12760", "authors": ["Lantian Li", "Yejian Liang", "Zhongxing Yu"], "title": "IDOL: Improved Different Optimization Levels Testing for Solidity Compilers", "comment": "Accepted by QRS 2025 (Fast Abstracts track)", "summary": "As blockchain technology continues to evolve and mature, smart contracts have\nbecome a key driving force behind the digitization and automation of\ntransactions. Smart contracts greatly simplify and refine the traditional\nbusiness transaction processes, and thus have had a profound impact on various\nindustries such as finance and supply chain management. However, because smart\ncontracts cannot be modified once deployed, any vulnerabilities or design flaws\nwithin the contract cannot be easily fixed, potentially leading to significant\nfinancial losses or even legal issues. The compiler, as a critical component in\nthe development process, directly affects the quality and security of smart\ncontracts. This paper innovatively proposes a method, known as the Improved\nDifferent Optimization Levels (IDOL), for testing the Solidity compiler. The\nkey idea behind IDOL is to perform reverse optimization transformations (i.e.,\nchange optimized form into unoptimized form) to generate semantically\nequivalent variants of the smart contracts under test, aiming to maximize the\nopportunities to trigger the optimization logic of compilers. We conducted a\npreliminary evaluation of IDOL and three confirmed compiler optimization bugs\nhave been uncovered at the time of writing.", "AI": {"tldr": "The paper proposes IDOL, a method for testing Solidity compilers by generating contract variants through reverse optimizations. This enhances the detection of bugs in compiler optimizations, as shown by the discovery of three new bugs.", "motivation": "Smart contracts are immutable once deployed, so any vulnerabilities or flaws can result in major financial or legal risks. The compiler's reliability directly impacts contract quality and security. Thus, there's a pressing need for robust compiler testing.", "method": "The study introduces the Improved Different Optimization Levels (IDOL) approach, which generates semantically equivalent variants of a smart contract by reversing optimization transformations. By comparing optimized and unoptimized contract forms, IDOL aims to more thoroughly exercise and test the Solidity compiler's optimization logic.", "result": "Preliminary evaluations using IDOL have uncovered three confirmed optimization-related bugs in the Solidity compiler.", "conclusion": "IDOL enables more effective testing of smart contract compilers by maximizing the scenarios in which optimization logic can be triggered, helping discover security or correctness issues."}}
{"id": "2506.12858", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.12858", "abs": "https://arxiv.org/abs/2506.12858", "authors": ["Nick Battle", "Peter Gorm Larsen"], "title": "Towards Operation Proof Obligation Generation for VDM", "comment": "Presented at the 23rd Overture workshop, June 2025\n  (arXiv:cs/2506.08680)", "summary": "All formalisms have the ability to ensure that their models are internally\nconsistent. Potential inconsistencies are generally highlighted by assertions\ncalled proof obligations, and the generation of these obligations is an\nimportant role of the tools that support the method. This capability has been\navailable for VDM tools for many years. However, support for obligation\ngeneration for explicit operation bodies has always been limited. This work\ndescribes the current state of work to address this, showing the capabilities\nso far and highlighting the work remaining.", "AI": {"tldr": "This paper reviews progress and ongoing work to enhance proof obligation generation for explicit operation bodies in VDM tools, identifying both improvements and remaining gaps.", "motivation": "The paper is motivated by the need to improve the generation of proof obligations (which ensure model consistency) in formal modeling tools, particularly for VDM tools where support for explicit operation bodies has been insufficient.", "method": "The authors describe and analyze the progress made in developing improved support for proof obligation generation related to explicit operation bodies, including a discussion of tool capabilities and incomplete areas.", "result": "The work presents the capabilities achieved so far in generating proof obligations for explicit operation bodies within VDM tools, as well as areas where work is still outstanding.", "conclusion": "Improved support for proof obligation generation in VDM tools, particularly for explicit operation bodies, is ongoing. Progress has been made, but some challenges and incomplete areas remain."}}
{"id": "2506.13114", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.13114", "abs": "https://arxiv.org/abs/2506.13114", "authors": ["Yanzhou Mu", "Rong Wang", "Juan Zhai", "Chunrong Fang", "Xiang Chen", "Jiacong Wu", "An Guo", "Jiawei Shen", "Bingzhuo Li", "Zhenyu Chen"], "title": "Designing Deep Learning Frameworks for LLMs:Challenges, Expectations, and Opportunities", "comment": "12 pages, 2 figures", "summary": "Large language models (LLMs) drive significant advancements in real industry\napplications. LLMs rely on DL frameworks for efficient model construction,\ndistributed execution, and optimized deployment. Their large parameter scale\nand long execution cycles place extreme demands on DL frameworks in terms of\nscalability, stability, and efficiency. Therefore, poor usability, limited\nfunctionality, and subtle bugs in DL frameworks may hinder development\nefficiency and cause severe failures or resource waste. However, a fundamental\nquestion remains underinvestigated, i.e., What challenges do DL frameworks face\nin supporting LLMs? To seek an answer, we investigate these challenges through\na large-scale analysis of issue reports from three major DL frameworks\n(MindSpore, PyTorch, TensorFlow) and eight associated LLM toolkits (e.g.,\nMegatron). We construct a taxonomy of LLM-centric bugs, requirements, and user\nquestions and enrich it through interviews with 11 LLM users and eight DL\nframework developers, uncovering key technical challenges and misalignments\nbetween user needs and developer priorities. Our contributions are threefold:\n(1) we develop a comprehensive taxonomy comprising four question themes (nine\nsub-themes), four requirement themes (15 sub-themes), and ten bug themes (45\nsub-themes); (2) we assess the perceived importance and priority of these\nchallenges based on practitioner insights; and (3) we identify five key\nfindings across the LLM development and propose five actionable recommendations\nto improve the reliability, usability, and testability of DL frameworks. Our\nresults highlight critical limitations in current DL frameworks and offer\nconcrete guidance for advancing their support for the next generation of LLM\nconstruction and applications.", "AI": {"tldr": "The paper systematically analyzes challenges faced by deep learning frameworks in supporting large language models (LLMs) via issue reports and interviews, builds a detailed taxonomy of problems, and offers recommendations to improve LLM development by addressing identified technical shortcomings.", "motivation": "Large language models (LLMs) are increasingly used in industry, and their reliance on deep learning (DL) frameworks brings unique challenges due to their scale, complexity, and requirements. There is limited understanding of what specific challenges DL frameworks face when supporting LLM development.", "method": "The authors performed a large-scale analysis of issue reports from three widely-used DL frameworks (MindSpore, PyTorch, and TensorFlow) and eight LLM toolkits (such as Megatron). This analysis was augmented by interviews with 11 LLM users and 8 developers of DL frameworks. They then constructed a comprehensive taxonomy of issues, requirements, and questions, and assessed their importance and priority with practitioner input.", "result": "The study developed a detailed taxonomy of LLM-related problems, including four question themes (with 9 sub-themes), four requirement themes (with 15 sub-themes), and ten bug themes (with 45 sub-themes). Based on this, they identified key technical challenges and misalignments between user needs and developer priorities. Five major findings were discovered and five actionable recommendations were proposed to enhance DL framework reliability, usability, and testability.", "conclusion": "Current DL frameworks have critical limitations in supporting LLMs. Addressing these issues with targeted improvements in reliability, usability, and testability is essential for the advancement of LLM applications. The study\u2019s taxonomy and recommendations provide concrete guidance to framework developers and the broader community."}}
{"id": "2506.13171", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13171", "abs": "https://arxiv.org/abs/2506.13171", "authors": ["Lukasz Mazur", "Nenad Petrovic", "James Pontes Miranda", "Ansgar Radermacher", "Robert Rasche", "Alois Knoll"], "title": "Querying Large Automotive Software Models: Agentic vs. Direct LLM Approaches", "comment": null, "summary": "Large language models (LLMs) offer new opportunities for interacting with\ncomplex software artifacts, such as software models, through natural language.\nThey present especially promising benefits for large software models that are\ndifficult to grasp in their entirety, making traditional interaction and\nanalysis approaches challenging. This paper investigates two approaches for\nleveraging LLMs to answer questions over software models: direct prompting,\nwhere the whole software model is provided in the context, and an agentic\napproach combining LLM-based agents with general-purpose file access tools. We\nevaluate these approaches using an Ecore metamodel designed for timing analysis\nand software optimization in automotive and embedded domains. Our findings show\nthat while the agentic approach achieves accuracy comparable to direct\nprompting, it is significantly more efficient in terms of token usage. This\nefficiency makes the agentic approach particularly suitable for the automotive\nindustry, where the large size of software models makes direct prompting\ninfeasible, establishing LLM agents as not just a practical alternative but the\nonly viable solution. Notably, the evaluation was conducted using small LLMs,\nwhich are more feasible to be executed locally - an essential advantage for\nmeeting strict requirements around privacy, intellectual property protection,\nand regulatory compliance. Future work will investigate software models in\ndiverse formats, explore more complex agent architectures, and extend agentic\nworkflows to support not only querying but also modification of software\nmodels.", "AI": {"tldr": "Comparing direct prompting and agentic LLM approaches for querying large software models, the paper shows agentic methods are as accurate but far more efficient, making them the most viable solution for large-scale, privacy-sensitive industrial applications.", "motivation": "Large software models are hard to interact with and analyze using traditional methods. The emergence of large language models (LLMs) offers an opportunity to overcome these challenges by enabling natural language interactions with software artifacts.", "method": "The paper investigates two LLM-based methods for querying software models: (1) direct prompting (putting the entire software model into the context) and (2) an agentic approach that uses LLM-driven agents along with file-access tools. Both methods are evaluated with an Ecore metamodel for timing analysis and optimization in automotive/embedded domains.", "result": "The agentic approach achieves similar accuracy to direct prompting but with significantly greater efficiency regarding token usage. This makes agentic methods more suitable for large models, especially in the automotive industry, where model size makes direct prompting impractical. The experiments demonstrate this using small LLMs suitable for local, private deployment.", "conclusion": "Agentic LLM-based approaches present a more efficient and practical way to interact with large software models compared to direct prompting, especially when privacy and resource-efficiency are required. This is particularly relevant in industries like automotive, where these constraints are significant."}}
{"id": "2506.13182", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13182", "abs": "https://arxiv.org/abs/2506.13182", "authors": ["Anh Ho", "Thanh Le-Cong", "Bach Le", "Christine Rizkallah"], "title": "From Empirical Evaluation to Context-Aware Enhancement: Repairing Regression Errors with LLMs", "comment": null, "summary": "[...] Since then, various APR approaches, especially those leveraging the\npower of large language models (LLMs), have been rapidly developed to fix\ngeneral software bugs. Unfortunately, the effectiveness of these advanced\ntechniques in the context of regression bugs remains largely unexplored. This\ngap motivates the need for an empirical study evaluating the effectiveness of\nmodern APR techniques in fixing real-world regression bugs.\n  In this work, we conduct an empirical study of APR techniques on Java\nregression bugs. To facilitate our study, we introduce RegMiner4APR, a\nhigh-quality benchmark of Java regression bugs integrated into a framework\ndesigned to facilitate APR research. The current benchmark includes 99\nregression bugs collected from 32 widely used real-world Java GitHub\nrepositories. We begin by conducting an in-depth analysis of the benchmark,\ndemonstrating its diversity and quality. Building on this foundation, we\nempirically evaluate the capabilities of APR to regression bugs by assessing\nboth traditional APR tools and advanced LLM-based APR approaches. Our\nexperimental results show that classical APR tools fail to repair any bugs,\nwhile LLM-based APR approaches exhibit promising potential. Motivated by these\nresults, we investigate impact of incorporating bug-inducing change information\ninto LLM-based APR approaches for fixing regression bugs. Our results highlight\nthat this context-aware enhancement significantly improves the performance of\nLLM-based APR, yielding 1.8x more successful repairs compared to using\nLLM-based APR without such context.", "AI": {"tldr": "The study shows that while traditional APR tools can't fix real-world Java regression bugs, LLM-based approaches can, and using bug-inducing code changes as extra context makes them nearly twice as effective.", "motivation": "Despite significant advances in Automated Program Repair (APR), especially with large language models (LLMs), little is known about their effectiveness in fixing regression bugs. There is a need to empirically evaluate modern APR techniques on real-world regression bugs to address this gap.", "method": "The authors introduce RegMiner4APR, a curated benchmark of 99 regression bugs from 32 mainstream Java GitHub repositories. They analyze the benchmark and empirically evaluate both traditional and LLM-based APR tools on these bugs. They further experiment with enriching LLM-based APR tools using bug-inducing code change context information.", "result": "Conventional APR tools failed to repair any regression bugs. LLM-based APR approaches showed promising results, and augmenting them with bug-inducing change context boosted their repair success by 1.8x.", "conclusion": "LLM-based APR approaches demonstrate significant promise for fixing regression bugs, especially when provided with bug-inducing change context. Traditional APR tools are ineffective in this setting. Future research should further leverage context information to enhance APR capabilities."}}
{"id": "2506.13186", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.13186", "abs": "https://arxiv.org/abs/2506.13186", "authors": ["Jiajun Sun", "Fengjie Li", "Xinzhu Qi", "Hongyu Zhang", "Jiajun Jiang"], "title": "Empirical Evaluation of Large Language Models in Automated Program Repair", "comment": null, "summary": "The increasing prevalence of software bugs has made automated program repair\n(APR) a key research focus. Large language models (LLMs) offer new\nopportunities for APR, but existing studies mostly rely on smaller,\nearlier-generation models and Java benchmarks. The repair capabilities of\nmodern, large-scale LLMs across diverse languages and scenarios remain\nunderexplored. To address this, we conduct a comprehensive empirical study of\nfour open-source LLMs, CodeLlama, LLaMA, StarCoder, and DeepSeek-Coder,\nspanning 7B to 33B parameters, diverse architectures, and purposes. We evaluate\nthem across two bug scenarios (enterprise-grades and algorithmic), three\nlanguages (Java, C/C++, Python), and four prompting strategies, analyzing over\n600K generated patches on six benchmarks. Key findings include: (1) model\nspecialization (e.g., CodeLlama) can outperform larger general-purpose models\n(e.g., LLaMA); (2) repair performance does not scale linearly with model size;\n(3) correct patches often appear early in generation; and (4) prompts\nsignificantly affect results. These insights offer practical guidance for\ndesigning effective and efficient LLM-based APR systems.", "AI": {"tldr": "Modern large language models for automated program repair do not always benefit from larger size or generality; specialized models, prompt strategies, and early generations matter more. The findings guide future LLM-based repair system designs.", "motivation": "Automated program repair (APR) is essential due to rising software bugs, but existing research primarily focuses on earlier, smaller language models and Java benchmarks, leaving the capabilities of modern, large-scale LLMs underexplored.", "method": "A comprehensive empirical study evaluating four open-source large language models (CodeLlama, LLaMA, StarCoder, DeepSeek-Coder) with varying parameters, architectures, and intended purposes. The evaluation spans two bug scenarios, three programming languages, and four prompting strategies, analyzing over 600,000 generated patches across six benchmarks.", "result": "Key findings are: (1) specialized models like CodeLlama can outperform larger, general-purpose models; (2) increased model size does not guarantee better repair performance; (3) correct patches frequently appear early during generation; (4) prompt choice notably impacts outcomes.", "conclusion": "The study\u2019s results deliver practical insights for building more effective and efficient LLM-based automated program repair systems, highlighting model specialization, prompt engineering, and the importance of not assuming linear improvement with larger models."}}
{"id": "2506.13273", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.13273", "abs": "https://arxiv.org/abs/2506.13273", "authors": ["Charaka Geethal Kapugama"], "title": "Isolating Noisy Labelled Test Cases in Human-in-the-Loop Oracle Learning", "comment": "2025 International Research Conference on Smart Computing and Systems\n  Engineering (SCSE)", "summary": "Incorrectly labelled test cases can adversely affect the training process of\nhuman-in-the-loop oracle learning tech-niques. This paper introduces ISONOISE,\na technique designed to identify such mislabelled test cases introduced during\nhuman-in-the-loop oracle learning. This technique can be applied to programs\ntaking numeric inputs. Given a compromised automatic test oracle and its\ntraining test suite, ISONOISE first isolates thetest cases suspected of being\nmislabelled. This task is performed based on the level of disagreement of a\ntest case with respect to the others. An intermediate automatic test oracle is\ntrained based on the slightly disagreeing test cases. Based on the predictions\nof this intermediate oracle, the test cases suspected of being mislabelled are\nsystematically presented for relabelling. When mislabelled test cases are\nfound, the intermediate test oracle is updated. This process repeats until no\nmislabelled test case is found in relabelling. ISONOISE was evaluated within\nthe human-in-the-loop oracle learning method used in LEARN2FIX. Experimental\nresults demonstrate that ISONOISE can identify mislabelled test cases\nintroduced by the human in LEARN2FIX with over 67% accuracy, while requiring\nonly a small number of relabelling queries. These findings highlight the\npotential of ISONOISE to enhance the reliability of human-in-the-loop oracle\nlearning.", "AI": {"tldr": "ISONOISE is a technique that effectively detects incorrectly labeled test cases in human-in-the-loop oracle learning, improving reliability with over 67% accuracy and little relabeling needed.", "motivation": "Incorrectly labeled test cases introduced during the human-in-the-loop oracle learning can degrade the performance and reliability of machine learning models. Detecting and correcting such mislabeled data is crucial for improving the training process.", "method": "The paper proposes ISONOISE, a technique for programs with numeric inputs. ISONOISE first isolates suspected mislabeled test cases by measuring their disagreement with the rest of the test suite. It then trains an intermediate test oracle on slightly disagreeing cases and presents the most suspect cases for relabeling. Mislabeled cases trigger updates to the intermediate oracle, and this loop repeats until no further mislabeled cases are found.", "result": "ISONOISE was evaluated in the LEARN2FIX human-in-the-loop oracle learning setting and showed over 67% accuracy in identifying mislabeled test cases, while needing only a small number of relabeling queries.", "conclusion": "ISONOISE enhances the reliability of human-in-the-loop oracle learning by efficiently identifying and enabling correction of mislabeled test cases with high accuracy and low relabeling effort."}}
{"id": "2506.13303", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.13303", "abs": "https://arxiv.org/abs/2506.13303", "authors": ["Julian Frattini", "Anja Frattini"], "title": "Adopting Use Case Descriptions for Requirements Specification: an Industrial Case Study", "comment": null, "summary": "Context: Use case (UC) descriptions are a prominent format for specifying\nfunctional requirements. Existing literature abounds with recommendations on\nhow to write high-quality UC descriptions but lacks insights into (1) their\nreal-world adoption, (2) whether these recommendations correspond to actual\nquality, and (3) which factors influence the quality of UCs. Objectives: We aim\nto contribute empirical evidence about the adoption of UC descriptions in a\nlarge, globally distributed case company. Methods: We surveyed 1188 business\nrequirements of a case company that were elicited from 2020-01-01 until\n2024-12-31 and contained 1192 UCs in various forms. Among these, we manually\nevaluated the 273 template-style UC descriptions against established quality\nguidelines. We generated descriptive statistics of the format's adoption over\nthe surveyed time frame. Furthermore, we used inferential statistics to\ndetermine (a) how properties of the requirements engineering process affected\nthe UC quality and (b) how UC quality affects subsequent software development\nactivities. Results and Conclusions: Our descriptive results show how the\nadoption of UC descriptions in practice deviates from textbook recommendations.\nHowever, our inferential results suggest that only a few phenomena like\nsolution-orientation show an actual impact in practice. These results can steer\nUC quality research into a more relevant direction.", "AI": {"tldr": "This paper studied how use case descriptions are actually used in industry versus textbook recommendations, analyzing over a thousand requirements in a large company. It found that real-world practices differ notably from guidelines, and only a few factors (like solution-orientation) truly affect use case quality and software development. This points to a need for research to focus on what really matters in practice.", "motivation": "The paper aims to address gaps in the literature regarding the practical use of use case (UC) descriptions for specifying functional requirements. Specifically, it seeks to understand the real-world adoption of quality recommendations, whether these recommendations align with actual UC quality, and which factors influence UC quality in industry settings.", "method": "The authors conducted an empirical study within a large, globally distributed case company. They analyzed 1188 business requirements comprising 1192 UCs collected from 2020 to 2024. 273 template-style UC descriptions were manually assessed for quality based on established guidelines. The study used descriptive statistics to examine adoption patterns and inferential statistics to explore factors affecting UC quality and its impact on software development.", "result": "The findings show a notable deviation between the actual use of UC descriptions in the company and the recommendations found in literature. Among the various factors investigated, only a few\u2014such as solution-orientation\u2014were found to significantly impact UC quality or subsequent software development processes.", "conclusion": "The study concludes that real-world adoption of UC description guidelines differs from textbook advice, and most recommended factors do not significantly affect UC quality. This insight suggests a need to realign future UC quality research with practical realities and to focus on factors that genuinely affect quality in practice."}}
{"id": "2506.13538", "categories": ["cs.SE", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.13538", "abs": "https://arxiv.org/abs/2506.13538", "authors": ["Mohammed Mehedi Hasan", "Hao Li", "Emad Fallahzadeh", "Bram Adams", "Ahmed E. Hassan"], "title": "Model Context Protocol (MCP) at First Glance: Studying the Security and Maintainability of MCP Servers", "comment": null, "summary": "Although Foundation Models (FMs), such as GPT-4, are increasingly used in\ndomains like finance and software engineering, reliance on textual interfaces\nlimits these models' real-world interaction. To address this, FM providers\nintroduced tool calling-triggering a proliferation of frameworks with distinct\ntool interfaces. In late 2024, Anthropic introduced the Model Context Protocol\n(MCP) to standardize this tool ecosystem, which has become the de facto\nstandard with over eight million weekly SDK downloads. Despite its adoption,\nMCP's AI-driven, non-deterministic control flow introduces new risks to\nsustainability, security, and maintainability, warranting closer examination.\n  Towards this end, we present the first large-scale empirical study of MCP.\nUsing state-of-the-art health metrics and a hybrid analysis pipeline, combining\na general-purpose static analysis tool with an MCP-specific scanner, we\nevaluate 1,899 open-source MCP servers to assess their health, security, and\nmaintainability. Despite MCP servers demonstrating strong health metrics, we\nidentify eight distinct vulnerabilities-only three overlapping with traditional\nsoftware vulnerabilities. Additionally, 7.2% of servers contain general\nvulnerabilities and 5.5% exhibit MCP-specific tool poisoning. Regarding\nmaintainability, while 66% exhibit code smells, 14.4% contain ten bug patterns\noverlapping prior research. These findings highlight the need for MCP-specific\nvulnerability detection techniques while reaffirming the value of traditional\nanalysis and refactoring practices.", "AI": {"tldr": "The paper examines 1,899 open-source MCP servers and finds several security and maintainability issues unique to MCP's architecture, highlighting the necessity for new MCP-specific vulnerability detection, while also showing the ongoing importance of traditional software analysis techniques.", "motivation": "With the rise of Foundation Models (FMs) like GPT-4 across various industries, these models' real-world impact is limited by their reliance on text-based interfaces. The recent introduction of the Model Context Protocol (MCP) aims to standardize tool integration for FMs, but its AI-driven, non-deterministic design may present new risks that have not been systematically studied.", "method": "The authors conducted the first large-scale empirical analysis of MCP. They evaluated 1,899 open-source MCP servers using a hybrid analysis pipeline: a combination of a general-purpose static analysis tool and a specialized MCP-specific scanner. State-of-the-art health metrics were employed to assess the security, health, and maintainability of the servers.", "result": "The study found that while MCP servers generally display strong health metrics, there are unique risks: eight distinct types of vulnerabilities were identified, only three of which are common to traditional software systems. 7.2% of the servers have general vulnerabilities, 5.5% have MCP-specific tool poisoning, 66% suffer from code smells, and 14.4% contain at least ten previously-reported bug patterns.", "conclusion": "The non-deterministic nature of MCP introduces new and unique vulnerabilities not adequately addressed by current tools. There is a critical need for vulnerability detection techniques specific to MCP, although traditional analysis and refactoring continue to be valuable."}}
{"id": "2506.13663", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.13663", "abs": "https://arxiv.org/abs/2506.13663", "authors": ["Yunnong Chen", "Shixian Ding", "YingYing Zhang", "Wenkai Chen", "Jinzhou Du", "Lingyun Sun", "Liuqing Chen"], "title": "DesignCoder: Hierarchy-Aware and Self-Correcting UI Code Generation with Large Language Models", "comment": "11 pages,6 figures", "summary": "Multimodal large language models (MLLMs) have streamlined front-end interface\ndevelopment by automating code generation. However, these models also introduce\nchallenges in ensuring code quality. Existing approaches struggle to maintain\nboth visual consistency and functional completeness in the generated\ncomponents. Moreover, they lack mechanisms to assess the fidelity and\ncorrectness of the rendered pages. To address these issues, we propose\nDesignCoder, a novel hierarchical-aware and self-correcting automated code\ngeneration framework. Specifically, we introduce UI Grouping Chains, which\nenhance MLLMs' capability to understand and predict complex nested UI\nhierarchies. Subsequently, DesignCoder employs a hierarchical\ndivide-and-conquer approach to generate front-end code. Finally, we incorporate\na self-correction mechanism to improve the model's ability to identify and\nrectify errors in the generated code. Extensive evaluations on a dataset of UI\nmockups collected from both open-source communities and industry projects\ndemonstrate that DesignCoder outperforms state-of-the-art baselines in React\nNative, a widely adopted UI framework. Our method achieves a 37.63%, 9.52%,\n12.82% performance increase in visual similarity metrics (MSE, CLIP, SSIM) and\nsignificantly improves code structure similarity in terms of TreeBLEU,\nContainer Match, and Tree Edit Distance by 30.19%, 29.31%, 24.67%. Furthermore,\nwe conducted a user study with professional developers to assess the quality\nand practicality of the generated code. Results indicate that DesignCoder\naligns with industry best practices, demonstrating high usability, readability,\nand maintainability. Our approach provides an efficient and practical solution\nfor agile front-end development, enabling development teams to focus more on\ncore functionality and product innovation.", "AI": {"tldr": "DesignCoder introduces a hierarchical and self-correcting framework for automated front-end code generation, achieving substantial improvements in both visual and structural code quality over current methods, as verified by benchmarks and developer evaluations.", "motivation": "Multimodal large language models (MLLMs) have enabled automated code generation for front-end interfaces, streamlining development. However, these models often struggle to ensure visual consistency and functional completeness, and lack mechanisms to assess fidelity and correctness of the rendered components.", "method": "The authors propose DesignCoder, a hierarchical-aware and self-correcting code generation framework. Key innovations include UI Grouping Chains for nested UI understanding, a hierarchical divide-and-conquer strategy for code generation, and a self-correction mechanism to identify and fix errors in the generated code.", "result": "DesignCoder, evaluated on a diverse UI mockup dataset from both open-source and industry sources, outperforms state-of-the-art baselines in React Native. The method achieves significant performance increases in visual similarity metrics (MSE, CLIP, SSIM) and code structure similarity measures (TreeBLEU, Container Match, Tree Edit Distance). A user study with professional developers further confirms that the generated code is highly usable, readable, and maintainable.", "conclusion": "DesignCoder offers an efficient and practical solution for agile front-end development by improving code quality, maintainability, and alignment with industry best practices. It enables teams to focus on core functionalities rather than interface code quality concerns."}}
