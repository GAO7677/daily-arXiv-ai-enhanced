{"id": "2507.18509", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.18509", "abs": "https://arxiv.org/abs/2507.18509", "authors": ["Henning Urbat"], "title": "Higher-Order Behavioural Conformances via Fibrations", "comment": null, "summary": "Coinduction is a widely used technique for establishing behavioural\nequivalence of programs in higher-order languages. In recent years, the rise of\nlanguages with quantitative (e.g.~probabilistic) features has led to extensions\nof coinductive methods to more refined types of behavioural conformances, most\nnotably notions of behavioural distance. To guarantee soundness of coinductive\nreasoning, one needs to show that the behavioural conformance at hand forms a\nprogram congruence, i.e. it is suitably compatible with the operations of the\nlanguage. This is usually achieved by a complex proof technique known as\n\\emph{Howe's method}, which needs to be carefully adapted to both the specific\nlanguage and the targeted notion of behavioural conformance. We develop a\nuniform categorical approach to Howe's method that features two orthogonal\ndimensions of abstraction: (1) the underlying higher-order language is modelled\nby an \\emph{abstract higher-order specification} (AHOS), a novel and very\ngeneral categorical account of operational semantics, and (2) notions of\nbehavioural conformance (such as relations or metrics) are modelled via\nfibrations over the base category of an AHOS. Our main result is a fundamental\ncongruence theorem at this level of generality: Under natural conditions on the\ncategorical ingredients and the operational rules of a language modelled by an\nAHOS, the greatest behavioural (bi)conformance on its operational model forms a\ncongruence. We illustrate our theory by deriving congruence of bisimilarity and\nbehavioural pseudometrics for probabilistic higher-order languages.", "AI": {"tldr": "This paper presents a unified categorical framework for Howe's method, enabling generalized, sound proofs of congruence for behavioral equivalence and distances in higher-order (including probabilistic) languages.", "motivation": "Coinduction is crucial for proving behavioral equivalence in higher-order programming languages, especially as these languages gain quantitative features like probabilities. Existing proof techniques like Howe's method are complex and must be tailored to each specific language and behavioral conformance notion. The paper seeks a more uniform, general approach to simplify and standardize these proofs.", "method": "The authors introduce a categorical framework for Howe's method, based on two layers: (1) abstract higher-order specifications (AHOS) for modeling the operational semantics of languages, and (2) modeling behavioral conformances using fibrations over the base category of AHOS. This structure allows the approach to apply across various languages and types of behavioral conformance.", "result": "The paper establishes a fundamental congruence theorem: given natural assumptions about the categorical structures and operational semantics, the greatest behavioral conformance (such as bisimilarity or behavioral distance) in an AHOS-modeled operational semantics forms a congruence. The theory is demonstrated by showing congruence results for bisimilarity and behavioral pseudometrics in probabilistic higher-order languages.", "conclusion": "The proposed categorical approach generalizes and abstracts Howe's method, offering a sound, uniform process for proving congruence of behavioral conformances in a wide range of higher-order languages, including those with probabilistic features."}}
{"id": "2507.17930", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.17930", "abs": "https://arxiv.org/abs/2507.17930", "authors": ["Vahid Garousi", "Zafar Jafarov"], "title": "How Software Engineers Engage with AI: A Pragmatic Process Model and Decision Framework Grounded in Industry Observations", "comment": null, "summary": "Artificial Intelligence (AI) has the potential to transform Software\nEngineering (SE) by enhancing productivity, efficiency, and decision support.\nTools like GitHub Copilot and ChatGPT have given rise to \"vibe coding\"-an\nexploratory, prompt-driven development style. Yet, how software engineers\nengage with these tools in daily tasks, especially in deciding whether to\ntrust, refine, or reject AI-generated outputs, remains underexplored. This\npaper presents two complementary contributions. First, a pragmatic process\nmodel capturing real-world AI-assisted SE activities, including prompt design,\ninspection, fallback, and refinement. Second, a 2D decision framework that\ncould help developers reason about trade-offs between effort saved and output\nquality. Grounded in practitioner reports and direct observations in three\nindustry settings across Turkiye and Azerbaijan, our work illustrates how\nengineers navigate AI use with human oversight. These models offer structured,\nlightweight guidance to support more deliberate and effective use of AI tools\nin SE, contributing to ongoing discussions on practical human-AI collaboration.", "AI": {"tldr": "This paper explores how software engineers use AI tools like Copilot and ChatGPT in real-world settings, introducing a practical process model and a decision framework to help developers balance effort and quality when using AI-generated solutions.", "motivation": "The motivation of this paper is to understand how software engineers practically engage with AI tools like GitHub Copilot and ChatGPT, particularly regarding how they trust, refine, or reject AI-generated outputs during their daily software engineering tasks. This area has been underexplored despite the growing impact of AI on software development workflows.", "method": "The authors use a combination of practitioner reports and direct observations in three industry settings across Turkiye and Azerbaijan to gather real-world data on AI-assisted software engineering practices.", "result": "The results include a pragmatic process model that maps out typical AI-assisted software engineering activities, such as prompt design, inspection, fallback, and refinement. Additionally, the authors propose a 2D decision framework to help developers weigh the trade-offs between the effort saved by using AI tools and the quality of the resultant outputs.", "conclusion": "The paper concludes that their process model and decision framework provide structured and lightweight guidance, supporting more thoughtful and effective use of AI tools in software engineering. These contributions further the discussion on integrating human oversight with AI assistance in practice."}}
{"id": "2507.17991", "categories": ["cs.SE", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.17991", "abs": "https://arxiv.org/abs/2507.17991", "authors": ["Peter Eckmann", "Adrian Barnett", "Alexandra Bannach-Brown", "Elisa Pilar Bascunan Atria", "Guillaume Cabanac", "Louise Delwen Owen Franzen", "Ma\u0142gorzata Anna Gazda", "Kaitlyn Hair", "James Howison", "Halil Kilicoglu", "Cyril Labbe", "Sarah McCann", "Vladislav Nachev", "Martijn Roelandse", "Maia Salholz-Hillel", "Robert Schulz", "Gerben ter Riet", "Colby Vorland", "Anita Bandrowski", "Tracey Weissgerber"], "title": "Use as Directed? A Comparison of Software Tools Intended to Check Rigor and Transparency of Published Work", "comment": null, "summary": "The causes of the reproducibility crisis include lack of standardization and\ntransparency in scientific reporting. Checklists such as ARRIVE and CONSORT\nseek to improve transparency, but they are not always followed by authors and\npeer review often fails to identify missing items. To address these issues,\nthere are several automated tools that have been designed to check different\nrigor criteria. We have conducted a broad comparison of 11 automated tools\nacross 9 different rigor criteria from the ScreenIT group. We found some\ncriteria, including detecting open data, where the combination of tools showed\na clear winner, a tool which performed much better than other tools. In other\ncases, including detection of inclusion and exclusion criteria, the combination\nof tools exceeded the performance of any one tool. We also identified key areas\nwhere tool developers should focus their effort to make their tool maximally\nuseful. We conclude with a set of insights and recommendations for stakeholders\nin the development of rigor and transparency detection tools. The code and data\nfor the study is available at https://github.com/PeterEckmann1/tool-comparison.", "AI": {"tldr": "Automated tools can help check scientific rigor and transparency, but no single tool is best for all criteria. Combining tools can improve performance. The paper offers practical recommendations for improving these tools and shares its code and data for reproducibility.", "motivation": "There is a reproducibility crisis in science, partly caused by lack of standardization and transparency in reporting. Existing checklists to improve reporting are not reliably used or enforced, so automated tools have been developed to check rigor criteria.", "method": "The authors conducted a broad comparison of 11 different automated tools across 9 rigor criteria (from the ScreenIT group) to assess their effectiveness in detecting adherence to transparency and standardization guidelines.", "result": "Some rigor criteria (e.g., detecting open data) had a clear best-performing tool, while for others (e.g., detection of inclusion/exclusion criteria) combining tools gave better results than any single tool. The study also identified areas for improvement for developers of such tools.", "conclusion": "No single tool is sufficient to guarantee rigor and transparency in scientific reporting; combining tools is more effective for some criteria. Recommendations are provided for stakeholders and developers to improve automated detection tools. The study\u2019s resources are publicly available."}}
{"id": "2507.18029", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18029", "abs": "https://arxiv.org/abs/2507.18029", "authors": ["Xiang Echo Chen", "Wenhan Zhu", "Guoshuai Albert Shi", "Michael W. Godfrey"], "title": "An Empirical Study of GenAI Adoption in Open-Source Game Development: Tools, Tasks, and Developer Challenges", "comment": null, "summary": "The growing capabilities of generative AI (GenAI) have begun to reshape how\ngames are designed and developed, offering new tools for content creation,\ngameplay simulation, and design ideation. While prior research has explored\ntraditional uses of AI in games, such as controlling agents or generating\nprocedural content. There is limited empirical understanding of how GenAI is\nadopted by developers in real-world contexts, especially within the open-source\ncommunity. This study aims to explore how GenAI technologies are discussed,\nadopted, and integrated into open-source game development by analyzing issue\ndiscussions on GitHub. We investigate the tools, tasks, and challenges\nassociated with GenAI by comparing GenAI-related issues to those involving\ntraditional AI (TradAI) and NonAI topics. Our goal is to uncover how GenAI\ndiffers from other approaches in terms of usage patterns, developer concerns,\nand integration practices. To address this objective, we construct a dataset of\nopen-source game repositories that discuss AI-related topics. We apply open\ncard sorting and thematic analysis to a stratified sample of GitHub issues,\nlabelling each by type and content. These annotations enable comparative\nanalysis across GenAI, TradAI, and NonAI groups, and provide insight into how\nGenAI is shaping the workflows and pain points of open-source game developers.", "AI": {"tldr": "This paper investigates how open-source game developers discuss and integrate generative AI by analyzing GitHub issues, revealing unique patterns, challenges, and adoption trends for GenAI compared to traditional AI and other topics.", "motivation": "While generative AI (GenAI) has begun transforming game design and development, there is little empirical knowledge about how open-source developers actually use, discuss, and integrate GenAI compared to traditional AI in real-world projects. This paper seeks to fill that gap.", "method": "The authors analyze GitHub issue discussions related to AI in open-source game development. They construct a dataset of game repositories, then use open card sorting and thematic analysis to label and compare issues related to GenAI, traditional AI (TradAI), and topics not involving AI (NonAI).", "result": "The study characterizes the tools, tasks, and challenges unique to GenAI adoption and how they differ from TradAI and NonAI topics. It provides comparative insight into usage patterns, developer concerns, and GenAI integration practices in the open-source game community.", "conclusion": "GenAI is influencing the workflows and challenges faced by open-source game developers in distinctive ways. The findings highlight differences in the adoption and discussion of GenAI versus other AI and non-AI technologies, shedding light on its growing impact in collaborative software development."}}
{"id": "2507.18037", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.18037", "abs": "https://arxiv.org/abs/2507.18037", "authors": ["Sivana Hamer", "Jacob Bowen", "Md Nazmul Haque", "Chris Madden", "Laurie Williams"], "title": "Your ATs to Ts: MITRE ATT&CK Attack Technique to P-SSCRM Task Mapping", "comment": "Mapping generated from: arXiv:2503.12192", "summary": "The MITRE Adversarial Tactics, Techniques and Common Knowledge (MITRE ATT&CK)\nAttack Technique to Proactive Software Supply Chain Risk Management Framework\n(P-SSCRM) Task mapping described in this document helps software organizations\nto determine how different tasks mitigate the attack techniques of software\nsupply chain attacks. The mapping was created through four independent\nstrategies to find agreed-upon mappings. Because each P-SSCRM task is mapped to\none or more tasks from the 10 frameworks, the mapping we provide is also a\nmapping between MITRE ATT&CK and other prominent government and industry\nframeworks.", "AI": {"tldr": "This paper creates a mapping between P-SSCRM tasks and MITRE ATT&CK techniques (and other frameworks), enabling better, proactive management of software supply chain risks.", "motivation": "Software supply chain attacks are a serious and increasing threat. Organizations require comprehensive frameworks to proactively manage these risks. However, there is a need for clear guidance on how specific tasks align with recognized attack models and other frameworks.", "method": "The paper uses four independent strategies to create a mapping between MITRE ATT&CK attack techniques and P-SSCRM tasks. Each P-SSCRM task was also cross-referenced with tasks from 10 other prominent government and industry frameworks.", "result": "The outcome is a detailed mapping showing how specific P-SSCRM tasks mitigate MITRE ATT&CK software supply chain attack techniques. It also provides indirect mappings to other important frameworks, helping organizations understand overlaps and coverage.", "conclusion": "The mapping enables software organizations to better understand and select risk mitigation activities relevant to software supply chain attacks. It additionally bridges MITRE ATT&CK with other frameworks, improving harmonization and comprehensive defense strategies."}}
{"id": "2507.18039", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18039", "abs": "https://arxiv.org/abs/2507.18039", "authors": ["Ahmad D. Suleiman", "Yiming Tang", "Daqing Hou"], "title": "Factors Impacting Faculty Adoption of Project-Based Learning in Computing Education: a Survey", "comment": "Accepted at IEEE Frontiers in Education (FIE) 2025. This work has\n  been submitted to the IEEE for possible publication", "summary": "This research full paper investigates the factors influencing computing\neducators' adoption of project-based learning (PjBL) in software engineering\nand computing curricula. Recognized as a student-centered pedagogical approach,\nPjBL has the potential to enhance student motivation, engagement, critical\nthinking, collaboration, and problem-solving skills. Despite these benefits,\nfaculty adoption remains inconsistent due to challenges such as insufficient\ninstitutional support, time constraints, limited training opportunities,\ndesigning or sourcing projects, and aligning them with course objectives. This\nresearch explores these barriers and investigates the strategies and resources\nthat facilitate a successful adoption. Using a mixed-methods approach, data\nfrom 80 computing faculty were collected through an online survey comprising\nclosed-ended questions to quantify barriers, enablers, and resource needs,\nalong with an open-ended question to gather qualitative insights. Quantitative\ndata were analyzed using statistical methods, while qualitative responses\nunderwent thematic analysis. Results reveal that while PjBL is widely valued,\nits adoption is often selective and impacted by challenges in planning and\nmanaging the learning process, designing suitable projects, and a lack of\ninstitutional support, such as time, funding, and teaching assistants. Faculty\nare more likely to adopt or sustain PjBL when they have access to peer\ncollaboration, professional development, and institutional incentives. In\naddition, sourcing projects from research, industry partnerships, and borrowing\nfrom peers emerged as key facilitators for new projects. These findings\nunderscore the need for systemic support structures to empower faculty to\nexperiment with and scale PjBL practices.", "AI": {"tldr": "Despite its recognized benefits, project-based learning is not widely or consistently adopted by computing educators due to institutional barriers and resource constraints. Support such as collaboration, professional development, and incentives increases adoption. Systemic support structures are needed to enable broader and sustained use of PjBL.", "motivation": "Project-based learning (PjBL) is recognized for its benefits in enhancing student motivation, engagement, and critical skills in computing education, yet its adoption by faculty is inconsistent. The motivation of the paper is to systematically identify and understand the barriers and enablers to the adoption of PjBL among computing educators.", "method": "The study used a mixed-methods approach, collecting data from 80 computing faculty through an online survey with closed-ended questions to quantify barriers, enablers, and resource needs, and an open-ended question for qualitative insights. Quantitative data were analyzed statistically, and qualitative data underwent thematic analysis.", "result": "The results show that PjBL is valued but selectively adopted due to challenges like limited institutional support, time constraints, difficulties in project design, and securing resources. Successful adoption is associated with access to peer collaboration, professional development, and institutional incentives. Borrowing project ideas from research, industry, and colleagues also facilitates adoption.", "conclusion": "There are significant obstacles to widespread adoption of PjBL in computing education, mainly due to insufficient institutional resources and support. Systemic support structures are needed to help faculty experiment with and scale PjBL effectively."}}
{"id": "2507.18062", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18062", "abs": "https://arxiv.org/abs/2507.18062", "authors": ["Edward Abrokwah", "Taher A. Ghaleb"], "title": "An Empirical Study of Complexity, Heterogeneity, and Compliance of GitHub Actions Workflows", "comment": "Registered Report Accepted at the 41st IEEE International Conference\n  on Software Maintenance and Evolution 2025 (ICSME'25)", "summary": "Continuous Integration (CI) has evolved from a tooling strategy to a\nfundamental mindset in modern CI engineering. It enables teams to develop,\ntest, and deliver software rapidly and collaboratively. Among CI services,\nGitHub Actions (GHA) has emerged as a dominant service due to its deep\nintegration with GitHub and a vast ecosystem of reusable workflow actions.\nAlthough GHA provides official documentation and community-supported best\npractices, there appears to be limited empirical understanding of how\nopen-source real-world CI workflows align with such practices. Many workflows\nmight be unnecessarily complex and not aligned with the simplicity goals of CI\npractices. This study will investigate the structure, complexity,\nheterogeneity, and compliance of GHA workflows in open-source software\nrepositories. Using a large dataset of GHA workflows from Java, Python, and C++\nrepositories, our goal is to (a) identify workflow complexities, (b) analyze\nrecurring and heterogeneous structuring patterns, (c) assess compliance with\nGHA best practices, and (d) uncover differences in CI pipeline design across\nprogramming languages. Our findings are expected to reveal both areas of strong\nadherence to best practices and areas for improvement where needed. These\ninsights will also have implications for CI services, as they will highlight\nthe need for clearer guidelines and comprehensive examples in CI documentation.", "AI": {"tldr": "This paper analyzes a large set of GitHub Actions CI workflows in open-source Java, Python, and C++ repositories, uncovering common complexities, structure patterns, and alignment with best practices. Results indicate where workflows meet or miss established practices and propose clearer guidance and documentation to enhance future CI use.", "motivation": "While GitHub Actions (GHA) is a widely used CI service with documented best practices, there is limited empirical understanding of how real-world, open-source GHA CI workflows actually follow these practices. There is concern that many such workflows are unnecessarily complex and do not adhere to the intended simplicity of CI.", "method": "The study utilizes a large dataset of GHA workflows drawn from open-source Java, Python, and C++ repositories. It analyzes the structure, complexity, heterogeneity, and compliance of these workflows through empirical investigation. The research aims to identify complex patterns, recurring structures, and variations by programming language, as well as to assess alignment with documented best practices.", "result": "The study expects to find areas where open-source GHA workflows strongly adhere to best practices, as well as areas showing unnecessary complexity or poor alignment. Differences in workflow design across programming languages are also anticipated to be revealed.", "conclusion": "The analysis will provide insights for both researchers and CI tool developers, pointing out where CI documentation and guidelines can be improved to foster better adherence to best practices. The findings advocate for clearer guidelines, better documentation, and more comprehensive examples to assist CI users."}}
{"id": "2507.18081", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18081", "abs": "https://arxiv.org/abs/2507.18081", "authors": ["Carol Wong", "Mai Abe", "Silvia De Benedictis", "Marissa Halim", "Anthony Peruma"], "title": "Identifier Name Similarities: An Exploratory Study", "comment": "The 19th ACM/IEEE International Symposium on Empirical Software\n  Engineering and Measurement - Emerging Results and Vision Track", "summary": "Identifier names, which comprise a significant portion of the codebase, are\nthe cornerstone of effective program comprehension. However, research has shown\nthat poorly chosen names can significantly increase cognitive load and hinder\ncollaboration. Even names that appear readable in isolation may lead to\nmisunderstandings in contexts when they closely resemble other names in either\nstructure or functionality. In this exploratory study, we present our\npreliminary findings on the occurrence of identifier name similarity in\nsoftware projects through the development of a taxonomy that categorizes\ndifferent forms of identifier name similarity. We envision our initial taxonomy\nproviding researchers with a platform to analyze and evaluate the impact of\nidentifier name similarity on code comprehension, maintainability, and\ncollaboration among developers, while also allowing for further refinement and\nexpansion of the taxonomy.", "AI": {"tldr": "This paper develops a preliminary taxonomy for types of identifier name similarity in code, with the aim of improving studies of program comprehension and developer collaboration.", "motivation": "Poorly chosen identifier names in code increase cognitive load and hinder collaboration, even if the names seem readable in isolation. Similar-looking or similar-function names can cause misunderstandings.", "method": "The authors conducted an exploratory study, developing a preliminary taxonomy to categorize types of identifier name similarity in software projects.", "result": "They created an initial taxonomy that categorizes various forms of identifier name similarity.", "conclusion": "The taxonomy is intended as a platform for further research, helping to analyze the impact of name similarity on code comprehension, maintainability, and collaboration, and is open to refinement and expansion."}}
{"id": "2507.18105", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.18105", "abs": "https://arxiv.org/abs/2507.18105", "authors": ["Yujie Ma", "Lili Quan", "Xiaofei Xie", "Qiang Hu", "Jiongchi Yu", "Yao Zhang", "Sen Chen"], "title": "Understanding the Supply Chain and Risks of Large Language Model Applications", "comment": "26 pages", "summary": "The rise of Large Language Models (LLMs) has led to the widespread deployment\nof LLM-based systems across diverse domains. As these systems proliferate,\nunderstanding the risks associated with their complex supply chains is\nincreasingly important. LLM-based systems are not standalone as they rely on\ninterconnected supply chains involving pretrained models, third-party\nlibraries, datasets, and infrastructure. Yet, most risk assessments narrowly\nfocus on model or data level, overlooking broader supply chain vulnerabilities.\nWhile recent studies have begun to address LLM supply chain risks, there\nremains a lack of benchmarks for systematic research.\n  To address this gap, we introduce the first comprehensive dataset for\nanalyzing and benchmarking LLM supply chain security. We collect 3,859\nreal-world LLM applications and perform interdependency analysis, identifying\n109,211 models, 2,474 datasets, and 9,862 libraries. We extract model\nfine-tuning paths, dataset reuse, and library reliance, mapping the ecosystem's\nstructure. To evaluate security, we gather 1,555 risk-related issues-50 for\napplications, 325 for models, 18 for datasets, and 1,229 for libraries from\npublic vulnerability databases.\n  Using this dataset, we empirically analyze component dependencies and risks.\nOur findings reveal deeply nested dependencies in LLM applications and\nsignificant vulnerabilities across the supply chain, underscoring the need for\ncomprehensive security analysis. We conclude with practical recommendations to\nguide researchers and developers toward safer, more trustworthy LLM-enabled\nsystems.", "AI": {"tldr": "The paper introduces the first benchmark dataset mapping LLM supply chains, revealing deeply nested dependencies and widespread vulnerabilities. Their analysis highlights the urgent need for holistic supply chain security measures to ensure the safety of LLM-based applications.", "motivation": "Large Language Models (LLMs) are increasingly deployed in various domains, but their complex supply chains introduce significant security risks that are not well understood or systematically benchmarked.", "method": "The authors compiled the first comprehensive dataset of LLM supply chain components, collecting data from 3,859 LLM applications. They conducted interdependency analysis, mapped ecosystem structures, and gathered risk-related issues from public vulnerability databases to empirically assess dependencies and vulnerabilities.", "result": "They identified extensive and deeply nested dependencies among LLM applications, models, datasets, and libraries. Their analysis surfaced significant vulnerabilities throughout the LLM supply chain, with the majority found in third-party libraries.", "conclusion": "Comprehensive security analysis of LLM supply chains is urgently needed, rather than focusing solely on models or datasets. The paper offers a foundational dataset and practical recommendations to improve security and trust in LLM-based systems."}}
{"id": "2507.18130", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18130", "abs": "https://arxiv.org/abs/2507.18130", "authors": ["Le Deng", "Zhonghao Jiang", "Jialun Cao", "Michael Pradel", "Zhongxin Liu"], "title": "NoCode-bench: A Benchmark for Evaluating Natural Language-Driven Feature Addition", "comment": null, "summary": "Natural language-driven no-code development allows users to specify software\nfunctionality using natural language (NL) instead of editing source code,\npromising increased productivity and democratized development. Large language\nmodels (LLMs) show potential in enabling this paradigm. In this context,\nsoftware documentation acts as an NL specification for functionality. This work\nintroduces NoCode-bench, a benchmark designed to evaluate LLMs on real-world\nNL-driven feature addition tasks, consisting of 634 tasks across 10 projects\nand 114k code changes. Each task pairs documentation updates with corresponding\ncode implementations, validated by developer-written test cases. A subset of\n114 high-quality, human-verified instances, NoCode-bench Verified, ensures\nreliable evaluation. Our experiments reveal that, despite high token usage, the\nbest LLMs achieve a task success rate of only 15.79%, highlighting challenges\nin cross-file editing, codebase understanding, and tool calling. These findings\nindicate that LLMs are not yet ready for fully NL-driven no-code development.\nNoCode-bench lays the foundation for future advances in this area.", "AI": {"tldr": "The paper introduces NoCode-bench, a benchmark for evaluating LLMs on natural language-driven software development tasks. Results show that even the best LLMs succeed in only 15.79% of tasks, indicating that fully NL-driven no-code development is not yet feasible with current models.", "motivation": "Natural language-driven no-code development aims to enable users to develop software using natural language instead of code, thus improving productivity and accessibility. The paper investigates whether large language models (LLMs) can achieve this, identifying the need to assess current LLM capabilities on realistic NL-driven software development tasks.", "method": "The authors introduce NoCode-bench, a benchmark comprising 634 real-world feature addition tasks from 10 projects, where each task links software documentation changes (NL specifications) with code implementations validated by test cases. A high-quality, human-verified subset (NoCode-bench Verified) further supports reliable model evaluation. They use NoCode-bench to systematically test LLMs on these tasks and report their performance.", "result": "Top-performing LLMs achieve a task success rate of only 15.79%, revealing major limitations in their ability to perform feature addition via NL instructions. Notable challenges include understanding entire codebases, making edits across files, and invoking tools as needed.", "conclusion": "Current LLMs face significant obstacles in fully enabling NL-driven no-code software development, demonstrated by their low success rates on realistic tasks. NoCode-bench provides a foundation for measuring progress and identifying bottlenecks in this domain."}}
{"id": "2507.18159", "categories": ["cs.SE", "cs.DL"], "pdf": "https://arxiv.org/pdf/2507.18159", "abs": "https://arxiv.org/abs/2507.18159", "authors": ["Stephan Ferenz", "Aida Jafarbigloo", "Oliver Werth", "Astrid Nie\u00dfe"], "title": "SMECS: A Software Metadata Extraction and Curation Software", "comment": null, "summary": "Metadata play a crucial role in adopting the FAIR principles for research\nsoftware and enables findability and reusability. However, creating\nhigh-quality metadata can be resource-intensive for researchers and research\nsoftware engineers. To address this challenge, we developed the Software\nMetadata Extraction and Curation Software (SMECS) which integrates the\nextraction of metadata from existing sources together with a user-friendly\ninterface for metadata curation. SMECS extracts metadata from online\nrepositories such as GitHub and presents it to researchers through an\ninteractive interface for further curation and export as a CodeMeta file. The\nusability of SMECS was evaluated through usability experiments which confirmed\nthat SMECS provides a satisfactory user experience. SMECS supports the\nFAIRification of research software by simplifying metadata creation.", "AI": {"tldr": "The SMECS tool streamlines and simplifies the process of generating and refining metadata for research software, making it easier to support FAIR principles by automating extraction and offering an intuitive curation interface.", "motivation": "Creating high-quality metadata for research software is crucial for findability and reusability under the FAIR principles, but it is resource-intensive for researchers.", "method": "The authors developed SMECS, a software tool that automatically extracts metadata from sources like GitHub and provides an interactive, user-friendly interface for researchers to curate and export this metadata as CodeMeta files.", "result": "Usability experiments demonstrated that SMECS offers a satisfactory user experience for metadata creation and curation.", "conclusion": "SMECS simplifies and supports the creation of high-quality metadata, aiding the FAIRification of research software by making metadata generation easier and more accessible."}}
{"id": "2507.18223", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18223", "abs": "https://arxiv.org/abs/2507.18223", "authors": ["Nenad Petrovic", "Fengjunjie Pan", "Vahid Zolfaghari", "Krzysztof Lebioda", "Andre Schamschurko", "Alois Knoll"], "title": "GenAI for Automotive Software Development: From Requirements to Wheels", "comment": null, "summary": "This paper introduces a GenAI-empowered approach to automated development of\nautomotive software, with emphasis on autonomous and Advanced Driver Assistance\nSystems (ADAS) capabilities. The process starts with requirements as input,\nwhile the main generated outputs are test scenario code for simulation\nenvironment, together with implementation of desired ADAS capabilities\ntargeting hardware platform of the vehicle connected to testbench. Moreover, we\nintroduce additional steps for requirements consistency checking leveraging\nModel-Driven Engineering (MDE). In the proposed workflow, Large Language Models\n(LLMs) are used for model-based summarization of requirements (Ecore metamodel,\nXMI model instance and OCL constraint creation), test scenario generation,\nsimulation code (Python) and target platform code generation (C++).\nAdditionally, Retrieval Augmented Generation (RAG) is adopted to enhance test\nscenario generation from autonomous driving regulations-related documents. Our\napproach aims shorter compliance and re-engineering cycles, as well as reduced\ndevelopment and testing time when it comes to ADAS-related capabilities.", "AI": {"tldr": "The paper presents a GenAI-driven method that automates requirements-to-code for automotive ADAS systems, combining LLMs, MDE, and RAG to produce both testing and implementation code efficiently, aiming for faster, more reliable development cycles.", "motivation": "The paper is motivated by the need to streamline and automate the development of automotive software, especially for autonomous and Advanced Driver Assistance Systems (ADAS). Current processes are time-consuming, and achieving compliance and consistency with requirements is challenging.", "method": "The proposed method uses GenAI, particularly Large Language Models (LLMs), to automate the transformation of requirements into code for both test scenarios and ADAS capability implementations. It incorporates Model-Driven Engineering (MDE) for requirements consistency checking, and employs Retrieval Augmented Generation (RAG) to enhance test scenario generation using regulatory documents.", "result": "The approach can automatically generate simulation test scenarios, code for simulation (Python), and embedded target code (C++) for ADAS features. It also checks consistency in requirements and leverages external, regulation-based knowledge to improve test generation.", "conclusion": "This GenAI-enabled workflow improves automotive software development by reducing compliance cycles, re-engineering effort, and overall development/testing time for ADAS functionalities."}}
{"id": "2507.18267", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18267", "abs": "https://arxiv.org/abs/2507.18267", "authors": ["Zeqin Liao", "Zibin Zheng", "Peifan Reng", "Henglong Liang", "Zixu Gao", "Zhixiang Chen", "Wei Li", "Yuhong Nan"], "title": "An Empirical Study on Embodied Artificial Intelligence Robot (EAIR) Software Bugs", "comment": null, "summary": "Embodied Artificial Intelligence Robots (EAIR) is an emerging and rapidly\nevolving technological domain. Ensuring their program correctness is\nfundamental to their successful deployment. However, a general and in-depth\nunderstanding of EAIR system bugs remains lacking, which hinders the\ndevelopment of practices and techniques to tackle EAIR system bugs.\n  To bridge this gap, we conducted the first systematic study of 885 EAIR\nsystem bugs collected from 80 EAIR system projects to investigate their\nsymptoms, underlying causes, and module distribution. Our analysis takes\nconsiderable effort, which classifies these bugs into 18 underlying causes, 15\ndistinct symptoms, and identifies 13 affected modules. It reveals several new\ninteresting findings and implications which help shed light on future research\non tackling or repairing EAIR system bugs. First, among the 15 identified\nsymptoms, our findings highlight 8 symptoms specific to EAIR systems, which is\ncharacterized by severe functional failures and potential physical hazards.\nSecond, within the 18 underlying causes, we define 8 EAIR-specific causes, the\nmajority of which stem from the intricate issues of AI- agent reasoning and\ndecision making. Finally, to facilitate precise and efficient bug prediction,\ndetection, and repair, we constructed a mapping between underlying causes and\nthe modules in which they most frequently occur, which enables researchers to\nfocus diagnostic efforts on the modules most susceptible to specific bug types.", "AI": {"tldr": "The paper systematically studies bugs in Embodied AI Robots, identifies unique bug symptoms and causes, and maps them to system modules to improve bug management and guide future research.", "motivation": "Embodied Artificial Intelligence Robots (EAIR) are increasingly used, but understanding of their system bugs is lacking, impeding the development of effective techniques for their reliability.", "method": "The authors performed a systematic study by collecting and analyzing 885 EAIR system bugs from 80 projects, classifying the bugs by symptoms, underlying causes, and affected modules.", "result": "The study classified the bugs into 18 causes, 15 symptoms, and 13 modules. It identified 8 symptoms and 8 causes specific to EAIR systems, highlighting unique challenges such as functional failures and safety hazards. The research also created a mapping between bug causes and the modules most frequently affected.", "conclusion": "This work provides the first comprehensive understanding of EAIR bugs, outlines specific areas for improvement in bug prediction, detection, and repair, and helps direct future research and diagnostic efforts more efficiently."}}
{"id": "2507.18289", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.18289", "abs": "https://arxiv.org/abs/2507.18289", "authors": ["Yan Li", "Wenzhang Yang", "Yuekun Wang", "Jian Gao", "Shaohua Wang", "Yinxing Xue", "Lijun Zhang"], "title": "Scheduzz: Constraint-based Fuzz Driver Generation with Dual Scheduling", "comment": "15 pages, 12 figures, 5 tables", "summary": "Fuzzing a library requires experts to understand the library usage well and\ncraft high-quality fuzz drivers, which is tricky and tedious. Therefore, many\ntechniques have been proposed to automatically generate fuzz drivers. However,\nthey fail to generate rational fuzz drivers due to the lack of adherence to\nproper library usage conventions, such as ensuring a resource is closed after\nbeing opened. To make things worse, existing library fuzzing techniques\nunconditionally execute each driver, resulting in numerous irrational drivers\nthat waste computational resources while contributing little coverage and\ngenerating false positive bug reports.\n  To tackle these challenges, we propose a novel automatic library fuzzing\ntechnique, Scheduzz, an LLM-based library fuzzing technique. It leverages LLMs\nto understand rational usage of libraries and extract API combination\nconstraints. To optimize computational resource utilization, a dual scheduling\nframework is implemented to efficiently manage API combinations and fuzz\ndrivers. The framework models driver generation and the corresponding fuzzing\ncampaign as an online optimization problem. Within the scheduling loop,\nmultiple API combinations are selected to generate fuzz drivers, while\nsimultaneously, various optimized fuzz drivers are scheduled for execution or\nsuspension.\n  We implemented Scheduzz and evaluated it in 33 real-world libraries. Compared\nto baseline approaches, Scheduzz significantly reduces computational overhead\nand outperforms UTopia on 16 out of 21 libraries. It achieves 1.62x, 1.50x, and\n1.89x higher overall coverage than the state-of-the-art techniques CKGFuzzer,\nPromptfuzz, and the handcrafted project OSS-Fuzz, respectively. In addition,\nScheduzz discovered 33 previously unknown bugs in these well-tested libraries,\n3 of which have been assigned CVEs.", "AI": {"tldr": "Scheduzz is an LLM-based tool for automating library fuzzing. By understanding API usage, it generates smarter fuzz drivers and schedules tests efficiently, outperforming existing methods in coverage and bug detection while reducing wasted resources.", "motivation": "Fuzzing libraries typically requires manual and expert-driven crafting of fuzz drivers, which is labor-intensive and error-prone. Automated approaches exist but struggle to respect proper API usage patterns, leading to wasted computation and false positives.", "method": "Scheduzz, the proposed method, uses large language models (LLMs) to understand rational library usage and extract constraints on API combinations. It introduces a dual scheduling framework that treats driver generation and fuzzing campaigns as an online optimization problem, selectively generating and executing fuzz drivers for maximal efficiency.", "result": "In tests on 33 real-world libraries, Scheduzz achieved significantly higher coverage than existing techniques\u2014outperforming UTopia in most cases and showing substantial improvements (1.62x, 1.50x, 1.89x higher coverage) over CKGFuzzer, Promptfuzz, and OSS-Fuzz. It also identified 33 previously unknown bugs, 3 with assigned CVEs.", "conclusion": "Scheduzz advances automated library fuzzing by using LLMs to generate more rational fuzz drivers and efficiently manages computation, resulting in greater coverage and the discovery of more bugs."}}
{"id": "2507.18316", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18316", "abs": "https://arxiv.org/abs/2507.18316", "authors": ["Michael Konstantinou", "Renzo Degiovanni", "Jie M. Zhang", "Mark Harman", "Mike Papadakis"], "title": "YATE: The Role of Test Repair in LLM-Based Unit Test Generation", "comment": "12 pages, 4 figures", "summary": "Recent advances in automated test generation utilises language models to\nproduce unit tests. While effective, language models tend to generate many\nincorrect tests with respect to both syntax and semantics. Although such\nincorrect tests can be easily detected and discarded, they constitute a \"missed\nopportunity\" -- if fixed, they are often valuable as they directly add testing\nvalue (they effectively target the underlying program logic to be tested) and\nindirectly form good seeds for generating additional tests. To this end, we\npropose a simple technique for repairing some of these incorrect tests through\na combination of rule-based static analysis and re-prompting. We evaluate this\nsimple approach, named YATE, on a set of 6 open-source projects and show that\nit can effectively produce tests that cover on average 32.06% more lines and\nkill 21.77% more mutants than a plain LLM-based method. We also compare YATE\nwith four other LLM-based methods, namely HITS, SYMPROMPT, TESTSPARK and\nCOVERUP and show that it produces tests that cover substantially more code.\nYATE achieves 22% higher line coverage, 20% higher branch coverage and kill 20%\nmore mutants at a comparable cost (number of calls to LLMs).", "AI": {"tldr": "YATE fixes incorrect tests produced by language models, leading to much better test coverage and bug detection than existing LLM-based methods, without extra computational cost.", "motivation": "Language models are increasingly used for automated test generation in software development, but they often generate many syntactically and semantically incorrect tests. These discarded, incorrect tests present a missed opportunity since fixing them could directly improve testing coverage and generate better test seeds.", "method": "The authors propose YATE, a technique that repairs incorrect tests generated by language models using rule-based static analysis and targeted re-prompting. This approach aims to salvage valuable test logic from initially incorrect outputs.", "result": "YATE was evaluated on 6 open-source projects, achieving an average of 32.06% more line coverage and 21.77% more mutants killed compared to plain language model-based methods. YATE also outperformed four other LLM-based test generation techniques (HITS, SYMPROMPT, TESTSPARK, COVERUP), showing 22% higher line coverage, 20% higher branch coverage, and 20% more mutants killed, with a similar number of LLM invocations.", "conclusion": "Repairing incorrect LLM-generated tests using rule-based analysis and re-prompting can significantly boost the effectiveness of automated test generation, providing higher coverage and fault detection at a comparable computational cost."}}
{"id": "2507.18319", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18319", "abs": "https://arxiv.org/abs/2507.18319", "authors": ["Jesse Maarleveld", "Jiapan Guo", "Daniel Feitosa"], "title": "Gotta catch 'em all! Towards File Localisation from Issues at Large", "comment": "12 pages, 6 figures", "summary": "Bug localisation, the study of developing methods to localise the files\nrequiring changes to resolve bugs, has been researched for a long time to\ndevelop methods capable of saving developers' time. Recently, researchers are\nstarting to consider issues outside of bugs. Nevertheless, most existing\nresearch into file localisation from issues focusses on bugs or uses other\nselection methods to ensure only certain types of issues are considered as part\nof the focus of the work. Our goal is to work on all issues at large, without\nany specific selection.\n  In this work, we provide a data pipeline for the creation of issue file\nlocalisation datasets, capable of dealing with arbitrary branching and merging\npractices. We provide a baseline performance evaluation for the file\nlocalisation problem using traditional information retrieval approaches.\nFinally, we use statistical analysis to investigate the influence of biases\nknown in the bug localisation community on our dataset.\n  Our results show that methods designed using bug-specific heuristics perform\npoorly on general issue types, indicating a need for research into general\npurpose models. Furthermore, we find that there are small, but statistically\nsignificant differences in performance between different issue types. Finally,\nwe find that the presence of identifiers have a small effect on performance for\nmost issue types. Many results are project-dependent, encouraging the\ndevelopment of methods which can be tuned to project-specific characteristics.", "AI": {"tldr": "Bug-specific localisation methods don't work well for general issue localisation tasks. The authors propose a new dataset creation pipeline, show the importance of project-specific tuning, and call for research on general-purpose localisation models.", "motivation": "Existing research mainly focuses on bug localisation or selectively targets only specific issue types. There is a need to address file localisation for all issue types in a comprehensive manner.", "method": "The authors developed a data pipeline to create issue file localisation datasets that can handle arbitrary branching and merging. They evaluated baseline performance using traditional information retrieval techniques and conducted statistical analyses to study known biases from bug localisation in the context of broader issues.", "result": "Methods tailored to bugs do not generalise well to other issue types, highlighting the need for general-purpose models. There are small but statistically significant performance differences across issue types, and the impact of identifiers is generally minor. Project-specific factors significantly affect results, suggesting the need for adaptable methods.", "conclusion": "General-purpose localisers are needed instead of bug-specific approaches, and project adaptability is important due to varied project characteristics and issue types."}}
{"id": "2507.18339", "categories": ["cs.SE", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.18339", "abs": "https://arxiv.org/abs/2507.18339", "authors": ["Nils Bosbach", "Meik Schmidt", "Lukas J\u00fcnger", "Matthias Berthold", "Rainer Leupers"], "title": "FMI Meets SystemC: A Framework for Cross-Tool Virtual Prototyping", "comment": "PREPRINT - accepted by the 16th International Modelica and FMI\n  Conference 2025", "summary": "As systems become more complex, the demand for thorough testing and virtual\nprototyping grows. To simulate whole systems, multiple tools are usually needed\nto cover different parts. These parts include the hardware of a system and the\nenvironment with which the system interacts. The Functional Mock-up Interface\n(FMI) standard for co-simulation can be used to connect these tools.\n  The control part of modern systems is usually a computing unit, such as a\nSystem-on-a-Chip (SoC) or Microcontroller Unit (MCU), which executes software\nfrom a connected memory and interacts with peripherals. To develop software\nwithout requiring access to physical hardware, full-system simulators, the\nso-called Virtual Platforms (VPs), are commonly used. The IEEE-standardized\nframework for VP development is SystemC TLM. SystemC provides interfaces and\nconcepts that enable modular design and model exchange. However, SystemC lacks\nnative FMI support, which limits the integration into broader co-simulation\nenvironments.\n  This paper presents a novel framework to control and interact with\nSystemC-based VPs using the FMI. We present a case study showing how a\nsimulated temperature sensor in a SystemC simulation can obtain temperature\nvalues from an external tool via FMI. This approach allows the unmodified\ntarget software to run on the VP and receive realistic environmental input data\nsuch as temperature, velocity, or acceleration values from other tools. Thus,\nextensive software testing and verification is enabled. By having tests ready\nand the software pre-tested using a VP once the physical hardware is available,\ncertifications like ISO 26262 can be done earlier.", "AI": {"tldr": "The paper introduces a framework enabling SystemC-based virtual platforms to interact with external simulators via the FMI standard, allowing for realistic and integrated co-simulation. This facilitates thorough and early software testing even before physical hardware is available, streamlining development and certification workflows.", "motivation": "As system complexity increases, comprehensive testing and virtual prototyping become more important. Existing simulation tools are fragmented and do not natively integrate, especially SystemC-based virtual platforms (VPs), which lack native support for the Functional Mock-up Interface (FMI) standard. This hinders the ability to co-simulate hardware/software systems with realistic external inputs.", "method": "The authors propose a new framework that enables control and interaction between SystemC-based VPs and external simulation tools via the FMI standard. They demonstrate this with a case study where a SystemC-simulated temperature sensor receives real-time values from an external tool through FMI, without modifying the target software.", "result": "The framework successfully enables SystemC-based VPs to receive realistic environmental data like temperature or velocity from external simulation tools. This allows for more extensive and earlier software testing and verification, including compliance with standards like ISO 26262 before the actual hardware is available.", "conclusion": "Integrating SystemC-based virtual platforms with external simulators via FMI enhances virtual prototyping, enabling earlier and more realistic software testing. This approach can speed up development cycles and regulatory certification processes by allowing comprehensive pre-hardware software testing."}}
{"id": "2507.18476", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18476", "abs": "https://arxiv.org/abs/2507.18476", "authors": ["Busra Icoz", "Goksel Biricik"], "title": "Automated Code Review Using Large Language Models with Symbolic Reasoning", "comment": null, "summary": "Code review is one of the key processes in the software development lifecycle\nand is essential to maintain code quality. However, manual code review is\nsubjective and time consuming. Given its rule-based nature, code review is well\nsuited for automation. In recent years, significant efforts have been made to\nautomate this process with the help of artificial intelligence. Recent\ndevelopments in Large Language Models (LLMs) have also emerged as a promising\ntool in this area, but these models often lack the logical reasoning\ncapabilities needed to fully understand and evaluate code. To overcome this\nlimitation, this study proposes a hybrid approach that integrates symbolic\nreasoning techniques with LLMs to automate the code review process. We tested\nour approach using the CodexGlue dataset, comparing several models, including\nCodeT5, CodeBERT, and GraphCodeBERT, to assess the effectiveness of combining\nsymbolic reasoning and prompting techniques with LLMs. Our results show that\nthis approach improves the accuracy and efficiency of automated code review.", "AI": {"tldr": "The paper introduces a hybrid approach that combines symbolic reasoning and LLMs to automate code review, demonstrating improved accuracy and efficiency over existing models.", "motivation": "Manual code review is subjective and time consuming, and while code review is well suited for automation, current AI approaches, including LLMs, lack sufficient logical reasoning to fully automate the process.", "method": "The authors propose a hybrid approach that integrates symbolic reasoning techniques with Large Language Models (LLMs) to automate code review. They test this approach using the CodexGlue dataset and compare several models (CodeT5, CodeBERT, and GraphCodeBERT) to evaluate the effectiveness of the proposed integration.", "result": "The results indicate that combining symbolic reasoning and prompting techniques with LLMs improves both the accuracy and efficiency of automated code review.", "conclusion": "Integrating symbolic reasoning with LLMs is an effective way to enhance automated code review, surpassing the capabilities of LLMs alone in terms of accuracy and efficiency."}}
{"id": "2507.18515", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18515", "abs": "https://arxiv.org/abs/2507.18515", "authors": ["Zezhou Yang", "Ting Peng", "Cuiyun Gao", "Chaozheng Wang", "Hailiang Huang", "Yuetang Deng"], "title": "A Deep Dive into Retrieval-Augmented Generation for Code Completion: Experience on WeChat", "comment": "Accepted in ICSME 25 Industry Track", "summary": "Code completion, a crucial task in software engineering that enhances\ndeveloper productivity, has seen substantial improvements with the rapid\nadvancement of large language models (LLMs). In recent years,\nretrieval-augmented generation (RAG) has emerged as a promising method to\nenhance the code completion capabilities of LLMs, which leverages relevant\ncontext from codebases without requiring model retraining. While existing\nstudies have demonstrated the effectiveness of RAG on public repositories and\nbenchmarks, the potential distribution shift between open-source and\nclosed-source codebases presents unique challenges that remain unexplored. To\nmitigate the gap, we conduct an empirical study to investigate the performance\nof widely-used RAG methods for code completion in the industrial-scale codebase\nof WeChat, one of the largest proprietary software systems. Specifically, we\nextensively explore two main types of RAG methods, namely identifier-based RAG\nand similarity-based RAG, across 26 open-source LLMs ranging from 0.5B to 671B\nparameters. For a more comprehensive analysis, we employ different retrieval\ntechniques for similarity-based RAG, including lexical and semantic retrieval.\nBased on 1,669 internal repositories, we achieve several key findings: (1) both\nRAG methods demonstrate effectiveness in closed-source repositories, with\nsimilarity-based RAG showing superior performance, (2) the effectiveness of\nsimilarity-based RAG improves with more advanced retrieval techniques, where\nBM25 (lexical retrieval) and GTE-Qwen (semantic retrieval) achieve superior\nperformance, and (3) the combination of lexical and semantic retrieval\ntechniques yields optimal results, demonstrating complementary strengths.\nFurthermore, we conduct a developer survey to validate the practical utility of\nRAG methods in real-world development environments.", "AI": {"tldr": "The paper finds that advanced RAG methods (combining lexical and semantic search) significantly improve code completion in large, closed-source codebases, and are validated as useful by industry developers.", "motivation": "The motivation of this paper is to address the unexplored challenges of applying retrieval-augmented generation (RAG) methods for code completion within industrial-scale, closed-source codebases, where potential distribution shifts from open-source environments may limit the effectiveness of existing techniques.", "method": "The authors conducted an empirical study of widely-used RAG methods, specifically identifier-based RAG and similarity-based RAG, on the large proprietary codebase of WeChat. They evaluated 26 open-source LLMs (ranging from 0.5B to 671B parameters) and tested different retrieval techniques for similarity-based RAG, including lexical (BM25) and semantic (GTE-Qwen) retrieval, as well as their combination. They also conducted a developer survey to assess practical usefulness.", "result": "Both RAG methods are effective in closed-source repositories, with similarity-based RAG outperforming identifier-based RAG. Advanced retrieval techniques improve the effectiveness of similarity-based RAG, especially BM25 for lexical and GTE-Qwen for semantic retrieval. The combination of lexical and semantic retrieval produces the best results owing to their complementary advantages. Developer feedback also supports the usefulness of RAG methods in practice.", "conclusion": "Retrieval-augmented generation (RAG) methods, especially when combining advanced lexical and semantic retrieval techniques, significantly enhance code completion performance for large-scale, closed-source codebases like WeChat. These methods are validated as practical tools for real-world developers, bridging the gap between open-source research and proprietary software engineering needs."}}
