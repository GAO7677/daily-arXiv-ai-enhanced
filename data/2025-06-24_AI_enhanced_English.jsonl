{"id": "2506.17306", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17306", "abs": "https://arxiv.org/abs/2506.17306", "authors": ["Jake Zappin", "Trevor Stalnaker", "Oscar Chaparro", "Denys Poshyvanyk"], "title": "Challenges and Practices in Quantum Software Testing and Debugging: Insights from Practitioners", "comment": null, "summary": "Quantum software engineering is an emerging discipline with distinct\nchallenges, particularly in testing and debugging. As quantum computing\ntransitions from theory to implementation, developers face issues not present\nin classical software development, such as probabilistic execution, limited\nobservability, shallow abstractions, and low awareness of quantum-specific\ntools. To better understand current practices, we surveyed 26 quantum software\ndevelopers from academia and industry and conducted follow-up interviews\nfocused on testing, debugging, and recurring challenges. All participants\nreported engaging in testing, with unit testing (88%), regression testing\n(54%), and acceptance testing (54%) being the most common. However, only 31%\nreported using quantum-specific testing tools, relying instead on manual\nmethods. Debugging practices were similarly grounded in classical strategies,\nsuch as print statements, circuit visualizations, and simulators, which\nrespondents noted do not scale well. The most frequently cited sources of bugs\nwere classical in nature-library updates (81%), developer mistakes (68%), and\ncompatibility issues (62%)-often worsened by limited abstraction in existing\nSDKs. These findings highlight the urgent need for better-aligned testing and\ndebugging tools, integrated more seamlessly into the workflows of quantum\ndevelopers. We present these results in detail and offer actionable\nrecommendations grounded in the real-world needs of practitioners.", "AI": {"tldr": "Quantum software developers mostly use classical, manual testing and debugging practices due to a lack of quantum-specific tools. Major bugs are classical, stemming from libraries, coding mistakes, and poor SDK abstraction. The field urgently needs tailored tools and workflows for quantum development, and this paper suggests detailed, actionable improvements.", "motivation": "The motivation of the paper is to address the unique challenges faced in quantum software engineering, particularly in testing and debugging, as the field transitions from theory to practical implementation. Issues such as probabilistic execution, limited observability, and a lack of quantum-specific tools present significant obstacles that have not been systematically studied from the perspective of current developers' practices.", "method": "The authors conducted a survey of 26 quantum software developers from both academia and industry, followed by in-depth interviews, focusing on testing, debugging, and recurring challenges in quantum software development.", "result": "The study found that although all participants engage in software testing, the majority rely on manual or classical methods, such as unit, regression, and acceptance testing, rather than quantum-specific tools. Only 31% reported using quantum-specific testing tools. Debugging also predominantly used traditional methods that do not scale well for quantum systems. The main sources of bugs in quantum software were classical in nature, such as library updates, developer mistakes, and compatibility issues, which are compounded by limited abstraction in current SDKs.", "conclusion": "There is an urgent need for better-aligned and more seamlessly integrated quantum software testing and debugging tools to support developers. The paper provides detailed findings from the study and offers actionable recommendations to address the real-world needs of quantum software practitioners."}}
{"id": "2506.17313", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17313", "abs": "https://arxiv.org/abs/2506.17313", "authors": ["Jonathan Reif", "Daniel Dittler", "Milapji Singh Gill", "Tam\u00e1s Farkas", "Valentin Stegmaier", "Felix Gehlhoff", "Tobias Kleinert", "Michael Weyrich"], "title": "An Expert Survey on Models and Digital Twins", "comment": "This article is accepted at CIRP ICME and for publication in Procedia\n  CIRP", "summary": "Digital Twins (DTs) are becoming increasingly vital for future industrial\napplications, enhancing monitoring, control, and optimization of physical\nassets. This enhancement is made possible by integrating various Digital Models\n(DMs) within DTs, which must interoperate to represent different system aspects\nand fulfill diverse application purposes. However, industry perspectives on the\nchallenges and research needs for integrating these models are rarely obtained.\nThus, this study conducts an expert survey across multiple application domains\nto identify and analyze the challenges in utilizing diverse DMs within DTs. The\nresults reveal missing standardized interfaces, high manual adaptation effort,\nand limited support for model reuse across lifecycle phases, highlighting\nfuture research needs in automated model composition and semantics-based\ninteroperability.", "AI": {"tldr": "An expert survey across industries revealed major challenges in integrating digital models within Digital Twins: lack of standard interfaces, high manual effort, and limited model reuse. The study highlights the need for automation and better interoperability methods.", "motivation": "Digital Twins are increasingly used in industry for monitoring, control, and optimization, but integrating various digital models within DTs poses practical challenges. Industry views on these integration issues and related research needs are not well studied.", "method": "The study conducted an expert survey across multiple industrial application domains to collect insights about the challenges of utilizing diverse digital models within digital twins.", "result": "Key challenges identified include the lack of standardized interfaces, high manual effort required for model adaptation, and insufficient support for model reuse throughout different lifecycle phases. The study also identifies research needs in areas such as automated model composition and improved semantics-based interoperability.", "conclusion": "Standardization gaps, manual integration burdens, and lifecycle reuse limitations hinder effective digital model integration in Digital Twins. Advancements in automation and semantics-driven interoperability are necessary to address these challenges."}}
{"id": "2506.17330", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17330", "abs": "https://arxiv.org/abs/2506.17330", "authors": ["Simon Thorne"], "title": "Large Language Models for Spreadsheets: Benchmarking Progress and Evaluating Performance with FLARE", "comment": "18 Pages, 10 Tables, 1 Colour Figure", "summary": "Large Language Models (LLMs) have demonstrated some significant capabilities\nacross various domains; however, their effectiveness in spreadsheet related\ntasks remains underexplored. This study introduces a foundation for a\ncomprehensive benchmark framework to evaluate the performance of leading LLMs\nin executing spreadsheet functions, formula generation and data manipulation\ntasks. The benchmark encompasses tasks ranging from basic formula creation to\ncomplex, real world spreadsheet scenarios. Our findings reveal that while LLMs\nexhibit proficiency in straightforward tasks, they often falter in complex,\nmulti step operations, frequently producing plausible yet incorrect outputs.\nThese results underscore the limitations of current LLMs in handling\nspreadsheet tasks that require precise logical reasoning and highlight the need\nfor integrating symbolic reasoning capabilities into LLM architectures. To\nsupport this, we introduce FLARE (Formula Logic, Auditing, Reasoning and\nEvaluation) a new benchmark for evaluating LLM performance on real-world\nspreadsheet logic, auditing, and reasoning tasks.", "AI": {"tldr": "LLMs do well with basic spreadsheet tasks but struggle with complex ones. The new FLARE benchmark shows these weaknesses and emphasizes the need for better reasoning abilities in LLMs for spreadsheet-related tasks.", "motivation": "Although LLMs show strong performance in many domains, their capabilities with spreadsheet-specific tasks like formula generation and data manipulation are not well understood. There is a need to assess and improve how LLMs handle such tasks, especially complex ones requiring logical reasoning.", "method": "The authors proposed and developed a comprehensive benchmark framework, called FLARE (Formula Logic, Auditing, Reasoning and Evaluation). This framework evaluates state-of-the-art LLMs on a wide range of spreadsheet tasks, from basic to complex, real-world scenarios.", "result": "The study found that while LLMs perform well on simple spreadsheet tasks, they often make mistakes in complex, multi-step tasks, sometimes producing answers that sound plausible but are actually incorrect. This suggests current LLMs lack the necessary logical reasoning for complicated spreadsheet work.", "conclusion": "Current LLMs are limited in their ability to handle advanced spreadsheet tasks. There is a clear need to integrate symbolic reasoning capabilities into LLMs to improve accuracy and reliability in such scenarios. The FLARE benchmark provides a foundation for further research and evaluation in this area."}}
{"id": "2506.17335", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17335", "abs": "https://arxiv.org/abs/2506.17335", "authors": ["Shuo Yan", "Ruochen Li", "Ziming Luo", "Zimu Wang", "Daoyang Li", "Liqiang Jing", "Kaiyu He", "Peilin Wu", "George Michalopoulos", "Yue Zhang", "Ziyang Zhang", "Mian Zhang", "Zhiyu Chen", "Xinya Du"], "title": "LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research", "comment": null, "summary": "Large language model (LLM) agents have demonstrated remarkable potential in\nadvancing scientific discovery. However, their capability in the fundamental\nyet crucial task of reproducing code from research papers, especially in the\nNLP domain, remains underexplored. This task includes unique complex reasoning\nchallenges in the intellectual synthesis of abstract concepts and the\ncomprehension of code repositories with interdependent files. Motivated by this\ngap, we present LMR-BENCH, a benchmark designed to systematically evaluate the\ncapability of LLM agents on code reproduction from Language Modeling Research.\nIt consists of 28 code reproduction tasks derived from 23 research papers\npublished in top-tier NLP venues over the past five years, spanning nine\nfundamental categories. Models are provided with a research paper, a code\nrepository containing one or more masked functions, and instructions for\nimplementing these functions. We conduct extensive experiments in standard\nprompting and LLM agent settings with state-of-the-art LLMs, evaluating the\naccuracy of unit tests and performing LLM-based evaluation of code correctness.\nExperimental results reveal that even the most advanced models still exhibit\npersistent limitations in scientific reasoning and code synthesis, highlighting\ncritical gaps in LLM agents' ability to autonomously reproduce scientific\nresearch", "AI": {"tldr": "A new benchmark (LMR-BENCH) reveals that state-of-the-art LLMs struggle significantly with reproducing code from top NLP research papers, exposing persistent shortcomings in reasoning and code synthesis necessary for scientific research reproducibility.", "motivation": "While large language models (LLMs) have shown significant promise in advancing scientific discovery, their ability to reproduce code from research papers, particularly in NLP, has not been thoroughly examined. Reproducing such code involves complex reasoning and understanding of both abstract concepts and interdependent code structures. Addressing this gap is important for verifying and building upon scientific work.", "method": "The authors introduce LMR-BENCH, a benchmark containing 28 code reproduction tasks based on 23 recent NLP research papers. The benchmark assesses how well LLM agents can reproduce masked functions from provided code repositories and paper instructions. The evaluation uses standard prompting and agent-based approaches with state-of-the-art LLMs, measuring performance via unit test accuracy and LLM-based code correctness checks.", "result": "Experimental results indicate that current leading LLMs face significant challenges in scientific reasoning and accurate code synthesis when tasked with reproducing research code. These models still have notable deficiencies, as revealed by persistent errors even under optimal prompting and agent strategies.", "conclusion": "Despite their progress, LLM agents are still limited in their ability to autonomously reproduce scientific research code, especially within the NLP domain. The findings underscore a gap between the current capabilities of LLMs and the nuanced, complex requirements of research code reproduction."}}
{"id": "2506.17369", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17369", "abs": "https://arxiv.org/abs/2506.17369", "authors": ["Zhiyuan Pan", "Xing Hu", "Xin Xia", "Xiaohu Yang"], "title": "Re-Evaluating Code LLM Benchmarks Under Semantic Mutation", "comment": null, "summary": "In the era of large language models (LLMs), code benchmarks have become an\nimportant research area in software engineering and are widely used by\npractitioners. These benchmarks evaluate the performance of LLMs on specific\ncode-related tasks, such as code understanding and generation. A critical step\nin constructing code benchmarks is the design of prompts. However, as existing\ncode benchmarks typically rely on a single prompt template per task, they are\nprone to the issue of prompt sensitivity, where minor prompt variations could\nresult in substantial performance variations, leading to unreliable evaluations\nof model capabilities.\n  While previous studies have explored prompt sensitivity, their experimental\ndesigns and findings are limited to traditional natural language processing\n(NLP) tasks. In this paper, we present an empirical study to investigate prompt\nsensitivity in code benchmarks. We first propose a general framework that\nmodifies prompt templates in a manner that preserves both their semantics and\ntheir structure as much as possible. Based on the framework, we conduct\nextensive experiments across eight code benchmark tasks on 10 representative\nopen-source LLMs, with each task featuring 100 semantically similar prompt\ntemplates. We then analyze the evaluation results using various statistical\nmetrics, focusing on both absolute and relative model performance. Our findings\nsuggest that even slight prompt variations can lead to significant shifts in\nperformance. Additionally, we observe that such variations can introduce\ninconsistencies in the performance rankings across different models. These\ninsights highlight the need for considering prompt sensitivity when designing\nfuture code benchmarks, to ensure more reliable and accurate evaluation of LLM\ncapabilities.", "AI": {"tldr": "Changes to prompt templates, even if subtle, can greatly affect how LLMs perform on code-related tasks. This makes current code benchmarks unreliable, and future designs must take prompt sensitivity into account for trustworthy evaluations.", "motivation": "Existing code benchmarks often use only one prompt per task, making evaluations sensitive to minor prompt changes, which results in unreliable model comparisons. Previous research on prompt sensitivity has mostly focused on standard NLP tasks, not code-related tasks.", "method": "The authors propose a general framework to systematically modify code benchmark prompt templates while preserving their meaning and structure. They conduct experiments using this framework on eight code tasks with 10 open-source LLMs, each tested with 100 semantically similar prompts. They use various statistical metrics for evaluation.", "result": "Slight prompt modifications can cause significant changes in LLM performance and can even alter the relative ranking among models. This demonstrates high prompt sensitivity for code benchmarks.", "conclusion": "Prompt sensitivity is a significant issue in code benchmarks. More robust benchmark design is needed to ensure reliable and accurate LLM assessment."}}
{"id": "2506.17539", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17539", "abs": "https://arxiv.org/abs/2506.17539", "authors": ["Sidong Feng", "Changhao Du", "Huaxiao Liu", "Qingnan Wang", "Zhengwei Lv", "Mengfei Wang", "Chunyang Chen"], "title": "Breaking Single-Tester Limits: Multi-Agent LLMs for Multi-User Feature Testing", "comment": "Accepted to International Conference on Software Engineering (ICSE\n  2026)", "summary": "The growing dependence on mobile phones and their apps has made multi-user\ninteractive features, like chat calls, live streaming, and video conferencing,\nindispensable for bridging the gaps in social connectivity caused by physical\nand situational barriers. However, automating these interactive features for\ntesting is fraught with challenges, owing to their inherent need for timely,\ndynamic, and collaborative user interactions, which current automated testing\nmethods inadequately address. Inspired by the concept of agents designed to\nautonomously and collaboratively tackle problems, we propose MAdroid, a novel\nmulti-agent approach powered by the Large Language Models (LLMs) to automate\nthe multi-user interactive task for app feature testing. Specifically, MAdroid\nemploys two functional types of multi-agents: user agents (Operator) and\nsupervisor agents (Coordinator and Observer). Each agent takes a specific role:\nthe Coordinator directs the interactive task; the Operator mimics user\ninteractions on the device; and the Observer monitors and reviews the task\nautomation process. Our evaluation, which included 41 multi-user interactive\ntasks, demonstrates the effectiveness of our approach, achieving 82.9% of the\ntasks with 96.8% action similarity, outperforming the ablation studies and\nstate-of-the-art baselines. Additionally, a preliminary investigation\nunderscores MAdroid's practicality by helping identify 11 multi-user\ninteractive bugs during regression app testing, confirming its potential value\nin real-world software development contexts.", "AI": {"tldr": "MAdroid, a multi-agent testing framework powered by LLMs, effectively automates multi-user interactive feature testing for mobile apps, demonstrating high success rates, action similarity, and bug finding ability beyond current tools.", "motivation": "Modern mobile apps rely heavily on multi-user interactive features (e.g., chats, streaming, conferencing), which are challenging to test automatically due to their need for coordinated, dynamic, and collaborative user actions. Existing automated testing tools do not adequately handle these requirements.", "method": "The proposed method, MAdroid, leverages a multi-agent system run by Large Language Models (LLMs). It utilizes user agents (Operators) to simulate user actions, and supervisor agents (Coordinators and Observers) to manage and review the test processes. Each agent performs a specific role in orchestrating and simulating multi-user interactions for thorough app feature testing.", "result": "MAdroid was evaluated over 41 multi-user interactive tasks and was able to accomplish 82.9% of them with 96.8% similarity to real user actions, surpassing prior work and ablation study baselines. It also identified 11 bugs during regression testing in real app scenarios.", "conclusion": "MAdroid demonstrates significant promise in automating multi-user interactive feature testing for mobile apps, improving coverage, and helping uncover bugs that are hard to detect with traditional automation methods. The multi-agent, LLM-powered approach is effective and practical for real-world app development."}}
{"id": "2506.17627", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17627", "abs": "https://arxiv.org/abs/2506.17627", "authors": ["Hongzhou Rao", "Yanjie Zhao", "Wenjie Zhu", "Ling Xiao", "Meizhen Wang", "Haoyu Wang"], "title": "CodeMorph: Mitigating Data Leakage in Large Language Model Assessment", "comment": "Accepted by ICSE 2025 (Industry Challenge Track)", "summary": "Concerns about benchmark leakage in large language models for code (Code\nLLMs) have raised issues of data contamination and inflated evaluation metrics.\nThe diversity and inaccessibility of many training datasets make it difficult\nto prevent data leakage entirely, even with time lag strategies. Consequently,\ngenerating new datasets through code perturbation has become essential.\nHowever, existing methods often fail to produce complex and diverse variations,\nstruggle with complex cross-file dependencies, and lack support for multiple\nprogramming languages, which limits their effectiveness in enhancing LLM\nevaluations for coding tasks. To fill this gap, we propose CodeMorph, an\napproach designed to support multiple programming languages while preserving\ncross-file dependencies to mitigate data leakage. CodeMorph consists of two\nmain components that work together to enhance the perturbation process. The\nfirst component employs 26 semantic-preserving transformation methods to\niteratively perturb code, generating diverse variations while ensuring that the\nmodified code remains compilable. The second component introduces a genetic\nalgorithm-based selection algorithm, PESO, to identify the more effective\nperturbation method for each iteration by targeting lower similarity scores\nbetween the perturbed and original code, thereby enhancing overall perturbation\neffectiveness. Experimental results demonstrate that after applying CodeMorph,\nthe accuracy of the LLM on code completion tasks across five programming\nlanguages decreased by an average of 24.67%, with Python showing the most\nsignificant reduction at 45%. The similarity score of code optimized by PESO\nis, on average, 7.01% lower than that of randomly perturbed code, peaking at a\nreduction of 42.86%.", "AI": {"tldr": "CodeMorph is a new method for generating harder, contamination-resistant code evaluation datasets for LLMs. It uses advanced perturbations and a genetic algorithm to lower code similarity, supporting multiple languages and real program structures, leading to more realistic LLM evaluation.", "motivation": "There are growing concerns about benchmark leakage in code-focused large language models (Code LLMs) due to accidental overlap between training and evaluation data, leading to inflated performance metrics. Current approaches to avoiding data contamination are not foolproof due to inaccessible and diverse datasets.", "method": "The authors introduce CodeMorph, an approach that generates new code datasets via semantic-preserving code perturbations. It has two main components: (1) a set of 26 iterative transformation methods to create diverse, compilable code variations, and (2) PESO, a genetic algorithm-based selection algorithm that optimizes for lower similarity between original and perturbed code. CodeMorph supports multiple languages and maintains cross-file dependencies.", "result": "Applying CodeMorph to code completion benchmarks for five programming languages led to an average 24.67% drop in LLM accuracy, with up to 45% for Python. The code perturbed using PESO had, on average, a 7.01% lower similarity score compared to random perturbations, with a maximum reduction of 42.86%.", "conclusion": "CodeMorph effectively mitigates data leakage concerns by generating diverse, challenging benchmark datasets for code LLMs, thus providing more accurate evaluation. It supports multiple languages and maintains code dependencies, outperforming existing perturbation methods."}}
{"id": "2506.17638", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17638", "abs": "https://arxiv.org/abs/2506.17638", "authors": ["Yanzhou Mu", "Rong Wang", "Juan Zhai", "Chunrong Fang", "Xiang Chen", "Zhiyuan Peng", "Peiran Yang", "Ruixiang Qian", "Shaoyu Yang", "Zhenyu Chen"], "title": "Deep Learning Framework Testing via Model Mutation: How Far Are We?", "comment": "27 pages, 9 figures", "summary": "Deep Learning (DL) frameworks are a fundamental component of DL development.\nTherefore, the detection of DL framework defects is important and challenging.\nAs one of the most widely adopted DL testing techniques, model mutation has\nrecently gained significant attention. In this study, we revisit the defect\ndetection ability of existing mutation-based testing methods and investigate\nthe factors that influence their effectiveness. To begin with, we reviewed\nexisting methods and observed that many of them mutate DL models (e.g.,\nchanging their parameters) without any customization, ignoring the unique\nchallenges in framework testing. Another issue with these methods is their\nlimited effectiveness, characterized by a high rate of false positives caused\nby illegal mutations arising from the use of generic, non-customized mutation\noperators. Moreover, we tracked the defects identified by these methods and\ndiscovered that most of them were ignored by developers. Motivated by these\nobservations, we investigate the effectiveness of existing mutation-based\ntesting methods in detecting important defects that have been authenticated by\nframework developers. We begin by collecting defect reports from three popular\nframeworks and classifying them based on framework developers' ratings to build\na comprehensive dataset. We then perform an in-depth analysis to uncover\nvaluable insights. Based on our findings, we propose optimization strategies to\naddress the shortcomings of existing approaches. Following these optimizations,\nwe identified seven new defects, four of which were confirmed by developers as\nhigh-priority issues, with three resolved. In summary, we identified 39 unique\ndefects across just 23 models, of which 31 were confirmed by developers, and\neight have been fixed.", "AI": {"tldr": "Current mutation-based testing for deep learning frameworks is hampered by ineffectiveness and high false positives. This study analyzes and optimizes these methods, resulting in improved detection and resolution of real framework defects.", "motivation": "Deep Learning frameworks are essential for DL development, but detecting their defects remains both crucial and challenging. Current mutation-based testing techniques often lack customization for framework testing and suffer from high false positive rates, as well as limited attention from developers. There is a need to reassess the effectiveness of these techniques and propose optimizations.", "method": "The authors reviewed existing mutation-based defect detection methods, collected defect reports from three popular DL frameworks, and classified the reports using developer validation. They performed an analysis to understand the effectiveness and shortcomings of current techniques, leading to the proposal of new optimization strategies.", "result": "The optimized approach led to the identification of seven new defects, with four confirmed as high-priority and three already resolved by developers. In total, 39 unique defects were found in 23 models; 31 confirmed by developers, and eight fixed.", "conclusion": "Existing mutation-based methods for DL framework testing have significant limitations, especially in terms of customization and false positives. By optimizing these methods, significant improvements in defect detection and resolution can be achieved, benefiting both framework reliability and developer workflow."}}
{"id": "2506.17642", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17642", "abs": "https://arxiv.org/abs/2506.17642", "authors": ["Shaoyu Yang", "Chunrong Fang", "Haifeng Lin", "Xiang Chen", "Zhenyu Chen"], "title": "May the Feedback Be with You! Unlocking the Power of Feedback-Driven Deep Learning Framework Fuzzing via LLMs", "comment": null, "summary": "Artificial Intelligence (AI) Infrastructures, represented by Deep Learning\n(DL) frameworks, have served as fundamental DL systems over the last decade.\nHowever, the bugs in DL frameworks could lead to catastrophic consequences in\nsome critical scenarios (e.g., healthcare and autonomous driving). A simple yet\neffective way to find bugs in DL frameworks is fuzz testing (Fuzzing).\nUnfortunately, existing fuzzing techniques have not comprehensively considered\nmultiple types of feedback. Additionally, they analyze feedback in a\ncoarse-grained manner, such as mutating the test cases only according to\nwhether the coverage increases. Recently, researchers introduced Large Language\nModels (LLMs) into fuzzing. However, current LLM-based fuzzing techniques only\nfocus on using LLMs to generate test cases while overlooking their potential to\nanalyze feedback information, failing to create more valid and diverse test\ncases. To fill this gap, we propose FUEL to break the seal of Feedback-driven\nfuzzing for DL frameworks. The backbone of FUEL comprises two LLM-based agents,\nnamely analysis LLM and generation LLM. Analysis LLM agent infers analysis\nsummaries from feedback information, while the generation LLM agent creates\ntests guided by these analysis summaries. So far, FUEL has detected 104 bugs\nfor PyTorch and TensorFlow, with 93 confirmed as new bugs, 47 already fixed,\nand 5 assigned with CVE IDs. Our work indicates that considering multiple types\nof feedback is beneficial to fuzzing performance, and leveraging LLMs to\nanalyze feedback information is a promising direction. Our artifact is\navailable at https://github.com/NJU-iSE/FUEL", "AI": {"tldr": "FUEL, a novel fuzzing framework using LLMs for both feedback analysis and test generation, effectively found numerous and impactful bugs in major DL frameworks, showing that deep, multi-type feedback integration is key to advancing AI infrastructure reliability.", "motivation": "The motivation behind this work is the critical necessity to reliably detect bugs in Deep Learning (DL) frameworks, as such bugs can have catastrophic effects in high-stakes domains like healthcare and autonomous driving. Existing fuzzing techniques lack comprehensive, multi-type feedback usage and perform feedback analysis in a coarse-grained way. Moreover, current LLM-based fuzzing only utilizes LLMs for test case generation, overlooking feedback analysis.", "method": "The authors propose FUEL, a fuzzing framework that integrates two Large Language Model (LLM) agents: an analysis LLM to infer summaries from feedback information, and a generation LLM to create test cases based on these summaries. This method leverages diverse feedback for more effective and varied test case generation.", "result": "FUEL identified 104 bugs in PyTorch and TensorFlow. Among these, 93 were confirmed as new bugs, 47 have already been fixed, and 5 have been assigned CVE IDs, demonstrating significant impact and effectiveness.", "conclusion": "Leveraging LLMs for both feedback analysis and test case generation, and considering multiple feedback types, enhances fuzzing efficacy for DL frameworks. FUEL\u2019s results suggest this is a promising path for future research and practical application."}}
{"id": "2506.17647", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17647", "abs": "https://arxiv.org/abs/2506.17647", "authors": ["Yixian Qi", "Jiajun Jiang", "Fengjie Li", "Bowen Chen", "Hongyu Zhang", "Junjie Chen"], "title": "Improving Compiler Bug Isolation by Leveraging Large Language Models", "comment": "12 pages, 7 figures", "summary": "Compilers play a foundational role in building reliable software systems, and\nbugs within them can lead to catastrophic consequences. The compilation process\ntypically involves hundreds of files, making traditional automated bug\nisolation techniques inapplicable due to scalability or effectiveness issues.\nCurrent mainstream compiler bug localization techniques have limitations in\ntest program mutation and resource consumption. Inspired by the recent advances\nof pre-trained Large Language Models (LLMs), we propose an innovative approach\nnamed AutoCBI, which (1) uses LLMs to summarize compiler file functions and (2)\nemploys specialized prompts to guide LLM in reordering suspicious file\nrankings. This approach leverages four types of information: the failing test\nprogram, source file function summaries, lists of suspicious files identified\nthrough analyzing test coverage, as well as compilation configurations with\nrelated output messages, resulting in a refined ranking of suspicious files.\nOur evaluation of AutoCBI against state-of-the-art approaches (DiWi, RecBi and\nFuseFL) on 120 real-world bugs from the widely-used GCC and LLVM compilers\ndemonstrates its effectiveness. Specifically, AutoCBI isolates 66.67%/69.23%,\n300%/340%, and 100%/57.14% more bugs than RecBi, DiWi, and FuseFL,\nrespectively, in the Top-1 ranked results for GCC/LLVM. Additionally, the\nablation study underscores the significance of each component in our approach.", "AI": {"tldr": "AutoCBI uses large language models to summarize and rank suspicious files in compilers, significantly improving bug isolation over current methods in real-world GCC and LLVM bugs.", "motivation": "Bugs in compilers can have severe consequences, but current automated bug localization techniques struggle with the scale and complexity of modern compilers. Existing methods have limitations in test mutability and efficiency. There is a need for more scalable and effective approaches.", "method": "The authors introduce AutoCBI, an innovative method that leverages large language models (LLMs) to summarize the functions of compiler source files and uses specialized prompting to guide the LLM in reordering suspicious file rankings. AutoCBI integrates four key sources of information: failing test programs, summaries of source file functions, lists of suspicious files from test coverage analysis, and compilation-related configurations/output messages, to refine the ranking of suspected buggy files.", "result": "AutoCBI was evaluated against leading approaches (DiWi, RecBi, and FuseFL) using 120 real bugs from GCC and LLVM compilers. AutoCBI outperformed the others, isolating 66.67%/69.23% (GCC/LLVM), 300%/340%, and 100%/57.14% more bugs in the Top-1 ranked files compared to RecBi, DiWi, and FuseFL respectively. An ablation study confirmed the importance of each component of AutoCBI.", "conclusion": "By leveraging LLMs and integrating multiple sources of information, AutoCBI provides a more scalable and effective solution for compiler bug isolation, surpassing current state-of-the-art approaches in accuracy and efficiency."}}
{"id": "2506.17772", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17772", "abs": "https://arxiv.org/abs/2506.17772", "authors": ["Haoran Xue", "Gias Uddin", "Song Wang"], "title": "PAGENT: Learning to Patch Software Engineering Agents", "comment": null, "summary": "LLM Agents produce patches automatically to resolve an issue. However, they\ncan generate inaccurate patches. Little is known about the root causes behind\nthose failed patches or how those could be fixed. This paper reports an\nempirical study of the failed patches generated by seven top LLM code agents.\nWe collected 114 issues from the SWE-bench Lite dataset that remained\nunresolved across the agents. The seven agents produced a total of 769 failed\npatches for those issues, which we checked with a combination of GPT-4o and\nmanual analysis. We present a taxonomy of the failure reasons across the\npatches. The taxonomy contains six categories, with several sub-categories\nunder each category. For example, a frequently observed category is the\ninability of an LLM to correctly infer/produce the appropriate variable type in\nthe produced patch. As a first step towards addressing such type-related\nerrors, we designed PAGENT (Patch Agent). PAGENT utilizes program analysis\ntechniques like CFG creation and exploration to infer the type of information\nof a patch. PAGENT does this by applying repository-level static code analysis\ntechniques. Then, PAGENT refines the inferred type by further utilizing an\nLLM-based inference technique. We tested PAGENT on all 127 type-related failed\npatches from the top three agents in our study. PAGENT could fix 29 of the 127\nfailed patches.", "AI": {"tldr": "The paper investigates why LLM code agents often generate faulty patches, identifying key failure types\u2014including type errors. PAGENT, a tool combining static analysis and LLMs, can fix a notable portion of such errors, advancing automated code repair.", "motivation": "Large Language Model (LLM) agents auto-generate code patches but often create inaccurate ones. The root causes of these failures are poorly understood, and solutions to improve patch accuracy are needed.", "method": "The authors conducted an empirical study of failed patches generated by seven top LLM code agents using 114 unresolved issues from the SWE-bench Lite dataset. They analyzed 769 failed patches (using GPT-4o and manual review), created a taxonomy of failure reasons, and focused on type-related errors. To address these, they developed PAGENT, which combines traditional program analysis (static code analysis and control flow graph exploration) with LLM-based inference to improve patch type information.", "result": "A taxonomy with six main categories of failure reasons was established. Among these, type inference errors were common. The new tool, PAGENT, was able to fix 29 out of 127 type-related failed patches produced by the top agents, showing promise in addressing a common cause of failure.", "conclusion": "LLM agents often fail to generate correct patches due to several reasons, with type inference being a major problem. Addressing this, PAGENT demonstrates that combining static code analysis with LLM inference can repair a notable subset of type-related patch failures. Further improvements can likely increase this success rate and help improve automated patching."}}
{"id": "2506.17798", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.17798", "abs": "https://arxiv.org/abs/2506.17798", "authors": ["Wang Lingxiang", "Quanzhi Fu", "Wenjia Song", "Gelei Deng", "Yi Liu", "Dan Williams", "Ying Zhang"], "title": "SAVANT: Vulnerability Detection in Application Dependencies through Semantic-Guided Reachability Analysis", "comment": null, "summary": "The integration of open-source third-party library dependencies in Java\ndevelopment introduces significant security risks when these libraries contain\nknown vulnerabilities. Existing Software Composition Analysis (SCA) tools\nstruggle to effectively detect vulnerable API usage from these libraries due to\nlimitations in understanding API usage semantics and computational challenges\nin analyzing complex codebases, leading to inaccurate vulnerability alerts that\nburden development teams and delay critical security fixes.\n  To address these challenges, we proposed SAVANT by leveraging two insights:\nproof-of-vulnerability test cases demonstrate how vulnerabilities can be\ntriggered in specific contexts, and Large Language Models (LLMs) can understand\ncode semantics. SAVANT combines semantic preprocessing with LLM-powered context\nanalysis for accurate vulnerability detection. SAVANT first segments source\ncode into meaningful blocks while preserving semantic relationships, then\nleverages LLM-based reflection to analyze API usage context and determine\nactual vulnerability impacts. Our evaluation on 55 real-world applications\nshows that SAVANT achieves 83.8% precision, 73.8% recall, 69.0% accuracy, and\n78.5% F1-score, outperforming state-of-the-art SCA tools.", "AI": {"tldr": "SAVANT uses LLMs and semantic code analysis to better detect true vulnerable API usages in Java dependencies, reducing false alerts and improving security response, with evaluation showing higher performance than leading tools.", "motivation": "Java projects increasingly rely on open-source third-party libraries, but when these libraries have vulnerabilities, they pose significant security risks. Current SCA tools are not effective enough, leading to many false alerts and missed critical vulnerabilities due to poor understanding of API usage semantics and difficulty analyzing complex code.", "method": "The authors propose SAVANT, a method that (1) semantically pre-processes source code to segment it into meaningful, semantically-related blocks, and (2) applies Large Language Models (LLMs) for context-aware reflection on how APIs from potentially vulnerable libraries are actually used, determining the real risk.", "result": "SAVANT was evaluated on 55 real-world Java applications and achieved an 83.8% precision, 73.8% recall, 69.0% accuracy, and 78.5% F1-score\u2014showing superior performance compared to leading SCA tools.", "conclusion": "Leveraging LLMs and semantic pre-processing enables more accurate detection of vulnerable API usage in third-party libraries, significantly reducing the burden of inaccurate alerts and accelerating necessary security fixes."}}
{"id": "2506.17812", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17812", "abs": "https://arxiv.org/abs/2506.17812", "authors": ["Noble Saji Mathews", "Meiyappan Nagappan"], "title": "Is Your Automated Software Engineer Trustworthy?", "comment": null, "summary": "Large Language Models (LLMs) are being increasingly used in software\nengineering tasks, with an increased focus on bug report resolution over the\npast year. However, most proposed systems fail to properly handle uncertain or\nincorrect inputs and outputs. Existing LLM-based tools and coding agents\nrespond to every issue and generate a patch for every case, even when the input\nis vague or their own output is incorrect. There are no mechanisms in place to\nabstain when confidence is low. This leads to unreliable behaviour, such as\nhallucinated code changes or responses based on vague issue reports. We\nintroduce BouncerBench, a benchmark that evaluates whether LLM-based software\nagents can refuse to act when inputs are ill-defined or refuse to respond when\ntheir own outputs are likely to be incorrect. Unlike prior benchmarks that\nimplicitly incentivize models to generate responses even when uncertain,\nBouncerBench aims to improve precision by targeting two overlooked failure\npoints: (1) vague or underspecified issue descriptions in tickets and (2)\nlogically or functionally incorrect code patches created by the system. It\nmeasures whether proposed systems can distinguish actionable issues from vague\ntickets and valid patches from untrustworthy ones. We also implement a basic\ninput and output bouncer, evaluating how well current LLMs can abstain when\nneeded. Our results show that most models fail to abstain from underspecified\ninputs or incorrect outputs. Hence, we conclude that there is significant room\nfor improvement before LLMs can be trusted to make correct decisions and\nrecommendations in real-world software engineering workflows. BouncerBench\nprovides a first step toward evaluating and building more cautious, trustworthy\ncode agents. The replication package, dataset, and leaderboard can be found at\nbouncerbench.com", "AI": {"tldr": "BouncerBench is a new benchmark for testing if LLMs can refuse to answer when bug reports are too vague or suggested code patches are wrong. Most current models still answer even when uncertain, showing a key shortcoming. This benchmark aims to push LLMs to be more cautious before being used reliably in real-world software development.", "motivation": "Large Language Models (LLMs) are increasingly used in software engineering, especially for bug report resolution. However, current LLM-based systems always generate responses, even when input is unclear or outputs are potentially wrong, leading to unreliable and untrustworthy behavior. There's a need for mechanisms enabling models to abstain in the face of uncertainty.", "method": "The authors introduce BouncerBench, a benchmark designed to evaluate whether LLM-based software agents can refuse to act on ill-defined inputs or abstain from providing outputs when their answers are likely incorrect. BouncerBench focuses on two failure points: vague issue reports and functionally incorrect code patches, and includes tools to test and assess model abstention.", "result": "Experiments show that most current LLMs consistently fail to abstain when presented with vague inputs or when their outputs are likely incorrect, often generating responses or patches regardless of confidence or quality.", "conclusion": "Significant improvements are needed before LLMs can be reliably trusted in software engineering tasks. BouncerBench establishes an important framework for evaluating and fostering the development of more cautious, trustworthy code-generation agents."}}
{"id": "2506.17833", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17833", "abs": "https://arxiv.org/abs/2506.17833", "authors": ["Giorgio Amasanti", "Jasmin Jahic"], "title": "The Impact of AI-Generated Solutions on Software Architecture and Productivity: Results from a Survey Study", "comment": "Accepted for presentation at the International Workshop on\n  AI-Assisted Software Architecting (AISA 2025), colocated with the 19th\n  European Conference on Software Architecture (ECSA 2025), to be held 15-19\n  September 2025 in Limassol, Cyprus", "summary": "AI-powered software tools are widely used to assist software engineers.\nHowever, there is still a need to understand the productivity benefits of such\ntools for software engineers. In addition to short-term benefits, there is a\nquestion of how adopting AI-generated solutions affects the quality of software\nover time (e.g., maintainability and extendability).\n  To provide some insight on these questions, we conducted a survey among\nsoftware practitioners who use AI tools. Based on the data collected from our\nsurvey, we conclude that AI tools significantly increase the productivity of\nsoftware engineers. However, the productivity benefits of using AI tools reduce\nas projects become more complex. The results also show that there are no\nsignificant negative influences of adopting AI-generated solutions on software\nquality, as long as those solutions are limited to smaller code snippets.\nHowever, when solving larger and more complex problems, AI tools generate\nsolutions of a lower quality, indicating the need for architects to perform\nproblem decomposition and solution integration.", "AI": {"tldr": "AI tools help software engineers work faster, especially on smaller, simpler tasks, without harming code quality. However, for complex problems, their effectiveness drops and solution quality suffers, requiring human oversight.", "motivation": "There is widespread use of AI-powered tools in software engineering, but the actual productivity benefits and the long-term effects on software quality are not fully understood.", "method": "A survey was conducted among software practitioners who actively use AI tools during software development.", "result": "AI tools significantly boost software engineers' productivity, but the benefit declines as project complexity increases. AI-generated solutions do not significantly harm software quality for small code snippets, but for larger and more complex tasks, the quality drops, suggesting the need for human architectural oversight.", "conclusion": "AI tools are useful for boosting productivity and do not negatively affect software quality for simple tasks. However, for complex software engineering tasks, human intervention remains vital."}}
{"id": "2506.17937", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17937", "abs": "https://arxiv.org/abs/2506.17937", "authors": ["Tommi Mikkonen", "Antero Taivalsaari"], "title": "Software Reuse in the Generative AI Era: From Cargo Cult Towards AI Native Software Engineering", "comment": null, "summary": "Software development is currently under a paradigm shift in which artificial\nintelligence and generative software reuse are taking the center stage in\nsoftware creation. Consequently, earlier software reuse practices and methods\nare rapidly being replaced by AI-assisted approaches in which developers place\ntheir trust on code that has been generated by artificial intelligence. This is\nleading to a new form of software reuse that is conceptually not all that\ndifferent from cargo cult development. In this paper we discuss the\nimplications of AI-assisted generative software reuse in the context of\nemerging \"AI native\" software engineering, bring forth relevant questions, and\ndefine a tentative research agenda and call to action for tackling some of the\ncentral issues associated with this approach.", "AI": {"tldr": "AI-powered software reuse is reshaping software engineering, bringing both opportunities and risks. The paper calls for urgent research to address issues similar to 'cargo cult development' and guides the community towards informed and responsible adoption of these new practices.", "motivation": "The motivation behind this paper is the rapidly changing landscape of software development, where AI-assisted and generative software reuse is replacing traditional software reuse practices. The paper aims to analyze the implications of this paradigm shift.", "method": "The paper employs a discussion-based method, critically assessing the impact of AI-assisted generative software reuse and raising key questions for the emerging 'AI native' software engineering context. It also outlines a preliminary research agenda.", "result": "The paper presents the potential issues and challenges arising from AI-driven software reuse, likening it to cargo cult development, and suggests focal points for future research.", "conclusion": "The paper concludes that there are significant unanswered questions and risks associated with AI-assisted software reuse, necessitating comprehensive research and community action to better understand and address these challenges."}}
{"id": "2506.17948", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17948", "abs": "https://arxiv.org/abs/2506.17948", "authors": ["Mahzabin Tamanna", "Yash Chandrani", "Matthew Burrows", "Brandon Wroblewski", "Laurie Williams", "Dominik Wermke"], "title": "Build It Clean: Large-Scale Detection of Code Smells in Build Scripts", "comment": "12 pages, 5 tables, 2 figures", "summary": "Build scripts are files that automate the process of compiling source code,\nmanaging dependencies, running tests, and packaging software into deployable\nartifacts. These scripts are ubiquitous in modern software development\npipelines for streamlining testing and delivery. While developing build\nscripts, practitioners may inadvertently introduce code smells. Code smells are\nrecurring patterns of poor coding practices that may lead to build failures or\nincrease risk and technical debt. The goal of this study is to aid\npractitioners in avoiding code smells in build scripts through an empirical\nstudy of build scripts and issues on GitHub. We employed a mixed-methods\napproach, combining qualitative and quantitative analysis. We conducted a\nqualitative analysis of 2000 build-script-related GitHub issues. Next, we\ndeveloped a static analysis tool, Sniffer, to identify code smells in 5882\nbuild scripts of Maven, Gradle, CMake, and Make files, collected from 4877\nopen-source GitHub repositories. We identified 13 code smell categories, with a\ntotal of 10,895 smell occurrences, where 3184 were in Maven, 1214 in Gradle,\n337 in CMake, and 6160 in Makefiles.\n  Our analysis revealed that Insecure URLs were the most prevalent code smell\nin Maven build scripts, while Hardcoded Paths/URLs were commonly observed in\nboth Gradle and CMake scripts. Wildcard Usage emerged as the most frequent\nsmell in Makefiles. The co-occurrence analysis revealed strong associations\nbetween specific smell pairs of Hardcoded Paths/URLs with Duplicates, and\nInconsistent Dependency Management with Empty or Incomplete Tags, indicating\npotential underlying issues in the build script structure and maintenance\npractices. Based on our findings, we recommend strategies to mitigate the\nexistence of code smells in build scripts to improve the efficiency,\nreliability, and maintainability of software projects.", "AI": {"tldr": "By analyzing thousands of GitHub build scripts and issues, the authors identified common code smells across Maven, Gradle, CMake, and Makefiles, and developed a tool (Sniffer) for detection. Their findings inform practical strategies for writing cleaner, more robust build scripts.", "motivation": "Build scripts are widely used in modern software pipelines but are susceptible to code smells\u2014patterns of poor practices that affect reliability and maintainability. There is a need to systematically study and mitigate these issues to improve software quality.", "method": "A mixed-methods approach was used: first, a qualitative analysis of 2000 build-script-related GitHub issues; then, development of a static analysis tool (Sniffer) that scanned 5882 build scripts from 4877 open-source repositories using Maven, Gradle, CMake, and Make.", "result": "13 code smell categories were identified with nearly 11,000 occurrences. The most common smells varied between build systems: Insecure URLs in Maven, Hardcoded Paths/URLs in Gradle and CMake, and Wildcard Usage in Makefiles. Specific smell pairs were found to co-occur, highlighting deeper structural or maintenance issues.", "conclusion": "This study provides insights into the prevalence and types of code smells in build scripts across different tools. It offers recommendations and strategies for practitioners to avoid these pitfalls, thereby enhancing the efficiency, reliability, and maintainability of software projects."}}
{"id": "2506.18050", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18050", "abs": "https://arxiv.org/abs/2506.18050", "authors": ["Lyuye Zhang", "Jian Zhang", "Kaixuan Li", "Chong Wang", "Chengwei Liu", "Jiahui Wu", "Sen Chen", "Yaowen Zheng", "Yang Liu"], "title": "VFArch\u0113: A Dual-Mode Framework for Locating Vulnerable Functions in Open-Source Software", "comment": "15 pages", "summary": "Software Composition Analysis (SCA) has become pivotal in addressing\nvulnerabilities inherent in software project dependencies. In particular,\nreachability analysis is increasingly used in Open-Source Software (OSS)\nprojects to identify reachable vulnerabilities (e.g., CVEs) through call\ngraphs, enabling a focus on exploitable risks. Performing reachability analysis\ntypically requires the vulnerable function (VF) to track the call chains from\ndownstream applications. However, such crucial information is usually\nunavailable in modern vulnerability databases like NVD. While directly\nextracting VF from modified functions in vulnerability patches is intuitive,\npatches are not always available. Moreover, our preliminary study shows that\nover 26% of VF do not exist in the modified functions. Meanwhile, simply\nignoring patches to search vulnerable functions suffers from overwhelming\nnoises and lexical gaps between descriptions and source code. Given that almost\nhalf of the vulnerabilities are equipped with patches, a holistic solution that\nhandles both scenarios with and without patches is required. To meet real-world\nneeds and automatically localize VF, we present VFArch\\=e, a dual-mode approach\ndesigned for disclosed vulnerabilities, applicable in scenarios with or without\navailable patch links. The experimental results of VFArch\\=e on our constructed\nbenchmark dataset demonstrate significant efficacy regarding three metrics,\nachieving 1.3x and 1.9x Mean Reciprocal Rank over the best baselines for\nPatch-present and Patch-absent modes, respectively. Moreover, VFArch\\=e has\nproven its applicability in real-world scenarios by successfully locating VF\nfor 43 out of 50 latest vulnerabilities with reasonable efforts and\nsignificantly reducing 78-89% false positives of SCA tools.", "AI": {"tldr": "The paper presents VFArch=e, a method for more accurately and efficiently finding vulnerable functions in open source software, regardless of whether patches are available. It shows strong results in both benchmark tests and real-world usage, outperforming previous methods and reducing false positives.", "motivation": "Software Composition Analysis (SCA) is important for identifying vulnerabilities in dependencies of open-source software projects. However, existing vulnerability databases often lack crucial information about vulnerable functions (VF), making it challenging to perform reachability analysis that identifies exploitable risks via call graphs. Both the absence of patches and the fact that not all VFs exist in modified functions further complicate the automatic localization of VFs.", "method": "The paper introduces VFArch=e, a dual-mode approach for automatically localizing vulnerable functions (VF) for disclosed vulnerabilities. This method is applicable both when patch links are available and when they are absent. The approach is evaluated on a constructed benchmark dataset and on real-world vulnerabilities.", "result": "VFArch=e achieved significant improvements in locating vulnerable functions, with 1.3x and 1.9x Mean Reciprocal Rank over the best existing baselines for scenarios with and without patches, respectively. In real-world tests, VFArch=e successfully located VFs for 43 out of 50 recent vulnerabilities, while significantly reducing false positives (by 78-89%) compared to existing SCA tools.", "conclusion": "VFArch=e effectively and automatically localizes vulnerable functions both with and without the availability of vulnerability patches. It outperforms existing approaches in accuracy and reduces the noise in identifying true vulnerabilities, proving applicable and beneficial in practical software security scenarios."}}
{"id": "2506.18191", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.18191", "abs": "https://arxiv.org/abs/2506.18191", "authors": ["Masudul Hasan Masud Bhuiyan", "Gianluca De Stefano", "Giancarlo Pellegrino", "Cristian-Alexandru Staicu"], "title": "Call Me Maybe: Enhancing JavaScript Call Graph Construction using Graph Neural Networks", "comment": null, "summary": "Static analysis plays a key role in finding bugs, including security issues.\nA critical step in static analysis is building accurate call graphs that model\nfunction calls in a program. However, due to hard-to-analyze language features,\nexisting call graph construction algorithms for JavaScript are neither sound\nnor complete. Prior work shows that even advanced solutions produce false edges\nand miss valid ones. In this work, we assist these tools by identifying missed\ncall edges. Our main idea is to frame the problem as link prediction on full\nprogram graphs, using a rich representation with multiple edge types. Our\napproach, GRAPHIA, leverages recent advances in graph neural networks to model\nnon-local relationships between code elements. Concretely, we propose\nrepresenting JavaScript programs using a combination of syntactic- and\nsemantic-based edges. GRAPHIA can learn from imperfect labels, including static\ncall edges from existing tools and dynamic edges from tests, either from the\nsame or different projects. Because call graphs are sparse, standard machine\nlearning metrics like ROC are not suitable. Instead, we evaluate GRAPHIA by\nranking function definitions for each unresolved call site. We conduct a\nlarge-scale evaluation on 50 popular JavaScript libraries with 163K call edges\n(150K static and 13K dynamic). GRAPHIA builds program graphs with 6.6M\nstructural and 386K semantic edges. It ranks the correct target as the top\ncandidate in over 42% of unresolved cases and within the top 5 in 72% of cases,\nreducing the manual effort needed for analysis. Our results show that\nlearning-based methods can improve the recall of JavaScript call graph\nconstruction. To our knowledge, this is the first work to apply GNN-based link\nprediction to full multi-file program graphs for interprocedural analysis.", "AI": {"tldr": "GRAPHIA uses graph neural networks to predict and recover missed call edges in JavaScript program call graphs, improving the accuracy and recall of static analysis tools. Evaluated on major libraries, it identified top candidate matches in over 42% of unresolved cases, representing the first GNN-based interprocedural link prediction for full JavaScript programs.", "motivation": "Existing call graph construction algorithms for JavaScript are neither sound nor complete due to hard-to-analyze language features. Even advanced solutions miss valid call edges or introduce false ones. This hinders the accuracy and utility of static analysis, which is central to finding bugs and security issues.", "method": "The paper presents GRAPHIA, a method that frames the problem of finding missed call edges as a link prediction task on full program graphs. It leverages graph neural networks (GNNs) and represents JavaScript programs using both syntactic and semantic edges. GRAPHIA learns from imperfect labels (from static analysis tools and dynamic tests) and ranks function definitions for each unresolved call site.", "result": "In a large-scale evaluation on 50 popular JavaScript libraries with 163K call edges, GRAPHIA achieved ranking the correct target as the top candidate in over 42% of unresolved cases and within the top 5 in 72% of cases. It reduces manual effort and shows that learning-based methods improve recall in JavaScript call graph construction.", "conclusion": "GRAPHIA, by applying GNN-based link prediction to multi-file program graphs, enhances interprocedural call graph construction and recall for JavaScript programs. This innovative learning-based approach can supplement existing static analysis tools, reducing missed call edges and the associated manual effort."}}
{"id": "2506.18219", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18219", "abs": "https://arxiv.org/abs/2506.18219", "authors": ["Ulrike M. Graetsch", "Rashina Hoda", "Hourieh Khalazjadeh", "Mojtaba Shahin", "John Grundy"], "title": "Managing Technical Debt in a Multidisciplinary Data Intensive Software Team: an Observational Case Study", "comment": "25 pages", "summary": "Context: There is an increase in the investment and development of\ndata-intensive (DI) solutions, systems that manage large amounts of data.\nWithout careful management, this growing investment will also grow associated\ntechnical debt (TD). Delivery of DI solutions requires a multidisciplinary\nskill set, but there is limited knowledge about how multidisciplinary teams\ndevelop DI systems and manage TD.\n  Objective: This research contributes empirical, practice based insights about\nmultidisciplinary DI team TD management practices.\n  Method: This research was conducted as an exploratory observation case study.\nWe used socio-technical grounded theory (STGT) for data analysis to develop\nconcepts and categories that articulate TD and TDs debt management practices.\n  Results: We identify TD that the DI team deals with, in particular technical\ndata components debt and pipeline debt. We explain how the team manages the TD,\nassesses TD, what TD treatments they consider and how they implement TD\ntreatments to fit sprint capacity constraints.\n  Conclusion: We align our findings to existing TD and TDM taxonomies, discuss\ntheir implications and highlight the need for new implementation patterns and\ntool support for multidisciplinary DI teams.", "AI": {"tldr": "This paper investigates how multidisciplinary teams managing data-intensive systems handle growing technical debt, using case study and grounded theory methods. It identifies specific types of debt, team management practices, and highlights the need for better tools and new patterns tailored to such teams.", "motivation": "There is a rise in investment and development of data-intensive (DI) solutions, which manage large data volumes. However, such growth can also escalate technical debt (TD) if not managed properly. Despite the multidisciplinary nature of teams delivering DI solutions, little is known about how they handle technical debt.", "method": "The study uses an exploratory observation case study, leveraging socio-technical grounded theory (STGT) to analyze data and develop concepts and categories describing technical debt (TD) and its management within a multidisciplinary DI team.", "result": "The researchers identify the types of technical debt confronting the DI team\u2014particularly in technical data components and pipeline debt. They detail the team's approaches to managing, assessing, and treating TD, including strategies for implementing debt treatments within sprint capacity limitations.", "conclusion": "The study aligns its findings with current technical debt (TD) and technical debt management (TDM) taxonomies, discusses practical implications, and underscores the need for new implementation patterns and tool support for multidisciplinary data-intensive teams."}}
{"id": "2506.18289", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18289", "abs": "https://arxiv.org/abs/2506.18289", "authors": ["Saurabhsingh Rajput", "Mootez Saad", "Tushar Sharma"], "title": "Tu(r)ning AI Green: Exploring Energy Efficiency Cascading with Orthogonal Optimizations", "comment": "In review", "summary": "AI's exponential growth intensifies computational demands and energy\nchallenges. While practitioners employ various optimization techniques, that we\nrefer as \"knobs\" in this paper, to tune model efficiency, these are typically\nafterthoughts and reactive ad-hoc changes applied in isolation without\nunderstanding their combinatorial effects on energy efficiency. This paper\nemphasizes on treating energy efficiency as the first-class citizen and as a\nfundamental design consideration for a compute-intensive pipeline. We show that\nstrategic selection across five AI pipeline phases (data, model, training,\nsystem, inference) creates cascading efficiency. Experimental validation shows\northogonal combinations reduce energy consumption by up to $94.6$% while\npreserving $95.95$% of the original F1 score of non-optimized pipelines. This\ncurated approach provides actionable frameworks for informed sustainable AI\nthat balance efficiency, performance, and environmental responsibility.", "AI": {"tldr": "Treating energy efficiency as a first-class goal across the whole AI pipeline achieves dramatic energy savings (up to 94.6%) with little loss in accuracy, and the paper presents actionable frameworks for sustainable AI design.", "motivation": "The rapid expansion of AI leads to increased computational demand and significant energy consumption. Current optimization techniques for AI efficiency are used reactively and individually, often missing the comprehensive impact on energy and model performance.", "method": "The paper proposes treating energy efficiency as a primary objective throughout all five phases of the AI pipeline: data, model, training, system, and inference. It encourages a holistic strategy by strategically combining efficiency measures across these phases, instead of applying them in isolation.", "result": "Experimental results indicate that selectively combining optimizations throughout the pipeline can reduce energy consumption by up to 94.6%, while maintaining 95.95% of the original F1 score.", "conclusion": "By proactively incorporating energy efficiency into fundamental design decisions and optimizing the entire AI pipeline, practitioners can achieve substantial sustainability benefits without significant performance loss. The paper offers practical frameworks to enable more sustainable and efficient AI development."}}
{"id": "2506.18315", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18315", "abs": "https://arxiv.org/abs/2506.18315", "authors": ["Lehan He", "Zeren Chen", "Zhe Zhang", "Jing Shao", "Xiang Gao", "Lu Sheng"], "title": "Use Property-Based Testing to Bridge LLM Code Generation and Validation", "comment": null, "summary": "Large Language Models (LLMs) excel at code generation, but ensuring their\noutputs to be functionally correct, especially in complex programming tasks, is\na persistent challenge. While traditional Test-Driven Development (TDD) offers\na path for code refinement, its efficacy with LLMs is often undermined by the\nscarcity of high-quality test cases or the pitfalls of automated test\ngeneration, including biased tests or inaccurate output predictions that can\nmisdirect the correction process. This paper introduces Property-Generated\nSolver, a novel framework that leverages Property-Based Testing (PBT) to\nvalidate high-level program properties or invariants, instead of relying on\nspecific input-output examples. These properties are often simpler to define\nand verify than directly predicting exhaustive test oracles, breaking the\n\"cycle of self-deception\" where tests might share flaws with the code they are\nmeant to validate. Property-Generated Solver employs two collaborative\nLLM-based agents: a Generator dedicated to code generation and iterative\nrefinement, and a Tester that manages the PBT life-cycle and formulate\nsemantically rich feedback from property violations. The resulting\ncomprehensive and actionable feedback then guides the Generator in its\nrefinement efforts. By establishing PBT as the core validation engine within\nthis iterative, closed-loop paradigm, Property-Generated Solver provides a\nrobust mechanism for steering LLMs towards more correct and generalizable code.\nExtensive experimental results on multiple code generation benchmarks\ndemonstrate that Property-Generated Solver achieves substantial pass@1\nimprovements, ranging from 23.1% to 37.3% relative gains over established TDD\nmethods.", "AI": {"tldr": "A novel LLM-driven code synthesis approach using Property-Based Testing achieves significantly better correctness than conventional TDD, making LLM output more reliable for complex programming tasks.", "motivation": "Ensuring that code generated by Large Language Models (LLMs) is functionally correct is a major challenge, especially for complex programming tasks. Traditional Test-Driven Development (TDD) methods either suffer from a lack of quality test cases or from pitfalls in automated test generation. This motivates the search for more reliable means to validate and refine LLM-generated code.", "method": "The paper introduces the Property-Generated Solver framework, which uses Property-Based Testing (PBT) instead of standard test cases. This framework involves two LLM-based agents: a Generator, which generates and iteratively refines code, and a Tester, which controls the PBT process and provides detailed feedback based on property violations. The Generator uses this feedback to improve the code iteratively.", "result": "Experiments on various code generation benchmarks show that the Property-Generated Solver significantly improves functional correctness of generated code, achieving pass@1 improvements between 23.1% and 37.3% compared to traditional TDD approaches.", "conclusion": "Property-Generated Solver, which leverages Property-Based Testing in an iterative refinement loop using LLM agents, is an effective mechanism for producing more correct and generalizable code from LLMs. It outperforms established TDD-based code synthesis methods by a substantial margin in benchmark tests."}}
{"id": "2506.18329", "categories": ["cs.SE", "D.2.8; C.4; I.2.7; I.2.6; I.5.1"], "pdf": "https://arxiv.org/pdf/2506.18329", "abs": "https://arxiv.org/abs/2506.18329", "authors": ["Elijah Zolduoarrati", "Sherlock A. Licorish", "Nigel Stanger"], "title": "Predictive Analytics for Collaborators Answers, Code Quality, and Dropout on Stack Overflow", "comment": "46 pages, 17 tables, 7 figures", "summary": "Previous studies that used data from Stack Overflow to develop predictive\nmodels often employed limited benchmarks of 3-5 models or adopted arbitrary\nselection methods. Despite being insightful, their limited scope suggests the\nneed to benchmark more models to avoid overlooking untested algorithms. Our\nstudy evaluates 21 algorithms across three tasks: predicting the number of\nquestion a user is likely to answer, their code quality violations, and their\ndropout status. We employed normalisation, standardisation, as well as\nlogarithmic and power transformations paired with Bayesian hyperparameter\noptimisation and genetic algorithms. CodeBERT, a pre-trained language model for\nboth natural and programming languages, was fine-tuned to classify user dropout\ngiven their posts (questions and answers) and code snippets. We found Bagging\nensemble models combined with standardisation achieved the highest R2 value\n(0.821) in predicting user answers. The Stochastic Gradient Descent regressor,\nfollowed by Bagging and Epsilon Support Vector Machine models, consistently\ndemonstrated superior performance to other benchmarked algorithms in predicting\nuser code quality across multiple quality dimensions and languages. Extreme\nGradient Boosting paired with log-transformation exhibited the highest F1-score\n(0.825) in predicting user dropout. CodeBERT was able to classify user dropout\nwith a final F1-score of 0.809, validating the performance of Extreme Gradient\nBoosting that was solely based on numerical data. Overall, our benchmarking of\n21 algorithms provides multiple insights. Researchers can leverage findings\nregarding the most suitable models for specific target variables, and\npractitioners can utilise the identified optimal hyperparameters to reduce the\ninitial search space during their own hyperparameter tuning processes.", "AI": {"tldr": "Benchmarked 21 algorithms for Stack Overflow predictive tasks. Found best-performing algorithms and preprocessing methods for each task, offering valuable insights for future model selection and optimization.", "motivation": "Previous studies using Stack Overflow data for predictive modeling relied on few models (3-5) or arbitrary selection, potentially overlooking better-performing algorithms; this paper aims to give a comprehensive benchmark across more algorithms.", "method": "Benchmarked 21 algorithms on three prediction tasks (questions answered, code quality violations, dropout) using data transforms (normalisation, standardisation, logarithmic, power), Bayesian hyperparameter optimization, genetic algorithms, and fine-tuned CodeBERT for dropout classification.", "result": "Standardised Bagging ensemble models had the highest R2 (0.821) for answer prediction. Stochastic Gradient Descent regressor, Bagging, and Epsilon SVM outperformed others for code quality prediction. For dropout prediction, Extreme Gradient Boosting with log-transform had F1=0.825; CodeBERT F1=0.809, confirming similar performance from numerical and textual features.", "conclusion": "Comprehensive benchmarking reveals optimal models for different Stack Overflow prediction tasks and optimal hyperparameters, helping future researchers and practitioners select better models and streamline hyperparameter tuning."}}
{"id": "2506.18359", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18359", "abs": "https://arxiv.org/abs/2506.18359", "authors": ["Juanita Gomez", "Emily Lovell", "Stephanie Lieggi", "Alvaro A. Cardenas", "James Davis"], "title": "Recipe for Discovery: A Framework for Systematic Open Source Project Identification", "comment": null, "summary": "Open source software development, particularly within institutions such as\nuniversities and research laboratories, is often decentralized and difficult to\ntrack. Despite producing highly impactful tools in science, these efforts often\ngo unrecognized due to a lack of visibility and institutional awareness. This\npaper addresses the challenge of discovering, classifying, and analyzing open\nsource software projects developed across distributed institutional systems. We\npresent a framework for systematically identifying institutional affiliated\nrepositories, using the University of California (UC) system as a case study.\n  Using GitHub's REST API, we build a pipeline to discover relevant\nrepositories and extract meaningful metadata. We then propose and evaluate\nmultiple classification strategies, including both traditional machine learning\nmodels and large language models (LLMs), to distinguish affiliated projects\nfrom unrelated repositories and generate accurate insights into the academic\nopen source landscape. Our results show that the framework is effective at\nscale, discovering over 52,000 repositories and predicting institutional\naffiliation with high accuracy.", "AI": {"tldr": "The paper introduces a scalable framework utilizing GitHub data and machine learning to discover and classify open source projects affiliated with the University of California. It finds over 52,000 relevant repositories and achieves high affiliation prediction accuracy, addressing the recognition gap for academic software work.", "motivation": "Open source software projects in universities and labs have major impact but are hard to recognize or track due to decentralization and poor institutional awareness. There is a lack of methods to systematically discover and analyze these contributions.", "method": "The paper presents a systematic framework to identify and analyze open source projects affiliated with institutions, specifically taking the University of California as a case study. The approach uses GitHub's REST API to discover repositories and extract metadata. Several classification strategies are tested, including traditional machine learning models and large language models (LLMs), to determine project affiliation and generate insights.", "result": "The proposed framework successfully discovers over 52,000 repositories and predicts institutional affiliation with high accuracy, demonstrating scalability and effectiveness.", "conclusion": "Systematic discovery and classification of institution-affiliated open source projects is feasible and accurate at scale, offering a replicable approach for increasing visibility and recognition of academic software contributions."}}
{"id": "2506.18394", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18394", "abs": "https://arxiv.org/abs/2506.18394", "authors": ["Xiao Cheng", "Zhihao Guo", "Huan Huo", "Yulei Sui"], "title": "Tracing Errors, Constructing Fixes: Repository-Level Memory Error Repair via Typestate-Guided Context Retrieval", "comment": null, "summary": "Memory-related errors in C programming continue to pose significant\nchallenges in software development, primarily due to the complexities of manual\nmemory management inherent in the language. These errors frequently serve as\nvectors for severe vulnerabilities, while their repair requires extensive\nknowledge of program logic and C's memory model. Automated Program Repair (APR)\nhas emerged as a critical research area to address these challenges.\nTraditional APR approaches rely on expert-designed strategies and predefined\ntemplates, which are labor-intensive and constrained by the effectiveness of\nmanual specifications. Deep learning techniques offer a promising alternative\nby automatically extracting repair patterns, but they require substantial\ntraining datasets and often lack interpretability.\n  This paper introduces LTFix, a novel approach that harnesses the potential of\nLarge Language Models (LLMs) for automated memory error repair, especially for\ncomplex repository-level errors that span multiple functions and files. We\naddress two fundamental challenges in LLM-based memory error repair: a limited\nunderstanding of interprocedural memory management patterns and context window\nlimitations for repository-wide analysis. Our approach utilizes a finite\ntypestate automaton to guide the tracking of error-propagation paths and\ncontext trace, capturing both spatial (memory states) and temporal (execution\nhistory) dimensions of error behavior. This typestate-guided context retrieval\nstrategy provides the LLM with concise yet semantically rich information\nrelevant to erroneous memory management, effectively addressing the token\nlimitation of LLMs.", "AI": {"tldr": "Memory errors in C are hard to fix and existing automated repair tools have drawbacks. This paper introduces LTFix, which uses a typestate automaton to provide summary information for LLMs, helping them fix complex, repository-wide memory errors more effectively despite limited context windows.", "motivation": "C programming is prone to memory-related errors due to complex manual memory management, leading to software vulnerabilities and requiring deep expertise for effective repairs. Traditional automated program repair (APR) methods rely on manual, expert-defined approaches, while deep learning-based solutions face issues with data requirements and interpretability. There is a need for more effective, scalable, and interpretable automated solutions.", "method": "The paper proposes LTFix, a novel approach leveraging Large Language Models (LLMs) for automated repair of memory errors in C programs\u2014especially for errors spanning multiple functions and files. The core method addresses LLMs\u2019 limitations in understanding interprocedural memory management and context-window size by employing a finite typestate automaton. This automaton tracks error propagation paths and context traces, capturing key spatial and temporal aspects of error behavior, and generates concise, semantically rich context for the LLM.", "result": "LTFix provides a typestate-guided context retrieval mechanism that assists LLMs in effective diagnosis and repair of complex memory errors. By delivering crucial and condensed semantic information to the LLM, the approach overcomes token limitations and enables efficient repair across repository-level code bases.", "conclusion": "LTFix demonstrates that combining typestate-based program analysis with LLMs can significantly improve automated memory error repair in C programs. This hybrid approach offers scalability, greater accuracy, and better applicability than traditional template-based or purely deep learning-driven methods."}}
{"id": "2506.18398", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18398", "abs": "https://arxiv.org/abs/2506.18398", "authors": ["Hao Wu", "Haijun Wang", "Shangwang Li", "Yin Wu", "Ming Fan", "Wuxia Jin", "Yitao Zhao", "Ting Liu"], "title": "Your Token Becomes Worthless: Unveiling Rug Pull Schemes in Crypto Token via Code-and-Transaction Fusion Analysis", "comment": null, "summary": "Rug pull scams have emerged as a persistent threat to cryptocurrency, causing\nsignificant financial losses. A typical scenario involves scammers deploying\nhoneypot contracts to attract investments, restricting token sales, and\ndraining the funds, which leaves investors with worthless tokens. Current\nmethods either rely on predefined patterns to detect code risks or utilize\nstatistical transaction data to train detection models. However, real-world Rug\nPull schemes often involve a complex interplay between malicious code and\nsuspicious transaction behaviors. These methods, which solely focus on one\naspect, fall short in detecting such schemes effectively.\n  In this paper, we propose RPhunter, a novel technique that integrates code\nand transaction for Rug Pull detection. First, RPhunter establishes declarative\nrules and performs flow analysis to extract code risk information, further\nconstructing a semantic risk code graph (SRCG). Meanwhile, to leverage\ntransaction information, RPhunter formulates dynamic token transaction\nactivities as a token flow behavior graph (TFBG) in which nodes and edges are\ncharacterized from network structure and market manipulation perspectives.\nFinally, RPhunter employs graph neural networks to extract complementary\nfeatures from SRCG and TFBG, integrating them through an attention fusion model\nto enhance the detection of Rug Pull. We manually analyzed 645 Rug Pull\nincidents from code and transaction aspects and constructed a ground-truth\ndataset. We evaluated RPhunter on our dataset, achieving a precision of 95.3%,\na recall of 93.8% and an F1 score of 94.5%, which highlights superior\nperformance compared to existing state-of-the-art methods. Furthermore, when\napplied to the real-world scenarios, RPhunter has identified 4801 Rug Pull\ntokens, achieving a precision of 91%.", "AI": {"tldr": "RPhunter is a novel Rug Pull scam detection system that fuses contract code and transaction behavior analysis using graph neural networks, yielding higher detection accuracy than previous approaches.", "motivation": "Rug pull scams are a serious and persistent problem in cryptocurrency markets, leading to substantial financial losses for investors. Existing detection methods are insufficient because they only analyze code or transaction data separately, missing the complex interaction between the two in real attacks.", "method": "The paper introduces RPhunter, a comprehensive detection technique that combines both smart contract code analysis and token transaction behavior analysis. RPhunter creates two graph models: a semantic risk code graph (SRCG) for code features and a token flow behavior graph (TFBG) for transaction features. It then uses graph neural networks and an attention fusion model to integrate these features for more effective Rug Pull detection.", "result": "RPhunter was tested on a manually curated dataset of 645 Rug Pull incidents as well as real-world data. It achieved a precision of 95.3%, recall of 93.8%, and F1 score of 94.5%\u2014outperforming existing methods. In real-world applications, RPhunter discovered 4801 Rug Pull tokens with a precision of 91%.", "conclusion": "RPhunter significantly improves Rug Pull scam detection by integrating code and transaction analysis through advanced graph-based techniques and neural networks, outperforming state-of-the-art methods in both experimental and real-world settings."}}
{"id": "2506.18403", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18403", "abs": "https://arxiv.org/abs/2506.18403", "authors": ["Muntasir Adnan", "Carlos C. N. Kuhn"], "title": "The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs", "comment": null, "summary": "The effectiveness of AI debugging follows a predictable exponential decay\npattern; most models lose 60-80% of their debugging capability within just 2-3\nattempts, despite iterative debugging being a critical capability for practical\ncode generation systems. We introduce the Debugging Decay Index (DDI), a\nmathematical framework that quantifies when debugging becomes ineffective and\npredicts intervention points. Our strategic fresh start approach shifts from\nexploitation to exploration at strategic points in the debugging process,\ndemonstrating that well-timed interventions can rescue the effectiveness of\ndebugging. DDI reveals a fundamental limitation in current AI debugging and\nprovides the first quantitative framework for optimising iterative code\ngeneration strategies.", "AI": {"tldr": "AI code debugging rapidly becomes ineffective after a few tries. The proposed DDI framework allows developers to quantify and strategically counteract this decay, markedly improving debugging outcomes.", "motivation": "Iterative debugging is vital for practical code generation with AI, but its effectiveness sharply declines with repeated attempts. A framework is needed to quantify this decay and optimize intervention strategies.", "method": "The authors propose the Debugging Decay Index (DDI), a mathematical framework that measures and predicts the effectiveness of AI debugging over multiple attempts. They also implement a 'strategic fresh start' approach to empirical debugging sequences.", "result": "Most AI models lose 60-80% of their debugging capability within 2-3 attempts. Using DDI, timing fresh interventions optimizes debugging effectiveness and provides a strategy to rescue degraded performance.", "conclusion": "The study concludes that AI debugging capability decays rapidly during iterative attempts, and introduces the Debugging Decay Index (DDI) to quantify and predict this phenomenon. Well-timed fresh interventions significantly improve debugging outcomes."}}
{"id": "2506.18790", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18790", "abs": "https://arxiv.org/abs/2506.18790", "authors": ["Mohamad Omar Nachawati"], "title": "ModeliHub: A Web-based, Federated Analytics Platform for Modelica-centric, Model-based Systems Engineering", "comment": null, "summary": "This paper introduces ModeliHub, a Web-based, federated analytics platform\ndesigned specifically for model-based systems engineering with Modelica.\nModeliHub's key innovation lies in its Modelica-centric, hub-and-spoke\nfederation architecture that provides systems engineers with a Modelica-based,\nunified system model of repositories containing heterogeneous engineering\nartifacts. From this unified system model, ModeliHub's Virtual Twin engine\nprovides a real-time, interactive simulation environment for deploying Modelica\nsimulation models that represent digital twins of the virtual prototype of the\nsystem under development at a particular iteration of the iterative systems\nengineering life cycle. The implementation of ModeliHub is centered around its\nextensible, Modelica compiler frontend developed in Isomorphic TypeScript that\ncan run seamlessly across browser, desktop and server environments. This\narchitecture aims to strike a balance between rigor and agility, enabling\nseamless integration and analysis across various engineering domains.", "AI": {"tldr": "ModeliHub is an innovative, web-based platform for model-based systems engineering that leverages a federated hub-and-spoke Modelica architecture to unify and simulate heterogeneous engineering models, enabling seamless, interactive deployment of digital twins for collaborative systems development.", "motivation": "The motivation of this paper is to support model-based systems engineering (MBSE) by addressing the challenge of integrating and analyzing heterogeneous engineering artifacts using Modelica, and to provide a real-time and interactive simulation environment for evolving digital twins throughout the systems engineering life cycle.", "method": "The paper presents ModeliHub, a web-based federated analytics platform with a Modelica-centric, hub-and-spoke federation architecture. The platform features an extensible Modelica compiler frontend developed in TypeScript, capable of running on browsers, desktops, and servers, and supports seamless integration of various engineering artifacts.", "result": "ModeliHub enables systems engineers to access a unified Modelica-based system model from distributed and heterogeneous repositories. Its Virtual Twin engine allows real-time, interactive deployment of Modelica simulation models, effectively facilitating the use of digital twins for virtual prototyping at different life cycle stages.", "conclusion": "ModeliHub successfully balances rigor and agility in MBSE by providing a unified, scalable, and interactive simulation platform that integrates diverse engineering domains, streamlining the deployment of digital twins and the collaborative development of complex systems."}}
{"id": "2506.18796", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18796", "abs": "https://arxiv.org/abs/2506.18796", "authors": ["Kishanthan Thangarajah", "Boyuan Chen", "Shi Chang", "Ahmed E. Hassan"], "title": "Context-Aware CodeLLM Eviction for AI-assisted Coding", "comment": "12 pages, 6 figures", "summary": "AI-assisted coding tools powered by Code Large Language Models (CodeLLMs) are\nincreasingly integrated into modern software development workflows. To address\nconcerns around privacy, latency, and model customization, many enterprises opt\nto self-host these models. However, the diversity and growing number of\nCodeLLMs, coupled with limited accelerator memory, introduce practical\nchallenges in model management and serving efficiency. This paper presents\nCACE, a novel context-aware model eviction strategy designed specifically to\noptimize self-hosted CodeLLM serving under resource constraints. Unlike\ntraditional eviction strategies based solely on recency (e.g., Least Recently\nUsed), CACE leverages multiple context-aware factors, including model load\ntime, task-specific latency sensitivity, expected output length, and recent\nusage and future demand tracked through a sliding window. We evaluate CACE\nusing realistic workloads that include both latency-sensitive code completion\nand throughput-intensive code reasoning tasks. Our experiments show that CACE\nreduces Time-to-First-Token (TTFT) and end-to-end (E2E) latency, while\nsignificantly lowering the number of model evictions compared to\nstate-of-the-art systems. Ablation studies further demonstrate the importance\nof multi-factor eviction in balancing responsiveness and resource efficiency.\nThis work contributes practical strategies for deploying scalable, low-latency\nAI coding assistants in real-world software engineering environments.", "AI": {"tldr": "CACE is a new model eviction strategy for self-hosted AI coding tools that uses multiple context-aware factors to improve performance and resource use, outperforming traditional methods like LRU and making it easier to deploy efficient coding assistants at scale.", "motivation": "The increasing adoption of AI-assisted coding tools using Code Large Language Models (CodeLLMs) brings challenges when self-hosting these models due to limited accelerator memory and the need for efficient model management. Enterprises require solutions that address privacy, latency, and model customization, necessitating smarter resource management.", "method": "The paper introduces CACE, a novel context-aware model eviction strategy tailored for self-hosted CodeLLM environments. Unlike conventional eviction policies such as Least Recently Used (LRU), CACE incorporates multiple context-aware parameters\u2014model load time, task-specific latency requirements, expected output length, and patterns of recent or anticipated usage via a sliding window\u2014to optimize which models are kept in memory and which are evicted.", "result": "Empirical evaluations using hybrid workloads (latency-sensitive code completion and throughput-oriented code reasoning tasks) demonstrate that CACE achieves improved Time-to-First-Token (TTFT) and end-to-end (E2E) latency, while reducing the frequency of model evictions compared to state-of-the-art strategies. Ablation studies confirm the benefits and necessity of multi-factor considerations in model eviction.", "conclusion": "CACE provides a practical, effective context-aware model eviction strategy that enables scalable and low-latency deployment of AI coding assistants under constrained resources, offering a significant contribution for AI-assisted software engineering in real-world production settings."}}
{"id": "2506.18824", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18824", "abs": "https://arxiv.org/abs/2506.18824", "authors": ["Islem Bouzenia", "Michael Pradel"], "title": "Understanding Software Engineering Agents: A Study of Thought-Action-Result Trajectories", "comment": null, "summary": "Large Language Model (LLM)-based agents are increasingly employed to automate\ncomplex software engineering tasks such as program repair and issue resolution.\nThese agents operate by autonomously generating natural language thoughts,\ninvoking external tools, and iteratively refining their solutions. Despite\ntheir widespread adoption, the internal decision-making processes of these\nagents remain largely unexplored, limiting our understanding of their\noperational dynamics and failure modes. In this paper, we present a large-scale\nempirical study of the thought-action-result trajectories of three\nstate-of-the-art LLM-based agents: \\textsc{RepairAgent},\n\\textsc{AutoCodeRover}, and \\textsc{OpenHands}. We unify their interaction logs\ninto a common format, capturing 120 trajectories and 2822 LLM interactions\nfocused on program repair and issue resolution. Our study combines quantitative\nanalyses of structural properties, action patterns, and token usage with\nqualitative assessments of reasoning coherence and feedback integration. We\nidentify key trajectory characteristics such as iteration counts and token\nconsumption, recurring action sequences, and the semantic coherence linking\nthoughts, actions, and their results. Our findings reveal behavioral motifs and\nanti-patterns that distinguish successful from failed executions, providing\nactionable insights for improving agent design, including prompting strategies,\nfailure diagnosis, and anti-pattern detection. We release our dataset and\nannotation framework to support further research on transparent and robust\nautonomous software engineering agents.", "AI": {"tldr": "The paper empirically analyzes three LLM-based software engineering agents, revealing key behavioral patterns and anti-patterns that correlate with success or failure. Their insights inform improved agent design, and they release a dataset and framework for community use.", "motivation": "LLM-based agents are widely used for automating complex software engineering tasks, but their internal reasoning and decision-making processes are poorly understood. This gap hinders the improvement and reliability of such systems.", "method": "The authors conducted a large-scale empirical study by unifying the interaction logs of three advanced LLM-based agents (\textsc{RepairAgent}, \textsc{AutoCodeRover}, and \textsc{OpenHands}) into a common format. They analyzed 120 problem-solving trajectories and 2822 LLM-agent interactions, combining quantitative metrics (iteration count, token usage, action sequences) and qualitative assessments (reasoning and feedback coherence).", "result": "The study identified recurring behavioral patterns and anti-patterns in the agents' operation\u2014such as how thoughts, actions, and results are interconnected, and how iteration and token usage varies between successful and failed trajectories. They also highlight behavioral motifs and anti-patterns that help differentiate successes from failures.", "conclusion": "The findings provide actionable insights to improve LLM-based agent design, specifically in prompting strategies, diagnosing failures, and detecting failure-prone behavior. The authors also released their dataset and annotation framework to promote further research in this area."}}
