{"id": "2510.06296", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06296", "abs": "https://arxiv.org/abs/2510.06296", "authors": ["Lingfei Zeng", "Fengdi Che", "Xuhan Huang", "Fei Ye", "Xu Xu", "Binhang Yuan", "Jie Fu"], "title": "VeriEquivBench: An Equivalence Score for Ground-Truth-Free Evaluation of Formally Verifiable Code", "comment": null, "summary": "Formal verification is the next frontier for ensuring the correctness of code\ngenerated by Large Language Models (LLMs). While methods that co-generate code\nand formal specifications in formal languages, like Dafny, can, in principle,\nprove alignment with user intent, progress is bottlenecked by specification\nquality evaluation. Current benchmarks rely on matching against ground-truth\nspecifications, a manual and expertise-intensive process that has limited\nexisting datasets to a few hundred simple problems and also suffers from a\nreliability issue. To address this, we introduce VeriEquivBench, a new\nbenchmark with $2,389$ complex algorithmic problems that probe the limitations\nof current models in both code generation and formal reasoning. Our evaluation\nframework replaces ground-truth matching with a formally grounded metric, the\nequivalence score, and rigorously verifies the quality of generated\nspecifications and code. Our results show that generating formally verifiable\ncode remains a profound challenge for state-of-the-art LLMs. This underscores\nboth the difficulty of the task and the need for benchmarks like VeriEquivBench\nto drive progress toward scalable and reliable coding agents.", "AI": {"tldr": "Manually evaluating formal specifications limits progress in verifying LLM-generated code, so the authors introduce VeriEquivBench\u2014a large benchmark with an automated formal equivalence metric. Results show state-of-the-art LLMs struggle with verifiable code generation, stressing the need for better benchmarks and methods.", "motivation": "Current verification of LLM-generated code depends on manually created ground-truth formal specifications, which is time-consuming, expertise-intensive, and results in small and potentially unreliable datasets.", "method": "The authors introduce VeriEquivBench, a new benchmark containing 2,389 complex algorithmic problems. They replace manual ground-truth matching with an automated equivalence score as a formal metric to assess the correctness of generated code and specifications.", "result": "Experiments reveal that state-of-the-art LLMs struggle to generate code and formal specifications that can be formally verified, highlighting significant challenges.", "conclusion": "Generating formally verifiable code is still very difficult for current LLMs. The new VeriEquivBench benchmark and equivalence score framework provide more scalable and rigorous ways to assess and drive improvement in this area."}}
{"id": "2510.06343", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.06343", "abs": "https://arxiv.org/abs/2510.06343", "authors": ["Fikret Mert G\u00fcltekin", "Oscar Lilja", "Ranim Khojah", "Rebekka Wohlrab", "Marvin Damschen", "Mazen Mohamad"], "title": "Leveraging Large Language Models for Cybersecurity Risk Assessment -- A Case from Forestry Cyber-Physical Systems", "comment": "Accepted at Autonomous Agents in Software Engineering (AgenticSE)\n  Workshop, co-located with ASE 2025", "summary": "In safety-critical software systems, cybersecurity activities become\nessential, with risk assessment being one of the most critical. In many\nsoftware teams, cybersecurity experts are either entirely absent or represented\nby only a small number of specialists. As a result, the workload for these\nexperts becomes high, and software engineers would need to conduct\ncybersecurity activities themselves. This creates a need for a tool to support\ncybersecurity experts and engineers in evaluating vulnerabilities and threats\nduring the risk assessment process. This paper explores the potential of\nleveraging locally hosted large language models (LLMs) with retrieval-augmented\ngeneration to support cybersecurity risk assessment in the forestry domain\nwhile complying with data protection and privacy requirements that limit\nexternal data sharing. We performed a design science study involving 12 experts\nin interviews, interactive sessions, and a survey within a large-scale project.\nThe results demonstrate that LLMs can assist cybersecurity experts by\ngenerating initial risk assessments, identifying threats, and providing\nredundancy checks. The results also highlight the necessity for human oversight\nto ensure accuracy and compliance. Despite trust concerns, experts were willing\nto utilize LLMs in specific evaluation and assistance roles, rather than solely\nrelying on their generative capabilities. This study provides insights that\nencourage the use of LLM-based agents to support the risk assessment process of\ncyber-physical systems in safety-critical domains.", "AI": {"tldr": "Safety-critical software teams lack enough cybersecurity experts. The study shows that locally hosted LLMs can help with risk assessment tasks when privacy is important, but human oversight remains crucial. Experts see LLMs as assistants, not replacements, for evaluating threats and risks in domains like forestry.", "motivation": "Safety-critical software in domains like forestry require robust cybersecurity activities, specifically risk assessment. There's a lack of cybersecurity experts on software teams, putting high pressure on the few available and forcing non-experts to handle risk assessments. This motivates the development of tools to aid both experts and engineers, particularly in scenarios with data privacy constraints.", "method": "The paper uses a design science study involving 12 subject-matter experts. Methods include interviews, interactive sessions, and survey feedback, conducted within a large-scale project. The core technological method is leveraging locally hosted large language models with retrieval-augmented generation.", "result": "Locally hosted LLMs can help generate initial risk assessments, identify threats, and provide redundancy checks in safety-critical cybersecurity workflows, specifically in the forestry domain. Human oversight remains essential for ensuring accuracy and compliance, and while trust issues exist, experts are open to using LLMs for assistance but not total automation.", "conclusion": "LLMs, particularly when augmented and hosted locally to respect privacy constraints, can significantly support cybersecurity risk assessment but should be deployed as decision-support tools rather than replacements for human experts. The study provides evidence and encouragement for integrating LLMs into the risk assessment of cyber-physical systems in safety-critical environments."}}
{"id": "2510.06363", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06363", "abs": "https://arxiv.org/abs/2510.06363", "authors": ["Ololade Babatunde", "Tomisin Ayodabo", "Raqibul Raqibul"], "title": "Improving Assignment Submission in Higher Education through a Git-Enabled System: An Iterative Case Study", "comment": null, "summary": "This study addresses challenges in traditional assignment submission methods\nused in higher education by introducing and evaluating a customized Git-based\nsubmission system. Employing iterative software development and user-centered\ndesign methodologies, the system was integrated within a real-world university\nenvironment. Empirical evaluation, including usability testing and student\nfeedback, indicated significant improvements in assignment tracking,\ncollaboration, and submission efficiency. Students reported positive\nexperiences using distributed version control workflows, highlighting improved\nlearning outcomes and reduced administrative burden. Challenges related to\ninitial adoption and student learning curves were identified and mitigated\nthrough iterative improvements. The proposed system contributes practical\ninsights for integrating distributed version control into educational settings,\nenhancing both instructor oversight and student engagement in software\nengineering and related disciplines. Based on our results, the research showed\nthat 85% of instructors found the git based system easier to use, with 84% of\nstudents preferring it over traditional methods, as it provides a 38% reduction\nin time taken for submission and review, while also leading to a 48% reduction\nin storage requirements.", "AI": {"tldr": "A custom Git-based assignment submission system significantly improved the efficiency, collaboration, and user experience for both students and instructors in a university setting, reducing time and storage requirements while being highly preferred over traditional methods.", "motivation": "Traditional assignment submission methods in higher education have various challenges, such as inefficiency, difficulty in tracking assignments, limited collaboration opportunities, and administrative burden on both students and instructors.", "method": "The researchers designed and implemented a customized Git-based assignment submission system using iterative software development and user-centered design. The system was deployed in an actual university setting, and its effectiveness was evaluated through empirical methods, including usability testing and collecting student feedback.", "result": "Empirical evaluations showed significant improvements in assignment tracking, collaboration, and efficiency. 85% of instructors found the Git-based system easier to use, and 84% of students preferred it over traditional submission methods. The new system led to a 38% reduction in the time required for submissions and reviews and a 48% decrease in storage requirements. Challenges related to initial adoption and student learning curves were encountered but were successfully mitigated through iterative improvements.", "conclusion": "The customized Git-based submission system offers practical advantages for assignment management in educational settings. Its integration into coursework can enhance instructor oversight and student engagement, especially in software engineering and related fields, and serves as a model for using distributed version control in education."}}
{"id": "2510.06483", "categories": ["cs.SE", "D.2.1; D.2.2; K.4.2; D.2.13"], "pdf": "https://arxiv.org/pdf/2510.06483", "abs": "https://arxiv.org/abs/2510.06483", "authors": ["Judith Michael", "Lukas Netz", "Bernhard Rumpe", "Ingo M\u00fcller", "John Grundy", "Shavindra Wickramathilaka", "Hourieh Khalajzadeh"], "title": "Addressing Visual Impairments with Model-Driven Engineering: A Systematic Literature Review", "comment": "41 pages", "summary": "Software applications often pose barriers for users with accessibility needs,\ne.g., visual impairments. Model-driven engineering (MDE), with its systematic\nnature of code derivation, offers systematic methods to integrate accessibility\nconcerns into software development while reducing manual effort. This paper\npresents a systematic literature review on how MDE addresses accessibility for\nvision impairments. From 447 initially identified papers, 30 primary studies\nmet the inclusion criteria. About two-thirds reference the Web Content\nAccessibility Guidelines (WCAG), yet their project-specific adaptions and\nend-user validations hinder wider adoption in MDE. The analyzed studies model\nuser interface structures, interaction and navigation, user capabilities,\nrequirements, and context information. However, only few specify concrete\nmodeling techniques on how to incorporate accessibility needs or demonstrate\nfully functional systems. Insufficient details on MDE methods, i.e.,\ntransformation rules or code templates, hinder the reuse, generalizability, and\nreproducibility. Furthermore, limited involvement of affected users and limited\ndeveloper expertise in accessibility contribute to weak empirical validation.\nOverall, the findings indicate that current MDE research insufficiently\nsupports vision-related accessibility. Our paper concludes with a research\nagenda outlining how support for vision impairments can be more effectively\nembedded in MDE processes.", "AI": {"tldr": "A systematic review of model-driven engineering for accessibility in software finds that most current research falls short in supporting vision impairments due to vague techniques, limited empirical validation, and scarce user involvement. A new research agenda is proposed to address these gaps.", "motivation": "Software applications often lack adequate support for users with accessibility needs, such as vision impairments, creating barriers to usability. Model-driven engineering (MDE) could potentially streamline incorporating accessibility into software but it's unclear how effective current MDE approaches are.", "method": "The paper presents a systematic literature review, starting with 447 papers and narrowing down to 30 primary studies that focus on how MDE addresses accessibility for vision impairments. The review analyzes references to guidelines (e.g. WCAG), modeling practices, involvement of end-users, details about MDE techniques, and empirical validation.", "result": "About two-thirds of the studies reference WCAG, but project-specific adaptations and lack of end-user validation limit broader adoption. Few studies specify concrete MDE techniques or demonstrate fully functional systems. There is insufficient detail on MDE methods, hindering reuse and generalization. Limited involvement of affected users and developer expertise leads to weak empirical evidence.", "conclusion": "Current MDE research does not sufficiently support vision-related accessibility. The paper proposes a research agenda to better embed accessibility for vision impairments within MDE processes."}}
{"id": "2510.06606", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06606", "abs": "https://arxiv.org/abs/2510.06606", "authors": ["Uswat Yusuf", "Genevieve Caumartin", "Diego Elias Costa"], "title": "Beyond More Context: How Granularity and Order Drive Code Completion Quality", "comment": null, "summary": "Context plays an important role in the quality of code completion, as Large\nLanguage Models (LLMs) require sufficient and relevant information to assist\ndevelopers in code generation tasks. However, composing a relevant context for\ncode completion poses challenges in large repositories: First, the limited\ncontext length of LLMs makes it impractical to include all repository files.\nSecond, the quality of generated code is highly sensitive to noisy or\nirrelevant context. In this paper, we present our approach for the ASE 2025\nContext Collection Challenge. The challenge entails outperforming JetBrains\nbaselines by designing effective retrieval and context collection strategies.\nWe develop and evaluate a series of experiments that involve retrieval\nstrategies at both the file and chunk levels. We focus our initial experiments\non examining the impact of context size and file ordering on LLM performance.\nOur results show that the amount and order of context can significantly\ninfluence the performance of the models. We introduce chunk-based retrieval\nusing static analysis, achieving a 6% improvement over our best file-retrieval\nstrategy and a 16% improvement over the no-context baseline for Python in the\ninitial phase of the competition. Our results highlight the importance of\nretrieval granularity, ordering and hybrid strategies in developing effective\ncontext collection pipelines for real-world development scenarios.", "AI": {"tldr": "This paper tackles the challenge of improving LLM code completion quality in large code repositories by comparing various context retrieval strategies. Key findings show that chunk-based retrieval via static analysis outperforms file-level approaches, emphasizing the importance of granular and well-ordered context selection.", "motivation": "The motivation of this paper stems from the need to improve code completion quality by providing LLMs with sufficient, relevant context, while overcoming the constraints of context length and mitigating the impact of noisy or irrelevant information in large code repositories.", "method": "The paper develops and evaluates retrieval strategies for composing context, at both the repository file and code chunk levels. They conduct experiments to analyze the effect of context size and file ordering on LLM performance, and introduce chunk-based retrieval utilizing static analysis.", "result": "Chunk-based retrieval with static analysis yields a 6% improvement over the best file-level retrieval strategy and a 16% gain over the no-context baseline for Python, highlighting the significance of retrieval granularity and context ordering.", "conclusion": "Effective context collection strategies, especially those considering chunk-level granularity and smart ordering, can significantly enhance LLM-assisted code completion in large repositories. Retrieval granularity and hybrid strategies are crucial for real-world code generation tasks."}}
{"id": "2510.06708", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06708", "abs": "https://arxiv.org/abs/2510.06708", "authors": ["Aleksi Huotala", "Miikka Kuutila", "Olli-Pekka Turtio", "Mika M\u00e4ntyl\u00e4"], "title": "AISysRev -- LLM-based Tool for Title-abstract Screening", "comment": "4 pages", "summary": "Systematic reviews are a standard practice for summarizing the state of\nevidence in software engineering. Conducting systematic reviews is laborious,\nespecially during the screening or study selection phase, where the number of\npapers can be overwhelming. During this phase, papers are assessed against\ninclusion and exclusion criteria based on their titles and abstracts. Recent\nresearch has demonstrated that large language models (LLMs) can perform\ntitle-abstract screening at a level comparable to that of a master's student.\nWhile LLMs cannot be fully trusted, they can help, for example, in Rapid\nReviews, which try to expedite the review process. Building on recent research,\nwe developed AiSysRev, an LLM-based screening tool implemented as a web\napplication running in a Docker container. The tool accepts a CSV file\ncontaining paper titles and abstracts. Users specify inclusion and exclusion\ncriteria. One can use multiple LLMs for screening via OpenRouter. AiSysRev\nsupports both zero-shot and few-shot screening, and also allows for manual\nscreening through interfaces that display LLM results as guidance for human\nreviewers.We conducted a trial study with 137 papers using the tool. Our\nfindings indicate that papers can be classified into four categories: Easy\nIncludes, Easy Excludes, Boundary Includes, and Boundary Excludes. The Boundary\ncases, where LLMs are prone to errors, highlight the need for human\nintervention. While LLMs do not replace human judgment in systematic reviews,\nthey can significantly reduce the burden of assessing large volumes of\nscientific literature. Video: https://www.youtube.com/watch?v=jVbEj4Y4tQI Tool:\nhttps://github.com/EvoTestOps/AISysRev", "AI": {"tldr": "AiSysRev leverages LLMs to speed up systematic review screening, works well for clear cases, but still relies on humans for edge cases where AI might err.", "motivation": "Systematic literature reviews are labor-intensive, especially during the screening phase. LLMs can potentially expedite this process but require human oversight to avoid errors.", "method": "Developed AiSysRev, an LLM-based screening web tool for systematic reviews. Supports zero-shot/few-shot classification and multi-model screening via OpenRouter. Allows manual review with LLM guidance.", "result": "Trial study with 137 papers showed effective categorization into Easy Includes, Easy Excludes, Boundary Includes, and Boundary Excludes. LLMs reduce workload but are error-prone with boundary cases, needing human intervention.", "conclusion": "LLMs and tools like AiSysRev can meaningfully reduce the effort needed for systematic review screening, improving efficiency, but cannot fully replace human experts for final decisions."}}
{"id": "2510.06718", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06718", "abs": "https://arxiv.org/abs/2510.06718", "authors": ["Ranim Khojah", "Mazen Mohamad", "Linda Erlenhov", "Francisco Gomes de Oliveira Neto", "Philipp Leitner"], "title": "LLM Company Policies and Policy Implications in Software Organizations", "comment": "Accepted at IEEE Software Special Issue on AIware in the Foundation\n  Models Era", "summary": "The risks associated with adopting large language model (LLM) chatbots in\nsoftware organizations highlight the need for clear policies. We examine how 11\ncompanies create these policies and the factors that influence them, aiming to\nhelp managers safely integrate chatbots into development workflows.", "AI": {"tldr": "This paper studies how 11 software companies create policies for adopting LLM chatbots, identifying influencing factors and offering insights for managers to safely integrate chatbots into workflows.", "motivation": "There are significant risks when integrating LLM chatbots into software organizations, and these risks necessitate the creation of clear usage policies.", "method": "The paper examines the approaches of 11 companies in developing LLM chatbot policies. It investigates the influencing factors behind policy creation.", "result": "The study reveals how different companies formulate their chatbot adoption policies, highlighting the key factors that managers must consider for safe integration.", "conclusion": "Clear, well-considered policies are essential for the responsible adoption of LLM chatbots in software organizations, and understanding how others approach policy creation benefits safe integration."}}
{"id": "2510.06844", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06844", "abs": "https://arxiv.org/abs/2510.06844", "authors": ["Nicole Hoess", "Carlos Paradis", "Rick Kazman", "Wolfgang Mauerer"], "title": "Oops!... I did it again. Conclusion (In-)Stability in Quantitative Empirical Software Engineering: A Large-Scale Analysis", "comment": null, "summary": "Context: Mining software repositories is a popular means to gain insights\ninto a software project's evolution, monitor project health, support decisions\nand derive best practices. Tools supporting the mining process are commonly\napplied by researchers and practitioners, but their limitations and agreement\nare often not well understood.\n  Objective: This study investigates some threats to validity in complex tool\npipelines for evolutionary software analyses and evaluates the tools' agreement\nin terms of data, study outcomes and conclusions for the same research\nquestions.\n  Method: We conduct a lightweight literature review to select three studies on\ncollaboration and coordination, software maintenance and software quality from\nhigh-ranked venues, which we formally replicate with four independent,\nsystematically selected mining tools to quantitatively and qualitatively\ncompare the extracted data, analysis results and conclusions.\n  Results: We find that numerous technical details in tool design and\nimplementation accumulate along the complex mining pipelines and can cause\nsubstantial differences in the extracted baseline data, its derivatives,\nsubsequent results of statistical analyses and, under specific circumstances,\nconclusions.\n  Conclusions: Users must carefully choose tools and evaluate their limitations\nto assess the scope of validity in an adequate way. Reusing tools is\nrecommended. Researchers and tool authors can promote reusability and help\nreducing uncertainties by reproduction packages and comparative studies\nfollowing our approach.", "AI": {"tldr": "Mining software repositories with different tools can lead to significant differences in data and conclusions due to technical details. Researchers should reuse tools, document reproducibility, and compare results to ensure valid insights.", "motivation": "Mining software repositories is common for understanding project evolution, health, and best practices. However, the limitations and agreement between different mining tools are not well understood.", "method": "The study conducted a lightweight literature review to select three prior studies and formally replicated them using four independent mining tools. Data, results, and conclusions from these tools were quantitatively and qualitatively compared.", "result": "Technical differences in tool design and implementation can cause substantial discrepancies in data extraction, analysis results, and study conclusions.", "conclusion": "Users must carefully select and evaluate tools for mining software repositories, considering their limitations. Tool reuse and reproducibility are recommended, with reproduction packages and comparative studies helping reduce uncertainties."}}
{"id": "2510.06984", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06984", "abs": "https://arxiv.org/abs/2510.06984", "authors": ["Masanari Kondo", "Mahmoud Alfadel", "Shane McIntosh", "Yasutaka Kamei", "Naoyasu Ubayashi"], "title": "An empirical study on declined proposals: why are these proposals declined?", "comment": null, "summary": "Design-level decisions in open-source software (OSS) projects are often made\nthrough structured mechanisms such as proposals, which require substantial\ncommunity discussion and review. Despite their importance, the proposal process\nis resource-intensive and often leads to contributor frustration, especially\nwhen proposals are declined without clear feedback. Yet, the reasons behind\nproposal rejection remain poorly understood, limiting opportunities to\nstreamline the process or guide contributors effectively. This study\ninvestigates the characteristics and outcomes of proposals in the Go\nprogramming language to understand why proposals are declined and how such\noutcomes might be anticipated. We conduct a mixed-method empirical study on\n1,091 proposals submitted to the Go project. We quantify proposal outcomes,\nbuild a taxonomy of decline reasons, and evaluate large language models (LLMs)\nfor predicting these outcomes. We find that proposals are more often declined\nthan accepted, and resolution typically takes over a month. Only 14.7% of\ndeclined proposals are ever resubmitted. Through qualitative coding, we\nidentify nine key reasons for proposal decline, such as duplication, limited\nuse cases, or violations of project principles. This taxonomy can help\ncontributors address issues in advance, e.g., checking for existing\nalternatives can reduce redundancy. We also demonstrate that GPT-based models\ncan predict decline decisions early in the discussion (F1 score = 0.71 with\npartial comments), offering a practical tool for prioritizing review effort.\nOur findings reveal inefficiencies in the proposal process and highlight\nactionable opportunities for improving both contributor experience and reviewer\nworkload by enabling early triage and guiding contributors to strengthen their\nproposals using a structured understanding of past decline reasons.", "AI": {"tldr": "This paper analyzes proposal outcomes in Go development, finds most are declined for identifiable reasons, and shows that AI tools can help predict and prioritize which proposals will be declined early, enabling more effective and efficient management of the process.", "motivation": "The paper is motivated by the resource-intensive nature of design-level proposal processes in open-source software (OSS), particularly the Go language, and the lack of clear understanding behind proposal rejections. This gap limits contributors' ability to improve proposals and streamlines the review process.", "method": "A mixed-method empirical study of 1,091 proposals submitted to the Go project. Quantitative analysis tracks outcomes, while qualitative coding builds a taxonomy of decline reasons. Evaluation of large language models (LLMs), specifically GPT-based models, for predicting proposal outcomes early in the discussion.", "result": "Proposals in Go are more often declined than accepted, with decision-making taking over a month typically. Only 14.7% of declined proposals are resubmitted. Nine key decline reasons were identified (e.g., duplication, limited use cases), and GPT-based models can effectively predict decline outcomes early (F1 score = 0.71), aiding review prioritization.", "conclusion": "Proposal review in OSS, especially Go, is inefficient, with many declined proposals and valuable but underused resubmission opportunities. Structured decline reason taxonomy, combined with predictive tools like LLMs, can improve contributor experience and reviewer workload by enabling earlier, more informed triage and guidance for stronger proposals."}}
{"id": "2510.06989", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06989", "abs": "https://arxiv.org/abs/2510.06989", "authors": ["Pengyue Yang", "Haolin Jin", "Qingwen Zeng", "Jiawen Wen", "Harry Rao", "Huaming Chen"], "title": "Human-aligned AI Model Cards with Weighted Hierarchy Architecture", "comment": "10 pages, 5 figures. Submitted to ICSE SEIP 2026 (Software\n  Engineering in Practice)", "summary": "The proliferation of Large Language Models (LLMs) has led to a burgeoning\necosystem of specialized, domain-specific models. While this rapid growth\naccelerates innovation, it has simultaneously created significant challenges in\nmodel discovery and adoption. Users struggle to navigate this landscape due to\ninconsistent, incomplete, and imbalanced documentation across platforms.\nExisting documentation frameworks, such as Model Cards and FactSheets, attempt\nto standardize reporting but are often static, predominantly qualitative, and\nlack the quantitative mechanisms needed for rigorous cross-model comparison.\nThis gap exacerbates model underutilization and hinders responsible adoption.\nTo address these shortcomings, we introduce the Comprehensive Responsible AI\nModel Card Framework (CRAI-MCF), a novel approach that transitions from static\ndisclosures to actionable, human-aligned documentation. Grounded in Value\nSensitive Design (VSD), CRAI-MCF is built upon an empirical analysis of 240\nopen-source projects, distilling 217 parameters into an eight-module,\nvalue-aligned architecture. Our framework introduces a quantitative sufficiency\ncriterion to operationalize evaluation and enables rigorous cross-model\ncomparison under a unified scheme. By balancing technical, ethical, and\noperational dimensions, CRAI-MCF empowers practitioners to efficiently assess,\nselect, and adopt LLMs with greater confidence and operational integrity.", "AI": {"tldr": "The paper presents CRAI-MCF, an improved model documentation framework for LLMs that enables robust, quantitative, value-based comparison and supports ethical, responsible adoption.", "motivation": "The rapid growth of specialized LLMs has made it difficult for users to find, assess, and responsibly adopt models due to inconsistent and incomplete documentation. Existing frameworks do not sufficiently support rigorous comparison.", "method": "The authors introduce CRAI-MCF, grounded in Value Sensitive Design and empirical analysis of 240 open-source projects. They distilled 217 parameters into an eight-module architecture with a quantitative sufficiency criterion for evaluation.", "result": "CRAI-MCF enables actionable, human-aligned documentation, balancing technical, ethical, and operational aspects. It supports rigorous cross-model comparison under a unified framework and facilitates more confident, responsible adoption of LLMs.", "conclusion": "CRAI-MCF addresses critical gaps in model documentation by providing a comprehensive, value-aligned, and quantitative solution that supports responsible and efficient model selection and adoption."}}
{"id": "2510.07070", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.07070", "abs": "https://arxiv.org/abs/2510.07070", "authors": ["Gopi Krishnan Rajbahadur", "Keheliya Gallaba", "Elyas Rashno", "Arthit Suriyawongkul", "Karen Bennet", "Kate Stewart", "Ahmed E. Hassan"], "title": "Building an Open AIBOM Standard in the Wild", "comment": null, "summary": "Modern software engineering increasingly relies on open, community-driven\nstandards, yet how such standards are created in fast-evolving domains like\nAI-powered systems remains underexplored. This paper presents a detailed\nexperience report on the development of the AI Bill of Materials AIBOM\nspecification, an extension of the ISO/IEC 5962:2021 Software Package Data\nExchange (SPDX) software bill of materials (SBOM) standard, which captures AI\ncomponents such as datasets and iterative training artifacts. Framed through\nthe lens of Action Research (AR), we document a global, multi-stakeholder\neffort involving over 90 contributors and structured AR cycles. The resulting\nspecification was validated through four complementary approaches: alignment\nwith major regulations and ethical standards (e.g., EU AI Act and IEEE 7000\nstandards), systematic mapping to six industry use cases, semi-structured\npractitioner interviews, and an industrial case study. Beyond delivering a\nvalidated artefact, our paper documents the process of building the AIBOM\nspecification in the wild, and reflects on how it aligns with the AR cycle, and\ndistills lessons that can inform future standardization efforts in the software\nengineering community.", "AI": {"tldr": "This paper reports on how a global team created and validated a new standard, AIBOM, to document AI-specific components in software supply chains using the Action Research method. They detail the challenges, validation steps, and lessons that can help others build standards for emerging fields.", "motivation": "Modern software projects increasingly depend on open, community-driven standards, especially as domains like AI systems evolve rapidly. However, the process and challenges of creating such standards in these fast-moving fields are not well understood.", "method": "The authors use Action Research (AR) methodology, involving structured AR cycles and contributions from over 90 global stakeholders. They document the process of developing the AI Bill of Materials (AIBOM) specification through these cycles. The new specification is evaluated via four methods: alignment with existing regulations and ethical standards, mapping to industry use cases, practitioner interviews, and a case study in industry.", "result": "The result of the study is the validated AIBOM specification for software bill of materials in AI systems, which extends the existing SPDX standard to include AI-specific elements like datasets and training artifacts. The validation uses regulatory mapping, use cases, interviews, and a practical application. The paper also provides insights and lessons learned from the standardization process.", "conclusion": "The paper delivers both a validated standard for documenting AI components in software supply chains and a detailed report on the collaborative, iterative process that created it, offering guidance for future standards development in similar domains."}}
{"id": "2510.07189", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.07189", "abs": "https://arxiv.org/abs/2510.07189", "authors": ["Junjie Li", "Fazle Rabbi", "Bo Yang", "Song Wang", "Jinqiu Yang"], "title": "Prompt, Synthesize, Fine-Tune: A Secure Code Generation Recipe", "comment": null, "summary": "Although Large Language Models (LLMs) show promising solutions to automated\ncode generation, they often produce insecure code that threatens software\nsecurity. Current approaches (e.g., SafeCoder) to improve secure code\ngeneration suffer from limited and imbalanced datasets, reducing their\neffectiveness and generalizability. In this work, we present Secure-Instruct, a\nnovel framework that automatically synthesizes high-quality vulnerable and\nsecure code examples, generates fine-tuning instructions, and instruction-tunes\nLLMs to align task description and secure code generation abilities. We\nevaluate Secure-Instruct on four representative LLMs using two benchmarks: our\nown CWEBench and the existing CWEval. CWEBench comprises 93 scenarios on 44\nCWEs, all without overlap with Secure-Instruct's synthetic instruction-tuning\ndataset, while CWEval covers 31 CWEs with 119 manually verified\nsecurity-critical tasks. We find that Secure-Instruct improves not only the\nsecurity but also the functional correctness of the generated code. On\nCWEBench, Secure-Instruct substantially improves secure code generation, giving\na 14.3% average increase in secure ratio over the pretrained models and\noutperforms SafeCoder by 7.6%. On CWEval, Secure-Instruct achieves a 14%\nincrease for CodeLlama-7B and 5.8% for Mistral-7B in Func-Sec@1 over pretrained\nmodels, and surpasses SafeCoder by 15.8% and 6.8% respectively.", "AI": {"tldr": "Secure-Instruct automatically synthesizes code examples and instructions to fine-tune LLMs, enhancing both the security and correctness of code generation. It consistently outperforms prior methods like SafeCoder on security-critical benchmarks.", "motivation": "LLMs often generate insecure code, which poses risks to software security. Existing solutions (like SafeCoder) struggle with dataset limitations and imbalanced examples, hindering their ability to generalize and improve secure code generation.", "method": "The paper introduces Secure-Instruct, a framework that automatically synthesizes both vulnerable and secure code examples, generates fine-tuning instructions, and then uses these to instruction-tune LLMs, improving both task alignment and secure coding capabilities.", "result": "Secure-Instruct leads to significant improvements in secure code generation and functional correctness across multiple benchmarks and models. For example, on the CWEBench benchmark it achieves a 14.3% increase in secure code generation ratio over pretrained models and outperforms SafeCoder by 7.6%. Similar gains are noted on CWEval with CodeLlama-7B and Mistral-7B models.", "conclusion": "Secure-Instruct is an effective framework for enhancing LLM-based secure code generation, surpassing current approaches in both security and correctness across various tasks and models."}}
