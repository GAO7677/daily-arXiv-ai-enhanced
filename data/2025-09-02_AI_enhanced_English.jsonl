{"id": "2508.21097", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21097", "abs": "https://arxiv.org/abs/2508.21097", "authors": ["Nazanin Siavash", "Armin Moin"], "title": "Model-Driven Quantum Code Generation Using Large Language Models and Retrieval-Augmented Generation", "comment": "This paper is accepted to the New Ideas and Emerging Results (NIER)\n  track of the ACM/IEEE 28th International Conference on Model Driven\n  Engineering Languages and Systems (MODELS)", "summary": "This paper introduces a novel research direction for model-to-text/code\ntransformations by leveraging Large Language Models (LLMs) that can be enhanced\nwith Retrieval-Augmented Generation (RAG) pipelines. The focus is on quantum\nand hybrid quantum-classical software systems, where model-driven approaches\ncan help reduce the costs and mitigate the risks associated with the\nheterogeneous platform landscape and lack of developers' skills. We validate\none of the proposed ideas regarding generating code out of UML model instances\nof software systems. This Python code uses a well-established library, called\nQiskit, to execute on gate-based or circuit-based quantum computers. The RAG\npipeline that we deploy incorporates sample Qiskit code from public GitHub\nrepositories. Experimental results show that well-engineered prompts can\nimprove CodeBLEU scores by up to a factor of four, yielding more accurate and\nconsistent quantum code. However, the proposed research direction can go beyond\nthis through further investigation in the future by conducting experiments to\naddress our other research questions and ideas proposed here, such as deploying\nsoftware system model instances as the source of information in the RAG\npipelines, or deploying LLMs for code-to-code transformations, for instance,\nfor transpilation use cases.", "AI": {"tldr": "This paper explores the use of LLMs with a RAG pipeline to automatically generate quantum code from software models, achieving significant improvements in code quality and highlighting future expansion opportunities.", "motivation": "Quantum and hybrid quantum-classical software systems are complex due to heterogeneous platforms and a shortage of skilled developers. Model-driven approaches can help address these challenges.", "method": "The authors validate a model-to-code generation technique using Large Language Models (LLMs) enhanced with a Retrieval-Augmented Generation (RAG) pipeline. Specifically, they transform UML model instances into Python code utilizing the Qiskit library, with RAG incorporating example code from public GitHub repositories.", "result": "Experimental results demonstrate that well-crafted prompts in the RAG-augmented LLM pipeline can improve CodeBLEU scores by up to four times, resulting in more accurate and consistent quantum code.", "conclusion": "The integration of LLMs with RAG for model-to-code transformations is promising for quantum software development. Further research could extend these techniques to use system model instances as RAG sources or for code-to-code transformations, including transpilation."}}
{"id": "2508.21107", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21107", "abs": "https://arxiv.org/abs/2508.21107", "authors": ["Dongjun Lee", "Changho Hwang", "Kimin Lee"], "title": "Learning to Generate Unit Test via Adversarial Reinforcement Learning", "comment": "Code is available at: https://github.com/dgjun32/UTRL", "summary": "Unit testing is a core practice in programming, enabling systematic\nevaluation of programs produced by human developers or large language models\n(LLMs). Given the challenges in writing comprehensive unit tests, LLMs have\nbeen employed to automate test generation, yet methods for training LLMs to\nproduce high-quality tests remain underexplored. In this work, we propose UTRL,\na novel reinforcement learning framework that trains an LLM to generate\nhigh-quality unit tests given a programming instruction. Our key idea is to\niteratively train two LLMs, the unit test generator and the code generator, in\nan adversarial manner via reinforcement learning. The unit test generator is\ntrained to maximize a discrimination reward, which reflects its ability to\nproduce tests that expose faults in the code generator's solutions, and the\ncode generator is trained to maximize a code reward, which reflects its ability\nto produce solutions that pass the unit tests generated by the test generator.\nIn our experiments, we demonstrate that unit tests generated by Qwen3-4B\ntrained via UTRL show higher quality compared to unit tests generated by the\nsame model trained via supervised fine-tuning on human-written ground-truth\nunit tests, yielding code evaluations that more closely align with those\ninduced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL\noutperforms frontier models such as GPT-4.1 in generating high-quality unit\ntests, highlighting the effectiveness of UTRL in training LLMs for this task.", "AI": {"tldr": "UTRL is a reinforcement learning-based framework that adversarially trains LLMs to generate better unit tests. It outperforms standard approaches and even top models like GPT-4.1 in test quality.", "motivation": "Writing comprehensive unit tests for programs is challenging, and while LLMs are used to automate test generation, there is limited exploration into training methods for LLMs to produce high-quality tests. This work addresses the need for better approaches to train LLMs for effective unit test generation.", "method": "UTRL is a novel reinforcement learning framework where two LLMs\u2014a unit test generator and a code generator\u2014are iteratively trained in an adversarial way. The test generator maximizes a reward for finding faults in code produced by the code generator, while the code generator maximizes a reward for passing the generated tests.", "result": "Experiments show that Qwen3-4B trained via UTRL produces higher-quality unit tests than standard supervised fine-tuning on human-written tests. Additionally, Qwen3-4B with UTRL surpasses even advanced models like GPT-4.1 in generating effective unit tests.", "conclusion": "UTRL demonstrates superior performance in training LLMs to generate comprehensive unit tests, both compared to traditional supervised methods and leading models like GPT-4.1."}}
{"id": "2508.21156", "categories": ["cs.SE", "D.2.7; I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.21156", "abs": "https://arxiv.org/abs/2508.21156", "authors": ["Kiana Kiashemshaki", "Arsham Khosravani", "Alireza Hosseinpour", "Arshia Akhavan"], "title": "Automated Bug Triaging using Instruction-Tuned Large Language Models", "comment": "11 pages, 7 figures", "summary": "Bug triaging, the task of assigning new issues to developers, is often slow\nand inconsistent in large projects. We present a lightweight framework that\ninstruction-tuned large language model (LLM) with LoRA adapters and uses\ncandidate-constrained decoding to ensure valid assignments. Tested on\nEclipseJDT and Mozilla datasets, the model achieves strong shortlist quality\n(Hit at 10 up to 0.753) despite modest exact Top-1 accuracy. On recent\nsnapshots, accuracy rises sharply, showing the framework's potential for\nreal-world, human-in-the-loop triaging. Our results suggest that\ninstruction-tuned LLMs offer a practical alternative to costly feature\nengineering and graph-based methods.", "AI": {"tldr": "A new framework using instruction-tuned LLMs and LoRA adapters improves bug triaging efficiency and accuracy, providing a strong alternative to traditional, resource-intensive methods.", "motivation": "Bug triaging is slow and inconsistent in large software projects, making it difficult to efficiently assign new issues to developers. Traditional approaches involve costly feature engineering or complex graph-based methods.", "method": "The paper proposes a lightweight framework that uses an instruction-tuned large language model (LLM) with LoRA adapters and employs candidate-constrained decoding to ensure valid developer assignments.", "result": "The framework demonstrates strong shortlist quality on EclipseJDT and Mozilla datasets (Hit at 10 up to 0.753), although exact Top-1 accuracy remains modest. However, accuracy improves significantly on recent data snapshots, highlighting its adaptability.", "conclusion": "Instruction-tuned LLMs with LoRA adapters represent a viable and practical alternative to traditional bug triaging approaches, especially when used in real-world, human-in-the-loop scenarios."}}
{"id": "2508.21433", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21433", "abs": "https://arxiv.org/abs/2508.21433", "authors": ["Tobias Lindenbauer", "Igor Slinko", "Ludwig Felder", "Egor Bogomolov", "Yaroslav Zharov"], "title": "The Complexity Trap: Simple Observation Masking Is as Efficient as LLM Summarization for Agent Context Management", "comment": null, "summary": "Large Language Model (LLM)-based agents solve complex tasks through iterative\nreasoning, exploration, and tool-use, a process that can result in long,\nexpensive context histories. While state-of-the-art Software Engineering ( SE)\nagents like OpenHands or Cursor use LLM-based summarization to tackle this\nissue, it is unclear whether the increased complexity offers tangible\nperformance benefits compared to simply omitting older observations. We present\na systematic comparison of these strategies within SWE-agent on SWE-bench\nVerified across five diverse model configurations. We find that a simple\nobservation-masking strategy halves cost relative to a raw agent while\nmatching, and sometimes slightly exceeding, the solve rate of LLM\nsummarization. For example, with Qwen3-Coder 480B, masking improves solve rate\nfrom 53.8% (raw agent) to 54.8%, while remaining competitive with summarization\nat a lower cost. These results suggest that, at least within SWE-agent on\nSWE-bench Verified, the most effective and efficient context management can be\nthe simplest. We release code and data for reproducibility", "AI": {"tldr": "Simple observation-masking as a context management strategy in LLM-based Software Engineering agents matches or slightly exceeds the performance of more complex LLM summarization approaches, while significantly reducing computational cost. ", "motivation": "LLM-based agents generate long context histories during complex tasks, leading to high computational cost. Existing SE agents use summarization as a mitigation, but it's unclear if this added complexity is justified over simpler approaches.", "method": "Systematic comparison of context management strategies\u2014specifically, LLM-based summarization versus simple observation-masking\u2014within the SWE-agent framework on the SWE-bench Verified benchmark, across five model configurations.", "result": "Observation-masking halves computational cost relative to using raw agent context, while matching or sometimes slightly outperforming LLM summarization in solve rate. Example: Qwen3-Coder 480B achieves 54.8% solve rate with masking (vs. 53.8% raw, competitive with summarization) at a lower cost.", "conclusion": "In the SWE-agent context, simple observation-masking provides equally (or slightly more) effective and considerably more efficient context management than more complex LLM summarization strategies. Code and data are released for reproducibility."}}
{"id": "2508.21256", "categories": ["cs.PL", "cs.CL", "cs.GR", "68N20, 68N15, 68W10", "D.3.4; D.3.2; D.1.3"], "pdf": "https://arxiv.org/pdf/2508.21256", "abs": "https://arxiv.org/abs/2508.21256", "authors": ["Nripesh Niketan", "Vaatsalya Shrivastva"], "title": "CrossTL: A Universal Programming Language Translator with Unified Intermediate Representation", "comment": "15 Pages, 5 Figures, 1 Table. Introduces CrossTL, a universal\n  programming language translator enabling bidirectional translation between 8\n  programming languages (CUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan\n  SPIR-V, Rust, Mojo) through a unified intermediate representation called\n  CrossGL. Includes comprehensive evaluation with complex real-world examples", "summary": "We present CrossTL, a universal programming language translator enabling\nbidirectional translation between multiple languages through a unified\nintermediate representation called CrossGL. Traditional approaches require\nseparate translators for each language pair, leading to exponential complexity\ngrowth. CrossTL uses a single universal IR to facilitate translations between\nCUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan SPIR-V, Rust, and Mojo,\nwith Slang support in development. Our system consists of: language-specific\nlexers/parsers converting source code to ASTs, bidirectional CrossGL\ntranslation modules implementing ToCrossGLConverter classes for importing code\nand CodeGen classes for target generation, and comprehensive backend\nimplementations handling full translation pipelines. We demonstrate\neffectiveness through comprehensive evaluation across programming domains,\nachieving successful compilation and execution across all supported backends.\nThe universal IR design enables adding new languages with minimal effort,\nrequiring only language-specific frontend/backend components. Our contributions\ninclude: (1) a unified IR capturing semantics of multiple programming\nparadigms, (2) a modular architecture enabling extensibility, (3) a\ncomprehensive framework supporting GPU compute, graphics programming, and\nsystems languages, and (4) empirical validation demonstrating practical\nviability of universal code translation. CrossTL represents a significant step\ntoward language-agnostic programming, enabling write-once, deploy-everywhere\ndevelopment.", "AI": {"tldr": "CrossTL is a universal programming language translator that uses a unified intermediate representation (CrossGL) to enable easy, bidirectional code translation among various languages (GPU, graphics, systems). Its modular design greatly simplifies supporting new languages and achieves successful compilation and execution across domains.", "motivation": "Current code translation approaches require a separate tool for every language pair, creating exponential complexity when adding support for new languages. The field lacks a universal, scalable solution for translating code between multiple, diverse programming languages efficiently.", "method": "The authors introduce CrossTL, which uses a single, unified intermediate representation (CrossGL). Their system employs language-specific lexers and parsers to translate source code into ASTs, a set of ToCrossGLConverter classes to import code into the CrossGL IR, and CodeGen classes for code generation targeting various languages. This modular pipeline makes translation between any supported language pair possible via the universal IR, and adding new languages requires only creating corresponding frontend/backend modules.", "result": "CrossTL successfully translates and compiles code across a wide variety of programming domains and languages (e.g., CUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan SPIR-V, Rust, Mojo). Tests confirm that translated code runs as intended across all backends. The IR-based design proves extensible and practical for supporting new languages and paradigms with minimal extra work.", "conclusion": "CrossTL demonstrates that a universal IR-based, modular architecture can enable practical, extensible, and accurate translation between diverse programming languages. This paves the way for language-agnostic, write-once, deploy-anywhere development across platforms and paradigms."}}
{"id": "2508.21454", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.21454", "abs": "https://arxiv.org/abs/2508.21454", "authors": ["Baijun Cheng", "Kailong Wang", "Ling Shi", "Haoyu Wang", "Yao Guo", "Ding Li", "Xiangqun Chen"], "title": "Enhancing Semantic Understanding in Pointer Analysis using Large Language Models", "comment": "Accepted by LMPL 2025", "summary": "Pointer analysis has been studied for over four decades. However, existing\nframeworks continue to suffer from the propagation of incorrect facts. A major\nlimitation stems from their insufficient semantic understanding of code,\nresulting in overly conservative treatment of user-defined functions. Recent\nadvances in large language models (LLMs) present new opportunities to bridge\nthis gap. In this paper, we propose LMPA (LLM-enhanced Pointer Analysis), a\nvision that integrates LLMs into pointer analysis to enhance both precision and\nscalability. LMPA identifies user-defined functions that resemble system APIs\nand models them accordingly, thereby mitigating erroneous cross-calling-context\npropagation. Furthermore, it enhances summary-based analysis by inferring\ninitial points-to sets and introducing a novel summary strategy augmented with\nnatural language. Finally, we discuss the key challenges involved in realizing\nthis vision.", "AI": {"tldr": "This paper proposes LMPA, a framework leveraging LLMs to improve pointer analysis by better handling user-defined functions, increasing accuracy and scalability, and outlines future challenges.", "motivation": "Existing pointer analysis frameworks are limited due to their lack of semantic understanding of code, resulting in imprecise and conservative analysis of user-defined functions.", "method": "The authors propose LMPA (LLM-enhanced Pointer Analysis), a framework that incorporates large language models (LLMs) to identify and model user-defined functions similar to system APIs, and to enhance summary-based analysis via initial points-to set inference and a natural-language-augmented summary strategy.", "result": "LMPA mitigates erroneous propagation of facts across calling contexts and improves both the precision and scalability of pointer analysis. The challenges of implementing this vision are also discussed.", "conclusion": "Incorporating LLMs into pointer analysis addresses key limitations of existing methods, especially with respect to user-defined functions, but challenges remain in practical realization."}}
{"id": "2508.21593", "categories": ["cs.PL", "cs.MS", "math.HO"], "pdf": "https://arxiv.org/pdf/2508.21593", "abs": "https://arxiv.org/abs/2508.21593", "authors": ["Anne Baanen", "Matthew Robert Ballard", "Johan Commelin", "Bryan Gin-ge Chen", "Michael Rothgang", "Damiano Testa"], "title": "Growing Mathlib: maintenance of a large scale mathematical library", "comment": "21 pages, 1 figure. To appear at Conference on Intelligent Computer\n  Mathematics (CICM) 2025", "summary": "The Lean mathematical library Mathlib is one of the fastest-growing libraries\nof formalised mathematics. We describe various strategies to manage this\ngrowth, while allowing for change and avoiding maintainer overload. This\nincludes dealing with breaking changes via a deprecation system, using code\nquality analysis tools (linters) to provide direct user feedback about common\npitfalls, speeding up compilation times through conscious library (re-)design,\ndealing with technical debt as well as writing custom tooling to help with the\nreview and triage of new contributions.", "AI": {"tldr": "This paper discusses how the Mathlib community manages rapid library growth by implementing deprecation controls, quality checks, design optimizations, debt management, and custom contribution tools to keep maintenance efficient and sustainable.", "motivation": "The paper addresses challenges posed by the rapid growth of the Lean mathematical library Mathlib, particularly focusing on how to manage this expansion without overwhelming maintainers and while supporting ongoing changes.", "method": "The approach includes implementing deprecation systems to manage breaking changes, deploying code quality tools (linters) for immediate feedback, optimizing (re-)design to enhance compilation speeds, tackling technical debt, and developing custom review and triage tooling.", "result": "By using these strategies, the Mathlib community has managed to effectively cope with rapid growth, maintained code quality, and reduced maintainer burden.", "conclusion": "The combination of systematic project management, technical enhancements, and custom tooling offers a scalable way to handle growth and change in large formal mathematics libraries like Mathlib."}}
{"id": "2508.21553", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.21553", "abs": "https://arxiv.org/abs/2508.21553", "authors": ["J\u00f8rn Eirik Betten", "Quentin Mazouni", "Dennis Gross", "Pedro Lind", "Helge Spieker"], "title": "Reusable Test Suites for Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) agents show great promise in solving sequential\ndecision-making tasks. However, validating the reliability and performance of\nthe agent policies' behavior for deployment remains challenging. Most\nreinforcement learning policy testing methods produce test suites tailored to\nthe agent policy being tested, and their relevance to other policies is\nunclear. This work presents Multi-Policy Test Case Selection (MPTCS), a novel\nautomated test suite selection method for RL environments, designed to extract\ntest cases generated by any policy testing framework based on their\nsolvability, diversity, and general difficulty. MPTCS uses a set of policies to\nselect a diverse collection of reusable policy-agnostic test cases that reveal\ntypical flaws in the agents' behavior. The set of policies selects test cases\nfrom a candidate pool, which can be generated by any policy testing method,\nbased on a difficulty score. We assess the effectiveness of the difficulty\nscore and how the method's effectiveness and cost depend on the number of\npolicies in the set. Additionally, a method for promoting diversity in the test\nsuite, a discretized general test case descriptor surface inspired by\nquality-diversity algorithms, is examined to determine how it covers the state\nspace and which policies it triggers to produce faulty behaviors.", "AI": {"tldr": "MPTCS is a new framework for creating automated, reusable, and diverse test suites for RL agents, leveraging multiple policies and difficulty scoring. It improves reliability testing by finding agent flaws more effectively and generalizing test cases beyond single-policy approaches.", "motivation": "Validating the reliability and performance of reinforcement learning agents before deployment is challenging, especially because existing test suites are typically tailored to specific policies and may not generalize to others.", "method": "The paper proposes Multi-Policy Test Case Selection (MPTCS), an automated method for selecting test suites in RL environments. MPTCS identifies test cases based on their solvability, diversity, and general difficulty, regardless of the policy used to generate them. It uses a pool of policies to extract diverse, reusable, policy-agnostic test cases using a difficulty score, and incorporates a test case descriptor surface inspired by quality-diversity algorithms to ensure state space coverage and trigger policy faults.", "result": "The study demonstrates the effectiveness of the difficulty score for test case selection and analyzes how the number of policies used affects both the effectiveness and cost of MPTCS. The proposed diversity-enhancing method successfully increases coverage of the state space and identifies faulty policy behaviors across different agents.", "conclusion": "MPTCS provides a robust mechanism for generating reusable, diverse, and policy-agnostic test suites, improving the reliability assessment of a wide range of RL agents and facilitating broader, more reliable testing for deployment."}}
{"id": "2508.21634", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.21634", "abs": "https://arxiv.org/abs/2508.21634", "authors": ["Domenico Cotroneo", "Cristina Improta", "Pietro Liguori"], "title": "Human-Written vs. AI-Generated Code: A Large-Scale Study of Defects, Vulnerabilities, and Complexity", "comment": "Accepted to the 36th IEEE International Symposium on Software\n  Reliability Engineering (ISSRE, 2025)", "summary": "As AI code assistants become increasingly integrated into software\ndevelopment workflows, understanding how their code compares to human-written\nprograms is critical for ensuring reliability, maintainability, and security.\nIn this paper, we present a large-scale comparison of code authored by human\ndevelopers and three state-of-the-art LLMs, i.e., ChatGPT, DeepSeek-Coder, and\nQwen-Coder, on multiple dimensions of software quality: code defects, security\nvulnerabilities, and structural complexity. Our evaluation spans over 500k code\nsamples in two widely used languages, Python and Java, classifying defects via\nOrthogonal Defect Classification and security vulnerabilities using the Common\nWeakness Enumeration. We find that AI-generated code is generally simpler and\nmore repetitive, yet more prone to unused constructs and hardcoded debugging,\nwhile human-written code exhibits greater structural complexity and a higher\nconcentration of maintainability issues. Notably, AI-generated code also\ncontains more high-risk security vulnerabilities. These findings highlight the\ndistinct defect profiles of AI- and human-authored code and underscore the need\nfor specialized quality assurance practices in AI-assisted programming.", "AI": {"tldr": "AI code assistants produce simpler, more repetitive code but are more prone to security vulnerabilities compared to human-written code. Each has unique quality issues, requiring tailored QA practices for safe, effective use of AI in programming.", "motivation": "With the widespread adoption of AI code assistants in software development, there's a critical need to understand how AI-generated code compares to human-written code, particularly regarding software quality, reliability, maintainability, and security.", "method": "The paper conducts a large-scale comparison of over 500,000 code samples written by human developers and three leading large language models (ChatGPT, DeepSeek-Coder, Qwen-Coder) in Python and Java. It evaluates code defects (via Orthogonal Defect Classification), security vulnerabilities (using the Common Weakness Enumeration), and structural complexity.", "result": "AI-generated code is overall simpler and more repetitive, with more unused constructs and hardcoded debugging information. Human-written code, on the other hand, is structurally more complex and tends to have more maintainability issues. Notably, AI-generated code contains a higher rate of high-risk security vulnerabilities.", "conclusion": "There are clear and distinct differences in the software quality profiles between AI-generated and human-written code. AI code tends to be simpler but presents specific risks, especially concerning security vulnerabilities, thus necessitating specialized quality assurance approaches for AI-assisted programming."}}
{"id": "2508.21811", "categories": ["cs.SE", "68", "D.2.9"], "pdf": "https://arxiv.org/pdf/2508.21811", "abs": "https://arxiv.org/abs/2508.21811", "authors": ["Ashley Hourigan", "Ridewaan Hanslo"], "title": "The Integration of Agile Methodologies in DevOps Practices within the Information Technology Industry", "comment": "10 pages, 2 figures, conference", "summary": "The demand for rapid software delivery in the Information Technology (IT)\nindustry has significantly intensified, emphasising the need for faster\nsoftware products and service releases with enhanced features to meet customer\nexpectations. Agile methodologies are replacing traditional approaches such as\nWaterfall, where flexibility, iterative development and adaptation to change\nare favoured over rigid planning and execution. DevOps, a subsequent evolution\nfrom Agile, emphasises collaborative efforts in development and operations\nteams, focusing on continuous integration and deployment to deliver resilient\nand high-quality software products and services. This study aims to critically\nassess both Agile and DevOps practices in the IT industry to identify the\nfeasibility and applicability of Agile methods in DevOps practices. Eleven\nsemi-structured interviews were conducted with Agile and DevOps practitioners\nin varying capacities across several sectors within the IT industry. Through\nthematic analysis, 51 unique codes were extracted and synthesised into 19\nthemes that reported on each phase of the DevOps lifecycle, specifically\nregarding the integration and implementation of Agile methods into DevOps\npractices. Based on the findings, a new understanding detailing the\ninterrelationship of Agile methods in DevOps practices was discussed that met\nthe research objectives.", "AI": {"tldr": "Growing software delivery speed expectations have driven a transition from Waterfall to Agile, and further to DevOps practices in IT. Through interviews and thematic analysis, the paper maps how Agile methods are implemented in DevOps, revealing a nuanced interrelationship. The study provides actionable insights into merging Agile and DevOps to better meet industry demands.", "motivation": "The motivation behind this paper is the need for faster software delivery in the IT industry to meet heightened customer expectations for rapid, feature-rich product releases. This trend has led to a shift from traditional software development approaches like Waterfall to Agile methodologies and further towards DevOps, which promises increased collaboration and continuous delivery.", "method": "The paper conducted eleven semi-structured interviews with practitioners of Agile and DevOps from various IT sectors. Thematic analysis was employed, extracting 51 unique codes, which were synthesized into 19 key themes reflecting the phases of the DevOps lifecycle and the integration of Agile methods.", "result": "The study identified nuanced themes related to the feasibility and applicability of Agile methods within DevOps practices. It provided a synthesized understanding of how Agile methods are integrated and implemented throughout the DevOps lifecycle, meeting the research objectives.", "conclusion": "The paper concludes with a new understanding of the interrelationship between Agile methods and DevOps practices, providing insights into the effective integration of Agile into DevOps within the IT industry, thus responding better to industry needs for rapid, high-quality software delivery."}}
