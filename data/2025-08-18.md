<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 10]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [The Impact of Large Language Models (LLMs) on Code Review Process](https://arxiv.org/abs/2508.11034)
*Antonio Collante,Samuel Abedu,SayedHassan Khatoonabadi,Ahmad Abdellatif,Ebube Alor,Emad Shihab*

Main category: cs.SE

TL;DR: Using GPT in GitHub PR workflows dramatically speeds up code review and acceptance—resolution times drop by more than 60%, especially by optimizing code, fixing bugs, and updating documentation. This highlights clear productivity gains and practical workflow improvements for software teams.


<details>
  <summary>Details</summary>
Motivation: While large language models are increasingly used in software development, their specific effects during different phases of code review, especially in GitHub pull request workflows, are not well understood. The study seeks to fill this gap by quantifying how LLM assistance (specifically GPT) affects efficiency and outcomes in PR processes.

Method: The authors curated a large dataset of over 25,000 GitHub pull requests from more than 9,000 projects. They identified GPT-assisted PRs using a heuristic that combines keyword-based detection, regular expression filtering, and manual verification to ensure high labeling accuracy. They then used statistical modeling techniques, including multiple linear regression and the Mann-Whitney U test, to analyze differences between GPT-assisted and non-assisted PRs, both in terms of total resolution time and particular review phases.

Result: GPT-assisted pull requests saw a median resolution time reduction of over 60% (9 hours vs. 23 hours). Review times dropped by 33%, and waiting times before acceptance decreased by 87%. Usage analysis of 300 GPT-assisted PRs showed GPT was primarily used for code optimization (60%), bug fixing (26%), and documentation (12%).

Conclusion: Incorporating GPT into PR workflows can greatly improve efficiency and productivity, mainly by reducing resolution and waiting times. The study provides key insights for teams aiming to leverage LLMs to streamline code review and foster better collaboration.

Abstract: Large language models (LLMs) have recently gained prominence in the field of
software development, significantly boosting productivity and simplifying
teamwork. Although prior studies have examined task-specific applications, the
phase-specific effects of LLM assistance on the efficiency of code review
processes remain underexplored. This research investigates the effect of GPT on
GitHub pull request (PR) workflows, with a focus on reducing resolution time,
optimizing phase-specific performance, and assisting developers. We curated a
dataset of 25,473 PRs from 9,254 GitHub projects and identified GPT-assisted
PRs using a semi-automated heuristic approach that combines keyword-based
detection, regular expression filtering, and manual verification until
achieving 95% labeling accuracy. We then applied statistical modeling,
including multiple linear regression and Mann-Whitney U test, to evaluate
differences between GPT-assisted and non-assisted PRs, both at the overall
resolution level and across distinct review phases. Our research has revealed
that early adoption of GPT can substantially boost the effectiveness of the PR
process, leading to considerable time savings at various stages. Our findings
suggest that GPT-assisted PRs reduced median resolution time by more than 60%
(9 hours compared to 23 hours for non-assisted PRs). We discovered that
utilizing GPT can reduce the review time by 33% and the waiting time before
acceptance by 87%. Analyzing a sample dataset of 300 GPT-assisted PRs, we
discovered that developers predominantly use GPT for code optimization (60%),
bug fixing (26%), and documentation updates (12%). This research sheds light on
the impact of the GPT model on the code review process, offering actionable
insights for software teams seeking to enhance workflows and promote seamless
collaboration.

</details>


### [2] [Diffusion is a code repair operator and generator](https://arxiv.org/abs/2508.11110)
*Mukul Singh,Gust Verbruggen,Vu Le,Sumit Gulwani*

Main category: cs.SE

TL;DR: The paper shows code diffusion models can fix broken code snippets ('last-mile repair') and efficiently produce training data for such tasks, validated on Python, Excel, and PowerShell experiments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve 'last-mile' code repair—the final modifications needed to fix broken or incomplete code—by exploiting the capabilities of code diffusion models.

Method: The paper explores using pre-trained code diffusion models for last-mile code repair in two ways: (1) by adding noise to broken code and then allowing the diffusion model to repair it via its denoising process, and (2) by generating large amounts of synthetic training data for repair tasks using the diffusion process.

Result: Experiments are performed on three domains: Python, Excel, and PowerShell. The results demonstrate the potential of both leveraging the diffusion process for repairing code and for creating training data for such tasks.

Conclusion: Pre-trained code diffusion models can be successfully adapted to last-mile code repair and data generation tasks, offering promising results across multiple programming domains.

Abstract: Code diffusion models generate code by iteratively removing noise from the
latent representation of a code snippet. During later steps of the diffusion
process, when the code snippet has almost converged, differences between
discrete representations of these snippets look like last-mile repairs applied
to broken or incomplete code. We evaluate the extent to which this resemblance
can be exploited to leverage pre-trained code diffusion models for the problem
of last-mile repair by considering two applications with significant potential.
First, we can leverage the diffusion model for last-mile repair by adding noise
to a broken code snippet and resuming the diffusion process. Second, we can
leverage the diffusion model to generate arbitrary amount of training data for
last-mile repair tasks (that are computationally more efficient) by sampling an
intermediate program (input) and the final program (output) from the diffusion
process. We perform experiments on 3 domains (Python, Excel and PowerShell) to
evaluate applications, as well as analyze properties.

</details>


### [3] [AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities](https://arxiv.org/abs/2508.11126)
*Huanting Wang,Jingzhi Gong,Huawei Zhang,Zheng Wang*

Main category: cs.SE

TL;DR: This comprehensive survey reviews AI agentic programming, providing a taxonomy, technical overview, and analysis of evaluation methods. It highlights current limitations and research challenges while suggesting directions for building more capable and reliable AI coding agents.


<details>
  <summary>Details</summary>
Motivation: AI agentic programming, where LLMs autonomously handle complex software tasks, is a rapidly developing field. Its capabilities are poised to significantly impact software development, but its scope, technical foundations, and research challenges need to be clarified and organized as the field evolves.

Method: The paper conducts a comprehensive survey, presenting a taxonomy of agent behaviors and system architectures. It reviews core techniques (planning, memory/context management, tool integration, execution monitoring), benchmarks, and evaluation methodologies. The study synthesizes recent research advances and discusses challenges and future opportunities.

Result: The survey identifies key challenges for AI agentic programming, such as issues with handling long context, limited persistent memory, safety and alignment concerns, and difficulties collaborating with humans. It also highlights existing benchmarks and evaluation practices, and points out emerging opportunities to enhance reliability, adaptability, and transparency.

Conclusion: This survey consolidates the state of AI agentic programming by reviewing techniques, challenges, and evaluation methods. It creates a foundational reference to guide future research and development in building more advanced and trustworthy AI coding agents.

Abstract: AI agentic programming is an emerging paradigm in which large language models
(LLMs) autonomously plan, execute, and interact with external tools like
compilers, debuggers, and version control systems to iteratively perform
complex software development tasks. Unlike conventional code generation tools,
agentic systems are capable of decomposing high-level goals, coordinating
multi-step processes, and adapting their behavior based on intermediate
feedback. These capabilities are transforming the software development
practice. As this emerging field evolves rapidly, there is a need to define its
scope, consolidate its technical foundations, and identify open research
challenges. This survey provides a comprehensive and timely review of AI
agentic programming. We introduce a taxonomy of agent behaviors and system
architectures, and examine core techniques including planning, memory and
context management, tool integration, and execution monitoring. We also analyze
existing benchmarks and evaluation methodologies used to assess coding agent
performance. Our study identifies several key challenges, including limitations
in handling long context, a lack of persistent memory across tasks, and
concerns around safety, alignment with user intent, and collaboration with
human developers. We discuss emerging opportunities to improve the reliability,
adaptability, and transparency of agentic systems. By synthesizing recent
advances and outlining future directions, this survey aims to provide a
foundation for research and development in building the next generation of
intelligent and trustworthy AI coding agents.

</details>


### [4] [From Feedback to Failure: Automated Android Performance Issue Reproduction](https://arxiv.org/abs/2508.11147)
*Zhengquan Li,Zhenhao Li,Zishuo Ding*

Main category: cs.SE

TL;DR: RevPerf uses Google Play app reviews and prompt engineering to automatically reproduce and detect mobile performance issues, achieving a 70% success rate.


<details>
  <summary>Details</summary>
Motivation: Mobile app performance greatly impacts user experience, but performance issues are hard to detect and diagnose in development environments. Traditional approaches do not leverage real user feedback or automated reproduction of such problems.

Method: The authors propose RevPerf, a tool that extracts relevant information from Google Play app reviews using prompt engineering, enriches those reviews with performance issue details, and uses an execution agent to generate and execute commands to reproduce reported issues. Detection involves monitoring logs, GUI changes, and resource usage.

Result: RevPerf was experimentally evaluated on a constructed dataset and achieved a 70% success rate in reproducing performance issues that were manually validated.

Conclusion: RevPerf efficiently utilizes user reviews to automate the reproduction and detection of mobile app performance issues, significantly increasing the ability to diagnose real-world problems that affect user experience.

Abstract: Mobile application performance is a vital factor for user experience. Yet,
performance issues are notoriously difficult to detect within development
environments, where their manifestations are often less conspicuous and
diagnosis proves more challenging. To address this limitation, we propose
RevPerf, an advanced performance issue reproduction tool that leverages app
reviews from Google Play to acquire pertinent information. RevPerf employs
relevant reviews and prompt engineering to enrich the original review with
performance issue details. An execution agent is then employed to generate and
execute commands to reproduce the issue. After executing all necessary steps,
the system incorporates multifaceted detection methods to identify performance
issues by monitoring Android logs, GUI changes, and system resource utilization
during the reproduction process. Experimental results demonstrate that our
proposed framework achieves a 70\% success rate in reproducing performance
issues on the dataset we constructed and manually validated.

</details>


### [5] [PTMPicker: Facilitating Efficient Pretrained Model Selection for Application Developers](https://arxiv.org/abs/2508.11179)
*Pei Liu,Terry Zhuo,Jiawei Deng,Zhenchang Xing,Qinghua Lu,Xiaoning Du,Hongyu Zhan*

Main category: cs.SE

TL;DR: PTMPicker is a new system for finding pretrained models that goes beyond keyword search by using a structured format and embedding similarities. It better matches user needs including technical and legal constraints, achieving high success rates in experiments.


<details>
  <summary>Details</summary>
Motivation: Selecting appropriate pretrained models (PTMs) is challenging due to limitations of keyword-based search that often miss user intent and requirements like bias, hardware, or license constraints.

Method: The authors propose PTMPicker, which introduces a structured template of essential PTM attributes. It represents both models and user requests in this format, matches function-related features via embedding similarity, and addresses special constraints (e.g., license, hardware) using tailored prompts. The system processes over 500k models and synthesizes over 15k search requests for evaluation.

Result: PTMPicker was evaluated on a large dataset of scraped models and synthesized search requests. It successfully identified suitable models for 85% of sampled requests within the top-10 search results.

Conclusion: PTMPicker significantly improves the relevance and effectiveness of PTM search over traditional keyword-based approaches, especially when special requirements matter.

Abstract: The rapid emergence of pretrained models (PTMs) has attracted significant
attention from both Deep Learning (DL) researchers and downstream application
developers. However, selecting appropriate PTMs remains challenging because
existing methods typically rely on keyword-based searches in which the keywords
are often derived directly from function descriptions. This often fails to
fully capture user intent and makes it difficult to identify suitable models
when developers also consider factors such as bias mitigation, hardware
requirements, or license compliance. To address the limitations of
keyword-based model search, we propose PTMPicker to accurately identify
suitable PTMs. We first define a structured template composed of common and
essential attributes for PTMs and then PTMPicker represents both candidate
models and user-intended features (i.e., model search requests) in this unified
format. To determine whether candidate models satisfy user requirements, it
computes embedding similarities for function-related attributes and uses
well-crafted prompts to evaluate special constraints such as license compliance
and hardware requirements. We scraped a total of 543,949 pretrained models from
Hugging Face to prepare valid candidates for selection. PTMPicker then
represented them in the predefined structured format by extracting their
associated descriptions. Guided by the extracted metadata, we synthesized a
total of 15,207 model search requests with carefully designed prompts, as no
such search requests are readily available. Experiments on the curated PTM
dataset and the synthesized model search requests show that PTMPicker can help
users effectively identify models,with 85% of the sampled requests successfully
locating appropriate PTMs within the top-10 ranked candidates.

</details>


### [6] [ORFuzz: Fuzzing the "Other Side" of LLM Safety -- Testing Over-Refusal](https://arxiv.org/abs/2508.11222)
*Haonan Zhang,Dongxia Wang,Yi Liu,Kexin Chen,Jiashui Wang,Xinlei Ying,Long Liu,Wenhai Wang*

Main category: cs.SE

TL;DR: Over-refusal is a major problem in LLMs, where benign queries are wrongly rejected. Existing approaches to identify this problem are inadequate. The paper presents ORFuzz, a novel, automated framework with enhanced test generation and human-aligned evaluation to systematically detect over-refusal. ORFuzz produces more diverse and effective tests than prior methods, and its benchmark dataset sets a new standard for evaluating LLM reliability.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) often excessively refuse benign user requests due to overly stringent safety measures, which reduces their reliability and usability. Existing evaluation methods and benchmarks are not sufficient to adequately detect and analyze this issue.

Method: The paper introduces ORFuzz, an evolutionary testing framework designed to systematically uncover and analyze over-refusal behavior in LLMs. ORFuzz comprises three main components: (1) safety category-aware seed selection for diverse test coverage, (2) adaptive mutator optimization using reasoning LLMs to generate challenging test cases, and (3) OR-Judge, a human-aligned evaluation model for perceiving toxicity and refusal accuracy.

Result: Experimental results show that ORFuzz generates validated over-refusal instances at a rate of 6.98% on average, more than double that of previous methods. ORFuzz also produced a new benchmark dataset, ORFuzzSet, containing 1,855 transferable test cases with a 63.56% average over-refusal rate across 10 LLMs, substantially outperforming existing datasets.

Conclusion: ORFuzz and the ORFuzzSet benchmark offer an effective automated approach to detecting and diagnosing over-refusal in LLMs, contributing critical tools and resources to the community for improving the reliability and trustworthiness of LLM-based applications.

Abstract: Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously
rejecting benign queries due to overly conservative safety measures - a
critical functional flaw that undermines their reliability and usability.
Current methods for testing this behavior are demonstrably inadequate,
suffering from flawed benchmarks and limited test generation capabilities, as
highlighted by our empirical user study. To the best of our knowledge, this
paper introduces the first evolutionary testing framework, ORFuzz, for the
systematic detection and analysis of LLM over-refusals. ORFuzz uniquely
integrates three core components: (1) safety category-aware seed selection for
comprehensive test coverage, (2) adaptive mutator optimization using reasoning
LLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge
model validated to accurately reflect user perception of toxicity and refusal.
Our extensive evaluations demonstrate that ORFuzz generates diverse, validated
over-refusal instances at a rate (6.98% average) more than double that of
leading baselines, effectively uncovering vulnerabilities. Furthermore,
ORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly
transferable test cases that achieves a superior 63.56% average over-refusal
rate across 10 diverse LLMs, significantly outperforming existing datasets.
ORFuzz and ORFuzzSet provide a robust automated testing framework and a
valuable community resource, paving the way for developing more reliable and
trustworthy LLM-based software systems.

</details>


### [7] [Hallucination in LLM-Based Code Generation: An Automotive Case Study](https://arxiv.org/abs/2508.11257)
*Marc Pavel,Nenad Petrovic,Lukasz Mazur,Vahid Zolfaghari,Fengjunjie Pan,Alois Knoll*

Main category: cs.SE

TL;DR: The study finds that state-of-the-art LLMs frequently hallucinate in automotive code generation, with syntax and API errors common. Only context-rich prompts reliably produce correct code, indicating safer model use requires better mitigation techniques and explicit input.


<details>
  <summary>Details</summary>
Motivation: LLMs are promising for automated code generation, especially in software engineering. Their adoption is limited by hallucinations—plausible yet incorrect or nonsensical outputs—which is particularly concerning in safety-critical domains like automotive software.

Method: A case study evaluates several code-focused LLMs (GPT-4.1, Codex, GPT-4o) using three levels of prompt complexity: a simple one-liner, prompt with Covesa VSS context, and prompt with additional code skeleton.

Result: High hallucination rates were observed as syntax violations, invalid references, and API conflicts. Only GPT-4.1 and GPT-4o, with most detailed prompts, generated correct solutions. Simpler prompts failed even after refinements.

Conclusion: Context-rich prompting is essential for reliable code generation with current LLMs, especially for critical domains. Existing models still produce frequent errors unless very explicit information is provided, highlighting the urgent need for mitigation strategies.

Abstract: Large Language Models (LLMs) have shown significant potential in automating
code generation tasks offering new opportunities across software engineering
domains. However, their practical application remains limited due to
hallucinations - outputs that appear plausible but are factually incorrect,
unverifiable or nonsensical. This paper investigates hallucination phenomena in
the context of code generation with a specific focus on the automotive domain.
A case study is presented that evaluates multiple code LLMs for three different
prompting complexities ranging from a minimal one-liner prompt to a prompt with
Covesa Vehicle Signal Specifications (VSS) as additional context and finally to
a prompt with an additional code skeleton. The evaluation reveals a high
frequency of syntax violations, invalid reference errors and API knowledge
conflicts in state-of-the-art models GPT-4.1, Codex and GPT-4o. Among the
evaluated models, only GPT-4.1 and GPT-4o were able to produce a correct
solution when given the most context-rich prompt. Simpler prompting strategies
failed to yield a working result, even after multiple refinement iterations.
These findings highlight the need for effective mitigation techniques to ensure
the safe and reliable use of LLM generated code, especially in safety-critical
domains such as automotive software systems.

</details>


### [8] [Defects4Log: Benchmarking LLMs for Logging Code Defect Detection and Reasoning](https://arxiv.org/abs/2508.11305)
*Xin Wang,Zhenhao Li,Zishuo Ding*

Main category: cs.SE

TL;DR: This paper presents a thorough taxonomy and benchmark for logging code defects, showing that LLMs are limited when working with only source code but are noticeably improved with scenario-based guidance, raising accuracy by 10.9%. The work informs practitioners and future research on enhancing LLMs for logging code quality.


<details>
  <summary>Details</summary>
Motivation: Defects in logging code can negatively impact debugging and system monitoring, but prior studies lack comprehensive analyses and do not fully explore LLMs' potential in defect detection.

Method: The authors create a comprehensive taxonomy of logging code defects (seven patterns, 14 scenarios) and construct a developer-verified benchmark dataset. They then propose an automated framework that uses various prompting and contextual strategies to evaluate large language models (LLMs) in detecting and reasoning about logging defects.

Result: LLMs struggle with accurate detection and reasoning on source code alone, but providing them with targeted knowledge about defect scenarios boosts detection accuracy by 10.9%.

Conclusion: Systematic knowledge incorporation can improve LLM performance in logging code defect detection. The study offers guidance for practitioners to avoid defects and sets a foundation for future improvements in LLM-powered reasoning for logging defects.

Abstract: Logging code is written by developers to capture system runtime behavior and
plays a vital role in debugging, performance analysis, and system monitoring.
However, defects in logging code can undermine the usefulness of logs and lead
to misinterpretations. Although prior work has identified several logging
defect patterns and provided valuable insights into logging practices, these
studies often focus on a narrow range of defect patterns derived from limited
sources (e.g., commit histories) and lack a systematic and comprehensive
analysis. Moreover, large language models (LLMs) have demonstrated promising
generalization and reasoning capabilities across a variety of code-related
tasks, yet their potential for detecting logging code defects remains largely
unexplored.
  In this paper, we derive a comprehensive taxonomy of logging code defects,
which encompasses seven logging code defect patterns with 14 detailed
scenarios. We further construct a benchmark dataset, \dataset, consisting of
164 developer-verified real-world logging defects. Then we propose an automated
framework that leverages various prompting strategies and contextual
information to evaluate LLMs' capability in detecting and reasoning logging
code defects. Experimental results reveal that LLMs generally struggle to
accurately detect and reason logging code defects based on the source code
only. However, incorporating proper knowledge (e.g., detailed scenarios of
defect patterns) can lead to 10.9\% improvement in detection accuracy. Overall,
our findings provide actionable guidance for practitioners to avoid common
defect patterns and establish a foundation for improving LLM-based reasoning in
logging code defect detection.

</details>


### [9] [TRACY: Benchmarking Execution Efficiency of LLM-Based Code Translation](https://arxiv.org/abs/2508.11468)
*Zhihao Gong,Zeyu Sun,Dong Huang,Qingyuan Liang,Jie M. Zhang,Dan Hao*

Main category: cs.SE

TL;DR: TRACY is a new benchmark for measuring efficiency in LLM-generated code translation, revealing major models struggle with performance despite high correctness. Efficiency flaws like bad algorithms and resource handling slow down code drastically. Future LLMs need joint optimization for accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Existing large language models (LLMs) have greatly improved the correctness of automatic code translation, but ignore execution efficiency, which is crucial in real-world software development.

Method: The authors introduce TRACY, a new benchmark specifically designed to evaluate the execution efficiency of LLM-generated code translations. TRACY uses a two-stage LLM-driven pipeline: it first generates stress tests to highlight performance differences, then prunes tasks to focus on those that best distinguish efficiency.

Result: TRACY consists of 1,011 code translation tasks across three languages, with thorough reference translations and demanding efficiency tests. Evaluation of 26 LLMs shows that top models for correctness, like Claude-4-think, perform poorly in efficiency, and smaller open-source models can outperform them. The study identifies algorithmic flaws and poor resource management as causes of severe efficiency losses.

Conclusion: The study reveals a significant gap in execution efficiency among LLM-translated code and stresses the importance of optimizing both correctness and efficiency. TRACY provides the first comprehensive benchmark to drive future improvements.

Abstract: Automatic code translation is a fundamental task in modern software
development. While the advent of Large Language Models (LLMs) has significantly
improved the correctness of code translation, the critical dimension of
execution efficiency remains overlooked. To address this gap, we introduce
TRACY, the first comprehensive benchmark designed to evaluate the execution
efficiency of LLM-translated code. TRACY is constructed through an LLM-driven
two-stage pipeline: an initial stage generates a suite of stress tests to
amplify performance differences, followed by an efficiency-oriented task
pruning stage that isolates the efficiency-distinguishing tasks. The resulting
benchmark comprises 1,011 code translation tasks across C++, Java, and Python,
each accompanied by an average of 22.1 verified reference translations and 10
computationally demanding tests. Our extensive evaluation of 26 representative
LLMs reveals that even top-tier LLMs struggle to consistently produce efficient
code translations. For instance, Claude-4-think, the leading model for
correctness, ranks eighth overall when time efficiency is taken into account,
surpassed by several smaller open-source models. We further pinpoint that
algorithmic flaws and improper resource handling are the most detrimental,
causing a median time slowdown of 5.6$\times$ and memory increase of
12.0$\times$, respectively. Our work underscores the necessity of jointly
optimizing for correctness and efficiency in future LLM-based code translation.

</details>


### [10] [Temporal Network Analysis of Microservice Architectural Degradation](https://arxiv.org/abs/2508.11571)
*Alexander Bakhtin*

Main category: cs.SE

TL;DR: This paper explores using temporal network analysis for studying microservice architectures but finds limitations in data collection and analysis due to small scale and few time points.


<details>
  <summary>Details</summary>
Motivation: Microservice systems are dynamic, changing across releases or during operation. The traditional static analysis may not capture their evolving dependencies. The paper aims to leverage temporal network analysis to better understand these systems.

Method: The paper discusses obtaining temporal networks by analyzing service dependency graphs over time, either through system releases or runtime tracing. It then explores application of temporal network science methods to these networks.

Result: The most comprehensive temporal network constructed had only 7 time points and 42 microservices, which constrained the depth of achievable analysis.

Conclusion: There are significant challenges both in collecting sufficient temporal data from microservice systems and in applying meaningful temporal network analysis due to limited scale and temporal resolution.

Abstract: Microservice architecture can be modeled as a network of microservices making
calls to each other, commonly known as the service dependency graph. Network
Science can provide methods to study such networks. In particular, temporal
network analysis is a branch of Network Science that analyzes networks evolving
with time. In microservice systems, temporal networks can arise if we examine
the architecture of the system across releases or monitor a deployed system
using tracing.
  In this research summary paper, I discuss the challenges in obtaining
temporal networks from microservice systems and analyzing them with the
temporal network methods. In particular, the most complete temporal network
that we could obtain contains 7 time instances and 42 microservices, which
limits the potential analysis that could be applied.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [11] [Generic Reduction-Based Interpreters (Extended Version)](https://arxiv.org/abs/2508.11297)
*Casper Bach*

Main category: cs.PL

TL;DR: Generic programming techniques can greatly reduce the boilerplate code required to implement reduction-based interpreters, making their development easier and more efficient.


<details>
  <summary>Details</summary>
Motivation: Reduction-based interpreters are systematic to implement but demand significant boilerplate code, making development inefficient.

Method: The paper applies generic programming techniques to the design and implementation of reduction-based interpreters, aiming to minimize repetitive boilerplate.

Result: By leveraging generic programming, the authors demonstrate a method to significantly reduce boilerplate code in such interpreters.

Conclusion: Implementing reduction-based interpreters becomes more efficient and concise with the proposed generic programming strategy, streamlining the engineering process.

Abstract: Reduction-based interpreters are traditionally defined in terms of a one-step
reduction function which systematically decomposes a term into a potential
redex and context, contracts the redex, and recomposes it to construct the new
term to be further reduced. While implementing such interpreters follows a
systematic recipe, they often require interpreter engineers to write a
substantial amount of code -- much of it boilerplate. In this paper, we apply
well-known techniques from generic programming to reduce boilerplate code in
reduction-based interpreters.

</details>


### [12] [Towards Efficient Hash Maps in Functional Array Languages](https://arxiv.org/abs/2508.11443)
*William Henrich Due,Martin Elsman,Troels Henriksen*

Main category: cs.PL

TL;DR: This paper develops an efficient, collision-free data-parallel hash map algorithm for GPUs, implemented in Futhark. While outperforming traditional approaches, it still trails behind cuCollections due to compiler and programming model limitations. The authors suggest language model extensions to improve performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create efficient, collision-free, static hash maps suitable for parallel execution, addressing challenges in providing flexible and polymorphic interfaces in functional array languages, and solving issues related to dynamically sized keys.

Method: The authors provide a functional formulation of the Fredman et al. hash map construction, systematically derive a data-parallel implementation, and flatten the algorithm for execution in the Futhark language. They address the challenge of dynamically sized keys using associated contexts. GPU performance benchmarks are used for evaluation and comparison.

Result: Their implementation in Futhark is superior to tree/search-based methods but lags behind cuCollections in construction speed and, to a lesser extent, lookups. The performance gap arises from Futhark compiler limitations and the absence of specific constructs in data-parallel programming.

Conclusion: The paper concludes that their data-parallel hash map implementation in Futhark outperforms conventional tree/search-based approaches but falls short compared to the cuCollections library, especially in hash map construction. The limitations stem from the Futhark compiler's code generation and the intrinsic constraints of data-parallel programming. They propose that extending the functional array language model may address these weaknesses.

Abstract: We present a systematic derivation of a data-parallel implementation of
two-level, static and collision-free hash maps, by giving a functional
formulation of the Fredman et al. construction, and then flattening it. We
discuss the challenges of providing a flexible, polymorphic, and abstract
interface to hash maps in a functional array language, with particular
attention paid to the problem of dynamically sized keys, which we address by
associating each hash map with an arbitrary context. The algorithm is
implemented in Futhark, and the achieved GPU execution performance is compared
on simple benchmark problems. We find that our hash maps outperform
conventional tree/search-based approaches. Furthermore, our implementation is
compared against the state-of-the-art cuCollections library, which is
significantly faster for hash map construction, and to a lesser degree for
lookups. We explain to which extent the performance difference is due to
low-level code generation limitation in the Futhark compiler, and to which
extent it can be attributed to the data-parallel programming vocabulary not
providing the constructs necessary to express the equivalent of the algorithms
used by cuCollections. We end by reflecting to which extent the functional
array language programming model could, or should, be extended to address these
weaknesses.

</details>
