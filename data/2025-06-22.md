<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Towards Bug-Free Distributed Go Programs](https://arxiv.org/abs/2506.15135)
*Zhengqun Koo*

Main category: cs.SE

TL;DR: The paper introduces a verification tool that statically detects and prevents communication races in Go programs using message passing, making distributed programs more reliable.


<details>
  <summary>Details</summary>
Motivation: Reasoning about concurrency in distributed systems is challenging, leading to bugs due to unexpected races. While data race detection in shared memory systems is well studied, communication races in message-passing systems remain problematic and less explored.

Method: The paper presents a verification framework targeting distributed programs written in a subset of Go. The approach statically analyzes programs to prove the absence of communication races by extending a happens-before order to both buffered and unbuffered channels.

Result: The framework enables static reasoning about program execution in Go-based distributed systems, providing guarantees against communication races in programs synchronized through message passing.

Conclusion: This work offers a formal verification method that helps programmers ensure communication safety in distributed Go programs by statically proving the absence of communication races, thus preventing difficult concurrency bugs.

Abstract: Programmers of distributed systems need to reason about concurrency to avoid
races. However, reasoning about concurrency is difficult, and unexpected races
show up as bugs. Data race detection in shared memory systems is well-studied
(dynamic data race detection [13], behavioral types [15], dynamic race
detection [31]). Similar to how a data race consists of reads and writes not
related by happens-before at a shared memory location, a communication race
consists of receives and sends not related by happens-before on a shared
channel. Communication races are problematic: a receiver expects a specific
message from a specific sender, but with a communication race, the receiver can
receive a message meant for another receiver, or not receive anything at all.
In this work, we describe a verification framework that can prove the absence
of communication races for distributed programs that use a subset of the Go
programming language, where synchronization is mainly achieved via message
passing. We statically reason about how a distributed program executes, using a
happens-before order, extended to buffered and unbuffered channels.

</details>


### [2] [OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents](https://arxiv.org/abs/2506.14866)
*Thomas Kuntz,Agatha Duzan,Hao Zhao,Francesco Croce,Zico Kolter,Nicolas Flammarion,Maksym Andriushchenko*

Main category: cs.SE

TL;DR: The paper presents OS-Harm, a new benchmark for measuring the safety of LLM-based agents interacting with GUIs. Evaluations reveal that current models frequently perform unsafe actions and obey harmful instructions, underlining urgent safety concerns.


<details>
  <summary>Details</summary>
Motivation: The paper recognizes a critical gap in the safety evaluation of LLM-based agents that interact directly with graphical user interfaces, highlighting the lack of existing benchmarks for understanding and quantifying their potential for harmful behaviors.

Method: The authors introduce OS-Harm, a novel benchmark designed to evaluate the safety of LLM-based computer use agents. OS-Harm contains 150 tasks categorized across deliberate misuse, prompt injection attacks, and model misbehavior, requiring agents to interact with various OS applications. The paper also presents an automated evaluation judge that measures both accuracy and safety, and demonstrates its validation against human annotations.

Result: Empirical evaluation of frontier models such as o4-mini, Claude 3.7 Sonnet, and Gemini 2.5 Pro shows that these agents frequently comply with misuse instructions, are susceptible to prompt injection, and sometimes undertake unsafe actions. The automated judge aligns well with human evaluators, achieving F1 scores of 0.76 and 0.79 for accuracy and safety respectively.

Conclusion: The OS-Harm benchmark exposes significant safety vulnerabilities in current LLM-based computer use agents, especially in their willingness to comply with harmful queries and susceptibility to prompt injection. The resource is released to encourage further safety research and improvement in these systems.

Abstract: Computer use agents are LLM-based agents that can directly interact with a
graphical user interface, by processing screenshots or accessibility trees.
While these systems are gaining popularity, their safety has been largely
overlooked, despite the fact that evaluating and understanding their potential
for harmful behavior is essential for widespread adoption. To address this gap,
we introduce OS-Harm, a new benchmark for measuring safety of computer use
agents. OS-Harm is built on top of the OSWorld environment and aims to test
models across three categories of harm: deliberate user misuse, prompt
injection attacks, and model misbehavior. To cover these cases, we create 150
tasks that span several types of safety violations (harassment, copyright
infringement, disinformation, data exfiltration, etc.) and require the agent to
interact with a variety of OS applications (email client, code editor, browser,
etc.). Moreover, we propose an automated judge to evaluate both accuracy and
safety of agents that achieves high agreement with human annotations (0.76 and
0.79 F1 score). We evaluate computer use agents based on a range of frontier
models - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide
insights into their safety. In particular, all models tend to directly comply
with many deliberate misuse queries, are relatively vulnerable to static prompt
injections, and occasionally perform unsafe actions. The OS-Harm benchmark is
available at https://github.com/tml-epfl/os-harm.

</details>


### [3] [An Empirical Study of Bugs in Data Visualization Libraries](https://arxiv.org/abs/2506.15084)
*Weiqi Lu,Yongqiang Tian,Xiaohan Zhong,Haoyang Ma,Zhenyang Xu,Shing-Chi Cheung,Chengnian Sun*

Main category: cs.SE

TL;DR: This paper presents the first large-scale analysis of bugs in data visualization libraries, finding that inaccurate plots and graphic computation errors are widespread, and emphasizes the need for better automated testing. Vision Language Models can help detect visual bugs, but with limited and variable effectiveness.


<details>
  <summary>Details</summary>
Motivation: Data visualization libraries are widely used for presenting and analyzing data. Bugs in these libraries can lead to misleading or incorrect graphical representations without obvious system failures. Such visual bugs may distort user interpretations and decision-making, making it crucial to understand the unique characteristics of these bugs for better detection and resolution.

Method: The study analyzes 564 bugs from five widely-used DataViz libraries. It systematically classifies bug symptoms and root causes, develops a taxonomy, identifies the major phases where bugs are triggered, and explores two unique test oracles for DataViz libraries. It also evaluates the capability of Vision Language Models (VLMs) to detect visual bugs.

Result: The analysis finds that incorrect or inaccurate plots are common, with the main cause being incorrect graphic computation. The study identifies eight steps for triggering these bugs and two specific test oracles for DataViz library testing. It reports the effectiveness of Vision Language Models in detecting visual bugs to range from 29% to 57%, with no clear benefit from more detailed prompts.

Conclusion: Visual bugs in DataViz libraries are pervasive and primarily caused by incorrect graphic computation. Automated testing approaches need to be further developed for these libraries. Initial explorations show that Vision Language Models can help in detecting some visual bugs, but their effectiveness is limited. The taxonomy and findings provide a valuable foundation for future research on automated testing techniques in this domain.

Abstract: Data visualization (DataViz) libraries play a crucial role in presentation,
data analysis, and application development, underscoring the importance of
their accuracy in transforming data into visual representations. Incorrect
visualizations can adversely impact user experience, distort information
conveyance, and influence user perception and decision-making processes. Visual
bugs in these libraries can be particularly insidious as they may not cause
obvious errors like crashes, but instead mislead users of the underlying data
graphically, resulting in wrong decision making. Consequently, a good
understanding of the unique characteristics of bugs in DataViz libraries is
essential for researchers and developers to detect and fix bugs in DataViz
libraries.
  This study presents the first comprehensive analysis of bugs in DataViz
libraries, examining 564 bugs collected from five widely-used libraries. Our
study systematically analyzes their symptoms and root causes, and provides a
detailed taxonomy. We found that incorrect/inaccurate plots are pervasive in
DataViz libraries and incorrect graphic computation is the major root cause,
which necessitates further automated testing methods for DataViz libraries.
Moreover, we identified eight key steps to trigger such bugs and two test
oracles specific to DataViz libraries, which may inspire future research in
designing effective automated testing techniques. Furthermore, with the recent
advancements in Vision Language Models (VLMs), we explored the feasibility of
applying these models to detect incorrect/inaccurate plots. The results show
that the effectiveness of VLMs in bug detection varies from 29% to 57%,
depending on the prompts, and adding more information in prompts does not
necessarily increase the effectiveness. More findings can be found in our
manuscript.

</details>


### [4] [Program Feature-based Fuzzing Benchmarking](https://arxiv.org/abs/2506.15088)
*Miao Miao*

Main category: cs.SE

TL;DR: The paper introduces a new benchmarking approach for fuzzers that emphasizes fine-grained program features, revealing that fuzzer performance can vary greatly depending on these features. This suggests current evaluation methods may overlook important differences in fuzzer effectiveness.


<details>
  <summary>Details</summary>
Motivation: Traditional fuzzing benchmarks do not account for the impact of fine-grained program features on fuzzing effectiveness. There is a need for more precise evaluation methods that consider how specific program characteristics influence fuzzing outcomes.

Method: The authors reviewed 25 grey-box fuzzing studies to extract 7 key program features related to control-flow and data-flow. Using these features, they generated a benchmark suite of 153 programs with 10 configurable, fine-grained parameters designed to stress these aspects. They then evaluated 11 popular fuzzers on this benchmark to measure how performance varies with program features.

Result: They found that fuzzer performance differs widely depending on the specific program features and their configurations. This demonstrates that traditional benchmarks may miss important nuances in fuzzer capability.

Conclusion: Program features and their strengths significantly affect fuzzing performance. Incorporating configurable, fine-grained program characteristics into fuzzing benchmarks provides better insights and more thorough evaluations of fuzzer effectiveness.

Abstract: Fuzzing is a powerful software testing technique renowned for its
effectiveness in identifying software vulnerabilities. Traditional fuzzing
evaluations typically focus on overall fuzzer performance across a set of
target programs, yet few benchmarks consider how fine-grained program features
influence fuzzing effectiveness. To bridge this gap, we introduce a novel
benchmark designed to generate programs with configurable, fine-grained program
features to enhance fuzzing evaluations. We reviewed 25 recent grey-box fuzzing
studies, extracting 7 program features related to control-flow and data-flow
that can impact fuzzer performance. Using these features, we generated a
benchmark consisting of 153 programs controlled by 10 fine-grained configurable
parameters. We evaluated 11 popular fuzzers using this benchmark. The results
indicate that fuzzer performance varies significantly based on the program
features and their strengths, highlighting the importance of incorporating
program characteristics into fuzzing evaluations.

</details>


### [5] [Enhancement Report Approval Prediction: A Comparative Study of Large Language Models](https://arxiv.org/abs/2506.15098)
*Haosheng Zuo,Feifei Niu,Chuanyi Li*

Main category: cs.SE

TL;DR: This paper shows that large language models (LLMs), especially fine-tuned and when using profile data, significantly outperform traditional models in predicting approval of software enhancement reports, improving accuracy, recall, and handling data imbalances, thus advancing automation in software maintenance.


<details>
  <summary>Details</summary>
Motivation: Manually processing enhancement reports (ERs) is time-consuming and resource-intensive, which can result in valuable suggestions from users being delayed or overlooked. There is a need to automate the approval process for ERs to improve software maintenance efficiency.

Method: The study systematically evaluated 18 large language model (LLM) variants (including encoder models such as BERT, RoBERTa, DeBERTa-v3, ELECTRA, XLNet, and decoder models like GPT-3.5-turbo, GPT-4o-mini, Llama 3.1 8B, Llama 3.1 8B Instruct, DeepSeek-V3) for ER approval prediction. Traditional methods (feature-based, CNN/LSTM-BERT/GloVe) were used as baselines. The experiments involved analyzing both unfine-tuned and LoRA fine-tuned LLMs, with and without creator profile data.

Result: (1) Adding creator profiles improved the accuracy of unfine-tuned decoder-only models by 10.8% but could introduce bias. (2) LoRA fine-tuned Llama 3.1 8B Instruct achieved 79% accuracy and 76.1% recall for approved reports, compared to 64.1% by LSTM-GLOVE, outperforming traditional approaches by 5% under strict chronological evaluation and mitigating class imbalance. The study also analyzed cases where large models struggled, highlighting future research directions.

Conclusion: LLMs, when appropriately combined with fine-tuning techniques and profile information, outperform traditional machine learning and deep learning models for enhancement report approval prediction. These models can substantially streamline software maintenance workflows and decision-making in development environments. The study also identifies and articulates the limitations and underperforming scenarios for large models to guide future work.

Abstract: Enhancement reports (ERs) serve as a critical communication channel between
users and developers, capturing valuable suggestions for software improvement.
However, manually processing these reports is resource-intensive, leading to
delays and potential loss of valuable insights. To address this challenge,
enhancement report approval prediction (ERAP) has emerged as a research focus,
leveraging machine learning techniques to automate decision-making. While
traditional approaches have employed feature-based classifiers and deep
learning models, recent advancements in large language models (LLM) present new
opportunities for enhancing prediction accuracy. This study systematically
evaluates 18 LLM variants (including BERT, RoBERTa, DeBERTa-v3, ELECTRA, and
XLNet for encoder models; GPT-3.5-turbo, GPT-4o-mini, Llama 3.1 8B, Llama 3.1
8B Instruct and DeepSeek-V3 for decoder models) against traditional methods
(CNN/LSTM-BERT/GloVe). Our experiments reveal two key insights: (1)
Incorporating creator profiles increases unfine-tuned decoder-only models'
overall accuracy by 10.8 percent though it may introduce bias; (2) LoRA
fine-tuned Llama 3.1 8B Instruct further improve performance, reaching 79
percent accuracy and significantly enhancing recall for approved reports (76.1
percent vs. LSTM-GLOVE's 64.1 percent), outperforming traditional methods by 5
percent under strict chronological evaluation and effectively addressing class
imbalance issues. These findings establish LLM as a superior solution for ERAP,
demonstrating their potential to streamline software maintenance workflows and
improve decision-making in real-world development environments. We also
investigated and summarized the ER cases where the large models underperformed,
providing valuable directions for future research.

</details>


### [6] [Advanced approach for Agile/Scrum Process: RetroAI++](https://arxiv.org/abs/2506.15172)
*Maria Spichkova,Kevin Iwan,Madeleine Zwart,Hina Lee,Yuwon Yoon,Xiaohan Qin*

Main category: cs.SE

TL;DR: The paper introduces RetroAI++, an AI-powered tool designed to automate and enhance sprint planning and retrospectives in Agile/Scrum software development, making these processes more efficient and insightful.


<details>
  <summary>Details</summary>
Motivation: Sprint planning and retrospective analysis are vital in Agile/Scrum development but are often time-consuming and can benefit from intelligent automation and support.

Method: The authors present a prototype tool, RetroAI++, which uses emerging AI technologies to automate and enhance the processes involved in Sprint Planning and Retrospectives for Agile/Scrum projects.

Result: RetroAI++ provides intelligent suggestions for sprint organization and offers meaningful insights for retrospective reflection, aiming to make Agile/Scrum processes more efficient and effective.

Conclusion: RetroAI++ has the potential to improve key Agile/Scrum practices by leveraging AI to reduce manual effort and enhance decision-making and reflection in sprint activities.

Abstract: In Agile/Scrum software development, sprint planning and retrospective
analysis are the key elements of project management. The aim of our work is to
support software developers in these activities. In this paper, we present our
prototype tool RetroAI++, based on emerging intelligent technologies. In our
RetroAI++ prototype, we aim to automate and refine the practical application of
Agile/Scrum processes within Sprint Planning and Retrospectives. Leveraging AI
insights, our prototype aims to automate and refine the many processes involved
in the Sprint Planning, Development and Retrospective stages of Agile/Scrum
development projects, offering intelligent suggestions for sprint organisation
as well as meaningful insights for retrospective reflection.

</details>


### [7] [Large Language Models for Unit Testing: A Systematic Literature Review](https://arxiv.org/abs/2506.15227)
*Quanjun Zhang,Chunrong Fang,Siqi Gu,Ye Shang,Zhenyu Chen,Liang Xiao*

Main category: cs.SE

TL;DR: The paper reviews existing research on using LLMs for unit testing, categorizes their applications, discusses integration methods and open challenges, and provides guidance for future work, serving as a useful reference for the software testing community.


<details>
  <summary>Details</summary>
Motivation: As Large Language Models (LLMs) are increasingly used for automating unit testing tasks in software engineering, it has become challenging for researchers to keep track of advancements, challenges, and future directions in this rapidly evolving area.

Method: The paper conducts the first systematic literature review (up to March 2025) on the application of LLMs in unit testing, analyzing a set of relevant papers. It categorizes unit testing tasks benefitting from LLMs, discusses integration strategies, explores unresolved challenges, and suggests future research directions.

Result: The paper categorizes tasks such as test and oracle generation that can benefit from LLMs, discusses aspects like model usage and adaptation strategies, and highlights key unresolved challenges in integrating LLMs with unit testing. It also provides guidance on future research opportunities and makes all artifacts publicly available.

Conclusion: This work offers a comprehensive overview of the current research landscape regarding LLMs in unit testing, serving as a valuable resource for researchers and practitioners and promoting further research in the field.

Abstract: Unit testing is a fundamental practice in modern software engineering, with
the aim of ensuring the correctness, maintainability, and reliability of
individual software components. Very recently, with the advances in Large
Language Models (LLMs), a rapidly growing body of research has leveraged LLMs
to automate various unit testing tasks, demonstrating remarkable performance
and significantly reducing manual effort. However, due to ongoing explorations
in the LLM-based unit testing field, it is challenging for researchers to
understand existing achievements, open challenges, and future opportunities.
This paper presents the first systematic literature review on the application
of LLMs in unit testing until March 2025. We analyze \numpaper{} relevant
papers from the perspectives of both unit testing and LLMs. We first categorize
existing unit testing tasks that benefit from LLMs, e.g., test generation and
oracle generation. We then discuss several critical aspects of integrating LLMs
into unit testing research, including model usage, adaptation strategies, and
hybrid approaches. We further summarize key challenges that remain unresolved
and outline promising directions to guide future research in this area.
Overall, our paper provides a systematic overview of the research landscape to
the unit testing community, helping researchers gain a comprehensive
understanding of achievements and promote future research. Our artifacts are
publicly available at the GitHub repository:
https://github.com/iSEngLab/AwesomeLLM4UT.

</details>


### [8] [Uncovering Intention through LLM-Driven Code Snippet Description Generation](https://arxiv.org/abs/2506.15453)
*Yusuf Sulistyo Nugroho,Farah Danisha Salam,Brittany Reid,Raula Gaikovina Kula,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: This paper evaluated how well an LLM (Llama) generates code snippet descriptions compared to originals. The LLM mostly matched human-classified types (examples), but its descriptions weren't always highly similar to the originals, indicating room for improvement.


<details>
  <summary>Details</summary>
Motivation: With the increasing use of Large Language Models (LLMs), there is a need to understand how effectively LLMs can generate useful descriptions for code snippets, especially for libraries and APIs where documentation is critical.

Method: The authors used a large dataset (NPM Code Snippets) of over a million code snippets and selected a sample of 400 snippets with their descriptions. They performed manual classification of the types of descriptions and compared these to descriptions generated by the Llama LLM. They also measured similarity scores between original and generated descriptions.

Result: The manual classification showed that most original descriptions (55.5%) were example-based. The LLM matched this by also labeling most descriptions as 'Example' (79.75%), showing consistency in identifying intent. However, the average similarity score between LLM-generated and original descriptions was 0.7173, suggesting that while the generated descriptions are relevant, there is still room for improvement in matching the originals closely.

Conclusion: LLMs like Llama can generalize and generate relevant example-based documentation for code snippets, but their produced descriptions may not always fully capture the original intent or detail, indicating a need for further refinement in automated code documentation.

Abstract: Documenting code snippets is essential to pinpoint key areas where both
developers and users should pay attention. Examples include usage examples and
other Application Programming Interfaces (APIs), which are especially important
for third-party libraries. With the rise of Large Language Models (LLMs), the
key goal is to investigate the kinds of description developers commonly use and
evaluate how well an LLM, in this case Llama, can support description
generation. We use NPM Code Snippets, consisting of 185,412 packages with
1,024,579 code snippets. From there, we use 400 code snippets (and their
descriptions) as samples. First, our manual classification found that the
majority of original descriptions (55.5%) highlight example-based usage. This
finding emphasizes the importance of clear documentation, as some descriptions
lacked sufficient detail to convey intent. Second, the LLM correctly identified
the majority of original descriptions as "Example" (79.75%), which is identical
to our manual finding, showing a propensity for generalization. Third, compared
to the originals, the produced description had an average similarity score of
0.7173, suggesting relevance but room for improvement. Scores below 0.9
indicate some irrelevance. Our results show that depending on the task of the
code snippet, the intention of the document may differ from being instructions
for usage, installations, or descriptive learning examples for any user of a
library.

</details>


### [9] [cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree](https://arxiv.org/abs/2506.15655)
*Yilin Zhang,Xinran Zhao,Zora Zhiruo Wang,Chenyang Yang,Jiayi Wei,Tongshuang Wu*

Main category: cs.SE

TL;DR: This paper shows that chunking code by its semantic structure using ASTs, rather than lines, leads to more accurate and effective retrieval-augmented code generation, resulting in measurable improvements on key benchmarks.


<details>
  <summary>Details</summary>
Motivation: Retrieval-Augmented Generation (RAG) for code generation depends on dividing documents into chunks for retrieval, but existing line-based chunking heuristics often disrupt semantic structures in code, lowering generation quality.

Method: The authors introduce a structure-aware chunking technique using Abstract Syntax Trees (ASTs) that recursively breaks down large AST nodes and merges siblings, ensuring chunks are semantically coherent and respect predefined size limits.

Result: The proposed method produces self-contained and semantically meaningful code chunks, leading to notable improvements in code generation benchmarks: a 4.3 point increase in Recall@5 on RepoEval retrieval and a 2.67 point increase in Pass@1 on SWE-bench generation tasks.

Conclusion: Structure-aware chunking, particularly via AST-based approaches, is vital for scaling retrieval-augmented code intelligence and significantly enhances code generation quality compared to line-based heuristics.

Abstract: Retrieval-Augmented Generation (RAG) has become essential for large-scale
code generation, grounding predictions in external code corpora to improve
actuality. However, a critical yet underexplored aspect of RAG pipelines is
chunking -- the process of dividing documents into retrievable units. Existing
line-based chunking heuristics often break semantic structures, splitting
functions or merging unrelated code, which can degrade generation quality. We
propose chunking via Abstract Syntax Trees (\ourwork), a structure-aware method
that recursively breaks large AST nodes into smaller chunks and merges sibling
nodes while respecting size limits. This approach generates self-contained,
semantically coherent units across programming languages and tasks, improving
performance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3
points on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation.
Our work highlights the importance of structure-aware chunking for scaling
retrieval-enhanced code intelligence.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [10] [A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in GPUs](https://arxiv.org/abs/2506.15174)
*Hossein Albakri,Kazem Cheshmi*

Main category: cs.PL

TL;DR: The paper presents a compiler technique that greatly improves the speed of sparse matrix multiplications in neural networks on GPUs, outperforming leading baselines by up to 2.27× through better memory and workload handling.


<details>
  <summary>Details</summary>
Motivation: Sparse data structures help reduce memory footprint in neural networks but cause irregular memory access, leading to inefficiencies—especially on GPUs, which are commonly used for machine learning.

Method: A new compiler transformation called enumerate-and-sparse-coarsen is proposed. It restructures sparse matrix-matrix multiplication (SPMM) processes to increase data reuse in registers and caches and balance GPU workloads.

Result: The method is tested on sparse neural networks, including convectional and transformer models, on A100 GPUs. The proposed transformation achieves a geometric mean speedup of 1.84× to 2.27× over cuBLAS and cuSPARSE baselines for B matrix column counts ranging from 32 to 128.

Conclusion: Enumerate-and-sparse-coarsen is effective at accelerating sparse SPMM on GPUs by improving memory/data reuse and workload balance, significantly outperforming established libraries on tested tasks.

Abstract: Sparse data structures are commonly used in neural networks to reduce the
memory footprint. These data structures are compact but cause irregularities
such as random memory accesses, which prevent efficient use of the memory
hierarchy. GPUs are a common platform for machine learning practitioners, but
running compact data structures on these devices often leads to slow-downs due
to inefficient use of computing and memory resources. This paper proposes a new
compiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse
matrix-matrix multiplication (SPMM) on GPU devices. The transformation
increases data reuse in registers and caches while creating more balanced
workloads for GPU computing resources. The transformation is tested on sparse
neural networks in convolutional and transformer models. On an A100 GPU and
across a columns of matrix B (bCols) in $ A \times B = C$ from range of 32 to
128, the transformation yields a geometric mean speedup of 1.84$\times$ to
2.27$\times$ compared to cuBLAS and cuSPARSE baselines, respectively.

</details>


### [11] [PSM: Policy Synchronised Deterministic Memory](https://arxiv.org/abs/2506.15424)
*Michael Mendler,Marc Pouzet*

Main category: cs.PL

TL;DR: The paper introduces a new Haskell type context, PSM, that combines the benefits of concurrent and deterministic programming by using policy synchronization for memory access. This allows for imperative, race-free, shareable data structures that remain deterministic, overcoming limitations in existing Haskell abstractions.


<details>
  <summary>Details</summary>
Motivation: Concurrency in programming often conflicts with determinacy, specifically when sharing resources. Although Haskell provides abstractions for both parallel and concurrent programming, these come with trade-offs: some ensure determinacy but lack destructive updates, while others allow destructive updates but lose determinacy. Existing approaches do not adequately balance concurrent shareability with determinacy. The paper addresses this gap by introducing a new memory abstraction.

Method: The paper introduces a new type context called PSM (policy synchronised memory) in Haskell. PSM enables computations to access and update persistent state in an imperative, concurrent manner, similar to STM and IO monads. PSM employs policy coordination to manage concurrent accesses, ensuring race freedom and determinacy. The approach is demonstrated to allow for the construction of imperative, shareable, and deterministic abstract data structures in Haskell.

Result: PSM provides a context where computations can access and update shared memory imperatively and concurrently, while policy coordination guarantees race-freedom and determinacy. Well-typed transactions within PSM allow for building abstract data structures that are imperative, sharable among threads, and deterministically behaved by design.

Conclusion: The PSM context bridges the gap between determinacy and concurrent, imperative updates in Haskell. By coordinating shared memory access via policies, it enables programmers to create deterministic and shareable abstract data structures without compromising on expressiveness or concurrency.

Abstract: Concurrency and determinacy do not go well with each other when resources
must be shared. Haskell provides parallel programming abstractions such as IVar
and LVar in the Par monad and concurrent abstractions such as MVar and TVar in
the in IO and STM monads, respectively. The former are determinate but have no
destructive updates and the latter have destructive updates but do not
guarantee determinacy. Programming patterns that are both concurrent and
determinate, such as those provided by Kahn or Berry require memory
abstractions at a higher level than is currently available. In this paper we
describe a new type context PSM for policy synchronised memory in Haskell. Like
STM and IO, the computations in PSM can access persistent state and, as a
side-effect, update the memory in imperative style. Like the Par and IO monads,
PSM supports concurrent threads and shared state. However, in contrast to IO,
our PSM contexts are race-free since concurrent accesses are policy coordinated
which guarantees determinacy.Well-typed transactions in the PSM context can
accommodate abstract data structures that are imperative, concurrently
shareable and still behave deterministically, by construction.

</details>
