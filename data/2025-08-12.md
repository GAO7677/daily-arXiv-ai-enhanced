<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 15]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Refactoring-Aware Patch Integration Across Structurally Divergent Java Forks](https://arxiv.org/abs/2508.06718)
*Daniel Ogenrwot,John Businge*

Main category: cs.SE

TL;DR: Most software forks on GitHub are short-lived, but a few become independent long-lived variants. Merging bug-fixes between these variants is difficult due to code refactorings. This paper studies integration failures among Java project variants and introduces RePatch, a tool that better handles code structure changes than standard tools like Git cherry-pick. RePatch successfully integrates over half of patches that standard tools fail to handle, showing the necessity of semantic and refactoring-aware approaches.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of integrating bug-fix patches between divergent software variants on platforms like GitHub, which is complicated by structural drift due to refactorings and reorganization of code.

Method: An empirical study on patch integration failures in 14 divergent pairs of Java variants was conducted, followed by the design and implementation of RePatch, a refactoring-aware system built on top of RefMerge. RePatch works by inverting refactorings for both source and target, then applying and replaying transformations to maintain patch intent.

Result: In an evaluation of 478 bug-fix pull requests, Git's cherry-pick failed in 64.4% of cases because of structural misalignments. RePatch was able to integrate 52.8% of these previously failing patches successfully.

Conclusion: Syntax-based tools like Git cherry-pick are often inadequate for patch propagation across evolving software variants. Semantic, refactoring-aware tools such as RePatch significantly improve patch integration success rates in these contexts.

Abstract: While most forks on platforms like GitHub are short-lived and used for social
collaboration, a smaller but impactful subset evolve into long-lived forks,
referred to here as variants, that maintain independent development
trajectories. Integrating bug-fix patches across such divergent variants poses
challenges due to structural drift, including refactorings that rename,
relocate, or reorganize code elements and obscure semantic correspondence. This
paper presents an empirical study of patch integration failures in 14 divergent
pair of variants and introduces RePatch, a refactoring-aware integration system
for Java repositories. RePatch extends the RefMerge framework, originally
designed for symmetric merges, by supporting asymmetric patch transfer. RePatch
inverts refactorings in both the source and target to realign the patch
context, applies the patch, and replays the transformations to preserve the
intent of the variant. In our evaluation of 478 bug-fix pull requests, Git
cherry-pick fails in 64.4% of cases due to structural misalignments, while
RePatch successfully integrates 52.8% of the previously failing patches. These
results highlight the limitations of syntax-based tools and the need for
semantic reasoning in variant-aware patch propagation.

</details>


### [2] [Quo Vadis, Code Review? Exploring the Future of Code Review](https://arxiv.org/abs/2508.06879)
*Michael Dorner,Andreas Bauer,Darja Šmite,Lukas Thode,Daniel Mendez,Ricardo Britto,Stephan Lukasczyk,Ehsan Zabardast,Michael Kormann*

Main category: cs.SE

TL;DR: This paper analyzes current practitioner views on code review, predicts upcoming changes, and warns about potential long-term risks to its role in collaborative software engineering.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand how code review practices are perceived by current software practitioners and to anticipate how these practices might change in the future.

Method: The study explores practitioners' reflections and expectations through qualitative analysis, likely involving surveys, interviews, or discussions with software engineers.

Result: The paper identifies both current perceptions of code review and expected near-future changes, as well as potential long-term risks to code review as a collaborative practice.

Conclusion: The conclusion discusses the risks and implications of anticipated changes for the evolution of code review and collaborative software engineering.

Abstract: Code review has long been a core practice in collaborative software
engineering. In this research, we explore how practitioners reflect on code
review today and what changes they anticipate in the near future. We then
discuss the potential long-term risks of these anticipated changes for the
evolution of code review and its role in collaborative software engineering.

</details>


### [3] [Multi-Modal Requirements Data-based Acceptance Criteria Generation using LLMs](https://arxiv.org/abs/2508.06888)
*Fanyu Wang,Chetan Arora,Yonghui Liu,Kaicheng Huang,Chakkrit Tantithamthavorn,Aldeida Aleti,Dishan Sambathkumar,David Lo*

Main category: cs.SE

TL;DR: This paper presents a new automated method (RAGcceptance M2RE) using retrieval-augmented generation to generate acceptance criteria from both textual and visual requirements. Tested in a large educational software context, it significantly improved AC quality, minimized manual labor, and captured stakeholder needs better than manual methods, showing strong promise for industrial use.


<details>
  <summary>Details</summary>
Motivation: Creating high-quality, unambiguous acceptance criteria (ACs) for software features is difficult, especially for user interface-heavy systems, because domain-specific and visual information is often missing from textual requirements. This gap results in unclear expectations and increased manual effort.

Method: The paper introduces RAGcceptance M2RE, which uses Retrieval-Augmented Generation (RAG) to automatically generate ACs from multi-modal requirements data—combining both textual and visual user interface information. The approach was systematically evaluated through an industrial case study with a large-scale, education-focused software system.

Result: The integration of multi-modal data substantially improved the quality of generated ACs, making them more relevant, accurate, and comprehensible. Practitioner feedback verified reduced manual workload, better stakeholder intent capture, and surfaced important criteria often missed by domain experts. The tool and dataset were released to the public.

Conclusion: Multi-modal RAG methods can make acceptance criteria generation more efficient and reliable, with demonstrable practical benefits and strong potential for widespread industry adoption, particularly for visually complex software systems.

Abstract: Acceptance criteria (ACs) play a critical role in software development by
clearly defining the conditions under which a software feature satisfies
stakeholder expectations. However, manually creating accurate, comprehensive,
and unambiguous acceptance criteria is challenging, particularly in user
interface-intensive applications, due to the reliance on domain-specific
knowledge and visual context that is not always captured by textual
requirements alone. To address these challenges, we propose RAGcceptance M2RE,
a novel approach that leverages Retrieval-Augmented Generation (RAG) to
generate acceptance criteria from multi-modal requirements data, including both
textual documentation and visual UI information. We systematically evaluated
our approach in an industrial case study involving an education-focused
software system used by approximately 100,000 users. The results indicate that
integrating multi-modal information significantly enhances the relevance,
correctness, and comprehensibility of the generated ACs. Moreover, practitioner
evaluations confirm that our approach effectively reduces manual effort,
captures nuanced stakeholder intent, and provides valuable criteria that domain
experts may overlook, demonstrating practical utility and significant potential
for industry adoption. This research underscores the potential of multi-modal
RAG techniques in streamlining software validation processes and improving
development efficiency. We also make our implementation and a dataset
available.

</details>


### [4] [Integrating Rules and Semantics for LLM-Based C-to-Rust Translation](https://arxiv.org/abs/2508.06926)
*Feng Luo,Kexing Ji,Cuiyun Gao,Shuzheng Gao,Jia Feng,Kui Liu,Xin Xia,Michael R. Lyu*

Main category: cs.SE

TL;DR: This paper introduces IRENE, a novel framework to automate C-to-Rust translation. By combining rule-based retrieval, structured summarization, and iterative compiler-driven refinement, it outperforms previous methods in both accuracy and memory safety, as validated on public and industrial benchmarks.


<details>
  <summary>Details</summary>
Motivation: Automated migration from C to Rust is needed for improved memory safety, but existing translation methods are limited by rule coverage or semantic accuracy.

Method: The authors propose IRENE, an LLM-based framework consisting of: (1) rule-augmented retrieval using a custom static analyzer for relevant examples, (2) structured summarization for better semantic understanding, (3) error-driven translation using compiler diagnostics for iterative refinement.

Result: IRENE is evaluated on both public (xCodeEval) and industrial (HW-Bench) datasets using eight different LLMs, where it shows improvement in translation accuracy and safety compared to prior approaches.

Conclusion: IRENE effectively improves automated C-to-Rust code translation by addressing rule adherence and semantic consistency issues, resulting in safer and more accurate migrated code.

Abstract: Automated translation of legacy C code into Rust aims to ensure memory safety
while reducing the burden of manual migration. Early approaches in code
translation rely on static rule-based methods, but they suffer from limited
coverage due to dependence on predefined rule patterns. Recent works regard the
task as a sequence-to-sequence problem by leveraging large language models
(LLMs). Although these LLM-based methods are capable of reducing unsafe code
blocks, the translated code often exhibits issues in following Rust rules and
maintaining semantic consistency. On one hand, existing methods adopt a direct
prompting strategy to translate the C code, which struggles to accommodate the
syntactic rules between C and Rust. On the other hand, this strategy makes it
difficult for LLMs to accurately capture the semantics of complex code. To
address these challenges, we propose IRENE, an LLM-based framework that
Integrates RulEs aNd sEmantics to enhance translation. IRENE consists of three
modules: 1) a rule-augmented retrieval module that selects relevant translation
examples based on rules generated from a static analyzer developed by us,
thereby improving the handling of Rust rules; 2) a structured summarization
module that produces a structured summary for guiding LLMs to enhance the
semantic understanding of C code; 3) an error-driven translation module that
leverages compiler diagnostics to iteratively refine translations. We evaluate
IRENE on two datasets (xCodeEval, a public dataset, and HW-Bench, an industrial
dataset provided by Huawei) and eight LLMs, focusing on translation accuracy
and safety.

</details>


### [5] [When Prompt Engineering Meets Software Engineering: CNL-P as Natural and Robust "APIs'' for Human-AI Interaction](https://arxiv.org/abs/2508.06942)
*Zhenchang Xing,Yang Liu,Zhuo Cheng,Qing Huang,Dehai Zhao,Daniel Sun,Chenhua Liu*

Main category: cs.SE

TL;DR: This paper proposes CNL-P, a structured prompt language combining prompt and software engineering principles to reduce ambiguity in LLM prompts. Tools for converting and checking prompts are presented, and experiments demonstrate improved LLM performance and reliability.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) are being used in diverse applications, and prompt engineering practices help improve their effectiveness. However, natural language prompts remain ambiguous and lack rigorous structure.

Method: They propose Controlled Natural Language for Prompting (CNL-P), which blends prompt engineering best practices with software engineering principles by introducing strict grammar and semantics. They also develop tools: an NL2CNL-P converter and a CNL-P linting tool for checking prompt accuracy.

Result: CNL-P improves LLM response quality and consistency. Experiments show that integrating prompt engineering and software engineering principles with static analysis techniques enhances outputs and reduces ambiguity.

Conclusion: CNL-P eliminates NL ambiguity, raises LLM output quality, and can bridge prompt engineering with software engineering, pointing towards a new NLU-based programming paradigm.

Abstract: With the growing capabilities of large language models (LLMs), they are
increasingly applied in areas like intelligent customer service, code
generation, and knowledge management. Natural language (NL) prompts act as the
``APIs'' for human-LLM interaction. To improve prompt quality, best practices
for prompt engineering (PE) have been developed, including writing guidelines
and templates. Building on this, we propose Controlled NL for Prompt (CNL-P),
which not only incorporates PE best practices but also draws on key principles
from software engineering (SE). CNL-P introduces precise grammar structures and
strict semantic norms, further eliminating NL's ambiguity, allowing for a
declarative but structured and accurate expression of user intent. This helps
LLMs better interpret and execute the prompts, leading to more consistent and
higher-quality outputs. We also introduce an NL2CNL-P conversion tool based on
LLMs, enabling users to write prompts in NL, which are then transformed into
CNL-P format, thus lowering the learning curve of CNL-P. In particular, we
develop a linting tool that checks CNL-P prompts for syntactic and semantic
accuracy, applying static analysis techniques to NL for the first time.
Extensive experiments demonstrate that CNL-P enhances the quality of LLM
responses through the novel and organic synergy of PE and SE. We believe that
CNL-P can bridge the gap between emerging PE and traditional SE, laying the
foundation for a new programming paradigm centered around NL.

</details>


### [6] [An Empirical Study on Method-Level Performance Evolution in Open-Source Java Projects](https://arxiv.org/abs/2508.07084)
*Kaveh Shahedi,Nana Gyambrah,Heng Li,Maxime Lamothe,Foutse Khomh*

Main category: cs.SE

TL;DR: Method-level code changes in Java projects frequently impact performance, especially causing regressions. Developer intuition about which changes pose risks is often inaccurate. Complexity and experience matter, and instability is high in some domains. The evidence strongly supports automated performance testing in CI workflows.


<details>
  <summary>Details</summary>
Motivation: Performance is a critical but under-researched aspect at the method-level in software development, and there is little empirical validation of developer intuition regarding code changes and their impact on performance.

Method: A large-scale empirical study of 15 mature open-source Java projects was conducted, analyzing 739 commits and 1,499 method-level changes. Java Microbenchmark Harness and bytecode instrumentation were used for fine-grained performance measurement and statistical analysis, examining aspects like temporal patterns, change types, developer experience, complexity, and project domains.

Result: 32.7% of method-level changes affected performance, with regressions 1.3 times more common than improvements. No significant difference in performance impact by type of code change. Algorithmic changes have highest improvement potential but high regression risk. Senior developers produce more stable changes; higher complexity increases regression risk. Small web server projects show the most performance instability.

Conclusion: Automated performance testing should be integrated into continuous integration pipelines, as method-level changes often have significant performance effects that are not reliably predicted by developer intuition or conventional wisdom.

Abstract: Performance is a critical quality attribute in software development, yet the
impact of method-level code changes on performance evolution remains poorly
understood. While developers often make intuitive assumptions about which types
of modifications are likely to cause performance regressions or improvements,
these beliefs lack empirical validation at a fine-grained level. We conducted a
large-scale empirical study analyzing performance evolution in 15 mature
open-source Java projects hosted on GitHub. Our analysis encompassed 739
commits containing 1,499 method-level code changes, using Java Microbenchmark
Harness (JMH) for precise performance measurement and rigorous statistical
analysis to quantify both the significance and magnitude of performance
variations. We employed bytecode instrumentation to capture method-specific
execution metrics and systematically analyzed four key aspects: temporal
performance patterns, code change type correlations, developer and complexity
factors, and domain-size interactions. Our findings reveal that 32.7% of
method-level changes result in measurable performance impacts, with regressions
occurring 1.3 times more frequently than improvements. Contrary to conventional
wisdom, we found no significant differences in performance impact distributions
across code change categories, challenging risk-stratified development
strategies. Algorithmic changes demonstrate the highest improvement potential
but carry substantial regression risk. Senior developers produce more stable
changes with fewer extreme variations, while code complexity correlates with
increased regression likelihood. Domain-size interactions reveal significant
patterns, with web server + small projects exhibiting the highest performance
instability. Our study provides empirical evidence for integrating automated
performance testing into continuous integration pipelines.

</details>


### [7] [From Noise to Knowledge: Interactive Summaries for Developer Alerts](https://arxiv.org/abs/2508.07169)
*Burak Yetiştiren,Hong Jin Kang,Miryung Kim*

Main category: cs.SE

TL;DR: CLARITY helps programmers sort and understand bug warnings quicker by interactively grouping related issues, tailoring to individual preferences, and reducing effort via active learning.


<details>
  <summary>Details</summary>
Motivation: Programmers struggle to review numerous bug-finding tool warnings, often addressing each individually. Enhancing sensemaking through identifying recurring patterns could improve efficiency and cognitive processing.

Method: CLARITY is proposed as an interactive system that derives summary rules for grouping related warnings, using active feedback from users. The rule inference algorithm highlights structural similarities (containment, subtyping, method calls, fields, expressions) among warnings as users interactively mark them as interesting or uninteresting.

Result: A user study with 14 participants on Java projects showed users identified root causes for similar uninteresting warnings faster and with higher confidence using CLARITY. There was significant individual variation in preferred grouping, supporting the need for customization. Simulations revealed interactive feedback reduced the average interactions to align rule sets with user labels from 17.8 to 11.8.

Conclusion: CLARITY's active learning-based rule summarization supports more efficient and confident sensemaking of bug warnings, with customization to individual user preferences. The approach outperforms current warning review methods.

Abstract: Programmers using bug-finding tools often review their reported warnings one
by one. Based on the insight that identifying recurring themes and
relationships can enhance the cognitive process of sensemaking, we propose
CLARITY, which supports interpreting tool-generated warnings through
interactive inquiry. CLARITY derives summary rules for custom grouping of
related warnings with active feedback. As users mark warnings as interesting or
uninteresting, CLARITY's rule inference algorithm surfaces common symptoms,
highlighting structural similarities in containment, subtyping, invoked
methods, accessed fields, and expressions.
  We demonstrate CLARITY on Infer and SpotBugs warnings across two mature Java
projects. In a within-subject user study with 14 participants, users
articulated root causes for similar uninteresting warnings faster and with more
confidence using CLARITY. We observed significant individual variation in
desired grouping, reinforcing the need for customizable sensemaking. Simulation
shows that with rule-level feedback, only 11.8 interactions are needed on
average to align all inferred rules with a simulated user's labels (vs. 17.8
without). Our evaluation suggests that CLARITY's active learning-based
summarization enhances interactive warning sensemaking.

</details>


### [8] [Dynamic Benchmark Construction for Evaluating Large Language Models on Real-World Codes](https://arxiv.org/abs/2508.07180)
*Zhe Zhang,Runlin Liu,Aishan Liu,Xingyu Liu,Xiang Gao,Hailong Sun*

Main category: cs.SE

TL;DR: CODE2BENCH is a dynamic benchmarking pipeline that builds robust and contamination-free code generation benchmarks from recent GitHub projects. It uses advanced dependency analysis and automated testing to generate difficult testing instances. Results show LLMs still struggle with complex and cross-language coding tasks, supporting the need for better evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for evaluating large language models (LLMs) in code generation are limited by issues such as data contamination and insufficient testing rigor, which makes it difficult to accurately assess LLM performance on complex, real-world tasks.

Method: The paper introduces CODE2BENCH, a dynamic pipeline that builds robust and contamination-resistant benchmarks from real GitHub repositories. Key innovations include: 1) periodic ingestion of recent code to reduce training data overlap, 2) scope graph-based dependency analysis to classify tasks (Self-Contained vs. Weakly Self-Contained), and 3) property-based testing for creating thorough and automated test suites.

Result: CODE2BENCH-2505, created using this new pipeline, is a benchmark from 880 Python projects covering various domains, consisting of 1,163 code generation tasks with full branch coverage. Evaluation of 16 LLMs shows that models have difficulty with complex, logic-heavy SC tasks and cross-language transfer, while doing better on WSC tasks in Python.

Conclusion: The study presents a new, dynamic, and contamination-resistant method for constructing language-agnostic benchmarks, providing a solid foundation for more realistic and comprehensive evaluation of LLMs in software development.

Abstract: As large language models LLMs) become increasingly integrated into software
development workflows, rigorously evaluating their performance on complex,
real-world code generation tasks has become essential. However, existing
benchmarks often suffer from data contamination and limited test rigor,
constraining their ability to reveal model failures effectively. To address
these, we present CODE2BENCH, a end-to-end pipeline for dynamically
constructing robust and contamination-resistant benchmarks from real-world
GitHub repositories. Specifically, CODE2BENCH introduces three key innovations:
(1) Automated Dynamism, achieved through periodic ingestion of recent code to
minimize training data contamination; (2) Scope Graph-based dependency
analysis, which enables structured classification of functions into benchmark
instances with controlled dependency levels (distinguishing between
Self-Contained (SC) tasks for cross-language evaluation and Weakly
Self-Contained (WSC) tasks involving permitted library usage); and (3)
Property-Based Testing (PBT) for the automated synthesis of rigorous test
suites to enable thorough functional verification. Using this pipeline, we
construct CODE2BENCH-2505, the first benchmark derived from 880 recent Python
projects spanning diverse domains, comprising 1,163 code generation tasks with
100% average branch coverage on ground-truth implementations. Extensive
evaluation of 16 LLMs using CODE2BENCH-2505 reveals that models consistently
struggle with SC tasks requiring complex, non-standard logic and cross-language
transfer, while showing relatively stronger performance on WSC tasks in Python.
Our work introduces a contamination-resistant, language-agnostic methodology
for dynamic benchmark construction, offering a principled foundation for the
comprehensive and realistic evaluation of LLMs on real-world software
development tasks.

</details>


### [9] [TraceLens: Question-Driven Debugging for Taint Flow Understanding](https://arxiv.org/abs/2508.07198)
*Burak Yetiştiren,Hong Jin Kang,Miryung Kim*

Main category: cs.SE

TL;DR: TraceLens is an interactive, question-answer-based interface for taint analysis that outperforms traditional tools like CodeQL in both accuracy and user experience, making taint flow investigations easier and more effective for developers.


<details>
  <summary>Details</summary>
Motivation: Developers using existing taint analysis tools face difficulties in debugging why certain dataflows appear or are missing, primarily due to inadequate support for interactive reasoning (such as asking why, why-not, and what-if questions) and limited visualization options for understanding global dataflow impacts.

Method: The authors introduce TraceLens, a question-answer style debugging interface for taint analysis. TraceLens lets users query the existence or absence of specific dataflows and explore hypothetical scenarios by altering source, sink, or third-party library model configurations. The system’s effectiveness was validated through a user study with 12 participants compared against an established tool, CodeQL.

Result: Participants using TraceLens achieved 21% higher accuracy in identifying taint flows than with CodeQL. They also experienced a 45% reduction in mental demand (measured by NASA-TLX) and reported greater confidence in their findings.

Conclusion: TraceLens enhances the end-user taint analysis experience by enabling interactive, question-driven debugging. This approach improves user understanding, reduces cognitive load, and increases confidence and accuracy in security analyses compared to traditional tools.

Abstract: Taint analysis is a security analysis technique used to track the flow of
potentially dangerous data through an application and its dependent libraries.
Investigating why certain unexpected flows appear and why expected flows are
missing is an important sensemaking process during end-user taint analysis.
Existing taint analysis tools often do not provide this end-user debugging
capability, where developers can ask why, why-not, and what-if questions about
dataflows and reason about the impact of configuring sources and sinks, and
models of 3rd-party libraries that abstract permissible and impermissible data
flows. Furthermore, a tree-view or a list-view used in existing
taint-analyzer's visualization makes it difficult to reason about the global
impact on connectivity between multiple sources and sinks.
  Inspired by the insight that sensemaking tool-generated results can be
significantly improved by a QA inquiry process, we propose TraceLens, a first
end-user question-answer style debugging interface for taint analysis. It
enables a user to ask why, why-not, and what-if questions to investigate the
existence of suspicious flows, the non-existence of expected flows, and the
global impact of third-party library models. TraceLens performs speculative
what-if analysis, to help a user in debugging how different connectivity
assumptions affect overall results. A user study with 12 participants shows
that participants using TraceLens achieved 21% higher accuracy on average,
compared to CodeQL. They also reported a 45% reduction in mental demand
(NASA-TLX) and rated higher confidence in identifying relevant flows using
TraceLens.

</details>


### [10] [AutoAssert 1: A LoRA Fine-Tuned LLM Model for Efficient Automated Assertion Generation](https://arxiv.org/abs/2508.07371)
*Yi Zhong,Hongchao Liu,Di ZHao*

Main category: cs.SE

TL;DR: This paper introduces an automated HDL assertion generation method, leveraging a LLM and the Unsloth platform to cut training costs while preserving accuracy and generalization. The approach generates precise, logic-conforming test cases and promises a scalable solution for software testing and maintenance. Source code is available online.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of software systems has led to a rising need for automated testing and maintenance tools. Manual test case generation is costly and inefficient, prompting the search for automation methods that can maintain quality and adapt to specific hardware requirements.

Method: The paper proposes an assertion generation method for Hardware Description Language (HDL) by combining a lightweight, parameter-adjustable large language model (LLM) with the Unsloth platform. This system automates test case generation, aiming to reduce training costs without sacrificing accuracy or generalization.

Result: Empirical evaluation demonstrates that the approach efficiently produces assertions which strictly conform to hardware logic, meeting both robustness and flexibility requirements for software testing and maintenance.

Conclusion: The framework offers a robust and flexible solution to the challenges of modern software testing and maintenance, balancing cost-efficiency with high accuracy and generalization using automated tools.

Abstract: As the complexity of software systems continues to increase, the demand for
automated testing and maintenance tools is growing exponentially. To meet this
urgent need, we propose a new assertion generation method based on Hardware
Description Language (HDL). This method combines a lightweight,
parameter-adjustable large language model (LLM) with the Unsloth platform to
automatically generate test cases, thereby significantly reducing training
costs without sacrificing accuracy or generalization performance. Empirical
evaluation shows that our method can efficiently generate assertions that
strictly conform to the hardware logic. This framework provides a robust and
flexible solution to modern software testing and maintenance challenges.
https://github.com/liusu-orange/AutoAssert-1 and
https://gitee.com/OpenBPU/auto-assert1 are the locations of the source code.

</details>


### [11] [Extracting Overlapping Microservices from Monolithic Code via Deep Semantic Embeddings and Graph Neural Network-Based Soft Clustering](https://arxiv.org/abs/2508.07486)
*Morteza Ziabakhsh,Kiyan Rezaee,Sadegh Eskandari,Seyed Amir Hossein Tabatabaei,Mohammad M. Ghassemi*

Main category: cs.SE

TL;DR: This paper introduces Mo2oM, a soft clustering framework for converting monolithic systems to microservices, enabling components to overlap across services. Using modern embeddings and graph neural networks, Mo2oM outperforms previous methods across multiple modularity and efficiency metrics on open-source benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current trends in software architecture favor shifting from monolithic to microservices for enhanced scalability, maintainability, and deployment flexibility. However, most existing methods use hard clustering, assigning components to only one microservice, which can increase coupling between services and reduce cohesion within them.

Method: The paper proposes Mo2oM, a soft clustering framework for microservice extraction. It allows software components to probabilistically belong to multiple microservices. This is achieved by combining deep semantic embeddings and structural dependencies from method call graphs, with a graph neural network-based soft clustering algorithm to produce the final microservice decomposition.

Result: Mo2oM was evaluated on four open-source monolithic benchmarks and compared to eight state-of-the-art methods. It achieved up to 40.97% improvement in structural modularity, 58% improvement in inter-service call percentage, 26.16% improvement in interface number, and 38.96% improvement in non-extreme service size distribution.

Conclusion: Mo2oM provides a more effective strategy for microservice extraction by allowing overlapping components, leading to reduced coupling, improved cohesion, and better communication and modularity metrics.

Abstract: Modern software systems are increasingly shifting from monolithic
architectures to microservices to enhance scalability, maintainability, and
deployment flexibility. Existing microservice extraction methods typically rely
on hard clustering, assigning each software component to a single microservice.
This approach often increases inter-service coupling and reduces intra-service
cohesion. We propose Mo2oM (Monolithic to Overlapping Microservices), a
framework that formulates microservice extraction as a soft clustering problem,
allowing components to belong probabilistically to multiple microservices. This
approach is inspired by expert-driven decompositions, where practitioners
intentionally replicate certain software components across services to reduce
communication overhead. Mo2oM combines deep semantic embeddings with structural
dependencies extracted from methodcall graphs to capture both functional and
architectural relationships. A graph neural network-based soft clustering
algorithm then generates the final set of microservices. We evaluate Mo2oM on
four open-source monolithic benchmarks and compare it against eight
state-of-the-art baselines. Our results demonstrate that Mo2oM achieves
improvements of up to 40.97% in structural modularity (balancing cohesion and
coupling), 58% in inter-service call percentage (communication overhead),
26.16% in interface number (modularity and decoupling), and 38.96% in
non-extreme distribution (service size balance) across all benchmarks.

</details>


### [12] [Adopting Road-Weather Open Data in Route Recommendation Engine](https://arxiv.org/abs/2508.07881)
*Henna Tammia,Benjamin Kämä,Ella Peltonen*

Main category: cs.SE

TL;DR: The paper analyzes challenges in using Finland's large-scale road sensor data, develops an efficient data utilization methodology, and demonstrates successful personalized route recommendations for different driver profiles.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of working with large-scale, versatile road weather and traffic datasets by enabling more efficient and practical application usage, specifically for personalized road recommendations.

Method: The paper discusses data qualities, preprocessing phases, and integrates machine learning tools. It uses DigiTraffic attributes as a case study and develops a methodology for efficient data utilization tailored to a personalized routing engine.

Result: Validation on real-world data demonstrates the system can efficiently identify and recommend personalized routes for three distinct driver profiles.

Conclusion: The paper concludes that with a thorough methodology, DigiTraffic data can be effectively leveraged for personalized road recommendations, improving driver experiences for different needs.

Abstract: Digitraffic, Finland's open road data interface, provides access to
nationwide road sensors with more than 2,300 real-time attributes from 1,814
stations. However, efficiently utilizing such a versatile data API for a
practical application requires a deeper understanding of the data qualities,
preprocessing phases, and machine learning tools. This paper discusses the
challenges of large-scale road weather and traffic data. We go through the
road-weather-related attributes from DigiTraffic as a practical example of
processes required to work with such a dataset. In addition, we provide a
methodology for efficient data utilization for the target application, a
personalized road recommendation engine based on a simple routing application.
We validate our solution based on real-world data, showing we can efficiently
identify and recommend personalized routes for three different driver profiles.

</details>


### [13] [SHIELDA: Structured Handling of Exceptions in LLM-Driven Agentic Workflows](https://arxiv.org/abs/2508.07935)
*Jingwen Zhou,Jieshan Chen,Qinghua Lu,Dehai Zhao,Liming Zhu*

Main category: cs.SE

TL;DR: Current LLM agentic workflows struggle with robust exception handling. This paper proposes SHIELDA, a comprehensive exception handling framework that classifies, traces, and systematically recovers from workflow failures. Validated on the AutoPR agent, SHIELDA shows improved, cross-phase recovery and reliability.


<details>
  <summary>Details</summary>
Motivation: LLM-powered agentic systems increasingly manage complex workflows, autonomously reasoning and planning rather than following fixed steps. However, exception handling in these workflows is weak—most current approaches do not trace failures to their actual root causes during reasoning, nor do they offer robust recovery strategies if initial fixes fail. This paper seeks to improve reliability and recovery in such agent systems.

Method: The authors first create a detailed taxonomy covering 36 types of exceptions across 12 agent artifacts. They then introduce SHIELDA, a modular framework that uses an exception classifier and a pattern registry to identify and apply appropriate handling strategies. SHIELDA's structured executor enables phase-aware recovery, connecting run-time exceptions to their source in the reasoning phase.

Result: The paper validates SHIELDA with a case study using the AutoPR agent, showing that the framework can effectively manage exceptions—including those originating in reasoning—by employing structured, multi-phase recovery strategies.

Conclusion: SHIELDA significantly improves exception handling in LLM agentic workflows by offering systematic phase-aware recovery, tracing root causes, and employing composable recovery patterns, making LLM-driven systems more robust and reliable.

Abstract: Large Language Model (LLM) agentic systems are software systems powered by
LLMs that autonomously reason, plan, and execute multi-step workflows to
achieve human goals, rather than merely executing predefined steps. During
execution, these workflows frequently encounter exceptions. Existing exception
handling solutions often treat exceptions superficially, failing to trace
execution-phase exceptions to their reasoning-phase root causes. Furthermore,
their recovery logic is brittle, lacking structured escalation pathways when
initial attempts fail. To tackle these challenges, we first present a
comprehensive taxonomy of 36 exception types across 12 agent artifacts.
Building on this, we propose SHIELDA (Structured Handling of Exceptions in
LLM-Driven Agentic Workflows), a modular runtime exception handling framework
for LLM agentic workflows. SHIELDA uses an exception classifier to select a
predefined exception handling pattern from a handling pattern registry. These
patterns are then executed via a structured handling executor, comprising local
handling, flow control, and state recovery, to enable phase-aware recovery by
linking exceptions to their root causes and facilitating composable strategies.
We validate SHIELDA's effectiveness through a case study on the AutoPR agent,
demonstrating effective, cross-phase recovery from a reasoning-induced
exception.

</details>


### [14] [Exploring the Challenges and Opportunities of AI-assisted Codebase Generation](https://arxiv.org/abs/2508.07966)
*Philipp Eibl,Sadra Sabouri,Souti Chattopadhyay*

Main category: cs.SE

TL;DR: Developers are dissatisfied with AI codebase assistants, mainly due to issues with functionality, code quality, and communication. The study highlights major challenges and barriers to adoption and offers insights for improving future CBAs.


<details>
  <summary>Details</summary>
Motivation: While AI code assistants for generating small code snippets are well studied, more advanced codebase-level assistants (CBAs) remain less commonly adopted and poorly understood. Understanding developer interaction and dissatisfaction is key to improving their utility.

Method: The paper conducted a counterbalanced user study and interviews with 16 students and developers working on tasks using CBAs. Additionally, a survey of 21 commercial CBAs was performed to compare their features with user challenges.

Result: Users varied their prompt composition but overall satisfaction with CBAs was low (mean=2.8/5). Functionality was the largest complaint (77%), followed by code quality (42%) and communication issues (25%). Six key challenges and five barriers to adoption were identified.

Conclusion: CBAs currently fall short of developer expectations, with fundamental challenges and barriers to adoption identified. There are promising design opportunities to improve their efficiency and usefulness through better addressing developer needs.

Abstract: Recent AI code assistants have significantly improved their ability to
process more complex contexts and generate entire codebases based on a textual
description, compared to the popular snippet-level generation. These codebase
AI assistants (CBAs) can also extend or adapt codebases, allowing users to
focus on higher-level design and deployment decisions. While prior work has
extensively studied the impact of snippet-level code generation, this new class
of codebase generation models is relatively unexplored. Despite initial
anecdotal reports of excitement about these agents, they remain less frequently
adopted compared to snippet-level code assistants. To utilize CBAs better, we
need to understand how developers interact with CBAs, and how and why CBAs fall
short of developers' needs. In this paper, we explored these gaps through a
counterbalanced user study and interview with (n = 16) students and developers
working on coding tasks with CBAs. We found that participants varied the
information in their prompts, like problem description (48% of prompts),
required functionality (98% of prompts), code structure (48% of prompts), and
their prompt writing process. Despite various strategies, the overall
satisfaction score with generated codebases remained low (mean = 2.8, median =
3, on a scale of one to five). Participants mentioned functionality as the most
common factor for dissatisfaction (77% of instances), alongside poor code
quality (42% of instances) and communication issues (25% of instances). We
delve deeper into participants' dissatisfaction to identify six underlying
challenges that participants faced when using CBAs, and extracted five barriers
to incorporating CBAs into their workflows. Finally, we surveyed 21 commercial
CBAs to compare their capabilities with participant challenges and present
design opportunities for more efficient and useful CBAs.

</details>


### [15] [PyVeritas: On Verifying Python via LLM-Based Transpilation and Bounded Model Checking for C](https://arxiv.org/abs/2508.08171)
*Pedro Orvalho,Marta Kwiatkowska*

Main category: cs.SE

TL;DR: PyVeritas uses LLMs to accurately transpile Python to C, enabling formal verification and fault localization for Python programs via C model checkers, with up to 80-90% accuracy demonstrated on benchmarks.


<details>
  <summary>Details</summary>
Motivation: While Python is widely used for general-purpose programming, it lacks strong formal verification tools. In contrast, languages like C have mature model checking tools that support symbolic reasoning and fault localization. The complexity and low-level nature of existing Python-to-C transpilers have previously restricted formal verification for Python code.

Method: The authors introduce PyVeritas, a framework that uses Large Language Models (LLMs) to transpile Python code to C at a high level. The transpiled C code is then subjected to bounded model checking and MaxSAT-based fault localization, leveraging C's robust verification tool ecosystem.

Result: The empirical evaluation across two Python benchmarks reveals that LLM-based transpilation reaches 80-90% accuracy for some LLMs. This enables assertion-based verification and interpretable fault diagnosis for small, non-trivial Python programs.

Conclusion: PyVeritas effectively bridges the gap in formal verification tools for Python by utilizing LLM-driven transpilation and existing C verification technologies, markedly improving bug localization and verification capabilities for Python developers.

Abstract: Python has become the dominant language for general-purpose programming, yet
it lacks robust tools for formal verification. In contrast, programmers working
in languages such as C benefit from mature model checkers, for example CBMC,
which enable exhaustive symbolic reasoning and fault localisation. The inherent
complexity of Python, coupled with the verbosity and low-level nature of
existing transpilers (e.g., Cython), have historically limited the
applicability of formal verification to Python programs.
  In this paper, we propose PyVeritas, a novel framework that leverages Large
Language Models (LLMs) for high-level transpilation from Python to C, followed
by bounded model checking and MaxSAT-based fault localisation in the generated
C code. PyVeritas enables verification and bug localisation for Python code
using existing model checking tools for C. Our empirical evaluation on two
Python benchmarks demonstrates that LLM-based transpilation can achieve a high
degree of accuracy, up to 80--90% for some LLMs, enabling effective development
environment that supports assertion-based verification and interpretable fault
diagnosis for small yet non-trivial Python programs.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [16] [Checking Consistency of Event-driven Traces](https://arxiv.org/abs/2508.07855)
*Parosh Aziz Abdulla,Mohamed Faouzi Atig,R. Govind,Samuel Grahn,Ramanathan S. Thinniyam*

Main category: cs.PL

TL;DR: The paper models event-driven programs axiomatically, proves its equivalence to operational semantics, analyzes tractability, and finds consistency checking is NP-complete in general but polynomial without nested message posting. A prototype tool validates the approach.


<details>
  <summary>Details</summary>
Motivation: The motivation is to rigorously determine when a candidate execution of event-driven programs is consistent with the semantics—an important issue for correctness, debugging, and verification—especially given the popularity and complexity of event-driven paradigms.

Method: The paper employs formal modeling of event-driven programs via axiomatic semantics based on traces (execution graphs), proves equivalence with operational semantics, analyzes computational complexity, and implements a prototype tool for empirical validation.

Result: The study shows that checking execution consistency is NP-complete even with a bounded number of threads but polynomial when handlers do not post messages while processing. A prototype tool demonstrates practical applicability across benchmarks.

Conclusion: The paper concludes that consistency checking in event-driven programming is NP-complete in general, but there is a tractable fragment (no nested posting) where the problem becomes polynomial-time solvable. The proposed axiomatic semantics are proven equivalent to operational semantics and are practically validated via a prototype tool.

Abstract: Event-driven programming is a popular paradigm where the flow of execution is
controlled by two features: (1) shared memory and (2) sending and receiving of
messages between multiple handler threads (just called handler). Each handler
has a mailbox (modelled as a queue) for receiving messages, with the constraint
that the handler processes its messages sequentially. Executions of messages by
different handlers may be interleaved. A central problem in this setting is
checking whether a candidate execution is consistent with the semantics of
event-driven programs. In this paper, we propose an axiomatic semantics for
eventdriven programs based on the standard notion of traces (also known as
execution graphs). We prove the equivalence of axiomatic and operational
semantics. This allows us to rephrase the consistency problem axiomatically,
resulting in the event-driven consistency problem: checking whether a given
trace is consistent. We analyze the computational complexity of this problem
and show that it is NP-complete, even when the number of handler threads is
bounded. We then identify a tractable fragment: in the absence of nested
posting, where handlers do not post new messages while processing a message,
consistency checking can be performed in polynomial time. Finally, we implement
our approach in a prototype tool and report on experimental results on a wide
range of benchmarks.

</details>
