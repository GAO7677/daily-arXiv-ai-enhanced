{"id": "2507.23087", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23087", "abs": "https://arxiv.org/abs/2507.23087", "authors": ["Fabian Stiehle", "Hans Weytjens", "Ingo Weber"], "title": "On LLM-Assisted Generation of Smart Contracts from Business Processes", "comment": "Accepted at the Workshop on Distributed Ledger Technologies in\n  Business Process Management, At the International Conference for Business\n  Process Management (BPM), 2025", "summary": "Large language models (LLMs) have changed the reality of how software is\nproduced. Within the wider software engineering community, among many other\npurposes, they are explored for code generation use cases from different types\nof input. In this work, we present an exploratory study to investigate the use\nof LLMs for generating smart contract code from business process descriptions,\nan idea that has emerged in recent literature to overcome the limitations of\ntraditional rule-based code generation approaches. However, current LLM-based\nwork evaluates generated code on small samples, relying on manual inspection,\nor testing whether code compiles but ignoring correct execution. With this\nwork, we introduce an automated evaluation framework and provide empirical data\nfrom larger data sets of process models. We test LLMs of different types and\nsizes in their capabilities of achieving important properties of process\nexecution, including enforcing process flow, resource allocation, and\ndata-based conditions. Our results show that LLM performance falls short of the\nperfect reliability required for smart contract development. We suggest future\nwork to explore responsible LLM integrations in existing tools for code\ngeneration to ensure more reliable output. Our benchmarking framework can serve\nas a foundation for developing and evaluating such integrations.", "AI": {"tldr": "LLMs are being tested for generating smart contract code from business process descriptions, but their reliability is not yet sufficient for real-world requirements. The authors provide a new automated benchmarking framework and empirical results, suggesting further improvements and responsible integration are needed.", "motivation": "Traditional rule-based code generation approaches have limitations when translating business process descriptions into smart contract code. Recent advances with LLMs provide new opportunities, but existing work has not robustly evaluated the correctness and execution of generated code. There is a need for empirical, automated, and scalable evaluation approaches in this context.", "method": "The authors conducted an exploratory study using an automated evaluation framework to test various LLMs in generating smart contract code from business process descriptions. They used larger datasets of process models and measured LLMs' ability to enforce crucial aspects like process flow, resource allocation, and data-based conditions.", "result": "LLMs do not yet meet the high reliability standards required for smart contract development, as their code generation is imperfect. Empirical benchmarking with the new framework highlights these reliability gaps across different LLM types and sizes.", "conclusion": "While LLMs offer promise for smart contract generation, current performance is insufficient for production-level requirements. The introduced benchmarking framework establishes a foundation for responsible integration of LLMs and further research on improving reliability in code generation."}}
{"id": "2507.23118", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23118", "abs": "https://arxiv.org/abs/2507.23118", "authors": ["Mattia Di Profio", "Mingjun Zhong", "Yaji Sripada", "Marcel Jaspars"], "title": "FlowETL: An Autonomous Example-Driven Pipeline for Data Engineering", "comment": null, "summary": "The Extract, Transform, Load (ETL) workflow is fundamental for populating and\nmaintaining data warehouses and other data stores accessed by analysts for\ndownstream tasks. A major shortcoming of modern ETL solutions is the extensive\nneed for a human-in-the-loop, required to design and implement\ncontext-specific, and often non-generalisable transformations. While related\nwork in the field of ETL automation shows promising progress, there is a lack\nof solutions capable of automatically designing and applying these\ntransformations. We present FlowETL, a novel example-based autonomous ETL\npipeline architecture designed to automatically standardise and prepare input\ndatasets according to a concise, user-defined target dataset. FlowETL is an\necosystem of components which interact together to achieve the desired outcome.\nA Planning Engine uses a paired input-output datasets sample to construct a\ntransformation plan, which is then applied by an ETL worker to the source\ndataset. Monitoring and logging provide observability throughout the entire\npipeline. The results show promising generalisation capabilities across 14\ndatasets of various domains, file structures, and file sizes.", "AI": {"tldr": "FlowETL introduces an automated, example-based ETL pipeline that creates and applies transformation plans with minimal human input, achieving strong results across diverse datasets.", "motivation": "Current ETL solutions require extensive human intervention to design and implement context-specific data transformations that are not easily generalizable, demonstrating a need for more automated and general solutions.", "method": "FlowETL is introduced as an example-based autonomous ETL pipeline architecture. It uses an ecosystem of components including a Planning Engine that constructs transformation plans from paired input-output dataset samples, an ETL worker that executes these plans, and monitoring tools for observability.", "result": "The experiments demonstrate that FlowETL has promising generalization capabilities, effectively standardizing and preparing datasets across 14 different datasets with varied domains, structures, and sizes.", "conclusion": "FlowETL offers a significant advancement in ETL automation by reducing the need for manual, context-specific transformations, enabling automated, generalized dataset preparation through example-based planning and execution."}}
{"id": "2507.23120", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23120", "abs": "https://arxiv.org/abs/2507.23120", "authors": ["Jordi Cabot"], "title": "Vibe Modeling: Challenges and Opportunities", "comment": null, "summary": "There is a pressing need for better development methods and tools to keep up\nwith the growing demand and increasing complexity of new software systems. New\ntypes of user interfaces, the need for intelligent components, sustainability\nconcerns, ... bring new challenges that we need to handle. In the last years,\nmodel-driven engineering (MDE) has been key to improving the quality and\nproductivity of software development, but models themselves are becoming\nincreasingly complex to specify and manage. At the same time, we are witnessing\nthe growing popularity of vibe coding approaches that rely on Large Language\nModels (LLMs) to transform natural language descriptions into running code at\nthe expenses of code vulnerabilities, scalability issues and maintainability\nconcerns. In this paper, we introduce the concept of \\textit{vibe modeling} as\na novel approach to integrate the best of both worlds (AI and MDE) to speed up\nthe development of reliable complex systems. We outline the key concepts of\nvibe modeling and highlight the opportunities and open challenges it presents\nfor the future of modeling.", "AI": {"tldr": "This paper proposes 'vibe modeling'\u2014a novel approach that integrates AI (LLMs) with model-driven engineering to address growing complexity and demands in software development. It discusses the opportunities and challenges of this hybrid technique.", "motivation": "The paper is motivated by the increasing demand and complexity in software systems, as well as emerging challenges such as new user interfaces, intelligent components, and sustainability. Existing approaches like model-driven engineering (MDE) improve quality and productivity, but managing complex models is difficult. At the same time, large language models (LLMs) enable code generation from natural language, introducing new risks with code quality and maintainability.", "method": "The authors introduce 'vibe modeling' as a new approach that integrates AI-powered techniques (like LLMs) with model-driven engineering. This hybrid method aims to combine the strengths of both approaches to improve software development processes.", "result": "The paper outlines the key concepts of vibe modeling and discusses its potential benefits and challenges. It does not report empirical evaluation but sets an agenda for integrating AI and MDE.", "conclusion": "Vibe modeling is introduced as a promising direction for merging AI and model-driven engineering, with the goal of developing reliable, complex systems more efficiently. The approach offers new opportunities but also presents open problems that need further research."}}
{"id": "2507.23168", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23168", "abs": "https://arxiv.org/abs/2507.23168", "authors": ["Elmira Onagh", "Maleknaz Nayebi"], "title": "Extension Decisions in Open Source Software Ecosystem", "comment": "Paper published in JSS journal", "summary": "GitHub Marketplace is expanding by approximately 41% annually, with new\ntools; however, many additions replicate existing functionality. We study this\nphenomenon in the platform's largest segment, Continuous Integration (CI), by\nlinking 6,983 CI Actions to 3,869 providers and mining their version histories.\nOur graph model timestamps every functionality's debut, tracks its adoption,\nand clusters redundant tools. We find that approximately 65% of new CI Actions\nreplicate existing capabilities, typically within six months, and that a small\nset of first-mover Actions accounts for most subsequent forks and extensions.\nThese insights enable developers to choose the optimal moment to launch, target\nunmet functionality, and help maintainers eliminate redundant tools. We publish\nthe complete graph and dataset to encourage longitudinal research on innovation\nand competition in software ecosystems, and to provide practitioners with a\ndata-driven roadmap for identifying emerging trends and guiding product\nstrategy.", "AI": {"tldr": "The paper analyzes the growth and redundancy of CI tools in GitHub Marketplace, showing that most new tools duplicate existing features quickly. The results help developers identify opportunities for innovation and guide strategic tool launches, supported by a comprehensive dataset for further research.", "motivation": "The paper investigates the rapid growth and tool redundancy in the GitHub Marketplace, specifically within Continuous Integration (CI) tools, and aims to provide insights that will help developers and researchers understand innovation and competition dynamics in software ecosystems.", "method": "The authors linked 6,983 CI Actions to 3,869 providers, mined their version histories, and modeled the data as a graph. This graph model timestamps functionality debuts, tracks adoption, and clusters redundant tools.", "result": "About 65% of new CI Actions replicate existing capabilities, typically appearing within six months of the original. A small set of early Actions become the basis for most subsequent forks and extensions.", "conclusion": "The study offers actionable recommendations for developers and maintainers, such as when to launch new tools and which functionalities are underserved. The published dataset and model provide a resource for future research and practical guidance in software product strategy."}}
{"id": "2507.23151", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.23151", "abs": "https://arxiv.org/abs/2507.23151", "authors": ["Louis Rustenholz", "Pedro Lopez-Garcia", "Manuel V. Hermenegildo"], "title": "Abstractions of Sequences, Functions and Operators", "comment": "Under consideration for publication in STTT", "summary": "We present theoretical and practical results on the order theory of lattices\nof functions, focusing on Galois connections that abstract (sets of) functions\n- a topic known as higher-order abstract interpretation.\n  We are motivated by the challenge of inferring closed-form bounds on\nfunctions which are defined recursively, i.e. as the fixed point of an operator\nor, equivalently, as the solution to a functional equation. This has multiple\napplications in program analysis (e.g. cost analysis, loop acceleration,\ndeclarative language analysis) and in hybrid systems governed by differential\nequations.\n  Our main contribution is a new family of constraint-based abstract domains\nfor abstracting numerical functions, B-bound domains, which abstract a function\nf by a conjunction of bounds from a preselected set of boundary functions. They\nallow inferring highly non-linear numerical invariants, which classical\nnumerical abstract domains struggle with. We uncover a convexity property in\nthe constraint space that simplifies, and, in some cases, fully automates,\ntransfer function design.\n  We also introduce domain abstraction, a functor that lifts arbitrary mappings\nin value space to Galois connections in function space. This supports\nabstraction from symbolic to numerical functions (i.e. size abstraction), and\nenables dimensionality reduction of equations.\n  We base our constructions of transfer functions on a simple operator\nlanguage, starting with sequences, and extending to more general functions,\nincluding multivariate, piecewise, and non-discrete domains.", "AI": {"tldr": "This paper proposes new theoretical tools to abstract and analyze recursively defined numerical functions, enabling automatic inference of complex bounds and supporting advanced program analysis and hybrid systems.", "motivation": "The primary motivation is to enable automatic inference of closed-form bounds for recursively defined functions, which is important for applications in program analysis (such as cost analysis and loop acceleration) and hybrid systems involving differential equations.", "method": "The authors develop theoretical foundations in order theory using Galois connections and construct a new family of abstract domains (B-bound domains) for functions. They also introduce a domain abstraction functor and build transfer functions using an operator language for various classes of functions.", "result": "The introduced B-bound domains allow the abstraction of functions via bounds derived from selected boundary functions, enabling the inference of complex, non-linear invariants. The work also identifies a convexity property in constraint space that can simplify or automate transfer function construction. The domain abstraction functor aids abstraction and dimensionality reduction.", "conclusion": "The paper introduces new constraint-based abstract domains (B-bound domains) for abstracting numerical functions, facilitating the inference of complex non-linear invariants. It also presents a domain abstraction functor to generalize mappings to function space, supporting advanced program analysis and dimensionality reduction."}}
{"id": "2507.23178", "categories": ["cs.SE", "cs.AI", "I.2.5"], "pdf": "https://arxiv.org/pdf/2507.23178", "abs": "https://arxiv.org/abs/2507.23178", "authors": ["Siyuan Liu", "Zhice Yang", "Huangxun Chen"], "title": "AutoBridge: Automating Smart Device Integration with Centralized Platform", "comment": "14 pages, 12 figures, under review", "summary": "Multimodal IoT systems coordinate diverse IoT devices to deliver\nhuman-centered services. The ability to incorporate new IoT devices under the\nmanagement of a centralized platform is an essential requirement. However, it\nrequires significant human expertise and effort to program the complex IoT\nintegration code that enables the platform to understand and control the device\nfunctions. Therefore, we propose AutoBridge to automate IoT integration code\ngeneration. Specifically, AutoBridge adopts a divide-and-conquer strategy: it\nfirst generates device control logic by progressively retrieving\ndevice-specific knowledge, then synthesizes platformcompliant integration code\nusing platform-specific knowledge. To ensure correctness, AutoBridge features a\nmulti-stage debugging pipeline, including an automated debugger for virtual IoT\ndevice testing and an interactive hardware-in-the-loop debugger that requires\nonly binary user feedback (yes and no) for real-device verification. We\nevaluate AutoBridge on a benchmark of 34 IoT devices across two open-source IoT\nplatforms. The results demonstrate that AutoBridge can achieves an average\nsuccess rate of 93.87% and an average function coverage of 94.87%, without any\nhuman involvement. With minimal binary yes and no feedback from users, the code\nis then revised to reach 100% function coverage. A user study with 15\nparticipants further shows that AutoBridge outperforms expert programmers by\n50% to 80% in code accuracy, even when the programmers are allowed to use\ncommercial code LLMs.", "AI": {"tldr": "AutoBridge automatically generates and debugs IoT integration code with high accuracy, greatly reducing the need for human expertise and exceeding expert-level performance.", "motivation": "Integrating new IoT devices into centralized platforms typically requires complex and expert-level programming of integration code, making it hard, time-consuming, and inaccessible for non-experts.", "method": "The authors propose AutoBridge, an automated system for generating IoT integration code. AutoBridge uses a divide-and-conquer approach: it retrieves device- and platform-specific knowledge to automate code synthesis. It also has a multi-stage debugging process including virtual device testing and hardware-in-the-loop debugging using simple binary user feedback.", "result": "AutoBridge was evaluated on 34 IoT devices across two open-source IoT platforms, achieving an average success rate of 93.87% and function coverage of 94.87% autonomously. With minimal binary feedback from users, function coverage reached 100%. In a user study, AutoBridge outperformed expert programmers (even those using LLMs) by 50%-80% in code accuracy.", "conclusion": "AutoBridge significantly reduces the human effort and expertise required to integrate new IoT devices, automating code generation and debugging to outperform expert programmers."}}
{"id": "2507.23205", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23205", "abs": "https://arxiv.org/abs/2507.23205", "authors": ["Hebi Li", "Forrest Sheng Bao", "Qi Xiao", "Jin Tian"], "title": "Kernel-FFI: Transparent Foreign Function Interfaces for Interactive Notebooks", "comment": null, "summary": "Foreign Function Interfaces (FFIs) are essential for enabling\ninteroperability between programming languages, yet existing FFI solutions are\nill-suited for the dynamic, interactive workflows prevalent in modern notebook\nenvironments such as Jupyter. Current approaches require extensive manual\nconfiguration, introduce significant boilerplate, and often lack support for\nrecursive calls and object-oriented programming (OOP) constructs-features\ncritical for productive, multi-language development.\n  We present Kernel-FFI, a transparent, language-agnostic framework that\nenables seamless cross-language function calls and object manipulation within\ninteractive notebooks. Kernel-FFI employs source-level transformation to\nautomatically rewrite cross-language invocations, eliminating the need for\nmanual bindings or boilerplate. Kernel-FFI provides robust support for OOP by\nenabling foreign object referencing and automatic resource management across\nlanguage boundaries. Furthermore, to address the blocking nature of Jupyter\nkernels and support recursive and asynchronous foreign calls, we introduce a\nnovel side-channel communication mechanism. Our tool will be open-sourced and\navailable at https://codepod.io/docs/kernel-ffi", "AI": {"tldr": "Kernel-FFI is a new, automatic, and language-agnostic FFI framework designed specifically for interactive notebooks like Jupyter, making it much easier to mix languages, handle objects, and invoke functions across languages with minimal effort and without traditional constraints.", "motivation": "Foreign Function Interfaces (FFIs) are crucial for interoperability between programming languages, but current FFI tools are cumbersome and incompatible with the interactive workflows of modern notebook environments like Jupyter. These limitations hinder productivity in multi-language development, especially due to manual configuration, excessive boilerplate, and poor support for features like recursive calls and object-oriented programming.", "method": "Kernel-FFI uses source-level transformation to automatically rewrite cross-language calls within code, removing the need for manual binding or boilerplate. It also introduces a novel side-channel communication mechanism for managing asynchronous and recursive foreign function calls in environments where standard kernel communication is blocking. The framework provides full support for object-oriented programming, including foreign object referencing and automatic resource management across different programming languages.", "result": "Kernel-FFI enables transparent, language-agnostic cross-language function calls and object manipulation in interactive notebooks. It supports robust object-oriented programming features and overcomes the blocking limitations of Jupyter kernels, allowing recursive and asynchronous foreign calls. The framework will be open-sourced and made publicly available.", "conclusion": "Kernel-FFI significantly streamlines multi-language development in interactive notebook environments by automating cross-language invocations, supporting OOP features, and enabling asynchronous, recursive calls without boilerplate or manual configuration."}}
{"id": "2507.23269", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.23269", "abs": "https://arxiv.org/abs/2507.23269", "authors": ["Peter Fettke", "Fabiana Fournier", "Lior Limonad", "Andreas Metzger", "Stefanie Rinderle-Ma", "Barbara Weber"], "title": "XABPs: Towards eXplainable Autonomous Business Processes", "comment": null, "summary": "Autonomous business processes (ABPs), i.e., self-executing workflows\nleveraging AI/ML, have the potential to improve operational efficiency, reduce\nerrors, lower costs, improve response times, and free human workers for more\nstrategic and creative work. However, ABPs may raise specific concerns\nincluding decreased stakeholder trust, difficulties in debugging, hindered\naccountability, risk of bias, and issues with regulatory compliance. We argue\nfor eXplainable ABPs (XABPs) to address these concerns by enabling systems to\narticulate their rationale. The paper outlines a systematic approach to XABPs,\ncharacterizing their forms, structuring explainability, and identifying key BPM\nresearch challenges towards XABPs.", "AI": {"tldr": "ABPs can revolutionize business processes but bring trust, accountability, and compliance issues. Making ABPs explainable (XABPs) is essential, and the paper outlines methods and research challenges for achieving this.", "motivation": "Autonomous business processes (ABPs), powered by AI/ML, can greatly enhance business operations but also introduce significant concerns such as trust, debugging challenges, accountability issues, biases, and regulatory compliance risks.", "method": "The paper presents a systematic approach to creating explainable autonomous business processes (XABPs), including the characterization of explainability forms, structuring explainability, and identification of key research challenges in business process management (BPM) for XABPs.", "result": "The authors propose a structured framework for integrating explainability into ABPs, define the forms that explainability can take in these systems, and highlight essential research problems that need to be addressed for effective adoption of XABPs.", "conclusion": "XABPs are necessary for balancing the operational advantages of ABPs with stakeholder concerns, and a systematic approach to explainability will be crucial for building trustworthy, reliable, and compliant autonomous business processes."}}
{"id": "2507.23348", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23348", "abs": "https://arxiv.org/abs/2507.23348", "authors": ["Han Li", "Yuling Shi", "Shaoxin Lin", "Xiaodong Gu", "Heng Lian", "Xin Wang", "Yantao Jia", "Tao Huang", "Qianxiang Wang"], "title": "SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution", "comment": "Our code and data are available at\n  https://github.com/YerbaPage/SWE-Debate", "summary": "Issue resolution has made remarkable progress thanks to the advanced\nreasoning capabilities of large language models (LLMs). Recently, agent-based\nframeworks such as SWE-agent have further advanced this progress by enabling\nautonomous, tool-using agents to tackle complex software engineering tasks.\nWhile existing agent-based issue resolution approaches are primarily based on\nagents' independent explorations, they often get stuck in local solutions and\nfail to identify issue patterns that span across different parts of the\ncodebase. To address this limitation, we propose SWE-Debate, a competitive\nmulti-agent debate framework that encourages diverse reasoning paths and\nachieves more consolidated issue localization. SWE-Debate first creates\nmultiple fault propagation traces as localization proposals by traversing a\ncode dependency graph. Then, it organizes a three-round debate among\nspecialized agents, each embodying distinct reasoning perspectives along the\nfault propagation trace. This structured competition enables agents to\ncollaboratively converge on a consolidated fix plan. Finally, this consolidated\nfix plan is integrated into an MCTS-based code modification agent for patch\ngeneration. Experiments on the SWE-bench benchmark show that SWE-Debate\nachieves new state-of-the-art results in open-source agent frameworks and\noutperforms baselines by a large margin.", "AI": {"tldr": "SWE-Debate introduces a multi-agent debate system for software issue resolution that coordinates diverse reasoning agents to consolidate solutions, resulting in superior performance compared to existing approaches.", "motivation": "Existing agent-based issue resolution frameworks in software engineering rely largely on independent exploration by agents. This approach often leads to agents getting stuck in local solutions and failing to recognize issue patterns that span multiple parts of the codebase.", "method": "The authors propose SWE-Debate, a competitive multi-agent debate framework. SWE-Debate generates multiple fault propagation traces as localization proposals by traversing a code dependency graph. It then conducts a three-round debate among specialized agents, each representing distinct reasoning strategies along the propagation trace. The consensus fix plan from the debate is used by a Monte Carlo Tree Search (MCTS)-based code modification agent for generating patches.", "result": "SWE-Debate achieves state-of-the-art performance on the SWE-bench benchmark. The approach notably outperforms existing open-source agent frameworks and other baseline methods by a significant margin.", "conclusion": "A competitive multi-agent debate framework, SWE-Debate, can overcome the limitations of independent exploration, consolidate reasoning paths, and significantly improve issue resolution in complex software engineering tasks."}}
{"id": "2507.23356", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23356", "abs": "https://arxiv.org/abs/2507.23356", "authors": ["Shmulik Froimovich", "Raviv Gal", "Wesam Ibraheem", "Avi Ziv"], "title": "Quality Evaluation of COBOL to Java Code Transformation", "comment": "Submitted to ASE 2025", "summary": "We present an automated evaluation system for assessing COBOL-to-Java code\ntranslation within IBM's watsonx Code Assistant for Z (WCA4Z). The system\naddresses key challenges in evaluating LLM-based translators, including model\nopacity and the complexity of translation quality assessment. Our approach\ncombines analytic checkers with LLM-as-a-judge (LaaJ) techniques to deliver\nscalable, multi-faceted evaluations. The system supports continuous integration\nworkflows, enables large-scale benchmarking, and reduces reliance on manual\nreview. We describe the system architecture, evaluation strategies, and\nreporting mechanisms that provide actionable insights for developers and\nproject managers, facilitating the evolution of high-quality, modernized\ncodebases.", "AI": {"tldr": "An automated evaluation system combining analytic and LLM-based methods provides scalable, continuous assessment of COBOL-to-Java translations, improving benchmarking and reducing manual review for modernization projects.", "motivation": "Assessing the quality of COBOL-to-Java code translation, especially through LLM-based translators, is challenging due to model opacity and the difficulty in evaluating translation outputs at scale. There is a need for automated, objective, and comprehensive evaluation methods to support modernization projects and continuous integration workflows.", "method": "The paper introduces an automated evaluation system that integrates analytic checkers with LLM-as-a-judge (LaaJ) techniques. The system is designed to be scalable and to support continuous integration, providing multi-faceted assessment and benchmarking of translation quality. They detail the system\u2019s architecture, the evaluation strategies employed, and the reporting mechanisms for stakeholders.", "result": "The system enables automated, scalable, and continuous evaluation of COBOL-to-Java translations. It reduces the dependency on manual review, supports large-scale benchmarking, and delivers actionable feedback to developers and project managers, thus aiding modernization efforts.", "conclusion": "The proposed evaluation system addresses critical challenges in assessing LLM-based code translation through a hybrid approach, supporting quality assurance and modernization at scale. The combination of analytic and LLM-based judging techniques offers reliable, multi-dimensional evaluation, streamlining the process for developers and managers."}}
{"id": "2507.23361", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23361", "abs": "https://arxiv.org/abs/2507.23361", "authors": ["Silin Chen", "Shaoxin Lin", "Xiaodong Gu", "Yuling Shi", "Heng Lian", "Longfei Yun", "Dong Chen", "Weiguo Sun", "Lin Cao", "Qianxiang Wang"], "title": "SWE-Exp: Experience-Driven Software Issue Resolution", "comment": "Our code and data are available at\n  https://github.com/YerbaPage/SWE-Exp", "summary": "Recent advances in large language model (LLM) agents have shown remarkable\nprogress in software issue resolution, leveraging advanced techniques such as\nmulti-agent collaboration and Monte Carlo Tree Search (MCTS). However, current\nagents act as memoryless explorers - treating each problem separately without\nretaining or reusing knowledge from previous repair experiences. This leads to\nredundant exploration of failed trajectories and missed chances to adapt\nsuccessful issue resolution methods to similar problems. To address this\nproblem, we introduce SWE-Exp, an experience - enhanced approach that distills\nconcise and actionable experience from prior agent trajectories, enabling\ncontinuous learning across issues. Our method introduces a multi-faceted\nexperience bank that captures both successful and failed repair attempts.\nSpecifically, it extracts reusable issue resolution knowledge at different\nlevels - from high-level problem comprehension to specific code changes.\nExperiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%\nPass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach\nestablishes a new paradigm in which automated software engineering agents\nsystematically accumulate and leverage repair expertise, fundamentally shifting\nfrom trial-and-error exploration to strategic, experience-driven issue\nresolution.", "AI": {"tldr": "SWE-Exp enhances LLM agents for software issue resolution by enabling experience accumulation and reuse, significantly boosting problem-solving performance and creating a more strategic, knowledge-driven workflow.", "motivation": "Current large language model (LLM) agents for software issue resolution do not utilize prior experience, resulting in redundant efforts and inability to apply learned methods to new but similar issues. This inefficiency motivates the search for a way to enable agents to learn from and reuse past repair experiences.", "method": "The proposed method, SWE-Exp, creates an experience-enhanced approach by building a multi-faceted experience bank. This bank collects and distills actionable experience from previous agent trajectories, capturing both successful and failed attempts at various granularity levels, such as overall problem understanding and specific code edits. This allows agents to leverage stored knowledge in future problem-solving endeavors.", "result": "Experiments demonstrate that SWE-Exp achieves a state-of-the-art resolution rate of 41.6% Pass@1 on the SWE-bench-Verified dataset when used with open-source agent frameworks.", "conclusion": "SWE-Exp establishes a new paradigm for automated software engineering agents, enabling them to systematically learn from and leverage past repair attempts. This shifts the field from inefficient trial-and-error approaches to more effective, experience-driven problem resolution."}}
{"id": "2507.23370", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23370", "abs": "https://arxiv.org/abs/2507.23370", "authors": ["Trae Research Team", "Pengfei Gao", "Zhao Tian", "Xiangxin Meng", "Xinchen Wang", "Ruida Hu", "Yuanan Xiao", "Yizhou Liu", "Zhao Zhang", "Junjie Chen", "Cuiyun Gao", "Yun Lin", "Yingfei Xiong", "Chao Peng", "Xia Liu"], "title": "Trae Agent: An LLM-based Agent for Software Engineering with Test-time Scaling", "comment": "Pengfei Gao and Zhao Tian contributed equally to this technical\n  report", "summary": "Software issue resolution is a critical challenge in software engineering and\nhas garnered increasing attention in recent years. With the rapid advancement\nof large language models (LLMs), substantial progress has been made in\naddressing real-world software engineering tasks. Recent studies have\nintroduced ensemble reasoning techniques to enhance the performance of\nLLM-based issue resolution. However, existing prompting-based methods still\nface limitations in effectively exploring large ensemble spaces and lack the\ncapacity for repository-level understanding, both of which constrain their\noverall effectiveness. In this paper, we propose Trae Agent, the first\nagent-based ensemble reasoning approach for repository-level issue resolution.\nTrae Agent formulates our goal as an optimal solution search problem and\naddresses two key challenges, i.e., large ensemble spaces and repository-level\nunderstanding, through modular agents for generation, pruning, and selection.\nWe conduct extensive experiments using three leading LLMs on the widely-adopted\nSWE-bench benchmark, comparing Trae Agent against four state-of-the-art\nensemble reasoning techniques. Experimental results demonstrate that Trae Agent\nconsistently achieves superior performance, with an average improvement of\n10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first\nplace on the SWE-bench Verified leaderboard, with a notable Pass@1 score of\n75.20%. We are pleased to release Trae Agent as an open-source project to\nsupport the research community, with all resources available at\nhttps://github.com/bytedance/trae-agent.", "AI": {"tldr": "Trae Agent is a new agent-based system for software issue resolution that significantly outperforms previous LLM ensemble methods, notably by handling large solution spaces and repository-level challenges. It ranks first in the SWE-bench benchmark and is available open-source.", "motivation": "Software issue resolution remains a core challenge in software engineering, especially with the increasing potential of large language models (LLMs). Recent ensemble reasoning approaches have improved LLM-based issue resolution, but face difficulties with effectively searching large ensemble spaces and understanding code at the repository level. These limitations reduce their practical effectiveness.", "method": "The authors propose Trae Agent, an agent-based ensemble reasoning approach designed explicitly for repository-level software issue resolution. Trae Agent frames issue resolution as an optimal solution search and utilizes modular agents for three main tasks: generation of solutions, pruning of suboptimal solutions, and selection of the best solution. The method is evaluated using three state-of-the-art LLMs on the SWE-bench benchmark and compared with four leading ensemble reasoning approaches.", "result": "Experiments show that Trae Agent outperforms all baseline methods, achieving an average Pass@1 improvement of 10.22%. Notably, it secures first place on the SWE-bench Verified leaderboard with a Pass@1 score of 75.20%.", "conclusion": "Trae Agent, through its agent-based and modular design, effectively addresses the limitations of previous prompting-based methods, particularly in handling large ensemble spaces and repository-level context. It establishes itself as a new state-of-the-art approach for LLM-based software issue resolution and is released open-source."}}
{"id": "2507.23425", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23425", "abs": "https://arxiv.org/abs/2507.23425", "authors": ["Daphn\u00e9 Larrivain", "Shinhyung Yang", "Wilhelm Hasselbring"], "title": "Dynamic and Static Analysis of Python Software with Kieker Including Reconstructed Architectures", "comment": "9 pages, 9 figures", "summary": "The Kieker observability framework is a tool that provides users with the\nmeans to design a custom observability pipeline for their application.\nOriginally tailored for Java, supporting Python with Kieker is worthwhile.\nPython's popularity has exploded over the years, thus making structural\ninsights of Python applications highly valuable. Our Python analysis pipeline\ncombines static and dynamic analysis in order to build a complete picture of a\ngiven system.", "AI": {"tldr": "This paper extends the Kieker observability framework to support Python by developing a pipeline that combines static and dynamic analysis, addressing the growing need for insights into Python applications.", "motivation": "The motivation for this paper is the growing popularity of Python and the increasing need for structural insights into Python applications. Since the Kieker observability framework was originally designed for Java, there is value in extending its capabilities to include Python.", "method": "The approach involves developing a Python analysis pipeline that integrates both static and dynamic analysis, allowing for comprehensive observation and understanding of Python applications within the Kieker framework.", "result": "The result is an enhanced Kieker framework that supports Python, enabling users to analyze Python applications with a combination of static and dynamic techniques to gain more complete system insights.", "conclusion": "Supporting Python in the Kieker observability framework extends its usefulness and applicability, allowing users to design custom observability pipelines for Python applications and gain structural insights similar to those possible with Java."}}
{"id": "2507.23640", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23640", "abs": "https://arxiv.org/abs/2507.23640", "authors": ["Samah Kansab", "Mohammed Sayagh", "Francis Bordeleau", "Ali Tizghadam"], "title": "An Empirical Study on the Amount of Changes Required for Merge Request Acceptance", "comment": null, "summary": "Code review (CR) is essential to software development, helping ensure that\nnew code is properly integrated. However, the CR process often involves\nsignificant effort, including code adjustments, responses to reviewers, and\ncontinued implementation. While past studies have examined CR delays and\niteration counts, few have investigated the effort based on the volume of code\nchanges required, especially in the context of GitLab Merge Requests (MRs),\nwhich remains underexplored. In this paper, we define and measure CR effort as\nthe amount of code modified after submission, using a dataset of over 23,600\nMRs from four GitLab projects. We find that up to 71% of MRs require\nadjustments after submission, and 28% of these involve changes to more than 200\nlines of code. Surprisingly, this effort is not correlated with review time or\nthe number of participants. To better understand and predict CR effort, we\ntrain an interpretable machine learning model using metrics across multiple\ndimensions: text features, code complexity, developer experience, review\nhistory, and branching. Our model achieves strong performance (AUC 0.84-0.88)\nand reveals that complexity, experience, and text features are key predictors.\nHistorical project characteristics also influence current review effort. Our\nfindings highlight the feasibility of using machine learning to explain and\nanticipate the effort needed to integrate code changes during review.", "AI": {"tldr": "This paper studies code review effort in GitLab by measuring code changes after submission and develops a machine learning model that predicts this effort with high accuracy, finding key factors like code complexity, developer experience, and textual features are most predictive.", "motivation": "While code review is essential, the effort required for code changes during review is underexplored, especially in GitLab Merge Requests. Existing studies have focused on delays and iterations, not on the actual code modification effort and its predictors.", "method": "Analyzed over 23,600 GitLab Merge Requests from four projects, defined code review (CR) effort as post-submission code changes, and trained interpretable machine learning models using multifaceted metrics including text, code complexity, developer experience, review history, and branching.", "result": "Up to 71% of MRs needed post-submission adjustments, with 28% involving over 200 lines of code. CR effort was not linked to review time or number of participants. The machine learning model achieved high performance (AUC 0.84-0.88) and identified code complexity, experience, and text features as key predictors.", "conclusion": "Machine learning models can effectively predict code review effort based on code complexity, developer experience, and text features, offering strong performance in anticipating integration effort during code review."}}
