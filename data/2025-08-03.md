<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [On LLM-Assisted Generation of Smart Contracts from Business Processes](https://arxiv.org/abs/2507.23087)
*Fabian Stiehle,Hans Weytjens,Ingo Weber*

Main category: cs.SE

TL;DR: The paper finds that while LLMs are promising for generating smart contract code from business process descriptions, they are not yet reliable enough for practical use. The study introduces an automated framework for more thorough evaluation and suggests that future efforts should focus on safer integration of LLMs in code generation tools.


<details>
  <summary>Details</summary>
Motivation: Current rule-based code generation approaches have limitations in generating smart contract code from business process descriptions. The community is interested in using large language models (LLMs) for this task, but current evaluations are insufficient, as they mostly rely on small samples and surface-level checks.

Method: The paper introduces an automated evaluation framework to empirically assess LLM-generated smart contract code. The authors tested LLMs of different types and sizes on large data sets of process models, evaluating the extent to which the codes fulfill essential execution properties such as process flow, resource allocation, and data-based conditions.

Result: LLMs do not yet provide the perfect reliability needed for smart contract development. The performance of LLMs in generating reliable smart contract code falls short when tested on comprehensive properties and larger datasets.

Conclusion: LLMs are not yet fully reliable for smart contract code generation from business process descriptions. The proposed automated evaluation framework offers a basis for further research and development, calling for responsible integration of LLMs into code generation tools to improve dependability.

Abstract: Large language models (LLMs) have changed the reality of how software is
produced. Within the wider software engineering community, among many other
purposes, they are explored for code generation use cases from different types
of input. In this work, we present an exploratory study to investigate the use
of LLMs for generating smart contract code from business process descriptions,
an idea that has emerged in recent literature to overcome the limitations of
traditional rule-based code generation approaches. However, current LLM-based
work evaluates generated code on small samples, relying on manual inspection,
or testing whether code compiles but ignoring correct execution. With this
work, we introduce an automated evaluation framework and provide empirical data
from larger data sets of process models. We test LLMs of different types and
sizes in their capabilities of achieving important properties of process
execution, including enforcing process flow, resource allocation, and
data-based conditions. Our results show that LLM performance falls short of the
perfect reliability required for smart contract development. We suggest future
work to explore responsible LLM integrations in existing tools for code
generation to ensure more reliable output. Our benchmarking framework can serve
as a foundation for developing and evaluating such integrations.

</details>


### [2] [FlowETL: An Autonomous Example-Driven Pipeline for Data Engineering](https://arxiv.org/abs/2507.23118)
*Mattia Di Profio,Mingjun Zhong,Yaji Sripada,Marcel Jaspars*

Main category: cs.SE

TL;DR: FlowETL is a novel autonomous ETL system that uses example data pairs to automatically generate and apply transformations, reducing manual work and generalising well across different datasets.


<details>
  <summary>Details</summary>
Motivation: Current ETL solutions require significant manual intervention for context-specific and non-generalisable data transformations. Existing automation approaches have not addressed the full automation of transformation design and application.

Method: FlowETL is introduced as an example-based, autonomous ETL pipeline. It takes a paired input-output dataset as examples. A Planning Engine constructs a transformation plan, which is executed by an ETL worker. The architecture provides monitoring and logging for transparency.

Result: FlowETL demonstrates promising generalisation abilities across 14 datasets with diverse domains, structures, and sizes.

Conclusion: FlowETL significantly reduces the human-in-the-loop requirement by automating the design and application of ETL transformations through an example-based approach, showing effectiveness on various datasets.

Abstract: The Extract, Transform, Load (ETL) workflow is fundamental for populating and
maintaining data warehouses and other data stores accessed by analysts for
downstream tasks. A major shortcoming of modern ETL solutions is the extensive
need for a human-in-the-loop, required to design and implement
context-specific, and often non-generalisable transformations. While related
work in the field of ETL automation shows promising progress, there is a lack
of solutions capable of automatically designing and applying these
transformations. We present FlowETL, a novel example-based autonomous ETL
pipeline architecture designed to automatically standardise and prepare input
datasets according to a concise, user-defined target dataset. FlowETL is an
ecosystem of components which interact together to achieve the desired outcome.
A Planning Engine uses a paired input-output datasets sample to construct a
transformation plan, which is then applied by an ETL worker to the source
dataset. Monitoring and logging provide observability throughout the entire
pipeline. The results show promising generalisation capabilities across 14
datasets of various domains, file structures, and file sizes.

</details>


### [3] [Vibe Modeling: Challenges and Opportunities](https://arxiv.org/abs/2507.23120)
*Jordi Cabot*

Main category: cs.SE

TL;DR: This paper proposes 'vibe modeling' as a new hybrid method combining AI-powered code generation and model-driven engineering to tackle modern software complexity, detailing its benefits and highlighting research directions.


<details>
  <summary>Details</summary>
Motivation: There is a growing demand for software systems that are increasingly complex, featuring new types of user interfaces, intelligent components, and sustainability requirements, which current development methods struggle to adequately address.

Method: The paper introduces 'vibe modeling,' which aims to integrate Large Language Model (LLM) powered 'vibe coding' with established Model-Driven Engineering (MDE) approaches, combining the strengths of both for software development.

Result: The paper presents the key concepts of vibe modeling, discusses its potential benefits, and outlines open challenges and opportunities for further research in this hybrid approach.

Conclusion: Vibe modeling offers a promising path to address complexity, quality, and productivity challenges in software development by marrying AI-driven coding with MDE principles, though it presents new research challenges.

Abstract: There is a pressing need for better development methods and tools to keep up
with the growing demand and increasing complexity of new software systems. New
types of user interfaces, the need for intelligent components, sustainability
concerns, ... bring new challenges that we need to handle. In the last years,
model-driven engineering (MDE) has been key to improving the quality and
productivity of software development, but models themselves are becoming
increasingly complex to specify and manage. At the same time, we are witnessing
the growing popularity of vibe coding approaches that rely on Large Language
Models (LLMs) to transform natural language descriptions into running code at
the expenses of code vulnerabilities, scalability issues and maintainability
concerns. In this paper, we introduce the concept of \textit{vibe modeling} as
a novel approach to integrate the best of both worlds (AI and MDE) to speed up
the development of reliable complex systems. We outline the key concepts of
vibe modeling and highlight the opportunities and open challenges it presents
for the future of modeling.

</details>


### [4] [Extension Decisions in Open Source Software Ecosystem](https://arxiv.org/abs/2507.23168)
*Elmira Onagh,Maleknaz Nayebi*

Main category: cs.SE

TL;DR: A detailed analysis of nearly 7,000 CI Actions on GitHub Marketplace reveals that 65% are redundant copies of existing tools. Most new, redundant tools appear within six months of the original, clustered around a small core of popular first-movers. The data and methods aim to help developers innovate more strategically and cut down on duplicated efforts.


<details>
  <summary>Details</summary>
Motivation: The GitHub Marketplace is rapidly expanding, but many new tools seem to duplicate existing functionalities. The paper seeks to investigate the prevalence, timing, and impact of such functional redundancy, focusing especially on the popular Continuous Integration (CI) segment.

Method: The study analyzes 6,983 CI Actions and maps them to 3,869 providers. By mining their version histories and employing a graph model, the research timestamps when specific functionalities first appeared, tracks their adoption, and identifies clusters of redundant tools.

Result: The study finds that about 65% of new CI Actions replicate pre-existing functionalities, often launching within six months of an original debut. Furthermore, most forks and extensions are tied to a small core of first-mover Actions.

Conclusion: The findings can help developers strategically time their launches and focus on unmet needs, while assisting maintainers in identifying and reducing unnecessary redundancy. The team releases a comprehensive dataset and graph to facilitate future research and to guide practitioners in trend identification and strategy.

Abstract: GitHub Marketplace is expanding by approximately 41% annually, with new
tools; however, many additions replicate existing functionality. We study this
phenomenon in the platform's largest segment, Continuous Integration (CI), by
linking 6,983 CI Actions to 3,869 providers and mining their version histories.
Our graph model timestamps every functionality's debut, tracks its adoption,
and clusters redundant tools. We find that approximately 65% of new CI Actions
replicate existing capabilities, typically within six months, and that a small
set of first-mover Actions accounts for most subsequent forks and extensions.
These insights enable developers to choose the optimal moment to launch, target
unmet functionality, and help maintainers eliminate redundant tools. We publish
the complete graph and dataset to encourage longitudinal research on innovation
and competition in software ecosystems, and to provide practitioners with a
data-driven roadmap for identifying emerging trends and guiding product
strategy.

</details>


### [5] [AutoBridge: Automating Smart Device Integration with Centralized Platform](https://arxiv.org/abs/2507.23178)
*Siyuan Liu,Zhice Yang,Huangxun Chen*

Main category: cs.SE

TL;DR: AutoBridge automates the creation and debugging of IoT integration code, achieving high accuracy and coverage with minimal human input, and outperforms both manual programming and commercial code generation tools.


<details>
  <summary>Details</summary>
Motivation: Integrating new IoT devices into centralized platforms is currently labor-intensive, demanding significant programming expertise and manual effort. This limits scalability and efficiency, highlighting a need for automated solutions.

Method: The paper proposes AutoBridge, a system that automates IoT integration code generation using a divide-and-conquer strategy. First, it generates device control logic by retrieving device-specific knowledge, then synthesizes platform-compliant code using platform knowledge. It includes a multi-stage debugging pipeline with both automated and interactive (binary feedback) debugging for validation.

Result: AutoBridge was evaluated on 34 IoT devices with two open-source platforms, achieving a 93.87% average success rate and 94.87% average function coverage without human input. With minimal user feedback, it reached 100% coverage. In a user study with 15 participants, AutoBridge produced code that was 50%-80% more accurate than that produced by expert programmers, even when they used commercial code LLMs.

Conclusion: AutoBridge significantly simplifies and automates IoT integration, outperforming human experts and existing tools in both accuracy and efficiency. Its combination of automated generation and streamlined debugging greatly reduces human involvement, making large-scale, human-centered IoT system deployment much more practical.

Abstract: Multimodal IoT systems coordinate diverse IoT devices to deliver
human-centered services. The ability to incorporate new IoT devices under the
management of a centralized platform is an essential requirement. However, it
requires significant human expertise and effort to program the complex IoT
integration code that enables the platform to understand and control the device
functions. Therefore, we propose AutoBridge to automate IoT integration code
generation. Specifically, AutoBridge adopts a divide-and-conquer strategy: it
first generates device control logic by progressively retrieving
device-specific knowledge, then synthesizes platformcompliant integration code
using platform-specific knowledge. To ensure correctness, AutoBridge features a
multi-stage debugging pipeline, including an automated debugger for virtual IoT
device testing and an interactive hardware-in-the-loop debugger that requires
only binary user feedback (yes and no) for real-device verification. We
evaluate AutoBridge on a benchmark of 34 IoT devices across two open-source IoT
platforms. The results demonstrate that AutoBridge can achieves an average
success rate of 93.87% and an average function coverage of 94.87%, without any
human involvement. With minimal binary yes and no feedback from users, the code
is then revised to reach 100% function coverage. A user study with 15
participants further shows that AutoBridge outperforms expert programmers by
50% to 80% in code accuracy, even when the programmers are allowed to use
commercial code LLMs.

</details>


### [6] [XABPs: Towards eXplainable Autonomous Business Processes](https://arxiv.org/abs/2507.23269)
*Peter Fettke,Fabiana Fournier,Lior Limonad,Andreas Metzger,Stefanie Rinderle-Ma,Barbara Weber*

Main category: cs.SE

TL;DR: ABPs can greatly benefit organizations but have significant challenges regarding trust and accountability. This paper introduces the concept of explainable ABPs (XABPs), provides a structure for them, and identifies important research directions to make them more transparent and trustworthy.


<details>
  <summary>Details</summary>
Motivation: Autonomous business processes (ABPs), powered by AI/ML, offer significant operational benefits but introduce concerns around trust, debugging, accountability, bias, and regulatory compliance.

Method: The paper proposes a systematic approach to eXplainable Autonomous Business Processes (XABPs), outlining ways to structure explainability and formulating major business process management (BPM) research challenges in this area.

Result: A framework for XABPs is characterized, including forms and structure for explainability, with key BPM research challenges identified to further enable explainable ABPs.

Conclusion: Explainability is essential for trustworthy and effective deployment of ABPs. The proposed approach provides groundwork for research on making ABPs more transparent and reliable.

Abstract: Autonomous business processes (ABPs), i.e., self-executing workflows
leveraging AI/ML, have the potential to improve operational efficiency, reduce
errors, lower costs, improve response times, and free human workers for more
strategic and creative work. However, ABPs may raise specific concerns
including decreased stakeholder trust, difficulties in debugging, hindered
accountability, risk of bias, and issues with regulatory compliance. We argue
for eXplainable ABPs (XABPs) to address these concerns by enabling systems to
articulate their rationale. The paper outlines a systematic approach to XABPs,
characterizing their forms, structuring explainability, and identifying key BPM
research challenges towards XABPs.

</details>


### [7] [SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution](https://arxiv.org/abs/2507.23348)
*Han Li,Yuling Shi,Shaoxin Lin,Xiaodong Gu,Heng Lian,Xin Wang,Yantao Jia,Tao Huang,Qianxiang Wang*

Main category: cs.SE

TL;DR: SWE-Debate is a novel multi-agent debate framework for software issue resolution that enables agents to collaborate and reason diversely, leading to superior results over previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing agent-based frameworks for issue resolution, despite leveraging advanced large language models, are limited by independent agent explorations that lead to local solutions and overlook issue patterns across the codebase.

Method: The authors introduce SWE-Debate, a competitive multi-agent debate framework. It generates multiple fault propagation traces via code dependency traversal, organizes a structured three-round debate among specialized agents with diverse reasoning perspectives, consolidates their findings into a fix plan, and uses an MCTS-based agent for code patch generation.

Result: SWE-Debate achieves state-of-the-art results on the SWE-bench benchmark and significantly outperforms existing open-source agent-based baselines for software issue resolution.

Conclusion: Structured debate among agents with different reasoning approaches enables more accurate issue localization and better software repair outcomes compared to independent agent strategies.

Abstract: Issue resolution has made remarkable progress thanks to the advanced
reasoning capabilities of large language models (LLMs). Recently, agent-based
frameworks such as SWE-agent have further advanced this progress by enabling
autonomous, tool-using agents to tackle complex software engineering tasks.
While existing agent-based issue resolution approaches are primarily based on
agents' independent explorations, they often get stuck in local solutions and
fail to identify issue patterns that span across different parts of the
codebase. To address this limitation, we propose SWE-Debate, a competitive
multi-agent debate framework that encourages diverse reasoning paths and
achieves more consolidated issue localization. SWE-Debate first creates
multiple fault propagation traces as localization proposals by traversing a
code dependency graph. Then, it organizes a three-round debate among
specialized agents, each embodying distinct reasoning perspectives along the
fault propagation trace. This structured competition enables agents to
collaboratively converge on a consolidated fix plan. Finally, this consolidated
fix plan is integrated into an MCTS-based code modification agent for patch
generation. Experiments on the SWE-bench benchmark show that SWE-Debate
achieves new state-of-the-art results in open-source agent frameworks and
outperforms baselines by a large margin.

</details>


### [8] [Quality Evaluation of COBOL to Java Code Transformation](https://arxiv.org/abs/2507.23356)
*Shmulik Froimovich,Raviv Gal,Wesam Ibraheem,Avi Ziv*

Main category: cs.SE

TL;DR: The paper introduces an automated, hybrid evaluation system for LLM-based COBOL-to-Java code translation that combines analytic and LLM judgment techniques, improves scalability and automation, and supports high-quality legacy code modernization.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the difficulty of evaluating code translated by large language model (LLM)-based systems, especially the lack of transparency and the challenge of accurately measuring translation quality in COBOL-to-Java migration. There is a need to automate this process to support large scale modernizations and reduce manual effort.

Method: The authors designed an automated evaluation system that leverages both analytic checkers and LLM-as-a-judge (LaaJ) methods. This hybrid approach allows the system to provide comprehensive, scalable, and multifaceted assessments of translation quality.

Result: The evaluation system integrates seamlessly with continuous integration workflows, supports large-scale benchmarking, and minimizes dependence on manual code review. It offers architecture, strategies, and reporting that help developers and managers improve the quality of migrated codebases.

Conclusion: The presented system enhances the ability to assess LLM-based COBOL-to-Java code translations, making the evaluation process more scalable, actionable, and less reliant on manual intervention. This enables more effective modernization of legacy systems.

Abstract: We present an automated evaluation system for assessing COBOL-to-Java code
translation within IBM's watsonx Code Assistant for Z (WCA4Z). The system
addresses key challenges in evaluating LLM-based translators, including model
opacity and the complexity of translation quality assessment. Our approach
combines analytic checkers with LLM-as-a-judge (LaaJ) techniques to deliver
scalable, multi-faceted evaluations. The system supports continuous integration
workflows, enables large-scale benchmarking, and reduces reliance on manual
review. We describe the system architecture, evaluation strategies, and
reporting mechanisms that provide actionable insights for developers and
project managers, facilitating the evolution of high-quality, modernized
codebases.

</details>


### [9] [SWE-Exp: Experience-Driven Software Issue Resolution](https://arxiv.org/abs/2507.23361)
*Silin Chen,Shaoxin Lin,Xiaodong Gu,Yuling Shi,Heng Lian,Longfei Yun,Dong Chen,Weiguo Sun,Lin Cao,Qianxiang Wang*

Main category: cs.SE

TL;DR: The paper proposes SWE-Exp, an approach that enables LLM agents to remember and reuse past repair experiences for software issue resolution, leading to higher success rates by avoiding redundant mistakes and applying learned solutions.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents for software issue resolution don't reuse knowledge from previous experiences. As a result, they redundantly repeat failed exploration and miss the chance to apply successful strategies to similar problems.

Method: The authors introduce SWE-Exp, an experience-enhanced approach that builds an experience bank from prior agent trajectories. This bank captures both successful and failed repair attempts at multiple levels, from problem understanding to specific code changes, enabling continuous learning and knowledge reuse.

Result: SWE-Exp achieves a state-of-the-art resolution rate (41.6% Pass@1) on the SWE-bench-Verified benchmark when used with open-source agent frameworks.

Conclusion: SWE-Exp allows software engineering agents to systematically accumulate and leverage repair expertise, moving from a pure trial-and-error approach to a strategic, experience-driven resolution process.

Abstract: Recent advances in large language model (LLM) agents have shown remarkable
progress in software issue resolution, leveraging advanced techniques such as
multi-agent collaboration and Monte Carlo Tree Search (MCTS). However, current
agents act as memoryless explorers - treating each problem separately without
retaining or reusing knowledge from previous repair experiences. This leads to
redundant exploration of failed trajectories and missed chances to adapt
successful issue resolution methods to similar problems. To address this
problem, we introduce SWE-Exp, an experience - enhanced approach that distills
concise and actionable experience from prior agent trajectories, enabling
continuous learning across issues. Our method introduces a multi-faceted
experience bank that captures both successful and failed repair attempts.
Specifically, it extracts reusable issue resolution knowledge at different
levels - from high-level problem comprehension to specific code changes.
Experiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%
Pass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach
establishes a new paradigm in which automated software engineering agents
systematically accumulate and leverage repair expertise, fundamentally shifting
from trial-and-error exploration to strategic, experience-driven issue
resolution.

</details>


### [10] [Trae Agent: An LLM-based Agent for Software Engineering with Test-time Scaling](https://arxiv.org/abs/2507.23370)
*Trae Research Team,Pengfei Gao,Zhao Tian,Xiangxin Meng,Xinchen Wang,Ruida Hu,Yuanan Xiao,Yizhou Liu,Zhao Zhang,Junjie Chen,Cuiyun Gao,Yun Lin,Yingfei Xiong,Chao Peng,Xia Liu*

Main category: cs.SE

TL;DR: Trae Agent is a novel agent-based ensemble approach for software issue resolution, leveraging modular agents to improve repository-level understanding and solution quality. It significantly outperforms previous methods on the SWE-bench benchmark, is open source, and sets a new state-of-the-art in the field.


<details>
  <summary>Details</summary>
Motivation: With the advancement of large language models (LLMs), software issue resolution has seen improved automated solutions, yet current prompting-based ensemble methods struggle with large solution spaces and lack deep, repository-level understanding. This constrains their effectiveness on real-world software repositories.

Method: The authors introduce Trae Agent, an agent-based ensemble reasoning framework. Trae Agent models issue resolution as a search for optimal fixes using specialized modular agents for generation, pruning, and selection. This modularity enables better handling of extensive ensemble spaces and repository-wide understanding. Experiments were run on the SWE-bench benchmark using three leading LLMs, with comparisons to four ensemble reasoning baselines.

Result: Trae Agent surpassed all four state-of-the-art ensemble reasoning baselines, with an average Pass@1 improvement of 10.22%. It achieved a leading score of 75.20% Pass@1 on the SWE-bench Verified leaderboard, demonstrating robust effectiveness.

Conclusion: The introduction of Trae Agent represents a major advancement in agent-based ensemble reasoning for software issue resolution, particularly at the repository level. Its modular design overcomes key limitations of prior prompting-based methods, delivering state-of-the-art results. The open-source release supports further research in this domain.

Abstract: Software issue resolution is a critical challenge in software engineering and
has garnered increasing attention in recent years. With the rapid advancement
of large language models (LLMs), substantial progress has been made in
addressing real-world software engineering tasks. Recent studies have
introduced ensemble reasoning techniques to enhance the performance of
LLM-based issue resolution. However, existing prompting-based methods still
face limitations in effectively exploring large ensemble spaces and lack the
capacity for repository-level understanding, both of which constrain their
overall effectiveness. In this paper, we propose Trae Agent, the first
agent-based ensemble reasoning approach for repository-level issue resolution.
Trae Agent formulates our goal as an optimal solution search problem and
addresses two key challenges, i.e., large ensemble spaces and repository-level
understanding, through modular agents for generation, pruning, and selection.
We conduct extensive experiments using three leading LLMs on the widely-adopted
SWE-bench benchmark, comparing Trae Agent against four state-of-the-art
ensemble reasoning techniques. Experimental results demonstrate that Trae Agent
consistently achieves superior performance, with an average improvement of
10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first
place on the SWE-bench Verified leaderboard, with a notable Pass@1 score of
75.20%. We are pleased to release Trae Agent as an open-source project to
support the research community, with all resources available at
https://github.com/bytedance/trae-agent.

</details>


### [11] [Dynamic and Static Analysis of Python Software with Kieker Including Reconstructed Architectures](https://arxiv.org/abs/2507.23425)
*Daphné Larrivain,Shinhyung Yang,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: This paper extends the Kieker observability framework to support Python by developing a new analysis pipeline that uses both static and dynamic analyses, thus providing deeper insights into Python applications.


<details>
  <summary>Details</summary>
Motivation: Kieker was originally designed for Java applications. However, due to the increasing popularity of Python, there is growing value in providing structural insight and observability tools for Python applications.

Method: The paper presents a Python analysis pipeline that integrates both static and dynamic analysis techniques to achieve comprehensive application assessment.

Result: The developed pipeline enables users to gain a thorough understanding of Python systems, enhancing observability and analysis capabilities in the Kieker framework for Python applications.

Conclusion: Supporting Python in the Kieker observability framework is beneficial, and the combination of static and dynamic analysis offers robust insights into Python systems for users.

Abstract: The Kieker observability framework is a tool that provides users with the
means to design a custom observability pipeline for their application.
Originally tailored for Java, supporting Python with Kieker is worthwhile.
Python's popularity has exploded over the years, thus making structural
insights of Python applications highly valuable. Our Python analysis pipeline
combines static and dynamic analysis in order to build a complete picture of a
given system.

</details>


### [12] [An Empirical Study on the Amount of Changes Required for Merge Request Acceptance](https://arxiv.org/abs/2507.23640)
*Samah Kansab,Mohammed Sayagh,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: This paper analyzes the effort required in GitLab Merge Requests by measuring post-submission code changes and finds that most MRs demand further modification, often at a significant scale. Machine learning models, using project and developer metrics, predict review effort with high accuracy, highlighting potential for process improvement.


<details>
  <summary>Details</summary>
Motivation: Code review (CR) is a critical but effort-intensive phase in software development, especially in collaborative environments like GitLab Merge Requests. While previous research has focused on delays and the number of review iterations, little is known about the actual code modification effort required post-submission, a gap this study seeks to address.

Method: The study quantitatively defines and measures CR effort as the amount of code changed after MR submission. It analyzes a dataset of over 23,600 GitLab MRs, assessing the extent of post-submission code changes. Additionally, it applies interpretable machine learning, utilizing features such as text, code complexity, developer experience, review history, and branching, to predict CR effort.

Result: The study found that up to 71% of MRs require changes after initial submission, with 28% of those requiring over 200 lines of modifications. Notably, the effort expended isn't correlated with review duration or the number of reviewers. The machine learning model used in the study achieved an AUC between 0.84 and 0.88, identifying complexity, developer experience, and text features as key predictors. Past project characteristics also impact current review effort.

Conclusion: Machine learning models can effectively explain and predict code review effort in GitLab MRs. Key drivers include code complexity, developer experience, and characteristics reflected in text and project history. This opens opportunities for more data-driven management of code review integration.

Abstract: Code review (CR) is essential to software development, helping ensure that
new code is properly integrated. However, the CR process often involves
significant effort, including code adjustments, responses to reviewers, and
continued implementation. While past studies have examined CR delays and
iteration counts, few have investigated the effort based on the volume of code
changes required, especially in the context of GitLab Merge Requests (MRs),
which remains underexplored. In this paper, we define and measure CR effort as
the amount of code modified after submission, using a dataset of over 23,600
MRs from four GitLab projects. We find that up to 71% of MRs require
adjustments after submission, and 28% of these involve changes to more than 200
lines of code. Surprisingly, this effort is not correlated with review time or
the number of participants. To better understand and predict CR effort, we
train an interpretable machine learning model using metrics across multiple
dimensions: text features, code complexity, developer experience, review
history, and branching. Our model achieves strong performance (AUC 0.84-0.88)
and reveals that complexity, experience, and text features are key predictors.
Historical project characteristics also influence current review effort. Our
findings highlight the feasibility of using machine learning to explain and
anticipate the effort needed to integrate code changes during review.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [13] [Abstractions of Sequences, Functions and Operators](https://arxiv.org/abs/2507.23151)
*Louis Rustenholz,Pedro Lopez-Garcia,Manuel V. Hermenegildo*

Main category: cs.PL

TL;DR: This paper introduces new theoretical tools (B-bound domains and domain abstraction) for inferring complex bounds on recursively defined functions, improving automation and precision in program and systems analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation comes from the challenge of inferring closed-form bounds on recursively defined functions (i.e., fixed points of operators or solutions to functional equations), relevant for applications in program analysis (such as cost analysis, loop acceleration, and declarative language analysis) and hybrid systems governed by differential equations.

Method: The authors introduce a new family of constraint-based abstract domains, called B-bound domains, to abstract numerical functions. They also present the concept of domain abstraction, a functor that lifts arbitrary mappings in value space to Galois connections in function space. Their constructions are based on a simple operator language, starting with sequences and extending to multivariate, piecewise, and non-discrete functions.

Result: The B-bound domains enable the inference of highly non-linear numerical invariants, overcoming limitations of classical numerical abstract domains. The authors uncover a convexity property in the constraint space that simplifies or even fully automates the design of transfer functions. Their approach supports abstraction from symbolic to numerical functions, allows for dimensionality reduction, and generalizes to a variety of functional spaces.

Conclusion: The paper advances higher-order abstract interpretation by proposing new abstract domains (B-bound domains) and domain abstraction techniques. These enable efficient and automated analysis of complex recursive functions, supporting a wide range of applications in program analysis and systems governed by differential equations.

Abstract: We present theoretical and practical results on the order theory of lattices
of functions, focusing on Galois connections that abstract (sets of) functions
- a topic known as higher-order abstract interpretation.
  We are motivated by the challenge of inferring closed-form bounds on
functions which are defined recursively, i.e. as the fixed point of an operator
or, equivalently, as the solution to a functional equation. This has multiple
applications in program analysis (e.g. cost analysis, loop acceleration,
declarative language analysis) and in hybrid systems governed by differential
equations.
  Our main contribution is a new family of constraint-based abstract domains
for abstracting numerical functions, B-bound domains, which abstract a function
f by a conjunction of bounds from a preselected set of boundary functions. They
allow inferring highly non-linear numerical invariants, which classical
numerical abstract domains struggle with. We uncover a convexity property in
the constraint space that simplifies, and, in some cases, fully automates,
transfer function design.
  We also introduce domain abstraction, a functor that lifts arbitrary mappings
in value space to Galois connections in function space. This supports
abstraction from symbolic to numerical functions (i.e. size abstraction), and
enables dimensionality reduction of equations.
  We base our constructions of transfer functions on a simple operator
language, starting with sequences, and extending to more general functions,
including multivariate, piecewise, and non-discrete domains.

</details>


### [14] [Kernel-FFI: Transparent Foreign Function Interfaces for Interactive Notebooks](https://arxiv.org/abs/2507.23205)
*Hebi Li,Forrest Sheng Bao,Qi Xiao,Jin Tian*

Main category: cs.PL

TL;DR: Kernel-FFI automates and simplifies cross-language calls and object management in Jupyter notebooks, removing manual setup and supporting complex features like recursion and OOP. It greatly improves developer productivity and will be open-sourced.


<details>
  <summary>Details</summary>
Motivation: Existing Foreign Function Interface (FFI) solutions are inadequate for modern, dynamic workflows in notebook environments like Jupyter. They require manual setup, too much boilerplate, and often do not support essential features such as recursion and object-oriented constructs.

Method: The authors introduce Kernel-FFI, a framework that uses source-level transformation to automatically rewrite cross-language function calls and object usage. This removes the need for manual bindings or boilerplate code. The system also incorporates a new side-channel communication method to handle asynchronous and recursive cross-language calls, and supports object-oriented programming across language boundaries with automatic resource management.

Result: Kernel-FFI enables transparent, language-agnostic interoperability for function calls and object manipulation in interactive notebooks. It successfully supports recursion, asynchronous calls, and OOP constructs, providing a seamless developer experience.

Conclusion: Kernel-FFI fills a crucial gap in supporting dynamic, multi-language programming in environments such as Jupyter notebooks. It automates the integration process, reduces manual overhead, and supports greater flexibility and productivity for developers. The tool will be made open-source for broader community use.

Abstract: Foreign Function Interfaces (FFIs) are essential for enabling
interoperability between programming languages, yet existing FFI solutions are
ill-suited for the dynamic, interactive workflows prevalent in modern notebook
environments such as Jupyter. Current approaches require extensive manual
configuration, introduce significant boilerplate, and often lack support for
recursive calls and object-oriented programming (OOP) constructs-features
critical for productive, multi-language development.
  We present Kernel-FFI, a transparent, language-agnostic framework that
enables seamless cross-language function calls and object manipulation within
interactive notebooks. Kernel-FFI employs source-level transformation to
automatically rewrite cross-language invocations, eliminating the need for
manual bindings or boilerplate. Kernel-FFI provides robust support for OOP by
enabling foreign object referencing and automatic resource management across
language boundaries. Furthermore, to address the blocking nature of Jupyter
kernels and support recursive and asynchronous foreign calls, we introduce a
novel side-channel communication mechanism. Our tool will be open-sourced and
available at https://codepod.io/docs/kernel-ffi

</details>
