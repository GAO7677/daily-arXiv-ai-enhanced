<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 8]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Model-Driven Quantum Code Generation Using Large Language Models and Retrieval-Augmented Generation](https://arxiv.org/abs/2508.21097)
*Nazanin Siavash,Armin Moin*

Main category: cs.SE

TL;DR: The paper demonstrates that augmenting large language models with RAG—and incorporating real-world quantum code samples—significantly improves the quality of quantum code generated from UML models. More research will explore additional applications in the future.


<details>
  <summary>Details</summary>
Motivation: Quantum and hybrid quantum-classical software systems face challenges such as heterogeneous platforms and a shortage of skilled developers. Model-driven approaches could help address these issues, and augmenting LLMs with RAG could further improve automation and code quality in this domain.

Method: The authors propose and validate leveraging LLMs enhanced by Retrieval-Augmented Generation (RAG) pipelines to generate code from UML model instances for quantum software systems. They used sample Qiskit code from public GitHub repositories within the RAG pipeline and tested prompt engineering effects on CodeBLEU scores for quantum code generation.

Result: Experimental results show that engineered prompts in the RAG-enhanced LLM pipeline can improve CodeBLEU scores by up to four times, leading to more accurate and consistent quantum code.

Conclusion: Model-to-code generation for quantum systems using LLMs enhanced with RAG is a promising approach. Well-designed prompts and relevant retrieved examples can greatly boost code quality, but there is further research needed to explore broader applications such as code transpilation and using software model instances as primary RAG sources.

Abstract: This paper introduces a novel research direction for model-to-text/code
transformations by leveraging Large Language Models (LLMs) that can be enhanced
with Retrieval-Augmented Generation (RAG) pipelines. The focus is on quantum
and hybrid quantum-classical software systems, where model-driven approaches
can help reduce the costs and mitigate the risks associated with the
heterogeneous platform landscape and lack of developers' skills. We validate
one of the proposed ideas regarding generating code out of UML model instances
of software systems. This Python code uses a well-established library, called
Qiskit, to execute on gate-based or circuit-based quantum computers. The RAG
pipeline that we deploy incorporates sample Qiskit code from public GitHub
repositories. Experimental results show that well-engineered prompts can
improve CodeBLEU scores by up to a factor of four, yielding more accurate and
consistent quantum code. However, the proposed research direction can go beyond
this through further investigation in the future by conducting experiments to
address our other research questions and ideas proposed here, such as deploying
software system model instances as the source of information in the RAG
pipelines, or deploying LLMs for code-to-code transformations, for instance,
for transpilation use cases.

</details>


### [2] [Learning to Generate Unit Test via Adversarial Reinforcement Learning](https://arxiv.org/abs/2508.21107)
*Dongjun Lee,Changho Hwang,Kimin Lee*

Main category: cs.SE

TL;DR: The paper proposes UTRL, an adversarial reinforcement learning framework for training LLMs to generate high-quality unit tests. Experiments show that models trained with UTRL outperform both supervised methods and state-of-the-art models like GPT-4.1.


<details>
  <summary>Details</summary>
Motivation: Writing comprehensive unit tests is challenging and automating this process with high quality is an unsolved problem. There is a need for better methods to train large language models (LLMs) to generate effective unit tests.

Method: The paper introduces UTRL, a reinforcement learning framework where two LLMs (a unit test generator and a code generator) are trained adversarially. The test generator is rewarded for generating tests that reveal faults, while the code generator is rewarded for producing code that passes these tests. Both are refined through iterative adversarial training.

Result: Experimental results show that the Qwen3-4B model trained with UTRL generates higher quality unit tests than when it is trained via supervised fine-tuning on human-written tests. The model also outperforms leading models like GPT-4.1 in creating effective unit tests.

Conclusion: UTRL effectively trains LLMs to generate superior unit tests compared to traditional supervised fine-tuning or even top-performing LLMs, advancing the automation of high-quality software testing.

Abstract: Unit testing is a core practice in programming, enabling systematic
evaluation of programs produced by human developers or large language models
(LLMs). Given the challenges in writing comprehensive unit tests, LLMs have
been employed to automate test generation, yet methods for training LLMs to
produce high-quality tests remain underexplored. In this work, we propose UTRL,
a novel reinforcement learning framework that trains an LLM to generate
high-quality unit tests given a programming instruction. Our key idea is to
iteratively train two LLMs, the unit test generator and the code generator, in
an adversarial manner via reinforcement learning. The unit test generator is
trained to maximize a discrimination reward, which reflects its ability to
produce tests that expose faults in the code generator's solutions, and the
code generator is trained to maximize a code reward, which reflects its ability
to produce solutions that pass the unit tests generated by the test generator.
In our experiments, we demonstrate that unit tests generated by Qwen3-4B
trained via UTRL show higher quality compared to unit tests generated by the
same model trained via supervised fine-tuning on human-written ground-truth
unit tests, yielding code evaluations that more closely align with those
induced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL
outperforms frontier models such as GPT-4.1 in generating high-quality unit
tests, highlighting the effectiveness of UTRL in training LLMs for this task.

</details>


### [3] [Automated Bug Triaging using Instruction-Tuned Large Language Models](https://arxiv.org/abs/2508.21156)
*Kiana Kiashemshaki,Arsham Khosravani,Alireza Hosseinpour,Arshia Akhavan*

Main category: cs.SE

TL;DR: Proposes an LLM-based framework for faster, consistent bug triaging, outperforming traditional methods in shortlist quality, with practical real-world potential.


<details>
  <summary>Details</summary>
Motivation: Bug triaging in large projects is slow and inconsistent, and current solutions are expensive or complex. The study aims to provide a more practical and scalable alternative.

Method: Lightweight framework using instruction-tuned LLMs with LoRA adapters and candidate-constrained decoding for bug triaging. Evaluated on EclipseJDT and Mozilla datasets.

Result: The model shows high shortlist quality (Hit@10 up to 0.753) but moderate exact Top-1 accuracy. Recent test data shows increased accuracy, indicating strong real-world applicability.

Conclusion: Instruction-tuned LLMs, with proper adaptation and validation constraints, provide a feasible and cost-effective approach for bug triaging versus traditional expensive or complex solutions.

Abstract: Bug triaging, the task of assigning new issues to developers, is often slow
and inconsistent in large projects. We present a lightweight framework that
instruction-tuned large language model (LLM) with LoRA adapters and uses
candidate-constrained decoding to ensure valid assignments. Tested on
EclipseJDT and Mozilla datasets, the model achieves strong shortlist quality
(Hit at 10 up to 0.753) despite modest exact Top-1 accuracy. On recent
snapshots, accuracy rises sharply, showing the framework's potential for
real-world, human-in-the-loop triaging. Our results suggest that
instruction-tuned LLMs offer a practical alternative to costly feature
engineering and graph-based methods.

</details>


### [4] [The Complexity Trap: Simple Observation Masking Is as Efficient as LLM Summarization for Agent Context Management](https://arxiv.org/abs/2508.21433)
*Tobias Lindenbauer,Igor Slinko,Ludwig Felder,Egor Bogomolov,Yaroslav Zharov*

Main category: cs.SE

TL;DR: Simple observation-masking manages LLM context in engineering agents as effectively—or better than—summarization methods, at half the computational cost.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents generate long, costly histories when solving complex tasks. Summarization is used to control this, but its benefit over much simpler strategies (like omitting old observations) has not been robustly compared.

Method: Empirical, systematic comparison between observation-masking and LLM summarization methods for context pruning in SWE-agent on SWE-bench Verified using five model configurations. Key metrics: solve rate and computational cost.

Result: The paper compares two strategies for managing long context histories in LLM-based software engineering agents (like SWE-agent): summarization (used by tools like OpenHands and Cursor) versus a simple observation-masking (pruning old observations). The experiments use the SWE-bench Verified benchmark across five different model configurations. Results show: masking reduces cost by half compared to using the raw agent and matches or slightly exceeds the solve rate of LLM-based summarization. With a large model (Qwen3-Coder 480B), masking even slightly improves the solve rate over the raw agent and remains as effective as summarization at a lower cost.

Conclusion: In the SWE-agent on SWE-bench Verified, simple observation-masking is as effective and more efficient than LLM-based summarization for context management.

Abstract: Large Language Model (LLM)-based agents solve complex tasks through iterative
reasoning, exploration, and tool-use, a process that can result in long,
expensive context histories. While state-of-the-art Software Engineering ( SE)
agents like OpenHands or Cursor use LLM-based summarization to tackle this
issue, it is unclear whether the increased complexity offers tangible
performance benefits compared to simply omitting older observations. We present
a systematic comparison of these strategies within SWE-agent on SWE-bench
Verified across five diverse model configurations. We find that a simple
observation-masking strategy halves cost relative to a raw agent while
matching, and sometimes slightly exceeding, the solve rate of LLM
summarization. For example, with Qwen3-Coder 480B, masking improves solve rate
from 53.8% (raw agent) to 54.8%, while remaining competitive with summarization
at a lower cost. These results suggest that, at least within SWE-agent on
SWE-bench Verified, the most effective and efficient context management can be
the simplest. We release code and data for reproducibility

</details>


### [5] [Enhancing Semantic Understanding in Pointer Analysis using Large Language Models](https://arxiv.org/abs/2508.21454)
*Baijun Cheng,Kailong Wang,Ling Shi,Haoyu Wang,Yao Guo,Ding Li,Xiangqun Chen*

Main category: cs.SE

TL;DR: LMPA integrates large language models into pointer analysis to improve precision and scalability by better handling user-defined functions and enhancing summary-based analysis.


<details>
  <summary>Details</summary>
Motivation: Pointer analysis frameworks often propagate incorrect facts due to conservative treatments and lack of semantic understanding, especially regarding user-defined functions. This issue limits their precision and scalability.

Method: LMPA uses LLMs to analyze code semantics, recognize user-defined functions similar to system APIs, infer initial points-to sets, and augment summary-based analysis with natural language strategies.

Result: The proposed LMPA vision incorporates LLMs into pointer analysis to identify and properly model user-defined functions that resemble system APIs, reducing erroneous propagation and improving analysis quality.

Conclusion: Incorporating LLMs into pointer analysis is a promising strategy to address longstanding issues of incorrect fact propagation, though there remain key implementation challenges.

Abstract: Pointer analysis has been studied for over four decades. However, existing
frameworks continue to suffer from the propagation of incorrect facts. A major
limitation stems from their insufficient semantic understanding of code,
resulting in overly conservative treatment of user-defined functions. Recent
advances in large language models (LLMs) present new opportunities to bridge
this gap. In this paper, we propose LMPA (LLM-enhanced Pointer Analysis), a
vision that integrates LLMs into pointer analysis to enhance both precision and
scalability. LMPA identifies user-defined functions that resemble system APIs
and models them accordingly, thereby mitigating erroneous cross-calling-context
propagation. Furthermore, it enhances summary-based analysis by inferring
initial points-to sets and introducing a novel summary strategy augmented with
natural language. Finally, we discuss the key challenges involved in realizing
this vision.

</details>


### [6] [Reusable Test Suites for Reinforcement Learning](https://arxiv.org/abs/2508.21553)
*Jørn Eirik Betten,Quentin Mazouni,Dennis Gross,Pedro Lind,Helge Spieker*

Main category: cs.SE

TL;DR: The paper introduces MPTCS, a new automated method for selecting diverse and reusable test cases in reinforcement learning environments, improving reliability testing across multiple policies.


<details>
  <summary>Details</summary>
Motivation: Validating RL policies for deployment is difficult, and most policy testing methods generate test cases tailored to specific agents with unclear relevance to others.

Method: The method, MPTCS, uses a set of diverse policies to select test cases from a candidate pool (from any testing framework) using a difficulty score. It promotes diversity using a discretized test case descriptor surface inspired by quality-diversity algorithms and analyzes the effect of policy count on effectiveness and cost.

Result: MPTCS can select diverse, reusable, and policy-agnostic test cases that reveal common flaws, with its effectiveness and cost dependent on policy set size. The quality-diversity-inspired approach increases coverage and exposes faulty behaviors across different policies.

Conclusion: This method enables more generalizable and effective RL policy testing by generating test suites that expose typical agent flaws regardless of the agent tested, promoting broader state space coverage.

Abstract: Reinforcement learning (RL) agents show great promise in solving sequential
decision-making tasks. However, validating the reliability and performance of
the agent policies' behavior for deployment remains challenging. Most
reinforcement learning policy testing methods produce test suites tailored to
the agent policy being tested, and their relevance to other policies is
unclear. This work presents Multi-Policy Test Case Selection (MPTCS), a novel
automated test suite selection method for RL environments, designed to extract
test cases generated by any policy testing framework based on their
solvability, diversity, and general difficulty. MPTCS uses a set of policies to
select a diverse collection of reusable policy-agnostic test cases that reveal
typical flaws in the agents' behavior. The set of policies selects test cases
from a candidate pool, which can be generated by any policy testing method,
based on a difficulty score. We assess the effectiveness of the difficulty
score and how the method's effectiveness and cost depend on the number of
policies in the set. Additionally, a method for promoting diversity in the test
suite, a discretized general test case descriptor surface inspired by
quality-diversity algorithms, is examined to determine how it covers the state
space and which policies it triggers to produce faulty behaviors.

</details>


### [7] [Human-Written vs. AI-Generated Code: A Large-Scale Study of Defects, Vulnerabilities, and Complexity](https://arxiv.org/abs/2508.21634)
*Domenico Cotroneo,Cristina Improta,Pietro Liguori*

Main category: cs.SE

TL;DR: AI-generated code is less complex but more prone to certain defects and security risks, while human-written code is structurally complex but with more maintainability issues. Both types have unique quality profiles, necessitating targeted quality assurance.


<details>
  <summary>Details</summary>
Motivation: As AI code assistants like ChatGPT, DeepSeek-Coder, and Qwen-Coder are increasingly used in software development, there's a critical need to understand how their code quality compares to that of human developers, focusing on reliability, maintainability, and security.

Method: A large-scale study evaluating over 500,000 code samples in Python and Java, comparing code from human developers and three LLMs across code defects (using Orthogonal Defect Classification), security vulnerabilities (using Common Weakness Enumeration), and structural complexity.

Result: The study finds that AI-generated code is typically simpler and more repetitive, but tends to have more unused constructs and hardcoded debugging statements. Human-written code is more structurally complex with more maintainability concerns. Importantly, AI-generated code has more high-risk security vulnerabilities than human code.

Conclusion: AI- and human-generated code each have distinct profiles of defects and vulnerabilities. AI code presents particular risks, notably higher security vulnerabilities, emphasizing the necessity of tailored quality assurance approaches for AI-assisted development.

Abstract: As AI code assistants become increasingly integrated into software
development workflows, understanding how their code compares to human-written
programs is critical for ensuring reliability, maintainability, and security.
In this paper, we present a large-scale comparison of code authored by human
developers and three state-of-the-art LLMs, i.e., ChatGPT, DeepSeek-Coder, and
Qwen-Coder, on multiple dimensions of software quality: code defects, security
vulnerabilities, and structural complexity. Our evaluation spans over 500k code
samples in two widely used languages, Python and Java, classifying defects via
Orthogonal Defect Classification and security vulnerabilities using the Common
Weakness Enumeration. We find that AI-generated code is generally simpler and
more repetitive, yet more prone to unused constructs and hardcoded debugging,
while human-written code exhibits greater structural complexity and a higher
concentration of maintainability issues. Notably, AI-generated code also
contains more high-risk security vulnerabilities. These findings highlight the
distinct defect profiles of AI- and human-authored code and underscore the need
for specialized quality assurance practices in AI-assisted programming.

</details>


### [8] [The Integration of Agile Methodologies in DevOps Practices within the Information Technology Industry](https://arxiv.org/abs/2508.21811)
*Ashley Hourigan,Ridewaan Hanslo*

Main category: cs.SE

TL;DR: This paper investigates how Agile methods are integrated into DevOps practices in the IT industry by interviewing practitioners and analysing their insights. The study identifies key themes demonstrating how Agile approaches fit within each stage of the DevOps lifecycle, revealing a new conceptual understanding of how both methodologies work together to deliver faster and better software.


<details>
  <summary>Details</summary>
Motivation: The IT industry faces increasing pressure for faster software delivery and better features to meet growing customer expectations. Traditional development models like Waterfall are seen as too rigid, prompting a shift toward Agile and DevOps methodologies, which offer more flexibility and speed. The motivation is to understand how Agile practices are being adapted and integrated into the newer DevOps processes to maintain high-quality, resilient software delivery.

Method: The study conducted eleven semi-structured interviews with Agile and DevOps practitioners from various IT sectors. The collected qualitative interview data was then analysed using thematic analysis, leading to the extraction and synthesis of 51 unique codes into 19 distinct themes. These themes described the interplay between Agile practices and each phase of the DevOps lifecycle.

Result: The research identified 19 themes regarding the integration of Agile methods within DevOps, each corresponding to specific phases of the DevOps lifecycle. The findings shed light on the practical feasibility and applicability of Agile approaches within DevOps environments across different IT sectors.

Conclusion: A new understanding of the relationships and interdependencies between Agile and DevOps practices was established. The study provided evidence for how Agile methods can be integrated within DevOps workflows to satisfy contemporary IT industry demands for rapid, reliable, and high-quality software deliveries.

Abstract: The demand for rapid software delivery in the Information Technology (IT)
industry has significantly intensified, emphasising the need for faster
software products and service releases with enhanced features to meet customer
expectations. Agile methodologies are replacing traditional approaches such as
Waterfall, where flexibility, iterative development and adaptation to change
are favoured over rigid planning and execution. DevOps, a subsequent evolution
from Agile, emphasises collaborative efforts in development and operations
teams, focusing on continuous integration and deployment to deliver resilient
and high-quality software products and services. This study aims to critically
assess both Agile and DevOps practices in the IT industry to identify the
feasibility and applicability of Agile methods in DevOps practices. Eleven
semi-structured interviews were conducted with Agile and DevOps practitioners
in varying capacities across several sectors within the IT industry. Through
thematic analysis, 51 unique codes were extracted and synthesised into 19
themes that reported on each phase of the DevOps lifecycle, specifically
regarding the integration and implementation of Agile methods into DevOps
practices. Based on the findings, a new understanding detailing the
interrelationship of Agile methods in DevOps practices was discussed that met
the research objectives.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [9] [CrossTL: A Universal Programming Language Translator with Unified Intermediate Representation](https://arxiv.org/abs/2508.21256)
*Nripesh Niketan,Vaatsalya Shrivastva*

Main category: cs.PL

TL;DR: CrossTL introduces a universal programming language translator capable of bidirectional translation among many languages using a single intermediate representation, CrossGL, greatly simplifying language interoperability.


<details>
  <summary>Details</summary>
Motivation: Existing language translation systems require separate, complex translators for each language pair, leading to inefficiency and scalability challenges.

Method: CrossTL employs language-specific frontends to convert source code to ASTs and bidirectional translation modules utilizing a unified IR (CrossGL), with backends generating code for each target language.

Result: The system supports translation and execution between CUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan SPIR-V, Rust, and Mojo, demonstrating successful compilation and practical extensibility for new languages.

Conclusion: CrossTL enables practical universal code translation across several languages and paradigms, facilitating write-once, deploy-everywhere development.

Abstract: We present CrossTL, a universal programming language translator enabling
bidirectional translation between multiple languages through a unified
intermediate representation called CrossGL. Traditional approaches require
separate translators for each language pair, leading to exponential complexity
growth. CrossTL uses a single universal IR to facilitate translations between
CUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan SPIR-V, Rust, and Mojo,
with Slang support in development. Our system consists of: language-specific
lexers/parsers converting source code to ASTs, bidirectional CrossGL
translation modules implementing ToCrossGLConverter classes for importing code
and CodeGen classes for target generation, and comprehensive backend
implementations handling full translation pipelines. We demonstrate
effectiveness through comprehensive evaluation across programming domains,
achieving successful compilation and execution across all supported backends.
The universal IR design enables adding new languages with minimal effort,
requiring only language-specific frontend/backend components. Our contributions
include: (1) a unified IR capturing semantics of multiple programming
paradigms, (2) a modular architecture enabling extensibility, (3) a
comprehensive framework supporting GPU compute, graphics programming, and
systems languages, and (4) empirical validation demonstrating practical
viability of universal code translation. CrossTL represents a significant step
toward language-agnostic programming, enabling write-once, deploy-everywhere
development.

</details>


### [10] [Growing Mathlib: maintenance of a large scale mathematical library](https://arxiv.org/abs/2508.21593)
*Anne Baanen,Matthew Robert Ballard,Johan Commelin,Bryan Gin-ge Chen,Michael Rothgang,Damiano Testa*

Main category: cs.PL

TL;DR: Mathlib manages rapid growth by using deprecation systems, linters, optimized design, technical debt management, and custom tools—helping maintain quality and sustainability without overburdening maintainers.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of Mathlib as a formalized mathematics library creates challenges in maintainability, quality control, and contributor scalability, necessitating systematic solutions to prevent maintainer overload and to ensure high-quality evolution of the codebase.

Method: The paper presents and discusses various practical strategies and tooling: deprecation systems for breaking changes, linters for code quality, design adjustments to improve compilation times, active management of technical debt, and custom review tools for new contributions.

Result: The library has adopted several effective practices and tools—including deprecation mechanisms, linters, redesign for performance, debt management, and contribution review tools—to balance swift expansion with manageable maintenance workload and quality assurance.

Conclusion: Effective strategies can support the sustainable growth of large-scale mathematical libraries such as Mathlib, addressing change management, code quality, technical debt, and contributor workflow.

Abstract: The Lean mathematical library Mathlib is one of the fastest-growing libraries
of formalised mathematics. We describe various strategies to manage this
growth, while allowing for change and avoiding maintainer overload. This
includes dealing with breaking changes via a deprecation system, using code
quality analysis tools (linters) to provide direct user feedback about common
pitfalls, speeding up compilation times through conscious library (re-)design,
dealing with technical debt as well as writing custom tooling to help with the
review and triage of new contributions.

</details>
