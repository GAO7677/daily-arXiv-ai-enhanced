<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.PL](#cs.PL) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Is Research Software Science a Metascience?](https://arxiv.org/abs/2509.13436)
*Evan Eisinger,Michael A. Heroux*

Main category: cs.SE

TL;DR: Examining whether Research Software Science (RSS) is a form of metascience, the paper finds RSS advances metascience goals and fits within broader definitions. Recognizing RSS as metascience could enhance its impact in research, but scientific rigor in software remains vital regardless of classification.


<details>
  <summary>Details</summary>
Motivation: As research increasingly uses computational methods, the reliability of results depends on the quality, reproducibility, and transparency of research software. The paper aims to analyze if Research Software Science (RSS) should be classified as metascience, which could impact its recognition and funding.

Method: The paper defines and compares metascience and RSS, examining their principles, objectives, and areas of overlap. It reviews arguments for and against considering RSS as metascience based on shared values and focus.

Result: RSS advances the goals of metascience, particularly in computational reproducibility, and bridges technical and social aspects. Its classification depends on whether metascience is defined broadly or narrowly. The paper concludes that RSS is best understood as a distinct but aligned interdisciplinary domain.

Conclusion: RSS fits within some definitions of metascience, and recognizing it as such could strengthen research integrity, funding, and the role of software development. Applying scientific rigor to research software is crucial regardless of its classification.

Abstract: As research increasingly relies on computational methods, the reliability of
scientific results depends on the quality, reproducibility, and transparency of
research software. Ensuring these qualities is critical for scientific
integrity and discovery. This paper asks whether Research Software Science
(RSS)--the empirical study of how research software is developed and
used--should be considered a form of metascience, the science of science.
Classification matters because it could affect recognition, funding, and
integration of RSS into research improvement. We define metascience and RSS,
compare their principles and objectives, and examine their overlaps. Arguments
for classification highlight shared commitments to reproducibility,
transparency, and empirical study of research processes. Arguments against
portraying RSS as a specialized domain focused on a tool rather than the
broader scientific enterprise. Our analysis finds RSS advances core goals of
metascience, especially in computational reproducibility, and bridges
technical, social, and cognitive aspects of research. Its classification
depends on whether one adopts a broad definition of metascience--any empirical
effort to improve science--or a narrow one focused on systemic and
epistemological structures. We argue RSS is best understood as a distinct
interdisciplinary domain that aligns with, and in some definitions fits within,
metascience. Recognizing it as such can strengthen its role in improving
reliability, justify funding, and elevate software development in research
institutions. Regardless of classification, applying scientific rigor to
research software ensures the tools of discovery meet the standards of the
discoveries themselves.

</details>


### [2] [An LLM Agentic Approach for Legal-Critical Software: A Case Study for Tax Prep Software](https://arxiv.org/abs/2509.13471)
*Sina Gogani-Khiabani,Ashutosh Trivedi,Diptikalyan Saha,Saeid Tizpaz-Niari*

Main category: cs.SE

TL;DR: This paper introduces a multi-agent, LLM-driven framework that uses advanced metamorphic testing to translate US tax codes into executable software, achieving higher robustness and performance on difficult legal tasks than more powerful models, suggesting a promising approach for creating reliable legal-critical applications.


<details>
  <summary>Details</summary>
Motivation: Large language models hold promise for translating legal statutes into executable code, but their reliability in critical legal applications is hindered by ambiguity and hallucination issues.

Method: The authors propose an agentic, multi-agent system that translates US tax code into executable software. Their approach leverages higher-order metamorphic relations for test generation under the oracle problem, and utilizes an LLM-driven, role-based framework to automate both test creation and code synthesis.

Result: Their framework, using a smaller LLM (GPT-4o-mini), achieved a worst-case pass rate of 45% on complex tax-code tasks, substantially outperforming larger frontier models (which scored 9-15%).

Conclusion: Agentic LLM-based methodologies, combined with automated metamorphic testing, can produce more robust and trustworthy legal-critical software from natural language specifications than direct reliance on larger frontier models.

Abstract: Large language models (LLMs) show promise for translating natural-language
statutes into executable logic, but reliability in legally critical settings
remains challenging due to ambiguity and hallucinations. We present an agentic
approach for developing legal-critical software, using U.S. federal tax
preparation as a case study. The key challenge is test-case generation under
the oracle problem, where correct outputs require interpreting law. Building on
metamorphic testing, we introduce higher-order metamorphic relations that
compare system outputs across structured shifts among similar individuals.
Because authoring such relations is tedious and error-prone, we use an
LLM-driven, role-based framework to automate test generation and code
synthesis. We implement a multi-agent system that translates tax code into
executable software and incorporates a metamorphic-testing agent that searches
for counterexamples. In experiments, our framework using a smaller model
(GPT-4o-mini) achieves a worst-case pass rate of 45%, outperforming frontier
models (GPT-4o and Claude 3.5, 9-15%) on complex tax-code tasks. These results
support agentic LLM methodologies as a path to robust, trustworthy
legal-critical software from natural-language specifications.

</details>


### [3] [Prompt2DAG: A Modular Methodology for LLM-Based Data Enrichment Pipeline Generation](https://arxiv.org/abs/2509.13487)
*Abubakari Alidu,Michele Ciavotta,Flavio DePaoli*

Main category: cs.SE

TL;DR: Prompt2DAG translates natural language into data workflows. Among tested methods, the Hybrid approach delivers the best reliability and efficiency, supporting accessible and automated data pipeline creation.


<details>
  <summary>Details</summary>
Motivation: Building reliable data enrichment pipelines is challenging, requiring substantial engineering knowledge. The paper aims to automate this process by turning natural language descriptions into executable workflows.

Method: The authors propose Prompt2DAG, which converts natural language into Apache Airflow DAGs. They test four approaches (Direct, LLM-only, Hybrid, Template-based) using thirteen large language models and extensive experiments (260 runs, five case studies). Performance is scored on reliability, quality, structural integrity, and executability.

Result: The Hybrid approach yields the highest success rate (78.5%), outperforming LLM-only (66.2%) and Direct (29.2%) methods, with superior quality and efficiency.

Conclusion: Structured, hybrid methods best balance automation flexibility and reliability, making data pipeline development more accessible and cost-effective.

Abstract: Developing reliable data enrichment pipelines demands significant engineering
expertise. We present Prompt2DAG, a methodology that transforms natural
language descriptions into executable Apache Airflow DAGs. We evaluate four
generation approaches -- Direct, LLM-only, Hybrid, and Template-based -- across
260 experiments using thirteen LLMs and five case studies to identify optimal
strategies for production-grade automation. Performance is measured using a
penalized scoring framework that combines reliability with code quality (SAT),
structural integrity (DST), and executability (PCT). The Hybrid approach
emerges as the optimal generative method, achieving a 78.5% success rate with
robust quality scores (SAT: 6.79, DST: 7.67, PCT: 7.76). This significantly
outperforms the LLM-only (66.2% success) and Direct (29.2% success) methods.
Our findings show that reliability, not intrinsic code quality, is the primary
differentiator. Cost-effectiveness analysis reveals the Hybrid method is over
twice as efficient as Direct prompting per successful DAG. We conclude that a
structured, hybrid approach is essential for balancing flexibility and
reliability in automated workflow generation, offering a viable path to
democratize data pipeline development.

</details>


### [4] [Crash Report Enhancement with Large Language Models: An Empirical Study](https://arxiv.org/abs/2509.13535)
*S M Farah Al Fahim,Md Nakhla Rafi,Zeyang Ma,Dong Jae Kim,Tse-Hsun,Chen*

Main category: cs.SE

TL;DR: Using LLMs to enhance crash reports leads to major improvements in fault localization, explanation, and fix suggestions; the iterative Agentic-LLM strategy is especially effective, and user studies confirm the increased usefulness for debugging.


<details>
  <summary>Details</summary>
Motivation: Crash reports are vital for software maintenance, but typically lack sufficient diagnostic detail for efficient debugging. The authors seek to leverage LLMs to address this gap and provide developers with deeper, more actionable insights from crash reports.

Method: The paper studies two LLM-based enhancement strategies for crash reports: Direct-LLM, which uses stack-trace context in a single shot, and Agentic-LLM, which iteratively explores the repository for evidence. Evaluations use a dataset of 492 real-world crash reports, with both automatic metrics and user studies.

Result: LLM-enhanced reports improve problem-localization accuracy from 10.6% (original) to 40.2–43.1%, and suggested fixes closely resemble developer patches (CodeBLEU ~56–57%). The Agentic-LLM strategy offers superior root-cause and repair guidance. User studies confirm enhanced reports help users understand and resolve crashes more effectively, especially with improved repair guidance.

Conclusion: Supplying large language models (LLMs) with stack traces and repository code produces crash reports that significantly improve debugging utility. Enhanced reports deliver better fault localization, root-cause explanations, and actionable repair suggestions.

Abstract: Crash reports are central to software maintenance, yet many lack the
diagnostic detail developers need to debug efficiently. We examine whether
large language models can enhance crash reports by adding fault locations,
root-cause explanations, and repair suggestions. We study two enhancement
strategies: Direct-LLM, a single-shot approach that uses stack-trace context,
and Agentic-LLM, an iterative approach that explores the repository for
additional evidence. On a dataset of 492 real-world crash reports, LLM-enhanced
reports improve Top-1 problem-localization accuracy from 10.6% (original
reports) to 40.2-43.1%, and produce suggested fixes that closely resemble
developer patches (CodeBLEU around 56-57%). Both our manual evaluations and
LLM-as-a-judge assessment show that Agentic-LLM delivers stronger root-cause
explanations and more actionable repair guidance. A user study with 16
participants further confirms that enhanced reports make crashes easier to
understand and resolve, with the largest improvement in repair guidance. These
results indicate that supplying LLMs with stack traces and repository code
yields enhanced crash reports that are substantially more useful for debugging.

</details>


### [5] [GitHub's Copilot Code Review: Can AI Spot Security Flaws Before You Commit?](https://arxiv.org/abs/2509.13650)
*Amena Amro,Manar H. Alalfi*

Main category: cs.SE

TL;DR: GitHub Copilot's new code review feature often misses serious security issues and mostly flags minor coding errors. Developers still need proper security tools and manual reviews for real protection.


<details>
  <summary>Details</summary>
Motivation: With the widespread adoption of AI-powered tools in software development, there is a growing need to ensure these tools can effectively support secure coding practices. The newly introduced code review feature in GitHub Copilot claims to automate security checks, raising expectations about its capabilities for vulnerability detection.

Method: The study used a curated and labeled set of vulnerable code samples from various open-source projects, covering different programming languages and application domains. The effectiveness of Copilot's code review feature in detecting and providing feedback on common security vulnerabilities was systematically assessed.

Result: Copilot's code review feature failed to reliably detect critical security vulnerabilities such as SQL injection, cross-site scripting (XSS), and insecure deserialization. Its feedback was mostly limited to low-severity issues like coding style and typographical errors.

Conclusion: The perceived capabilities of AI-assisted code review tools like Copilot do not match their actual performance in supporting secure coding practices. Dedicated security tools and manual code audits remain necessary to ensure robust software security.

Abstract: As software development practices increasingly adopt AI-powered tools,
ensuring that such tools can support secure coding has become critical. This
study evaluates the effectiveness of GitHub Copilot's recently introduced code
review feature in detecting security vulnerabilities. Using a curated set of
labeled vulnerable code samples drawn from diverse open-source projects
spanning multiple programming languages and application domains, we
systematically assessed Copilot's ability to identify and provide feedback on
common security flaws. Contrary to expectations, our results reveal that
Copilot's code review frequently fails to detect critical vulnerabilities such
as SQL injection, cross-site scripting (XSS), and insecure deserialization.
Instead, its feedback primarily addresses low-severity issues, such as coding
style and typographical errors. These findings expose a significant gap between
the perceived capabilities of AI-assisted code review and its actual
effectiveness in supporting secure development practices. Our results highlight
the continued necessity of dedicated security tools and manual code audits to
ensure robust software security.

</details>


### [6] [A Regression Testing Framework with Automated Assertion Generation for Machine Learning Notebooks](https://arxiv.org/abs/2509.13656)
*Yingao Elaine Yao,Vedant Nimje,Varun Viswanath,Saikat Dutta*

Main category: cs.SE

TL;DR: NBTest introduces cell-level regression testing and automated assertions for ML notebooks, integrates seamlessly into CI and development workflows, detects bugs and reduces flakiness, and is intuitive and useful according to user studies.


<details>
  <summary>Details</summary>
Motivation: Notebooks are widely used for ML development, but lack robust support for testing, resulting in unnoticed subtle bugs and performance regressions.

Method: Introduces NBTest, a regression testing framework for ML notebooks supporting cell-level assertions, integration with pytest and CI, offering assertion APIs, a JupyterLab plugin, and automated cell-level assertion generation for data processing, model building, and evaluation.

Result: NBTest generated 21,163 assertions across 592 Kaggle notebooks, achieving a mutation score of 0.57. It detected regression bugs and minimized flaky assertions via statistical techniques. NBTest has been adopted in a popular ML library's CI, and user studies report high intuitiveness (4.3/5) and usefulness (4.24/5).

Conclusion: NBTest improves the reliability and maintainability of ML notebooks by enabling effective, developer-friendly regression testing and assertion generation, with demonstrated practicality and adoption.

Abstract: Notebooks have become the de-facto choice for data scientists and machine
learning engineers for prototyping and experimenting with machine learning (ML)
pipelines. Notebooks provide an interactive interface for code, data, and
visualization. However, notebooks provide very limited support for testing.
Thus, during continuous development, many subtle bugs that do not lead to
crashes often go unnoticed and cause silent errors that manifest as performance
regressions.
  To address this, we introduce NBTest - the first regression testing framework
that allows developers to write cell-level assertions in notebooks and run such
notebooks in pytest or in continuous integration (CI) pipelines. NBTest offers
a library of assertion APIs, and a JupyterLab plugin that enables executing
assertions. We also develop the first automated approach for generating
cell-level assertions for key components in ML notebooks, such as data
processing, model building, and model evaluation. NBTest aims to improve the
reliability and maintainability of ML notebooks without adding developer
burden.
  We evaluate NBTest on 592 Kaggle notebooks. Overall, NBTest generates 21163
assertions (35.75 on average per notebook). The generated assertions obtain a
mutation score of 0.57 in killing ML-specific mutations. NBTest can catch
regression bugs in previous versions of the Kaggle notebooks using assertions
generated for the latest versions. Because ML pipelines involve non
deterministic computations, the assertions can be flaky. Hence, we also show
how NBTest leverages statistical techniques to minimize flakiness while
retaining high fault-detection effectiveness. NBTest has been adopted in the CI
of a popular ML library. Further, we perform a user study with 17 participants
that shows that notebook users find NBTest intuitive (Rating 4.3/5) and useful
in writing assertions and testing notebooks (Rating 4.24/5).

</details>


### [7] [Prompt Stability in Code LLMs: Measuring Sensitivity across Emotion- and Personality-Driven Variations](https://arxiv.org/abs/2509.13680)
*Wei Ma,Yixiao Yang,Jingquan Ge,Xiaofei Xie,Lingxiao Jiang*

Main category: cs.SE

TL;DR: PromptSE is a new framework to evaluate how changes in prompt phrasing affect code generation models, offering metrics for stability across models. The study finds that prompt stability and performance do not always go hand in hand, advocating for stability as another critical factor in choosing and deploying AI code tools.


<details>
  <summary>Details</summary>
Motivation: Code generation models often produce varying outputs depending on how prompts are phrased, especially when prompts differ in emotion or communication style. This sensitivity is not thoroughly examined in existing benchmarks, which mainly focus on peak performance.

Method: The authors present PromptSE, a framework for evaluating prompt sensitivity. It creates semantically equivalent prompt variants using emotion and personality templates, and utilizes probability-aware continuous scoring or binary pass rates to measure stability. Results are summarized using a novel AUC-E metric for cross-model comparison.

Result: The study, which tested 14 models from Llama, Qwen, and DeepSeek families, found that stability and performance are mostly independent objectives. It also uncovered new insights regarding model robustness related to architecture and scale, opposing some common beliefs.

Conclusion: PromptSE enables practitioners to systematically measure and balance performance stability in code generation models. It supports screening of closed-source models and allows stability analysis, thus establishing prompt stability as a key evaluation metric alongside performance and fairness, and contributing to the reliability of AI software tools.

Abstract: Code generation models are widely used in software development, yet their
sensitivity to prompt phrasing remains under-examined. Identical requirements
expressed with different emotions or communication styles can yield divergent
outputs, while most benchmarks emphasize only peak performance. We present
PromptSE (Prompt Sensitivity Evaluation), a framework that creates semantically
equivalent prompt variants with emotion and personality templates, and that
evaluates stability using probability aware continuous scoring or using binary
pass rates when logits are unavailable. The results are aggregated into a
proposed area under curve metric (AUC-E) for cross model comparison. Across 14
models from three families (Llama, Qwen, and DeepSeek), our study shows that
performance and stability behave as largely decoupled optimization objectives,
and it reveals architectural and scale related patterns that challenge common
assumptions about model robustness. The framework supports rapid screening for
closed-source models as well as detailed stability analysis in research
settings. PromptSE enables practitioners to quantify performance stability
trade offs for deployment and model selection, positioning prompt stability as
a complementary evaluation dimension alongside performance and fairness, and
contributing to more trustworthy AI-assisted software development tools.

</details>


### [8] [Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning](https://arxiv.org/abs/2509.13755)
*Zhaoyang Chu,Yao Wan,Zhikun Zhang,Di Wang,Zhou Yang,Hongyu Zhang,Pan Zhou,Xuanhua Shi,Hai Jin,David Lo*

Main category: cs.SE

TL;DR: CLMs can leak sensitive information by memorizing training data. This paper proposes CodeEraser, a machine unlearning technique to efficiently erase such memorization without retraining, while maintaining code usefulness.


<details>
  <summary>Details</summary>
Motivation: CLMs are widely used but pose security risks due to their ability to memorize and reproduce sensitive data. Existing protections are resource-intensive, requiring model retraining. The authors seek a post-hoc, efficient, and effective method to address privacy risks inherent to deployed CLMs.

Method: The authors use gradient ascent-based unlearning algorithms (vanilla and constraint-based) and propose CodeEraser—a selective approach that removes sensitive data at the memorization level while maintaining code structure and functionality. They perform extensive experiments on three model families (CodeParrot, CodeGen-Mono, Qwen2.5-Coder) using a curated set of 50,000 sensitive samples.

Result: The paper investigates the critical privacy issues in Code Language Models (CLMs) caused by memorization of sensitive training data, which can lead to unintended exposure of confidential information. It studies methods to erase memorized sensitive data without the costly process of full-model retraining.

Conclusion: CodeEraser is effective and efficient in removing targeted sensitive information memorized by CLMs and preserves the models' utility. The work demonstrates the advantages of gradient ascent-based unlearning compared to traditional approaches.

Abstract: While Code Language Models (CLMs) have demonstrated superior performance in
software engineering tasks such as code generation and summarization, recent
empirical studies reveal a critical privacy vulnerability: these models exhibit
unintended memorization of sensitive training data, enabling verbatim
reproduction of confidential information when specifically prompted. To address
this issue, several approaches, including training data de-duplication and
differential privacy augmentation, have been proposed. However, these methods
require full-model retraining for deployed CLMs, which incurs substantial
computational costs. In this paper, we aim to answer the following research
question: Can sensitive information memorized by CLMs be erased effectively and
efficiently?
  We conduct a pioneering investigation into erasing sensitive memorization in
CLMs through machine unlearning - a post-hoc modification method that removes
specific information from trained models without requiring full retraining.
Specifically, we first quantify the memorization risks of sensitive data within
CLM training datasets and curate a high-risk dataset of 50,000 sensitive
memorized samples as unlearning targets. We study two widely used gradient
ascent-based unlearning approaches: the vanilla and constraint-based methods,
and introduce CodeEraser, an advanced variant that selectively unlearns
sensitive memorized segments in code while preserving the structural integrity
and functional correctness of the surrounding code. Extensive experiments on
three families of CLMs, i.e., CodeParrot, CodeGen-Mono, and Qwen2.5-Coder,
validate the effectiveness and efficiency of CodeEraser in erasing targeted
sensitive memorization while maintaining model utility.

</details>


### [9] [A Study on Thinking Patterns of Large Reasoning Models in Code Generation](https://arxiv.org/abs/2509.13758)
*Kevin Halim,Sin G. Teo,Ruitao Feng,Zhenpeng Chen,Yang Gu,Chong Wang,Yang Liu*

Main category: cs.SE

TL;DR: This paper empirically analyzes how advanced reasoning models (LRMs) approach code generation, revealing distinct reasoning patterns and their impact on code quality. By identifying a taxonomy of reasoning actions and studying their effects, the researchers show that prompts leveraging these insights can enhance LRM performance in software engineering tasks.


<details>
  <summary>Details</summary>
Motivation: Although LRMs have shown superior multi-step reasoning capabilities for code generation, there is limited research on how their reasoning patterns affect generated code.

Method: The authors comprehensively study reasoning traces in multiple state-of-the-art LRMs by prompting them with code generation tasks and manually annotating their reasoning patterns through open coding. This results in taxonomy development and empirical analysis.

Result: They classify 15 reasoning actions across 4 phases. Key findings include: (1) LRMs mimic human coding workflows, introducing extra reasoning actions for complex tasks; (2) Different LRMs exhibit distinct reasoning styles (e.g., Qwen3 iterative, DeepSeek-R1-7B linear); (3) Some reasoning actions (like scaffolding, unit testing) strongly correlate with functional code outcomes; (4) Lightweight, reasoning-informed prompts can enhance LRM-generated code quality.

Conclusion: Understanding and leveraging LRM reasoning patterns leads to better code generation and paves the way for more effective prompting strategies.

Abstract: Currently, many large language models (LLMs) are utilized for software
engineering tasks such as code generation. The emergence of more advanced
models known as large reasoning models (LRMs), such as OpenAI's o3, DeepSeek
R1, and Qwen3. They have demonstrated the capability of performing multi-step
reasoning. Despite the advancement in LRMs, little attention has been paid to
systematically analyzing the reasoning patterns these models exhibit and how
such patterns influence the generated code. This paper presents a comprehensive
study aimed at investigating and uncovering the reasoning behavior of LRMs
during code generation. We prompted several state-of-the-art LRMs of varying
sizes with code generation tasks and applied open coding to manually annotate
the reasoning traces. From this analysis, we derive a taxonomy of LRM reasoning
behaviors, encompassing 15 reasoning actions across four phases.
  Our empirical study based on the taxonomy reveals a series of findings.
First, we identify common reasoning patterns, showing that LRMs generally
follow a human-like coding workflow, with more complex tasks eliciting
additional actions such as scaffolding, flaw detection, and style checks.
Second, we compare reasoning across models, finding that Qwen3 exhibits
iterative reasoning while DeepSeek-R1-7B follows a more linear, waterfall-like
approach. Third, we analyze the relationship between reasoning and code
correctness, showing that actions such as unit test creation and scaffold
generation strongly support functional outcomes, with LRMs adapting strategies
based on task context. Finally, we evaluate lightweight prompting strategies
informed by these findings, demonstrating the potential of context- and
reasoning-oriented prompts to improve LRM-generated code. Our results offer
insights and practical implications for advancing automatic code generation.

</details>


### [10] [Who is Introducing the Failure? Automatically Attributing Failures of Multi-Agent Systems via Spectrum Analysis](https://arxiv.org/abs/2509.13782)
*Yu Ge,Linna Xie,Zhong Li,Yu Pei,Tian Zhang*

Main category: cs.SE

TL;DR: FAMAS is a new method for tracing failures back to specific agent actions in multi-agent systems, using systematic trajectory analysis and a tailored formula to outperform existing approaches.


<details>
  <summary>Details</summary>
Motivation: Large Language Model Powered Multi-Agent Systems (MASs) are increasingly used for automating complex tasks, but failure attribution—identifying which specific agent actions cause failures—remains difficult, underexplored, and labor-intensive, creating obstacles for debugging and improvement.

Method: The authors propose FAMAS, a spectrum-based failure attribution method for MASs. FAMAS uses systematic trajectory replay and abstraction, combined with spectrum analysis. A novel suspiciousness formula is introduced, incorporating both agent behavior and action behavior groups to analyze activation patterns in agent execution trajectories.

Result: FAMAS was evaluated against 12 baselines using the Who and When benchmark, where it demonstrated superior performance by outperforming all compared methods.

Conclusion: FAMAS provides an effective and systematic approach for failure attribution in Multi-Agent Systems, making the debugging process more efficient and accurate.

Abstract: Large Language Model Powered Multi-Agent Systems (MASs) are increasingly
employed to automate complex real-world problems, such as programming and
scientific discovery. Despite their promising, MASs are not without their
flaws. However, failure attribution in MASs - pinpointing the specific agent
actions responsible for failures - remains underexplored and labor-intensive,
posing significant challenges for debugging and system improvement. To bridge
this gap, we propose FAMAS, the first spectrum-based failure attribution
approach for MASs, which operates through systematic trajectory replay and
abstraction, followed by spectrum analysis.The core idea of FAMAS is to
estimate, from variations across repeated MAS executions, the likelihood that
each agent action is responsible for the failure. In particular, we propose a
novel suspiciousness formula tailored to MASs, which integrates two key factor
groups, namely the agent behavior group and the action behavior group, to
account for the agent activation patterns and the action activation patterns
within the execution trajectories of MASs. Through expensive evaluations
against 12 baselines on the Who and When benchmark, FAMAS demonstrates superior
performance by outperforming all the methods in comparison.

</details>


### [11] [Trace Sampling 2.0: Code Knowledge Enhanced Span-level Sampling for Distributed Tracing](https://arxiv.org/abs/2509.13852)
*Yulun Wu,Guangba Yu,Zhihan Jiang,Yichen Li,Michael R. Lyu*

Main category: cs.SE

TL;DR: Autoscope improves distributed tracing by sampling at the span level, reducing trace storage by 81% while maintaining nearly full coverage of faults and improving root cause analysis, offering a better solution than traditional trace sampling.


<details>
  <summary>Details</summary>
Motivation: Traditional trace sampling in microservice systems often discards valuable information by retaining mostly anomalous traces, making it difficult to perform comparative analyses and negatively impacting observability.

Method: The authors propose Trace Sampling 2.0, a span-level sampling approach that maintains trace structure consistency. They design Autoscope, which uses static analysis to extract execution logic and selectively samples spans, ensuring critical information is preserved.

Result: Autoscope was evaluated using two open-source microservices and demonstrated a reduction in trace size by 81.2%, while maintaining 98.1% coverage of faulty spans. It also improved root cause analysis effectiveness by an average of 8.3%, surpassing existing sampling methods.

Conclusion: Autoscope enables significant reductions in storage overhead for distributed tracing without sacrificing trace coverage or analysis quality, thereby enhancing both observability and performance monitoring in microservice environments.

Abstract: Distributed tracing is an essential diagnostic tool in microservice systems,
but the sheer volume of traces places a significant burden on backend storage.
A common approach to mitigating this issue is trace sampling, which selectively
retains traces based on specific criteria, often preserving only anomalous
ones. However, this method frequently discards valuable information, including
normal traces that are essential for comparative analysis. To address this
limitation, we introduce Trace Sampling 2.0, which operates at the span level
while maintaining trace structure consistency. This approach allows for the
retention of all traces while significantly reducing storage overhead. Based on
this concept, we design and implement Autoscope, a span-level sampling method
that leverages static analysis to extract execution logic, ensuring that
critical spans are preserved without compromising structural integrity. We
evaluated Autoscope on two open-source microservices. Our results show that it
reduces trace size by 81.2% while maintaining 98.1% faulty span coverage,
outperforming existing trace-level sampling methods. Furthermore, we
demonstrate its effectiveness in root cause analysis, achieving an average
improvement of 8.3%. These findings indicate that Autoscope can significantly
enhance observability and storage efficiency in microservices, offering a
robust solution for performance monitoring.

</details>


### [12] [Are Prompts All You Need? Evaluating Prompt-Based Large Language Models (LLM)s for Software Requirements Classification](https://arxiv.org/abs/2509.13868)
*Manal Binkhonain,Reem Alfayaz*

Main category: cs.SE

TL;DR: Prompt-based large language models can classify requirements accurately with much less labeled data than traditional methods, reducing annotation effort and transferring better across tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional requirements classification relies on supervised learning, requiring large labeled datasets that are time-consuming, costly, and limited in generalizability.

Method: The authors experiment with prompt-based large language models (LLMs), testing different prompting styles (zero-shot, few-shot, persona, chain of thought) on two datasets (PROMISE and SecReq) and compare their performance to a fine-tuned transformer baseline.

Result: Prompt-based LLMs, especially with few-shot prompts, perform as well as or better than the baseline. Persona and chain-of-thought prompting further improve results.

Conclusion: Prompt-based LLMs greatly reduce data annotation needs, generalize better across domains, and are a scalable, practical solution for requirements classification.

Abstract: Requirements classification assigns natural language requirements to
predefined classes, such as functional and non functional. Accurate
classification reduces risk and improves software quality. Most existing models
rely on supervised learning, which needs large labeled data that are costly,
slow to create, and domain dependent; they also generalize poorly and often
require retraining for each task. This study tests whether prompt based large
language models can reduce data needs. We benchmark several models and
prompting styles (zero shot, few shot, persona, and chain of thought) across
multiple tasks on two English datasets, PROMISE and SecReq. For each task we
compare model prompt configurations and then compare the best LLM setups with a
strong fine tuned transformer baseline. Results show that prompt based LLMs,
especially with few shot prompts, can match or exceed the baseline. Adding a
persona, or persona plus chain of thought, can yield further gains. We conclude
that prompt based LLMs are a practical and scalable option that reduces
dependence on large annotations and can improve generalizability across tasks.

</details>


### [13] [Mind the Ethics! The Overlooked Ethical Dimensions of GenAI in Software Modeling Education](https://arxiv.org/abs/2509.13896)
*Shalini Chakraborty,Lola Burgueño,Nathalie Moreno,Javier Troya,Paula Muñoz*

Main category: cs.SE

TL;DR: Despite GenAI's growing role in software modeling education, almost no existing research seriously addresses its ethical implications. Only 3 out of 1,386 papers reviewed discussed ethical issues, signaling a pressing need for ethical frameworks and further study in this area.


<details>
  <summary>Details</summary>
Motivation: GenAI is increasingly shaping learning outcomes in software modeling education without clear ethical oversight or pedagogical guidelines, prompting concerns about responsibility, fairness, transparency, diversity, and inclusion.

Method: A systematic literature review was conducted across six major computer science digital libraries, screening 1,386 unique papers for explicit discussion of ethical aspects tied to GenAI in modeling education.

Result: Of the 1,386 papers reviewed, only three explicitly addressed ethical considerations, demonstrating a critical gap in current research and ethical discussions related to GenAI in this context.

Conclusion: There is a significant lack of research and ethical discourse regarding the integration of Generative AI (GenAI) in software modeling education, highlighting the urgent need for structured ethical frameworks.

Abstract: Generative Artificial Intelligence (GenAI) is rapidly gaining momentum in
software modeling education, embraced by both students and educators. As GenAI
assists with interpreting requirements, formalizing models, and translating
students' mental models into structured notations, it increasingly shapes core
learning outcomes such as domain comprehension, diagrammatic thinking, and
modeling fluency without clear ethical oversight or pedagogical guidelines.
Yet, the ethical implications of this integration remain underexplored.
  In this paper, we conduct a systematic literature review across six major
digital libraries in computer science (ACM Digital Library, IEEE Xplore,
Scopus, ScienceDirect, SpringerLink, and Web of Science). Our aim is to
identify studies discussing the ethical aspects of GenAI in software modeling
education, including responsibility, fairness, transparency, diversity, and
inclusion among others.
  Out of 1,386 unique papers initially retrieved, only three explicitly
addressed ethical considerations. This scarcity highlights the critical absence
of ethical discourse surrounding GenAI in modeling education and raises urgent
questions about the responsible integration of AI in modeling curricula, as
well as it evinces the pressing need for structured ethical frameworks in this
emerging educational landscape. We examine these three studies and explore the
emerging research opportunities as well as the challenges that have arisen in
this field.

</details>


### [14] [An Empirical Study on Failures in Automated Issue Solving](https://arxiv.org/abs/2509.13941)
*Simiao Liu,Fang Liu,Liehao Li,Xin Tan,Yinghao Zhu,Xiaoli Lian,Li Zhang*

Main category: cs.SE

TL;DR: This paper diagnoses why automated code repair tools (evaluated on SWE-Bench) fail, categorizes those failures, and shows that a new collaborative framework can fix many previously unsolvable problems by overcoming typical agentic model weaknesses.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of automated issue solving in code (especially using SWE-Bench) mainly give aggregate success rates, but don't dig into why tools succeed or fail. This makes it hard to target improvements or deeply understand model weaknesses.

Method: Analyzed three state-of-the-art automated issue-solving tools (covering pipeline-based and agentic architectures) on diverse SWE-Bench-Verified tasks. Then, manually examined 150 failed cases to develop a taxonomy of failure types, and studied how these failures distribute across tools. Finally, designed and tested a new Expert-Executor framework to address common failure causes in agentic tools.

Result: Identified three main phases, nine categories, and twenty-five subcategories of failure modes, with agentic models most often failing due to flawed reasoning and cognitive deadlocks. The proposed Expert-Executor framework can resolve 22.2% of issues previously unsolvable by top single-agent models.

Conclusion: Understanding and categorizing the reasons for failure enables more targeted improvements. The introduced collaborative Expert-Executor approach substantially boosts issue-solving rates and breaks through common agentic limitations, paving the way for more robust future models.

Abstract: Automated issue solving seeks to autonomously identify and repair defective
code snippets across an entire codebase. SWE-Bench has emerged as the most
widely adopted benchmark for evaluating progress in this area. While LLM-based
agentic tools show great promise, they still fail on a substantial portion of
tasks. Moreover, current evaluations primarily report aggregate issue-solving
rates, which obscure the underlying causes of success and failure, making it
challenging to diagnose model weaknesses or guide targeted improvements. To
bridge this gap, we first analyze the performance and efficiency of three SOTA
tools, spanning both pipeline-based and agentic architectures, in automated
issue solving tasks of SWE-Bench-Verified under varying task characteristics.
Furthermore, to move from high-level performance metrics to underlying cause
analysis, we conducted a systematic manual analysis of 150 failed instances.
From this analysis, we developed a comprehensive taxonomy of failure modes
comprising 3 primary phases, 9 main categories, and 25 fine-grained
subcategories. Then we systematically analyze the distribution of the
identified failure modes, the results reveal distinct failure fingerprints
between the two architectural paradigms, with the majority of agentic failures
stemming from flawed reasoning and cognitive deadlocks. Motivated by these
insights, we propose a collaborative Expert-Executor framework. It introduces a
supervisory Expert agent tasked with providing strategic oversight and
course-correction for a primary Executor agent. This architecture is designed
to correct flawed reasoning and break the cognitive deadlocks that frequently
lead to failure. Experiments show that our framework solves 22.2% of previously
intractable issues for a leading single agent. These findings pave the way for
building more robust agents through diagnostic evaluation and collaborative
design.

</details>


### [15] [Evaluating Classical Software Process Models as Coordination Mechanisms for LLM-Based Software Generation](https://arxiv.org/abs/2509.13942)
*Duc Minh Ha,Phu Trac Kien,Tho Quan,Anh Nguyen-Duc*

Main category: cs.SE

TL;DR: The paper investigates how classic software engineering processes (Waterfall, V-Model, Agile) can steer LLM-based multi-agent systems. Experiments show that process choice impacts code quality, size, and development cost. Process selection should match project priorities like efficiency or quality.


<details>
  <summary>Details</summary>
Motivation: Large Language Model (LLM)-based multi-agent systems (MAS) are changing software development. Classical software development processes provide proven coordination structures, but it is unclear how these can be adapted for LLM-driven MAS and what their impact would be.

Method: The authors executed 11 different software projects, using three classical process models (Waterfall, V-Model, Agile) and four GPT LLM variants, making 132 total runs. Each run's output was analyzed for size, cost, and quality using standardized metrics.

Result: Both the choice of classical process and LLM greatly impacted outcomes. Waterfall provided highest efficiency, V-Model generated the most verbose code, and Agile resulted in the highest code quality but at greater computational cost.

Conclusion: Traditional software processes can be successfully adapted for use in LLM-based multi-agent systems, but each has different trade-offs between quality, cost, and adaptability. The best process model to use depends on the specific goals and needs of the software project.

Abstract: [Background] Large Language Model (LLM)-based multi-agent systems (MAS) are
transforming software development by enabling autonomous collaboration.
Classical software processes such asWaterfall, V-Model, and Agile offer
structured coordination patterns that can be repurposed to guide these agent
interactions. [Aims] This study explores how traditional software development
processes can be adapted as coordination scaffolds for LLM based MAS and
examines their impact on code quality, cost, and productivity. [Method] We
executed 11 diverse software projects under three process models and four GPT
variants, totaling 132 runs. Each output was evaluated using standardized
metrics for size (files, LOC), cost (execution time, token usage), and quality
(code smells, AI- and human detected bugs). [Results] Both process model and
LLM choice significantly affected system performance. Waterfall was most
efficient, V-Model produced the most verbose code, and Agile achieved the
highest code quality, albeit at higher computational cost. [Conclusions]
Classical software processes can be effectively instantiated in LLM-based MAS,
but each entails trade-offs across quality, cost, and adaptability. Process
selection should reflect project goals, whether prioritizing efficiency,
robustness, or structured validation.

</details>


### [16] [Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework](https://arxiv.org/abs/2509.14093)
*Kerui Huang,Shuhan Liu,Xing Hu,Tongtong Xu,Lingfeng Bao,Xin Xia*

Main category: cs.SE

TL;DR: Longer CoT reasoning in LLMs often increases computational costs without guaranteeing higher accuracy. SEER is an adaptive framework that compresses CoT outputs, reduces latency, prevents common failures, and improves overall efficiency and robustness, particularly for software engineering tasks.


<details>
  <summary>Details</summary>
Motivation: While Chain-of-Thought (CoT) reasoning can improve the performance of large language models (LLMs), especially on complex tasks, it introduces substantial computational costs that hinder efficiency and practicality for tasks that require concise and deterministic outputs, such as in software engineering.

Method: The paper conducts an empirical study on the effects of CoT reasoning length in code generation benchmarks, then proposes SEER (Self-Enhancing Efficient Reasoning), a framework that adaptively compresses CoT via Best-of-N sampling and task-aware adaptive filtering with dynamic thresholding based on pre-inference outputs.

Result: SEER shortens CoT by 42.1% on average, improves accuracy by reducing output truncation, and eliminates most infinite loops in outputs, as demonstrated across three software engineering tasks and one math task.

Conclusion: Longer CoT does not guarantee better results and may hurt performance and efficiency. SEER offers a promising adaptive approach that maintains the benefits of CoT reasoning while significantly reducing computational overhead and making LLMs more robust and efficient for practical applications.

Abstract: Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by
prompting intermediate steps, improving accuracy and robustness in arithmetic,
logic, and commonsense tasks. However, this benefit comes with high
computational costs: longer outputs increase latency, memory usage, and
KV-cache demands. These issues are especially critical in software engineering
tasks where concise and deterministic outputs are required. To investigate
these trade-offs, we conduct an empirical study based on code generation
benchmarks. The results reveal that longer CoT does not always help. Excessive
reasoning often causes truncation, accuracy drops, and latency up to five times
higher, with failed outputs consistently longer than successful ones. These
findings challenge the assumption that longer reasoning is inherently better
and highlight the need for adaptive CoT control. Motivated by this, we propose
SEER (Self-Enhancing Efficient Reasoning), an adaptive framework that
compresses CoT while preserving accuracy. SEER combines Best-of-N sampling with
task-aware adaptive filtering, dynamically adjusting thresholds based on
pre-inference outputs to reduce verbosity and computational overhead. We then
evaluate SEER on three software engineering tasks and one math task. On
average, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,
and eliminates most infinite loops. These results demonstrate SEER as a
practical method to make CoT-enhanced LLMs more efficient and robust, even
under resource constraints.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [17] [Catalpa: GC for a Low-Variance Software Stack](https://arxiv.org/abs/2509.13429)
*Anthony Arnold,Mark Marron*

Main category: cs.PL

TL;DR: The paper proposes Catalpa, a novel garbage collector for Bosque, which uses language features to guarantee low, predictable response times and minimize performance variability, focusing on real-world usability (user-noticeable delays) rather than just average-case performance.


<details>
  <summary>Details</summary>
Motivation: Traditional performance metrics focus on continuous measures like average memory or time use, but real-world application usability is often determined by binary thresholds of responsiveness (e.g., responding in under 100 ms). Industrial developers care more about tail-latencies (95th and 99th percentile) than simple averages.

Method: The paper introduces the Catalpa garbage collector for the Bosque programming language. The collector exploits language features such as immutability and reference-cycle freedom to minimize latency and variability, while maintaining high throughput and low memory overhead. It specifically aims to achieve bounded collection pauses, constant memory overheads, and avoids synchronization/barrier requirements in application code.

Result: The Catalpa collector provides predictable, low-latency garbage collection for Bosque applications, keeping collection pauses within fixed bounds. It achieves high throughput and does not significantly increase memory use, all without the need for additional synchronization between the application and collector.

Conclusion: Through tailored garbage collection design leveraging language-specific guarantees, it's possible to significantly reduce latency and variability in application performance, making programs consistently responsive and usable.

Abstract: The performance of an application/runtime is usually conceptualized as a
continuous function where, the lower the amount of memory/time used on a given
workload, then the better the compiler/runtime is. However, in practice, good
performance of an application is viewed as more of a binary function - either
the application responds in under, say 100 ms, and is fast enough for a user to
barely notice, or it takes a noticeable amount of time, leaving the user
waiting and potentially abandoning the task. Thus, performance really means how
often the application is fast enough to be usable, leading industrial
developers to focus on the 95th and 99th percentile tail-latencies as heavily,
or moreso, than average response time. Our vision is to create a software stack
that actively supports these needs via programming language and runtime system
design. In this paper we present a novel garbage-collector design, the Catalpa
collector, for the Bosque programming language and runtime. This allocator is
designed to minimize latency and variability while maintaining high-throughput
and incurring small memory overheads. To achieve these goals we leverage
various features of the Bosque language, including immutability and
reference-cycle freedom, to construct a collector that has bounded collection
pauses, incurs fixed-constant memory overheads, and does not require any
barriers or synchronization with application code.

</details>


### [18] [Extended Abstract: Towards a Performance Comparison of Syntax and Type-Directed NbE](https://arxiv.org/abs/2509.13489)
*Chester J. F. Gould,William J. Bowman*

Main category: cs.PL

TL;DR: This paper sets up a real-world experimental platform to directly compare syntax-directed and type-directed type equality methods in dependent type-checkers, quantifies their relative performance, and investigates opportunities to optimize the slower, more expressive type-directed approach.


<details>
  <summary>Details</summary>
Motivation: The motivation is to rigorously compare syntax-directed and type-directed equality checking in dependent type-checkers, addressing the commonly heard, but unsubstantiated, claims about their performance and expressiveness.

Method: The authors are developing a realistic platform that enables a direct, apples-to-apples comparison of both approaches. This experimental setup aims to quantify performance differences and analyze their causes.

Result: Preliminary results indicate that type-directed equality checking is slower, and the research provides insights into the factors contributing to this performance gap and strategies for improvement.

Conclusion: Having a direct comparative platform illuminates the actual trade-offs between syntax-directed and type-directed equality approaches, making previously anecdotal claims more precise and actionable.

Abstract: A key part of any dependent type-checker is the method for checking whether
two types are equal. A common claim is that syntax-directed equality is more
performant, although type-directed equality is more expressive. However, this
claim is difficult to make precise, since implementations choose only one or
the other approach, making a direct comparison impossible. We present some
work-in-progress developing a realistic platform for direct, apples-to-apples,
comparison of the two approaches, quantifying how much slower type-directed
equality checking is, and analyzing why and how it can be improved.

</details>


### [19] [CLMTracing: Black-box User-level Watermarking for Code Language Model Tracing](https://arxiv.org/abs/2509.13982)
*Boyu Zhang,Ping He,Tianyu Du,Xuhong Zhang,Lei Yun,Kingsum Chow,Jianwei Yin*

Main category: cs.PL

TL;DR: The paper proposes CLMTracing, a novel framework that enables robust user-level tracing of open-source code language models in black-box settings using advanced watermarking techniques. It surpasses existing methods in defending against watermark removal and maintaining model utility.


<details>
  <summary>Details</summary>
Motivation: With the growing use of open-source code language models (code LMs), protecting intellectual property (IP) has become more urgent. Existing watermarking methods struggle to trace models at the individual user level, especially in black-box settings.

Method: The paper introduces CLMTracing—a black-box watermarking framework for code LMs that uses rule-based watermarks and a utility-preserving injection method specifically for user-level tracing. It also features a parameter selection algorithm tailored for robust watermarking and incorporates adversarial training to resist watermark removal attacks.

Result: CLMTracing is tested on multiple state-of-the-art code LMs and demonstrates effectiveness, robustness against watermark removal attacks, and superior performance compared to existing SOTA baselines with minimal impact on model utility.

Conclusion: CLMTracing provides a practical and robust solution for user-level tracing and IP protection in open-source code LMs within black-box scenarios, outperforming previous approaches in robustness and harmlessness.

Abstract: With the widespread adoption of open-source code language models (code LMs),
intellectual property (IP) protection has become an increasingly critical
concern. While current watermarking techniques have the potential to identify
the code LM to protect its IP, they have limitations when facing the more
practical and complex demand, i.e., offering the individual user-level tracing
in the black-box setting. This work presents CLMTracing, a black-box code LM
watermarking framework employing the rule-based watermarks and
utility-preserving injection method for user-level model tracing. CLMTracing
further incorporates a parameter selection algorithm sensitive to the robust
watermark and adversarial training to enhance the robustness against watermark
removal attacks. Comprehensive evaluations demonstrate CLMTracing is effective
across multiple state-of-the-art (SOTA) code LMs, showing significant harmless
improvements compared to existing SOTA baselines and strong robustness against
various removal attacks.

</details>


### [20] [Parallelizable Feynman-Kac Models for Universal Probabilistic Programming](https://arxiv.org/abs/2509.14092)
*Michele Boreale,Luisa Collodi*

Main category: cs.PL

TL;DR: The paper formalizes semantics and proves correctness for SMC inference in universal probabilistic programs, introduces a vectorized PF algorithm for these models, and shows strong empirical results compared to existing tools.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop provably correct and efficient Sequential Monte Carlo (SMC) inference techniques for universal probabilistic programs (PPs), particularly those with complex features like arbitrary measures and unbounded loops, which are challenging to analyze and scale.

Method: The paper introduces an expectation-based semantics for Probabilistic Program Graphs (PPGs) that incorporates infinite execution traces and trace weights. A finite approximation theorem is proven, and the semantics are framed within a Feynman-Kac model. Consistency of the Particle Filtering (PF) algorithm with this semantics is established. The authors also design a vectorized Particle Filtering algorithm (VPF) tailored to PPGs and validate it experimentally.

Result: The proposed VPF algorithm is shown through experiments to offer very promising performance compared to state-of-the-art probabilistic program inference tools, suggesting both correctness and efficiency.

Conclusion: This work formalizes the operational semantics for universal probabilistic programs, connects SMC and PF methods rigorously to these semantics, and introduces an efficient vectorized inference algorithm (VPF) with superior empirical results over existing methods.

Abstract: We study provably correct and efficient instantiations of Sequential Monte
Carlo (SMC) inference in the context of formal operational semantics of
Probabilistic Programs (PPs). We focus on universal PPs featuring sampling from
arbitrary measures and conditioning/reweighting in unbounded loops. We first
equip Probabilistic Program Graphs (PPGs), an automata-theoretic description
format of PPs, with an expectation-based semantics over infinite execution
traces, which also incorporates trace weights. We then prove a finite
approximation theorem that provides bounds to this semantics based on
expectations taken over finite, fixed-length traces. This enables us to frame
our semantics within a Feynman-Kac (FK) model, and ensures the consistency of
the Particle Filtering (PF) algorithm, an instance of SMC, with respect to our
semantics. Building on these results, we introduce VPF, a vectorized version of
the PF algorithm tailored to PPGs and our semantics. Experiments conducted with
a proof-of-concept implementation of VPF show very promising results compared
to state-of-the-art PP inference tools.

</details>
