<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce Formula Generators](https://arxiv.org/abs/2508.20340)
*Maolin Sun,Yibiao Yang,Yuming Zhou*

Main category: cs.SE

TL;DR: Chimera is a new LLM-assisted fuzzing tool for SMT solvers that generates syntactically valid and diverse test formulas more efficiently by using grammar-based term synthesis. It detected dozens of real bugs in Z3 and cvc5, with most quickly fixed, demonstrating practical impact.


<details>
  <summary>Details</summary>
Motivation: SMT solvers are essential for verification tasks in systems and programming languages, but their correctness depends on high-quality test formulas. Existing methods for generating those formulas fall short with new solver features, and LLM-based approaches suffer from syntactic errors and high computational costs.

Method: This paper presents Chimera, an LLM-assisted fuzzing framework. Instead of generating formulas directly via LLMs, Chimera uses LLMs to extract context-free grammars (CFGs) from documentation and synthesize reusable Boolean term generators. These generators ensure syntactic validity and composability, allowing Chimera to populate skeletons of formulas during fuzzing, with only a single LLM interaction required.

Result: Chimera was evaluated on the SMT solvers Z3 and cvc5. It successfully discovered 43 confirmed bugs, 40 of which have been fixed by developers, demonstrating improved efficacy and efficiency compared to prior approaches.

Conclusion: Chimera overcomes previous limitations of LLM-based testing by focusing on grammar-based term generation rather than direct formula synthesis, yielding syntactic validity, semantic diversity, and lower computational cost. Its use has led to the discovery and fixing of numerous bugs in popular SMT solvers.

Abstract: Satisfiability Modulo Theory (SMT) solvers are foundational to modern systems
and programming languages research, providing the foundation for tasks like
symbolic execution and automated verification. Because these solvers sit on the
critical path, their correctness is essential, and high-quality test formulas
are key to uncovering bugs. However, while prior testing techniques performed
well on earlier solver versions, they struggle to keep pace with rapidly
evolving features. Recent approaches based on Large Language Models (LLMs) show
promise in exploring advanced solver capabilities, but two obstacles remain:
nearly half of the generated formulas are syntactically invalid, and iterative
interactions with the LLMs introduce substantial computational overhead. In
this study, we present Chimera, a novel LLM-assisted fuzzing framework that
addresses both issues by shifting from direct formula generation to the
synthesis of reusable term (i.e., logical expression) generators. Particularly,
Chimera uses LLMs to (1) automatically extract context-free grammars (CFGs) for
SMT theories, including solver-specific extensions, from documentation, and (2)
synthesize composable Boolean term generators that adhere to these grammars.
During fuzzing, Chimera populates structural skeletons derived from existing
formulas with the terms iteratively produced by the LLM-synthesized generators.
This design ensures syntactic validity while promoting semantic diversity.
Notably, Chimera requires only one-time LLM interaction investment,
dramatically reducing runtime cost. We evaluated Chimera on two leading SMT
solvers: Z3 and cvc5. Our experiments show that Chimera has identified 43
confirmed bugs, 40 of which have already been fixed by developers.

</details>


### [2] [Evaluating LLMs on microservice-based applications: how complex is your specification?](https://arxiv.org/abs/2508.20119)
*Daniel M. Yellin*

Main category: cs.SE

TL;DR: LLMs can generate code for microservice applications of moderate complexity but struggle with more difficult, real-world tasks due to advanced requirements. The paper proposes a framework to benchmark and analyze these limitations, offering insights for further research.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are increasingly used for code generation, but their ability to handle real-world, complex tasks like microservice-based applications is unclear. The paper is motivated to assess LLM capability in this critical and practical coding domain.

Method: The paper defines a standard template for microservice-based applications, proposes a new metric to assess specification difficulty, and develops a framework to automatically test LLM-generated code using unit tests. It then conducts experiments on LLMs like GPT-3o-mini, analyzing performance at different difficulty levels and reporting specific error types.

Result: LLMs like GPT-3o-mini can adequately generate code for medium-difficulty microservice specifications, but their performance drops sharply as problem complexity increases due to challenges with advanced business logic, external services, database integration, and non-functional requirements.

Conclusion: While current strong LLMs show promise, significant limitations remain for complex, real-world code synthesis tasks. Understanding the sources of LLM errors can guide future research to improve their practical programming capabilities.

Abstract: In this paper we evaluate how far LLMs have advanced in generating code for
real-world problems. Specifically, we explore code synthesis for
microservice-based applications, a widely used architecture pattern. We define
a standard template for specifying these applications, and we propose a metric
for judging the difficulty level of a specification. The higher the score, the
more difficult it is to generate code for the specification. We develop a
framework to automate the process of testing LLM-synthesized code for a
microservice using unit tests. Our experimental results show that strong LLMs
(like GPT-3o-mini) do fairly well on medium difficulty specifications but do
very poorly on those of higher difficulty levels. This is due to more intricate
business logic, a greater use of external services, database integration and
inclusion of non-functional capabilities such as authentication. We analyzed
the errors in LLM-synthesized code and report on the key challenges LLMs face
in generating code for these specifications thereby suggesting future research
directions to improve code synthesis for real-world problems.

</details>


### [3] [Towards Better Correctness and Efficiency in Code Generation](https://arxiv.org/abs/2508.20124)
*Yunlong Feng,Yang Xu,Xiao Xu,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: The paper presents a two-stage RL-based tuning method that significantly improves both the accuracy and runtime efficiency of code generated by large language models, making smaller models perform comparably to larger ones.


<details>
  <summary>Details</summary>
Motivation: Code generation with large language models often produces inefficient code at runtime, which is problematic for performance-sensitive applications. Existing methods based on offline fine-tuning are limited by static data and cannot explore more efficient implementations.

Method: The authors introduce an efficiency-oriented reinforcement learning framework with a novel performance reward. Key contributions include dynamic exploration to surpass static data constraints, an error-insensitive RL approach aided by high-contrast efficiency signals, and a two-stage tuning process starting from a highly correct baseline to further optimize for efficiency.

Result: The proposed method improves code correctness by 10.18% and runtime efficiency by 7.75% on a 7B model, yielding results comparable to much larger models.

Conclusion: The efficiency-oriented RL framework effectively enhances both correctness and runtime efficiency of code generated by large language models, offering a practical approach for deploying code LLMs in performance-sensitive settings.

Abstract: While code large language models have demonstrated remarkable progress in
code generation, the generated code often exhibits poor runtime efficiency,
limiting its practical application in performance-sensitive scenarios. To
address this limitation, we propose an efficiency-oriented reinforcement
learning framework guided by a novel performance reward. Based on this
framework, we take a deeper dive into the code efficiency problem, identifying
then proposing methods to overcome key bottlenecks: (1) Dynamic exploration
overcomes the static data constraints of offline fine-tuning, enabling the
discovery of more efficient code implementations. (2) The error-insensitive
reinforcement learning method and high-contrast efficiency signals are crucial
for mitigating systematic errors and achieving effective optimization. (3)
Online exploration is most effective when starting from a high-correctness
baseline, as this allows for efficiency improvements without sacrificing
accuracy. With these discoveries, we finally propose a two-stage tuning method,
which achieves high and balanced performance across correctness and efficiency.
The results of experiments show the effectiveness of the method, which improves
code correctness by 10.18\% and runtime efficiency by 7.75\% on a 7B model,
achieving performance comparable to much larger model.

</details>


### [4] [Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought](https://arxiv.org/abs/2508.20370)
*Lingzhe Zhang,Tong Jia,Kangjin Wang,Weijie Hong,Chiming Duan,Minghua He,Ying Li*

Main category: cs.SE

TL;DR: This paper studies how SREs identify root causes of failures in microservice systems and introduces RCLAgent, an adaptive, interpretable framework using multi-agent LLM reasoning. RCLAgent surpasses prior methods in accuracy and efficiency by pinpointing root causes from just one request.


<details>
  <summary>Details</summary>
Motivation: Modern microservice architectures are highly complex and experience frequent failures. Existing root cause localization methods are either rigid due to reliance on pre-defined schemas or are hard for SREs to interpret, leaving room for improvement in adaptability and explainability.

Method: The authors conducted a comprehensive study with professional SREs to understand their root cause analysis process, identifying its key characteristics: recursiveness, multi-dimensional expansion, and cross-modal reasoning. Based on these findings, they propose RCLAgent, an adaptive root cause localization method that uses a multi-agent recursion-of-thought framework, leveraging LLMs, multi-agent collaboration, and tool-assisted analyses.

Result: Experimental results on public datasets show that RCLAgent outperforms current state-of-the-art methods by successfully locating root causes from a single request, unlike others that require multiple requests. This demonstrates greater efficiency and accuracy in complex microservice environments.

Conclusion: RCLAgent delivers more effective and precise root cause localization for large-scale microservice systems, increasing both efficiency and interpretability thanks to its adaptive, multi-agent recursion-of-thought approach.

Abstract: As contemporary microservice systems become increasingly popular and
complex-often comprising hundreds or even thousands of fine-grained,
interdependent subsystems-they are facing more frequent failures. Ensuring
system reliability thus demands accurate root cause localization. While traces
and metrics have proven to be effective data sources for this task, existing
methods either heavily rely on pre-defined schemas, which struggle to adapt to
evolving operational contexts, or lack interpretability in their reasoning
process, thereby leaving Site Reliability Engineers (SREs) confused. In this
paper, we conduct a comprehensive study on how SREs localize the root cause of
failures, drawing insights from multiple professional SREs across different
organizations. Our investigation reveals that human root cause analysis
exhibits three key characteristics: recursiveness, multi-dimensional expansion,
and cross-modal reasoning. Motivated by these findings, we introduce RCLAgent,
an adaptive root cause localization method for microservice systems that
leverages a multi-agent recursion-of-thought framework. RCLAgent employs a
novel recursion-of-thought strategy to guide the LLM's reasoning process,
effectively integrating data from multiple agents and tool-assisted analysis to
accurately pinpoint the root cause. Experimental evaluations on various public
datasets demonstrate that RCLAgent achieves superior performance by localizing
the root cause using only a single request-outperforming state-of-the-art
methods that depend on aggregating multiple requests. These results underscore
the effectiveness of RCLAgent in enhancing the efficiency and precision of root
cause localization in complex microservice environments.

</details>


### [5] [AI and Agile Software Development: A Research Roadmap from the XP2025 Workshop](https://arxiv.org/abs/2508.20563)
*Zheying Zhang,Tomas Herda,Victoria Pichler,Pekka Abrahamsson,Geir K. Hanssen,Joshua Kerievsky,Alex Polyakov,Mohit Chandna,Marius Irgens,Kai-Kristian Kemell,Ayman Asad Khan,Crystal Kwok,Evan Leybourn,Munish Malik,Dorota Mleczko,Morteza Moalagh,Christopher Morales,Yuliia Pieskova,Daniel Planötscher,Mika Saari,Anastasiia Tkalich,Karl Josef Gstettner,Xiaofeng Wang*

Main category: cs.SE

TL;DR: A workshop with academics and practitioners discussed challenges and opportunities of using Generative AI in agile development. Key barriers like tool fragmentation and lack of skills were mapped, and a research roadmap was co-created to guide future work towards responsibly using GenAI in agile practices.


<details>
  <summary>Details</summary>
Motivation: To address the challenges and opportunities arising from the integration of Generative AI (GenAI) into agile software development processes, where existing pain points are poorly understood.

Method: A full-day interdisciplinary workshop was conducted with over 30 academic and industry participants, featuring structured and interactive breakout sessions to identify, analyze, and prioritize issues at the intersection of GenAI and agile development.

Result: Key shared challenges such as tool fragmentation, governance, data quality, AI literacy, and prompt engineering skills gaps were identified, analyzed, and synthesized into a collaboratively developed, multi-thematic research roadmap outlining both short-term actionable steps and long-term strategic goals.

Conclusion: The paper presents a unified research agenda designed to support the responsible and effective integration of GenAI in agile development, providing guidance for both immediate and future research efforts.

Abstract: This paper synthesizes the key findings from a full-day XP2025 workshop on
"AI and Agile: From Frustration to Success", held in Brugg-Windisch,
Switzerland. The workshop brought together over 30 interdisciplinary academic
researchers and industry practitioners to tackle the concrete challenges and
emerging opportunities at the intersection of Generative Artificial
Intelligence (GenAI) and agile software development. Through structured,
interactive breakout sessions, participants identified shared pain points like
tool fragmentation, governance, data quality, and critical skills gaps in AI
literacy and prompt engineering. These issues were further analyzed, revealing
underlying causes and cross-cutting concerns. The workshop concluded by
collaboratively co-creating a multi-thematic research roadmap, articulating
both short-term, implementable actions and visionary, long-term research
directions. This cohesive agenda aims to guide future investigation and drive
the responsible, human-centered integration of GenAI into agile practices.

</details>


### [6] [Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol](https://arxiv.org/abs/2508.20737)
*Wei Ma,Yixiao Yang,Qiang Hu,Shi Ying,Zhi Jin,Bo Du,Zhenchang Xing,Tianlin Li,Junjie Shi,Yang Liu,Linxiao Jiang*

Main category: cs.SE

TL;DR: LLM apps need new quality assurance approaches due to their complexity. This paper proposes a three-layer architecture, compares testing frameworks from software and AI fields, outlines key challenges, and introduces the AICL protocol—a standardized communication language for agent-based testing. Practical strategies and protocols for reliable LLM testing are presented.


<details>
  <summary>Details</summary>
Motivation: LLM applications have become complex, integrating retrieval, tool use, and multi-turn interactions, making traditional quality assurance methods insufficient. Ensuring reliable testing and assurance for these dynamic and context-dependent systems is increasingly urgent.

Method: The paper decomposes LLM applications into three layers: System Shell, Prompt Orchestration, and LLM Inference Core. It analyzes how traditional and AI-based software testing methods apply to each layer, compares approaches from software engineering and AI safety, identifies challenges, and proposes collaborative strategies. It introduces a standardized protocol (AICL) for agent communication focused on testing and integration.

Result: The analysis reveals four fundamental differences between software engineering and AI safety testing, leading to six key challenges in LLM application assurance. Four collaborative strategies (Retain, Translate, Integrate, Runtime) and a closed-loop assurance framework (combining pre-deployment and runtime testing) are proposed. The AICL protocol is presented as a practical solution for standardization and tooling.

Conclusion: Traditional software testing methods alone are inadequate for LLM applications due to their unique characteristics. Combining insights from software engineering and AI safety, and utilizing a specialized agent communication protocol like AICL, can facilitate trustworthy and efficient quality assurance for these systems. The paper sets actionable guidance for standardizing and tooling LLM application testing.

Abstract: Applications of Large Language Models~(LLMs) have evolved from simple text
generators into complex software systems that integrate retrieval augmentation,
tool invocation, and multi-turn interactions. Their inherent non-determinism,
dynamism, and context dependence pose fundamental challenges for quality
assurance. This paper decomposes LLM applications into a three-layer
architecture: \textbf{\textit{System Shell Layer}}, \textbf{\textit{Prompt
Orchestration Layer}}, and \textbf{\textit{LLM Inference Core}}. We then assess
the applicability of traditional software testing methods in each layer:
directly applicable at the shell layer, requiring semantic reinterpretation at
the orchestration layer, and necessitating paradigm shifts at the inference
core. A comparative analysis of Testing AI methods from the software
engineering community and safety analysis techniques from the AI community
reveals structural disconnects in testing unit abstraction, evaluation metrics,
and lifecycle management. We identify four fundamental differences that
underlie 6 core challenges. To address these, we propose four types of
collaborative strategies (\emph{Retain}, \emph{Translate}, \emph{Integrate},
and \emph{Runtime}) and explore a closed-loop, trustworthy quality assurance
framework that combines pre-deployment validation with runtime monitoring.
Based on these strategies, we offer practical guidance and a protocol proposal
to support the standardization and tooling of LLM application testing. We
propose a protocol \textbf{\textit{Agent Interaction Communication Language}}
(AICL) that is used to communicate between AI agents. AICL has the
test-oriented features and is easily integrated in the current agent framework.

</details>


### [7] [From Law to Gherkin: A Human-Centred Quasi-Experiment on the Quality of LLM-Generated Behavioural Specifications from Food-Safety Regulations](https://arxiv.org/abs/2508.20744)
*Shabnam Hassani,Mehrdad Sabetzadeh,Daniel Amyot*

Main category: cs.SE

TL;DR: This paper shows that Large Language Models can accurately and efficiently generate software requirements from legal texts, streamlining compliance work for developers, with little difference between the tested models.


<details>
  <summary>Details</summary>
Motivation: Legal requirements increasingly shape how software is built, but translating complex, technology-neutral legal language into actionable development artifacts is difficult and usually demands significant expert effort. Automating this process could save time and improve compliance.

Method: The authors conducted a systematic quasi-experimental study involving ten human participants. They evaluated Gherkin behavioral specifications generated by two popular Large Language Models (Claude and Llama) from food-safety legal texts. A total of 60 specifications were assessed across five quality criteria, with each reviewed by two participants to ensure reliability.

Result: The generated specifications were rated highly: over 68% of the assessments gave the highest possible score across criteria such as Relevance, Clarity, Completeness, Singularity, and Time Savings. Llama slightly outperformed Claude on most criteria, except for Singularity where Claude led. No lowest ratings occurred, and statistical analysis found no significant differences across participants or models. Some issues with hallucinations and omissions were noted, but overall, the generated specifications were found useful.

Conclusion: Large Language Models can reliably and efficiently generate high-quality, developer-friendly behavioral specifications from legal texts. This automation reduces manual workload and supports structured software compliance, assurance, and testing processes.

Abstract: Context: Laws and regulations increasingly affect software design and quality
assurance, but legal texts are written in technology-neutral language. This
creates challenges for engineers who must develop compliance artifacts such as
requirements and acceptance criteria. Manual creation is labor-intensive,
error-prone, and requires domain expertise. Advances in Generative AI (GenAI),
especially Large Language Models (LLMs), offer a way to automate deriving such
artifacts.
  Objective: We present the first systematic human-subject study of LLMs'
ability to derive behavioral specifications from legal texts using a
quasi-experimental design. These specifications translate legal requirements
into a developer-friendly form.
  Methods: Ten participants evaluated specifications generated from food-safety
regulations by Claude and Llama. Using Gherkin, a structured BDD language, 60
specifications were produced. Each participant assessed 12 across five
criteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Each
specification was reviewed by two participants, yielding 120 assessments.
  Results: For Relevance, 75% of ratings were highest and 20% second-highest.
Clarity reached 90% highest. Completeness: 75% highest, 19% second.
Singularity: 82% highest, 12% second. Time Savings: 68% highest, 24% second. No
lowest ratings occurred. Mann-Whitney U tests showed no significant differences
across participants or models. Llama slightly outperformed Claude in Clarity,
Completeness, and Time Savings, while Claude was stronger in Singularity.
Feedback noted hallucinations and omissions but confirmed the utility of the
specifications.
  Conclusion: LLMs can generate high-quality Gherkin specifications from legal
texts, reducing manual effort and providing structured artifacts useful for
implementation, assurance, and test generation.

</details>


### [8] [Towards an Architectural Perspective for Sustainability: Bundle the Needs from Industry](https://arxiv.org/abs/2508.20774)
*Markus Funke,Patricia Lago*

Main category: cs.SE

TL;DR: The paper proposes a structured approach for architects to integrate sustainability into software design using a revised architectural perspective, validated through literature review and expert input.


<details>
  <summary>Details</summary>
Motivation: Sustainability is becoming a critical quality property in software-intensive systems, but there is a lack of structured guidance for architects to effectively address sustainability during the software design phase.

Method: The paper introduces the concept of a 'sustainability perspective' for software architecture. This is developed using evidence gathered from snowballing seminal literature and conducting a focus group with domain experts.

Result: The findings validate the relevance of different architectural perspective elements and indicate how a sustainability perspective can be shaped to address real-world industrial requirements.

Conclusion: Architectural perspectives can be effectively adapted to provide structured guidance for incorporating sustainability into software architectures, and the proposed vision aligns well with practical needs in industry.

Abstract: Sustainability is increasingly recognized as an emerging quality property in
software-intensive systems, yet architects lack structured guidance to address
it effectively throughout the software design phase. Architectural
perspectives-an architectural knowledge artifact composed of concerns,
activities, tactics, pitfalls, and checklists-offer a promising approach to
tackle such emerging quality properties across architectural views and are also
independent of architecture frameworks and industry contexts. In this paper, we
present a sustainability perspective vision, i.e., a revised notion of
architectural perspective meant to be filled with its own elements to target
sustainability concerns. We formulate our sustainability perspective vision
through evidence from applying snowballing to seminal literature and from
conducting a focus group with experts in the field. Our findings confirm the
relevance of the different perspective elements in practice and highlight
implications for shaping a sustainability perspective that meets industrial
needs.

</details>


### [9] [Automated Test Oracles for Flaky Cyber-Physical System Simulators: Approach and Evaluation](https://arxiv.org/abs/2508.20902)
*Baharin A. Jodat,Khouloud Gaaloul,Mehrdad Sabetzadeh,Shiva Nejati*

Main category: cs.SE

TL;DR: Replacing expensive and unreliable CPS simulation-based testing, this paper introduces assertion-based test oracles created via genetic programming and decision models, finding that GP with Ochiai is the most accurate and robust method, especially in flaky simulator situations.


<details>
  <summary>Details</summary>
Motivation: Simulation-based testing for cyber-physical systems (CPS) is resource-intensive due to slow simulation runtimes and inconsistent outcomes from flaky CPS simulators. There is a need for robust, interpretable, and non-execution-based automated test oracles to reduce cost and ensure reliability.

Method: The paper proposes assertion-based test oracles, which are sets of logical and arithmetic predicates applied to system inputs. Two generation methods are introduced: (1) genetic programming (GP) using spectrum-based fault localization (SBFL) ranking formulas like Ochiai, Tarantula, and Naish as fitness functions, and (2) decision trees (DT) and decision rules (DR).

Result: GP with Ochiai ranking formula produces the most accurate test oracles, outperforming DT/DR and other GP variants (Tarantula, Naish). This holds true even in the presence of simulator flakiness. The oracles generated by GP with Ochiai have only 4% average accuracy variation in flaky conditions across four diverse CPS domains.

Conclusion: Assertion-based test oracles generated by GP with Ochiai provide highly accurate and robust predictions for CPS testing, reducing reliance on costly and inconsistent simulations.

Abstract: Simulation-based testing of cyber-physical systems (CPS) is costly due to the
time-consuming execution of CPS simulators. In addition, CPS simulators may be
flaky, leading to inconsistent test outcomes and requiring repeated test
re-execution for reliable test verdicts. Automated test oracles that do not
require system execution are therefore crucial for reducing testing costs.
Ideally, such test oracles should be interpretable to facilitate human
understanding of test verdicts, and they must be robust against the potential
flakiness of CPS simulators. In this article, we propose assertion-based test
oracles for CPS as sets of logical and arithmetic predicates defined over the
inputs of the system under test. Given a test input, our assertion-based test
oracle determines, without requiring test execution, whether the test passes,
fails, or if the oracle is inconclusive in predicting a verdict. We describe
two methods for generating assertion-based test oracles: one using genetic
programming~(GP) that employs well-known spectrum-based fault localization
(SBFL) ranking formulas, namely Ochiai, Tarantula, and Naish, as fitness
functions; and the other using decision trees (DT) and decision rules (DR). We
evaluate our assertion-based test oracles through case studies in the domains
of aerospace, networking and autonomous driving. We show that test oracles
generated using GP with Ochiai are significantly more accurate than those
obtained using GP with Tarantula and Naish or using DT or DR. Moreover, this
accuracy advantage remains even when accounting for the flakiness of the system
under test. We further show that the assertion-based test oracles generated by
GP with Ochiai are robust against flakiness with only 4% average variation in
their accuracy results across four different network and autonomous driving
systems with flaky behaviours.

</details>


### [10] [Deep Learning Based Concurrency Bug Detection and Localization](https://arxiv.org/abs/2508.20911)
*Zuocheng Feng,Kaiwen Zhang,Miaomiao Wang,Yiming Cheng,Yuandao Cai,Xiaofeng Li,Guanjun Liu*

Main category: cs.SE

TL;DR: The paper introduces a novel approach for detecting and localizing concurrency bugs, combining a new graph-based representation and a specialized dataset to greatly improve both bug detection and localization over prior techniques.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for concurrency bug detection suffer from a lack of diverse datasets, insufficient modeling of concurrency semantics, and coarse binary classification results that do not help with precise debugging.

Method: The authors build a specialized concurrency bug dataset, utilize a pre-trained model combined with a heterogeneous Graph Neural Network (GNN), and introduce the Concurrency-Aware Code Property Graph (CCPG) to better capture concurrency semantics. SubgraphX is also leveraged for fine-grained bug localization.

Result: The new approach achieves an average increase of 10% in both accuracy and precision, and 26% in recall, compared to leading existing methods on various benchmarks.

Conclusion: The proposed method outperforms state-of-the-art approaches in both detecting and localizing concurrency bugs, achieving significant improvements in accuracy, precision, and recall.

Abstract: Concurrency bugs, caused by improper synchronization of shared resources in
multi-threaded or distributed systems, are notoriously hard to detect and thus
compromise software reliability and security. The existing deep learning
methods face three main limitations. First, there is an absence of large and
dedicated datasets of diverse concurrency bugs for them. Second, they lack
sufficient representation of concurrency semantics. Third, binary
classification results fail to provide finer-grained debug information such as
precise bug lines. To address these problems, we propose a novel method for
effective concurrency bug detection as well as localization. We construct a
dedicated concurrency bug dataset to facilitate model training and evaluation.
We then integrate a pre-trained model with a heterogeneous graph neural network
(GNN), by incorporating a new Concurrency-Aware Code Property Graph (CCPG) that
concisely and effectively characterizes concurrency semantics. To further
facilitate debugging, we employ SubgraphX, a GNN-based interpretability method,
which explores the graphs to precisely localize concurrency bugs, mapping them
to specific lines of source code. On average, our method demonstrates an
improvement of 10\% in accuracy and precision and 26\% in recall compared to
state-of-the-art methods across diverse evaluation settings.

</details>


### [11] [ConfLogger: Enhance Systems' Configuration Diagnosability through Configuration Logging](https://arxiv.org/abs/2508.20977)
*Shiwen Shan,Yintong Huo,Yuxin Su,Zhining Wang,Dan Li,Zibin Zheng*

Main category: cs.SE

TL;DR: This paper proposes ConfLogger, a tool combining static taint analysis and LLMs, to automatically generate configuration-focused logs in software. On eight systems and in user studies, ConfLogger greatly improved error diagnosability, logging coverage, and troubleshooting accuracy compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Modern configurable systems are highly customizable, but this flexibility leads to complex configuration-related bugs and issues. Traditional diagnosability methods focus on analyzing failures after they occur, but do not assess whether logs contain enough information to aid in diagnosis. This paper addresses the gap by proposing methods to proactively improve configuration-related logging to support diagnosis.

Method: The authors introduce ConfLogger, a tool that combines configuration-aware static taint analysis with large language model (LLM)-based log generation. ConfLogger traces configuration data flows throughout the project to identify configuration-sensitive code segments and then generates diagnostic log statements based on the gathered context.

Result: ConfLogger was evaluated on eight popular software systems. Enhanced logs enabled a misconfiguration diagnosis tool to achieve 100% accuracy in localizing errors in 30 silent misconfiguration scenarios, and 80% of these were directly resolvable due to explicit configuration information. ConfLogger covered 74% of existing logging points, outperforming baseline LLM loggers by 12%-30%, and showed superior variable logging precision (+8.6%), recall (+79.3%), and F1 score (+26.2%). A user study demonstrated a 1.25x speedup in diagnostics and 251.4% improved troubleshooting accuracy.

Conclusion: ConfLogger significantly enhances diagnosability in configurable systems by proactively generating configuration-aware logs, outperforming state-of-the-art logging approaches in both automated and human troubleshooting scenarios.

Abstract: Modern configurable systems offer customization via intricate configuration
spaces, yet such flexibility introduces pervasive configuration-related issues
such as misconfigurations and latent softwarebugs. Existing diagnosability
supports focus on post-failure analysis of software behavior to identify
configuration issues, but none of these approaches look into whether the
software clue sufficient failure information for diagnosis. To fill in the
blank, we propose the idea of configuration logging to enhance existing logging
practices at the source code level. We develop ConfLogger, the first tool that
unifies configuration-aware static taint analysis with LLM-based log generation
to enhance software configuration diagnosability. Specifically, our method 1)
identifies configuration-sensitive code segments by tracing
configuration-related data flow in the whole project, and 2) generates
diagnostic log statements by analyzing configuration code contexts. Evaluation
results on eight popular software systems demonstrate the effectiveness of
ConfLogger to enhance configuration diagnosability. Specifically,
ConfLogger-enhanced logs successfully aid a log-based misconfiguration
diagnosis tool to achieve 100% accuracy on error localization in 30 silent
misconfiguration scenarios, with 80% directly resolvable through explicit
configuration information exposed. In addition, ConfLogger achieves 74%
coverage of existing logging points, outperforming baseline LLM-based loggers
by 12% and 30%. It also gains 8.6% higher in precision, 79.3% higher in recall,
and 26.2% higher in F1 compared to the state-of-the-art baseline in terms of
variable logging while also augmenting diagnostic value. A controlled user
study on 22 cases further validated its utility, speeding up diagnostic time by
1.25x and improving troubleshooting accuracy by 251.4%.

</details>


### [12] [Dynamics of Gender Bias in Software Engineering](https://arxiv.org/abs/2508.21050)
*Thomas J. Misa*

Main category: cs.SE

TL;DR: This paper highlights the ongoing gender bias within software engineering, analyzing historical trends and author participation at a major conference from 1976–2010, and urges new policy directions to combat bias in research.


<details>
  <summary>Details</summary>
Motivation: The paper aims to investigate the presence and impact of gender bias in the traditionally male-dominated fields of engineering and computer science, focusing specifically on software engineering.

Method: The study first surveys the historical background and professionalism within software engineering, profiling five prominent leaders. It then examines the field’s attention to gender-related issues and conducts a quantitative analysis of female participation as research authors in the International Conference of Software Engineering from 1976 to 2010.

Result: The analysis reveals statistically significant periods—specifically a dozen years—where there was notable gender exclusion of women as research authors within the conference.

Conclusion: There is a persistent gender bias in software engineering, especially evident in research authorship. The paper recommends further policy-driven research to address gender bias in computing.

Abstract: The field of software engineering is embedded in both engineering and
computer science, and may embody gender biases endemic to both. This paper
surveys software engineering's origins and its long-running attention to
engineering professionalism, profiling five leaders; it then examines the
field's recent attention to gender issues and gender bias. It next
quantitatively analyzes women's participation as research authors in the
field's leading International Conference of Software Engineering (1976-2010),
finding a dozen years with statistically significant gender exclusion. Policy
dimensions of research on gender bias in computing are suggested.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [13] [Solvable Tuple Patterns and Their Applications to Program Verification](https://arxiv.org/abs/2508.20365)
*Naoki Kobayashi,Ryosuke Sato,Ayumi Shinohara,Ryo Yoshinaka*

Main category: cs.PL

TL;DR: This paper introduces a novel invariant inference technique for programs manipulating list-like recursive data structures. By leveraging solvable tuple patterns, it enables efficient invariant discovery with only positive examples, and strengthens automated verification tools, demonstrated by a significant win in the CHC-COMP 2025 competition.


<details>
  <summary>Details</summary>
Motivation: Automated verification of programs handling recursive data structures is difficult and current solutions are still limited, especially for complex invariants in list-like structures.

Method: The authors introduce solvable tuple patterns (STPs) as a new way to express invariants for list-like recursive data structures. They present an inference algorithm for STPs which only needs positive samples, and connect this method to SMT (Satisfiability Modulo Theories) solvers that use sequence theory. They further integrate the inference approach into a CHC solver supporting list-like structures.

Result: The method enables efficient and robust inference of invariants without requiring negative samples. Integration with a CHC solver shows superior performance and contributed to winning the ADT-LIN category of CHC-COMP 2025.

Conclusion: STPs provide an effective approach to inferring invariants in recursive data structures, improving the capabilities of program verification tools and outperforming prior systems.

Abstract: Despite the recent progress of automated program verification techniques,
fully automated verification of programs manipulating recursive data structures
remains a challenge. We introduce the notion of solvable tuple patterns (STPs)
to express invariants between list-like recursive data structures. A
distinguishing feature of STPs is that they can be efficiently inferred from
only a small number of positive samples; no negative samples are required. An
SMT solver that supports the sequence theory can be used to check that an
inferred STP is indeed an inductive invariant. After presenting basic
properties of STPs and an STP inference algorithm, we show how to incorporate
the STP inference into a CHC (Constrained Horn Clauses) solver supporting
list-like data structures, which serves as a uniform backend for automated
program verification tools. A CHC solver incorporating the STP inference has
won the ADT-LIN category of CHC-COMP 2025 by a big margin.

</details>


### [14] [Static Factorisation of Probabilistic Programs With User-Labelled Sample Statements and While Loops](https://arxiv.org/abs/2508.20922)
*Markus Böck,Jürgen Cito*

Main category: cs.PL

TL;DR: This paper shows that complex probabilistic programs with loops can be represented and analyzed graphically, extending Bayesian network concepts. The approach enables correct, empirically verified optimizations for inference tasks, matching or surpassing current methods.


<details>
  <summary>Details</summary>
Motivation: There is a clear need to understand whether complex probabilistic programs—with features like user-labelled sample statements and while loops—can have an interpretable graphical representation similar to Bayesian networks. Existing solutions do not cover dynamic program constructs, leaving open questions on their analysis and optimization.

Method: The authors extend operational semantics to accommodate user-labelled sample statements and while loops, then translate probabilistic programs into control-flow graphs. With this, they perform static analysis to approximate dependencies among random variables, and introduce a novel static factorization technique. They also develop a sound program slicing method for leveraging this structure in inference optimization.

Result: The static factorization coincides with classic Bayesian network structures for loop-less, constant-labelled programs, but introduces a new graphical representation for more dynamic programs. The slicing technique enables three proven optimizations: reduced variance in gradient estimates for variational inference, and improved efficiency in single-site Metropolis Hastings and sequential Monte Carlo algorithms. These optimizations are both theoretically validated and empirically shown to be effective.

Conclusion: Probabilistic programs with loops and dynamic structure can be analyzed graphically via sound static analysis utilizing control-flow graphs. This enables the application of classic and novel inference optimizations, making complex probabilistic programs more understandable and efficient to execute.

Abstract: It is commonly known that any Bayesian network can be implemented as a
probabilistic program, but the reverse direction is not so clear. In this work,
we address the open question to what extent a probabilistic program with
user-labelled sample statements and while loops - features found in languages
like Gen, Turing, and Pyro - can be represented graphically. To this end, we
extend existing operational semantics to support these language features. By
translating a program to its control-flow graph, we define a sound static
analysis that approximates the dependency structure of the random variables in
the program. As a result, we obtain a static factorisation of the implicitly
defined program density, which is equivalent to the known Bayesian network
factorisation for programs without loops and constant labels, but constitutes a
novel graphical representation for programs that define an unbounded number of
random variables via loops or dynamic labels. We further develop a sound
program slicing technique to leverage this structure to statically enable three
well-known optimisations for the considered program class: we reduce the
variance of gradient estimates in variational inference and we speed up both
single-site Metropolis Hastings and sequential Monte Carlo. These optimisations
are proven correct and empirically shown to match or outperform existing
techniques.

</details>
