{"id": "2510.01379", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01379", "abs": "https://arxiv.org/abs/2510.01379", "authors": ["Huashan Chen", "Zhenyu Qi", "Haotang Li", "Hong Chen", "Jinfu Chen", "Kebin Peng", "In Kee Kim", "Kyu Hyung Lee", "Sen He"], "title": "Beyond Single LLMs: Enhanced Code Generation via Multi-Stage Performance-Guided LLM Orchestration", "comment": null, "summary": "While Large Language Models (LLMs) have become the predominant paradigm for\nautomated code generation, current single-model approaches fundamentally ignore\nthe heterogeneous computational strengths that different models exhibit across\nprogramming languages, algorithmic domains, and development stages. This paper\nchallenges the single-model convention by introducing a multi-stage,\nperformance-guided orchestration framework that dynamically routes coding tasks\nto the most suitable LLMs within a structured generate-fix-refine workflow. Our\napproach is grounded in a comprehensive empirical study of 17 state-of-the-art\nLLMs across five programming languages (Python, Java, C++, Go, and Rust) using\nHumanEval-X benchmark. The study, which evaluates both functional correctness\nand runtime performance metrics (execution time, mean/max memory utilization,\nand CPU efficiency), reveals pronounced performance heterogeneity by language,\ndevelopment stage, and problem category. Guided by these empirical insights, we\npresent PerfOrch, an LLM agent that orchestrates top-performing LLMs for each\ntask context through stage-wise validation and rollback mechanisms. Without\nrequiring model fine-tuning, PerfOrch achieves substantial improvements over\nstrong single-model baselines: average correctness rates of 96.22% and 91.37%\non HumanEval-X and EffiBench-X respectively, surpassing GPT-4o's 78.66% and\n49.11%. Beyond correctness gains, the framework delivers consistent performance\noptimizations, improving execution time for 58.76% of problems with median\nspeedups ranging from 17.67% to 27.66% across languages on two benchmarks. The\nframework's plug-and-play architecture ensures practical scalability, allowing\nnew LLMs to be profiled and integrated seamlessly, thereby offering a paradigm\nfor production-grade automated software engineering that adapts to the rapidly\nevolving generative AI landscape.", "AI": {"tldr": "Instead of relying on one LLM for code generation, dynamically routing tasks to the best-suited LLMs in a structured workflow delivers higher correctness and performance. PerfOrch achieves substantial accuracy and efficiency gains without fine-tuning, and its modular design enables easy integration of new models.", "motivation": "Existing approaches to automated code generation rely on using a single LLM model, which ignores the diverse strengths of different models across various programming languages, algorithmic domains, and development stages.", "method": "The paper conducts an extensive empirical study of 17 state-of-the-art LLMs across 5 programming languages using the HumanEval-X benchmark, measuring functional correctness and runtime performance (execution time, memory utilization, CPU efficiency). Guided by these insights, they introduce PerfOrch, a multi-stage orchestration framework that dynamically directs tasks to the most suitable LLM in a generate-fix-refine workflow, featuring stage-wise validation and rollback.", "result": "PerfOrch substantially outperforms single-model baselines, with average correctness rates of 96.22% and 91.37% on HumanEval-X and EffiBench-X (vs. GPT-4o's 78.66% and 49.11%). The framework optimizes execution time for 58.76% of problems, with median speedups between 17.67% and 27.66%. Its plug-and-play architecture allows easy integration of new models.", "conclusion": "A multi-model, performance-guided orchestration framework for code generation offers significant improvements in correctness and execution efficiency over single-model approaches, and its scalable architecture supports the rapid evolution of generative AI for automated software engineering."}}
{"id": "2510.01514", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01514", "abs": "https://arxiv.org/abs/2510.01514", "authors": ["J. Alexander Curtis", "Sharadha Kasiviswanathan", "Nasir Eisty"], "title": "Deciphering WONTFIX: A Mixed-Method Study on Why GitHub Issues Get Rejected", "comment": null, "summary": "Context: The ``wontfix'' label is a widely used yet narrowly understood tool\nin GitHub repositories, indicating that an issue will not be pursued further.\nDespite its prevalence, the impact of this label on project management and\ncommunity dynamics within open-source software development is not clearly\ndefined. Objective: This study examines the prevalence and reasons behind\nissues being labeled as wontfix across various open-source repositories on\nGitHub. Method: Employing a mixed-method approach, we analyze both quantitative\ndata to assess the prevalence of the wontfix label and qualitative data to\nexplore the reasoning that it was used. Data were collected from 3,132 of\nGitHub's most-popular repositories. Later, we employ open coding and thematic\nanalysis to categorize the reasons behind wontfix labels, providing a\nstructured understanding of the issue management landscape. Results: Our\nfindings show that about 30% of projects on GitHub apply the wontfix label to\nsome issues. These issues most often occur on user-submitted issues for bug\nreports and feature requests. The study identified eight common themes behind\nlabeling issues as wontfix, ranging from user-specific control factors to\nmaintainer-specific decisions. Conclusions: The wontfix label is a critical\ntool for managing resources and guiding contributor efforts in GitHub projects.\nHowever, it can also discourage community involvement and obscure the\ntransparency of project management. Understanding these reasons aids project\nmanagers in making informed decisions and fostering efficient collaboration\nwithin open-source communities.", "AI": {"tldr": "The study analyzes the prevalence and motivations of the 'wontfix' label in GitHub repositories, finding it's widely used to manage issues, but may discourage contributors if not communicated transparently.", "motivation": "The use of the 'wontfix' label on GitHub is widespread, but its implications for project management and community dynamics in open-source software are not well understood. This study aims to clarify the prevalence and motivations behind applying this label.", "method": "The paper uses a mixed-method approach, combining quantitative analysis of 3,132 popular GitHub repositories to measure the prevalence of the wontfix label, and qualitative techniques like open coding and thematic analysis to understand the reasons for applying it.", "result": "About 30% of the analyzed GitHub projects used the wontfix label on some issues, with the majority being user-submitted bug reports and feature requests. Eight common themes were identified as motivations for the wontfix label, related to both user factors and maintainer decisions.", "conclusion": "The wontfix label is an important tool for prioritizing resources and guiding contribution efforts, but it can also discourage participation and reduce transparency in project management. Clear understanding and communication of the reasoning behind the label can help project leaders foster better collaboration."}}
{"id": "2510.01635", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01635", "abs": "https://arxiv.org/abs/2510.01635", "authors": ["Yifei Chen", "Sarra Habchi", "Lili Wei"], "title": "MIMIC: Integrating Diverse Personality Traits for Better Game Testing Using Large Language Model", "comment": "13 pages, 7 figures, 6 tables. This paper is accepted by the 40th\n  IEEE/ACM International Conference on Automated Software Engineering, ASE 2025", "summary": "Modern video games pose significant challenges for traditional automated\ntesting algorithms, yet intensive testing is crucial to ensure game quality. To\naddress these challenges, researchers designed gaming agents using\nReinforcement Learning, Imitation Learning, or Large Language Models. However,\nthese agents often neglect the diverse strategies employed by human players due\nto their different personalities, resulting in repetitive solutions in similar\nsituations. Without mimicking varied gaming strategies, these agents struggle\nto trigger diverse in-game interactions or uncover edge cases.\n  In this paper, we present MIMIC, a novel framework that integrates diverse\npersonality traits into gaming agents, enabling them to adopt different gaming\nstrategies for similar situations. By mimicking different playstyles, MIMIC can\nachieve higher test coverage and richer in-game interactions across different\ngames. It also outperforms state-of-the-art agents in Minecraft by achieving a\nhigher task completion rate and providing more diverse solutions. These results\nhighlight MIMIC's significant potential for effective game testing.", "AI": {"tldr": "MIMIC is a novel framework that equips automated game testing agents with diverse personality traits, allowing them to mimic varied human playstyles. This leads to higher test coverage, richer interactions, and better results than previous solutions, especially in games like Minecraft.", "motivation": "Traditional automated testing algorithms struggle with the complexity and variability of modern video games. Existing gaming agents based on Reinforcement Learning, Imitation Learning, or LLMs often lack diversity in their strategies because they do not mimic the varied playstyles of human players with different personalities. This results in repetitive agent behavior and an inability to uncover edge cases or achieve comprehensive test coverage.", "method": "The paper introduces MIMIC, a framework that integrates diverse personality traits into gaming agents. The framework allows agents to emulate different gaming strategies and playstyles depending on personality traits, providing variety in responses to similar in-game situations.", "result": "MIMIC enables gaming agents to achieve higher test coverage and richer in-game interactions across various games. Specifically, in Minecraft, MIMIC outperformed state-of-the-art agents by completing more tasks and offering a broader range of solutions.", "conclusion": "Integrating personality-driven strategy diversity in gaming agents significantly enhances their effectiveness for automated game testing, allowing for broader coverage, more varied interactions, and improved uncovering of edge cases compared to traditional agents."}}
{"id": "2510.01740", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01740", "abs": "https://arxiv.org/abs/2510.01740", "authors": ["Kypros Iacovou", "Georgia M. Kapitsaki", "Evangelia Vanezi"], "title": "FOSS-chain: using blockchain for Open Source Software license compliance", "comment": null, "summary": "Open Source Software (OSS) is widely used and carries licenses that indicate\nthe terms under which the software is provided for use, also specifying\nmodification and distribution rules. Ensuring that users are respecting OSS\nlicense terms when creating derivative works is a complex process. Compliance\nissues arising from incompatibilities among licenses may lead to legal\ndisputes. At the same time, the blockchain technology with immutable entries\noffers a mechanism to provide transparency when it comes to licensing and\nensure software changes are recorded. In this work, we are introducing an\nintegration of blockchain and license management when creating derivative\nworks, in order to tackle the issue of OSS license compatibility. We have\ndesigned, implemented and performed a preliminary evaluation of FOSS-chain, a\nweb platform that uses blockchain and automates the license compliance process,\ncovering 14 OSS licenses. We have evaluated the initial prototype version of\nthe FOSS-chain platform via a small scale user study. Our preliminary results\nare promising, demonstrating the potential of the platform for adaptation on\nrealistic software systems.", "AI": {"tldr": "The paper proposes FOSS-chain, a blockchain-based web platform for OSS license compliance. Preliminary user study results indicate promise for real-world adaptation in managing license compatibility issues.", "motivation": "Ensuring compliance with Open Source Software (OSS) licenses is complex, especially when creating derivative works due to compatibility issues among various licenses. Legal disputes can arise from such incompatibilities. There is a need for transparent, automated mechanisms to manage license compliance.", "method": "The authors designed, implemented, and preliminarily evaluated a web platform called FOSS-chain. This platform integrates blockchain technology to automate the license compliance process for OSS derivatives, covering 14 different OSS licenses. The platform was evaluated using a small-scale user study.", "result": "The preliminary evaluation of the FOSS-chain platform showed promising results, suggesting its potential for adaptation in realistic software development environments.", "conclusion": "Integrating blockchain with OSS license management can increase transparency and automate compliance, potentially reducing legal risks and complexity in managing OSS derivatives. FOSS-chain shows early promise as a practical solution."}}
{"id": "2510.01754", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01754", "abs": "https://arxiv.org/abs/2510.01754", "authors": ["Hina Anwar"], "title": "ARENA: A tool for measuring and analysing the energy efficiency of Android apps", "comment": null, "summary": "To build energy-efficient apps, there is a need to estimate and analyze their\nenergy consumption in typical usage scenarios. The energy consumption of\nAndroid apps could be estimated via software-based and hardware-based\napproaches. Software-based approaches, while easier to implement, are not as\naccurate as hardware-based approaches. The process of measuring the energy\nconsumption of an Android app via a hardware-based approach typically involves\n1) setting up a measurement environment, 2) executing the app under test on a\nmobile device, 3) recording current/voltage data via a hardware device to\nmeasure energy consumption, and 4) cleaning and aggregating data for analyses,\nreports, and visualizations. Specialized scripts are written for selected\nhardware and software components to ensure reliable energy measurements. The\nenergy measurement process is repeated many times and aggregated to remove\nnoise. These steps make the hardware-based energy measurement process\ntime-consuming and not easy to adapt or reproduce. There is a lack of\nopen-source tools available for developers and researchers to take reliable\nenergy measurements via hardware devices. In this paper, we present and\ndemonstrate ARENA, a support tool that enables developers and researchers to\nconnect to a physical measurement device without leaving the comfort of their\nIDE. Developers could use ARENA during development to compare energy\nconsumption between different apps or versions of the same app. ARENA\ncalculates energy consumption on an Android smartphone by executing a test\nscenario on the app under development. Further, ARENA helps aggregate,\nstatistically analyze, report, and visualize the data, allowing developers and\nresearchers to dig into the data directly or visually. We implemented ARENA as\nan IntelliJ and Android Studio plugin.", "AI": {"tldr": "ARENA is a plugin for popular Android IDEs that automates reliable, hardware-based measurement and analysis of Android app energy consumption, helping developers easily compare and analyze energy usage during app development.", "motivation": "Developers and researchers need accurate and repeatable methods to measure Android apps' energy usage, but current hardware-based approaches are time-consuming, complex, and lack open-source tooling.", "method": "The authors present ARENA, an open-source tool integrated as a plugin for IntelliJ and Android Studio, allowing seamless connection to a physical hardware measurement device from the IDE. The tool automates the execution of apps, data collection from hardware, aggregation, analysis, and visualization.", "result": "ARENA facilitates reliable, reproducible, and more accessible measurement of Android app energy consumption, supporting developers in comparing energy use between app versions and providing detailed statistical analyses and visualizations.", "conclusion": "ARENA simplifies and standardizes the previously cumbersome process of hardware-based energy measurement for Android apps, making it accessible in mainstream development environments."}}
{"id": "2510.01825", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01825", "abs": "https://arxiv.org/abs/2510.01825", "authors": ["Zhenyu Yang", "Yue Pan", "Zhen Yang", "Zhongxing Yu"], "title": "Towards Speeding up Program Repair with Non-Autoregressive Model", "comment": "30 pages, 8 figures, 7 tables. arXiv admin note: substantial text\n  overlap with arXiv:2406.16526", "summary": "Enlightened by the success of machine learning techniques in various\napplication areas, recent years have witnessed a surge of research efforts on\nautomatic program repair (APR) using machine learning techniques. Previous\nmachine learning-based APR techniques essentially modified bugs in the\nautoregressive (AR) manner, which predicts future values based on past values.\nDue to the manner of token-by-token generation, the AR-based APR technique has\na huge time delay. In particular, the delay of the APR model with a large\nnumber of parameters is more serious. To address the issue, we aim to apply the\nnon-autoregressive (NAR) method to the APR task, which can output target code\nin a parallel manner to avoid huge repair delays. However, the naive use of the\nNAR manner for the APR task suffers from the issue of compromised patch\nquality. To effectively adapt the NAR manner for the APR task, we in this paper\npropose NARRepair, the first customized NAR code generation model for the APR\ntask. The NARRepair model features three major novelties, including 1) the\nrepair action predictor for alleviating the over-correction issue, 2) the\ninter-token dependency extractor for alleviating the issue of lacking\ninter-token dependency information, and 3) the two-stage decoder for\nalleviating the issue of lacking contextual information. We evaluated NARRepair\non three widely used datasets in the APR community, and the results show that\n1) compared to other APR techniques, the NARRepair model has the best\nperformance within the limited repair time, and 2) compared to AR-based APR\ntechniques, the repair speed of NARRepair has been increased by 1.4-6.4 times\nin the GPU environment. Overall, the results show that NARRepair has achieved\nstate-of-the-art comprehensive performance in terms of repair speed and\naccuracy.", "AI": {"tldr": "NARRepair is a pioneering non-autoregressive model for automatic program repair, enabling fast and accurate bug fixing in source code. It solves key limitations of previous autoregressive models and achieves much higher repair speeds and top-tier accuracy on benchmark datasets.", "motivation": "Existing machine learning-based automatic program repair (APR) methods use autoregressive (AR) models which generate code token-by-token. This approach leads to significant time delays, especially for models with many parameters. The paper aims to mitigate this delay and improve efficiency in APR.", "method": "The authors propose NARRepair, a novel non-autoregressive (NAR) code generation model specifically designed for APR. The approach incorporates three innovations: a repair action predictor to avoid over-correction, an inter-token dependency extractor to capture dependencies between tokens, and a two-stage decoder to enhance contextual understanding.", "result": "Experiments on three commonly used APR datasets show that NARRepair outperforms existing techniques in limited repair time scenarios, and delivers repair speeds 1.4-6.4 times faster than AR-based APR models on GPUs. It achieves state-of-the-art results in both repair speed and accuracy.", "conclusion": "NARRepair demonstrates substantial improvements in both the efficiency and accuracy of automatic program repair, setting a new benchmark for the field."}}
{"id": "2510.01960", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01960", "abs": "https://arxiv.org/abs/2510.01960", "authors": ["Victor Lira", "Paulo Borba", "Rodrigo Bonif\u00e1cio", "Galileu Santos e Matheus barbosa"], "title": "RefFilter: Improving Semantic Conflict Detection via Refactoring-Aware Static Analysis", "comment": null, "summary": "Detecting semantic interference remains a challenge in collaborative software\ndevelopment. Recent lightweight static analysis techniques improve efficiency\nover SDG-based methods, but they still suffer from a high rate of false\npositives. A key cause of these false positives is the presence of\nbehavior-preserving code refactorings, which current techniques cannot\neffectively distinguish from changes that impact behavior and can interfere\nwith others. To handle this problem we present RefFilter, a refactoring-aware\ntool for semantic interference detection. It builds on existing static\ntechniques by incorporating automated refactoring detection to improve\nprecision. RefFilter discards behavior-preserving refactorings from reports,\nreducing false positives while preserving detection coverage. To evaluate\neffectiveness and scalability, use two datasets: a labeled dataset with 99\nscenarios and ground truth, and a novel dataset of 1,087 diverse merge\nscenarios that we have built. Experimental results show that RefFilter reduces\nfalse positives by nearly 32% on the labeled dataset. While this reduction\ncomes with a non significant increase in false negatives, the overall gain in\nprecision significantly outweighs the minor trade-off in recall. These findings\ndemonstrate that refactoring-aware interference detection is a practical and\neffective strategy for improving merge support in modern development workflows.", "AI": {"tldr": "RefFilter, a new tool for semantic interference detection in software merging, reduces false positives by filtering behavior-preserving refactorings. Evaluations show a 32% reduction in false positives with minimal impact on recall, making it a practical solution for development workflows.", "motivation": "Detecting semantic interference in collaborative software development remains difficult due to high false positive rates in current static analysis techniques, largely because these techniques cannot distinguish between behavior-preserving refactorings and changes that actually interfere with software behavior.", "method": "The paper presents RefFilter, a refactoring-aware static analysis tool that incorporates automated refactoring detection. RefFilter filters out behavior-preserving refactorings, thus improving static analysis precision. The tool's effectiveness was evaluated on two datasets: a labeled dataset (99 scenarios) and a larger, newly constructed dataset (1,087 merge scenarios).", "result": "RefFilter reduces false positives by nearly 32% on the labeled dataset. The reduction in false positives results in only a non-significant increase in false negatives, leading to a significant overall gain in precision without a major loss in recall.", "conclusion": "Refactoring-aware semantic interference detection, as implemented with RefFilter, is a practical and effective way to improve merge support in modern collaborative development, demonstrating significant precision improvements over baseline static analysis tools."}}
{"id": "2510.01994", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01994", "abs": "https://arxiv.org/abs/2510.01994", "authors": ["Chen Yang", "Lin Yang", "Ziqi Wang", "Dong Wang", "Jianyi Zhou", "Junjie Chen"], "title": "Clarifying Semantics of In-Context Examples for Unit Test Generation", "comment": "accepted in the research track of ASE 2025", "summary": "Recent advances in large language models (LLMs) have enabled promising\nperformance in unit test generation through in-context learning (ICL). However,\nthe quality of in-context examples significantly influences the effectiveness\nof generated tests-poorly structured or semantically unclear test examples\noften lead to suboptimal outputs. In this paper, we propose CLAST, a novel\ntechnique that systematically refines unit tests to improve their semantic\nclarity, thereby enhancing their utility as in-context examples. The approach\ndecomposes complex tests into logically clearer ones and improves semantic\nclarity through a combination of program analysis and LLM-based rewriting. We\nevaluated CLAST on four open-source and three industrial projects. The results\ndemonstrate that CLAST largely outperforms UTgen, the state-of-the-art\nrefinement technique, in both preserving test effectiveness and enhancing\nsemantic clarity. Specifically, CLAST fully retains the original effectiveness\nof unit tests, while UTgen reduces compilation success rate (CSR), pass rate\n(PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%,\n35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user\nstudy preferred the semantic clarity of CLAST-refined tests. Notably,\nincorporating CLAST-refined tests as examples effectively improves ICL-based\nunit test generation approaches such as RAGGen and TELPA, resulting in an\naverage increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for\ngenerated tests, compared to incorporating UTgen-refined tests. The insights\nfrom the follow-up user study not only reinforce CLAST's potential impact in\nsoftware testing practice but also illuminate avenues for future research.", "AI": {"tldr": "CLAST refines unit tests to make them clearer and more effective as in-context examples for large language model-based test generation. It outperforms previous methods in effectiveness and user preference, leading to better generated tests and advancing software testing practices.", "motivation": "Recent progress in large language models (LLMs) has improved unit test generation via in-context learning (ICL), but the effectiveness of this approach largely depends on the quality and clarity of in-context test examples. Poorly structured or unclear tests result in worse generated outputs. Addressing the need for more semantically clear and well-structured tests as ICL examples is a key motivation.", "method": "The paper introduces CLAST, a technique to systematically refine unit tests by decomposing complex tests and enhancing their semantic clarity using a combination of program analysis and LLM-based rewriting. CLAST is evaluated on several open-source and industrial projects, and its performance is benchmarked against the state-of-the-art UTgen refinement method.", "result": "CLAST significantly outperforms UTgen by fully retaining the effectiveness of the original tests while UTgen reduces test effectiveness metrics (CSR, PR, Cov, MS) by notable percentages. Over 85% of users prefer CLAST-refined tests for semantic clarity. When CLAST-refined examples are used in ICL-based test generation systems, those systems show substantial improvements in compilation success rate, pass rate, and coverage compared to using UTgen-refined tests.", "conclusion": "CLAST effectively refines unit tests for superior semantic clarity, preserving or improving test effectiveness and significantly boosting the performance of ICL-based unit test generation systems. It outperforms prior refinement approaches and is strongly preferred by users, paving the way for improvement in software testing and future research in this area."}}
{"id": "2510.02002", "categories": ["cs.SE", "D.2.1; D.2.2; D.2.3; D.3.4; G.1.6"], "pdf": "https://arxiv.org/pdf/2510.02002", "abs": "https://arxiv.org/abs/2510.02002", "authors": ["Maximilian Kratz", "Steffen Zschaler", "Jens Kosiol", "Gabriele Taentzer"], "title": "Automatic Generation of Combinatorial Reoptimisation Problem Specifications: A Vision", "comment": null, "summary": "Once an optimisation problem has been solved, the solution may need\nadaptation when contextual factors change. This challenge, also known as\nreoptimisation, has been addressed in various problem domains, such as railway\ncrew rescheduling, nurse rerostering, or aircraft recovery. This requires a\nmodified problem to be solved again to ensure that the adapted solution is\noptimal in the new context. However, the new optimisation problem differs\nnotably from the original problem: (i) we want to make only minimal changes to\nthe original solution to minimise the impact; (ii) we may be unable to change\nsome parts of the original solution (e.g., because they refer to past\nallocations); and (iii) we need to derive a change script from the original\nsolution to the new solution. In this paper, we argue that Model-Driven\nEngineering (MDE) - in particular, the use of declarative modelling languages\nand model transformations for the high-level specification of optimisation\nproblems - offers new opportunities for the systematic derivation of\nreoptimisation problems from the original optimisation problem specification.\nWe focus on combinatorial reoptimisation problems and provide an initial\ncategorisation of changing problems and strategies for deriving the\ncorresponding reoptimisation specifications. We introduce an initial\nproof-of-concept implementation based on the GIPS (Graph-Based (Mixed) Integer\nLinear Programming Problem Specification) tool and apply it to an example\nresource-allocation problem: the allocation of teaching assistants to teaching\nsessions.", "AI": {"tldr": "The paper proposes using model-driven engineering and model transformations to automatically and systematically derive reoptimization problems from original optimization models, with a proof-of-concept shown in teaching assistant allocation. This helps minimize solution changes and manage practical constraints in adapting existing solutions.", "motivation": "When contextual factors change, an already solved optimization problem may need its solution adapted. Traditional reoptimization requires solving the modified problem, which introduces challenges such as minimizing changes to the original solution, handling immutable components, and deriving a script of solution changes. Existing approaches may not systematically derive reoptimization problems from the original model specification.", "method": "The paper proposes leveraging Model-Driven Engineering (MDE)\u2014specifically declarative modeling languages and model transformations\u2014to systematically derive reoptimization problems from original optimization problem specifications. The authors categorize changes in problems and the corresponding reoptimization strategies. A proof-of-concept implementation is realized using the GIPS tool and demonstrated on a teaching assistant allocation problem.", "result": "Presented an initial categorization framework for changing optimization problems and reoptimization strategies. Provided a proof-of-concept implementation using GIPS and successfully applied it to a resource allocation case study involving the assignment of teaching assistants to sessions.", "conclusion": "Model-Driven Engineering approaches can facilitate the systematic derivation of reoptimization problems, offering a structured way to adapt solutions when contextual factors change. MDE and model transformation can minimize impact and handle constraints inherent in real reoptimization tasks. The initial implementation demonstrates the feasibility of this approach."}}
{"id": "2510.02007", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02007", "abs": "https://arxiv.org/abs/2510.02007", "authors": ["Justus Bogner", "Roberto Verdecchia"], "title": "ACM SIGSOFT SEN Empirical Software Engineering: Introducing Our New Regular Column", "comment": "Published in ACM SIGSOFT Software Engineering Notes (SIGSOFT-SEN).\n  Volume 50, Issue 4, 2025", "summary": "From its early foundations in the 1970s, empirical software engineering (ESE)\nhas evolved into a mature research discipline that embraces a plethora of\ndifferent topics, methodologies, and industrial practices. Despite its\nremarkable progress, the ESE research field still needs to keep evolving, as\nnew impediments, shortcoming, and technologies emerge. Research\nreproducibility, limited external validity, subjectivity of reviews, and\nporting research results to industrial practices are just some examples of the\ndrivers for improvements to ESE research. Additionally, several facets of ESE\nresearch are not documented very explicitly, which makes it difficult for\nnewcomers to pick them up. With this new regular ACM SIGSOFT SEN column\n(SEN-ESE), we introduce a venue for discussing meta-aspects of ESE research,\nranging from general topics such as the nature and best practices for\nreplication packages, to more nuanced themes such as statistical methods,\ninterview transcription tools, and publishing interdisciplinary research. Our\naim for the column is to be a place where we can regularly spark conversations\non ESE topics that might not often be touched upon or are left implicit.\nContributions to this column will be grounded in expert interviews, focus\ngroups, surveys, and position pieces, with the goal of encouraging reflection\nand improvement in how we conduct, communicate, teach, and ultimately improve\nESE research. Finally, we invite feedback from the ESE community on\nchallenging, controversial, or underexplored topics, as well as suggestions for\nvoices you would like to hear from. While we cannot promise to act on every\nidea, we aim to shape this column around the community interests and are\ngrateful for all contributions.", "AI": {"tldr": "The paper announces a new ACM SIGSOFT SEN column to openly discuss meta-aspects and improvement opportunities in empirical software engineering, aiming for community-driven reflection and better research practices.", "motivation": "Empirical software engineering (ESE) has made significant strides since the 1970s, but faces ongoing challenges like reproducibility, limited external validity, subjective reviews, and the difficulty of transferring research to industry. Additionally, many ESE research practices remain undocumented, hindering new researchers' entry and progress.", "method": "The paper introduces a new, regular ACM SIGSOFT SEN column (SEN-ESE) as a platform for discussing meta-aspects of ESE research. The column will be based on expert interviews, focus groups, surveys, and position pieces.", "result": "This column aims to foster discussions on underexplored or implicit topics in ESE, promoting reflection and improvement in research practice, communication, and teaching. It seeks community feedback and intends to adapt to the interests and needs of the ESE community.", "conclusion": "By establishing SEN-ESE, the paper hopes to make ESE's meta-aspects more explicit, improve research practices and documentation, and foster a collaborative community environment that evolves with new challenges."}}
{"id": "2510.02165", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02165", "abs": "https://arxiv.org/abs/2510.02165", "authors": ["Peter Wauyo", "Dalia Bwiza", "Alain Murara", "Edwin Mugume", "Eric Umuhoza"], "title": "Towards fairer public transit: Real-time tensor-based multimodal fare evasion and fraud detection", "comment": "10 pages", "summary": "This research introduces a multimodal system designed to detect fraud and\nfare evasion in public transportation by analyzing closed circuit television\n(CCTV) and audio data. The proposed solution uses the Vision Transformer for\nVideo (ViViT) model for video feature extraction and the Audio Spectrogram\nTransformer (AST) for audio analysis. The system implements a Tensor Fusion\nNetwork (TFN) architecture that explicitly models unimodal and bimodal\ninteractions through a 2-fold Cartesian product. This advanced fusion technique\ncaptures complex cross-modal dynamics between visual behaviors (e.g.,\ntailgating,unauthorized access) and audio cues (e.g., fare transaction sounds).\nThe system was trained and tested on a custom dataset, achieving an accuracy of\n89.5%, precision of 87.2%, and recall of 84.0% in detecting fraudulent\nactivities, significantly outperforming early fusion baselines and exceeding\nthe 75% recall rates typically reported in state-of-the-art transportation\nfraud detection systems. Our ablation studies demonstrate that the tensor\nfusion approach provides a 7.0% improvement in the F1 score and an 8.8% boost\nin recall compared to traditional concatenation methods. The solution supports\nreal-time detection, enabling public transport operators to reduce revenue\nloss, improve passenger safety, and ensure operational compliance.", "AI": {"tldr": "This paper proposes a transformer-based multimodal system using video and audio data to detect fraud in public transport. Using an advanced tensor fusion technique, it exceeds current detection accuracies and recall, supports real-time monitoring, and offers a major improvement over traditional approaches.", "motivation": "Fraud and fare evasion in public transportation lead to revenue loss, compromised safety, and non-compliance. Existing detection systems, especially those using unimodal data or early fusion approaches, struggle to catch complex fraudulent activities effectively, motivating the need for a more robust multimodal system.", "method": "The paper introduces a multimodal fraud detection system that analyzes CCTV video and audio data. Video features are extracted using the ViViT (Vision Transformer for Video) model, while audio is processed via the Audio Spectrogram Transformer (AST). The features are fused using a Tensor Fusion Network (TFN) architecture, which models both unimodal and bimodal interactions through a 2-fold Cartesian product, capturing complex cross-modal dynamics. The system is evaluated on a custom dataset.", "result": "The system achieved an accuracy of 89.5%, precision of 87.2%, and recall of 84.0% for fraud detection, outperforming early fusion baselines and surpassing the usual state-of-the-art recall rates (typically 75%) in transportation fraud detection. Tensor fusion improved the F1 score by 7.0% and recall by 8.8% compared to feature concatenation. Real-time detection capability was demonstrated.", "conclusion": "A multimodal system combining ViViT and AST models with a Tensor Fusion Network significantly enhances the detection of fare evasion and fraud in public transportation. It outperforms traditional baselines, provides real-time detection, and enables operators to reduce revenue loss, improve safety, and maintain compliance."}}
{"id": "2510.02166", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02166", "abs": "https://arxiv.org/abs/2510.02166", "authors": ["Fatou Ndiaye Mbodji", "El-hacen Diallo", "Jordan Samhi", "Kui Liu", "Jacques Klein", "Tegawend\u00e9 F. Bissyande"], "title": "SIEVE: Towards Verifiable Certification for Code-datasets", "comment": "5", "summary": "Code agents and empirical software engineering rely on public code datasets,\nyet these datasets lack verifiable quality guarantees. Static 'dataset cards'\ninform, but they are neither auditable nor do they offer statistical\nguarantees, making it difficult to attest to dataset quality. Teams build\nisolated, ad-hoc cleaning pipelines. This fragments effort and raises cost. We\npresent SIEVE, a community-driven framework. It turns per-property checks into\nConfidence Cards-machine-readable, verifiable certificates with anytime-valid\nstatistical bounds. We outline a research plan to bring SIEVE to maturity,\nreplacing narrative cards with anytime-verifiable certification. This shift is\nexpected to lower quality-assurance costs and increase trust in code-datasets.", "AI": {"tldr": "SIEVE is a proposed framework to statistically certify the quality of public code datasets using machine-readable, auditable 'Confidence Cards,' aiming to replace current manual efforts with automated, trustworthy certification.", "motivation": "Public code datasets are essential for code agents and empirical software engineering, but current practices lack verifiable quality guarantees, leading to duplicated effort and higher costs for quality assurance.", "method": "The paper introduces SIEVE, a community-driven framework that converts dataset property checks into machine-readable, auditable 'Confidence Cards' which provide statistical, anytime-valid certifications of dataset quality.", "result": "SIEVE aims to standardize and automate the quality verification process for code datasets, making it easier to assure quality without repeated, isolated manual efforts.", "conclusion": "If adopted, SIEVE could replace narrative dataset documentation with statistically-backed, verifiable certification, reducing costs, preventing redundant work, and improving trust in public code datasets."}}
{"id": "2510.02169", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.02169", "abs": "https://arxiv.org/abs/2510.02169", "authors": ["Vadim Safronov", "Anthony McCaigue", "Nicholas Allott", "Andrew Martin"], "title": "TAIBOM: Bringing Trustworthiness to AI-Enabled Systems", "comment": "This paper has been accepted at the First International Workshop on\n  Security and Privacy-Preserving AI/ML (SPAIML 2025), co-located with the 28th\n  European Conference on Artificial Intelligence (ECAI 2025)", "summary": "The growing integration of open-source software and AI-driven technologies\nhas introduced new layers of complexity into the software supply chain,\nchallenging existing methods for dependency management and system assurance.\nWhile Software Bills of Materials (SBOMs) have become critical for enhancing\ntransparency and traceability, current frameworks fall short in capturing the\nunique characteristics of AI systems -- namely, their dynamic, data-driven\nnature and the loosely coupled dependencies across datasets, models, and\nsoftware components. These challenges are compounded by fragmented governance\nstructures and the lack of robust tools for ensuring integrity, trust, and\ncompliance in AI-enabled environments.\n  In this paper, we introduce Trusted AI Bill of Materials (TAIBOM) -- a novel\nframework extending SBOM principles to the AI domain. TAIBOM provides (i) a\nstructured dependency model tailored for AI components, (ii) mechanisms for\npropagating integrity statements across heterogeneous AI pipelines, and (iii) a\ntrust attestation process for verifying component provenance. We demonstrate\nhow TAIBOM supports assurance, security, and compliance across AI workflows,\nhighlighting its advantages over existing standards such as SPDX and CycloneDX.\nThis work lays the foundation for trustworthy and verifiable AI systems through\nstructured software transparency.", "AI": {"tldr": "The paper proposes TAIBOM, an advanced framework for tracking and assuring AI system components, surpassing traditional SBOMs to enable secure, transparent, and compliant AI software supply chains.", "motivation": "The increasing use of open-source software and AI-driven technologies complicates software supply chains, making traditional management and assurance methods inadequate. Existing frameworks like SBOMs don't address the special challenges of AI, such as dynamic dependencies, data-driven processes, and fragmented governance.", "method": "The paper introduces Trusted AI Bill of Materials (TAIBOM), a new framework that extends SBOM principles to AI. TAIBOM offers a dependency model for AI, mechanisms to propagate integrity information, and a process for trust attestation, tailored specifically for AI's unique workflows and components.", "result": "TAIBOM is shown to enhance assurance, security, and compliance in AI systems by providing structured and verifiable transparency, outperforming established standards like SPDX and CycloneDX in the AI context.", "conclusion": "TAIBOM creates a foundation for trustworthy and transparent AI systems by formally documenting dependencies, provenance, and trust within complex AI pipelines."}}
{"id": "2510.02185", "categories": ["cs.SE", "cs.CR", "cs.MA", "D.2.4; F.3.1"], "pdf": "https://arxiv.org/pdf/2510.02185", "abs": "https://arxiv.org/abs/2510.02185", "authors": ["Paschal C. Amusuo", "Dongge Liu", "Ricardo Andres Calvo Mendez", "Jonathan Metzman", "Oliver Chang", "James C. Davis"], "title": "FalseCrashReducer: Mitigating False Positive Crashes in OSS-Fuzz-Gen Using Agentic AI", "comment": "12 pages, 2 figures", "summary": "Fuzz testing has become a cornerstone technique for identifying software bugs\nand security vulnerabilities, with broad adoption in both industry and\nopen-source communities. Directly fuzzing a function requires fuzz drivers,\nwhich translate random fuzzer inputs into valid arguments for the target\nfunction. Given the cost and expertise required to manually develop fuzz\ndrivers, methods exist that leverage program analysis and Large Language Models\nto automatically generate these drivers. However, the generated fuzz drivers\nfrequently lead to false positive crashes, especially in functions highly\nstructured input and complex state requirements. This problem is especially\ncrucial in industry-scale fuzz driver generation efforts like OSS-Fuzz-en, as\nreporting false positive crashes to maintainers impede trust in both the system\nand the team.\n  This paper presents two AI-driven strategies to reduce false positives in\nOSS-Fuzz-Gen, a multi-agent system for automated fuzz driver generation. First,\nconstraint-based fuzz driver generation proactively enforces constraints on a\nfunction's inputs and state to guide driver creation. Second, context-based\ncrash validation reactively analyzes function callers to determine whether\nreported crashes are feasible from program entry points. Using 1,500 benchmark\nfunctions from OSS-Fuzz, we show that these strategies reduce spurious crashes\nby up to 8%, cut reported crashes by more than half, and demonstrate that\nfrontier LLMs can serve as reliable program analysis agents. Our results\nhighlight the promise and challenges of integrating AI into large-scale fuzzing\npipelines.", "AI": {"tldr": "This paper presents two AI-powered methods to reduce false positive crashes in large-scale automated fuzz driver generation for fuzz testing. By enforcing input/state constraints and validating crashes within context, they cut spurious crash reports and improve trust in automated fuzzing, demonstrating the feasibility of using advanced LLMs as program analysis agents.", "motivation": "Fuzz testing requires fuzz drivers to translate input, but creating them manually is time-consuming and requires expertise. Automated approaches exist, but they often produce false positives due to the complexity of input requirements and program state, especially in industrial-scale settings like OSS-Fuzz-Gen. Reducing these false positives is important to improve trust and efficiency in large-scale fuzzing initiatives.", "method": "The paper introduces two AI-based strategies within OSS-Fuzz-Gen: (1) constraint-based fuzz driver generation, which proactively enforces input and state constraints during driver creation, and (2) context-based crash validation, which analyzes program call context to validate whether reported crashes are feasible entry-point events.", "result": "Applying these strategies to 1,500 OSS-Fuzz functions led to up to an 8% reduction in spurious crashes, halved the total number of reported crashes, and established that advanced LLMs can function as reliable agents for program analysis.", "conclusion": "AI-driven strategies such as proactive constraint enforcement and reactive context analysis can significantly improve the reliability and effectiveness of automated fuzz driver generation, reducing false positives and enhancing trust in automated fuzzing pipelines."}}
