{"id": "2508.00005", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00005", "abs": "https://arxiv.org/abs/2508.00005", "authors": ["Tilman Hinnerichs", "Bart Swinkels", "Jaap de Jong", "Reuben Gardos Reid", "Tudor Magirescu", "Neil Yorke-Smith", "Sebastijan Dumancic"], "title": "Modelling Program Spaces in Program Synthesis with Constraints", "comment": null, "summary": "A core challenge in program synthesis is taming the large space of possible\nprograms. Since program synthesis is essentially a combinatorial search, the\ncommunity has sought to leverage powerful combinatorial constraint solvers.\nHere, constraints are used to express the program semantics, but not as a\npotentially potent tool to remove unwanted programs. Recent inductive logic\nprogramming approaches introduce constraints on the program's syntax to be\nsynthesized. These syntactic constraints allow for checking and propagating a\nconstraint without executing the program, and thus for arbitrary operators. In\nthis work, we leverage syntactic constraints to model program spaces, defining\nnot just solutions that are feasible, but also ones that are likely useful. To\ndemonstrate this idea, we introduce BART, a solver that efficiently propagates\nand solves these constraints. We evaluate BART on program space enumeration\ntasks, finding that the constraints eliminate up to 99 percent of the program\nspace, and that modeling program spaces significantly reduces enumeration time.", "AI": {"tldr": "This paper proposes using syntactic constraints to reduce the program space in program synthesis. The BART solver is introduced, which eliminates up to 99% of unwanted programs and significantly speeds up program enumeration tasks.", "motivation": "Program synthesis faces the challenge of navigating a vast space of possible programs. Current methods focus on using constraints to express semantics rather than as a means to prune irrelevant or unwanted programs effectively. There is a need for more potent constraint handling to improve the efficiency of program synthesis.", "method": "This paper introduces the use of syntactic constraints to model and constrain the program search space, not only ensuring feasibility but also enhancing the likelihood of producing useful solutions. The authors present BART, a solver leveraging these constraints, enabling efficient propagation and solving without the need for program execution.", "result": "BART was evaluated on program space enumeration tasks, demonstrating its ability to eliminate up to 99% of the program space using syntactic constraints. This significant pruning resulted in a notable reduction in enumeration time compared to previous approaches.", "conclusion": "Utilizing syntactic constraints in program synthesis, as realized by BART, drastically reduces the search space and boosts efficiency. Syntactic constraints prove to be a powerful tool in modeling and managing possible programs, outperforming traditional semantic-only constraint approaches."}}
{"id": "2508.00013", "categories": ["cs.PL", "I.2.6; F.1.1"], "pdf": "https://arxiv.org/pdf/2508.00013", "abs": "https://arxiv.org/abs/2508.00013", "authors": ["Zurabi Kobaladze", "Anna Arnania", "Tamar Sanikidze"], "title": "From Provable Correctness to Probabilistic Generation: A Comparative Review of Program Synthesis Paradigms", "comment": "78 pages. Undergraduate thesis project submitted in partial\n  fulfillment of the requirements for the Bachelor's degree in Computer Science\n  at Kutaisi International University", "summary": "Program synthesis--the automated generation of executable code from\nhigh-level specifications--has been a central goal of computer science for over\nfifty years. This thesis provides a comparative literature review of the main\nparadigms that have shaped the field, tracing its evolution from formal logic\nbased methods to recent advances using large scale neural models. We examine\nfive key approaches: logic based (deductive) synthesis, inductive (example\nbased) synthesis, sketch/schema based synthesis, large language model based\nsynthesis, and neuro-symbolic hybrids. For each, we analyze foundational\nprinciples, notable systems, and practical applications, highlighting trade\noffs between correctness guarantees, specification requirements, search\ncomplexity, and expressive power. By reviewing developments from formally\nverified synthesis tools such as KIDS and Coq to data driven models generating\nprobabilistic code from natural language like Codex, we present a comprehensive\nnarrative of progress and ongoing challenges. This work emphasizes the\ntransition from symbolic to hybrid neuro-symbolic methods and outlines future\ndirections for reliable and scalable program synthesis.", "AI": {"tldr": "This thesis reviews the history and paradigms of program synthesis, comparing logic-driven, inductive, schema-based, large language model, and hybrid neuro-symbolic methods, highlighting how the field is moving toward combining symbolic and neural techniques for more scalable and reliable automated coding.", "motivation": "Program synthesis has been a core pursuit in computer science for decades, aiming to automate the generation of code from high-level specifications. There has been significant evolution in methodologies, and there is a need to comprehensively understand the trajectory and trade-offs in the field.", "method": "The thesis conducts a comparative literature review, systematically analyzing five principal paradigms: logic-based (deductive), inductive (example-based), sketch/schema-based, large language model-based, and neuro-symbolic hybrid approaches. It examines foundational principles, notable tools, and application domains for each.", "result": "The review outlines the features, advantages, and limitations of each synthesis paradigm. It traces developments from formally verified systems (like KIDS and Coq) to recent data-driven methods (such as Codex), analyzing their trade-offs regarding guarantees, specification requirements, search complexity, and expressiveness.", "conclusion": "There has been a marked transition from purely symbolic to neuro-symbolic and large language model-based methods in program synthesis. The thesis identifies ongoing challenges and future research directions to make synthesis both reliable and scalable."}}
{"id": "2508.00016", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.00016", "abs": "https://arxiv.org/abs/2508.00016", "authors": ["Matt Kaufmann", "Yahya Sohail", "Warren A. Hunt Jr"], "title": "Extended Abstract: Mutable Objects with Several Implementations", "comment": "In Proceedings ACL2 2025, arXiv:2507.18567", "summary": "This extended abstract outlines an ACL2 feature, attach-stobj, that first\nappeared in ACL2 Version 8.6 (October, 2024). This feature supports different\nexecutable operations for a given abstract stobj, without requiring\nrecertification of the book that introduces that stobj or theorems about it.\nThe paper provides background as well as a user-level overview and some\nimplementation notes.", "AI": {"tldr": "The paper presents the 'attach-stobj' ACL2 feature, enabling users to change executable operations of abstract stobjs without recertifying related formal content, thereby streamlining the verification process.", "motivation": "In ACL2, maintaining flexibility in executable operations for abstract stobjs is challenging due to the need for recertifying related books and theorems when changes occur.", "method": "The paper introduces and describes the 'attach-stobj' feature in ACL2, detailing its user-facing application as well as some aspects of its implementation.", "result": "With the 'attach-stobj' feature, users can now specify different executable behaviors for a given abstract stobj without having to recertify the book or any associated theorems, enhancing workflow efficiency and flexibility.", "conclusion": "Attach-stobj in ACL2 significantly reduces overhead for users needing to modify executable operations for abstract stobjs, improving usability and modularity in formal verification tasks."}}
{"id": "2508.00422", "categories": ["cs.PL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00422", "abs": "https://arxiv.org/abs/2508.00422", "authors": ["Varun Bharti", "Shashwat Jha", "Dhruv Kumar", "Pankaj Jalote"], "title": "Automated Type Annotation in Python Using Large Language Models", "comment": "Under Review", "summary": "Type annotations in Python enhance maintainability and error detection.\nHowever, generating these annotations manually is error prone and requires\nextra effort. Traditional automation approaches like static analysis, machine\nlearning, and deep learning struggle with limited type vocabularies, behavioral\nover approximation, and reliance on large labeled datasets. In this work, we\nexplore the use of LLMs for generating type annotations in Python. We develop a\ngenerate check repair pipeline: the LLM proposes annotations guided by a\nConcrete Syntax Tree representation, a static type checker (Mypy) verifies\nthem, and any errors are fed back for iterative refinement. We evaluate four\nLLM variants: GPT 4oMini, GPT 4.1mini (general-purpose), and O3Mini, O4Mini\n(reasoning optimized), on 6000 code snippets from the ManyTypes4Py benchmark.\nWe first measure the proportion of code snippets annotated by LLMs for which\nMyPy reported no errors (i.e., consistent results): GPT 4oMini achieved\nconsistency on 65.9% of cases (34.1% inconsistent), while GPT 4.1mini, O3Mini,\nand O4Mini each reached approximately 88.6% consistency (around 11.4%\nfailures). To measure annotation quality, we then compute exact-match and\nbase-type match accuracies over all 6000 snippets: GPT 4.1mini and O3Mini\nperform the best, achieving up to 70.5% exact match and 79.1% base type\naccuracy, requiring under one repair iteration on average. Our results\ndemonstrate that general-purpose and reasoning optimized LLMs, without any task\nspecific fine tuning or additional training can be effective in generating\nconsistent type annotations.They perform competitively with traditional deep\nlearning techniques which require large labeled dataset for training. While our\nwork focuses on Python, the pipeline can be extended to other optionally typed\nimperative languages like Ruby", "AI": {"tldr": "LLMs can automatically generate and refine type annotations for Python code with high accuracy and consistency. The proposed pipeline offers competitive results versus traditional methods, without needing extensive labeled datasets or special training.", "motivation": "Type annotations in Python improve code quality but are tedious and error-prone to generate manually. Existing automation methods have limitations such as small type vocabularies, over-approximation, and dependence on large annotated datasets. The paper seeks to address these challenges.", "method": "The authors propose a 'generate-check-repair' pipeline using Large Language Models (LLMs) aided by Concrete Syntax Trees (CST). LLMs initially generate type annotations, which are checked with the static type checker Mypy; errors trigger automatic iterative refinements. Four LLM variants are evaluated on 6000 Python code snippets from the ManyTypes4Py benchmark.", "result": "General-purpose and reasoning-optimized LLMs (GPT 4.1mini and O3Mini) achieved high consistency rates (~88.6%) and strong annotation accuracies (up to 70.5% exact match, 79.1% base type). Most cases required fewer than one repair iteration. LLMs performed competitively with traditional deep learning, despite not using any task-specific fine-tuning or extra training data.", "conclusion": "The 'generate-check-repair' pipeline leveraging LLMs is effective for Python type annotation, rivaling deep learning approaches that require extensive labeled data. The approach is also extensible to other languages with optional typing, such as Ruby."}}
{"id": "2508.00031", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00031", "abs": "https://arxiv.org/abs/2508.00031", "authors": ["Junde Wu"], "title": "Git Context Controller: Manage the Context of LLM-based Agents like Git", "comment": "in updating", "summary": "Large language model (LLM) based agents have shown impressive capabilities by\ninterleaving internal reasoning with external tool use. However, as these\nagents are deployed in long-horizon workflows, such as coding for a big,\nlong-term project, context management becomes a critical bottleneck. We\nintroduce Git-Context-Controller (GCC), a structured context management\nframework inspired by software version control systems. GCC elevates context as\nversioned memory hierarchy like Git. It structures agent memory as a persistent\nfile system with explicit operations: COMMIT, BRANCH, MERGE, and CONTEXT,\nenabling milestone-based checkpointing, exploration of alternative plans, and\nstructured reflection. Our approach empowers agents to manage long-term goals,\nisolate architectural experiments, and recover or hand off memory across\nsessions and agents. Empirically, agents equipped with GCC achieve\nstate-of-the-art performance on the SWE-Bench-Lite benchmark, resolving 48.00\nof software bugs, outperforming 26 competitive systems. In a self-replication\ncase study, a GCC-augmented agent builds a new CLI agent from scratch,\nachieving 40.7 task resolution, compared to only 11.7 without GCC. The code is\nreleased at: https://github.com/theworldofagents/GCC", "AI": {"tldr": "The paper presents GCC, a Git-inspired context manager for LLM agents that significantly enhances their ability to handle long-term, complex tasks by using versioned memory operations. This approach beats existing baselines for bug fixing and task resolution in software development workflows.", "motivation": "Standard LLM-based agents struggle with context management in long, complex tasks such as large software projects, limiting their effectiveness in these scenarios.", "method": "The paper introduces Git-Context-Controller (GCC), a context management framework inspired by version control systems like Git. GCC treats agent memory as a versioned, hierarchical file system, and introduces operations such as COMMIT, BRANCH, MERGE, and CONTEXT to structure and manage context and memory for LLM agents.", "result": "GCC-equipped agents outperform 26 other competitive systems on the SWE-Bench-Lite benchmark, resolving 48% of software bugs. In a self-replication case study, agents with GCC built a new CLI agent from scratch, resolving 40.7% of tasks versus only 11.7% without GCC.", "conclusion": "Structured, versioned context management substantially improves LLM agent performance on long-horizon and complex software tasks. GCC enables efficient milestone tracking, alternative solution exploration, memory isolation, and transfer, leading to state-of-the-art results. The framework is openly available for further research and application."}}
{"id": "2508.00482", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.00482", "abs": "https://arxiv.org/abs/2508.00482", "authors": ["Erdem Yildirim", "Albert Schimpf", "Stefan Wehr", "Annette Bieniusa"], "title": "Semantic Subtyping for Maps in Erlang", "comment": null, "summary": "In this paper we will construct a set-theoretic model of types featuring type\nvariables, base types, set-theoretic types and map types. Syntax of map types\nspans all the map types available in Erlang. The model of types is used to\ndefine a semantic subtyping relation based on set containment. The novelty of\nthis work is the definition of subtyping over parameteric map types.", "AI": {"tldr": "This paper introduces a set-theoretic model for Erlang's rich type system, defining a new semantic subtyping relation that, for the first time, formally describes subtyping for parametric map types.", "motivation": "To provide a formal, set-theoretic foundation for the complex type system used in Erlang, particularly focusing on map types with parameters, where no comprehensive semantic subtyping model existed before.", "method": "The authors construct a set-theoretic model for types, including type variables, base types, set-theoretic types, and map types. They define a semantic subtyping relation by using set containment as the formal basis and extend this to cover parametric map types.", "result": "A new formal semantic subtyping relation is established, specifically addressing the challenge of subtyping for parametric map types in Erlang's type system.", "conclusion": "The paper presents a novel semantic model for Erlang map types, providing a sound and formal approach to subtyping, especially for parametric maps, which had not been previously addressed."}}
{"id": "2508.00033", "categories": ["cs.SE", "cs.AI", "cs.CL", "68T50", "I.2.2; I.2.7; D.2.3"], "pdf": "https://arxiv.org/pdf/2508.00033", "abs": "https://arxiv.org/abs/2508.00033", "authors": ["Nuno Fachada", "Daniel Fernandes", "Carlos M. Fernandes", "Bruno D. Ferreira-Saraiva", "Jo\u00e3o P. Matos-Carvalho"], "title": "GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries", "comment": null, "summary": "Large Language Models (LLMs) have advanced rapidly as tools for automating\ncode generation in scientific research, yet their ability to interpret and use\nunfamiliar Python APIs for complex computational experiments remains poorly\ncharacterized. This study systematically benchmarks a selection of\nstate-of-the-art LLMs in generating functional Python code for two increasingly\nchallenging scenarios: conversational data analysis with the \\textit{ParShift}\nlibrary, and synthetic data generation and clustering using \\textit{pyclugen}\nand \\textit{scikit-learn}. Both experiments use structured, zero-shot prompts\nspecifying detailed requirements but omitting in-context examples. Model\noutputs are evaluated quantitatively for functional correctness and prompt\ncompliance over multiple runs, and qualitatively by analyzing the errors\nproduced when code execution fails. Results show that only a small subset of\nmodels consistently generate correct, executable code, with GPT-4.1 standing\nout as the only model to always succeed in both tasks. In addition to\nbenchmarking LLM performance, this approach helps identify shortcomings in\nthird-party libraries, such as unclear documentation or obscure implementation\nbugs. Overall, these findings highlight current limitations of LLMs for\nend-to-end scientific automation and emphasize the need for careful prompt\ndesign, comprehensive library documentation, and continued advances in language\nmodel capabilities.", "AI": {"tldr": "Most LLMs struggle with unfamiliar Python APIs for complex scientific code tasks, except for GPT-4.1. Success depends on both LLM capabilities and library documentation, highlighting areas for improvement in automation for scientific research.", "motivation": "To evaluate how well current large language models (LLMs) can generate correct and functional Python code when working with unfamiliar APIs in scientific contexts, a scenario relevant to automated scientific research.", "method": "The authors benchmark several state-of-the-art LLMs using two challenging tasks: conversational data analysis with the ParShift library, and synthetic data generation plus clustering with pyclugen and scikit-learn. Zero-shot, structured prompts are used with no in-context examples, and model outputs are assessed quantitatively (for correctness and prompt compliance) and qualitatively (by analyzing errors).", "result": "Few LLMs could consistently generate correct, executable code for the specified tasks. Only GPT-4.1 succeeded in every attempt. The evaluation also surfaced issues in the third-party libraries themselves.", "conclusion": "Current LLMs have significant limitations in automated scientific code generation, especially when dealing with unfamiliar or poorly documented APIs. Improved prompt design, better documentation, and further LLM development are needed for reliable scientific automation."}}
{"id": "2508.00534", "categories": ["cs.PL", "cs.CL", "D.3.2; F.3.2; D.3.1"], "pdf": "https://arxiv.org/pdf/2508.00534", "abs": "https://arxiv.org/abs/2508.00534", "authors": ["Mikel Vandeloise"], "title": "Towards a unified framework for programming paradigms: A systematic review of classification formalisms and methodological foundations", "comment": "Preprint submitted to the Journal of Object Technology on July 29,\n  2025. Data available upon request until peer-review is completed", "summary": "The rise of multi-paradigm languages challenges traditional classification\nmethods, leading to practical software engineering issues like interoperability\ndefects. This systematic literature review (SLR) maps the formal foundations of\nprogramming paradigms. Our objective is twofold: (1) to assess the state of the\nart of classification formalisms and their limitations, and (2) to identify the\nconceptual primitives and mathematical frameworks for a more powerful,\nreconstructive approach.\n  Based on a synthesis of 74 primary studies, we find that existing taxonomies\nlack conceptual granularity, a unified formal basis, and struggle with hybrid\nlanguages. In response, our analysis reveals a strong convergence toward a\ncompositional reconstruction of paradigms. This approach identifies a minimal\nset of orthogonal, atomic primitives and leverages mathematical frameworks,\npredominantly Type theory, Category theory and Unifying Theories of Programming\n(UTP), to formally guarantee their compositional properties.\n  We conclude that the literature reflects a significant intellectual shift\naway from classification towards these promising formal, reconstructive\nframeworks. This review provides a map of this evolution and proposes a\nresearch agenda for their unification.", "AI": {"tldr": "Traditional ways to classify programming paradigms are outdated due to the rise of hybrid languages. This literature review highlights a shift toward mathematically sound, compositional frameworks for understanding and unifying programming paradigms, with an emphasis on type theory, category theory, and the unifying theories of programming.", "motivation": "Multi-paradigm programming languages are blurring traditional boundaries, creating practical software engineering challenges such as interoperability issues. There is a need to understand and improve the way programming paradigms are classified and formally described.", "method": "This paper conducts a systematic literature review (SLR) of 74 primary studies to assess the current state of classification formalisms for programming paradigms, synthesize their limitations, and identify foundational conceptual and mathematical approaches for a more robust framework.", "result": "The review finds that current taxonomies are limited by conceptual vagueness, lack of unified formalism, and poor handling of hybrid languages. However, research is moving toward reconstructive frameworks that use minimal, orthogonal primitives and mathematical foundations such as Type theory, Category theory, and UTP, enabling better compositional reasoning.", "conclusion": "There is a major academic shift from static classification of programming paradigms toward compositional, formally grounded frameworks. The paper charts this movement and outlines a research agenda focused on unifying these emerging formal approaches."}}
{"id": "2508.00045", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00045", "abs": "https://arxiv.org/abs/2508.00045", "authors": ["Samah Kansab"], "title": "Machine Learning Pipeline for Software Engineering: A Systematic Literature Review", "comment": null, "summary": "The rapid advancement of software development practices has introduced\nchallenges in ensuring quality and efficiency across the software engineering\n(SE) lifecycle. As SE systems grow in complexity, traditional approaches often\nfail to scale, resulting in longer debugging times, inefficient defect\ndetection, and resource-heavy development cycles. Machine Learning (ML) has\nemerged as a key solution, enabling automation in tasks such as defect\nprediction, code review, and release quality estimation. However, the\neffectiveness of ML in SE depends on the robustness of its pipeline, including\ndata collection, preprocessing, feature engineering, algorithm selection,\nvalidation, and evaluation.\n  This systematic literature review (SLR) examines state-of-the-art ML\npipelines designed for SE, consolidating best practices, challenges, and gaps.\nOur findings show that robust preprocessing, such as SMOTE for data balancing\nand SZZ-based algorithms for feature selection, improves model reliability.\nEnsemble methods like Random Forest and Gradient Boosting dominate performance\nacross tasks, while simpler models such as Naive Bayes remain valuable for\nefficiency and interpretability. Evaluation metrics including AUC, F1-score,\nand precision are most common, with new metrics like Best Arithmetic Mean (BAM)\nemerging in niche applications. Validation techniques such as bootstrapping are\nwidely used to ensure model stability and generalizability.\n  This SLR highlights the importance of well-designed ML pipelines for\naddressing SE challenges and provides actionable insights for researchers and\npractitioners seeking to optimize software quality and efficiency. By\nidentifying gaps and trends, this study sets a foundation for advancing ML\nadoption and fostering innovation in increasingly complex development\nenvironments.", "AI": {"tldr": "This paper reviews recent advances in machine learning pipelines for software engineering, highlighting effective methods, best practices, and evaluation metrics. It finds that robust preprocessing and ensemble models improve outcomes and offers guidance to boost software quality and efficiency using ML.", "motivation": "Software engineering systems have become increasingly complex, making traditional quality assurance and efficiency approaches inadequate. The paper is motivated by the need to address longer debugging times, inefficient defect detection, and high resource usage. It explores how machine learning (ML) can address these challenges in the software engineering lifecycle.", "method": "The paper conducts a systematic literature review (SLR) focused on machine learning pipelines in software engineering. It surveys the literature to consolidate best practices, challenges, and knowledge gaps in the application of ML to software engineering tasks such as defect prediction and code review.", "result": "Robust preprocessing techniques (like SMOTE and SZZ-based feature selection) significantly improve ML model reliability. Ensemble methods (e.g., Random Forest, Gradient Boosting) are top performers across various tasks. While simpler models offer efficiency and interpretability, ensemble models usually yield better predictive performance. Standard evaluation metrics are AUC, F1-score, and precision, but new ones like Best Arithmetic Mean are being explored. Bootstrapping is frequently used to validate model stability and generalizability.", "conclusion": "Well-constructed ML pipelines are crucial for improving software quality and efficiency. The study offers actionable recommendations for optimizing these pipelines, identifies key trends and gaps, and lays the groundwork for future ML adoption and innovation in software engineering."}}
{"id": "2508.00244", "categories": ["cs.SE", "cs.PL", "D.3.2; D.2.11; D.2.13"], "pdf": "https://arxiv.org/pdf/2508.00244", "abs": "https://arxiv.org/abs/2508.00244", "authors": ["Briza Mel Dias de Sousa", "Renato Cordeiro Ferreira", "Alfredo Goldman"], "title": "Functional vs. Object-Oriented: Comparing How Programming Paradigms Affect the Architectural Characteristics of Systems", "comment": "11 pages, 16 figures (1 table, 3 diagrams, 5 graphics, 7 listings),\n  submitted to CTICQS capstone project competition at SBQS 2025", "summary": "After decades of dominance by object-oriented programming (OOP), functional\nprogramming (FP) is gaining increasing attention in the software industry. This\nstudy compares the impact of OOP and FP on the architectural characteristics of\nsoftware systems. For that, it examines the design and implementation of a\nDigital Wallet system, developed in Kotlin (representing OOP) and Scala\n(representing FP). The comparison is made through both qualitative and\nquantitative analyses to explore how each paradigm influences the system's\narchitectural characteristics. The self-ethnographic qualitative analysis\nprovides a side-by-side comparison of both implementations, revealing the\nperspective of those writing such code. The survey-based quantitative analysis\ngathers feedback from developers with diverse backgrounds, showing their\nimpressions of those reading this code. Hopefully, these results may be useful\nfor developers or organizations seeking to make more informed decisions about\nwhich paradigm is best suited for their next project.", "AI": {"tldr": "This paper compares OOP (Kotlin) and FP (Scala) via a Digital Wallet system. Using both qualitative and quantitative methods, it shows how each paradigm impacts software architecture, helping developers pick the best approach for future projects.", "motivation": "Functional programming (FP) is gaining traction after years of object-oriented programming (OOP) dominance. The paper aims to understand how each paradigm affects software architecture, providing insights for more informed development choices.", "method": "The study compares OOP and FP by implementing a Digital Wallet system in Kotlin (OOP) and Scala (FP). It employs qualitative (self-ethnography) and quantitative (surveys) analyses to evaluate the architectural impact of each paradigm.", "result": "Qualitative analysis offers a direct side-by-side comparison of coding experience and design implementation. The quantitative survey reveals how developers with different backgrounds perceive and evaluate the resulting code.", "conclusion": "The findings illustrate distinct influences of OOP and FP on architectural characteristics, aiding developers and organizations in choosing the most suitable paradigm for their needs."}}
{"id": "2508.00083", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00083", "abs": "https://arxiv.org/abs/2508.00083", "authors": ["Yihong Dong", "Xue Jiang", "Jiaru Qian", "Tian Wang", "Kechi Zhang", "Zhi Jin", "Ge Li"], "title": "A Survey on Code Generation with LLM-based Agents", "comment": "Work in progress", "summary": "Code generation agents powered by large language models (LLMs) are\nrevolutionizing the software development paradigm. Distinct from previous code\ngeneration techniques, code generation agents are characterized by three core\nfeatures. 1) Autonomy: the ability to independently manage the entire workflow,\nfrom task decomposition to coding and debugging. 2) Expanded task scope:\ncapabilities that extend beyond generating code snippets to encompass the full\nsoftware development lifecycle (SDLC). 3) Enhancement of engineering\npracticality: a shift in research emphasis from algorithmic innovation toward\npractical engineering challenges, such as system reliability, process\nmanagement, and tool integration. This domain has recently witnessed rapid\ndevelopment and an explosion in research, demonstrating significant application\npotential. This paper presents a systematic survey of the field of LLM-based\ncode generation agents. We trace the technology's developmental trajectory from\nits inception and systematically categorize its core techniques, including both\nsingle-agent and multi-agent architectures. Furthermore, this survey details\nthe applications of LLM-based agents across the full SDLC, summarizes\nmainstream evaluation benchmarks and metrics, and catalogs representative\ntools. Finally, by analyzing the primary challenges, we identify and propose\nseveral foundational, long-term research directions for the future work of the\nfield.", "AI": {"tldr": "This paper surveys the quickly developing field of code generation agents powered by large language models, outlining their evolution, technical approaches, real-world applications, evaluation practices, key tools, and future challenges. It serves as a comprehensive guide and roadmap for researchers and practitioners interested in this transformative technology.", "motivation": "The motivation is to synthesize and clarify the fast-evolving field of LLM-based code generation agents, which are transforming software development by introducing greater autonomy, broader capabilities, and a focus on solving engineering challenges. With rapid growth and increasing practical application, a comprehensive survey is needed to organize current progress and outline future research directions.", "method": "The paper takes a systematic survey approach, tracing the historical development of LLM-based code generation agents, categorizing key techniques (including single-agent and multi-agent systems), reviewing applications throughout the software development lifecycle, summarizing benchmarks and metrics, cataloging representative tools, and analyzing challenges to recommend research directions.", "result": "The study provides a structured overview of how LLM-based agents have evolved, describes core technical categories, details their application across the SDLC, presents mainstream evaluation methods and tools, and identifies both current obstacles and prospective research directions for continued advancement.", "conclusion": "The survey concludes that LLM-based code generation agents are rapidly advancing and hold significant promise for revolutionizing software engineering practices. By systematically categorizing the field and highlighting challenges, the paper establishes a foundation for future research and practical development in this area."}}
{"id": "2508.00508", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.00508", "abs": "https://arxiv.org/abs/2508.00508", "authors": ["Panagiotis Diamantakis", "Thanassis Avgerinos", "Yannis Smaragdakis"], "title": "Desyan: A Platform for Seamless Value-Flow and Symbolic Analysis", "comment": null, "summary": "Over the past two decades, two different types of static analyses have\nemerged as dominant paradigms both in academia and industry: value-flow\nanalysis (e.g., data-flow analysis or points-to analysis) and symbolic analysis\n(e.g., symbolic execution). Despite their individual successes in numerous\napplication fields, the two approaches have remained largely separate; an\nartifact of the simple reality that there is no broadly adopted unifying\nplatform for effortless and efficient integration of symbolic techniques with\nhigh-performance data-flow reasoning.\n  To bridge this gap, we introduce Desyan: a platform for writing program\nanalyses with seamless integration of value-flow and symbolic reasoning. Desyan\nexpands a production-ready Datalog fixpoint engine (Souffl\\'e) with\nfull-fledged SMT solving invoking industry-leading SMT engines. Desyan provides\nconstructs for automatically (and efficiently!) handling typical patterns that\ncome up in program analysis. At the same time, the integration is agnostic with\nrespect to the solving technology, and supports Datalog-native symbolic\nreasoning, via a bottom-up algebraic reasoning module.\n  The result is an engine that allows blending different kinds of reasoning, as\nneeded for the underlying analysis. For value-flow analysis, the engine is the\nbest-in-class Datalog evaluator (often by a factor of over 20x in execution\ntime); for applications that require full SMT (e.g., a concolic execution\nengine or other symbolic evaluator that needs to solve arbitrarily complex\nconditions), the engine is leveraging the leading SMT solvers; for lightweight\nsymbolic evaluation (e.g., solving simple conditionals in the context of a\npath-sensitive analysis), the engine can use Datalog-native symbolic reasoning,\nachieving large speedups (often of over 2x) compared to eagerly appealing to an\nSMT solver.", "AI": {"tldr": "Desyan is a new program analysis platform that effectively combines high-performance value-flow analysis with advanced symbolic reasoning (including SMT solving), delivering significant speedups and offering flexible integration of analysis techniques.", "motivation": "There are two dominant types of static program analyses: value-flow analysis (like data-flow or points-to analysis) and symbolic analysis (like symbolic execution). Despite each method's success, they remain separated due to the lack of a unified platform that efficiently integrates both high-performance data-flow with symbolic reasoning.", "method": "The authors introduce Desyan, a platform that integrates value-flow and symbolic reasoning. Desyan enhances the Souffl\u00e9 Datalog engine with support for SMT solving and offers constructs for efficiently handling common program analysis patterns. It supports multiple modes: high-performance Datalog evaluation, integration with external SMT solvers for complex symbolic reasoning, and fast, Datalog-native symbolic reasoning for simpler cases.", "result": "Desyan enables seamless blending of value-flow and symbolic reasoning. It achieves state-of-the-art performance for value-flow analyses, with speedups over 20x, and approaches to symbolic reasoning that are significantly faster (over 2x) than always resorting to SMT solvers. It provides efficiency and flexibility for complex program analyses requiring both analytic paradigms.", "conclusion": "Desyan successfully bridges the gap between value-flow and symbolic analysis, providing a powerful, efficient, and flexible platform for advanced program analysis tasks. The approach leverages the strengths of both paradigms and offers substantial performance improvements."}}
{"id": "2508.00128", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00128", "abs": "https://arxiv.org/abs/2508.00128", "authors": ["Md Nazmul Haque", "Hua Yang", "Zhou Yang", "Bowen Xu"], "title": "How Quantization Impacts Privacy Risk on LLMs for Code?", "comment": null, "summary": "Large language models for code (LLMs4Code) rely heavily on massive training\ndata, including sensitive data, such as cloud service credentials of the\nprojects and personal identifiable information of the developers, raising\nserious privacy concerns. Membership inference (MI) has recently emerged as an\neffective tool for assessing privacy risk by identifying whether specific data\nbelong to a model's training set. In parallel, model compression techniques,\nespecially quantization, have gained traction for reducing computational costs\nand enabling the deployment of large models. However, while quantized models\nstill retain knowledge learned from the original training data, it remains\nunclear whether quantization affects their ability to retain and expose privacy\ninformation. Answering this question is of great importance to understanding\nprivacy risks in real-world deployments. In this work, we conduct the first\nempirical study on how quantization influences task performance and privacy\nrisk simultaneously in LLMs4Code. To do this, we implement widely used\nquantization techniques (static and dynamic) to three representative model\nfamilies, namely Pythia, CodeGen, and GPTNeo. Our results demonstrate that\nquantization has a significant impact on reducing the privacy risk relative to\nthe original model. We also uncover a positive correlation between task\nperformance and privacy risk, indicating an underlying tradeoff. Moreover, we\nreveal the possibility that quantizing larger models could yield better balance\nthan using full-precision small models. Finally, we demonstrate that these\nfindings generalize across different architectures, model sizes and MI methods,\noffering practical guidance for safeguarding privacy when deploying compressed\nLLMs4Code.", "AI": {"tldr": "This paper shows that quantization (compressing) large language models for code both lowers privacy risk (membership inference) and maintains good task performance, revealing a tradeoff between the two. Larger, quantized models can be a better choice than smaller, uncompressed ones for balancing privacy and utility.", "motivation": "Large language models for code (LLMs4Code) are trained on massive datasets that can include sensitive information, raising significant privacy concerns. As quantization is widely used to compress these models for efficient deployment, it is crucial to understand whether such compression methods impact the models' tendency to leak training data, assessed via membership inference attacks.", "method": "The authors conduct the first empirical study on the impact of quantization on both task performance and privacy risk in LLMs4Code. They apply static and dynamic quantization techniques to three representative model families (Pythia, CodeGen, GPTNeo), and evaluate the models using membership inference attacks to assess privacy risks. The study examines the relationship between quantization level, model size, performance, and privacy across different architectures and MI methods.", "result": "Quantization significantly reduces the privacy risk of LLMs4Code compared to the original full-precision models. The study finds a positive correlation between model performance and privacy risk, highlighting a tradeoff. Larger, quantized models can strike a better balance between performance and privacy than smaller, full-precision models. These results generalize across architectures, sizes, and MI attack methods.", "conclusion": "Quantization not only provides computational benefits but also reduces the privacy risk in LLMs4Code. Model compression thus supports safer deployment in privacy-sensitive environments, but there is a tradeoff between maintaining model performance and minimizing privacy leakage. Careful model selection and quantization strategies can optimize both."}}
{"id": "2508.00772", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.00772", "abs": "https://arxiv.org/abs/2508.00772", "authors": ["Md Imranur Rahman Akib", "Fathima Binthe Muhammed", "Umit Saha", "Md Fazlul Karim Patwary", "Mehrin Anannya", "Md Alomgeer Hussein", "Md Biplob Hosen"], "title": "From Code to Career: Assessing Competitive Programmers for Industry Placement", "comment": null, "summary": "In today's fast-paced tech industry, there is a growing need for tools that\nevaluate a programmer's job readiness based on their coding performance. This\nstudy focuses on predicting the potential of Codeforces users to secure various\nlevels of software engineering jobs. The primary objective is to analyze how a\nuser's competitive programming activity correlates with their chances of\nobtaining positions, ranging from entry-level roles to jobs at major tech\ncompanies. We collect user data using the Codeforces API, process key\nperformance metrics, and build a prediction model using a Random Forest\nclassifier. The model categorizes users into four levels of employability,\nranging from those needing further development to those ready for top-tier tech\njobs. The system is implemented using Flask and deployed on Render for\nreal-time predictions. Our evaluation demonstrates that the approach\neffectively distinguishes between different skill levels based on coding\nproficiency and participation. This work lays a foundation for the use of\nmachine learning in career assessment and could be extended to predict job\nreadiness in broader technical fields.", "AI": {"tldr": "This paper presents a machine learning-driven system that predicts a programmer's job readiness using Codeforces data. By analyzing coding metrics and applying a Random Forest classifier, the model effectively categorizes users into employability levels. This approach proves reliable and could help automate career assessments in tech.", "motivation": "There is a demand for tools that objectively assess a programmer's job readiness using data from their coding performance, given the competitive and rapidly evolving tech industry.", "method": "The study collects user activity data from Codeforces using its API, extracts key performance metrics, and applies a Random Forest classifier to predict users' employability across four levels. The model is built into a web-based system using Flask and hosted on Render for real-time predictions.", "result": "The model successfully classifies users into different employability tiers, effectively differentiating skill levels based on coding activity and proficiency. The system provides real-time assessment to predict the likelihood of securing jobs at varying tiers of the tech industry.", "conclusion": "Machine learning models, specifically Random Forests, can reliably evaluate and distinguish programmer job readiness using competitive programming data, setting the stage for broader applications in technical career assessment."}}
{"id": "2508.00198", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00198", "abs": "https://arxiv.org/abs/2508.00198", "authors": ["Cleyton Magalhaes", "Italo Santos", "Brody Stuart-Verner", "Ronnie de Souza Santos"], "title": "Testing the Untestable? An Empirical Study on the Testing Process of LLM-Powered Software Systems", "comment": null, "summary": "Background: Software systems powered by large language models are becoming a\nroutine part of everyday technologies, supporting applications across a wide\nrange of domains. In software engineering, many studies have focused on how\nLLMs support tasks such as code generation, debugging, and documentation.\nHowever, there has been limited focus on how full systems that integrate LLMs\nare tested during development. Aims: This study explores how LLM-powered\nsystems are tested in the context of real-world application development.\nMethod: We conducted an exploratory case study using 99 individual reports\nwritten by students who built and deployed LLM-powered applications as part of\na university course. Each report was independently analyzed using thematic\nanalysis, supported by a structured coding process. Results: Testing strategies\ncombined manual and automated methods to evaluate both system logic and model\nbehavior. Common practices included exploratory testing, unit testing, and\nprompt iteration. Reported challenges included integration failures,\nunpredictable outputs, prompt sensitivity, hallucinations, and uncertainty\nabout correctness. Conclusions: Testing LLM-powered systems required\nadaptations to traditional verification methods, blending source-level\nreasoning with behavior-aware evaluations. These findings provide evidence on\nthe practical context of testing generative components in software systems.", "AI": {"tldr": "Real-world testing of LLM-powered software blends manual and automated techniques, faces unique challenges (like hallucinations and prompt sensitivity), and demands new verification approaches beyond traditional methods.", "motivation": "While LLMs are increasingly used in software systems for various tasks, there has been little research on how such systems are tested in real-world development, especially at the full system level.", "method": "An exploratory case study was conducted, analyzing 99 reports from students who developed and deployed LLM-powered applications within a university course. Thematic analysis and structured coding were used to assess the testing approaches described in these reports.", "result": "Developers used a mix of manual and automated testing strategies, including exploratory testing, unit testing, and prompt iteration, to test both traditional system logic and the unique behavior of LLMs. Common challenges included integration issues, unpredictable model outputs, prompt sensitivity, hallucinations, and difficulty verifying correctness.", "conclusion": "Testing LLM-based systems requires rethinking traditional verification by combining code-level checks with assessments of LLM behavior. The study sheds light on practical strategies and challenges in testing generative AI within software systems."}}
{"id": "2508.00253", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00253", "abs": "https://arxiv.org/abs/2508.00253", "authors": ["Moumita Asad", "Rafed Muhammad Yasir", "Armin Geramirad", "Sam Malek"], "title": "Leveraging Large Language Model for Information Retrieval-based Bug Localization", "comment": null, "summary": "Information Retrieval-based Bug Localization aims to identify buggy source\nfiles for a given bug report. While existing approaches -- ranging from vector\nspace models to deep learning models -- have shown potential in this domain,\ntheir effectiveness is often limited by the vocabulary mismatch between bug\nreports and source code. To address this issue, we propose a novel Large\nLanguage Model (LLM) based bug localization approach, called GenLoc. Given a\nbug report, GenLoc leverages an LLM equipped with code-exploration functions to\niteratively analyze the code base and identify potential buggy files. To gather\nbetter context, GenLoc may optionally retrieve semantically relevant files\nusing vector embeddings. GenLoc has been evaluated on over 9,000 real-world bug\nreports from six large-scale Java projects. Experimental results show that\nGenLoc outperforms five state-of-the-art bug localization techniques across\nmultiple metrics, achieving an average improvement of more than 60\\% in\nAccuracy@1.", "AI": {"tldr": "GenLoc, an LLM-driven bug localization system, significantly surpasses existing methods in accuracy by intelligently analyzing and exploring code, tackling a major challenge in the field.", "motivation": "Existing Information Retrieval-based bug localization techniques struggle due to vocabulary mismatches between bug reports and source code, limiting their effectiveness.", "method": "The paper proposes GenLoc, an LLM-based bug localization approach. GenLoc leverages a Large Language Model with code-exploration functions to iteratively analyze the codebase and identify buggy files. It can also retrieve semantically relevant files using vector embeddings for better context.", "result": "GenLoc was evaluated on 9,000+ real-world bug reports from six large-scale Java projects. It consistently outperformed five state-of-the-art techniques, with more than 60% improvement in Accuracy@1 across multiple metrics.", "conclusion": "GenLoc, the proposed LLM-based approach, effectively addresses vocabulary mismatch problems in bug localization and achieves notable accuracy improvements compared to leading methods."}}
{"id": "2508.00255", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00255", "abs": "https://arxiv.org/abs/2508.00255", "authors": ["Boqi Chen", "Ou Wei", "Bingzhou Zheng", "Gunter Mussbacher"], "title": "Accurate and Consistent Graph Model Generation from Text with Large Language Models", "comment": "Accepted at ACM / IEEE 28th International Conference on Model Driven\n  Engineering Languages and Systems (MODELS 2025)", "summary": "Graph model generation from natural language description is an important task\nwith many applications in software engineering. With the rise of large language\nmodels (LLMs), there is a growing interest in using LLMs for graph model\ngeneration. Nevertheless, LLM-based graph model generation typically produces\npartially correct models that suffer from three main issues: (1) syntax\nviolations: the generated model may not adhere to the syntax defined by its\nmetamodel, (2) constraint inconsistencies: the structure of the model might not\nconform to some domain-specific constraints, and (3) inaccuracy: due to the\ninherent uncertainty in LLMs, the models can include inaccurate, hallucinated\nelements. While the first issue is often addressed through techniques such as\nconstraint decoding or filtering, the latter two remain largely unaddressed.\nMotivated by recent self-consistency approaches in LLMs, we propose a novel\nabstraction-concretization framework that enhances the consistency and quality\nof generated graph models by considering multiple outputs from an LLM. Our\napproach first constructs a probabilistic partial model that aggregates all\ncandidate outputs and then refines this partial model into the most appropriate\nconcrete model that satisfies all constraints. We evaluate our framework on\nseveral popular open-source and closed-source LLMs using diverse datasets for\nmodel generation tasks. The results demonstrate that our approach significantly\nimproves both the consistency and quality of the generated graph models.", "AI": {"tldr": "The paper proposes an abstraction-concretization framework that aggregates and refines multiple LLM outputs for graph model generation, significantly improving consistency and quality beyond standard LLM approaches.", "motivation": "LLM-based graph model generation suffers from issues of syntax violations, constraint inconsistencies, and inaccuracies due to inherent uncertainty in large language models. While syntax issues can often be addressed, problems with constraint violations and inaccuracies remain unsolved, motivating the need for a new approach.", "method": "The authors propose a novel abstraction-concretization framework. This framework aggregates multiple candidate outputs from an LLM into a probabilistic partial model and then refines it into a concrete model that satisfies all domain constraints, improving consistency and quality.", "result": "Experimental evaluation over several LLMs and datasets shows that this framework significantly enhances the consistency and overall quality of automatically generated graph models compared to standard LLM-based approaches.", "conclusion": "The abstraction-concretization framework effectively addresses constraint inconsistencies and inaccuracies in LLM-generated graph models, yielding more reliable and higher-quality outputs."}}
{"id": "2508.00408", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00408", "abs": "https://arxiv.org/abs/2508.00408", "authors": ["Dong Huang", "Jie M. Zhang", "Mark Harman", "Qianru Zhang", "Mingzhe Du", "See-Kiong Ng"], "title": "Benchmarking LLMs for Unit Test Generation from Real-World Functions", "comment": "Under Review", "summary": "Recently, large language models (LLMs) have shown great promise in automating\nunit test generation, significantly reducing the manual effort required by\ndevelopers. To effectively evaluate the capabilities of LLMs in this domain, it\nis crucial to have a well-designed benchmark that accurately reflects\nreal-world scenarios and mitigates common pitfalls. Existing LLM test\ngeneration benchmarks are limited by two critical drawbacks: data contamination\nand structurally simple function code. As a result, we often cannot rely on the\nvalidity of scientific conclusions drawn from empirical studies using these\nlimited benchmarks. The empirical evidence presented may be biased due to\ncontamination and may fail to generalize beyond toy programs due to structural\nsimplicity.\n  To address these problems, we introduce ULT (UnLeakedTestbench), a new\nbenchmark specifically designed for function-level unit test generation from\nreal-world Python functions. ULT is constructed through a multi-stage curation\nprocess that ensures high cyclomatic complexity and mitigates test case\ncontamination. With 3,909 carefully selected function-level tasks, ULT provides\na more realistic and challenging evaluation of LLMs' test generation\ncapabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT\nwith leaked tests designed to enable a controlled analysis of memorization\nversus reasoning in test generation. Our evaluation results demonstrate that\nULT is significantly more challenging. For example, test cases generated by\nLLMs only achieve 41.32\\%, 45.10\\%, 30.22\\%, and 40.21\\% for accuracy,\nstatement coverage, branch coverage, and mutation score on average for all\nLLMs, respectively. These results are substantially lower than the\ncorresponding metrics on TestEval (91.79\\%, 92.18\\%, 82.04\\%, and 49.69\\%) and\nPLT (47.07\\%, 55.13\\%, 40.07\\%, and 50.80\\%).", "AI": {"tldr": "The paper introduces ULT, a new, realistic benchmark for unit test generation using LLMs, and shows that LLMs perform much worse on it than on existing benchmarks, revealing prior overestimations of LLM capabilities due to simpler, contaminated datasets.", "motivation": "Large language models (LLMs) have potential to automate unit test generation, but there is a need for reliable benchmarks that reflect real-world complexity and avoid biases such as data contamination and overly simple code.", "method": "The authors introduce ULT (UnLeakedTestbench), a new benchmark for unit test generation from real-world Python functions, created through a multi-stage curation process to ensure higher code complexity and avoid test-case contamination. They also provide PLT (PreLeakedTestbench), a paired benchmark with intentionally leaked tests, for analyzing LLM memorization versus reasoning capabilities. The performance of LLMs is evaluated across ULT, PLT, and existing benchmarks.", "result": "LLMs achieve significantly lower performance on ULT (e.g., 41.32% accuracy, 45.10% statement coverage) compared to TestEval and PLT, demonstrating that ULT is more challenging and that previous benchmarks may overestimate LLM capabilities due to contamination and simplicity.", "conclusion": "ULT provides a more challenging and realistic benchmark for evaluating LLMs' test generation abilities, highlighting the limitations of earlier benchmarks and emphasizing the need for robust, contamination-free evaluation environments."}}
{"id": "2508.00462", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00462", "abs": "https://arxiv.org/abs/2508.00462", "authors": ["Linus Ververs", "Lutz Prechelt"], "title": "Managing Power Gaps as a Topic of Pair Programming Skill: A Grounded Theory", "comment": null, "summary": "Context: Pair Programming as a work mode is used (occasionally or frequently)\nthroughout professional software development. Objective: Understand what\npower-related phenomena occur in pair programming as it is used in industry;\ngive advice to practitioners on how to do better pair programming. Method:\nAnalyze 22 industrial pair programming sessions using Grounded Theory\nMethodology. Formulate a Grounded Theory on power-related behaviors. Run a\nsurvey with 292 participants about that theory. Use it to demonstrate that the\nphenomena are common. Results: Our theory describes the phenomenon of Power\nGap: a perceived difference in participation opportunities. The theory shows\nthe behaviors that create a Power Gap or result from it. Power Gaps tend to\ndamage knowledge transfer, code quality, and process effi ciency. The survey\nresults show that all concepts from our theory are frequent in practice. They\nalso provide more grounding for concepts that are observable only indirectly.\nConclusions: It is a valuable component of pair programming skill to be able to\navoid Power Gaps. Specifically, pair partners need to avoid Hierarchical\nBehavior (which tends to create or increase a Power Gap) and should perform\nenough Equalizing Behavior (which prevents or reduces a Power Gap).", "AI": {"tldr": "Pair programming sessions in industry often display 'Power Gaps'\u2014unequal participation due to power dynamics\u2014which hurt knowledge sharing and code quality. Avoiding hierarchical behaviors and encouraging equal participation are key to improving outcomes.", "motivation": "Pair programming is widely used in professional software development, but there is limited understanding of power dynamics within pairs and how these affect outcomes. The study aims to identify and analyze power-related phenomena to offer practical advice for improving pair programming practices.", "method": "The researchers analyzed 22 industrial pair programming sessions using Grounded Theory Methodology to develop a theory of power-related behaviors. They then conducted a survey with 292 participants to validate the generalizability and frequency of the identified phenomena.", "result": "The study identifies the phenomenon of the 'Power Gap,' which is a perceived difference in participation opportunities between pair members. Power Gaps are associated with negative effects on knowledge transfer, code quality, and process efficiency. The study also differentiates between Hierarchical Behaviors (which create/increase Power Gaps) and Equalizing Behaviors (which prevent/reduce them). Survey results confirm these phenomena are common in real practice.", "conclusion": "To improve pair programming outcomes, practitioners should focus on minimizing Power Gaps. This involves avoiding Hierarchical Behaviors and actively engaging in Equalizing Behaviors to ensure equal participation and reduce the negative impacts associated with Power Gaps."}}
{"id": "2508.00546", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00546", "abs": "https://arxiv.org/abs/2508.00546", "authors": ["Wenchao Gu", "Zongyi Lyu", "Yanlin Wang", "Hongyu Zhang", "Cuiyun Gao", "Michael R. Lyu"], "title": "SPENCER: Self-Adaptive Model Distillation for Efficient Code Retrieval", "comment": null, "summary": "Code retrieval aims to provide users with desired code snippets based on\nusers' natural language queries. With the development of deep learning\ntechnologies, adopting pre-trained models for this task has become mainstream.\nConsidering the retrieval efficiency, most of the previous approaches adopt a\ndual-encoder for this task, which encodes the description and code snippet into\nrepresentation vectors, respectively. However, the model structure of the\ndual-encoder tends to limit the model's performance, since it lacks the\ninteraction between the code snippet and description at the bottom layer of the\nmodel during training. To improve the model's effectiveness while preserving\nits efficiency, we propose a framework, which adopts Self-AdaPtive Model\nDistillation for Efficient CodE Retrieval, named SPENCER. SPENCER first adopts\nthe dual-encoder to narrow the search space and then adopts the cross-encoder\nto improve accuracy. To improve the efficiency of SPENCER, we propose a novel\nmodel distillation technique, which can greatly reduce the inference time of\nthe dual-encoder while maintaining the overall performance. We also propose a\nteaching assistant selection strategy for our model distillation, which can\nadaptively select the suitable teaching assistant models for different\npre-trained models during the model distillation to ensure the model\nperformance. Extensive experiments demonstrate that the combination of\ndual-encoder and cross-encoder improves overall performance compared to solely\ndual-encoder-based models for code retrieval. Besides, our model distillation\ntechnique retains over 98% of the overall performance while reducing the\ninference time of the dual-encoder by 70%.", "AI": {"tldr": "SPENCER improves code retrieval by merging efficient dual-encoders and accurate cross-encoders with adaptive model distillation, significantly boosting performance and efficiency compared to traditional methods.", "motivation": "Traditional code retrieval methods predominantly use dual-encoder models due to their efficiency. However, these models lack deep interaction between code and description, limiting retrieval performance. There is a need to improve effectiveness without sacrificing efficiency.", "method": "The proposed framework, SPENCER, combines a dual-encoder to efficiently narrow the search space with a cross-encoder for increased retrieval accuracy. Additionally, a novel model distillation technique and an adaptive teaching assistant selection strategy are introduced to improve efficiency and maintain high performance.", "result": "Experiments show that SPENCER significantly outperforms models using only dual-encoders for code retrieval. The model distillation technique reduces inference time by 70% and retains over 98% of model performance.", "conclusion": "Combining dual-encoder and cross-encoder architectures with self-adaptive model distillation offers a highly efficient yet accurate code retrieval approach. The adaptive selection of teaching assistant models during distillation ensures robustness across different pre-trained models."}}
{"id": "2508.00593", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00593", "abs": "https://arxiv.org/abs/2508.00593", "authors": ["Shuyao Jiang", "Jiazhen Gu", "Wujie Zheng", "Yangfan Zhou", "Michael R. Lyu"], "title": "Can User Feedback Help Issue Detection? An Empirical Study on a One-billion-user Online Service System", "comment": "Accepted by the 19th ACM/IEEE International Symposium on Empirical\n  Software Engineering and Measurement (ESEM 2025)", "summary": "Background: It has long been suggested that user feedback, typically written\nin natural language by end-users, can help issue detection. However, for\nlarge-scale online service systems that receive a tremendous amount of\nfeedback, it remains a challenging task to identify severe issues from user\nfeedback. Aims: To develop a better feedback-based issue detection approach, it\nis crucial first to gain a comprehensive understanding of the characteristics\nof user feedback in real production systems. Method: In this paper, we conduct\nan empirical study on 50,378,766 user feedback items from six real-world\nservices in a one-billion-user online service system. We first study what users\nprovide in their feedback. We then examine whether certain features of feedback\nitems can be good indicators of severe issues. Finally, we investigate whether\nadopting machine learning techniques to analyze user feedback is reasonable.\nResults: Our results show that a large proportion of user feedback provides\nirrelevant information about system issues. As a result, it is crucial to\nfilter out issue-irrelevant information when processing user feedback.\nMoreover, we find severe issues that cannot be easily detected based solely on\nuser feedback characteristics. Finally, we find that the distributions of the\nfeedback topics in different time intervals are similar. This confirms that\ndesigning machine learning-based approaches is a viable direction for better\nanalyzing user feedback. Conclusions: We consider that our findings can serve\nas an empirical foundation for feedback-based issue detection in large-scale\nservice systems, which sheds light on the design and implementation of\npractical issue detection approaches.", "AI": {"tldr": "Mass user feedback often contains irrelevant information, so effective filtering is essential. Severe issues aren't always obvious from feedback characteristics alone, but stable topic patterns mean machine learning can help. These findings support better feedback-based issue detection in big online systems.", "motivation": "The motivation for this study comes from the challenge of detecting severe issues from massive amounts of natural language user feedback in large-scale online service systems. As user feedback can potentially assist in issue detection, understanding its nature and how it can be effectively analyzed is vital for improving feedback-based detection methods.", "method": "The authors conducted an empirical study using 50,378,766 user feedback items collected from six real-world services within a large online system. They analyzed what users report in their feedback, examined features that might indicate severe issues, and evaluated the feasibility of using machine learning techniques for feedback analysis.", "result": "The study found that much user feedback is irrelevant to system issues, making it crucial to filter out such noise. Some severe issues are not easily identified from feedback characteristics alone. They also observed that topic distributions in feedback are consistent over time, suggesting that machine learning approaches for detecting issues from feedback are feasible.", "conclusion": "Their empirical findings lay the groundwork for future feedback-based issue detection systems in large-scale service environments. The insights provided guide the development of more practical and robust detection approaches."}}
{"id": "2508.00630", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00630", "abs": "https://arxiv.org/abs/2508.00630", "authors": ["Khaled Ahmed", "Jialing Song", "Boqi Chen", "Ou Wei", "Bingzhou Zheng"], "title": "MCeT: Behavioral Model Correctness Evaluation using Large Language Models", "comment": "MODELS 2025", "summary": "Behavioral model diagrams, e.g., sequence diagrams, are an essential form of\ndocumentation that are typically designed by system engineers from requirements\ndocumentation, either fully manually or assisted by design tools. With the\ngrowing use of Large Language Models (LLM) as AI modeling assistants, more\nautomation will be involved in generating diagrams. This necessitates the\nadvancement of automatic model correctness evaluation tools. Such a tool can be\nused to evaluate both manually and AI automatically generated models; to\nprovide feedback to system engineers, and enable AI assistants to self-evaluate\nand self-enhance their generated models.\n  In this paper, we propose MCeT, the first fully automated tool to evaluate\nthe correctness of a behavioral model, sequence diagrams in particular, against\nits corresponding requirements text and produce a list of issues that the model\nhas. We utilize LLMs for the correctness evaluation tasks as they have shown\noutstanding natural language understanding ability. However, we show that\ndirectly asking an LLM to compare a diagram to requirements finds less than 35%\nof issues that experienced engineers can find. We propose to supplement the\ndirect check with a fine-grained, multi-perspective approach; we split the\ndiagram into atomic, non-divisible interactions, and split the requirements\ntext into atomic, self-contained items. We compare the diagram with atomic\nrequirements and each diagram-atom with the requirements. We also propose a\nself-consistency checking approach that combines perspectives to mitigate LLM\nhallucinated issues. Our combined approach improves upon the precision of the\ndirect approach from 0.58 to 0.81 in a dataset of real requirements. Moreover,\nthe approach finds 90% more issues that the experienced engineers found than\nthe direct approach, and reports an average of 6 new issues per diagram.", "AI": {"tldr": "The paper introduces MCeT, an automated tool using LLMs for more accurate evaluation of sequence diagrams against requirements. By breaking down the process into atomic comparisons and applying checks for consistency, MCeT detects more issues and offers clear improvements for both human engineers and AI assistants.", "motivation": "With the rise of Large Language Models (LLMs) as AI assistants in generating behavioral model diagrams, there is a growing need for automated tools to evaluate the correctness of such models compared to requirements documentation. Existing manual and automatic (LLM-based) evaluations are insufficient, often missing many issues.", "method": "The authors propose MCeT, an automated tool that evaluates the correctness of sequence diagrams against requirement texts using LLMs. Their method involves splitting both the diagrams and requirements into atomic elements, performing fine-grained, multi-perspective comparisons, and implementing a self-consistency check to reduce hallucinated issues by LLMs.", "result": "MCeT substantially improves the evaluation process: precision increases from 0.58 to 0.81 over the direct LLM-based approach. The combined approach detects 90% more issues found by experienced engineers, and on average, identifies 6 additional new issues per diagram.", "conclusion": "MCeT is an effective, fully automated tool for evaluating the correctness of behavioral sequence diagrams against textual requirements. By decomposing models and requirements into atomic elements and employing self-consistency checks, MCeT significantly outperforms direct LLM evaluation, aiding both engineers and LLM-based modeling assistants."}}
{"id": "2508.00700", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00700", "abs": "https://arxiv.org/abs/2508.00700", "authors": ["Alfred Santa Molison", "Marcia Moraes", "Glaucia Melo", "Fabio Santos", "Wesley K. G. Assuncao"], "title": "Is LLM-Generated Code More Maintainable \\& Reliable than Human-Written Code?", "comment": "Accepted ESEM2025", "summary": "Background: The rise of Large Language Models (LLMs) in software development\nhas opened new possibilities for code generation. Despite the widespread use of\nthis technology, it remains unclear how well LLMs generate code solutions in\nterms of software quality and how they compare to human-written code. Aims:\nThis study compares the internal quality attributes of LLM-generated and\nhuman-written code. Method: Our empirical study integrates datasets of coding\ntasks, three LLM configurations (zero-shot, few-shot, and fine-tuning), and\nSonarQube to assess software quality. The dataset comprises Python code\nsolutions across three difficulty levels: introductory, interview, and\ncompetition. We analyzed key code quality metrics, including maintainability\nand reliability, and the estimated effort required to resolve code issues.\nResults: Our analysis shows that LLM-generated code has fewer bugs and requires\nless effort to fix them overall. Interestingly, fine-tuned models reduced the\nprevalence of high-severity issues, such as blocker and critical bugs, and\nshifted them to lower-severity categories, but decreased the model's\nperformance. In competition-level problems, the LLM solutions sometimes\nintroduce structural issues that are not present in human-written code.\nConclusion: Our findings provide valuable insights into the quality of\nLLM-generated code; however, the introduction of critical issues in more\ncomplex scenarios highlights the need for a systematic evaluation and\nvalidation of LLM solutions. Our work deepens the understanding of the\nstrengths and limitations of LLMs for code generation.", "AI": {"tldr": "LLMs generate code with generally fewer bugs and lower fix effort, but in complex scenarios, can introduce serious structural problems. Careful evaluation is essential when using LLMs for advanced coding tasks.", "motivation": "The adoption of Large Language Models (LLMs) for code generation is growing, but it is unclear how the quality of code generated by LLMs compares to that written by humans, specifically regarding software quality attributes.", "method": "An empirical study was conducted that used datasets containing Python code solutions (across introductory, interview, and competition difficulty levels), compared three LLM configurations (zero-shot, few-shot, and fine-tuning), and employed SonarQube for quality assessment. Key metrics measured included maintainability, reliability, and the effort required to address code issues.", "result": "LLM-generated code had fewer bugs and required less corrective effort overall. Fine-tuning LLMs reduced high-severity code issues but lowered overall performance. For competition-level problems, LLM solutions sometimes introduced unique structural issues not present in human code.", "conclusion": "LLMs can generate code with good quality, often introducing fewer bugs and less effort to fix them. However, for complex tasks, LLMs may introduce critical issues, emphasizing the need for thorough evaluation. This study highlights both the strengths and the limitations of LLM-based code generation."}}
{"id": "2508.00738", "categories": ["cs.SE", "cs.FL", "68N30", "D.2.4"], "pdf": "https://arxiv.org/pdf/2508.00738", "abs": "https://arxiv.org/abs/2508.00738", "authors": ["Bernhard Rumpe", "Max Stachon", "Sebastian St\u00fcber", "Valdes Voufo"], "title": "Tool-Assisted Conformance Checking to Reference Process Models", "comment": null, "summary": "Reference models convey best practices and standards. The reference\nframeworks necessitate conformance checks to ensure adherence to established\nguidelines and principles, which is crucial for maintaining quality and\nconsistency in various processes. This paper explores automated conformance\nchecks for concrete process models against reference models using causal\ndependency analysis of tasks and events. Existing notions of conformance\nchecking for process models focus on verifying process execution traces and\nlack the expressiveness and automation needed for semantic model comparison,\nleaving this question unresolved. We integrate our approach into a broader\nsemantic framework for defining reference model conformance. We outline an\nalgorithm for reference process model conformance checking, evaluate it through\na case study, and discuss its strengths and limitations. Our research provides\na tool-assisted solution enhancing accuracy and flexibility in process model\nconformance verification.", "AI": {"tldr": "This paper proposes and tests an automated, semantics-based tool for verifying process models\u2019 adherence to reference models, improving on previous, less expressive methods.", "motivation": "Reference models are important for maintaining best practices and standards in processes, but current methods for checking if other models conform to these references often lack automation and semantic expressiveness. There is a need for improved ways to verify conformance that are both accurate and efficient.", "method": "The authors developed an algorithm based on causal dependency analysis to compare process models semantically, integrated it within a broader framework, and performed a case study to evaluate its effectiveness.", "result": "The paper presents an automated method for checking conformance of process models against reference models using causal dependency analysis. It is integrated into a semantic framework and validated through a case study, demonstrating improvements in flexibility and accuracy.", "conclusion": "The approach enhances the accuracy and flexibility of conformance checking between process models and standards, representing progress over previous methods mainly focused on execution traces. The research delivers practical, tool-supported verification."}}
{"id": "2508.00749", "categories": ["cs.SE", "cs.FL", "cs.SC", "68N30", "D.2.4"], "pdf": "https://arxiv.org/pdf/2508.00749", "abs": "https://arxiv.org/abs/2508.00749", "authors": ["Johanna Grahl", "Bernhard Rumpe", "Max Stachon", "Sebastian St\u00fcber"], "title": "Dynamic Symbolic Execution for Semantic Difference Analysis of Component and Connector Architectures", "comment": null, "summary": "In the context of model-driven development, ensuring the correctness and\nconsistency of evolving models is paramount. This paper investigates the\napplication of Dynamic Symbolic Execution (DSE) for semantic difference\nanalysis of component-and-connector architectures, specifically utilizing\nMontiArc models. We have enhanced the existing MontiArc-to-Java generator to\ngather both symbolic and concrete execution data at runtime, encompassing\ntransition conditions, visited states, and internal variables of automata. This\ndata facilitates the identification of significant execution traces that\nprovide critical insights into system behavior. We evaluate various execution\nstrategies based on the criteria of runtime efficiency, minimality, and\ncompleteness, establishing a framework for assessing the applicability of DSE\nin semantic difference analysis. Our findings indicate that while DSE shows\npromise for analyzing component and connector architectures, scalability\nremains a primary limitation, suggesting further research is needed to enhance\nits practical utility in larger systems.", "AI": {"tldr": "Dynamic Symbolic Execution can uncover important differences in evolving component architectures but currently struggles with scalability, highlighting the need for further work to make it practical for larger systems.", "motivation": "The motivation of the paper is to ensure the correctness and consistency of evolving models in model-driven development, focusing on the challenge of semantic difference analysis for component-and-connector architectures.", "method": "The authors use Dynamic Symbolic Execution (DSE) to analyze the semantic differences in MontiArc models. They enhance the MontiArc-to-Java generator to collect symbolic and concrete execution data during runtime, such as transition conditions, visited states, and internal variables. They then use this data to identify execution traces and evaluate multiple execution strategies.", "result": "The results show that DSE provides valuable insights and is promising for semantic difference analysis in component-and-connector architectures. However, scalability is a significant challenge, limiting its use in larger systems.", "conclusion": "The paper concludes that DSE is a promising technique for semantic difference analysis in component-and-connector architectures, but further research is needed to address scalability issues for application in larger models."}}
