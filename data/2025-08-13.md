<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 8]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Context Engineering for Multi-Agent LLM Code Assistants Using Elicit, NotebookLM, ChatGPT, and Claude Code](https://arxiv.org/abs/2508.08322)
*Muhammad Haseeb*

Main category: cs.SE

TL;DR: This paper proposes a multi-agent workflow for LLM-based code generation, leveraging intent clarification, semantic retrieval, and agent orchestration to address context limitations. The system markedly improves code assistant performance on complex software projects, surpassing existing frameworks and offering practical guidance for production deployment.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex, multi-file software projects due to context limitations and knowledge gaps. This limits their practical usefulness for real-world code generation and engineering tasks.

Method: The paper proposes a workflow that integrates several specialized AI components: a GPT-5 Intent Translator for requirement clarification, Elicit-derived semantic retrieval for domain knowledge, NotebookLM for document synthesis, and a Claude Code multi-agent system for code generation and validation. These components are orchestrated via Claude's agent framework using intent clarification, context injection, and agent role decomposition.

Result: Experiments on a large Next.js codebase show the multi-agent system can accurately plan, edit, and test complex features, requiring minimal human input. The approach demonstrates higher single-shot success rates and better context adherence compared to single-agent baselines, and outperforms recent systems like CodePlan, MASAI, and HyperAgent.

Conclusion: Combining multiple specialized AI agents for context engineering in code generation leads to significant improvements in the reliability and accuracy of LLM-based coding assistants. The system sets new standards for single-shot code generation in complex projects, providing practical insights for future deployment of LLM assistants.

Abstract: Large Language Models (LLMs) have shown promise in automating code generation
and software engineering tasks, yet they often struggle with complex,
multi-file projects due to context limitations and knowledge gaps. We propose a
novel context engineering workflow that combines multiple AI components: an
Intent Translator (GPT-5) for clarifying user requirements, an Elicit-powered
semantic literature retrieval for injecting domain knowledge, NotebookLM-based
document synthesis for contextual understanding, and a Claude Code multi-agent
system for code generation and validation. Our integrated approach leverages
intent clarification, retrieval-augmented generation, and specialized
sub-agents orchestrated via Claude's agent framework. We demonstrate that this
method significantly improves the accuracy and reliability of code assistants
in real-world repositories, yielding higher single-shot success rates and
better adherence to project context than baseline single-agent approaches.
Qualitative results on a large Next.js codebase show the multi-agent system
effectively plans, edits, and tests complex features with minimal human
intervention. We compare our system with recent frameworks like CodePlan,
MASAI, and HyperAgent, highlighting how targeted context injection and agent
role decomposition lead to state-of-the-art performance. Finally, we discuss
the implications for deploying LLM-based coding assistants in production, along
with lessons learned on context management and future research directions.

</details>


### [2] [Energy-Aware Code Generation with LLMs: Benchmarking Small vs. Large Language Models for Sustainable AI Programming](https://arxiv.org/abs/2508.08332)
*Humza Ashraf,Syed Muhammad Danish,Aris Leivadeas,Yazan Otoum,Zeeshan Sattar*

Main category: cs.SE

TL;DR: SLMs are generally more energy-efficient than LLMs for code generation when outputs are correct, though LLMs are more accurate overall.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) such as ChatGPT are popular for code generation but are resource-intensive, leading to high energy consumption and environmental concerns. The motivation behind this paper is to explore whether smaller, open-source language models (SLMs) can perform code generation tasks effectively while being more energy-efficient, thus reducing the environmental impact.

Method: The authors evaluated three open-source SLMs and two large commercial LLMs on 150 LeetCode programming problems of varying difficulty. They tested StableCode-3B, StarCoderBase-3B, and Qwen2.5-Coder-3B-Instruct (SLMs) versus GPT-4.0 and DeepSeek-Reasoner (LLMs). The generated Python code was compared to human-written solutions using four metrics: run-time, memory usage, energy consumption, and correctness.

Result: LLMs achieved the highest correctness across all problem difficulties. However, SLMs often generated more energy-efficient code, and in over 52% of the problems, the SLMs used the same or less energy than LLMs when producing correct solutions.

Conclusion: Open-source SLMs can generate programming solutions to LeetCode problems with competitive energy efficiency compared to large commercial LLMs. While LLMs remain more accurate, SLMs offer a viable trade-off when energy consumption is a priority and correctness can be maintained.

Abstract: Large Language Models (LLMs) are widely used for code generation. However,
commercial models like ChatGPT require significant computing power, which leads
to high energy use and carbon emissions. This has raised concerns about their
environmental impact. In this study, we evaluate open-source Small Language
Models (SLMs) trained explicitly for code generation and compare their
performance and energy efficiency against large LLMs and efficient
human-written Python code. The goal is to investigate whether SLMs can match
the performance of LLMs on certain types of programming problems while
producing more energy-efficient code. We evaluate 150 coding problems from
LeetCode, evenly distributed across three difficulty levels: easy, medium, and
hard. Our comparison includes three small open-source models, StableCode-3B,
StarCoderBase-3B, and Qwen2.5-Coder-3B-Instruct, and two large commercial
models, GPT-4.0 and DeepSeek-Reasoner. The generated code is evaluated using
four key metrics: run-time, memory usage, energy consumption, and correctness.
We use human-written solutions as a baseline to assess the quality and
efficiency of the model-generated code. Results indicate that LLMs achieve the
highest correctness across all difficulty levels, but SLMs are often more
energy-efficient when their outputs are correct. In over 52% of the evaluated
problems, SLMs consumed the same or less energy than LLMs.

</details>


### [3] [Improving Merge Pipeline Throughput in Continuous Integration via Pull Request Prioritization](https://arxiv.org/abs/2508.08342)
*Maximilian Jungwirth,Martin Gruber,Gordon Fraser*

Main category: cs.SE

TL;DR: This paper introduces a predictive, build-system-independent approach to prioritize pull requests in software merge pipelines. By using historical and contextual data, it improves throughput over naive ordering strategies, providing scalable and easily integrated optimization for large-scale development teams.


<details>
  <summary>Details</summary>
Motivation: Merge pipelines in large software repositories are essential for maintaining code stability but can become bottlenecks during high load, slowing down feature delivery and reducing development productivity. Existing optimizations are often tied to specific build systems, limiting their broader applicability.

Method: The paper proposes optimizing the order of pull requests (PRs) in merge pipelines using predictions based on historical build data, PR metadata, and contextual information. It prioritizes PRs likely to pass builds during peak hours, aiming for maximum throughput. The method is build-system agnostic and relies on practical data-driven predictions.

Result: Experiments on a real-world, large-scale project show that the predictive PR ordering method significantly outperforms FIFO and non-learning ordering strategies, resulting in higher throughput and easier integration into existing systems due to build-system independence.

Conclusion: Predictive ordering of PRs in merge pipelines, using practical build data and metadata, improves throughput and stability without dependence on specific build systems, making it an effective and easily adoptable optimization for organizations.

Abstract: Integrating changes into large monolithic software repositories is a critical
step in modern software development that substantially impacts the speed of
feature delivery, the stability of the codebase, and the overall productivity
of development teams. To ensure the stability of the main branch, many
organizations use merge pipelines that test software versions before the
changes are permanently integrated. However, the load on merge pipelines is
often so high that they become bottlenecks, despite the use of parallelization.
Existing optimizations frequently rely on specific build systems, limiting
their generalizability and applicability. In this paper we propose to optimize
the order of PRs in merge pipelines using practical build predictions utilizing
only historical build data, PR metadata, and contextual information to estimate
the likelihood of successful builds in the merge pipeline. By dynamically
prioritizing likely passing PRs during peak hours, this approach maximizes
throughput when it matters most. Experiments conducted on a real-world,
large-scale project demonstrate that predictive ordering significantly
outperforms traditional first-in-first-out (FIFO), as well as
non-learning-based ordering strategies. Unlike alternative optimizations, this
approach is agnostic to the underlying build system and thus easily integrable
into existing automated merge pipelines.

</details>


### [4] [OmniLLP: Enhancing LLM-based Log Level Prediction with Context-Aware Retrieval](https://arxiv.org/abs/2508.08545)
*Youssef Esseddiq Ouatiti,Mohammed Sayagh,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: OmniLLP improves LLM-based log level prediction by pulling learning examples from clusters based on code semantics and developer ownership, boosting accuracy by up to 8% AUC and providing better, context-aware logging recommendations.


<details>
  <summary>Details</summary>
Motivation: Log level choices in logging statements greatly impact software observability and performance, but are difficult to automate accurately. Recent ML approaches use large language models but ignore code structure and developer practices, limiting their effectiveness.

Method: OmniLLP clusters source files by semantic similarity and developer ownership. In-context examples for LLM-based log level predictors are then selected from these clusters to improve the coherence and relevance of prompts, thus enhancing prediction accuracy.

Result: Semantic and ownership-aware clustering improves predictor accuracy by up to 8% AUC compared to random in-context examples. Combining both signals achieves between 0.88 and 0.96 AUC, outperforming previous methods on multiple projects.

Conclusion: Incorporating code semantic and developer ownership context into LLM-based log level prediction provides significantly more accurate and contextually relevant predictions, improving software maintainability and observability.

Abstract: Developers insert logging statements in source code to capture relevant
runtime information essential for maintenance and debugging activities. Log
level choice is an integral, yet tricky part of the logging activity as it
controls log verbosity and therefore influences systems' observability and
performance. Recent advances in ML-based log level prediction have leveraged
large language models (LLMs) to propose log level predictors (LLPs) that
demonstrated promising performance improvements (AUC between 0.64 and 0.8).
Nevertheless, current LLM-based LLPs rely on randomly selected in-context
examples, overlooking the structure and the diverse logging practices within
modern software projects. In this paper, we propose OmniLLP, a novel LLP
enhancement framework that clusters source files based on (1) semantic
similarity reflecting the code's functional purpose, and (2) developer
ownership cohesion. By retrieving in-context learning examples exclusively from
these semantic and ownership aware clusters, we aim to provide more coherent
prompts to LLPs leveraging LLMs, thereby improving their predictive accuracy.
Our results show that both semantic and ownership-aware clusterings
statistically significantly improve the accuracy (by up to 8\% AUC) of the
evaluated LLM-based LLPs compared to random predictors (i.e., leveraging
randomly selected in-context examples from the whole project). Additionally,
our approach that combines the semantic and ownership signal for in-context
prediction achieves an impressive 0.88 to 0.96 AUC across our evaluated
projects. Our findings highlight the value of integrating software
engineering-specific context, such as code semantic and developer ownership
signals into LLM-LLPs, offering developers a more accurate, contextually-aware
approach to logging and therefore, enhancing system maintainability and
observability.

</details>


### [5] [Hallucinations in Code Change to Natural Language Generation: Prevalence and Evaluation of Detection Metrics](https://arxiv.org/abs/2508.08661)
*Chunhua Liu,Hong Yi Lin,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: The paper uncovers widespread hallucinations in LLM-generated code reviews and commit messages (up to 50%), and finds that combining multiple detection metrics—especially model confidence and feature attribution—significantly improves hallucination detection rates.


<details>
  <summary>Details</summary>
Motivation: While language models have demonstrated strong performance in software engineering tasks like code generation, their tendency to generate 'hallucinations'—i.e., incorrect or fabricated information—remains a significant problem. Specifically, there has been little exploration of hallucinations in code change tasks where the input-output structures are complex and context-dependent.

Method: This paper conducts the first in-depth analysis of hallucinations in two important code change tasks: commit message generation and code review comment generation. The authors quantify hallucination prevalence in outputs from recent language models and evaluate various metric-based approaches to automatically detect these hallucinations. They test individual metrics and combinations, and pay particular attention to model confidence and feature attribution.

Result: The study reveals that about 50% of generated code reviews and 20% of commit messages contain hallucinations. Individually, common detection metrics are not very effective, but combining metrics—especially utilizing model confidence and feature attribution—significantly enhances hallucination detection.

Conclusion: Hallucinations are highly prevalent in code-change-related natural language tasks performed by large language models. Automatic detection is challenging with standalone metrics, but approaches that combine multiple indicators, particularly model confidence and feature attribution, offer a promising path to effective, inference-time hallucination identification.

Abstract: Language models have shown strong capabilities across a wide range of tasks
in software engineering, such as code generation, yet they suffer from
hallucinations. While hallucinations have been studied independently in natural
language and code generation, their occurrence in tasks involving code changes
which have a structurally complex and context-dependent format of code remains
largely unexplored. This paper presents the first comprehensive analysis of
hallucinations in two critical tasks involving code change to natural language
generation: commit message generation and code review comment generation. We
quantify the prevalence of hallucinations in recent language models and explore
a range of metric-based approaches to automatically detect them. Our findings
reveal that approximately 50\% of generated code reviews and 20\% of generated
commit messages contain hallucinations. Whilst commonly used metrics are weak
detectors on their own, combining multiple metrics substantially improves
performance. Notably, model confidence and feature attribution metrics
effectively contribute to hallucination detection, showing promise for
inference-time detection.\footnote{All code and data will be released upon
acceptance.

</details>


### [6] [Description and Comparative Analysis of QuRE: A New Industrial Requirements Quality Dataset](https://arxiv.org/abs/2508.08868)
*Henning Femmer,Frank Houdek,Max Unterbusch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: The paper introduces and releases QuRE, a large, annotated industrial requirements dataset, addressing the lack of realistic and accessible data in requirements quality research. QuRE is detailed, systematically labeled, and comparable to existing datasets, making it a strong candidate for a community gold standard to advance empirical studies and collaboration in the field.


<details>
  <summary>Details</summary>
Motivation: Empirical research on requirements quality defects is hampered by the lack of large, detailed, accessible, and realistic datasets. Existing datasets are often small, inaccessible, or lack vital context, which reduces the effectiveness and generalizability of research.

Method: The authors introduce QuRE, a new dataset of 2,111 industrial requirements annotated through a real-world review process during five years of industrial use. They release this dataset to the research community and provide descriptive statistics, such as lexical diversity and readability, comparing QuRE to both existing and synthetic datasets.

Result: QuRE is linguistically similar to other existing datasets, but stands out through its detailed context information and systematically created labels from industrial practice. It presents more realistic and representative requirements data than synthetic datasets, with high-quality annotations developed over nearly a decade.

Conclusion: QuRE aims to serve as a common gold standard for requirements quality datasets, fostering transparency, comparability, and empirical rigor, and enabling more sound and collaborative research efforts in software requirements quality.

Abstract: Requirements quality is central to successful software and systems
engineering. Empirical research on quality defects in natural language
requirements relies heavily on datasets, ideally as realistic and
representative as possible. However, such datasets are often inaccessible,
small, or lack sufficient detail. This paper introduces QuRE (Quality in
Requirements), a new dataset comprising 2,111 industrial requirements that have
been annotated through a real-world review process. Previously used for over
five years as part of an industrial contract, this dataset is now being
released to the research community. In this work, we furthermore provide
descriptive statistics on the dataset, including measures such as lexical
diversity and readability, and compare it to existing requirements datasets and
synthetically generated requirements. In contrast to synthetic datasets, QuRE
is linguistically similar to existing ones. However, this dataset comes with a
detailed context description, and its labels have been created and used
systematically and extensively in an industrial context over a period of close
to a decade. Our goal is to foster transparency, comparability, and empirical
rigor by supporting the development of a common gold standard for requirements
quality datasets. This, in turn, will enable more sound and collaborative
research efforts in the field.

</details>


### [7] [Empirical Analysis of Temporal and Spatial Fault Characteristics in Multi-Fault Bug Repositories](https://arxiv.org/abs/2508.08872)
*Dylan Callaghan,Alexandra van der Spuy,Bernd Fischer*

Main category: cs.SE

TL;DR: This paper analyzes how software faults persist and cluster in real-world projects, revealing that multiple, long-lived faults often coexist and are spread out, challenging the assumptions made by popular datasets. This highlights the need to adjust current approaches for software testing and maintenance.


<details>
  <summary>Details</summary>
Motivation: Software maintenance is expensive largely due to the cost of fixing software faults. Existing techniques for reducing these costs require accurate datasets and an understanding of software fault characteristics, yet assumptions about these faults may not hold in real-world projects.

Method: The paper empirically analyzes the temporal (time-based) and spatial (location-based) characteristics of software faults in 16 open-source Java and Python projects, using the Defects4J and BugsInPy datasets.

Result: The study finds that many faults are long-lived and that most software versions have multiple coexisting faults, which contrasts with original dataset assumptions of primarily single-fault versions. Furthermore, faults are present in only a small subset of the system but are fairly evenly distributed among these parts, resulting in few concentrated bug hotspots.

Conclusion: The results suggest that existing assumptions about software faults in popular datasets underestimate both the longevity and the frequency of coexisting faults, indicating a need to revise current methodologies and datasets for more effective testing and evaluation.

Abstract: Fixing software faults contributes significantly to the cost of software
maintenance and evolution. Techniques for reducing these costs require datasets
of software faults, as well as an understanding of the faults, for optimal
testing and evaluation. In this paper, we present an empirical analysis of the
temporal and spatial characteristics of faults existing in 16 open-source Java
and Python projects, which form part of the Defects4J and BugsInPy datasets,
respectively. Our findings show that many faults in these software systems are
long-lived, leading to the majority of software versions having multiple
coexisting faults. This is in contrast to the assumptions of the original
datasets, where the majority of versions only identify a single fault. In
addition, we show that although the faults are found in only a small subset of
the systems, these faults are often evenly distributed amongst this subset,
leading to relatively few bug hotspots.

</details>


### [8] [Toward Automated Hypervisor Scenario Generation Based on VM Workload Profiling for Resource-Constrained Environments](https://arxiv.org/abs/2508.08952)
*Hyunwoo Kim,Jaeseong Lee,Sunpyo Hong,Changmin Han*

Main category: cs.SE

TL;DR: This paper presents an automated tool that optimizes hardware resource allocation for software-defined vehicles using advanced modeling techniques, showing real-world improvements in efficiency and development speed.


<details>
  <summary>Details</summary>
Motivation: The shift towards software-defined vehicles (SDVs) in the automotive industry requires more flexible and efficient resource allocation on shared hardware platforms. However, adapting static vendor configurations to diverse and dynamic workloads is challenging for Tier 1 integrators.

Method: The paper introduces an automated scenario generation framework that profiles the runtime behavior of systems. It incorporates both theoretical models and vendor heuristics to generate optimized hypervisor configurations. Two modeling approaches are compared: domain-guided parametric modeling and deep learning-based modeling, and these models are used as the basis for a resource allocation optimization strategy.

Result: The proposed framework demonstrates its effectiveness in real-world deployments by improving hardware integration efficiency and reducing development time in environments where resources are limited.

Conclusion: The automated framework provides a practical solution for Tier 1 automotive vendors, enabling efficient hardware resource allocation across multiple VMs and addressing the challenges of adapting to evolving SDV requirements.

Abstract: In the automotive industry, the rise of software-defined vehicles (SDVs) has
  driven a shift toward virtualization-based architectures that consolidate
  diverse automotive workloads on a shared hardware platform. To support this
  evolution, chipset vendors provide board support packages (BSPs), hypervisor
  setups, and resource allocation guidelines. However, adapting these static
  configurations to varying system requirements and workloads remain a
  significant challenge for Tier 1 integrators.
  This paper presents an automated scenario generation framework, which helps
  automotive vendors to allocate hardware resources efficiently across multiple
  VMs. By profiling runtime behavior and integrating both theoretical models
and
  vendor heuristics, the proposed tool generates optimized hypervisor
  configurations tailored to system constraints.
  We compare two main approaches for modeling target QoS based on profiled data
  and resource allocation: domain-guided parametric modeling and deep
  learning-based modeling. We further describe our optimization strategy using
  the selected QoS model to derive efficient resource allocations. Finally, we
  report on real-world deployments to demonstrate the effectiveness of our
  framework in improving integration efficiency and reducing development time
in
  resource-constrained environments.

</details>
