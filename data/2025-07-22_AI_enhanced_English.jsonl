{"id": "2507.14256", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14256", "abs": "https://arxiv.org/abs/2507.14256", "authors": ["Jakub Walczak", "Piotr Tomalak", "Artur Laskowski"], "title": "Impact of Code Context and Prompting Strategies on Automated Unit Test Generation with Modern General-Purpose Large Language Models", "comment": null, "summary": "Generative AI is gaining increasing attention in software engineering, where\ntesting remains an indispensable reliability mechanism. According to the widely\nadopted testing pyramid, unit tests constitute the majority of test cases and\nare often schematic, requiring minimal domain expertise. Automatically\ngenerating such tests under the supervision of software engineers can\nsignificantly enhance productivity during the development phase of the software\nlifecycle.\n  This paper investigates the impact of code context and prompting strategies\non the quality and adequacy of unit tests generated by various large language\nmodels (LLMs) across several families. The results show that including\ndocstrings notably improves code adequacy, while further extending context to\nthe full implementation yields definitely smaller gains. Notably, the\nchain-of-thought prompting strategy -- applied even to 'reasoning' models --\nachieves the best results, with up to 96.3\\% branch coverage, a 57\\% average\nmutation score, and near-perfect compilation success rate. Among the evaluated\nmodels, M5 (Gemini 2.5 Pro) demonstrated superior performance in both mutation\nscore and branch coverage being still in top in terms of compilation success\nrate.\n  All the code and resulting test suites are publicly available at\nhttps://github.com/peetery/LLM-analysis.", "AI": {"tldr": "Including docstrings and using the chain-of-thought prompting strategy significantly enhances the quality of LLM-generated unit tests, with Gemini 2.5 Pro achieving superior results in branch coverage and mutation score.", "motivation": "Testing is critical for software reliability, especially unit testing, which is highly repetitive and labor-intensive. Automatically generating unit tests with LLMs could improve productivity but requires understanding how context and prompting affect test quality.", "method": "The paper systematically investigates the impact of code context (e.g., including docstrings and full implementation) and different prompting strategies (such as chain-of-thought) on the quality and adequacy of unit tests generated by various LLMs from different model families. The evaluation uses metrics like branch coverage, mutation score, and compilation success rate.", "result": "Including docstrings improves test adequacy, while supplying the full code yields smaller additional gains. The chain-of-thought prompting strategy delivers the best results, achieving up to 96.3% branch coverage, 57% mutation score, and near-perfect compilation rate. Among all tested models, Gemini 2.5 Pro (M5) outperforms others in mutation score and branch coverage, and remains at the top in compilation rate.", "conclusion": "Prompting strategy, especially chain-of-thought, and code context (notably docstrings) significantly influence the effectiveness of LLM-generated unit tests. Gemini 2.5 Pro achieves the best overall results among evaluated models. The findings provide practical guidance for leveraging generative AI in software testing."}}
{"id": "2507.14330", "categories": ["cs.SE", "D.2.1; D.2.4; D.2.10; F.4.1; F.4.3"], "pdf": "https://arxiv.org/pdf/2507.14330", "abs": "https://arxiv.org/abs/2507.14330", "authors": ["Arshad Beg", "Diarmuid O'Donoghue", "Rosemary Monahan"], "title": "Leveraging LLMs for Formal Software Requirements -- Challenges and Prospects", "comment": "Submitted to Overlay2025 - 7th International Workshop on Artificial\n  Intelligence and fOrmal VERification, Logic, Automata, and sYnthesis. [under\n  review]", "summary": "Software correctness is ensured mathematically through formal verification,\nwhich involves the resources of generating formal requirement specifications\nand having an implementation that must be verified. Tools such as\nmodel-checkers and theorem provers ensure software correctness by verifying the\nimplementation against the specification. Formal methods deployment is\nregularly enforced in the development of safety-critical systems e.g.\naerospace, medical devices and autonomous systems. Generating these\nspecifications from informal and ambiguous natural language requirements\nremains the key challenge. Our project, VERIFAI^{1}, aims to investigate\nautomated and semi-automated approaches to bridge this gap, using techniques\nfrom Natural Language Processing (NLP), ontology-based domain modelling,\nartefact reuse, and large language models (LLMs). This position paper presents\na preliminary synthesis of relevant literature to identify recurring challenges\nand prospective research directions in the generation of verifiable\nspecifications from informal requirements.", "AI": {"tldr": "Formal verification relies on translating ambiguous natural language requirements into precise specifications, a challenging task. This paper reviews current literature and discusses automated approaches\u2014NLP, ontologies, LLMs\u2014to improve this process, outlining key challenges and future research directions.", "motivation": "There is a significant challenge in formal verification due to the difficulty of generating formal, unambiguous requirement specifications from natural language, a key bottleneck especially for safety-critical software systems.", "method": "The paper synthesizes existing literature and examines automated and semi-automated techniques\u2014including NLP, ontology-based modeling, artefact reuse, and LLMs\u2014for converting informal requirements into formal specifications.", "result": "The paper identifies recurring challenges and outlines prospective research directions for generating verifiable specifications from informal requirements, but does not present experimental or evaluative results.", "conclusion": "There is a need for further research on leveraging automated tools and advanced language models to bridge the gap between informal natural language requirements and formal verifiable specifications."}}
{"id": "2507.14396", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14396", "abs": "https://arxiv.org/abs/2507.14396", "authors": ["Carey Lai Zheng Hui", "Johnson Britto Jessia Esther Leena", "Kumuthini Subramanian", "Zhao Chenyu", "Shubham Rajeshkumar Jariwala"], "title": "Developing Shared Vocabulary System For Collaborative Software Engineering", "comment": "16 pages, including appendix", "summary": "Effective communication is a critical factor in successful software\nengineering collaboration. However, communication gaps remain a persistent\nchallenge, often leading to misunderstandings, inefficiencies, and defects.\nThis research investigates the technical factors contributing to such\nmisunderstandings and explores the measurable benefits of establishing shared\nvocabulary systems within software documentation and codebases. Using a Design\nScience Research (DSR) framework, the study was structured into three iterative\nphases: problem identification, method development, and empirical validation.\nThe problem identification phase involved thematic analysis of communication\ndata and semi-structured interviews, revealing key factors such as ambiguous\nmessaging, misalignment in documentation, inconsistent code review feedback,\nand API integration miscommunication. Grounded Theory principles were employed\nto design a structured methodology for collaborative vocabulary development.\nEmpirical validation through controlled experiments demonstrated that while\ninitial adoption introduced overhead, the shared vocabulary system\nsignificantly improved information density, documentation clarity, and\ncollaboration efficiency over time. Findings offer actionable insights for\nimproving communication practices in software engineering, while also\nidentifying limitations and directions for future research.", "AI": {"tldr": "Creating and adopting shared vocabularies in software engineering teams, while initially effortful, results in clearer documentation and better collaboration. This strengthens communication and helps reduce misunderstandings and defects.", "motivation": "Communication gaps in software engineering persistently cause misunderstandings, inefficiencies, and defects. This paper is motivated by the need to understand the technical reasons behind these communication breakdowns and to explore solutions for enhancing mutual understanding among collaborators.", "method": "The study used a Design Science Research (DSR) framework consisting of three iterative phases: (1) identifying problems through thematic analysis of communication data and semi-structured interviews, (2) developing a methodology for creating shared vocabularies using Grounded Theory principles, and (3) empirically validating the approach through controlled experiments.", "result": "Empirical validation found that, although adoption introduced some initial overhead, implementing shared vocabulary systems led to significant improvements in information density, documentation clarity, and collaboration efficiency within software engineering teams.", "conclusion": "Establishing structured, shared vocabulary systems can greatly enhance communication and collaboration in software development, overcoming many issues caused by ambiguous or inconsistent language. However, initial implementation may involve a learning curve."}}
{"id": "2507.14423", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14423", "abs": "https://arxiv.org/abs/2507.14423", "authors": ["Mootez Saad", "Hao Li", "Tushar Sharma", "Ahmed E. Hassan"], "title": "On the Effect of Token Merging on Pre-trained Models for Code", "comment": null, "summary": "Tokenization is a fundamental component of language models for code. It\ninvolves breaking down the input into units that are later passed to the\nlanguage model stack to learn high-dimensional representations used in various\ncontexts, from classification to generation. However, the output of these\ntokenizers is often longer than that traditionally used in compilers and\ninterpreters. This could result in undesirable effects, such as increased\ncomputational overhead. In this work, we investigate the effect of merging the\nhidden representations of subtokens that belong to the same semantic unit, such\nas subtokens that form a single identifier. We propose two strategies: one\nbased on averaging the representations and another that leverages a\nlearning-based approach. Both methods can be seamlessly integrated with\nexisting language models for code. We conduct experiments using six language\nmodels for code: CodeBERT, GraphCodeBERT, UniXCoder, CdoeT5, CodeT5+ (220M),\nand CodeT5+ (770M), across three software engineering tasks: vulnerability\ndetection, code classification, and code translation. Results show that these\nstrategies can reduce the number of floating-point operations by $1\\%$ to\n$19\\%$. Regarding downstream performance, the most significant degradation was\nobserved in the vulnerability detection task, where the F1 score decreased by\n$1.82$ points compared to the baseline. In contrast, for code translation, we\nobserved an improvement of $2.47$ points in CodeBLEU. This work contributes to\nthe broader effort of improving language models for code across multiple\ndimensions, including both computational efficiency and downstream performance.", "AI": {"tldr": "The paper investigates ways to merge subtoken representations in code models, showing that this can save computation and sometimes improve results, especially for code translation, with only minor losses in other tasks like vulnerability detection.", "motivation": "Tokenization in code language models produces longer sequences than traditional compiler token streams, which leads to higher computational costs and can impact model efficiency.", "method": "The paper proposes and evaluates two strategies for merging the hidden representations of subtokens forming the same semantic unit: one using average pooling and another using a learning-based method. These strategies are tested with six code-specific language models across three tasks.", "result": "The proposed methods reduce computational operations between 1% and 19%. For vulnerability detection, there is a modest decrease in F1 score (up to 1.82 points), while for code translation, there is an improvement of 2.47 CodeBLEU points.", "conclusion": "Merging subtoken representations can improve computational efficiency with minimal or even positive impact on downstream task performance, depending on the application. This represents a step toward more efficient language models for code."}}
{"id": "2507.14403", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.14403", "abs": "https://arxiv.org/abs/2507.14403", "authors": ["Sarunas Kalade", "Graham Schelle"], "title": "NPUEval: Optimizing NPU Kernels with LLMs and Open Source Compilers", "comment": null, "summary": "Neural processing units (NPUs) are gaining prominence in power-sensitive\ndevices like client devices, with AI PCs being defined by their inclusion of\nthese specialized processors. Running AI workloads efficiently on these devices\nrequires libraries of optimized kernels. Creating efficient kernels demands\nexpertise in domain-specific C++ with vector intrinsics and in-depth knowledge\nof the target architecture. Unlike GPU programming, which has had years to\nmature, NPU programming is new, with smaller and more fragmented developer\ncommunities across hardware platforms. This fragmentation poses a challenge\nwhen utilizing LLMs to assist in writing NPU kernels, as domain-specific\noptimized code examples are underrepresented in LLM pre-training data.\n  In this paper we introduce NPUEval -- a benchmark for writing and evaluating\nNPU kernels, consisting of 102 common operators for machine learning workloads.\nWe evaluate LLM generated code on actual hardware based on both functional\ncorrectness and vectorization efficiency using open source compiler tools\ntargeting the AMD NPU. We evaluate a range of state-of-the-art LLMs with a mix\nof proprietary and open-weight models. Latest reasoning models like DeepSeek\nR1, show promising results achieving out-of-the-box 50%+ vectorization on\nselect kernels. However, the average score across the entire dataset remains\nroughly 10% even with compiler feedback and vectorized kernel examples --\nshowing that this is a challenging dataset even for frontier models. The\ndataset and evaluation code will be released with a permissive open source\nlicense, providing an essential benchmark for advancing research in code\ngeneration and NPU kernel optimization.", "AI": {"tldr": "The paper introduces NPUEval, a new open-source benchmark for evaluating LLM-generated NPU kernels, showing current models struggle with the task, which highlights opportunities and challenges for future research in AI code generation for specialized hardware.", "motivation": "Efficiently running AI workloads on neural processing units (NPUs) in power-sensitive devices, like AI PCs, requires highly optimized kernel libraries. However, NPU programming is new and fragmented, lacking the rich ecosystem and resources found in GPU programming, making it especially challenging for large language models (LLMs) to assist with code generation.", "method": "The paper introduces NPUEval, a benchmark that comprises 102 common machine learning operators. The authors evaluate LLM-generated NPU kernel code on real hardware for functional correctness and vectorization efficiency. They leverage open-source compiler tools for the AMD NPU and test various state-of-the-art LLMs, including both proprietary and open-weight models.", "result": "Some advanced LLMs, such as DeepSeek R1, are able to achieve over 50% vectorization out-of-the-box on select kernels. However, the average efficiency score across all benchmarks is about 10%, even when feedback and optimized examples are provided, indicating the difficulty of the task for current LLMs.", "conclusion": "NPUEval is presented as a crucial open-source benchmark for evaluating and advancing LLM-assisted code generation and NPU kernel optimization. Current LLMs show only modest performance, underscoring the challenges and the need for further research."}}
{"id": "2507.14547", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14547", "abs": "https://arxiv.org/abs/2507.14547", "authors": ["Noman Ahmad", "Ruoyu Su", "Matteo Esposito", "Andrea Janes", "Valentina Lenarduzzi", "Davide Taibi"], "title": "Architectural Degradation: Definition, Motivations, Measurement and Remediation Approaches", "comment": null, "summary": "Architectural degradation, also known as erosion, decay, or aging, impacts\nsystem quality, maintainability, and adaptability. Although widely\nacknowledged, current literature shows fragmented definitions, metrics, and\nremediation strategies. Our study aims to unify understanding of architectural\ndegradation by identifying its definitions, causes, metrics, tools, and\nremediation approaches across academic and gray literature. We conducted a\nmultivocal literature review of 108 studies extracting definitions, causes,\nmetrics, measurement approaches, tools, and remediation strategies. We\ndeveloped a taxonomy encompassing architectural, code, and process debt to\nexplore definition evolution, methodological trends, and research gaps.\nArchitectural degradation has shifted from a low-level issue to a\nsocio-technical concern. Definitions now address code violations, design drift,\nand structural decay. Causes fall under architectural (e.g., poor\ndocumentation), code (e.g., hasty fixes), and process debt (e.g., knowledge\nloss). We identified 54 metrics and 31 measurement techniques, focused on\nsmells, cohesion/coupling, and evolution. Yet, most tools detect issues but\nrarely support ongoing or preventive remediation. Degradation is both technical\nand organizational. While detection is well-studied, continuous remediation\nremains lacking. Our study reveals missed integration between metrics, tools,\nand repair logic, urging holistic, proactive strategies for sustainable\narchitecture.", "AI": {"tldr": "Architectural degradation harms software sustainability, but literature is fragmented on definitions and solutions. This review synthesizes 108 studies, highlighting the evolution of the concept, detection maturity, but lack of integrated and proactive remediation. More holistic approaches are urgently needed.", "motivation": "Architectural degradation threatens software quality, maintainability, and adaptability, but existing studies present fragmented definitions, metrics, and remediation approaches, making cohesive understanding and effective management difficult.", "method": "The authors conducted a multivocal literature review of 108 sources from both academic and gray literature, systematically extracting and analyzing definitions, causes, metrics, measurement techniques, tools, and remediation strategies associated with architectural degradation. They also developed a taxonomy to explore the evolution of concepts and identify research gaps.", "result": "The review found evolving definitions of degradation, now including code violations, design drift, and structural decay, with causes spanning architectural, code, and process debt. The study cataloged 54 metrics and 31 measurement techniques, mostly focused on detection (e.g., smells, cohesion/coupling). However, tools rarely support ongoing remediation, and integration between metrics, tools, and repair processes is lacking.", "conclusion": "Architectural degradation is both a technical and organizational issue. While detection methods are mature, continuous remediation is underexplored. There is a critical need for cohesive, holistic, and proactive strategies that integrate detection metrics, tools, and remediation logic to ensure sustainable software architecture."}}
{"id": "2507.14471", "categories": ["cs.PL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.14471", "abs": "https://arxiv.org/abs/2507.14471", "authors": ["Logan Kenwright", "Partha Roop", "Nathan Allen", "C\u0103lin Ca\u015fcaval", "Avinash Malik"], "title": "Timetide: A programming model for logically synchronous distributed systems", "comment": "25 Pages, 21 Figures", "summary": "Massive strides in deterministic models have been made using synchronous\nlanguages. They are mainly focused on centralised applications, as the\ntraditional approach is to compile away the concurrency. Time triggered\nlanguages such as Giotto and Lingua Franca are suitable for distribution albeit\nthat they rely on expensive physical clock synchronisation, which is both\nexpensive and may suffer from scalability. Hence, deterministic programming of\ndistributed systems remains challenging. We address the challenges of\ndeterministic distribution by developing a novel multiclock semantics of\nsynchronous programs. The developed semantics is amenable to seamless\ndistribution. Moreover, our programming model, Timetide, alleviates the need\nfor physical clock synchronisation by building on the recently proposed logical\nsynchrony model for distributed systems. We discuss the important aspects of\ndistributing computation, such as network communication delays, and explore the\nformal verification of Timetide programs. To the best of our knowledge,\nTimetide is the first multiclock synchronous language that is both amenable to\ndistribution and formal verification without the need for physical clock\nsynchronisation or clock gating.", "AI": {"tldr": "Deterministic distributed programming is hard due to physical clock synchronization, but Timetide introduces a multiclock, logically synchronous model that is distributable and formally verifiable without expensive hardware clocks.", "motivation": "Deterministic programming in distributed systems is challenging due to reliance on expensive and non-scalable physical clock synchronization in existing time-triggered languages. Traditional synchronous languages typically target centralized applications.", "method": "The authors develop a novel multiclock semantics for synchronous programming, leading to the creation of their new programming model, Timetide. This model builds on logical synchrony instead of physical clock synchrony, and addresses aspects of distributed computation such as network communication delays and formal verification.", "result": "Timetide enables the deterministic distribution of synchronous programs without requiring physical clock synchronization or clock gating, and supports formal verification. It is also scalable and suitable for distributed applications.", "conclusion": "Timetide represents a significant advance as the first multiclock synchronous language that achieves both distributability and formal verification capabilities without the typical requirement for expensive physical clock synchronization. The approach makes deterministic distributed programming more practical and scalable."}}
{"id": "2507.14554", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14554", "abs": "https://arxiv.org/abs/2507.14554", "authors": ["Ruoyu Su", "Noman ahmad", "Matteo Esposito", "Andrea Janes", "Davide Taibi", "Valentina Lenarduzzi"], "title": "Emerging Trends in Software Architecture from the Practitioners Perspective: A Five Year Review", "comment": null, "summary": "Software architecture plays a central role in the design, development, and\nmaintenance of software systems. With the rise of cloud computing,\nmicroservices, and containers, architectural practices have diversified.\nUnderstanding these shifts is vital. This study analyzes software architecture\ntrends across eight leading industry conferences over five years. We\ninvestigate the evolution of software architecture by analyzing talks from top\npractitioner conferences, focusing on the motivations and contexts driving\ntechnology adoption. We analyzed 5,677 talks from eight major industry\nconferences, using large language models and expert validation to extract\ntechnologies, their purposes, and usage contexts. We also explored how\ntechnologies interrelate and fit within DevOps and deployment pipelines. Among\n450 technologies, Kubernetes, Cloud Native, Serverless, and Containers dominate\nby frequency and centrality. Practitioners present technology mainly related to\ndeployment, communication, AI, and observability. We identify five technology\ncommunities covering automation, coordination, cloud AI, monitoring, and\ncloud-edge. Most technologies span multiple DevOps stages and support hybrid\ndeployment. Our study reveals that a few core technologies, like Kubernetes and\nServerless, dominate the contemporary software architecture practice. These are\nmainly applied in later DevOps stages, with limited focus on early phases like\nplanning and coding. We also show how practitioners frame technologies by\npurpose and context, reflecting evolving industry priorities. Finally, we\nobserve how only research can provide a more holistic lens on architectural\ndesign, quality, and evolution.", "AI": {"tldr": "By analyzing over 5,000 talks from major industry conferences, this paper shows that a few core technologies (Kubernetes, Serverless) dominate software architecture practices, primarily in deployment and operation. The study highlights evolving industry priorities and the need for research to address architectural qualities more holistically.", "motivation": "The motivation is to understand how software architecture practices are shifting due to emerging technologies (cloud, microservices, containers) and to gain insights into what drives technology adoption in modern industry settings.", "method": "The authors analyzed 5,677 talks from eight major industry conferences over five years. They used large language models and expert validation to extract information on technologies, their purposes, and usage contexts. Relationships among technologies and their placement within DevOps pipelines were also examined.", "result": "The analysis identified Kubernetes, Cloud Native, Serverless, and Containers as dominant technologies. Practitioner focus is mainly on deployment, communication, AI, and observability, with five main technology communities identified. Most technologies are widely used across DevOps stages, especially in hybrid deployments, but there is less emphasis on early stages like planning and coding.", "conclusion": "A small set of core technologies dominates current software architecture practice, especially in later DevOps stages. Practitioners emphasize deployment and operational concerns, highlighting technology purpose and context, while calling for broader research-driven perspectives on architectural quality and evolution."}}
{"id": "2507.15007", "categories": ["cs.PL", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15007", "abs": "https://arxiv.org/abs/2507.15007", "authors": ["Sayed Mahbub Hasan Amiri", "Md. Mainul Islam", "Mohammad Shakhawat Hossen", "Sayed Majhab Hasan Amiri", "Mohammad Shawkat Ali Mamun", "Sk. Humaun Kabir", "Naznin Akter"], "title": "Hear Your Code Fail, Voice-Assisted Debugging for Python", "comment": "35 pages, 20 figures", "summary": "This research introduces an innovative voice-assisted debugging plugin for\nPython that transforms silent runtime errors into actionable audible\ndiagnostics. By implementing a global exception hook architecture with pyttsx3\ntext-to-speech conversion and Tkinter-based GUI visualization, the solution\ndelivers multimodal error feedback through parallel auditory and visual\nchannels. Empirical evaluation demonstrates 37% reduced cognitive load (p<0.01,\nn=50) compared to traditional stack-trace debugging, while enabling 78% faster\nerror identification through vocalized exception classification and\ncontextualization. The system achieves sub-1.2 second voice latency with under\n18% CPU overhead during exception handling, vocalizing error types and\nconsequences while displaying interactive tracebacks with documentation deep\nlinks. Criteria validate compatibility across Python 3.7+ environments on\nWindows, macOS, and Linux platforms. Needing only two lines of integration\ncode, the plugin significantly boosts availability for aesthetically impaired\ndesigners and supports multitasking workflows through hands-free error medical\ndiagnosis. Educational applications show particular promise, with pilot studies\nindicating 45% faster debugging skill acquisition among novice programmers.\nFuture development will incorporate GPT-based repair suggestions and real-time\nmultilingual translation to further advance auditory debugging paradigms. The\nsolution represents a fundamental shift toward human-centric error diagnostics,\nbridging critical gaps in programming accessibility while establishing new\nstandards for cognitive efficiency in software development workflows.", "AI": {"tldr": "This paper presents a Python plugin that turns runtime errors into both spoken and visual feedback, significantly lowering cognitive load and speeding up debugging\u2014especially helping the visually impaired and beginners. It works quickly and efficiently across major platforms, with plans for even smarter, multilingual features.", "motivation": "Debugging silent runtime errors in Python often relies on visual stack traces, which are challenging for individuals with visual impairments and can create high cognitive load for all users. There is a need to make debugging more accessible, efficient, and multimodal to improve cognitive efficiency and support diverse users, especially those who multitask or are learning programming.", "method": "The research implements a voice-assisted debugging plugin by combining a global exception hook in Python with pyttsx3 for text-to-speech, and a Tkinter-based graphical user interface. The system vocalizes errors and offers visual tracebacks with interactive documentation. It is evaluated empirically with 50 participants for cognitive load, identification speed, and system performance metrics (voice latency, CPU overhead).", "result": "Empirical results show a 37% reduction in cognitive load and 78% faster error identification compared to traditional stack-trace debugging. The plugin operates with sub-1.2 second voice latency and less than 18% CPU overhead. Pilot educational studies indicate 45% faster debugging skill acquisition for novices. The tool is validated for compatibility across major operating systems and Python 3.7+.", "conclusion": "The system fundamentally improves error diagnostics in Python by introducing efficient, accessible multimodal feedback. It benefits visually impaired users and multitaskers, with notable educational advantages for beginners. Future enhancements will add GPT-based repair suggestions and multilingual auditory output. The work sets a new standard for cognitive efficiency in debugging and bridges important accessibility gaps."}}
{"id": "2507.14558", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14558", "abs": "https://arxiv.org/abs/2507.14558", "authors": ["Bin Duan", "Tarek Mahmud", "Meiru Che", "Yan Yan", "Naipeng Dong", "Dan Dongseong Kim", "Guowei Yang"], "title": "Harnessing LLMs for Document-Guided Fuzzing of OpenCV Library", "comment": null, "summary": "The combination of computer vision and artificial intelligence is\nfundamentally transforming a broad spectrum of industries by enabling machines\nto interpret and act upon visual data with high levels of accuracy. As the\nbiggest and by far the most popular open-source computer vision library, OpenCV\nlibrary provides an extensive suite of programming functions supporting\nreal-time computer vision. Bugs in the OpenCV library can affect the downstream\ncomputer vision applications, and it is critical to ensure the reliability of\nthe OpenCV library. This paper introduces VISTAFUZZ, a novel technique for\nharnessing large language models (LLMs) for document-guided fuzzing of the\nOpenCV library. VISTAFUZZ utilizes LLMs to parse API documentation and obtain\nstandardized API information. Based on this standardized information, VISTAFUZZ\nextracts constraints on individual input parameters and dependencies between\nthese. Using these constraints and dependencies, VISTAFUZZ then generates new\ninput values to systematically test each target API. We evaluate the\neffectiveness of VISTAFUZZ in testing 330 APIs in the OpenCV library, and the\nresults show that VISTAFUZZ detected 17 new bugs, where 10 bugs have been\nconfirmed, and 5 of these have been fixed.", "AI": {"tldr": "VISTAFUZZ uses LLMs to intelligently fuzz OpenCV's API based on its documentation, found 17 new bugs during tests, and proved effective for automated, document-guided software testing.", "motivation": "Ensuring the reliability of the widely used OpenCV computer vision library is crucial, as bugs in its implementation can negatively impact a large range of downstream applications.", "method": "The paper proposes VISTAFUZZ, a novel document-guided fuzzing technique that leverages large language models (LLMs) to parse API documentation, extract standardized information and constraints, and then generates systematic test inputs for OpenCV APIs based on these.", "result": "VISTAFUZZ was evaluated on 330 OpenCV APIs. It successfully detected 17 new bugs, 10 of which were confirmed, and 5 were subsequently fixed.", "conclusion": "VISTAFUZZ demonstrates the effectiveness of combining LLMs with fuzzing techniques to improve the reliability of critical computer vision libraries like OpenCV, leading to the discovery and resolution of previously unknown issues."}}
{"id": "2507.15017", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.15017", "abs": "https://arxiv.org/abs/2507.15017", "authors": ["Xuran Cai", "Liqian Chen", "Hongfei Fu"], "title": "Invariant Generation for Floating-Point Programs via Constraint Solving", "comment": null, "summary": "In numeric-intensive computations, it is well known that the execution of\nfloating-point programs is imprecise as floating point arithmetics (e.g.,\naddition, subtraction, multiplication, division, etc.) incurs rounding errors.\nAlbeit the rounding error is small for every single floating-point operation,\nthe aggregation of such error in multiple operations may be dramatic and cause\ncatastrophic program failures. Therefore, to ensure the correctness of\nfloating-point programs, the effect of floating point error needs to be\ncarefully taken into account. In this work, we consider the invariant\ngeneration for floating point programs, whose aim is to generate tight\ninvariants under the perturbation of floating point errors. Our main\ncontribution is a theoretical framework on how to apply constraint solving\nmethods to address the invariant generation problem. In our framework, we\npropose a novel combination between the first-order differential\ncharacterization by FPTaylor (TOPLAS 2018) and constraint solving methods,\naiming to reduce the computational burden of constraint solving. Moreover, we\ndevise two polynomial invariant generation algorithms to instantiate the\nframework. The first algorithm is applicable to a wide range of floating-point\noperations but requires an initial (coarse) invariant as external input, while\nthe second does not require an initial invariant but is limited to polynomial\nprograms. Furthermore, we show how conditional branches, a difficult issue in\nfloating-point analysis, can be handled in our framework. Experimental results\nshow that our algorithms outperform SOTA approaches in both the time efficiency\nand the precision of the generated invariants over a variety of benchmarks.", "AI": {"tldr": "This paper presents a new constraint-based framework and algorithms for generating precise invariants in floating-point programs, achieving better accuracy and speed than existing methods.", "motivation": "Floating-point arithmetic introduces small rounding errors, which can accumulate and cause significant problems in numeric-intensive computations. Ensuring correctness in floating-point programs requires careful analysis of these errors, particularly when generating program invariants.", "method": "The paper develops a theoretical framework that combines the first-order differential characterization technique from FPTaylor with constraint solving methods to efficiently generate tight invariants for floating-point programs. Two polynomial invariant generation algorithms are introduced: one requiring an initial coarse invariant (but broadly applicable) and one independent of such input (but limited to polynomial programs). Conditional branches are also handled within this framework.", "result": "The proposed algorithms outperform state-of-the-art methods in both precision and time efficiency for invariant generation on a range of benchmarks.", "conclusion": "Combining differential characterization and constraint solving enables more efficient and precise invariant generation for floating-point programs, providing a practical tool to address floating-point errors and improving upon current techniques."}}
{"id": "2507.14594", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14594", "abs": "https://arxiv.org/abs/2507.14594", "authors": ["Weiwei Xu", "Hengzhi Ye", "Kai Gao", "Minghui Zhou"], "title": "A first look at License Variants in the PyPI Ecosystem", "comment": null, "summary": "Open-source licenses establish the legal foundation for software reuse, yet\nlicense variants, including both modified standard licenses and custom-created\nalternatives, introduce significant compliance complexities. Despite their\nprevalence and potential impact, these variants are poorly understood in modern\nsoftware systems, and existing tools do not account for their existence,\nleading to significant challenges in both effectiveness and efficiency of\nlicense analysis. To fill this knowledge gap, we conduct a comprehensive\nempirical study of license variants in the PyPI ecosystem. Our findings show\nthat textual variations in licenses are common, yet only 2% involve substantive\nmodifications. However, these license variants lead to significant compliance\nissues, with 10.7% of their downstream dependencies found to be\nlicense-incompatible.\n  Inspired by our findings, we introduce LV-Parser, a novel approach for\nefficient license variant analysis leveraging diff-based techniques and large\nlanguage models, along with LV-Compat, an automated pipeline for detecting\nlicense incompatibilities in software dependency networks. Our evaluation\ndemonstrates that LV-Parser achieves an accuracy of 0.936 while reducing\ncomputational costs by 30%, and LV-Compat identifies 5.2 times more\nincompatible packages than existing methods with a precision of 0.98.\n  This work not only provides the first empirical study into license variants\nin software packaging ecosystem but also equips developers and organizations\nwith practical tools for navigating the complex landscape of open-source\nlicensing.", "AI": {"tldr": "This paper reveals that license variants are common and create downstream compliance issues. The authors introduce accurate, efficient tools (LV-Parser, LV-Compat) that outperform existing methods in identifying license incompatibilities in software packages.", "motivation": "Open-source license variants make compliance analysis difficult due to modified and custom licenses, but their impact and prevalence are not well understood and current tools fail to handle them effectively.", "method": "The authors conducted a comprehensive empirical study of license variants in the PyPI ecosystem. They proposed LV-Parser, which uses diff-based techniques and large language models for license analysis, and LV-Compat, an automated pipeline for detecting license incompatibilities within dependency networks.", "result": "Textual license variations are common, but only 2% are substantial modifications. Despite this, variants create significant compliance risks\u201410.7% of downstream dependencies are license-incompatible. LV-Parser achieved 0.936 accuracy and reduced computational cost by 30%. LV-Compat identified 5.2 times more incompatible packages than existing tools, with 0.98 precision.", "conclusion": "License variants impose real compliance issues in open-source ecosystems. The authors provide the first significant empirical study on these variants and introduce new tools that enhance both the detection and efficiency of license compatibility analysis."}}
{"id": "2507.15277", "categories": ["cs.PL", "D.3.4"], "pdf": "https://arxiv.org/pdf/2507.15277", "abs": "https://arxiv.org/abs/2507.15277", "authors": ["Robert Hochgraf", "Sreepathi Pai"], "title": "A Few Fit Most: Improving Performance Portability of SGEMM on GPUs using Multi-Versioning", "comment": "13 pages, 8 figures", "summary": "Hand-optimizing linear algebra kernels for different GPU devices and\napplications is complex and labor-intensive. Instead, many developers use\nautomatic performance tuning (autotuning) to achieve high performance on a\nvariety of devices. However, autotuning \"overfits\", and must be redone if any\npart of the environment changes, such as if the device or input characteristics\nchange.\n  In most non-trivial cases, a single compute kernel cannot maintain\nnear-optimal performance across all environments. Changing the kernel to\nspecialize it to the current execution environment is possible, but on GPUs,\nruntime tuning and compilation can be expensive.\n  In this work, we use multi-versioning -- producing several variants of the\nsame code -- as a way to generate performance portable code. We describe a\nframework called portability tuning that can automatically generate\nmulti-versioned code whose performance is portable, requiring no retuning.\n  We evaluate our framework on a dataset of execution times for GEMM kernels\nfrom the CLBlast linear algebra library. We find our portability tuning\ntechniques outperform CLBlast's default kernels -- often approaching within 10%\nof the theoretical maximum performance -- despite CLBlast using autotuning\ntechniques. Further, we find that our generated programs generalize well to new\nand unseen devices, matching the performance of autotuning without ever\nportability tuning for those devices.", "AI": {"tldr": "Manual optimization for GPUs is tedious, and autotuning isn't flexible to changes. This paper introduces a framework that automatically generates multiple code versions for better performance portability. It beats standard methods in speed and generalizes well to new hardware, without repeatedly retuning.", "motivation": "Hand-optimizing linear algebra kernels for various GPU devices is complex and labor-intensive. While autotuning is commonly used for automatic performance optimization, it tends to overfit and requires re-tuning whenever the environment changes (e.g., device, input).", "method": "The authors propose using multi-versioning, which generates several variants of the same code, to achieve performance portability. They introduce a framework called 'portability tuning' that automatically generates multi-versioned code, eliminating the need for retuning.", "result": "The framework was evaluated on execution times for GEMM kernels from the CLBlast linear algebra library. The portability tuning technique outperformed CLBlast's default kernels, often approaching within 10% of theoretical maximum performance. The generated programs generalized effectively to unseen devices, performing comparably to autotuning without device-specific retuning.", "conclusion": "Portability tuning, through automatic code multi-versioning, enables high and portable performance for linear algebra kernels across diverse GPU devices and execution environments, without repeated expensive autotuning."}}
{"id": "2507.14687", "categories": ["cs.SE", "68Q60, 03B70", "D.2.5"], "pdf": "https://arxiv.org/pdf/2507.14687", "abs": "https://arxiv.org/abs/2507.14687", "authors": ["Robin Lee", "Youngho Nam"], "title": "An Efficient Algorithm for Generating Minimal Unique-Cause MC/DC Test cases for Singular Boolean Expressions", "comment": "10 pages, 5 figures", "summary": "Modified Condition/Decision Coverage (MC/DC) is a mandatory structural\ncoverage criterion for ensuring the reliability and safety of critical systems.\nWhile its strictest form, Unique-Cause MC/DC, offers the highest assurance,\nresearch on its efficient test generation has been lacking. This gap is\nparticularly significant, as an analysis of large-scale avionics systems shows\nthat 99.7% of all conditional decisions are, in fact, Singular Boolean\nExpressions (SBEs) the ideal structure for applying Unique-Cause MC/DC. This\npaper proposes 'Robin's Rule', a deterministic algorithm that directly\nconstructs a minimal test set of N + 1 cases to guarantee 100% Unique-Cause\nMC/DC for SBEs with N conditions, without generating a full truth table. To\nvalidate our approach, we constructed a benchmark by reformulating the TCAS-II\nspecifications into SBEs and verified the results using an industry-standard,\ncertified commercial tool. The results confirm that our method consistently\nachieves 100% coverage with the theoretical minimum number of tests and is more\nefficient than the commercial tool. This work provides a practical and provably\noptimal solution for verifying safety-critical systems, ensuring both rigor and\nefficiency.", "AI": {"tldr": "The paper introduces 'Robin\u2019s Rule,' an optimal algorithm for generating minimal test sets to achieve the strictest MC/DC coverage for SBEs in critical systems, validated to be both more efficient and just as rigorous as current commercial tools.", "motivation": "Modified Condition/Decision Coverage (MC/DC) is essential for critical systems' safety, but generating efficient test sets for its strictest form (Unique-Cause MC/DC) has been largely unexplored. The problem is pressing because nearly all conditional decisions in large-scale avionics (99.7%) are Singular Boolean Expressions (SBEs), which are ideally suited for this coverage.", "method": "The authors propose 'Robin's Rule,' a deterministic algorithm to generate a minimal test set\u2014N + 1 cases for N conditions\u2014that achieves 100% Unique-Cause MC/DC for SBEs, without constructing a full truth table. They validate their approach using benchmarks formed from TCAS-II specifications and industry-standard verification tools.", "result": "The proposed method reliably achieves full (100%) Unique-Cause MC/DC using the smallest theoretically possible test set, and outperforms a commercial tool in efficiency.", "conclusion": "'Robin\u2019s Rule' provides a provably optimal and practical solution for efficiently achieving safety-critical verification, specifically for SBEs dominant in real-world avionics, ensuring both rigor and resource savings."}}
{"id": "2507.15530", "categories": ["cs.PL", "cs.LO", "F.3.1; F.3.2"], "pdf": "https://arxiv.org/pdf/2507.15530", "abs": "https://arxiv.org/abs/2507.15530", "authors": ["Shing Hin Ho", "Nicolas Wu", "Azalea Raad"], "title": "Bayesian Separation Logic", "comment": null, "summary": "Bayesian probabilistic programming languages (BPPLs) let users denote\nstatistical models as code while the interpreter infers the posterior\ndistribution. The semantics of BPPLs are usually mathematically complex and\nunable to reason about desirable properties such as expected values and\nindependence of random variables. To reason about these properties in a\nnon-Bayesian setting, probabilistic separation logics such as PSL and Lilac\ninterpret separating conjunction as probabilistic independence of random\nvariables. However, no existing separation logic can handle Bayesian updating,\nwhich is the key distinguishing feature of BPPLs.\n  To close this gap, we introduce Bayesian separation logic (BaSL), a\nprobabilistic separation logic that gives semantics to BPPL. We prove an\ninternal version of Bayes' theorem using a result in measure theory known as\nthe Rokhlin-Simmons disintegration theorem. Consequently, BaSL can model\nprobabilistic programming concepts such as Bayesian updating, unnormalised\ndistribution, conditional distribution, soft constraint, conjugate prior and\nimproper prior while maintaining modularity via the frame rule. The model of\nBaSL is based on a novel instantiation of Kripke resource monoid via\n$\\sigma$-finite measure spaces over the Hilbert cube, and the semantics of\nHoare triple is compatible with an existing denotational semantics of BPPL\nbased on the category of $s$-finite kernels. Using BaSL, we then prove\nproperties of statistical models such as the expected value of Bayesian coin\nflip, correlation of random variables in the collider Bayesian network, and the\nposterior distributions of the burglar alarm model, a parameter estimation\nalgorithm, and the Gaussian mixture model.", "AI": {"tldr": "The paper introduces a new probabilistic separation logic, BaSL, that extends reasoning to capture Bayesian updating in Bayesian probabilistic programming languages. BaSL is mathematically rigorous and enables formal reasoning about key properties and constructs in such languages, filling a major gap in current semantics and logic tools.", "motivation": "Bayesian probabilistic programming languages (BPPLs) allow modeling uncertainty via code, but existing logics cannot reason about Bayesian updating\u2014the core feature of BPPLs. There is a need for a formal logic that can reason about expected values, independence, and Bayesian updating within these languages.", "method": "The authors introduce Bayesian separation logic (BaSL), which extends probabilistic separation logic to handle Bayesian updating. BaSL is built on Kripke resource monoids instantiated with \u03c3-finite measure spaces over the Hilbert cube, and it is semantically compatible with denotational semantics of BPPLs using s-finite kernels. The approach leverages the Rokhlin-Simmons disintegration theorem from measure theory to prove an internal version of Bayes' theorem.", "result": "BaSL can model key Bayesian probabilistic programming concepts like Bayesian updating, unnormalised and conditional distributions, soft and conjugate priors, and more. The semantics maintain modularity. Using BaSL, the paper proves properties such as expected values, variable correlations in network models, and computes posterior distributions for various statistical models.", "conclusion": "BaSL bridges the gap between existing probabilistic logics and Bayesian updating, providing the foundation to reason formally about BPPLs while supporting modular proofs and crucial probabilistic programming constructs."}}
{"id": "2507.14716", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14716", "abs": "https://arxiv.org/abs/2507.14716", "authors": ["Shahidul Islam", "Ashik Aowal", "Md Sharif Uddin", "Shaiful Chowdhury"], "title": "HistoryFinder: Advancing Method-Level Source Code History Generation with Accurate Oracles and Enhanced Algorithm", "comment": null, "summary": "Reconstructing a method's change history efficiently and accurately is\ncritical for many software engineering tasks, including maintenance,\nrefactoring, and comprehension. Despite the availability of method history\ngeneration tools such as CodeShovel and CodeTracker, existing evaluations of\ntheir effectiveness are limited by inaccuracies in the ground truth oracles\nused. In this study, we systematically construct two new oracles -- the\ncorrected CodeShovel oracle and a newly developed HistoryFinder oracle -- by\ncombining automated analysis with expert-guided manual validation. We also\nintroduce HistoryFinder, a new method history generation tool designed to\nimprove not only the accuracy and completeness of method change histories but\nalso to offer competitive runtime performance. Through extensive evaluation\nacross 400 methods from 40 open-source repositories, we show that HistoryFinder\nconsistently outperforms CodeShovel, CodeTracker, IntelliJ, and Git-based\nbaselines in terms of precision, recall, and F1 score. Moreover, HistoryFinder\nachieves competitive runtime performance, offering the lowest mean and median\nexecution times among all the research-based tools.\n  While Git-based tools exhibit the fastest runtimes, this efficiency comes at\nthe cost of significantly lower precision and recall -- leaving HistoryFinder\nas the best overall choice when both accuracy and efficiency are important. To\nfacilitate adoption, we provide a web interface, CLI, and Java library for\nflexible usage.", "AI": {"tldr": "The paper introduces HistoryFinder, a new, validated method history generation tool that offers superior accuracy and competitive runtime compared to existing tools. Comprehensive evaluation shows it is the best overall choice for reconstructing method histories, and it's made readily accessible for practical use.", "motivation": "Accurately and efficiently reconstructing a method's change history is essential for various software engineering tasks, but current evaluation methods are limited by inaccurate ground truth oracles. This hinders the effective assessment and improvement of existing tools.", "method": "The authors constructed two new oracles\u2014the corrected CodeShovel oracle and a newly developed HistoryFinder oracle\u2014using automated analysis and expert-guided manual validation. They introduced HistoryFinder, a new tool for method history generation, and performed extensive evaluations across 400 methods from 40 open-source repositories, comparing it with existing tools based on precision, recall, F1 score, and runtime.", "result": "HistoryFinder consistently outperformed existing research-based tools (CodeShovel, CodeTracker, IntelliJ, and Git-based baselines) in terms of precision, recall, and F1 score, while also achieving the lowest mean and median execution times among research-based tools. Although Git-based tools had faster runtimes, they suffered from much lower precision and recall.", "conclusion": "HistoryFinder is the best overall tool for method history reconstruction when both accuracy and efficiency are required, outperforming current research and Git-based tools. The tool is made easily accessible via web, CLI, and Java library interfaces to encourage adoption."}}
{"id": "2507.15596", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.15596", "abs": "https://arxiv.org/abs/2507.15596", "authors": ["Jaeseo Lee", "Kyungmin Bae"], "title": "Formal Analysis of Networked PLC Controllers Interacting with Physical Environments", "comment": "To appear in Proceedings of the Static Analysis Symposium (SAS) 2025", "summary": "Programmable Logic Controllers (PLCs) are widely used in industrial\nautomation to control physical systems. As PLC applications become increasingly\ncomplex, ensuring their correctness is crucial. Existing formal verification\ntechniques focus on individual PLC programs in isolation, often neglecting\ninteractions with physical environments and network communication between\ncontrollers. This limitation poses significant challenges in analyzing\nreal-world industrial systems, where continuous dynamics and communication\ndelays play a critical role. In this paper, we present a unified formal\nframework that integrates discrete PLC semantics, networked communication, and\ncontinuous physical behaviors. To mitigate state explosion, we apply partial\norder reduction, significantly reducing the number of explored states while\nmaintaining correctness. Our framework enables precise analysis of PLC-driven\nsystems with continuous dynamics and networked communication.", "AI": {"tldr": "The paper presents a new verification framework for industrial PLC systems that models both network communication and continuous dynamics, using partial order reduction to keep verification efficient and thorough.", "motivation": "PLCs are critical in industrial automation and their applications are increasingly complex. Current verification focuses only on isolated PLC programs, neglecting important factors like physical environment interactions and network communication, which can cause errors in real-world systems.", "method": "The paper introduces a unified formal framework that combines discrete PLC semantics, networked communication, and continuous physical system behaviors. It uses partial order reduction to control state explosion during verification.", "result": "The proposed framework allows precise analysis of PLC-driven systems, accounting for both continuous dynamics and networked interactions, with improved scalability due to partial order reduction.", "conclusion": "The unified framework overcomes the limitations of previous approaches by enabling comprehensive formal verification of industrial systems, considering both communication and continuous behaviors."}}
{"id": "2507.14735", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14735", "abs": "https://arxiv.org/abs/2507.14735", "authors": ["Vladyslav Bulhakov", "Giordano d'Aloisio", "Claudio Di Sipio", "Antinisca Di Marco", "Davide Di Ruscio"], "title": "Investigating the Role of LLMs Hyperparameter Tuning and Prompt Engineering to Support Domain Modeling", "comment": "Accepted at 51st Euromicro Conference Series on Software Engineering\n  and Advanced Applications (SEAA)", "summary": "The introduction of large language models (LLMs) has enhanced automation in\nsoftware engineering tasks, including in Model Driven Engineering (MDE).\nHowever, using general-purpose LLMs for domain modeling has its limitations.\nOne approach is to adopt fine-tuned models, but this requires significant\ncomputational resources and can lead to issues like catastrophic forgetting.\n  This paper explores how hyperparameter tuning and prompt engineering can\nimprove the accuracy of the Llama 3.1 model for generating domain models from\ntextual descriptions. We use search-based methods to tune hyperparameters for a\nspecific medical data model, resulting in a notable quality improvement over\nthe baseline LLM. We then test the optimized hyperparameters across ten diverse\napplication domains.\n  While the solutions were not universally applicable, we demonstrate that\ncombining hyperparameter tuning with prompt engineering can enhance results\nacross nearly all examined domain models.", "AI": {"tldr": "Instead of relying on costly and risky model fine-tuning, smart tuning of hyperparameters plus tailored prompt design can meaningfully boost large language model performance for specialized domain modeling tasks.", "motivation": "General-purpose large language models (LLMs) struggle with creating accurate domain models, and fine-tuning them is resource-intensive and risky. There is a need for less resource-heavy ways to boost domain modeling performance in LLMs.", "method": "The paper uses search-based approaches for hyperparameter tuning and applies prompt engineering to the Llama 3.1 model, initially focusing on a medical data model. The optimized approach is then tested across ten diverse application domains to measure generalizability.", "result": "Hyperparameter tuning and prompt engineering significantly improved domain model generation for the targeted medical data model and offered notable improvements for most other tested domains, though results were not universally optimal across all domains.", "conclusion": "Combining hyperparameter tuning and prompt engineering can enhance the effectiveness of large language models in generating domain models from text, providing a practical alternative to resource-intensive fine-tuning."}}
{"id": "2507.15843", "categories": ["cs.PL", "D.3.1; F.3.1; F.3.2; D.2.4"], "pdf": "https://arxiv.org/pdf/2507.15843", "abs": "https://arxiv.org/abs/2507.15843", "authors": ["Beniamino Accattoli", "Dan Ghica", "Giulio Guerrieri", "Cl\u00e1udio Belo Louren\u00e7o", "Claudio Sacerdoti Coen"], "title": "Closure Conversion, Flat Environments, and the Complexity of Abstract Machines", "comment": null, "summary": "Closure conversion is a program transformation at work in compilers for\nfunctional languages to turn inner functions into global ones, by building\nclosures pairing the transformed functions with the environment of their free\nvariables. Abstract machines rely on similar and yet different concepts of\nclosures and environments.\n  In this paper, we study the relationship between the two approaches. We adopt\na very simple {\\lambda}-calculus with tuples as source language and study\nabstract machines for both the source language and the target of closure\nconversion. Moreover, we focus on the simple case of flat\nclosures/environments, that is, with no sharing of environments. We provide\nthree contributions.\n  Firstly, a new simple proof technique for the correctness of closure\nconversion, inspired by abstract machines.\n  Secondly, we show how the closure invariants of the target language allow us\nto design a new way of handling environments in abstract machines, not\nsuffering the shortcomings of other styles.\n  Thirdly, we study the machines from the point of view of time complexity,\nadapting analyses by Accattoli and co-authors. We show that closure conversion\ndecreases various dynamic costs while increasing the size of the initial code.\nDespite these changes, the overall complexity of the machines before and after\nclosure conversion turns out to be the same.", "AI": {"tldr": "This paper compares closure conversion and abstract machine approaches in compiling functional languages, presenting new techniques for correctness proofs and environment handling, and demonstrates that closure conversion alters execution dynamics but keeps total complexity unchanged.", "motivation": "Closure conversion and abstract machines handle closures and environments in related yet distinct ways. Understanding their relationship, especially in simple settings, can improve correctness proofs and efficiency in functional language compilation.", "method": "The study uses a simple lambda-calculus with tuples as the source language, defines abstract machines for both source and closure-converted target languages (with flat closures/environments), introduces a new correctness proof technique, designs a new environment handling method, and adapts existing complexity analyses to compare the time complexity before and after closure conversion.", "result": "The paper provides: (1) a new proof technique for closure conversion correctness; (2) a new way to handle environments in abstract machines leveraging closure invariants; (3) an adapted time complexity analysis showing closure conversion trades increased code size for decreased dynamic execution costs, but overall complexity remains unchanged.", "conclusion": "Closure conversion and abstract machines can be more closely related than traditionally thought. New techniques allow for simpler correctness proofs, more efficient environment handling, and show that closure conversion, while changing certain computational aspects, preserves overall complexity."}}
{"id": "2507.14770", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14770", "abs": "https://arxiv.org/abs/2507.14770", "authors": ["Manaal Basha", "Ivan Beschastnikh", "Gema Rodriguez-Perez", "Cleidson R. B. de Souza"], "title": "Toward Inclusive AI-Driven Development: Exploring Gender Differences in Code Generation Tool Interactions", "comment": "ESEM 2025 Registered Reports", "summary": "Context: The increasing reliance on Code Generation Tools (CGTs), such as\nWindsurf and GitHub Copilot, are revamping programming workflows and raising\ncritical questions about fairness and inclusivity. While CGTs offer potential\nproductivity enhancements, their effectiveness across diverse user groups have\nnot been sufficiently investigated. Objectives: We hypothesize that developers'\ninteractions with CGTs vary based on gender, influencing task outcomes and\ncognitive load, as prior research suggests that gender differences can affect\ntechnology use and cognitive processing. Methods: The study will employ a\nmixed-subjects design with 54 participants, evenly divided by gender for a\ncounterbalanced design. Participants will complete two programming tasks\n(medium to hard difficulty) with only CGT assistance and then with only\ninternet access. Task orders and conditions will be counterbalanced to mitigate\norder effects. Data collection will include cognitive load surveys, screen\nrecordings, and task performance metrics such as completion time, code\ncorrectness, and CGT interaction behaviors. Statistical analyses will be\nconducted to identify statistically significant differences in CGT usage.\nExpected Contributions: Our work can uncover gender differences in CGT\ninteraction and performance among developers. Our findings can inform future\nCGT designs and help address usability and potential disparities in interaction\npatterns across diverse user groups. Conclusion: While results are not yet\navailable, our proposal lays the groundwork for advancing fairness,\naccountability, transparency, and ethics (FATE) in CGT design. The outcomes are\nanticipated to contribute to inclusive AI practices and equitable tool\ndevelopment for all users.", "AI": {"tldr": "This study proposes a controlled experiment to examine gender differences in how developers use code generation tools. The goal is to uncover disparities in task performance and cognitive load, ultimately guiding the development of more inclusive and fair developer tools.", "motivation": "Code Generation Tools (CGTs) are transforming programming workflows, but there is a lack of understanding regarding their effectiveness and fairness across varying user demographics, particularly gender. Previous studies suggest that gender can impact technology use and cognitive load, motivating this study to investigate these differences in the context of CGTs.", "method": "The study uses a mixed-subjects counterbalanced design with 54 participants (equal gender distribution). Each participant will complete two programming tasks (medium to hard difficulty) using CGT assistance and, separately, internet access only. Tasks and conditions are counterbalanced. Data collected will include cognitive load surveys, screen recordings, and performance metrics (completion time, code correctness, CGT interactions). Statistical methods will be used to analyze differences between groups.", "result": "There are no results yet, as the paper describes a proposed study. The expected outcome is to identify gender-based differences in CGT usage, task performance, and cognitive load.", "conclusion": "The study aims to inform the design of more fair, accountable, transparent, and ethical CGTs. Anticipated contributions include better understanding of gender disparities and pathways to more inclusive and equitable AI tools for software development."}}
{"id": "2502.15441", "categories": ["cs.SE", "cs.AI", "cs.FL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2502.15441", "abs": "https://arxiv.org/abs/2502.15441", "authors": ["Yang Hong", "Shan Jiang", "Yulei Fu", "Sarfraz Khurshid"], "title": "On the Effectiveness of Large Language Models in Writing Alloy Formulas", "comment": null, "summary": "Declarative specifications have a vital role to play in developing safe and\ndependable software systems. Writing specifications correctly, however, remains\nparticularly challenging. This paper presents a controlled experiment on using\nlarge language models (LLMs) to write declarative formulas in the well-known\nlanguage Alloy. Our use of LLMs is three-fold. One, we employ LLMs to write\ncomplete Alloy formulas from given natural language descriptions (in English).\nTwo, we employ LLMs to create alternative but equivalent formulas in Alloy with\nrespect to given Alloy formulas. Three, we employ LLMs to complete sketches of\nAlloy formulas and populate the holes in the sketches by synthesizing Alloy\nexpressions and operators so that the completed formulas accurately represent\nthe desired properties (that are given in natural language). We conduct the\nexperimental evaluation using 11 well-studied subject specifications and employ\ntwo popular LLMs, namely ChatGPT and DeepSeek. The experimental results show\nthat the LLMs generally perform well in synthesizing complete Alloy formulas\nfrom input properties given in natural language or in Alloy, and are able to\nenumerate multiple unique solutions. Moreover, the LLMs are also successful at\ncompleting given sketches of Alloy formulas with respect to natural language\ndescriptions of desired properties (without requiring test cases). We believe\nLLMs offer a very exciting advance in our ability to write specifications, and\ncan help make specifications take a pivotal role in software development and\nenhance our ability to build robust software.", "AI": {"tldr": "The paper demonstrates that large language models can reliably generate and complete Alloy declarative specifications from natural language, making specification writing easier and potentially leading to more robust and dependable software systems.", "motivation": "Declarative specifications are crucial for developing safe and reliable software systems, but writing them accurately is a significant challenge. The paper is motivated by the need to simplify and improve the correctness of writing such specifications, specifically in the Alloy language.", "method": "The authors design a controlled experiment using two major large language models (LLMs), ChatGPT and DeepSeek. They evaluate the LLMs on three tasks: (1) generating complete Alloy formulas from natural language, (2) producing alternative but equivalent Alloy formulas, and (3) completing partial Alloy formula sketches based on English descriptions, across 11 established subject specifications.", "result": "The results indicate that both ChatGPT and DeepSeek perform well across all evaluated tasks. They effectively generate accurate and multiple unique Alloy formulas from both natural language and Alloy, and successfully complete formula sketches without needing test cases.", "conclusion": "LLMs like ChatGPT and DeepSeek markedly enhance the process of writing declarative specifications in Alloy. Their ability to synthesize and complete formulas from natural language helps make specification writing more accessible, potentially broadening the use and correctness of specifications in safe software development."}}
{"id": "2507.14776", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14776", "abs": "https://arxiv.org/abs/2507.14776", "authors": ["Kimia Tasnia", "Alexander Garcia", "Tasnuva Farheen", "Sazadur Rahman"], "title": "VeriOpt: PPA-Aware High-Quality Verilog Generation via Multi-Role LLMs", "comment": "9 pages, 7 figures, Accepted for ICCAD 2025, Munich, Germany", "summary": "The rapid adoption of large language models(LLMs) in hardware design has\nprimarily focused on generating functionally correct Verilog code, overlooking\ncritical Power Performance-Area(PPA) metrics essential for industrial-grade\ndesigns. To bridge this gap, we propose VeriOpt, a novel framework that\nleverages role-based prompting and PPA-aware optimization to enable LLMs to\nproduce high-quality, synthesizable Verilog. VeriOpt structures LLM\ninteractions into specialized roles (e.g., Planner, Programmer, Reviewer,\nEvaluator) to emulate human design workflows, while integrating PPA constraints\ndirectly into the prompting pipeline. By combining multi-modal feedback (e.g.,\nsynthesis reports, timing diagrams) with PPA aware prompting, VeriOpt achieves\nPPA-efficient code generation without sacrificing functional correctness.\nExperimental results demonstrate up to 88% reduction in power, 76% reduction in\narea and 73% improvement in timing closure compared to baseline LLM-generated\nRTL, validated using industry standard EDA tools. At the same time achieves 86%\nsuccess rate in functionality evaluation. Our work advances the\nstate-of-the-art AI-driven hardware design by addressing the critical gap\nbetween correctness and quality, paving the way for reliable LLM adoption in\nproduction workflows.", "AI": {"tldr": "VeriOpt enhances LLM-generated hardware code by optimizing for power, performance, and area, utilizing role-based prompting and PPA-aware feedback, achieving significant efficiency gains and high functional correctness in industrial benchmarks.", "motivation": "Existing LLM-based approaches for hardware design focus on functional correctness of Verilog code but overlook key Power, Performance, and Area (PPA) requirements crucial for industrial applications.", "method": "The authors introduce VeriOpt, which uses role-based prompting (with roles like Planner, Programmer, Reviewer, Evaluator) paired with direct integration of PPA constraints and multi-modal feedback (such as synthesis reports and timing diagrams) in the LLM prompting process.", "result": "VeriOpt yielded up to 88% reduction in power, 76% reduction in area, and 73% better timing closure compared to standard LLM RTL generation, while retaining 86% functionality success rate, as validated by industry EDA tools.", "conclusion": "VeriOpt effectively bridges the gap between functionally correct and PPA-optimized Verilog generated by LLMs, enabling more practical adoption in industrial hardware design workflows."}}
{"id": "2507.14791", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14791", "abs": "https://arxiv.org/abs/2507.14791", "authors": ["Yang Liu", "Li Zhang", "Fang Liu", "Zhuohang Wang", "Donglin Wei", "Zhishuo Yang", "Kechi Zhang", "Jia Li", "Lin Shi"], "title": "Enhancing Repository-Level Code Generation with Call Chain-Aware Multi-View Context", "comment": null, "summary": "Repository-level code generation aims to generate code within the context of\na specified repository. Existing approaches typically employ\nretrieval-augmented generation (RAG) techniques to provide LLMs with relevant\ncontextual information extracted from the repository. However, these approaches\noften struggle with effectively identifying truly relevant contexts that\ncapture the rich semantics of the repository, and their contextual perspectives\nremains narrow. Moreover, most approaches fail to account for the structural\nrelationships in the retrieved code during prompt construction, hindering the\nLLM's ability to accurately interpret the context. To address these issues, we\npropose RepoScope, which leverages call chain-aware multi-view context for\nrepository-level code generation. RepoScope constructs a Repository Structural\nSemantic Graph (RSSG) and retrieves a comprehensive four-view context,\nintegrating both structural and similarity-based contexts. We propose a novel\ncall chain prediction method that utilizes the repository's structural\nsemantics to improve the identification of callees in the target function.\nAdditionally, we present a structure-preserving serialization algorithm for\nprompt construction, ensuring the coherence of the context for the LLM.\nNotably, RepoScope relies solely on static analysis, eliminating the need for\nadditional training or multiple LLM queries, thus ensuring both efficiency and\ngeneralizability. Evaluation on widely-used repository-level code generation\nbenchmarks (CoderEval and DevEval) demonstrates that RepoScope outperforms\nstate-of-the-art methods, achieving up to a 36.35% relative improvement in\npass@1 scores. Further experiments emphasize RepoScope's potential to improve\ncode generation across different tasks and its ability to integrate effectively\nwith existing approaches.", "AI": {"tldr": "RepoScope, a new static analysis-based method, greatly improves repository-level code generation by providing LLMs with richer, structurally-informed context. It beats existing methods in benchmark tests, requires no retraining, and boosts generation accuracy by over 36%.", "motivation": "Existing repository-level code generation methods struggle to find relevant contextual information that captures the repository\u2019s rich semantics and often have a narrow contextual perspective. They also fail to consider structural relationships in code, which hurts large language models\u2019 (LLMs) ability to interpret context for code generation.", "method": "RepoScope is introduced as a new approach. It builds a Repository Structural Semantic Graph (RSSG) and retrieves a comprehensive, four-view context that combines both structural and similarity-based contexts. RepoScope uses a novel call chain prediction method that leverages repository structural semantics to enhance callee identification, and presents a structure-preserving serialization algorithm for constructing LLM prompts. Importantly, it relies only on static analysis and requires no extra training or multiple LLM queries.", "result": "RepoScope achieves up to a 36.35% relative improvement in pass@1 scores on CoderEval and DevEval benchmarks over prior state-of-the-art methods. Additional experiments show RepoScope improves code generation across various tasks and integrates well with existing solutions.", "conclusion": "RepoScope provides a more contextually aware and structurally rich method for repository-level code generation, significantly outperforming current methods in both accuracy and adaptability, without requiring extra model training or multiple LLM queries."}}
{"id": "2507.14969", "categories": ["cs.SE", "D.2.1"], "pdf": "https://arxiv.org/pdf/2507.14969", "abs": "https://arxiv.org/abs/2507.14969", "authors": ["Sai Zhang", "Zhenchang Xing", "Jieshan Chen", "Dehai Zhao", "Zizhong Zhu", "Xiaowang Zhang", "Zhiyong Feng", "Xiaohong Li"], "title": "Think Like an Engineer: A Neuro-Symbolic Collaboration Agent for Generative Software Requirements Elicitation and Self-Review", "comment": null, "summary": "The vision of End-User Software Engineering (EUSE) is to empower\nnon-professional users with full control over the software development\nlifecycle. It aims to enable users to drive generative software development\nusing only natural language requirements. However, since end-users often lack\nknowledge of software engineering, their requirement descriptions are\nfrequently ambiguous, raising significant challenges to generative software\ndevelopment. Although existing approaches utilize structured languages like\nGherkin to clarify user narratives, they still struggle to express the causal\nlogic between preconditions and behavior actions. This paper introduces\nRequireCEG, a requirement elicitation and self-review agent that embeds\ncausal-effect graphs (CEGs) in a neuro-symbolic collaboration architecture.\nRequireCEG first uses a feature tree to analyze user narratives hierarchically,\nclearly defining the scope of software components and their system behavior\nrequirements. Next, it constructs the self-healing CEGs based on the elicited\nrequirements, capturing the causal relationships between atomic preconditions\nand behavioral actions. Finally, the constructed CEGs are used to review and\noptimize Gherkin scenarios, ensuring consistency between the generated Gherkin\nrequirements and the system behavior requirements elicited from user\nnarratives. To evaluate our method, we created the RGPair benchmark dataset and\nconducted extensive experiments. It achieves an 87% coverage rate and raises\ndiversity by 51.88%.", "AI": {"tldr": "RequireCEG helps non-programmers clearly define software requirements by translating ambiguous user stories into structured, causally-expressive scenarios, improving coverage and variety over previous methods.", "motivation": "End-User Software Engineering (EUSE) seeks to let non-professional users control software development with natural language, but user requirements are often ambiguous, making generative software development difficult. Structured languages help but do not fully capture causal logic between requirements' preconditions and actions.", "method": "The authors propose RequireCEG, a requirement elicitation and review agent using a neuro-symbolic approach with causal-effect graphs (CEGs). It analyzes user narratives with feature trees to define scope and requirements, constructs CEGs to capture causality, and uses these CEGs to refine and validate Gherkin scenarios for consistency.", "result": "RequireCEG was evaluated on the new RGPair benchmark, achieving an 87% coverage rate and increasing requirements diversity by 51.88%.", "conclusion": "RequireCEG offers a novel, effective neuro-symbolic method for end-user-driven requirements elicitation and review, addressing ambiguity and causal logic in user-generated requirements. This approach significantly improves coverage and diversity in generated software requirements."}}
{"id": "2507.15003", "categories": ["cs.SE", "cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15003", "abs": "https://arxiv.org/abs/2507.15003", "authors": ["Hao Li", "Haoxiang Zhang", "Ahmed E. Hassan"], "title": "The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering", "comment": null, "summary": "The future of software engineering--SE 3.0--is unfolding with the rise of AI\nteammates: autonomous, goal-driven systems collaborating with human developers.\nAmong these, autonomous coding agents are especially transformative, now\nactively initiating, reviewing, and evolving code at scale. This paper\nintroduces AIDev, the first large-scale dataset capturing how such agents\noperate in the wild. Spanning over 456,000 pull requests by five leading\nagents--OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code--across\n61,000 repositories and 47,000 developers, AIDev provides an unprecedented\nempirical foundation for studying autonomous teammates in software development.\n  Unlike prior work that has largely theorized the rise of AI-native software\nengineering, AIDev offers structured, open data to support research in\nbenchmarking, agent readiness, optimization, collaboration modeling, and AI\ngovernance. The dataset includes rich metadata on PRs, authorship, review\ntimelines, code changes, and integration outcomes--enabling exploration beyond\nsynthetic benchmarks like SWE-bench. For instance, although agents often\noutperform humans in speed, their PRs are accepted less frequently, revealing a\ntrust and utility gap. Furthermore, while agents accelerate code\nsubmission--one developer submitted as many PRs in three days as they had in\nthree years--these are structurally simpler (via code complexity metrics).\n  We envision AIDev as a living resource: extensible, analyzable, and ready for\nthe SE and AI communities. Grounding SE 3.0 in real-world evidence, AIDev\nenables a new generation of research into AI-native workflows and supports\nbuilding the next wave of symbiotic human-AI collaboration. The dataset is\npublicly available at https://github.com/SAILResearch/AI_Teammates_in_SE3.\n  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering\nAgent", "AI": {"tldr": "This paper introduces AIDev, the first large-scale, real-world dataset of AI coding agent activity. It enables empirical research into how autonomous agents contribute to software engineering, revealing both their speed advantage and trust gaps compared to humans. AIDev is intended as a living resource to drive future studies and improvements in human-AI collaboration.", "motivation": "The paper is motivated by the emergence of AI teammates\u2014autonomous, goal-driven agents working alongside human software developers. While these agents, such as coding assistants, are transforming the software engineering landscape, most prior research has focused on theorizing their impact rather than providing practical, empirical resources.", "method": "The authors introduce AIDev, a large-scale dataset that captures real-world activity of autonomous coding agents. They collected and structured data from over 456,000 pull requests involving five major AI coding agents across 61,000 repositories and 47,000 developers, providing rich metadata on code changes, authorship, review processes, and integration outcomes.", "result": "AIDev reveals key findings: AI coding agents often submit code much faster than humans but have lower pull request acceptance rates, highlighting trust and utility gaps. The dataset uncovers that while submission frequency rises, the structural complexity of the agents\u2019 contributions is lower. The open dataset enables new empirical research into AI-native software development, benchmarking, collaboration, and governance.", "conclusion": "AIDev represents a foundational, extensible resource for studying AI teammates in real-world software engineering. It grounds discussions of AI-driven software development (SE 3.0) in empirical reality and is expected to spur research into human-AI collaboration, benchmarking, and optimization. The dataset is publicly available to facilitate ongoing innovation."}}
{"id": "2507.15025", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15025", "abs": "https://arxiv.org/abs/2507.15025", "authors": ["Nenad Petrovic", "Vahid Zolfaghari", "Andre Schamschurko", "Sven Kirchner", "Fengjunjie Pan", "Chengdng Wu", "Nils Purschke", "Aleksei Velsh", "Krzysztof Lebioda", "Yinglei Song", "Yi Zhang", "Lukasz Mazur", "Alois Knoll"], "title": "Survey of GenAI for Automotive Software Development: From Requirements to Executable Code", "comment": "Conference paper accepted for GACLM 2025", "summary": "Adoption of state-of-art Generative Artificial Intelligence (GenAI) aims to\nrevolutionize many industrial areas by reducing the amount of human\nintervention needed and effort for handling complex underlying processes.\nAutomotive software development is considered to be a significant area for\nGenAI adoption, taking into account lengthy and expensive procedures, resulting\nfrom the amount of requirements and strict standardization. In this paper, we\nexplore the adoption of GenAI for various steps of automotive software\ndevelopment, mainly focusing on requirements handling, compliance aspects and\ncode generation. Three GenAI-related technologies are covered within the\nstate-of-art: Large Language Models (LLMs), Retrieval Augmented Generation\n(RAG), Vision Language Models (VLMs), as well as overview of adopted prompting\ntechniques in case of code generation. Additionally, we also derive a\ngeneralized GenAI-aided automotive software development workflow based on our\nfindings from this literature review. Finally, we include a summary of a survey\noutcome, which was conducted among our automotive industry partners regarding\nthe type of GenAI tools used for their daily work activities.", "AI": {"tldr": "This paper reviews the role of generative AI (GenAI)\u2014including LLMs, RAG, and VLMs\u2014in automotive software development, focusing on requirements, compliance, and coding. It proposes a generalized workflow for GenAI integration and reports on industry adoption levels based on a survey.", "motivation": "The motivation is to improve efficiency and reduce human intervention and effort in automotive software development, which is currently hampered by complex, time-consuming, and expensive processes due to heavy requirements and stringent regulations.", "method": "The paper conducts a literature review exploring the use of generative AI (GenAI) technologies\u2014specifically Large Language Models (LLMs), Retrieval Augmented Generation (RAG), and Vision Language Models (VLMs)\u2014for multiple steps in automotive software development. It analyzes prompting techniques for code generation and gathers practical insights via a survey of industry partners.", "result": "The study maps out existing GenAI technologies relevant to automotive software development, highlights their applications in requirements handling, compliance, and code generation, and presents a generalized workflow for GenAI-aided development. It also reports on the actual GenAI tools being adopted by industry professionals, as identified through a survey.", "conclusion": "GenAI has significant potential to transform automotive software development across various key stages by automating tasks, streamlining compliance, and aiding code generation. The paper provides both a conceptual overview and some initial practical findings about GenAI adoption in the automotive sector."}}
{"id": "2507.15157", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15157", "abs": "https://arxiv.org/abs/2507.15157", "authors": ["Giovanni Quattrocchi", "Liliana Pasquale", "Paola Spoletini", "Luciano Baresi"], "title": "Can LLMs Generate User Stories and Assess Their Quality?", "comment": null, "summary": "Requirements elicitation is still one of the most challenging activities of\nthe requirements engineering process due to the difficulty requirements\nanalysts face in understanding and translating complex needs into concrete\nrequirements. In addition, specifying high-quality requirements is crucial, as\nit can directly impact the quality of the software to be developed. Although\nautomated tools allow for assessing the syntactic quality of requirements,\nevaluating semantic metrics (e.g., language clarity, internal consistency)\nremains a manual and time-consuming activity. This paper explores how LLMs can\nhelp automate requirements elicitation within agile frameworks, where\nrequirements are defined as user stories (US). We used 10 state-of-the-art LLMs\nto investigate their ability to generate US automatically by emulating customer\ninterviews. We evaluated the quality of US generated by LLMs, comparing it with\nthe quality of US generated by humans (domain experts and students). We also\nexplored whether and how LLMs can be used to automatically evaluate the\nsemantic quality of US. Our results indicate that LLMs can generate US similar\nto humans in terms of coverage and stylistic quality, but exhibit lower\ndiversity and creativity. Although LLM-generated US are generally comparable in\nquality to those created by humans, they tend to meet the acceptance quality\ncriteria less frequently, regardless of the scale of the LLM model. Finally,\nLLMs can reliably assess the semantic quality of US when provided with clear\nevaluation criteria and have the potential to reduce human effort in\nlarge-scale assessments.", "AI": {"tldr": "This paper finds that LLMs can generate and evaluate agile user stories at a quality level comparable to humans, though with less creativity and slightly lower adherence to acceptance criteria. LLMs can also reliably automate semantic quality assessment, potentially saving significant manual effort.", "motivation": "Eliciting and specifying high-quality software requirements is difficult, especially in agile frameworks. While automated tools handle syntactic assessment, semantic evaluation remains largely manual and time-consuming. The authors aim to explore how large language models (LLMs) can help automate these activities, particularly for user stories (US).", "method": "The authors used 10 state-of-the-art large language models (LLMs) to automatically generate user stories by simulating customer interviews. They compared the quality of LLM-generated US with those written by humans (domain experts and students) and examined LLMs' ability to assess semantic quality when given clear criteria.", "result": "LLMs can produce user stories that match human quality in terms of coverage and style, though they lag in diversity and creativity. LLM-generated stories less frequently meet acceptance quality criteria, and this gap is consistent across different LLM sizes. LLMs are reliable in assessing US semantic quality when clear evaluation guidelines are provided, potentially reducing human effort in large-scale evaluations.", "conclusion": "LLMs show promise for automating both the generation and semantic assessment of user stories, offering comparable results to humans in quality but with reduced creativity and acceptance fulfillment. Given specific evaluation criteria, LLMs can greatly assist in scaling up quality assessments."}}
{"id": "2507.15181", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15181", "abs": "https://arxiv.org/abs/2507.15181", "authors": ["Yinglong Zou", "Juan Zhai", "Chunrong Fang", "Yanzhou Mu", "Jiawei Liu", "Zhenyu Chen"], "title": "Deep Learning Framework Testing via Heuristic Guidance Based on Multiple Model Measurements", "comment": null, "summary": "Deep learning frameworks serve as the foundation for developing and deploying\ndeep learning applications. To enhance the quality of deep learning frameworks,\nresearchers have proposed numerous testing methods using deep learning models\nas test inputs. However, existing methods predominantly measure model bug\ndetection effectiveness as heuristic indicators, presenting three critical\nlimitations: Firstly, existing methods fail to quantitatively measure model's\noperator combination variety, potentially missing critical operator\ncombinations that could trigger framework bugs. Secondly, existing methods\nneglect measuring model execution time, resulting in the omission of numerous\nmodels potential for detecting more framework bugs within limited testing time.\nThirdly, existing methods overlook correlation between different model\nmeasurements, relying simply on single-indicator heuristic guidance without\nconsidering their trade-offs. To overcome these limitations, we propose DLMMM,\nthe first deep learning framework testing method to include multiple model\nmeasurements into heuristic guidance and fuse these measurements to achieve\ntheir trade-off. DLMMM firstly quantitatively measures model's bug detection\nperformance, operator combination variety, and model execution time. After\nthat, DLMMM fuses the above measurements based on their correlation to achieve\ntheir trade-off. To further enhance testing effectiveness, DLMMM designs\nmulti-level heuristic guidance for test input model generation.", "AI": {"tldr": "Existing testing methods for deep learning frameworks miss important aspects of model evaluation. DLMMM is proposed to address these issues by simultaneously considering and fusing multiple quantitative factors, including bug detection, operator combination diversity, and execution time, thus achieving better and more efficient testing outcomes.", "motivation": "Existing methods for testing deep learning frameworks rely on heuristic indicators to measure bug detection effectiveness but have significant limitations: lack of quantitative measurement of operator combinations, ignoring model execution time, and overlooking correlations between different model measurements.", "method": "The authors propose DLMMM, a new deep learning framework testing method. DLMMM quantitatively measures three aspects\u2014bug detection performance, operator combination variety, and model execution time\u2014then fuses these measurements by considering their mutual correlations to balance the trade-offs. It also introduces multi-level heuristic guidance during the model generation process.", "result": "DLMMM enables more comprehensive and effective testing by utilizing multiple fused measurements, addressing trade-offs among different aspects of model testing, and generating better test input models for deep learning frameworks.", "conclusion": "DLMMM overcomes the three core limitations of prior testing methods by integrating multiple quantitative measurements and their correlations, thus enhancing testing efficiency and the probability of uncovering framework bugs."}}
{"id": "2507.15188", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.15188", "abs": "https://arxiv.org/abs/2507.15188", "authors": ["Chowdhury Shahriar Muzammel", "Maria Spichkova", "James Harland"], "title": "Cultural Impact on Requirements Engineering Activities: Bangladeshi Practitioners' View", "comment": null, "summary": "Requirements Engineering (RE) is one of the most interaction-intensive phases\nof software development. This means that RE activities might be especially\nimpacted by stakeholders' national culture. Software development projects\nincreasingly have a very diverse range of stakeholders. To future-proof RE\nactivities, we need to help RE practitioners avoid misunderstandings and\nconflicts that might arise from not understanding potential Cultural Influences\n(CIs). Moreover, an awareness of CIs supports diversity and inclusion in the IT\nprofession. Bangladesh has a growing IT sector with some unique socio-cultural\ncharacteristics, and has been largely overlooked in this research field. In\nthis study, we aim to investigate how the RE process is adopted in the context\nof Bangladeshi culture and what cultural influences impact overall RE\nactivities.", "AI": {"tldr": "The study examines how Bangladeshi culture affects Requirements Engineering processes and highlights the importance of cultural awareness for effective and inclusive software development.", "motivation": "Requirements Engineering (RE) involves significant interactions among stakeholders, and cultural differences can affect these interactions. As software projects become more global, understanding Cultural Influences (CIs) is necessary to avoid misunderstandings and support diversity, especially since some regions like Bangladesh have not been thoroughly studied in this context.", "method": "The paper investigates the adoption of RE processes within the cultural context of Bangladesh by examining how local socio-cultural characteristics influence RE activities.", "result": "The study identifies specific cultural influences within Bangladesh that impact the implementation and effectiveness of RE activities in software development projects.", "conclusion": "Understanding and addressing cultural influences in RE can help practitioners in Bangladesh (and similar contexts) to reduce misunderstandings and support more inclusive software engineering practices."}}
{"id": "2507.15197", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.15197", "abs": "https://arxiv.org/abs/2507.15197", "authors": ["Chowdhury Shahriar Muzammel", "Maria Spichkova", "James Harland"], "title": "Towards Using Personas in Requirements Engineering: What Has Been Changed Recently?", "comment": null, "summary": "In requirements engineering (RE), personas are now being used to represent\nuser expectations and needs. This systematic mapping study (SMS) aims to\nexplore the most recent studies and to cover recent changes in trends,\nespecially related to the recent evolution of Generative AI approaches. Our SMS\ncovers the period between April 2023 and April 2025. We identified 22 relevant\npublications and analysed persona representation, construction, validation, as\nwell as RE activities covered by personas. We identified that a number of\nstudies applied AI-based solutions for persona construction and validation. We\nobserved that template-based personas are becoming more popular nowadays. We\nalso observed an increase in the proportion of studies covering validation\naspects.", "AI": {"tldr": "The paper maps recent studies (2023-2025) on personas in requirements engineering, highlighting that AI (especially generative) is now commonly used for creating and validating personas, with templates and validation gaining more attention.", "motivation": "Personas are increasingly used in requirements engineering (RE) to capture user needs and expectations. With the rise of Generative AI, there may be recent shifts in how personas are created and validated. The authors want to map current trends and approaches, especially those influenced by new AI methods.", "method": "The authors conducted a systematic mapping study (SMS) covering literature from April 2023 to April 2025. They reviewed 22 relevant publications, analyzing themes such as persona representation, construction, validation, and the range of RE activities involving personas.", "result": "The study found a growing application of AI-based approaches for both persona construction and validation. Template-based personas have become more popular, and there is a noticeable increase in studies that address persona validation.", "conclusion": "Recent work in RE shows an increasing integration of AI, particularly generative models, in the creation and validation of personas. There's a trend toward standardized, template-based personas and greater attention to their validation in the field."}}
{"id": "2507.15224", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15224", "abs": "https://arxiv.org/abs/2507.15224", "authors": ["Yibo He", "Shuoran Zhao", "Jiaming Huang", "Yingjie Fu", "Hao Yu", "Cunjian Huang", "Tao Xie"], "title": "SimdBench: Benchmarking Large Language Models for SIMD-Intrinsic Code Generation", "comment": null, "summary": "SIMD (Single Instruction Multiple Data) instructions and their compiler\nintrinsics are widely supported by modern processors to accelerate\nperformance-critical tasks. SIMD intrinsic programming, a trade-off between\ncoding productivity and high performance, is widely used in the development of\nmainstream performance-critical libraries and daily computing tasks. Large\nLanguage Models (LLMs), which have demonstrated strong and comprehensive\ncapabilities in code generation, show promise in assisting programmers with the\nchallenges of SIMD intrinsic programming. However, existing code-generation\nbenchmarks focus on only scalar code, and it is unclear how LLMs perform in\ngenerating vectorized code using SIMD intrinsics. To fill this gap, we propose\nSimdBench, the first code benchmark specifically designed for SIMD-intrinsic\ncode generation, comprising 136 carefully crafted tasks and targeting five\nrepresentative SIMD intrinsics: SSE (x86 Streaming SIMD Extension), AVX (x86\nAdvanced Vector Extension), Neon (ARM Advanced SIMD Extension), SVE (ARM\nScalable Vector Extension), and RVV (RISC-V Vector Extension). We conduct a\nsystematic evaluation (measuring both correctness and performance) of 18\nrepresentative LLMs on SimdBench, resulting in a series of novel and insightful\nfindings. Our evaluation results demonstrate that LLMs exhibit a universal\ndecrease in pass@k during SIMD-intrinsic code generation compared to\nscalar-code generation. Our in-depth analysis highlights promising directions\nfor the further advancement of LLMs in the challenging domain of SIMD-intrinsic\ncode generation. SimdBench is fully open source at\nhttps://anonymous.4open.science/r/SimdBench-1B3F/ to benefit the broader\nresearch community.", "AI": {"tldr": "A new benchmark (SimdBench) for generating SIMD code with LLMs reveals that current models are much less effective at this task than for scalar code, pointing to the need for improvements and further research.", "motivation": "SIMD intrinsic programming is essential for high performance in critical software, but coding it is challenging, and support for SIMD code generation with Large Language Models (LLMs) is unstudied. Existing code-generation benchmarks do not address SIMD, making performance and correctness in this domain unknown.", "method": "The authors introduce SimdBench, a new benchmark specifically for SIMD-intrinsic code generation. SimdBench features 136 tasks using five major SIMD instruction sets (SSE, AVX, Neon, SVE, RVV). They systematically evaluate 18 LLMs using this benchmark, assessing both code correctness and performance.", "result": "Their experiments show that all LLMs have a significant drop in successful code generation (pass@k metrics) when producing SIMD-intrinsic code compared to scalar code. The paper presents new findings and identifies potential research directions for improving LLMs\u2019 performance in this area.", "conclusion": "LLMs currently struggle more with SIMD-intrinsic code generation than scalar code generation. SimdBench highlights these challenges and will help the research community advance LLM capabilities in high-performance code generation."}}
{"id": "2507.15226", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15226", "abs": "https://arxiv.org/abs/2507.15226", "authors": ["Changguo Jia", "Yi Zhan", "Tianqi Zhao", "Hengzhi Ye", "Minghui Zhou"], "title": "Code Clone Detection via an AlphaFold-Inspired Framework", "comment": null, "summary": "Code clone detection, which aims to identify functionally equivalent code\nfragments, plays a critical role in software maintenance and vulnerability\nanalysis. Substantial methods have been proposed to detect code clones, but\nthey fall short in capturing code semantics or relying on language-specific\nanalyzers. Inspired by the remarkable success of AlphaFold in predicting\nthree-dimensional protein structures from protein sequences, in this paper, we\nleverage AlphaFold for code clone detection based on the insight that protein\nsequences and token sequences share a common linear sequential structure. In\nparticular, we propose AlphaCC, which represents code fragments as token\nsequences to ensure multi-language applicability and adapts AlphaFold's\nsequence-to-structure modeling capability to infer code semantics. The pipeline\nof AlphaCC goes through three steps. First, AlphaCC transforms each input code\nfragment into a token sequence and, motivated by AlphaFold's use of multiple\nsequence alignment (MSA) to enhance contextual understanding, constructs an MSA\nfrom lexically similar token sequences. Second, AlphaCC adopts a modified\nattention-based encoder based on AlphaFold to model dependencies within and\nacross token sequences. Finally, unlike AlphaFold's protein structure\nprediction task, AlphaCC computes similarity scores between token sequences\nthrough a late interaction strategy and performs binary classification to\ndetermine code clone pairs. Comprehensive evaluations on three language-diverse\ndatasets demonstrate AlphaCC's applicability across multiple programming\nlanguages. On two semantic clone detection datasets, it consistently\noutperforms all baselines, showing strong semantic understanding. Moreover,\nAlphaCC maintains competitive efficiency, enabling practical usage in\nlarge-scale clone detection tasks.", "AI": {"tldr": "AlphaCC, inspired by AlphaFold, introduces a novel, multi-language semantic code clone detector using protein sequence modeling strategies, and achieves superior accuracy and efficiency across diverse datasets.", "motivation": "Existing code clone detection methods either inadequately capture code semantics or rely heavily on language-specific analyzers. There is a need for a more general and semantically robust approach.", "method": "Inspired by AlphaFold, the paper proposes AlphaCC, which models code fragments as token sequences and uses a sequence-to-structure-like approach for semantic inference. It first converts code into token sequences and constructs multiple sequence alignments (MSA), applies a modified attention-based encoder (adapted from AlphaFold) to generate contextual semantic representations, and finally computes similarity scores using a late interaction strategy for binary classification of code clones.", "result": "AlphaCC demonstrates strong semantic understanding, outperforming all baselines on two semantic clone detection datasets and showing broad applicability across programming languages. It also achieves competitive efficiency suitable for large-scale deployment.", "conclusion": "AlphaCC leverages protein sequence modeling concepts to improve code clone detection's semantic understanding and language generality, confirming its effectiveness and practicality through extensive evaluation."}}
{"id": "2507.15241", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15241", "abs": "https://arxiv.org/abs/2507.15241", "authors": ["Vikram Nitin", "Baishakhi Ray", "Roshanak Zilouchian Moghaddam"], "title": "FaultLine: Automated Proof-of-Vulnerability Generation Using LLM Agents", "comment": null, "summary": "Despite the critical threat posed by software security vulnerabilities,\nreports are often incomplete, lacking the proof-of-vulnerability (PoV) tests\nneeded to validate fixes and prevent regressions. These tests are crucial not\nonly for ensuring patches work, but also for helping developers understand how\nvulnerabilities can be exploited. Generating PoV tests is a challenging\nproblem, requiring reasoning about the flow of control and data through deeply\nnested levels of a program.\n  We present FaultLine, an LLM agent workflow that uses a set of carefully\ndesigned reasoning steps, inspired by aspects of traditional static and dynamic\nprogram analysis, to automatically generate PoV test cases. Given a software\nproject with an accompanying vulnerability report, FaultLine 1) traces the flow\nof an input from an externally accessible API (\"source\") to the \"sink\"\ncorresponding to the vulnerability, 2) reasons about the conditions that an\ninput must satisfy in order to traverse the branch conditions encountered along\nthe flow, and 3) uses this reasoning to generate a PoV test case in a\nfeedback-driven loop. FaultLine does not use language-specific static or\ndynamic analysis components, which enables it to be used across programming\nlanguages.\n  To evaluate FaultLine, we collate a challenging multi-lingual dataset of 100\nknown vulnerabilities in Java, C and C++ projects. On this dataset, FaultLine\nis able to generate PoV tests for 16 projects, compared to just 9 for CodeAct\n2.1, a popular state-of-the-art open-source agentic framework. Thus, FaultLine\nrepresents a 77% relative improvement over the state of the art. Our findings\nsuggest that hierarchical reasoning can enhance the performance of LLM agents\non PoV test generation, but the problem in general remains challenging. We make\nour code and dataset publicly available in the hope that it will spur further\nresearch in this area.", "AI": {"tldr": "FaultLine, an LLM-powered workflow, outperforms prior methods in automatically generating vulnerability exploit tests across programming languages, but the task is still challenging; their resources are open for community use.", "motivation": "Software vulnerability reports often lack proof-of-vulnerability (PoV) tests, which are important for validating fixes, preventing regressions, and helping developers understand exploits. Manual generation of these tests is difficult due to the complexity of reasoning about control and data flow in programs.", "method": "The authors introduce FaultLine, an LLM agent workflow that leverages structured, step-by-step reasoning inspired by static and dynamic program analysis to automatically generate PoV test cases. FaultLine identifies input flow from an API to the vulnerable sink, determines path constraints, and generates tests in a feedback-driven loop, without employing language-specific analysis, enabling multi-language use.", "result": "FaultLine was evaluated on a challenging dataset of 100 vulnerabilities across Java, C, and C++ projects. It successfully generated PoV tests for 16 projects, compared to 9 by the state-of-the-art CodeAct 2.1, representing a 77% relative improvement.", "conclusion": "FaultLine demonstrates that hierarchical reasoning enhances LLM agent performance in PoV test generation across multiple languages. However, the general problem remains difficult. The project's code and dataset are publicly released to foster further research."}}
{"id": "2507.15251", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15251", "abs": "https://arxiv.org/abs/2507.15251", "authors": ["Boyang Yang", "Luyao Ren", "Xin Yin", "Jiadong Ren", "Haoye Tian", "Shunfu Jin"], "title": "Input Reduction Enhanced LLM-based Program Repair", "comment": null, "summary": "Large Language Models (LLMs) have shown great potential in Automated Program\nRepair (APR). Test inputs, being crucial for reasoning the root cause of\nfailures, are always included in the prompt for LLM-based APR. Unfortunately,\nLLMs struggle to retain key information in long prompts. When the test inputs\nare extensive in the prompt, this may trigger the \"lost-in-the-middle\" issue,\ncompromising repair performance. To address this, we propose ReduceFix, an\nLLM-based APR approach with a built-in component that automatically reduces\ntest inputs while retaining their failure-inducing behavior. ReduceFix prompts\nan LLM to generate a reducer that minimizes failure-inducing test inputs\nwithout human effort, and then feeds the reduced failure-inducing inputs to\nguide patch generation.\n  For targeted evaluation, we constructed LFTBench, the first long-input APR\nbenchmark with 200 real bugs from 20 programming tasks, each paired with a\nfailure-inducing input whose median size is 1 MB. On this benchmark, ReduceFix\nshrinks inputs by 89.1% on average and improves overall pass@10 by up to 53.8%\nrelative to a prompt that includes the original test, and by 17.6% compared\nwith omitting the test entirely. Adding the same reduction step to ChatRepair\nincreases its fix rate by 21.3% without other changes. Ablation studies further\nhighlight the impact of input length and compressed failure information on\nrepair success. These results underscore that automatically reducing failing\ninputs is a practical and powerful complement to LLM-based APR, significantly\nimproving its scalability and effectiveness.", "AI": {"tldr": "The paper introduces ReduceFix, an approach that prompts LLMs to generate smaller, failure-preserving test inputs for automated program repair. This significantly improves repair rates and overcomes the limitations of long-input prompts, as demonstrated on a new benchmark dataset.", "motivation": "Large Language Models (LLMs) struggle to effectively repair code when given long test inputs, as key information may be lost due to the \"lost-in-the-middle\" issue, negatively impacting Automated Program Repair (APR) performance.", "method": "The authors propose ReduceFix, an LLM-based APR approach that automatically reduces test inputs by generating a \"reducer\" via the LLM itself, ensuring that reduced inputs still induce the same failure. The reduced inputs are then used to guide the patch generation process.", "result": "ReduceFix was evaluated using LFTBench, a new benchmark of 200 real bugs with large failure-inducing inputs. ReduceFix shrank inputs by 89.1% on average, improved pass@10 repair rates by up to 53.8% compared to using the original test inputs, and by 17.6% versus omitting tests. Enhanced ChatRepair with this reduction achieved a 21.3% higher fix rate. Ablation studies confirmed the importance of input length and compressed failure information.", "conclusion": "Automatically reducing failure-inducing test inputs is an effective way to improve the scalability and performance of LLM-based APR approaches."}}
{"id": "2507.15296", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15296", "abs": "https://arxiv.org/abs/2507.15296", "authors": ["Qian Xiong", "Yuekai Huang", "Ziyou Jiang", "Zhiyuan Chang", "Yujia Zheng", "Tianhao Li", "Mingyang Li"], "title": "Butterfly Effects in Toolchains: A Comprehensive Analysis of Failed Parameter Filling in LLM Tool-Agent Systems", "comment": null, "summary": "The emergence of the tool agent paradigm has broadened the capability\nboundaries of the Large Language Model (LLM), enabling it to complete more\ncomplex tasks. However, the effectiveness of this paradigm is limited due to\nthe issue of parameter failure during its execution. To explore this phenomenon\nand propose corresponding suggestions, we first construct a parameter failure\ntaxonomy in this paper. We derive five failure categories from the invocation\nchain of a mainstream tool agent. Then, we explore the correlation between\nthree different input sources and failure categories by applying 15 input\nperturbation methods to the input. Experimental results show that parameter\nname hallucination failure primarily stems from inherent LLM limitations, while\nissues with input sources mainly cause other failure patterns. To improve the\nreliability and effectiveness of tool-agent interactions, we propose\ncorresponding improvement suggestions, including standardizing tool return\nformats, improving error feedback mechanisms, and ensuring parameter\nconsistency.", "AI": {"tldr": "The paper categorizes various parameter failures in tool agent LLMs, analyzes the causes, and proposes solutions to boost reliability\u2014including standardizing outputs and improving feedback mechanisms.", "motivation": "The paper is motivated by the desire to enhance the reliability and effectiveness of tool agents powered by Large Language Models (LLMs), particularly in addressing the issue of parameter failure that limits the paradigm's capabilities.", "method": "The authors construct a taxonomy of parameter failures by analyzing the invocation chain of a mainstream tool agent. They apply 15 input perturbation methods to three different input sources to investigate the link between input types and parameter failure categories.", "result": "Experimental results reveal that parameter name hallucination failure is mostly due to inherent LLM constraints, whereas other failure types are mainly caused by issues with the input sources.", "conclusion": "To mitigate parameter failures, the paper recommends standardizing tool return formats, enhancing error feedback mechanisms, and ensuring parameter consistency to improve tool-agent reliability."}}
{"id": "2507.15343", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15343", "abs": "https://arxiv.org/abs/2507.15343", "authors": ["Kechi Zhang", "Ge Li", "Jia Li", "Huangzhao Zhang", "Yihong Dong", "Jia Li", "Jingjing Xu", "Zhi Jin"], "title": "StackTrans: From Large Language Model to Large Pushdown Automata Model", "comment": "currently under development", "summary": "The Transformer architecture has emerged as a landmark advancement within the\nbroad field of artificial intelligence, effectively catalyzing the advent of\nlarge language models (LLMs). However, despite its remarkable capabilities and\nthe substantial progress it has facilitated, the Transformer architecture still\nhas some limitations. One such intrinsic limitation is its inability to\neffectively capture the Chomsky hierarchy, such as regular expressions or\ndeterministic context-free grammars. Drawing inspiration from pushdown\nautomata, which efficiently resolve deterministic context-free grammars using\nstacks, we propose StackTrans to address the aforementioned issue within LLMs.\nUnlike previous approaches that modify the attention computation, StackTrans\nexplicitly incorporates hidden state stacks between Transformer layers. This\ndesign maintains compatibility with existing frameworks like flash-attention.\nSpecifically, our design features stack operations -- such as pushing and\npopping hidden states -- that are differentiable and can be learned in an\nend-to-end manner. Our comprehensive evaluation spans benchmarks for both\nChomsky hierarchies and large-scale natural languages. Across these diverse\ntasks, StackTrans consistently outperforms standard Transformer models and\nother baselines. We have successfully scaled StackTrans up from 360M to 7B\nparameters. In particular, our from-scratch pretrained model StackTrans-360M\noutperforms several larger open-source LLMs with 2-3x more parameters,\nshowcasing its superior efficiency and reasoning capability.", "AI": {"tldr": "StackTrans is a stack-augmented Transformer that overcomes standard models' limitations with formal language structures. It outperforms larger LLMs in reasoning tasks, demonstrating enhanced efficiency and capability.", "motivation": "The Transformer architecture, despite powering advancements in large language models, struggles to effectively represent formal languages characterized by the Chomsky hierarchy (e.g., regular expressions, deterministic context-free grammars). This bottleneck constrains the reasoning and syntactic capabilities of LLMs.", "method": "The authors introduce StackTrans, a Transformer variant that incorporates differentiable hidden state stacks between layers, inspired by pushdown automata. These stack operations (push, pop) are fully differentiable, allowing StackTrans to learn stack manipulation in an end-to-end fashion. Importantly, this method is designed to be compatible with existing efficient attention mechanisms such as flash-attention.", "result": "StackTrans was evaluated on benchmarks for formal languages and large-scale natural language tasks. It consistently outperformed standard Transformers and other baseline architectures. StackTrans models were scaled up to 7B parameters, and the StackTrans-360M model, trained from scratch, surpassed open-source LLMs with much larger parameter counts on relevant tasks.", "conclusion": "Incorporating stack-based memory into Transformers enables them to effectively capture hierarchical language structures, which standard Transformers struggle with. StackTrans improves reasoning abilities and efficiency, opening new possibilities for language model design."}}
{"id": "2507.15599", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15599", "abs": "https://arxiv.org/abs/2507.15599", "authors": ["Manatsawin Hanmongkolchai"], "title": "Applying the Chinese Wall Reverse Engineering Technique to Large Language Model Code Editing", "comment": null, "summary": "Large language models for code (Code LLM) are increasingly utilized in\nprogramming environments. Despite their utility, the training datasets for top\nLLM remain undisclosed, raising concerns about potential copyright violations.\nSome models, such as Pleias and Comma put emphasis on data curation and\nlicenses, however, with limited training data these models are not competitive\nand only serve as proof of concepts. To improve the utility of these models, we\npropose an application of the \"Chinese Wall\" technique, inspired by the reverse\nengineering technique of the same name -- a high quality model is used to\ngenerate detailed instructions for a weaker model. By doing so, a weaker but\nethically aligned model may be used to perform complicated tasks that,\notherwise, can only be completed by more powerful models. In our evaluation,\nwe've found that this technique improves Comma v0.1 1T's performance in\nCanItEdit benchmark by over 66%, and Starcoder2 Instruct by roughly 20%\ncompared to when running the same model on the benchmark alone. The practical\napplication of this technique today, however, may be limited due to the lack of\nmodels trained on public domain content without copyright restrictions.", "AI": {"tldr": "Using a strong code model to guide a weaker, ethical model with instructions boosts task performance, but is constrained by the lack of public domain data for training.", "motivation": "Large language models for code are powerful but raise copyright concerns due to undisclosed training datasets; ethically aligned models with curated data are less competitive, creating a need for utility-improving methods.", "method": "They propose the 'Chinese Wall' technique, where a high-quality model produces detailed instructions for a weaker, ethically aligned model, enabling the latter to perform complex programming tasks.", "result": "This technique improves Comma v0.1 1T's benchmark performance by over 66% and Starcoder2 Instruct by about 20% compared to when those models run alone.", "conclusion": "The 'Chinese Wall' method can significantly boost the utility of ethically aligned code LLMs, but its practical impact is currently limited by the scarcity of copyright-clear training data."}}
{"id": "2507.15624", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15624", "abs": "https://arxiv.org/abs/2507.15624", "authors": ["Yusuf Sulistyo Nugroho", "Ganno Tribuana Kurniaji", "Syful Islam", "Mohammed Humayun Kabir", "Vanesya Aura Ardity", "Md. Kamal Uddin"], "title": "Hot Topics and Common Challenges: an Empirical Study of React Discussions on Stack Overflow", "comment": "6 pages, 4 figures, 4 tables, conference paper", "summary": "React is a JavaScript library used to build user interfaces for single-page\napplications. Although recent studies have shown the popularity and advantages\nof React in web development, the specific challenges users face remain unknown.\nThus, this study aims to analyse the React-related questions shared on Stack\nOverflow. The study utilizes an exploratory data analysis to investigate the\nmost frequently discussed keywords, error classification, and user\nreputation-based errors, which is the novelty of this work. The results show\nthe top eight most frequently used keywords on React-related questions, namely,\ncode, link, vir, href, connect, azure, windows, and website. The error\nclassification of questions from the sample shows that algorithmic error is the\nmost frequent issue faced by all groups of users, where mid-reputation users\ncontribute the most, accounting for 55.77%. This suggests the need for the\ncommunity to provide guidance materials in solving algorithm-related problems.\nWe expect that the results of this study will provide valuable insight into\nfuture research to support the React community during the early stages of\nimplementation, facilitating their ability to effectively overcome challenges\nto adoption.", "AI": {"tldr": "This paper analyzes Stack Overflow posts to uncover the main problems React users face, finding that algorithmic errors are the most frequent, particularly among mid-reputation users. The authors suggest the community should offer more support around algorithmic issues to help new adopters.", "motivation": "While React is popular for building user interfaces in single-page applications, the specific user challenges associated with React are not well-documented. The paper aims to fill this gap by analyzing React-related questions on Stack Overflow.", "method": "The study uses exploratory data analysis to examine Stack Overflow questions tagged with React. It analyzes frequently mentioned keywords, classifies the types of errors discussed, and examines the distribution of these errors based on user reputation.", "result": "Algorithmic errors are the most commonly discussed issue across all user groups. Mid-reputation users contribute the majority of these error reports (55.77%). The analysis also identifies the eight most common keywords related to React questions: code, link, vir, href, connect, azure, windows, and website.", "conclusion": "Error classification reveals algorithmic errors as the most prevalent challenge for React users, with mid-reputation users particularly affected. The study suggests that more resources and guidance on algorithmic problem-solving would benefit the community. The findings provide insight for future research and resource allocation to better support React developers."}}
{"id": "2507.15663", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15663", "abs": "https://arxiv.org/abs/2507.15663", "authors": ["Giordano d'Aloisio", "Tosin Fadahunsi", "Jay Choy", "Rebecca Moussa", "Federica Sarro"], "title": "SustainDiffusion: Optimising the Social and Environmental Sustainability of Stable Diffusion Models", "comment": null, "summary": "Background: Text-to-image generation models are widely used across numerous\ndomains. Among these models, Stable Diffusion (SD) - an open-source\ntext-to-image generation model - has become the most popular, producing over 12\nbillion images annually. However, the widespread use of these models raises\nconcerns regarding their social and environmental sustainability.\n  Aims: To reduce the harm that SD models may have on society and the\nenvironment, we introduce SustainDiffusion, a search-based approach designed to\nenhance the social and environmental sustainability of SD models.\n  Method: SustainDiffusion searches the optimal combination of hyperparameters\nand prompt structures that can reduce gender and ethnic bias in generated\nimages while also lowering the energy consumption required for image\ngeneration. Importantly, SustainDiffusion maintains image quality comparable to\nthat of the original SD model.\n  Results: We conduct a comprehensive empirical evaluation of SustainDiffusion,\ntesting it against six different baselines using 56 different prompts. Our\nresults demonstrate that SustainDiffusion can reduce gender bias in SD3 by 68%,\nethnic bias by 59%, and energy consumption (calculated as the sum of CPU and\nGPU energy) by 48%. Additionally, the outcomes produced by SustainDiffusion are\nconsistent across multiple runs and can be generalised to various prompts.\n  Conclusions: With SustainDiffusion, we demonstrate how enhancing the social\nand environmental sustainability of text-to-image generation models is possible\nwithout fine-tuning or changing the model's architecture.", "AI": {"tldr": "SustainDiffusion makes text-to-image models less biased and more energy-efficient without changing the model's architecture, achieving this by optimizing prompts and hyperparameters.", "motivation": "Text-to-image generation models like Stable Diffusion are widely used, but there are growing concerns about their social (e.g., biases) and environmental (e.g., energy use) impacts. The motivation is to mitigate these impacts without sacrificing the quality of generated images.", "method": "The paper introduces SustainDiffusion, a search-based method that optimizes hyperparameters and prompt structures for Stable Diffusion. The aim is to reduce gender and ethnic bias in generated images and decrease energy consumption, all while preserving image quality. The method does not require fine-tuning or changing the model's architecture.", "result": "SustainDiffusion significantly reduces gender bias by 68%, ethnic bias by 59%, and energy consumption by 48% when compared to six baseline methods using 56 test prompts. The improvements hold across multiple runs and different prompts.", "conclusion": "It is possible to enhance the social and environmental sustainability of text-to-image models like Stable Diffusion through careful optimization, without modifying the model itself."}}
{"id": "2507.15666", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15666", "abs": "https://arxiv.org/abs/2507.15666", "authors": ["Igor Turkin", "Lina Volobuieva", "Andriy Chukhray", "Oleksandr Liubimov"], "title": "Modeling CubeSat Storage Battery Discharge: Equivalent Circuit Versus Machine Learning Approaches", "comment": "13 pages, 15 figures", "summary": "The subject of the article is the study and comparison of two approaches to\nmodelling the battery discharge of a CubeSat satellite: analytical using\nequivalent circuit and machine learning. The article aims to make a reasoned\nchoice of the approach to modelling the battery discharge of a CubeSat\nsatellite. Modelling the battery discharge of a satellite will enable the\nprediction of the consequences of disconnecting the autonomous power system and\nensure the fault tolerance of equipment in orbit. Therefore, the selected study\nis relevant and promising. This study focuses on the analysis of CubeSat\nsatellite data, based explicitly on orbital data samples of the power system,\nwhich include data available at the time of the article publication. The\ndataset contains data on the voltage, current, and temperature of the battery\nand solar panels attached to the five sides of the satellite. In this context,\ntwo approaches are considered: analytical modelling based on physical laws and\nmachine learning, which uses empirical data to create a predictive model.\nResults: A comparative analysis of the modeling results reveals that the\nequivalent circuit approach has the advantage of transparency, as it identifies\npossible parameters that facilitate understanding of the relationships.\nHowever, the model is less flexible to environmental changes or non-standard\nsatellite behavior. The machine learning model demonstrated more accurate\nresults, as it can account for complex dependencies and adapt to actual\nconditions, even when they deviate from theoretical assumptions.", "AI": {"tldr": "The paper compares analytical (equivalent circuit) and machine learning approaches for modeling CubeSat battery discharge. While the analytical model is transparent and interpretable, the machine learning model offers superior accuracy and adaptability, making it preferable for practical satellite operations.", "motivation": "The motivation for this study is to enhance the prediction of battery discharge in CubeSat satellites, which is crucial for ensuring the fault tolerance and operational reliability of satellite equipment in orbit. The research seeks to inform the choice between analytical and machine learning modeling approaches.", "method": "The study analyzes CubeSat orbital power system data, including battery and solar panel voltage, current, and temperature, using two modeling approaches: (1) analytical modeling with equivalent circuit models based on physical laws, and (2) machine learning models trained on empirical data to develop predictive capabilities.", "result": "The comparative analysis shows that the equivalent circuit (analytical) model is transparent and interpretable, allowing for better understanding of parameter relationships, but is less flexible to changing conditions. The machine learning model, on the other hand, provides higher accuracy and adapts better to complex or non-standard behaviors seen in real-world satellite data.", "conclusion": "Machine learning models are more suitable for accurate and adaptive prediction of CubeSat battery discharge compared to analytical equivalent circuit models, especially under complex or non-standard operational conditions."}}
{"id": "2507.15671", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15671", "abs": "https://arxiv.org/abs/2507.15671", "authors": ["Jinyao Guo", "Chengpeng Wang", "Dominic Deluca", "Jinjie Liu", "Zhuo Zhang", "Xiangyu Zhang"], "title": "BugScope: Learn to Find Bugs Like Human", "comment": "19 pages, 2 figure, 6 tables, 4 listings", "summary": "Detecting software bugs remains a fundamental challenge due to the extensive\ndiversity of real-world defects. Traditional static analysis tools often rely\non symbolic workflows, which restrict their coverage and hinder adaptability to\ncustomized bugs with diverse anti-patterns. While recent advances incorporate\nlarge language models (LLMs) to enhance bug detection, these methods continue\nto struggle with sophisticated bugs and typically operate within limited\nanalysis contexts. To address these challenges, we propose BugScope, an\nLLM-driven multi-agent system that emulates how human auditors learn new bug\npatterns from representative examples and apply that knowledge during code\nauditing. Given a set of examples illustrating both buggy and non-buggy\nbehaviors, BugScope synthesizes a retrieval strategy to extract relevant\ndetection contexts via program slicing and then constructs a tailored detection\nprompt to guide accurate reasoning by the LLM. Our evaluation on a curated\ndataset of 40 real-world bugs drawn from 21 widely-used open-source projects\ndemonstrates that BugScope achieves 87.04% precision and 90.00% recall,\nsurpassing state-of-the-art industrial tools by 0.44 in F1 score. Further\ntesting on large-scale open-source systems, including the Linux kernel,\nuncovered 141 previously unknown bugs, of which 78 have been fixed and 7\nconfirmed by developers, highlighting BugScope's substantial practical impact.", "AI": {"tldr": "BugScope is a novel LLM-based system that learns from examples to detect software bugs more effectively than traditional and current LLM-based tools, as demonstrated by strong results on both benchmark datasets and real-world open-source projects.", "motivation": "Software bug detection is challenging due to the variety and complexity of real-world defects. Traditional static analysis tools have limited coverage and adaptability, while LLM-based methods still struggle with complex bugs and limited analysis context.", "method": "The paper presents BugScope, a multi-agent system driven by large language models. BugScope learns new bug patterns from examples, uses program slicing to synthesize detection contexts, and constructs tailored prompts to guide LLMs for accurate reasoning in bug detection tasks.", "result": "BugScope was evaluated on a dataset of 40 real-world bugs from 21 open-source projects, achieving 87.04% precision and 90.00% recall, outperforming industrial tools by 0.44 F1 score. It also found 141 new bugs in large projects (e.g., Linux kernel); 78 were fixed and 7 confirmed by developers.", "conclusion": "BugScope significantly improves bug detection capabilities by leveraging example-based learning and context-sensitive LLM prompting, achieving superior performance and real-world impact compared to existing tools."}}
{"id": "2507.15822", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15822", "abs": "https://arxiv.org/abs/2507.15822", "authors": ["Li Huang", "Ilgiz Mustafin", "Marco Piccioni", "Alessandro Schena", "Reto Weber", "Bertrand Meyer"], "title": "Do AI models help produce verified bug fixes?", "comment": null, "summary": "Among areas of software engineering where AI techniques -- particularly,\nLarge Language Models -- seem poised to yield dramatic improvements, an\nattractive candidate is Automatic Program Repair (APR), the production of\nsatisfactory corrections to software bugs. Does this expectation materialize in\npractice? How do we find out, making sure that proposed corrections actually\nwork? If programmers have access to LLMs, how do they actually use them to\ncomplement their own skills?\n  To answer these questions, we took advantage of the availability of a\nprogram-proving environment, which formally determines the correctness of\nproposed fixes, to conduct a study of program debugging with two randomly\nassigned groups of programmers, one with access to LLMs and the other without,\nboth validating their answers through the proof tools. The methodology relied\non a division into general research questions (Goals in the Goal-Query-Metric\napproach), specific elements admitting specific answers (Queries), and\nmeasurements supporting these answers (Metrics). While applied so far to a\nlimited sample size, the results are a first step towards delineating a proper\nrole for AI and LLMs in providing guaranteed-correct fixes to program bugs.\n  These results caused surprise as compared to what one might expect from the\nuse of AI for debugging and APR. The contributions also include: a detailed\nmethodology for experiments in the use of LLMs for debugging, which other\nprojects can reuse; a fine-grain analysis of programmer behavior, made possible\nby the use of full-session recording; a definition of patterns of use of LLMs,\nwith 7 distinct categories; and validated advice for getting the best of LLMs\nfor debugging and Automatic Program Repair.", "AI": {"tldr": "A controlled study using formal verification shows that while LLMs offer distinct advantages in automatic program repair, they also have unexpected limitations. The paper presents a reusable experimental method, analyzes usage patterns, and gives validated recommendations for effective use of LLMs in debugging.", "motivation": "The motivation is to investigate whether Large Language Models (LLMs) can significantly improve Automatic Program Repair (APR), and to determine how programmers actually utilize LLMs when debugging, ensuring that proposed code corrections are truly effective.", "method": "The study used a program-proving environment that formally verifies the correctness of code fixes. Two groups of programmers were randomly assigned: one with access to LLMs for assistance, and one without. Both groups validated their debugging solutions using proof tools. The experiment followed the Goal-Query-Metric (GQM) methodology to structure research questions, specific queries, and measurable metrics. Programmer behaviors were analyzed through full-session recordings.", "result": "Results indicate surprising findings regarding the practical role and effectiveness of LLMs in debugging and APR, contrasting with commonly held expectations. The study also produced: a reusable experimental methodology, a detailed fine-grain analysis of programmer behavior, a taxonomy of seven distinct usage patterns of LLMs during debugging, and actionable advice for effective use of LLMs in debugging tasks.", "conclusion": "This work takes a first, rigorous step toward defining the practical and optimal role of AI/LLMs in APR, identifying both strengths and unexpected limitations, and provides methodological and actionable insights for the research and practitioner community."}}
{"id": "2507.15828", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15828", "abs": "https://arxiv.org/abs/2507.15828", "authors": ["Mauro Marcelino", "Marcos Alves", "Bianca Trinkenreich", "Bruno Cartaxo", "S\u00e9rgio Soares", "Simone D. J. Barbosa", "Marcos Kalinowski"], "title": "Investigating the Use of LLMs for Evidence Briefings Generation in Software Engineering", "comment": "ESEM 2025 Registered Report with an IPA (In Principle Acceptance) for\n  the Empirical Software Engineering journal", "summary": "[Context] An evidence briefing is a concise and objective transfer medium\nthat can present the main findings of a study to software engineers in the\nindustry. Although practitioners and researchers have deemed Evidence Briefings\nuseful, their production requires manual labor, which may be a significant\nchallenge to their broad adoption. [Goal] The goal of this registered report is\nto describe an experimental protocol for evaluating LLM-generated evidence\nbriefings for secondary studies in terms of content fidelity, ease of\nunderstanding, and usefulness, as perceived by researchers and practitioners,\ncompared to human-made briefings. [Method] We developed an RAG-based LLM tool\nto generate evidence briefings. We used the tool to automatically generate two\nevidence briefings that had been manually generated in previous research\nefforts. We designed a controlled experiment to evaluate how the LLM-generated\nbriefings compare to the human-made ones regarding perceived content fidelity,\nease of understanding, and usefulness. [Results] To be reported after the\nexperimental trials. [Conclusion] Depending on the experiment results.", "AI": {"tldr": "This paper proposes an experiment to test whether evidence briefings generated by a RAG-based LLM are as accurate, understandable, and useful as those made by humans. Results and conclusions will be shared after the experimentation phase.", "motivation": "Evidence briefings are helpful tools for communicating research findings to software engineers, but their manual creation poses a bottleneck for widespread use. To overcome this, automating the production process may increase adoption in the software engineering community.", "method": "The authors developed a Retrieval-Augmented Generation (RAG) based LLM tool to generate evidence briefings. They then used this tool to automatically generate versions of two evidence briefings previously created by humans. The study is a controlled experiment that evaluates LLM-generated versus human-made briefings on three metrics: content fidelity, ease of understanding, and perceived usefulness, as rated by researchers and practitioners.", "result": "Results have not yet been reported, as the paper describes a registered report protocol and the experiments are pending.", "conclusion": "Final conclusions will depend on the outcome of the experimental trials, which are not yet completed at this stage."}}
{"id": "2507.15831", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.15831", "abs": "https://arxiv.org/abs/2507.15831", "authors": ["Sergey Titov", "Konstantin Grotov", "Cristina Sarasua", "Yaroslav Golubev", "Dhivyabharathi Ramasamy", "Alberto Bacchelli", "Abraham Bernstein", "Timofey Bryksin"], "title": "Observing Fine-Grained Changes in Jupyter Notebooks During Development Time", "comment": "32 pages, 6 figures", "summary": "In software engineering, numerous studies have focused on the analysis of\nfine-grained logs, leading to significant innovations in areas such as\nrefactoring, security, and code completion. However, no similar studies have\nbeen conducted for computational notebooks in the context of data science.\n  To help bridge this research gap, we make three scientific contributions: we\n(1) introduce a toolset for collecting code changes in Jupyter notebooks during\ndevelopment time; (2) use it to collect more than 100 hours of work related to\na data analysis task and a machine learning task (carried out by 20 developers\nwith different levels of expertise), resulting in a dataset containing 2,655\ncells and 9,207 cell executions; and (3) use this dataset to investigate the\ndynamic nature of the notebook development process and the changes that take\nplace in the notebooks.\n  In our analysis of the collected data, we classified the changes made to the\ncells between executions and found that a significant number of these changes\nwere relatively small fixes and code iteration modifications. This suggests\nthat notebooks are used not only as a development and exploration tool but also\nas a debugging tool. We report a number of other insights and propose potential\nfuture research directions on the novel data.", "AI": {"tldr": "The paper introduces tools and datasets for analyzing how Jupyter notebooks are developed in data science, revealing that much notebook activity involves small fixes and iterative changes. This positions notebooks as key debugging environments, and the results pave the way for further studies in this area.", "motivation": "Although software engineering has benefited from fine-grained log analysis, similar approaches have not been applied to computational notebooks\u2014which are increasingly used in data science. There is a gap in understanding how these notebooks are developed and changed over time.", "method": "The authors introduced a toolset to collect granular logs of code changes in Jupyter notebooks. They collected data from more than 100 hours of notebook work, involving 20 developers performing data analysis and machine learning tasks. The resulting dataset included code changes and cell executions, which were systematically analyzed to classify the types of changes made during development.", "result": "Analysis revealed that many changes between notebook cell executions were minor corrections and code refinements. This indicates that computational notebooks serve not only exploratory and development purposes but are also actively used for debugging. Additional insights and suggestions for future research are also presented based on the data.", "conclusion": "This study provides the first fine-grained log analysis of computational notebooks in data science, offering a new dataset and toolset for understanding how developers work with these tools. It highlights the frequent, iterative nature of code changes and positions notebooks as valuable debugging tools, opening avenues for future research."}}
