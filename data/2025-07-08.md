<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 23]
- [cs.PL](#cs.PL) [Total: 5]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [The Impact of LLM-Assistants on Software Developer Productivity: A Systematic Literature Review](https://arxiv.org/abs/2507.03156)
*Amr Mohamed,Maram Assi,Mariam Guizani*

Main category: cs.SE

TL;DR: This systematic review analyzes 37 studies on the use of LLM-assistants in software development. While these tools can boost productivity and automate tasks, they also present risks such as decreased collaboration and inconsistent code quality. Most research focuses on a few productivity dimensions, and there is a need for more holistic and long-term studies.


<details>
  <summary>Details</summary>
Motivation: Despite increasing use of large language model assistants (LLM-assistants) in software development, there is no comprehensive understanding of how these tools impact developer productivity. This paper aims to fill that gap by synthesizing current research findings.

Method: A systematic literature review was conducted, covering 37 peer-reviewed studies published from January 2014 to December 2024. The studies were analyzed with respect to their findings on LLM-assistants across multiple dimensions of developer productivity.

Result: LLM-assistants are shown to provide substantial benefits, such as minimizing code search, accelerating development, and automating repetitive tasks. However, they also introduce risks, including cognitive offloading, reduced team collaboration, and varied effects on code quality. Most studies investigate productivity from multiple perspectives but rarely cover all relevant dimensions. There is a lack of longitudinal and team-based studies.

Conclusion: LLM-assistants can significantly influence software developer productivity, both positively and negatively. Research so far has focused on certain aspects like satisfaction, performance, and efficiency, while other areas remain understudied. The field would benefit from more comprehensive, team-oriented, and longitudinal evaluations. The study provides recommendations for advancing research and practice.

Abstract: Large language model assistants (LLM-assistants) present new opportunities to
transform software development. Developers are increasingly adopting these
tools across tasks, including coding, testing, debugging, documentation, and
design. Yet, despite growing interest, there is no synthesis of how
LLM-assistants affect software developer productivity. In this paper, we
present a systematic literature review of 37 peer-reviewed studies published
between January 2014 and December 2024 that examine this impact. Our analysis
reveals that LLM-assistants offer both considerable benefits and critical
risks. Commonly reported gains include minimized code search, accelerated
development, and the automation of trivial and repetitive tasks. However,
studies also highlight concerns around cognitive offloading, reduced team
collaboration, and inconsistent effects on code quality. While the majority of
studies (92%) adopt a multi-dimensional perspective by examining at least two
SPACE dimensions, reflecting increased awareness of the complexity of developer
productivity, only 14% extend beyond three dimensions, indicating substantial
room for more integrated evaluations. Satisfaction, Performance, and Efficiency
are the most frequently investigated dimensions, whereas Communication and
Activity remain underexplored. Most studies are exploratory (64%) and
methodologically diverse, but lack longitudinal and team-based evaluations.
This review surfaces key research gaps and provides recommendations for future
research and practice. All artifacts associated with this study are publicly
available at https://zenodo.org/records/15788502.

</details>


### [2] [Assessing Small Language Models for Code Generation: An Empirical Study with Benchmarks](https://arxiv.org/abs/2507.03160)
*Md Mahade Hasan,Muhammad Waseem,Kai-Kristian Kemell,Jussi Raskua,Juha Ala-Rantalaa,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: This paper finds that small language models can generate code effectively and efficiently, but surpassing their performance often requires much larger, more resource-intensive models. Performance variations across programming languages are minor, making SLMs practical for diverse real-world code generation tasks.


<details>
  <summary>Details</summary>
Motivation: Small Language Models (SLMs) offer lightweight and efficient alternatives to large models. There is limited empirical analysis on their abilities, especially regarding code generation, functional correctness, computational efficiency, and multilingual performance.

Method: The paper evaluates 20 open-source SLMs, spanning 0.4B to 10B parameters, across five code-related benchmarks. The assessment includes tests for code correctness, efficiency, and performance in multiple programming languages. Statistical analysis is used to interpret performance differences.

Result: Some small SLMs perform competitively in code generation while being efficient. Larger models deliver better accuracy but at much higher computational costs, with 10% improvements requiring almost 4x more VRAM. SLMs generally perform well on Python, Java, and PHP, but less so on Go, C++, and Ruby. However, statistical analysis shows these differences are not significant, supporting SLMs' broad applicability.

Conclusion: Compact SLMs can be effective for code generation in resource-limited settings, striking a balance between accuracy and efficiency. Significant accuracy gains demand much larger models and resources. SLMs show robustness across languages, assisting in the informed selection of models for practical tasks.

Abstract: The recent advancements of Small Language Models (SLMs) have opened new
possibilities for efficient code generation. SLMs offer lightweight and
cost-effective alternatives to Large Language Models (LLMs), making them
attractive for use in resource-constrained environments. However, empirical
understanding of SLMs, particularly their capabilities, limitations, and
performance trade-offs in code generation remains limited. This study presents
a comprehensive empirical evaluation of 20 open-source SLMs ranging from 0.4B
to 10B parameters on five diverse code-related benchmarks (HumanEval, MBPP,
Mercury, HumanEvalPack, and CodeXGLUE). The models are assessed along three
dimensions: i) functional correctness of generated code, ii) computational
efficiency and iii) performance across multiple programming languages. The
findings of this study reveal that several compact SLMs achieve competitive
results while maintaining a balance between performance and efficiency, making
them viable for deployment in resource-constrained environments. However,
achieving further improvements in accuracy requires switching to larger models.
These models generally outperform their smaller counterparts, but they require
much more computational power. We observe that for 10% performance
improvements, models can require nearly a 4x increase in VRAM consumption,
highlighting a trade-off between effectiveness and scalability. Besides, the
multilingual performance analysis reveals that SLMs tend to perform better in
languages such as Python, Java, and PHP, while exhibiting relatively weaker
performance in Go, C++, and Ruby. However, statistical analysis suggests these
differences are not significant, indicating a generalizability of SLMs across
programming languages. Based on the findings, this work provides insights into
the design and selection of SLMs for real-world code generation tasks.

</details>


### [3] [Specification-Guided Repair of Arithmetic Errors in Dafny Programs using LLMs](https://arxiv.org/abs/2507.03659)
*Valentina Wu,Alexandra Mendes,Alexandre Abreu*

Main category: cs.SE

TL;DR: The paper presents a tool that uses formal specs and LLMs to automatically repair bugs in Dafny programs, showing strong fault localization (89.6%) and solid repair success with GPT-4o mini (74.18%).


<details>
  <summary>Details</summary>
Motivation: Debugging and repairing software can be difficult when formal verification fails. Traditional automated program repair methods rely on test suites, which may not guarantee correctness. There is a need for an approach that leverages the stronger assurances provided by formal specifications.

Method: The authors introduce an APR tool for Dafny, a verification-aware language. The tool assumes specifications are correct and targets arithmetic faults, using Hoare Logic for fault localization and Large Language Models (LLMs) like GPT-4o mini, Llama 3, Mistral 7B, and Llemma 7B to synthesize candidate repairs. Validation and experiments are conducted on the DafnyBench suite.

Result: The tool achieves 89.6% accuracy in fault localization. For repairs, the best model (GPT-4o mini) achieves a 74.18% success rate.

Conclusion: Combining formal reasoning from specifications with LLM-based synthesis is effective for automated program repair, providing high fault localization accuracy and strong repair capabilities, particularly for arithmetic bugs in verified programs.

Abstract: Formal verification offers strong assurances of software correctness.
However, debugging and repairing the underlying faults can be complex and
time-consuming when verification fails. Automated Program Repair (APR) aims to
ease this by automatically identifying and fixing faults. Traditional APR
techniques often depend on test suites for validation, but these may fail to
capture all scenarios. In contrast, formal specifications provide stronger
correctness criteria for effective repairs.
  We present an innovative APR tool for Dafny, a verification-aware programming
language that uses formal specifications - including pre-conditions,
post-conditions, and invariants - as oracles for fault localization and repair.
Assuming the correctness of the specifications and focusing on arithmetic bugs,
we localize faults through a series of steps, which include using Hoare Logic
to determine the state of each statement within the program and
state-of-the-art Large Language Models (LLMs) to synthesize candidate fixes.
The chosen models were GPT-4o mini, Llama 3, Mistral 7B, and Llemma 7B.
  We evaluate our approach using DafnyBench, a benchmark of real-world Dafny
programs. Our tool achieves 89.6% accuracy in fault localization, with GPT-4o
mini yielding the highest repair success rate (74.18%). These results highlight
the potential of combining formal reasoning with LLM-driven program synthesis
for automated program repair.

</details>


### [4] [Analyzing C/C++ Library Migrations at the Package-level: Prevalence, Domains, Targets and Rationals across Seven Package Management Tools](https://arxiv.org/abs/2507.03263)
*Haiqiao Gu,Yiliang Zhao,Kai Gao,Minghui Zhou*

Main category: cs.SE

TL;DR: This paper presents the first comprehensive study and dataset of C/C++ library migrations, revealing trends, key migration domains, and unique migration motivations, and provides actionable insights for tool development and decision-making in the C/C++ ecosystem.


<details>
  <summary>Details</summary>
Motivation: There is insufficient understanding of library migrations in C/C++ due to fragmented and complex dependency management, unlike ecosystems like Python's, which have centralized management and are better studied.

Method: The study analyzed 19,943 C/C++ projects across different package management tools, establishing a new dataset of C/C++ library migrations. It investigated the prevalence, domains, migration targets, and rationale for library migrations, and compared them to Python, JavaScript, and Java.

Result: The overall trend and number of C/C++ library migrations is similar to Java. Migrations happen across different package management tools, mainly in GUI, build, and OS development, and rarely in domains like testing and logging. 83.46% of C/C++ source libraries have only one migration target. Four distinct C/C++-specific migration reasons were found, including reduced compile time and dependency management unification.

Conclusion: The findings provide insights for C/C++ developers about migration trends and targets, and demonstrate unique dependency management requirements. The established dataset can directly support migration target recommendations and guide the design of library migration tools.

Abstract: Library migration happens when a library can not meet the project's
requirements and is non-trivial to accomplish. To mitigate the problem,
substantial efforts have been devoted to understanding its characteristics and
recommending alternative libraries, especially for programming language (PL)
ecosystems with a central package hosting platform, such as Python (PyPI).
However, to the best of our knowledge, understanding of C/C++ library
migrations is still lacking, possibly due to challenges resulting from the
fragmented and complicated dependency management practices in the C/C++
ecosystem. To bridge this knowledge gap, this paper analyzes 19,943 C/C++
projects that utilize different package management tools and establishes the
first C/C++ library migration dataset. Based on the dataset, we investigate the
prevalence, domains, target library, and rationale of C/C++ library migrations
and compare the results with three widely investigated PLs: Python, JavaScript,
and Java. We find that the overall trend in the number of C/C++ library
migrations is similar to Java. Migrations across different package management
tools are also observed. In C/C++, library migrations mainly occur in GUI,
Build, and OS development, but are rare in domains (e.g., Testing and Logging)
that dominate library migrations in the three compared PLs. 83.46\% of C/C++
source libraries only have one migration target, suggesting that our library
migration dataset could be used directly to recommend migration targets. We
find four C/C++-specific migration reasons, such as less compile time and
unification of dependency management, revealing the unique dependency
management requirements in C/C++ projects. We believe our findings can help
C/C++ developers make more informed library migration decisions and shed light
on the design of C/C++ library migration tools.

</details>


### [5] [scikit-package -- software packaging standards and roadmap for sharing reproducible scientific software](https://arxiv.org/abs/2507.03328)
*S. Lee,C. Myers,A. Yang,T. Zhang,S. J. L. Billinge*

Main category: cs.SE

TL;DR: scikit-package is introduced to help scientists with limited programming background reuse and share their research code more easily, providing tutorials, tools, and workflows to improve reproducibility and software quality.


<details>
  <summary>Details</summary>
Motivation: Scientists often face difficulties in sharing and reproducing research because of challenges with code versioning, quality, and distribution, especially when the software is written by non-professional developers.

Method: The paper introduces 'scikit-package'—a project providing tutorials, automated workflows, and centralized resources to help scientists make their code more reusable and shareable. It combines pedagogical materials with practical tools.

Result: scikit-package offers community-maintained tools and a structured roadmap that enables scientists to improve code reusability and reproducibility at varying levels of complexity.

Conclusion: scikit-package empowers scientists without formal software engineering training to create better, more maintainable research software, ultimately improving result reproducibility and software sharing.

Abstract: Scientific advancement relies on the ability to share and reproduce results.
When data analysis or calculations are carried out using software written by
scientists there are special challenges around code versions, quality and code
sharing. scikit-package provides a roadmap to facilitate code reuse and sharing
with minimal effort through tutorials coupled with automated and centralized
reusable workflows. The goal of the project is to provide pedagogical and
practical tools for scientists who are not professionally trained software
engineers to write more reusable and maintainable software code. Code reuse can
occur at multiple levels of complexity-from turning a code block into a
function within a single script, to publishing a publicly installable, fully
tested, and documented software package scikit-package provides a community
maintained set of tools, and a roadmap, to help scientists bring their software
higher levels of reproducibility and shareability.

</details>


### [6] [Prompt Engineering Guidelines for Using Large Language Models in Requirements Engineering](https://arxiv.org/abs/2507.03405)
*Krishna Ronanki,Simon Arvidsson,Johan Axell*

Main category: cs.SE

TL;DR: Generative AI models are increasingly used in Requirements Engineering, but there is little guidance on prompt engineering for this domain. This study compiles existing guidelines and assesses their relevance through expert interviews, finding a gap and mapping guidelines to RE. It concludes by calling for more research and domain-specific guidance.


<details>
  <summary>Details</summary>
Motivation: There is a surge of interest in using generative AI, specifically Large Language Models (LLMs), in Requirements Engineering (RE). However, current literature lacks detailed guidance on how prompt engineering can be effectively leveraged for RE activities.

Method: The study conducts a systematic review of existing literature to compile guidelines for prompt engineering. It follows this by interviewing RE experts to discuss the extracted guidelines, assessing their strengths and weaknesses in RE-specific contexts.

Result: The literature review revealed a significant shortage of prompt engineering guidelines tailored for domain-specific activities such as RE. The study offers a mapping of available guidelines to RE, addressing this gap.

Conclusion: This paper identifies a need for more research and specialized guidelines on prompt engineering in Requirements Engineering. The mapping provided and insights from experts contribute to filling this gap, and the study highlights future research directions for developing more effective, RE-specific prompt engineering best practices.

Abstract: The rapid emergence of generative AI models like Large Language Models (LLMs)
has demonstrated its utility across various activities, including within
Requirements Engineering (RE). Ensuring the quality and accuracy of
LLM-generated output is critical, with prompt engineering serving as a key
technique to guide model responses. However, existing literature provides
limited guidance on how prompt engineering can be leveraged, specifically for
RE activities. The objective of this study is to explore the applicability of
existing prompt engineering guidelines for the effective usage of LLMs within
RE. To achieve this goal, we began by conducting a systematic review of primary
literature to compile a non-exhaustive list of prompt engineering guidelines.
Then, we conducted interviews with RE experts to present the extracted
guidelines and gain insights on the advantages and limitations of their
application within RE. Our literature review indicates a shortage of prompt
engineering guidelines for domain-specific activities, specifically for RE. Our
proposed mapping contributes to addressing this shortage. We conclude our study
by identifying an important future line of research within this field.

</details>


### [7] [Enhancing Uncertainty Quantification for Runtime Safety Assurance Using Causal Risk Analysis and Operational Design Domain](https://arxiv.org/abs/2507.03515)
*Radouane Bouchekir,Michell Guzman Cancimance*

Main category: cs.SE

TL;DR: The paper proposes a Bayesian Network-based method that unifies system and environmental uncertainty quantification for autonomous systems, boosting runtime safety by dynamically adapting to real-world operating conditions, as shown in an Automated Valet Parking use case.


<details>
  <summary>Details</summary>
Motivation: Autonomous systems need robust runtime safety, but deep learning components introduce uncertainty and are sensitive to environment changes. Traditional uncertainty quantification methods do not explicitly factor in varying environmental conditions.

Method: The paper enhances traditional uncertainty quantification by integrating risk-based causal analysis. Specifically, it employs Hazard Analysis and Risk Assessment (HARA) and fault tree modeling to identify crucial operational and environmental conditions, which are then combined with data and model uncertainties in a Bayesian Network. This network is instantiated at runtime with actual environmental data to dynamically estimate safety and uncertainty.

Result: The approach provides a probabilistic safety estimation that is dynamically updated based on real-time environmental observations, allowing for both expected performance and variance to be computed in a context-sensitive manner. The method is demonstrated in a case study on the Object Detection module of an Automated Valet Parking system.

Conclusion: Incorporating environmental context into uncertainty quantification for autonomous systems enables more reliable and adaptive runtime safety measures. The proposed Bayesian Network framework dynamically reflects both known system hazards and real-time environmental uncertainties.

Abstract: Ensuring the runtime safety of autonomous systems remains challenging due to
deep learning components' inherent uncertainty and their sensitivity to
environmental changes. In this paper, we propose an enhancement of traditional
uncertainty quantification by explicitly incorporating environmental conditions
using risk-based causal analysis. We leverage Hazard Analysis and Risk
Assessment (HARA) and fault tree modeling to identify critical operational
conditions affecting system functionality. These conditions, together with
uncertainties from the data and model, are integrated into a unified Bayesian
Network (BN). At runtime, this BN is instantiated using real-time environmental
observations to infer a probabilistic distribution over the safety estimation.
This distribution enables the computation of both expected performance and its
associated variance, providing a dynamic and context-aware measure of
uncertainty. We demonstrate our approach through a case study of the Object
Detection (OD) component in an Automated Valet Parking (AVP).

</details>


### [8] [The Role of Humour in Software Engineering -- A Literature Review and Preliminary Taxonomy](https://arxiv.org/abs/2507.03527)
*Dulaji Hidellaarachchi,John Grundy,Rashina Hoda*

Main category: cs.SE

TL;DR: This paper reviews and categorizes the use of humour in software engineering teams, presenting a framework to better understand its forms and impact. It finds that humour can boost productivity and well-being but requires careful and responsible use, plus further study in SE settings.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the recognition that humour can enhance creativity, group effectiveness, and employee well-being, yet its role and impact in software engineering (SE) teams remain understudied. The authors aim to fill this gap by providing a structured understanding of humour in SE contexts.

Method: The paper uses a comprehensive literature review, drawing from studies in psychology, sociology, and organizational behaviour, to build a taxonomy categorizing humour in SE teams. The taxonomy includes theories, styles, models, and scales related to humour.

Result: The outcome is a framework that categorizes humour for SE professionals and researchers, providing a structured approach to understanding and using humour in SE teams. The study also highlights both the potential benefits and the need for empirical validation of humour's impact in SE.

Conclusion: The study concludes that understanding and strategically applying humour can foster more cohesive, creative, and supportive SE environments. However, further research is needed to empirically validate these benefits in SE contexts.

Abstract: Humour has long been recognized as a key factor in enhancing creativity,
group effectiveness, and employee well-being across various domains. However,
its occurrence and impact within software engineering (SE) teams remains
under-explored. This paper introduces a comprehensive, literature review-based
taxonomy exploring the characterisation and use of humour in SE teams, with the
goal of boosting productivity, improving communication, and fostering a
positive work environment while emphasising the responsible use of humour to
mitigate its potential negative impacts. Drawing from a wide array of studies
in psychology, sociology, and organizational behaviour, our proposed framework
categorizes humour into distinct theories, styles, models, and scales, offering
SE professionals and researchers a structured approach to understanding humour
in their work. This study also addresses the unique challenges of applying
humour in SE, highlighting its potential benefits while acknowledging the need
for further empirical validation in this context. Ultimately, our study aims to
pave the way for more cohesive, creative, and psychologically supportive SE
environments through the strategic use of humour.

</details>


### [9] [ACE: Automated Technical Debt Remediation with Validated Large Language Model Refactorings](https://arxiv.org/abs/2507.03536)
*Adam Tornhill,Markus Borg,Nadim Hagatulah,Emma Söderberg*

Main category: cs.SE

TL;DR: ACE leverages LLMs to automate code improvements, focusing on code clarity and correctness. It provides reliable refactoring suggestions, helping developers address technical debt and making codebases easier to sustain as they grow.


<details>
  <summary>Details</summary>
Motivation: Software development is increasingly aided by AI and Large Language Models (LLMs), allowing machines to write code quickly. Despite this, the main bottleneck is not writing code, but understanding it, which occupies about 70% of developers’ time. Enhancing code understandability is thus highly valuable, especially as codebases expand and developer resources remain limited.

Method: The paper introduces Augmented Code Engineering (ACE), an automated tool that uses validated LLM output for code improvement. The tool employs a data-driven approach to generate reliable code refactoring suggestions, focusing on both objective improvements to code quality and preservation of program correctness.

Result: ACE provides automated, reliable code refactoring suggestions. Early user feedback indicates that AI-enabled refactoring assists in reducing technical debt at the code level, which oftentimes remains unresolved.

Conclusion: AI-assisted tools like ACE can significantly ease the burden of code comprehension by automating code improvement and refactoring, leading to more maintainable software and helping developers manage increasingly large and complex codebases.

Abstract: The remarkable advances in AI and Large Language Models (LLMs) have enabled
machines to write code, accelerating the growth of software systems. However,
the bottleneck in software development is not writing code but understanding
it; program understanding is the dominant activity, consuming approximately 70%
of developers' time. This implies that improving existing code to make it
easier to understand has a high payoff and - in the age of AI-assisted coding -
is an essential activity to ensure that a limited pool of developers can keep
up with ever-growing codebases. This paper introduces Augmented Code
Engineering (ACE), a tool that automates code improvements using validated LLM
output. Developed through a data-driven approach, ACE provides reliable
refactoring suggestions by considering both objective code quality improvements
and program correctness. Early feedback from users suggests that AI-enabled
refactoring helps mitigate code-level technical debt that otherwise rarely gets
acted upon.

</details>


### [10] [Is It Time To Treat Prompts As Code? A Multi-Use Case Study For Prompt Optimization Using DSPy](https://arxiv.org/abs/2507.03620)
*Francisca Lemos,Victor Alves,Filipa Ferraz*

Main category: cs.SE

TL;DR: Automated prompt optimization with DSPy can boost LLM performance in some tasks, particularly when combining instruction tuning and example selection, but results are inconsistent across different use cases. Task-specific evaluation remains crucial.


<details>
  <summary>Details</summary>
Motivation: Prompt engineering for Large Language Models (LLMs) is crucial but often inefficient, relying on manual trial and error. This paper is motivated by the need to automate and improve the effectiveness and efficiency of prompt optimization for LLMs.

Method: The study investigates DSPy, a declarative and programmatic prompt optimization framework, by applying it to five specific use cases: guardrail enforcement, hallucination detection in code, code generation, routing agents, and prompt evaluation. The paper evaluates how DSPy can automate prompt creation and refinement for these tasks.

Result: The results are mixed: some tasks, like guardrail enforcement and hallucination detection, saw only minor improvements, while tasks like prompt evaluation demonstrated significant performance increases (e.g., accuracy rising from 46.2% to 64.0%). Additionally, prompt refinement improved router agent task accuracy from 85.0% to 90.0%, but did not enable smaller models to catch up to larger ones simply by optimizing prompts.

Conclusion: DSPy's systematic, programmatic prompt optimization can enhance the performance of LLMs, especially when combining instruction tuning with example selection. However, the degree of improvement is highly task-dependent, stressing the need for targeted evaluation of prompt optimization techniques.

Abstract: Although prompt engineering is central to unlocking the full potential of
Large Language Models (LLMs), crafting effective prompts remains a
time-consuming trial-and-error process that relies on human intuition. This
study investigates Declarative Self-improving Python (DSPy), an optimization
framework that programmatically creates and refines prompts, applied to five
use cases: guardrail enforcement, hallucination detection in code, code
generation, routing agents, and prompt evaluation. Each use case explores how
prompt optimization via DSPy influences performance. While some cases
demonstrated modest improvements - such as minor gains in the guardrails use
case and selective enhancements in hallucination detection - others showed
notable benefits. The prompt evaluation criterion task demonstrated a
substantial performance increase, rising accuracy from 46.2% to 64.0%. In the
router agent case, the possibility of improving a poorly performing prompt and
of a smaller model matching a stronger one through optimized prompting was
explored. Although prompt refinement increased accuracy from 85.0% to 90.0%,
using the optimized prompt with a cheaper model did not improve performance.
Overall, this study's findings suggest that DSPy's systematic prompt
optimization can enhance LLM performance, particularly when instruction tuning
and example selection are optimized together. However, the impact varies by
task, highlighting the importance of evaluating specific use cases in prompt
optimization research.

</details>


### [11] [Efficient Detection of Intermittent Job Failures Using Few-Shot Learning](https://arxiv.org/abs/2507.04173)
*Henri Aïdasso,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: The paper presents a few-shot learning approach to detect intermittent job failures in CI pipelines, outperforming existing methods with higher F1-scores (70-88% vs 34-52%) using far fewer manual labels, thus emphasizing data quality over quantity for practical industrial adoption.


<details>
  <summary>Details</summary>
Motivation: Developers struggle to manage intermittent job failures in continuous integration (CI) pipelines, as these are often caused by non-deterministic issues (like flaky tests or infrastructure problems) rather than actual code bugs. Existing machine learning (ML) models rely on large annotated datasets, and the state-of-the-art (SOTA) heuristic for data labeling—based on job reruns—leads to a high rate of mislabeling when reruns are not policy, limiting accuracy and usefulness.

Method: The proposed method introduces a few-shot learning (FSL) approach. The researchers fine-tuned a small language model with a small number of manually labeled job logs (few-shot examples) to generate feature-rich embeddings. These embeddings are used to train a machine learning classifier for distinguishing intermittent from regular job failures.

Result: The FSL-based method achieved F1-scores between 70% and 88% using only 12 labeled examples across all studied projects, which is significantly higher than the SOTA (34-52% F1-score in 4 projects). Manual analysis suggested that on average, 32% of intermittent failures were mislabeled with the SOTA method.

Conclusion: The study demonstrates that their few-shot learning approach greatly improves detection of intermittent job failures in CI pipelines, outperforming current SOTA methods with significantly less manual labeling effort. Data quality is more critical than quantity, and their framework is more efficient and practical for industry use.

Abstract: One of the main challenges developers face in the use of continuous
integration (CI) and deployment pipelines is the occurrence of intermittent job
failures, which result from unexpected non-deterministic issues (e.g., flaky
tests or infrastructure problems) rather than regular code-related errors such
as bugs. Prior studies developed machine-learning (ML) models trained on large
datasets of job logs to classify job failures as either intermittent or
regular. As an alternative to costly manual labeling of large datasets, the
state-of-the-art (SOTA) approach leveraged a heuristic based on
non-deterministic job reruns. However, this method mislabels intermittent job
failures as regular in contexts where rerunning suspicious job failures is not
an explicit policy, and therefore limits the SOTA's performance in practice. In
fact, our manual analysis of 2,125 job failures from 5 industrial and 1
open-source projects reveals that, on average, 32\% of intermittent job
failures are mislabeled as regular. To address these limitations, this paper
introduces a novel approach to intermittent job failure detection using
few-shot learning (FSL). Specifically, we fine-tune a small language model
using a few number of manually labeled log examples to generate rich
embeddings, which are then used to train an ML classifier. Our FSL-based
approach achieves 70-88\% F1-score with only 12 shots in all projects,
outperforming the SOTA, which proved ineffective (34-52\% F1-score) in 4
projects. Overall, this study underlines the importance of data quality over
quantity and provides a more efficient and practical framework for the
detection of intermittent job failures in organizations.

</details>


### [12] [From Legal Text to Tech Specs: Generative AI's Interpretation of Consent in Privacy Law](https://arxiv.org/abs/2507.04185)
*Aniket Kesari,Travis Breaux,Tom Norton,Sarah Santos,Anmol Singhal*

Main category: cs.SE

TL;DR: This paper investigates using LLMs to bridge the gap between privacy law and software, finding they can automate compliance tasks but have reasoning limits, highlighting both their utility and current weaknesses for legal compliance in software engineering.


<details>
  <summary>Details</summary>
Motivation: There is a gap between privacy law requirements (such as consent for data processing) and their practical implementation in software, due to the complexity of translating legal mandates into code and the opacity of software development processes.

Method: The study uses a three-step pipeline leveraging Large Language Models (LLMs): (1) classifying software use cases for legal compliance, (2) generating LLM-driven modifications for non-compliance, and (3) manual validation of these modifications against legal standards.

Result: Preliminary findings show that LLMs have potential to automate some compliance tasks but also possess limitations in their reasoning, with benchmark results reflecting both their strengths and areas needing improvement.

Conclusion: LLMs can be valuable tools to automate and support legal compliance in software engineering, although their limitations must be considered. Benchmarking LLMs provides practical insights for improving AI-driven compliance solutions.

Abstract: Privacy law and regulation have turned to "consent" as the legitimate basis
for collecting and processing individuals' data. As governments have rushed to
enshrine consent requirements in their privacy laws, such as the California
Consumer Privacy Act (CCPA), significant challenges remain in understanding how
these legal mandates are operationalized in software. The opaque nature of
software development processes further complicates this translation. To address
this, we explore the use of Large Language Models (LLMs) in requirements
engineering to bridge the gap between legal requirements and technical
implementation. This study employs a three-step pipeline that involves using an
LLM to classify software use cases for compliance, generating LLM modifications
for non-compliant cases, and manually validating these changes against legal
standards. Our preliminary findings highlight the potential of LLMs in
automating compliance tasks, while also revealing limitations in their
reasoning capabilities. By benchmarking LLMs against real-world use cases, this
research provides insights into leveraging AI-driven solutions to enhance legal
compliance of software.

</details>


### [13] [Improving Deep Learning Framework Testing with Model-Level Metamorphic Testing](https://arxiv.org/abs/2507.04354)
*Yanzhou Mu,Juan Zhai,Chunrong Fang,Xiang Chen,Zhixiang Cao,Peiran Yang,Kexin Zhao,An Guo,Zhenyu Chen*

Main category: cs.SE

TL;DR: ModelMeta enhances bug detection in deep learning frameworks by using model-level metamorphic testing focused on structural diversity and multi-interface combinations, overcoming limitations of prior methods.


<details>
  <summary>Details</summary>
Motivation: Deep learning framework bugs can cause significant problems, but effective testing is difficult due to floating-point errors, randomness, and complex test inputs. Current methods struggle because there are no suitable test oracles.

Method: The paper proposes ModelMeta, a model-level metamorphic testing method for deep learning frameworks. It introduces four metamorphic relations (MRs) based on the structural characteristics of DL models. ModelMeta generates diverse test inputs via interface combinations, applies the QR-DQN strategy for guidance, and conducts detailed analysis of training loss, gradients, resource usage, and execution time to detect bugs.

Result: ModelMeta addresses the main limitations of prior metamorphic testing approaches by increasing test input diversity, generalizing across more interfaces, and detecting bugs related to multi-interface interactions and runtime metrics.

Conclusion: ModelMeta is an effective and general method to improve bug detection in DL frameworks, going beyond previous single-interface and limited-metric strategies by leveraging model-structure-based metamorphic testing and multi-metric analysis.

Abstract: Deep learning (DL) frameworks are essential to DL-based software systems, and
framework bugs may lead to substantial disasters, thus requiring effective
testing. Researchers adopt DL models or single interfaces as test inputs and
analyze their execution results to detect bugs. However, floating-point errors,
inherent randomness, and the complexity of test inputs make it challenging to
analyze execution results effectively, leading to existing methods suffering
from a lack of suitable test oracles. Some researchers utilize metamorphic
testing to tackle this challenge. They design Metamorphic Relations (MRs) based
on input data and parameter settings of a single framework interface to
generate equivalent test inputs, ensuring consistent execution results between
original and generated test inputs. Despite their promising effectiveness, they
still face certain limitations. (1) Existing MRs overlook structural
complexity, limiting test input diversity. (2) Existing MRs focus on limited
interfaces, which limits generalization and necessitates additional
adaptations. (3) Their detected bugs are related to the result consistency of
single interfaces and far from those exposed in multi-interface combinations
and runtime metrics (e.g., resource usage). To address these limitations, we
propose ModelMeta, a model-level metamorphic testing method for DL frameworks
with four MRs focused on the structure characteristics of DL models. ModelMeta
augments seed models with diverse interface combinations to generate test
inputs with consistent outputs, guided by the QR-DQN strategy. It then detects
bugs through fine-grained analysis of training loss/gradients, memory/GPU
usage, and execution time.

</details>


### [14] [DevMuT: Testing Deep Learning Framework via Developer Expertise-Based Mutation](https://arxiv.org/abs/2507.04360)
*Yanzhou Mu,Juan Zhai,Chunrong Fang,Xiang Chen,Zhixiang Cao,Peiran Yang,Yinglong Zou,Tao Zheng,Zhenyu Chen*

Main category: cs.SE

TL;DR: DevMuT is a novel DL framework testing method that uses developer-inspired mutations to find more relevant and diverse bugs in popular frameworks, outperforming existing tools and already being adopted in industry.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning framework testing approaches often detect only trivial defects that are frequently ignored by developers, failing to uncover high-impact and relevant bugs. There is a need for testing methods that expose issues significant to developers, reflecting real-world development and usage scenarios.

Method: The paper introduces DevMuT, a DL framework testing technique that combines mutation operators and constraints derived from developer expertise. By simulating typical developer operations during different stages of the DL model lifecycle (including training and inference), it generates more diverse and relevant test models.

Result: DevMuT was tested on three popular DL frameworks (PyTorch, JAX, MindSpore) across 29 models from nine industry application categories. It demonstrated an average improvement of at least 71.68% in model diversity and 28.20% in the legal rate of generated models compared to state-of-the-art methods. DevMuT detected 117 defects, with 63 confirmed and 24 already fixed; eight high-value bugs were acknowledged by developers. DevMuT has been integrated into the MindSpore community since December 2023.

Conclusion: DevMuT is an effective framework testing tool that identifies more diverse and developer-relevant defects than existing methods, contributing practically actionable value by exposing issues that matter most to DL framework developers and being adopted by industry communities.

Abstract: Deep learning (DL) frameworks are the fundamental infrastructure for various
DL applications. Framework defects can profoundly cause disastrous accidents,
thus requiring sufficient detection. In previous studies, researchers adopt DL
models as test inputs combined with mutation to generate more diverse models.
Though these studies demonstrate promising results, most detected defects are
considered trivial (i.e., either treated as edge cases or ignored by the
developers). To identify important bugs that matter to developers, we propose a
novel DL framework testing method DevMuT, which generates models by adopting
mutation operators and constraints derived from developer expertise. DevMuT
simulates developers'common operations in development and detects more diverse
defects within more stages of the DL model lifecycle (e.g., model training and
inference). We evaluate the performance of DevMuT on three widely used DL
frameworks (i.e., PyTorch, JAX, and Mind- Spore) with 29 DL models from nine
types of industry tasks. The experiment results show that DevMuT outperforms
state-of-the-art baselines: it can achieve at least 71.68% improvement on
average in the diversity of generated models and 28.20% improvement on average
in the legal rates of generated models. Moreover, DevMuT detects 117 defects,
63 of which are confirmed, 24 are fixed, and eight are of high value confirmed
by developers. Finally, DevMuT has been deployed in the MindSpore community
since December 2023. These demonstrate the effectiveness of DevMuT in detecting
defects that are close to the real scenes and are of concern to developers.

</details>


### [15] [Exploring React Library Related Questions on Stack Overflow: Answered vs. Unanswered](https://arxiv.org/abs/2507.04390)
*Vanesya Aura Ardity,Yusuf Sulistyo Nugroho,Syful Islam*

Main category: cs.SE

TL;DR: The paper analyzes over half a million React-related Stack Overflow questions, revealing that concise questions with code snippets and higher user reputation are more likely to be answered, while longer questions and images decrease answerability. High-reputation users tend to ask tougher questions. Users should tailor questions for better responses.


<details>
  <summary>Details</summary>
Motivation: React is widely used in modern web development, leading to many developers seeking help on Stack Overflow (SO). However, a significant portion of React-related questions remain unanswered, highlighting the need to understand the factors that affect question answerability and difficulty.

Method: The study uses Exploratory Data Analysis on 534,820 Stack Overflow questions filtered by 23 React-related tags. It applies text mining and statistical analysis. Logistic regression is used to find factors affecting answerability, and simple linear regression analyses the relationship between user reputation and question difficulty (PD Score).

Result: Attributes such as the number of views, code snippets, lines of code, and user reputation increase the probability of a question being answered. Conversely, more comments, longer questions, and the inclusion of images decrease the probability of receiving responses. There is a negative correlation between user reputation and PD Score, meaning higher-reputation users tend to ask more complex questions.

Conclusion: Certain aspects of how questions are asked on Stack Overflow significantly impact their likelihood of being answered. The study suggests users should consider these factors, like including code snippets and keeping questions concise, to improve their chances of receiving answers to React-related questions.

Abstract: React is a popular JavaScript framework in modern web application
development. Due to its high performance and efficiency, many developers use
this framework. Although React library offers many advantages, it is not
without its challenges. When using React library, developers often face
problems where they often seek solutions through question-and-answer forums,
such as Stack Overflow (SO). However, despite its high popularity, many
React-related questions on SO remain unanswered. Thus, this study aims to
analyze the factors associated with question answerability and difficulty
levels of React-related questions on SO. To facilitate our study, Exploratory
Data Analysis was applied to 534,820 questions, where they are filtered based
on 23 React-related tags. We implemented a quantitative approach through text
mining and statistical analysis. A logistic regression model was used to
identify attributes associated with question answerability, while a simple
linear regression model was employed to examine the correlation between user
reputations and performance difficulty scores (PD Score). The results show that
some attributes, such as number of views, code snippet inclusion, number of
lines of code, and user reputation, positively affect the likelihood of
question answerability. In contrast, the number of comments, question lengths,
and presence of images in React-related questions reduce the probability of a
question receiving responses from users. Further investigation indicates a
negative correlation between user reputations and PD Score, where reputation
increase corresponds to -0.092 reduction in PD score, signaling experienced
users tend to propose more complex technical inquiries. This study provides
insights into the characteristics of technical question-and-answer platforms,
such as SO, that users need to consider the answerability factors when posting
questions related to React.

</details>


### [16] [Learning Software Bug Reports: A Systematic Literature Review](https://arxiv.org/abs/2507.04422)
*Guoming Long,Jingzhi Gong,Hui Fang,Tao Chen*

Main category: cs.SE

TL;DR: This paper provides a comprehensive review of 204 papers on machine learning for bug report analysis, highlighting common methods, trends, and gaps, and suggesting future research directions.


<details>
  <summary>Details</summary>
Motivation: Artificial intelligence, particularly machine learning (ML), has greatly influenced software engineering research tasks such as bug report analysis. There has been notable progress in automating bug report understanding and classification, but no comprehensive review has yet analyzed the scope, methods, and trends in ML for bug report analysis. This paper addresses this research gap.

Method: The authors conducted a systematic literature review of 1,825 papers, narrowing down to 204 for detailed study. They extracted information regarding machine learning models used, feature representation methods, data preprocessing techniques, software systems evaluated, types of tasks (e.g., bug categorization), evaluation metrics, model evaluation techniques, and statistical testing. Key trends and gaps in the literature were synthesized.

Result: Key findings include: (1) CNN, LSTM, and $k$NN are extensively used for bug report analysis, while advanced models like BERT are underutilized due to complexity. (2) Word2Vec and TF-IDF are common for feature representation, with deep learning methods on the rise. (3) Stop word removal dominates preprocessing, but structural modeling is increasing post-2020. (4) Eclipse and Mozilla are the most commonly evaluated projects. (5) Bug categorization is the most frequent task, followed by localization and severity prediction. (6) Focus on non-functional and performance bugs is increasing. (7) Standard metrics and $k$-fold cross-validation are widely used for evaluation, but (8) many studies lack robust statistical analyses.

Conclusion: ML is widely used for bug report analysis with evolving trends towards deep learning and structural preprocessing. Several challenges remain, such as limited use of advanced language models and weak statistical validation. The paper identifies six future research directions to guide practitioners in the field.

Abstract: The recent advancement of artificial intelligence, especially machine
learning (ML), has significantly impacted software engineering research,
including bug report analysis. ML aims to automate the understanding,
extraction, and correlation of information from bug reports. Despite its
growing importance, there has been no comprehensive review in this area. In
this paper, we present a systematic literature review covering 1,825 papers,
selecting 204 for detailed analysis. We derive seven key findings: 1) Extensive
use of CNN, LSTM, and $k$NN for bug report analysis, with advanced models like
BERT underutilized due to their complexity. 2) Word2Vec and TF-IDF are popular
for feature representation, with a rise in deep learning approaches. 3) Stop
word removal is the most common preprocessing, with structural methods rising
after 2020. 4) Eclipse and Mozilla are the most frequently evaluated software
projects. 5) Bug categorization is the most common task, followed by bug
localization and severity prediction. 6) There is increasing attention on
specific bugs like non-functional and performance bugs. 7) Common evaluation
metrics are F1-score, Recall, Precision, and Accuracy, with $k$-fold
cross-validation preferred for model evaluation. 8) Many studies lack robust
statistical tests. We also identify six promising future research directions to
provide useful insights for practitioners.

</details>


### [17] [SPIRA: Building an Intelligent System for Respiratory Insufficiency Detection](https://arxiv.org/abs/2507.04548)
*Renato Cordeiro Ferreira,Dayanne Gomes,Vitor Tamae,Francisco Wernke,Alfredo Goldman*

Main category: cs.SE

TL;DR: The paper describes building and testing SPIRA, an AI system to detect respiratory insufficiency from voice, and shares valuable insights on challenges and best practices for similar future work.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable early and non-invasive detection of respiratory insufficiency using voice, potentially providing a simple and scalable screening tool for medical use.

Method: The authors implemented two versions of an intelligent system (SPIRA) designed to detect respiratory insufficiency from voice signals. They faced and documented multiple challenges related to data collection, model training, and inference, drawing on real-world experiences in both iterations.

Result: The project resulted in two working iterations of the SPIRA system and a compilation of practical lessons regarding the implementation, focusing on crucial elements like data acquisition, model improvement, and deployment challenges.

Conclusion: The paper concludes by summarizing key lessons learned during the development of SPIRA, focusing on data collection, training, and inference, which can benefit future projects aiming to detect health conditions from voice.

Abstract: Respiratory insufficiency is a medic symptom in which a person gets a reduced
amount of oxygen in the blood. This paper reports the experience of building
SPIRA: an intelligent system for detecting respiratory insufficiency from
voice. It compiles challenges faced in two succeeding implementations of the
same architecture, summarizing lessons learned on data collection, training,
and inference for future projects in similar systems.

</details>


### [18] [Testing, Evaluation, Verification and Validation (TEVV) of Digital Twins: A Comprehensive Framework](https://arxiv.org/abs/2507.04555)
*Gabriella Waters*

Main category: cs.SE

TL;DR: The paper introduces a new TEVV framework to ensure digital twins, used for modeling and simulating complex systems, remain accurate, reliable, and ethically implemented as their use expands.


<details>
  <summary>Details</summary>
Motivation: Digital twins are becoming increasingly sophisticated and critical for decision-making across many domains, making it crucial to ensure their accuracy, reliability, and ethical use.

Method: The paper proposes a comprehensive framework specifically for Testing, Evaluation, Verification, and Validation (TEVV) tailored to digital twins, considering the unique challenges these dynamic models pose.

Result: The result is a structured and comprehensive framework that addresses how to test, evaluate, verify, and validate digital twins to ensure they function accurately, reliably, and ethically.

Conclusion: By providing a dedicated TEVV framework, the paper aims to enhance the credibility and trustworthiness of digital twins as they are integrated into essential processes across various fields.

Abstract: Digital twins have emerged as a powerful technology for modeling and
simulating complex systems across various domains (Fuller et al., 2020; Tao et
al., 2019). As virtual representations of physical assets, processes, or
systems, digital twins enable real-time monitoring, predictive analysis, and
optimization. However, as digital twins become more sophisticated and integral
to decision-making processes, ensuring their accuracy, reliability, and ethical
implementation is essential. This paper presents a comprehensive framework for
the Testing, Evaluation, Verification and Validation (TEVV) of digital twins to
address the unique challenges posed by these dynamic and complex virtual
models.

</details>


### [19] [Supporting Software Formal Verification with Large Language Models: An Experimental Study](https://arxiv.org/abs/2507.04857)
*Weiqi Wang,Marie Farrell,Lucas C. Cordeiro,Liping Zhao*

Main category: cs.SE

TL;DR: SpecVerify uses AI language models and formal verification tools to automate checking of system requirements written in natural language. It matches state-of-the-art results with fewer mistakes, identifies issues missed by existing tools, and shows human review is still necessary for best results.


<details>
  <summary>Details</summary>
Motivation: Automatically deriving formal properties from natural language requirements is a longstanding challenge in requirements verification. Existing formal methods struggle to flexibly and accurately translate these requirements, motivating an approach that leverages the capabilities of large language models.

Method: SpecVerify is proposed as a framework integrating large language models (LLMs), specifically Claude 3.5 Sonnet, with the ESBMC formal verification tool. The pipeline automates the translation of natural language requirements into formal assertions, which are then verified. The evaluation involved nine cyber-physical system models from Lockheed Martin. Comparative studies include CoCoSim and other LLMs (Claude, ChatGPT, Llama).

Result: SpecVerify achieved 46.5% verification accuracy, on par with NASA's CoCoSim but with fewer false positives. The framework generated assertions extending beyond LTL capabilities and detected cases missed by traditional tools. Analysis highlighted CoCoSim's pitfalls with model connections and numerical approximations. The study also found that the success of LLM-based verification depends on high-quality documentation and human oversight, as models can misinterpret.

Conclusion: Integrating LLMs with formal verification tools can automate and improve requirements verification by reducing false positives and capturing more expressive properties. However, human monitoring remains essential because current LLMs are not foolproof. Human-machine collaboration is key to optimizing formal verification outcomes.

Abstract: Formal methods have been employed for requirements verification for a long
time. However, it is difficult to automatically derive properties from natural
language requirements. SpecVerify addresses this challenge by integrating large
language models (LLMs) with formal verification tools, providing a more
flexible mechanism for expressing requirements. This framework combines Claude
3.5 Sonnet with the ESBMC verifier to form an automated workflow. Evaluated on
nine cyber-physical systems from Lockheed Martin, SpecVerify achieves 46.5%
verification accuracy, comparable to NASA's CoCoSim, but with lower false
positives. Our framework formulates assertions that extend beyond the
expressive power of LTL and identifies falsifiable cases that are missed by
more traditional methods. Counterexample analysis reveals CoCoSim's limitations
stemming from model connection errors and numerical approximation issues. While
SpecVerify advances verification automation, our comparative study of Claude,
ChatGPT, and Llama shows that high-quality requirements documentation and human
monitoring remain critical, as models occasionally misinterpret specifications.
Our results demonstrate that LLMs can significantly reduce the barriers to
formal verification, while highlighting the continued importance of
human-machine collaboration in achieving optimal results.

</details>


### [20] [Towards a Unifying Reference Model for Digital Twins of Cyber-Physical Systems](https://arxiv.org/abs/2507.04871)
*Jerome Pfeiffer,Jingxi Zhang,Benoit Combemale,Judith Michael,Bernhard Rumpe,Manuel Wimmer,Andreas Wortmann*

Main category: cs.SE

TL;DR: The paper develops a unified and detailed reference model for digital twins by analyzing and combining various existing abstract models, helping bridge the gap between conceptual understanding and industrial application.


<details>
  <summary>Details</summary>
Motivation: Existing digital twin definitions and reference models are overly abstract, which hinders comprehensive understanding and practical implementation. This lack of clarity creates a significant gap between conceptualization and industrial deployment.

Method: The authors analyze popular reference models for digital twins and synthesize them into a more detailed, unified reference model. This new model aims to bridge the gap between abstract concepts and their real-world implementations.

Result: The study presents a significantly detailed and unifying reference model for digital twins that aids in better understanding and helps developers implement digital twins more effectively in practice.

Conclusion: The proposed unified reference model narrows the gap between digital twin concepts and practical implementation, providing better guidance for industrial engineering and fostering effective digital twin development.

Abstract: Digital twins are sophisticated software systems for the representation,
monitoring, and control of cyber-physical systems, including automotive,
avionics, smart manufacturing, and many more. Existing definitions and
reference models of digital twins are overly abstract, impeding their
comprehensive understanding and implementation guidance. Consequently, a
significant gap emerges between abstract concepts and their industrial
implementations. We analyze popular reference models for digital twins and
combine these into a significantly detailed unifying reference model for
digital twins that reduces the concept-implementation gap to facilitate their
engineering in industrial practice. This enhances the understanding of the
concepts of digital twins and their relationships and guides developers to
implement digital twins effectively.

</details>


### [21] [Understanding Everything as Code: A Taxonomy and Conceptual Model](https://arxiv.org/abs/2507.05100)
*Haoran Wei,Nazim Madhavji,John Steinbacher*

Main category: cs.SE

TL;DR: The paper systematically reviews and organizes all aspects of Everything as Code, resulting in a validated taxonomy and conceptual model that clarify the domain and provide actionable guidance for practitioners and researchers.


<details>
  <summary>Details</summary>
Motivation: Everything as Code (EaC) is gaining traction, but there is a notable lack of industry standards and scholarly research that clearly defines its scope and offers guidance for adoption. The study aims to fill this gap by analyzing existing knowledge, clarifying what EaC encompasses, and structuring actionable insights for both researchers and practitioners.

Method: A large-scale multivocal literature review (MLR) was conducted, synthesizing both academic and grey literature. The findings were analyzed using quantitative and thematic approaches. From this analysis, a taxonomy and conceptual model of EaC were created and validated with input from industry experts.

Result: The study produced a taxonomy containing 25 distinct EaC practices, organized into six layers based on their functional role and industry awareness. A conceptual model that shows the focus areas, overlaps, and interactions between these practices in the software delivery lifecycle was provided. Also, practical code examples were developed in collaboration with industry experts.

Conclusion: This work provides the first comprehensive taxonomy and conceptual model for Everything as Code, filling the academic gap and offering clear guidance for practitioners. It also establishes a foundation for further research in this emerging field.

Abstract: Background: Everything as Code (EaC) is an emerging paradigm aiming to codify
all aspects of modern software systems. Despite its growing popularity,
comprehensive industry standards and peer-reviewed research clarifying its
scope and guiding its adoption remain scarce. Aims: This study systematically
analyzes existing knowledge and perceptions of EaC, clarifies its scope and
boundaries, and provides structured guidance for researchers and practitioners.
Method: We conducted a large-scale multivocal literature review (MLR),
synthesizing academic and grey literature sources. Findings were analyzed
quantitatively and thematically. Based on this analysis, we developed a
taxonomy and conceptual model of EaC, validated through collaboration with
industry experts. Results: The resulting taxonomy comprises 25 distinct EaC
practices organized into six layers based on industry awareness and functional
roles. The conceptual model illustrates focus areas, overlaps, and interactions
among these EaC practices within the software delivery lifecycle. Additionally,
practical code examples demonstrating the implementation of these practices
were developed in collaboration with industry experts. Conclusions: This work
addresses the current scarcity of academic discourse on EaC by providing the
first comprehensive taxonomy and conceptual model. These contributions enhance
conceptual clarity, offer actionable guidance to practitioners, and lay the
groundwork for future research in this emerging domain.

</details>


### [22] [In-Context Learning as an Effective Estimator of Functional Correctness of LLM-Generated Code](https://arxiv.org/abs/2507.05200)
*Susmita Das,Madhusudan Ghosh,Priyanka Swami,Debasis Ganguly,Gul Calikli*

Main category: cs.SE

TL;DR: The paper proposes using a few-shot in-context learning approach to better estimate the functional correctness of LLM-generated code, showing it outperforms existing zero-shot and QPP techniques in the absence of test cases.


<details>
  <summary>Details</summary>
Motivation: In LLM-based code generation for development workflows, it is challenging to estimate functional correctness of generated code when test cases are unavailable, especially for workflows involving feature-driven or rapid application development.

Method: This paper proposes an in-context learning (ICL) approach for code quality estimation, supplying few-shot functionally correct code examples to enhance prediction.

Result: Providing few-shot, functionally correct code examples improves the performance of both existing QPP approaches and zero-shot-based code quality estimation methods.

Conclusion: Incorporating few-shot in-context examples of correct code enables more accurate estimation of functional correctness for ranked lists of generated code, supporting better selection in generative software development workflows.

Abstract: When applying LLM-based code generation to software development projects that
follow a feature-driven or rapid application development approach, it becomes
necessary to estimate the functional correctness of the generated code in the
absence of test cases. Just as a user selects a relevant document from a ranked
list of retrieved ones, a software generation workflow requires a developer to
choose (and potentially refine) a generated solution from a ranked list of
alternative solutions, ordered by their posterior likelihoods. This implies
that estimating the quality of a ranked list -- akin to estimating "relevance"
for query performance prediction (QPP) in IR -- is also crucial for generative
software development, where quality is defined in terms of "functional
correctness". In this paper, we propose an in-context learning (ICL) based
approach for code quality estimation. Our findings demonstrate that providing
few-shot examples of functionally correct code from a training set enhances the
performance of existing QPP approaches as well as a zero-shot-based approach
for code quality estimation.

</details>


### [23] [An Investigation into Maintenance Support for Neural Networks](https://arxiv.org/abs/2507.05245)
*Fatema Tuz Zohra,Brittany Johnson*

Main category: cs.SE

TL;DR: The paper identifies gaps in current neural network maintenance tools and practices, mainly that they focus more on development than on understanding and resolving problematic behaviors, suggesting a need for better support tools.


<details>
  <summary>Details</summary>
Motivation: Neural networks are increasingly integrated into daily life, raising the need for effective testing, debugging, and maintenance due to their potential negative impacts. Traditional software engineering methods have gaps when applied to neural networks, particularly in addressing undesirable behaviors. There is limited knowledge about how practitioners manage these challenges.

Method: The study uses qualitative research involving interviews and survey responses from practitioners to understand their experiences and challenges in maintaining neural networks.

Result: Findings show that while current tools are focused on building and training neural network models, they inadequately support practitioners in understanding and addressing the root causes of unexpected or undesirable model behaviors.

Conclusion: Traditional methodologies and existing tools fall short in supporting neural network maintenance. A developer-centric reassessment is needed, and new tools and methods must be developed to address current shortcomings and improve support for neural network maintenance.

Abstract: As the potential for neural networks to augment our daily lives grows,
ensuring their quality through effective testing, debugging, and maintenance is
essential. This is especially the case as we acknowledge the prospects of
negative impacts from these technologies. Traditional software engineering
methods, such as testing and debugging, have proven effective in maintaining
software quality; however, they reveal significant research and practice gaps
in maintaining neural networks. In particular, there is a limited understanding
of how practitioners currently address challenges related to understanding and
mitigating undesirable behaviors in neural networks. In our ongoing research,
we explore the current state of research and practice in maintaining neural
networks by curating insights from practitioners through a preliminary study
involving interviews and supporting survey responses. Our findings thus far
indicate that existing tools primarily concentrate on building and training
models. While these tools can be beneficial, they often fall short of
supporting practitioners' understanding and addressing the underlying causes of
unexpected model behavior. By evaluating current procedures and identifying the
limitations of traditional methodologies, our study aims to offer a
developer-centric perspective on where current practices fall short and
highlight opportunities for improving maintenance support in neural networks.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [24] [Towards Automatic Error Recovery in Parsing Expression](https://arxiv.org/abs/2507.03629)
*Sérgio Queiroz de Medeiros,Fabio Mascarenhas*

Main category: cs.PL

TL;DR: The paper presents an algorithm for automatic error recovery annotation in PEG parsers, which can be applied with little manual effort to produce robust parsers suitable for IDEs.


<details>
  <summary>Details</summary>
Motivation: Error recovery is crucial for parsers in Integrated Development Environments (IDEs) because they need to generate Abstract Syntax Trees (ASTs) even when handling programs with syntax errors, in order to support features like code completion and automated refactoring.

Method: The paper proposes an algorithm that automatically annotates Parsing Expressions Grammars (PEGs) with labels and constructs the corresponding recovery expressions. These labels and recovery expressions enable PEG-based parsers to recover from syntax errors, facilitating the construction of error-tolerant ASTs.

Result: The algorithm is evaluated on the parser for the Titan programming language. The results demonstrate that, with minimal manual intervention, the algorithm is effective at producing error-recovering PEG parsers, particularly when most grammar alternatives are disjoint.

Conclusion: The proposed algorithm enables automatic error recovery annotation for PEGs, making it feasible to implement robust error-recovering parsers for use in IDEs, with limited manual effort.

Abstract: Error recovery is an essential feature for a parser that should be plugged in
Integrated Development Environments (IDEs), which must build Abstract Syntax
Trees (ASTs) even for syntactically invalid programs in order to offer features
such as automated refactoring and code completion.
  Parsing Expressions Grammars (PEGs) are a formalism that naturally describes
recursive top-down parsers using a restricted form of backtracking. Labeled
failures are a conservative extension of PEGs that adds an error reporting
mechanism for PEG parsers, and these labels can also be associated with
recovery expressions to also be an error recovery mechanism. These expressions
can use the full expressivity of PEGs to recover from syntactic errors.
  Manually annotating a large grammar with labels and recovery expressions can
be difficult. In this work, we present an algorithm that automatically
annotates a PEG with labels, and builds their corresponding recovery
expressions. We evaluate this algorithm by adding error recovery to the parser
of the Titan programming language. The results shown that with a small amount
of manual intervention our algorithm can be used to produce error recovering
parsers for PEGs where most of the alternatives are disjoint.

</details>


### [25] [Semantically Separating Nominal Wyvern for Usability and Decidability](https://arxiv.org/abs/2507.03867)
*Yu Xiang Zhu,Amos Robinson,Sophia Roshal,Timothy Mou,Julian Mackay,Jonathan Aldrich,Alex Potanin*

Main category: cs.PL

TL;DR: Nominal Wyvern introduces a nominal/structural separation in type declarations, enabling subtype decidability in DOT-like systems without losing expressive features crucial for practical programming.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the problem that the Dependent Object Types (DOT) calculus, which aims to combine functional and object-oriented language features for greater expressiveness, suffers from undecidable subtyping due to the unrestricted use of recursive types and type bounds. Prior solutions have compromised expressiveness or usability.

Method: Nominal Wyvern introduces a semantic separation between nominal (named) recursive type declarations and structural type refinements, rather than relying solely on structural syntax. By adapting the material/shape separation technique, the system clearly distinguishes types used for subtyping constraints from those representing data, facilitating decidability.

Result: The semantic separation enables the design of a DOT-like dependent type system (Nominal Wyvern) that ensures subtyping remains decidable while preserving key expressiveness features like F-bounded polymorphism and practical module systems.

Conclusion: Nominal Wyvern achieves subtype decidability without sacrificing the expressiveness required for advanced type systems found in both object-oriented and functional languages, providing a more usable and practical foundation for future language designs.

Abstract: The Dependent Object Types (DOT) calculus incorporates concepts from
functional languages (e.g. modules) with traditional object-oriented features
(e.g. objects, subtyping) to achieve greater expressivity (e.g. F-bounded
polymorphism). However, this merger of paradigms comes at the cost of subtype
decidability. Recent work on bringing decidability to DOT has either sacrificed
expressiveness or ease of use. The unrestricted construction of recursive types
and type bounds has made subtype decidability a much harder problem than in
traditional object-oriented programming.
  Recognizing this, our paper introduces Nominal Wyvern, a DOT-like dependent
type system that takes an alternative approach: instead of having a uniform
structural syntax like DOT, Nominal Wyvern is designed around a "semantic
separation" between the nominal declaration of recursive types on the one hand,
and the structural refinement of those types when they are used on the other.
This design naturally guides the user to avoid writing undecidably recursive
structural types.
  From a technical standpoint, this separation also makes guaranteeing
decidability possible by allowing for an intuitive adaptation of material/shape
separation, a technique for achieving subtype decidability by separating types
responsible for subtyping constraints from types that represent concrete data.
The result is a type system with syntax and structure familiar to OOP users
that achieves decidability without compromising the expressiveness of F-bounded
polymorphism and module systems as they are used in practice.

</details>


### [26] [CCR 2.0: High-level Reasoning for Conditional Refinements](https://arxiv.org/abs/2507.04298)
*Youngju Song,Minki Cho*

Main category: cs.PL

TL;DR: The paper introduces CCR 2.0, an improved framework for formal verification of low-level systems, combining strengths of refinement and separation logic. It enhances proof reuse and abstraction through a new compositionality theorem and user-friendly proof techniques, with all results formalized in Coq.


<details>
  <summary>Details</summary>
Motivation: Formal verification of low-level systems often relies on either refinement or separation logic, each offering unique benefits, especially regarding compositionality. The motivation of the paper is to unify the advantages of these two verification approaches, addressing the limitations in compositional reasoning and proof reuse.

Method: The paper advances the Conditional Contextual Refinement (CCR) model from version 1.0 to version 2.0, proposing new and intuitive reasoning principles. The methodology includes improving the compositionality theorem to facilitate proof reuse and designing a proof technique that abstracts away separation logic details from users. Novel notions are developed to address non-trivial counterexamples. The work is formalized in Coq.

Result: CCR 2.0 offers a better compositionality theorem, allowing for increased proof reuse in verification tasks. It also introduces a proof technique that hides resource-model details from the user, making formalization more accessible. The results and reasoning principles are implemented and verified in Coq.

Conclusion: CCR 2.0 successfully advances the unified framework for compositional reasoning in system verification, overcoming challenges from previous versions and enhancing usability, proof reuse, and abstraction. The formalization in Coq verifies its soundness and practicality.

Abstract: In recent years, great progress has been made in the field of formal
verification for low-level systems. Many of them are based on one of two
popular approaches: refinement or separation logic. These two approaches are
very different in nature and offer complementary benefits in terms of
compositionality. Recently, to fuse these benefits in a unified mechanism, a
new approach called Conditional Contextual Refinement (CCR 1.0 for short) was
proposed. In this paper, we advance the model of CCR 1.0 and provide novel and
intuitive reasoning principles, resulting in: CCR 2.0. Specifically, CCR 2.0
(i) comes with a better compositionality theorem, having the practical benefit
of facilitating more proof reuse, and (ii) provides a proof technique that
hides model-level (i.e., resources of the separation logic) details from the
user. Achieving this goal was challenging due to non-trivial counterexamples
which necessitated us to devise novel notions. Our results are formalized in
Coq.

</details>


### [27] [Retargeting an Abstract Interpreter for a New Language by Partial Evaluation](https://arxiv.org/abs/2507.04316)
*Jay Lee*

Main category: cs.PL

TL;DR: This paper introduces a technique that uses partial evaluation to transform an existing abstract interpreter for one language into a static analyzer for another, automating and simplifying the analyzer development process for new languages.


<details>
  <summary>Details</summary>
Motivation: Developing sound static analyzers is labor-intensive. The motivation is to mechanize and reduce the effort in creating static analyzers for new languages by reusing existing work.

Method: The method involves leveraging partial evaluation to specialize an existing abstract interpreter for source languages, based on the semantics of target languages represented in the source language.

Result: The proposed approach successfully and automatically derives correct analyzers for new languages from an existing abstract interpreter.

Conclusion: The paper concludes that it is feasible to automatically retarget an existing abstract interpreter to new languages using their approach, without the need to manually develop analyzers from scratch.

Abstract: It is well-known that abstract interpreters can be systematically derived
from their concrete counterparts using a "recipe," but developing sound static
analyzers remains a time-consuming task. Reducing the effort required and
mechanizing the process of developing analyzers continues to be a significant
challenge. Is it possible to automatically retarget an existing abstract
interpreter for a new language?
  We propose a novel technique to automatically derive abstract interpreters
for various languages from an existing abstract interpreter. By leveraging
partial evaluation, we specialize an abstract interpreter for a source
language. The specialization is performed using the semantics of target
languages written in the source language. Our approach eliminates the need to
develop analyzers for new targets from scratch. We show that our method can
effectively retarget an abstract interpreter for one language into a correct
analyzer for another language.

</details>


### [28] [React-tRace: A Semantics for Understanding React Hooks](https://arxiv.org/abs/2507.05234)
*Jay Lee,Joongwon Ahn,Kwangkeun Yi*

Main category: cs.PL

TL;DR: This paper formalizes the semantics of React Hooks, validating its model both theoretically and empirically, and provides a visualization tool to help developers understand and use Hooks more effectively.


<details>
  <summary>Details</summary>
Motivation: React Hooks are widely used to manage side effects in modern web applications, but their semantics are often unclear to developers, resulting in bugs and misunderstandings.

Method: The authors formalize the semantics of React Hooks through a model called React-tRace and validate it theoretically and empirically. They also present a visualization tool grounded in this formalization to aid developer understanding.

Result: The model (React-tRace) accurately captures essential properties of React Hooks, as shown by comparisons with a test suite. The visualization tool built on this formalization helps developers comprehend Hooks' semantics.

Conclusion: Formalizing React Hooks' semantics makes their behavior clearer, addresses previous opacities, and provides practical tools to help developers avoid bugs and design issues. The work improves both theoretical understanding and practical development with Hooks.

Abstract: React has become the most widely used web front-end framework, enabling the
creation of user interfaces in a declarative and compositional manner. Hooks
are a set of APIs that manage side effects in functional components in React.
However, their semantics are often seen as opaque to developers, leading to UI
bugs. In this paper, we formalize the semantics of the essence of React Hooks
we name React-tRace, providing a framework that clarifies their behavior. We
demonstrate that our model captures the behavior of React, by theoretically
showing that it embodies essential properties of Hooks and empirically
comparing our React-tRace-definitional interpreter against a test suite.
Furthermore, we showcase a practical visualization tool based on the
formalization to demonstrate how developers can better understand the semantics
of Hooks.

</details>
