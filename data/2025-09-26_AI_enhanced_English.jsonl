{"id": "2509.20380", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.20380", "abs": "https://arxiv.org/abs/2509.20380", "authors": ["Samyak Jhaveri", "Vanessa Klotzmann", "Crista Lopes"], "title": "ACCeLLiuM: Supervised Fine-Tuning for Automated OpenACC Pragma Generation", "comment": null, "summary": "The increasing ubiquity of GPUs is accompanied by the increasing complexity\nof their hardware and parallel programming frameworks. Directive-based parallel\nprogramming standards like OpenACC simplify GPU programming to some extent by\nabstracting away low-level complexities, but a fair amount of expertise is\nstill required in order to use those directives effectively.\n  We introduce ACCeLLiuM, two open weights Large Language Models specifically\nfine-tuned for generating expert OpenACC directives for data-parallel loops,\nalong with the supervised fine-tuning dataset that was used to train them. The\nACCeLLiuM SFT dataset contains 4,033 OpenACC pragma-loop pairs mined from\npublic GitHub C/C++ repositories, with 3,223 pairs for training and 810 for\ntesting. Experimental evaluations show a pronounced performance gap in\ngenerating correct OpenACC pragmas between base LLMs and our fine-tuned\nversions. On the held-out test set, base LLMs fail to consistently generate\nvalid pragmas, whereas LLMs fine-tuned on the ACCeLLiuM dataset generate valid\npragmas with the correct directive type for $87\\%$ of the data-parallel loops,\nand exact pragmas--including directives, clauses, clause order, and clause\nvariables--for $50\\%$ of the cases. Even when not exact, generated pragmas\nfrequently incorporate the correct clauses in a different order than the\nground-truth label, or include additional clauses that enable finer control\nover parallel execution, data movement, and concurrency, offering practical\nvalue beyond strict string-matching. By publicly releasing the code, models,\nand dataset as ACCeLLiuM we hope to establish a reproducible benchmark for\nLLM-powered OpenACC pragma generation, and lower the barrier to automated GPU\noffloading of serially written programs.", "AI": {"tldr": "ACCeLLiuM provides open-source LLMs and a dataset to automate and improve OpenACC directive generation for GPUs, making code offloading easier, more accurate, and reproducible.", "motivation": "GPUs are widely used, but programming them remains complex even with directive-based standards like OpenACC, which still require significant expertise.", "method": "The authors introduce ACCeLLiuM, two large language models (LLMs) specifically fine-tuned for generating OpenACC directives for data-parallel loops, trained using a supervised dataset of pragma-loop pairs collected from public GitHub C/C++ repositories.", "result": "Experimental results show that LLMs fine-tuned on the ACCeLLiuM dataset generate valid OpenACC pragmas with the correct directive type for 87% of cases and exact matches for 50%, outperforming base LLMs. Generated pragmas often include correct or even more practical clauses than ground truth.", "conclusion": "ACCeLLiuM, along with its models, code, and dataset, is released to encourage reproducibility and to facilitate automated GPU offloading via more accessible OpenACC pragma generation."}}
{"id": "2509.20385", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20385", "abs": "https://arxiv.org/abs/2509.20385", "authors": ["Ishara Devendra", "Chaman Wijesiriwardana", "Prasad Wimalaratne"], "title": "State-of-the-Art in Software Security Visualization: A Systematic Review", "comment": null, "summary": "Software security visualization is an interdisciplinary field that combines\nthe technical complexity of cybersecurity, including threat intelligence and\ncompliance monitoring, with visual analytics, transforming complex security\ndata into easily digestible visual formats. As software systems get more\ncomplex and the threat landscape evolves, traditional text-based and numerical\nmethods for analyzing and interpreting security concerns become increasingly\nineffective. The purpose of this paper is to systematically review existing\nresearch and create a comprehensive taxonomy of software security visualization\ntechniques through literature, categorizing these techniques into four types:\ngraph-based, notation-based, matrix-based, and metaphor-based visualization.\nThis systematic review explores over 60 recent key research papers in software\nsecurity visualization, highlighting its key issues, recent advancements, and\nprospective future research directions. From the comprehensive analysis, the\ntwo main areas were distinctly highlighted as extensive software development\nvisualization, focusing on advanced methods for depicting software\narchitecture: operational security visualization and cybersecurity\nvisualization. The findings highlight the necessity for innovative\nvisualization techniques that adapt to the evolving security landscape, with\npractical implications for enhancing threat detection, improving security\nresponse strategies, and guiding future research.", "AI": {"tldr": "This paper systematically reviews and categorizes software security visualization techniques, underscoring the importance of innovative visual methods to address growing cybersecurity challenges.", "motivation": "As software and the cybersecurity landscape become more complex, traditional analysis methods are insufficient. There\u2019s a need for better visualization techniques to effectively interpret and respond to security threats.", "method": "Systematic literature review of over 60 recent key papers, creating a taxonomy of software security visualization techniques (categorized into graph-based, notation-based, matrix-based, and metaphor-based methods).", "result": "The paper categorizes existing visualization techniques, highlights two main focus areas (software development visualization and operational/cybersecurity visualization), and extracts current issues, recent advancements, and future research prospects.", "conclusion": "Innovative and adaptive visualization techniques are crucial for effective security analysis. Improving these methods has practical benefits for threat detection, response, and guiding future research in software security visualization."}}
{"id": "2509.20386", "categories": ["cs.SE", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.20386", "abs": "https://arxiv.org/abs/2509.20386", "authors": ["Nishant Gaurav", "Adit Akarsh", "Ankit Ranjan", "Manoj Bajaj"], "title": "Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments", "comment": null, "summary": "We present Dynamic ReAct, a novel approach for enabling ReAct agents to ef-\nficiently operate with extensive Model Control Protocol (MCP) tool sets that\nexceed the contextual memory limitations of large language models. Our approach\naddresses the fundamental challenge of tool selection in environments\ncontaining hundreds or thousands of available tools, where loading all tools\nsimultaneously is computationally infeasible. We propose and evaluate five\ndistinct architectures that progressively refine the tool selection process,\nculminating in a search-and-load mechanism that achieves intelligent tool\nselection with minimal computational overhead. Our experimental results\ndemonstrate that the proposed approach reduces tool loading by up to 50% while\nmaintaining task completion accuracy, advancing the path towards truly\ngeneral-purpose AI agents capable of dynamically adapting to diverse task\nenvironments.", "AI": {"tldr": "Dynamic ReAct lets ReAct agents handle thousands of tools without overloading memory, using smart selection methods to load only what's needed, halving required loading while still getting tasks done correctly.", "motivation": "The paper is motivated by the need for ReAct agents to work efficiently with large sets of tools (hundreds or thousands) provided via the Model Control Protocol (MCP), which exceed the memory capacity of current large language models. Loading all tools at once is computationally expensive and often impossible.", "method": "The authors propose and evaluate five distinct architectures for tool selection, culminating in a 'search-and-load' mechanism. This process allows ReAct agents to intelligently select and dynamically load just the necessary tools from large sets, reducing memory and computational requirements.", "result": "The approach results in a 50% reduction in tool loading compared to baseline methods, while maintaining task completion accuracy.", "conclusion": "Dynamic ReAct enables general-purpose AI agents to dynamically adapt to various tasks and environments by efficiently managing tool selection and loading, even with extensive tool sets."}}
{"id": "2509.20387", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20387", "abs": "https://arxiv.org/abs/2509.20387", "authors": ["Qusai Ramadan", "Jukka Ruohonen", "Abhishek Tiwari", "Adam Alami", "Zeyd Boukhers"], "title": "Towards Systematic Specification and Verification of Fairness Requirements: A Position Paper", "comment": "Accepted at the 2025 IEEE 33rd International Requirements Engineering\n  Conference Workshops", "summary": "Decisions suggested by improperly designed software systems might be prone to\ndiscriminate against people based on protected characteristics, such as gender\nand ethnicity. Previous studies attribute such undesired behavior to flaws in\nalgorithmic design or biased data. However, these studies ignore that\ndiscrimination is often the result of a lack of well-specified fairness\nrequirements and their verification. The fact that experts' knowledge about\nfairness is often implicit makes the task of specifying precise and verifiable\nfairness requirements difficult. In related domains, such as security\nengineering, knowledge graphs have been proven to be effective in formalizing\nknowledge to assist requirements specification and verification. To address the\nlack of formal mechanisms for specifying and verifying fairness requirements,\nwe propose the development of a knowledge graph-based framework for fairness.\nIn this paper, we discuss the challenges, research questions, and a road map\ntowards addressing the research questions.", "AI": {"tldr": "Current fairness problems in software are due to weak requirements specification. Paper proposes using knowledge graphs to formalize and verify fairness, anticipates challenges, and outlines future research directions.", "motivation": "Motivated by the observation that discrimination in software often stems not only from algorithmic or data flaws, but also from insufficiently specified and verified fairness requirements, exacerbated by experts' implicit knowledge.", "method": "The method proposed is the development and application of a knowledge graph-based framework to formalize fairness requirements and assist in their specification and verification.", "result": "The result is primarily a conceptual contribution: identification of challenges and research questions related to fairness in software, and a roadmap for using knowledge graphs to formalize and verify fairness requirements.", "conclusion": "The paper concludes that current approaches fail to sufficiently specify and verify fairness requirements in software systems, contributing to discrimination. It suggests a knowledge graph-based framework can help address this gap."}}
{"id": "2509.20426", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20426", "abs": "https://arxiv.org/abs/2509.20426", "authors": ["Mahmoud Samir Fayed"], "title": "Dual-Language General-Purpose Self-Hosted Visual Language and new Textual Programming Language for Applications", "comment": "PhD thesis", "summary": "Most visual programming languages (VPLs) are domain-specific, with few\ngeneral-purpose VPLs like Programming Without Coding Technology (PWCT). These\ngeneral-purpose VPLs are developed using textual programming languages and\nimproving them requires textual programming. In this thesis, we designed and\ndeveloped PWCT2, a dual-language (Arabic/English), general-purpose,\nself-hosting visual programming language. Before doing so, we specifically\ndesigned a textual programming language called Ring for its development. Ring\nis a dynamically typed language with a lightweight implementation, offering\nsyntax customization features. It permits the creation of domain-specific\nlanguages through new features that extend object-oriented programming,\nallowing for specialized languages resembling Cascading Style Sheets (CSS) or\nSupernova language. The Ring Compiler and Virtual Machine are designed using\nthe PWCT visual programming language where the visual implementation is\ncomposed of 18,945 components that generate 24,743 lines of C code, which\nincreases the abstraction level and hides unnecessary details. Using PWCT to\ndevelop Ring allowed us to realize several issues in PWCT, which led to the\ndevelopment of the PWCT2 visual programming language using the Ring textual\nprogramming language. PWCT2 provides approximately 36 times faster code\ngeneration and requires 20 times less storage for visual source files. It also\nallows for the conversion of Ring code into visual code, enabling the creation\nof a self-hosting VPL that can be developed using itself. PWCT2 consists of\napproximately 92,000 lines of Ring code and comes with 394 visual components.\nPWCT2 is distributed to many users through the Steam platform and has received\npositive feedback, On Steam, 1772 users have launched the software, and the\ntotal recorded usage time exceeds 17,000 hours, encouraging further research\nand development.", "AI": {"tldr": "This paper presents the creation of PWCT2, a dual-language, self-hosting visual programming language built with the new Ring language. PWCT2 is faster, more efficient, and more user-friendly than earlier VPLs, with successful real-world adoption.", "motivation": "Most visual programming languages (VPLs) are domain-specific and require textual programming for development and improvement. The motivation is to create a general-purpose VPL that is self-hosting, eliminating the need for separate textual programming and supporting both Arabic and English.", "method": "Developed a new textual programming language, Ring, which is dynamically typed and customizable. Used the existing PWCT VPL to visually implement the Ring Compiler and Virtual Machine. PWCT2, the new VPL, was developed using Ring and is capable of self-hosting, supporting code conversion between textual and visual formats.", "result": "PWCT2 generated code about 36 times faster and required 20 times less visual source file storage than its predecessor. PWCT2 consists of roughly 92,000 lines of Ring code and 394 visual components, with strong adoption on Steam: 1772 users and over 17,000 recorded hours of use.", "conclusion": "PWCT2 achieves significant improvements over previous VPLs in terms of performance, storage efficiency, and usability. It empowers users to develop and enhance the language visually, advancing general-purpose VPL design and adoption."}}
{"id": "2509.20415", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20415", "abs": "https://arxiv.org/abs/2509.20415", "authors": ["Yu Pan", "Xiaocheng Li", "Hanzhao Wang"], "title": "Online-Optimized RAG for Tool Use and Function Calling", "comment": null, "summary": "In many applications, retrieval-augmented generation (RAG) drives tool use\nand function calling by embedding the (user) queries and matching them to\npre-specified tool/function descriptions. In this paper, we address an\nembedding misalignment issue that often arises in practical applications due to\nimperfect embedding models or noisy descriptions; such misalignment may lead to\nincorrect retrieval and task failure. We introduce Online-Optimized RAG, a\ndeployment-time framework that continually adapts retrieval embeddings from\nlive interactions using minimal feedback (e.g., task success). Online-Optimized\nRAG applies lightweight online gradient updates with negligible per-query\nlatency and requires no changes to the underlying LLM. The method is\nplug-and-play: it supports both single- and multi-hop tool use, dynamic tool\ninventories, and $K$-retrieval with re-ranking. We provide a problem-dependent\ntheoretical analysis that quantifies how the method's performance depends on\nthe initialization quality of the embeddings and other related quantities.\nAcross diverse tool-use and document-retrieval scenarios, our Online-Optimized\nRAG consistently improves tool selection accuracy and end-task success, thus\nproviding a simple, practical path to robust, self-improving RAG systems.", "AI": {"tldr": "Online-Optimized RAG continually adapts retrieval embeddings using live feedback to fix mismatch problems, resulting in robust and improved performance in a range of real-world retrieval-augmented generation tasks.", "motivation": "The motivation is to address the embedding misalignment issue in RAG systems, resulting from imperfect embedding models or noisy tool/function descriptions, which can cause incorrect retrieval and task failures in practical applications.", "method": "The proposed method applies lightweight, online gradient updates to retrieval embeddings at deployment time, using minimal feedback (such as task success), without any modification to the underlying large language model. It is designed to be plug-and-play and supports dynamic scenarios.", "result": "Online-Optimized RAG consistently improves tool selection accuracy and the end-task success rate across a range of tool-use and document-retrieval settings.", "conclusion": "The paper concludes that Online-Optimized RAG offers a robust and self-improving means of retrieval-augmented generation by continually adapting retrieval embeddings during deployment, leading to improved tool selection accuracy and end-task success."}}
{"id": "2509.20534", "categories": ["cs.PL", "cs.SC", "68W30", "I.1.1; G.4"], "pdf": "https://arxiv.org/pdf/2509.20534", "abs": "https://arxiv.org/abs/2509.20534", "authors": ["Bowen Zhu", "Aayush Sabharwal", "Songchen Tan", "Yingbo Ma", "Alan Edelman", "Christopher Rackauckas"], "title": "Efficient Symbolic Computation vis Hash Consing", "comment": null, "summary": "Symbolic computation systems suffer from memory inefficiencies due to\nredundant storage of structurally identical subexpressions, commonly known as\nexpression swell, which degrades performance in both classical computer algebra\nand emerging AI-driven mathematical reasoning tools. In this paper, we present\nthe first integration of hash consing into JuliaSymbolics, a high-performance\nsymbolic toolkit in Julia, by employing a global weak-reference hash table that\ncanonicalizes expressions and eliminates duplication. This approach reduces\nmemory consumption and accelerates key operations such as differentiation,\nsimplification, and code generation, while seamlessly integrating with Julia's\nmetaprogramming and just-in-time compilation infrastructure. Benchmark\nevaluations across different computational domains reveal substantial\nimprovements: symbolic computations are accelerated by up to 3.2 times, memory\nusage is reduced by up to 2 times, code generation is up to 5 times faster,\nfunction compilation up to 10 times faster, and numerical evaluation up to 100\ntimes faster for larger models. While certain workloads with fewer duplicate\nunknown-variable expressions show more modest gains or even slight overhead in\ninitial computation stages, downstream processing consistently benefits\nsignificantly. These findings underscore the importance of hash consing in\nscaling symbolic computation and pave the way for future work integrating hash\nconsing with e-graphs for enhanced equivalence-aware expression sharing in\nAI-driven pipelines.", "AI": {"tldr": "This paper integrates hash consing (a duplication-avoiding technique) into JuliaSymbolics, significantly boosting performance and reducing memory usage for symbolic computation, especially for large-scale and complex expressions.", "motivation": "Symbolic computation systems face severe memory inefficiencies because they redundantly store structurally identical subexpressions (expression swell), impacting performance in both traditional and AI-based mathematical tools.", "method": "The paper integrates hash consing\u2014a technique that canonicalizes and shares identical expressions\u2014into JuliaSymbolics using a global weak-reference hash table to eliminate duplication while ensuring compatibility with Julia's metaprogramming and just-in-time compilation systems.", "result": "Benchmarks show that with hash consing, symbolic computations in JuliaSymbolics are up to 3.2 times faster, memory usage is halved, code generation is up to 5 times faster, function compilation up to 10 times faster, and numerical evaluation up to 100 times faster for larger models. The gains are less pronounced for workloads with fewer duplicate expressions but downstream steps always benefit.", "conclusion": "Integrating hash consing into JuliaSymbolics greatly reduces memory consumption and boosts performance across many symbolic tasks. This enables scaling symbolic computation and provides a foundation for future improvements such as e-graph synergy in AI-driven mathematical reasoning."}}
{"id": "2509.20421", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20421", "abs": "https://arxiv.org/abs/2509.20421", "authors": ["Reiner H\u00e4hnle", "Cosimo Laneve", "Adele Veschetti"], "title": "Formal Verification of Legal Contracts: A Translation-based Approach", "comment": null, "summary": "Stipula is a domain-specific programming language designed to model legal\ncontracts with enforceable properties, especially those involving asset\ntransfers and obligations. This paper presents a methodology to formally verify\nthe correctness of Stipula contracts through translation into Java code\nannotated with Java Modeling Language specifications. As a verification\nbackend, the deductive verification tool KeY is used. Both, the translation and\nthe verification of partial and total correctness for a large subset of Stipula\ncontracts, those with disjoint cycles, is fully automatic. Our work\ndemonstrates that a general-purpose deductive verification tool can be used\nsuccessfully in a translation approach.", "AI": {"tldr": "The paper proposes a fully automated approach to verify legal contract programs written in Stipula by translating them into Java code and using the KeY tool for formal verification.", "motivation": "Legal contracts, especially those involving asset transfers and obligations, require enforceable and correct implementation. The motivation is to provide formal, automated verification to ensure such contracts behave as intended, leveraging existing verification tools.", "method": "Stipula code is automatically translated into Java code with embedded JML specifications. The KeY deductive verification tool is then used to verify partial and total correctness of the resulting Java code. The approach focuses on contracts with disjoint cycles for full automation.", "result": "The paper introduces a methodology to formally verify programs written in Stipula, a domain-specific language for modeling legal contracts. The verification is achieved by translating Stipula contracts into Java annotated with Java Modeling Language (JML) specifications, and then using the KeY verification tool to check correctness. This process is fully automated for Stipula contracts with disjoint cycles.", "conclusion": "The research shows that a general-purpose deductive verification tool like KeY can be effectively leveraged for formally verifying the correctness of Stipula contracts via a translation to Java."}}
{"id": "2509.20491", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20491", "abs": "https://arxiv.org/abs/2509.20491", "authors": ["Brahim Mahmoudi", "Naouel Moha", "Quentin Stievenert", "Florent Avellaneda"], "title": "AI-Specific Code Smells: From Specification to Detection", "comment": null, "summary": "The rise of Artificial Intelligence (AI) is reshaping how software systems\nare developed and maintained. However, AI-based systems give rise to new\nsoftware issues that existing detection tools often miss. Among these, we focus\non AI-specific code smells, recurring patterns in the code that may indicate\ndeeper problems such as unreproducibility, silent failures, or poor model\ngeneralization. We introduce SpecDetect4AI, a tool-based approach for the\nspecification and detection of these code smells at scale. This approach\ncombines a high-level declarative Domain-Specific Language (DSL) for rule\nspecification with an extensible static analysis tool that interprets and\ndetects these rules for AI-based systems. We specified 22 AI-specific code\nsmells and evaluated SpecDetect4AI on 826 AI-based systems (20M lines of code),\nachieving a precision of 88.66% and a recall of 88.89%, outperforming other\nexisting detection tools. Our results show that SpecDetect4AI supports the\nspecification and detection of AI-specific code smells through dedicated rules\nand can effectively analyze large AI-based systems, demonstrating both\nefficiency and extensibility (SUS 81.7/100).", "AI": {"tldr": "The paper proposes SpecDetect4AI, a new tool for detecting AI-specific code smells using rule-based specification and static analysis, showing high accuracy and outperforming previous tools on large-scale AI software.", "motivation": "Traditional code smell detection tools miss issues specific to AI-based systems, such as problems affecting reproducibility and model generalization.", "method": "The authors introduced SpecDetect4AI, using a declarative domain-specific language for specifying code smell rules and an extensible static analysis tool for detection.", "result": "SpecDetect4AI successfully specified 22 AI-specific code smells and achieved high precision (88.66%) and recall (88.89%) in detecting them on 826 systems, outperforming existing tools.", "conclusion": "SpecDetect4AI efficiently and extensibly detects AI-specific code smells at scale, aiding in the analysis and maintenance of large AI-based systems."}}
{"id": "2509.20518", "categories": ["cs.SE", "cs.PL", "D.2.3"], "pdf": "https://arxiv.org/pdf/2509.20518", "abs": "https://arxiv.org/abs/2509.20518", "authors": ["Sayed Mahbub Hasan Amiri", "Md Mainul Islam"], "title": "Enhancing Python Programming Education with an AI-Powered Code Helper: Design, Implementation, and Impact", "comment": "20 pages, 16 figures", "summary": "This is the study that presents an AI-Python-based chatbot that helps\nstudents to learn programming by demonstrating solutions to such problems as\ndebugging errors, solving syntax problems or converting abstract theoretical\nconcepts to practical implementations. Traditional coding tools like Integrated\nDevelopment Environments (IDEs) and static analyzers do not give robotic help\nwhile AI-driven code assistants such as GitHub Copilot focus on getting things\ndone. To close this gap, our chatbot combines static code analysis, dynamic\nexecution tracing, and large language models (LLMs) to provide the students\nwith relevant and practical advice, hence promoting the learning process. The\nchatbots hybrid architecture employs CodeLlama for code embedding, GPT-4 for\nnatural language interactions, and Docker-based sandboxing for secure\nexecution. Evaluated through a mixed-methods approach involving 1,500 student\nsubmissions, the system demonstrated an 85% error resolution success rate,\noutperforming standalone tools like pylint (62%) and GPT-4 (73%). Quantitative\nresults revealed a 59.3% reduction in debugging time among users, with pre- and\npost-test assessments showing a 34% improvement in coding proficiency,\nparticularly in recursion and exception handling. Qualitative feedback from 120\nstudents highlighted the chatbots clarity, accessibility, and\nconfidence-building impact, though critiques included occasional latency and\nrestrictive code sanitization. By balancing technical innovation with\npedagogical empathy, this research provides a blueprint for AI tools that\nprioritize educational equity and long-term skill retention over mere code\ncompletion. The chatbot exemplifies how AI can augment human instruction,\nfostering deeper conceptual understanding in programming education.", "AI": {"tldr": "A novel AI-chatbot significantly improves student learning and debugging in programming, outperforming traditional tools by combining code analysis, execution tracing, and LLMs. It enhances proficiency and engagement, illustrating how AI can foster deeper understanding, not just code completion.", "motivation": "Traditional programming tools, like IDEs and static code analyzers, do not provide interactive or adaptive support for students learning to code. AI-driven code assistants focus on task completion, rather than promoting learning or conceptual understanding. There is a gap in providing pedagogically effective, AI-driven support tailored to fostering students' programming skills.", "method": "A hybrid architecture chatbot was developed, integrating CodeLlama for code embedding, GPT-4 for natural language processing, and Docker-based sandboxing for secure execution. The system was evaluated using a mixed-methods approach: 1,500 student submissions for quantitative performance tests, pre- and post-test coding proficiency assessments, and qualitative student feedback from 120 participants.", "result": "The chatbot achieved an 85% error resolution success rate, outperforming standalone tools like pylint (62%) and GPT-4 (73%) individually. Users experienced a 59.3% reduction in debugging time. Pre/post-assessment showed a 34% improvement in coding proficiency, notably in recursion and exception handling skills. Qualitative feedback highlighted increased clarity, accessibility, and confidence, with some critiques regarding latency and code sanitization restrictions.", "conclusion": "The AI-Python-based chatbot effectively bridges the gap between code completion and pedagogy, promoting deeper learning and skill retention. Its technical innovation and educational focus offer a blueprint for future AI tools in programming education, emphasizing equity and sustainable skill development over simple task automation."}}
{"id": "2509.20497", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20497", "abs": "https://arxiv.org/abs/2509.20497", "authors": ["Ahmed Aljohani", "Hyunsook Do"], "title": "PromptDebt: A Comprehensive Study of Technical Debt Across LLM Projects", "comment": "Accepted at Proceedings of the 2025 Evaluation and Assessment in\n  Software Engineering (EASE '25)", "summary": "Large Language Models (LLMs) are increasingly embedded in software via APIs\nlike OpenAI, offering powerful AI features without heavy infrastructure. Yet\nthese integrations bring their own form of self-admitted technical debt (SATD).\nIn this paper, we present the first large-scale empirical study of LLM-specific\nSATD: its origins, prevalence, and mitigation strategies. By analyzing 93,142\nPython files across major LLM APIs, we found that 54.49% of SATD instances stem\nfrom OpenAI integrations and 12.35% from LangChain use. Prompt design emerged\nas the primary source of LLM-specific SATD, with 6.61% of debt related to\nprompt configuration and optimization issues, followed by hyperparameter tuning\nand LLM-framework integration. We further explored which prompt techniques\nattract the most debt, revealing that instruction-based prompts (38.60%) and\nfew-shot prompts (18.13%) are particularly vulnerable due to their dependence\non instruction clarity and example quality. Finally, we release a comprehensive\nSATD dataset to support reproducibility and offer practical guidance for\nmanaging technical debt in LLM-powered systems.", "AI": {"tldr": "This paper analyzes technical debt in Python projects using LLM APIs, finding prompt design\u2014especially instruction-based and few-shot prompts\u2014is the main source. OpenAI integrations account for most SATD. The authors release a dataset and offer guidance for reducing LLM-related debt.", "motivation": "With the rise of LLM integrations in software via APIs, developers face unique forms of technical debt, especially related to prompt design and LLM configuration. Understanding their prevalence and causes is crucial to improving software quality and maintainability.", "method": "The authors conducted a large-scale empirical study by analyzing 93,142 Python files involving major LLM APIs. They quantified SATD origins, prevalence, and mitigation, focusing on prompt techniques and configuration issues.", "result": "54.49% of SATD stems from OpenAI API usage, and 12.35% from LangChain. Prompt design is the top SATD contributor, with notable issues in instruction-based (38.60%) and few-shot (18.13%) prompt types. The study presents a SATD dataset and guidelines for managing LLM-specific debt.", "conclusion": "A significant proportion of self-admitted technical debt (SATD) in Python projects using LLM APIs originates from prompt design and configuration, especially with OpenAI integrations. Instruction-based and few-shot prompts are the most susceptible to SATD. The paper also provides a public dataset and practical guidance to help developers address SATD in LLM-powered systems."}}
{"id": "2509.20552", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20552", "abs": "https://arxiv.org/abs/2509.20552", "authors": ["Xinyu Shi", "Zhenhao Li", "An Ran Chen"], "title": "Enhancing LLM-based Fault Localization with a Functionality-Aware Retrieval-Augmented Generation Framework", "comment": null, "summary": "Fault localization (FL) is a critical but time-consuming task in software\ndebugging, aiming to identify faulty code elements. While recent advances in\nlarge language models (LLMs) have shown promise for FL, they often struggle\nwith complex systems due to the lack of project-specific knowledge and the\ndifficulty of navigating large projects. To address these limitations, we\npropose FaR-Loc, a novel framework that enhances method-level FL by integrating\nLLMs with retrieval-augmented generation (RAG). FaR-Loc consists of three key\ncomponents: LLM Functionality Extraction, Semantic Dense Retrieval, and LLM\nRe-ranking. First, given a failed test and its associated stack trace, the LLM\nFunctionality Extraction module generates a concise natural language\ndescription that captures the failing behavior. Next, the Semantic Dense\nRetrieval component leverages a pre-trained code-understanding encoder to embed\nboth the functionality description (natural language) and the covered methods\n(code) into a shared semantic space, enabling the retrieval of methods with\nsimilar functional behavior. Finally, the LLM Re-ranking module reorders the\nretrieved methods based on their contextual relevance. Our experiments on the\nwidely used Defects4J benchmark show that FaR-Loc outperforms state-of-the-art\nLLM-based baselines SoapFL and AutoFL, by 14.6% and 9.1% in Top-1 accuracy, by\n19.2% and 22.1% in Top-5 accuracy, respectively. It also surpasses all\nlearning-based and spectrum-based baselines across all Top-N metrics without\nrequiring re-training. Furthermore, we find that pre-trained code embedding\nmodels that incorporate code structure, such as UniXcoder, can significantly\nimprove fault localization performance by up to 49.0% in Top-1 accuracy.\nFinally, we conduct a case study to illustrate the effectiveness of FaR-Loc and\nto provide insights for its practical application.", "AI": {"tldr": "FaR-Loc is a new LLM+retrieval framework for method-level fault localization in software. It beats existing approaches in accuracy and doesn't need retraining, especially benefiting from code structure-aware models like UniXcoder.", "motivation": "Fault localization is essential but challenging in software debugging, especially for complex projects where large language models (LLMs) lack project-specific context and struggle with code navigation.", "method": "The paper introduces FaR-Loc, a framework integrating LLMs with retrieval-augmented generation. It has three steps: (1) LLM Functionality Extraction to produce a natural language description of failing behavior; (2) Semantic Dense Retrieval to embed and match the description and code methods using a code-understanding encoder; (3) LLM Re-ranking to reorder retrieved candidates based on context relevance.", "result": "FaR-Loc surpasses current LLM-based baselines (SoapFL and AutoFL) with improvements of up to 14.6% (Top-1) and 22.1% (Top-5) accuracy. It also outperforms learning-based and spectrum-based methods in all Top-N metrics without retraining. Using structured code embedding models like UniXcoder further boosts performance up to 49% in Top-1 accuracy.", "conclusion": "The study demonstrates that combining LLMs with semantic retrieval significantly advances method-level fault localization. The approach is effective, efficient, and practical\u2014especially with strong code embedding models\u2014providing a valuable tool for software debugging."}}
{"id": "2509.20631", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20631", "abs": "https://arxiv.org/abs/2509.20631", "authors": ["Michael Zhang", "Yuan Tian", "Mariam Guizani"], "title": "Design, Implementation and Evaluation of a Novel Programming Language Topic Classification Workflow", "comment": null, "summary": "As software systems grow in scale and complexity, understanding the\ndistribution of programming language topics within source code becomes\nincreasingly important for guiding technical decisions, improving onboarding,\nand informing tooling and education. This paper presents the design,\nimplementation, and evaluation of a novel programming language topic\nclassification workflow. Our approach combines a multi-label Support Vector\nMachine (SVM) with a sliding window and voting strategy to enable fine-grained\nlocalization of core language concepts such as operator overloading, virtual\nfunctions, inheritance, and templates. Trained on the IBM Project CodeNet\ndataset, our model achieves an average F1 score of 0.90 across topics and 0.75\nin code-topic highlight. Our findings contribute empirical insights and a\nreusable pipeline for researchers and practitioners interested in code analysis\nand data-driven software engineering.", "AI": {"tldr": "The paper introduces a high-performing SVM-based workflow for classifying and localizing programming language topics in source code, with strong results on a large dataset, offering valuable tools and insights for code analysis and software engineering.", "motivation": "The paper is motivated by the increasing scale and complexity of software systems, which necessitates a deeper understanding of how various programming language topics are distributed within source code. Such understanding is vital for guiding technical decisions, simplifying onboarding processes, and improving tooling and educational resources.", "method": "The paper proposes a novel workflow for programming language topic classification using a multi-label Support Vector Machine (SVM) combined with a sliding window and voting strategy. This design enables fine-grained localization of language concepts within source code. The workflow is trained and evaluated using the IBM Project CodeNet dataset.", "result": "The proposed model achieves an average F1 score of 0.90 for general topic classification and 0.75 in code-topic highlighting, indicating high accuracy and effective localization capabilities.", "conclusion": "The research offers both empirical insights and a reusable classification pipeline, which can benefit researchers and practitioners interested in code analysis and data-driven software engineering."}}
{"id": "2509.20780", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20780", "abs": "https://arxiv.org/abs/2509.20780", "authors": ["Daniela Grassi", "Fabio Calefato", "Darja Smite", "Nicole Novielli", "Filippo Lanubile"], "title": "Exploring Engagement in Hybrid Meetings", "comment": null, "summary": "Background. The widespread adoption of hybrid work following the COVID-19\npandemic has fundamentally transformed software development practices,\nintroducing new challenges in communication and collaboration as organizations\ntransition from traditional office-based structures to flexible working\narrangements. This shift has established a new organizational norm where even\ntraditionally office-first companies now embrace hybrid team structures. While\nremote participation in meetings has become commonplace in this new\nenvironment, it may lead to isolation, alienation, and decreased engagement\namong remote team members. Aims. This study aims to identify and characterize\nengagement patterns in hybrid meetings through objective measurements, focusing\non the differences between co-located and remote participants. Method. We\nstudied professionals from three software companies over several weeks,\nemploying a multimodal approach to measure engagement. Data were collected\nthrough self-reported questionnaires and physiological measurements using\nbiometric devices during hybrid meetings to understand engagement dynamics.\nResults. The regression analyses revealed comparable engagement levels between\nonsite and remote participants, though remote participants show lower\nengagement in long meetings regardless of participation mode. Active roles\npositively correlate with higher engagement, while larger meetings and\nafternoon sessions are associated with lower engagement. Conclusions. Our\nresults offer insights into factors associated with engagement and\ndisengagement in hybrid meetings, as well as potential meeting improvement\nrecommendations. These insights are potentially relevant not only for software\nteams but also for knowledge-intensive organizations across various sectors\nfacing similar hybrid collaboration challenges.", "AI": {"tldr": "Hybrid software meetings show similar engagement for onsite and remote members, but long, large, and afternoon meetings lower engagement\u2014especially for remote workers. Active involvement improves engagement. The findings guide how hybrid meetings can be better managed in tech and knowledge-driven sectors.", "motivation": "The motivation of this paper stems from the widespread transformation of software development practices due to the shift toward hybrid work models after COVID-19. As organizations adapt to hybrid structures, new challenges in communication and collaboration arise, particularly the risk of isolation and disengagement among remote team members in hybrid meetings.", "method": "The study used a multimodal approach to measure engagement in hybrid meetings among professionals at three software companies over several weeks. Data were gathered using both self-reported questionnaires and physiological measurements via biometric devices during meetings to objectively assess engagement dynamics.", "result": "Regression analyses indicated that on-site and remote participants generally maintained similar levels of engagement. However, engagement among remote participants decreased in longer meetings regardless of whether they were remote or in-person. Active participation increased engagement, but larger attendance and afternoon meetings saw decreased engagement.", "conclusion": "The research provides key insights into the factors influencing engagement and disengagement in hybrid meetings, offering actionable recommendations for improving meeting effectiveness. These findings hold relevance not just for software teams but for all knowledge-intense organizations adapting to hybrid work environments."}}
{"id": "2509.20837", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20837", "abs": "https://arxiv.org/abs/2509.20837", "authors": ["Srishti Gureja", "Elena Tommasone", "Jingyi He", "Sara Hooker", "Matthias Gall\u00e9", "Marzieh Fadaee"], "title": "Verification Limits Code LLM Training", "comment": null, "summary": "Large language models for code generation increasingly rely on synthetic\ndata, where both problem solutions and verification tests are generated by\nmodels. While this enables scalable data creation, it introduces a previously\nunexplored bottleneck: the verification ceiling, in which the quality and\ndiversity of training data are fundamentally constrained by the capabilities of\nsynthetic verifiers. In this work, we systematically study how verification\ndesign and strategies influence model performance. We investigate (i) what we\nverify by analyzing the impact of test complexity and quantity: richer test\nsuites improve code generation capabilities (on average +3 pass@1), while\nquantity alone yields diminishing returns, (ii) how we verify by exploring\nrelaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By\nallowing for relaxed thresholds or incorporating LLM-based soft verification,\nwe can recover valuable training data, leading to a 2-4 point improvement in\npass@1 performance. However, this benefit is contingent upon the strength and\ndiversity of the test cases used, and (iii) why verification remains necessary\nthrough controlled comparisons of formally correct versus incorrect solutions\nand human evaluation: retaining diverse correct solutions per problem yields\nconsistent generalization gains. Our results show that Verification as\ncurrently practiced is too rigid, filtering out valuable diversity. But it\ncannot be discarded, only recalibrated. By combining calibrated verification\nwith diverse, challenging problem-solution pairs, we outline a path to break\nthe verification ceiling and unlock stronger code generation models.", "AI": {"tldr": "This paper shows that strict verification practices limit code generation model performance by filtering out diverse, correct solutions. By using richer tests and relaxed criteria, training data quality and model performance improve. Verification is necessary but must be recalibrated to unlock stronger models.", "motivation": "Large language models (LLMs) for code generation increasingly use synthetic data generated by models for training. The process is scalable but faces a bottleneck: the verification ceiling, where the quality and diversity of training data are limited by the synthetic verifiers\u2019 capability. This paper aims to analyze and address how verification practices impact code generation performance.", "method": "The authors systematically study the design and strategies of verification in synthetic data generation for coding LLMs. They analyze test suite complexity, the quantity of test cases, the impact of rigid versus relaxed pass criteria, and the necessity of verification by comparing formally correct/incorrect solutions and human evaluation.", "result": "They find that richer and more complex test suites improve code generation (average +3 pass@1), while simply increasing the number of tests has diminishing returns. Loosening rigid 100% pass criteria by using relaxed or LLM-based soft verification recovers valuable data, yielding 2\u20134 point improvements in pass@1. However, the gains depend on the diversity and strength of test cases. Furthermore, retaining diverse correct solutions per problem yields generalization benefits.", "conclusion": "Current verification practices for training data in code generation models are too rigid and filter out valuable, diverse solutions. Verification remains essential, but should be recalibrated via more nuanced and diverse approaches to break the performance ceiling of code generation models."}}
{"id": "2509.20881", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20881", "abs": "https://arxiv.org/abs/2509.20881", "authors": ["Yixuan Li", "Xinyi Liu", "Weidong Yang", "Ben Fei", "Shuhao Li", "Mingjie Zhou", "Lipeng Ma"], "title": "PseudoBridge: Pseudo Code as the Bridge for Better Semantic and Logic Alignment in Code Retrieval", "comment": null, "summary": "Code search aims to precisely find relevant code snippets that match natural\nlanguage queries within massive codebases, playing a vital role in software\ndevelopment. Recent advances leverage pre-trained language models (PLMs) to\nbridge the semantic gap between unstructured natural language (NL) and\nstructured programming languages (PL), yielding significant improvements over\ntraditional information retrieval and early deep learning approaches. However,\nexisting PLM-based methods still encounter key challenges, including a\nfundamental semantic gap between human intent and machine execution logic, as\nwell as limited robustness to diverse code styles. To address these issues, we\npropose PseudoBridge, a novel code retrieval framework that introduces\npseudo-code as an intermediate, semi-structured modality to better align NL\nsemantics with PL logic. Specifically, PseudoBridge consists of two stages.\nFirst, we employ an advanced large language model (LLM) to synthesize\npseudo-code, enabling explicit alignment between NL queries and pseudo-code.\nSecond, we introduce a logic-invariant code style augmentation strategy and\nemploy the LLM to generate stylistically diverse yet logically equivalent code\nimplementations with pseudo-code, then align the code snippets of different\nstyles with pseudo-code, enhancing model robustness to code style variation. We\nbuild PseudoBridge across 10 different PLMs and evaluate it on 6 mainstream\nprogramming languages. Extensive experiments demonstrate that PseudoBridge\nconsistently outperforms baselines, achieving significant gains in retrieval\naccuracy and generalization, particularly under zero-shot domain transfer\nscenarios such as Solidity and XLCoST datasets. These results demonstrate the\neffectiveness of explicit logical alignment via pseudo-code and highlight\nPseudoBridge's potential as a robust, generalizable solution for code\nretrieval.", "AI": {"tldr": "PseudoBridge introduces a novel code search framework using pseudo-code for logical alignment of NL queries and code, coupled with style-invariant augmentation. It achieves significant accuracy gains and generalizes well across languages and domains, surpassing previous approaches, especially in zero-shot scenarios.", "motivation": "Current PLM-based code search methods struggle with the semantic gap between human intent and code execution, and their robustness to code style diversity is limited. The motivation is to provide a more precise and robust approach to align NL queries with code, enabling better retrieval across diverse programming scenarios.", "method": "PseudoBridge uses a two-stage process: (1) Large Language Models generate pseudo-code to explicitly align natural language queries with code logic; (2) It augments data by generating logically equivalent code snippets in diverse styles, improving robustness against code variation and aligning them with pseudo-code across multiple PLMs.", "result": "PseudoBridge, evaluated across 10 PLMs and 6 languages, achieves superior retrieval accuracy and generalization compared to baselines, particularly excelling in zero-shot domain transfer tasks (e.g., Solidity, XLCoST). The method proves effective in logical alignment and robustness to code style variation.", "conclusion": "PseudoBridge significantly improves code retrieval performance and generalization through explicit alignment between natural language and code logic using pseudo-code, outperforming existing methods and demonstrating strong results under zero-shot scenarios."}}
{"id": "2509.21067", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.21067", "abs": "https://arxiv.org/abs/2509.21067", "authors": ["Oka Kurniawan", "Erick Chandra", "Christopher M. Poskitt", "Yannic Noller", "Kenny Tsu Wei Choo", "Cyrille Jegourel"], "title": "Designing for Novice Debuggers: A Pilot Study on an AI-Assisted Debugging Tool", "comment": "Accepted by the 25th Koli Calling International Conference on\n  Computing Education Research (Koli Calling 2025)", "summary": "Debugging is a fundamental skill that novice programmers must develop.\nNumerous tools have been created to assist novice programmers in this process.\nRecently, large language models (LLMs) have been integrated with automated\nprogram repair techniques to generate fixes for students' buggy code. However,\nmany of these tools foster an over-reliance on AI and do not actively engage\nstudents in the debugging process. In this work, we aim to design an intuitive\ndebugging assistant, CodeHinter, that combines traditional debugging tools with\nLLM-based techniques to help novice debuggers fix semantic errors while\npromoting active engagement in the debugging process. We present findings from\nour second design iteration, which we tested with a group of undergraduate\nstudents. Our results indicate that the students found the tool highly\neffective in resolving semantic errors and significantly easier to use than the\nfirst version. Consistent with our previous study, error localization was the\nmost valuable feature. Finally, we conclude that any AI-assisted debugging tool\nshould be personalized based on user profiles to optimize their interactions\nwith students.", "AI": {"tldr": "The paper introduces 'CodeHinter', a debugging tool for students that blends traditional and AI-based methods to promote active learning. Testing confirmed it helps fix semantic errors and is user-friendly, and personalization is key for best results.", "motivation": "Novice programmers often struggle with debugging, and while many AI tools can generate code fixes, they may encourage passivity rather than active learning and skill development. The authors aim to design a tool that both helps correct errors and promotes student engagement in the debugging process.", "method": "The authors developed 'CodeHinter', an intuitive debugging assistant that merges traditional debugging techniques with LLM-based (large language model) approaches. They iteratively designed and improved the tool, testing this second version with undergraduate students, and gathered feedback regarding its usability and effectiveness.", "result": "The revised tool was found by students to be highly effective at resolving semantic errors and was regarded as significantly easier to use than its prior version. Error localization remained the most appreciated feature among students.", "conclusion": "Personalization of AI-assisted debugging tools to individual user profiles is essential for optimizing student interactions and maximizing the effectiveness of such tools in novice programming education."}}
{"id": "2509.21068", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21068", "abs": "https://arxiv.org/abs/2509.21068", "authors": ["Nek Dil Khan", "Javed Ali Khan", "Mobashir Husain", "Muhammad Sohail Khan", "Arif Ali Khan", "Muhammad Azeem Akbar", "Shahid Hussain"], "title": "An Improved Quantum Software Challenges Classification Approach using Transfer Learning and Explainable AI", "comment": null, "summary": "Quantum Software Engineering (QSE) is a research area practiced by tech\nfirms. Quantum developers face challenges in optimizing quantum computing and\nQSE concepts. They use Stack Overflow (SO) to discuss challenges and label\nposts with specialized quantum tags, which often refer to technical aspects\nrather than developer posts. Categorizing questions based on quantum concepts\ncan help identify frequent QSE challenges. We conducted studies to classify\nquestions into various challenges. We extracted 2829 questions from Q&A\nplatforms using quantum-related tags. Posts were analyzed to identify frequent\nchallenges and develop a novel grounded theory. Challenges include Tooling,\nTheoretical, Learning, Conceptual, Errors, and API Usage. Through content\nanalysis and grounded theory, discussions were annotated with common challenges\nto develop a ground truth dataset. ChatGPT validated human annotations and\nresolved disagreements. Fine-tuned transformer algorithms, including BERT,\nDistilBERT, and RoBERTa, classified discussions into common challenges. We\nachieved an average accuracy of 95% with BERT DistilBERT, compared to\nfine-tuned Deep and Machine Learning (D&ML) classifiers, including Feedforward\nNeural Networks (FNN), Convolutional Neural Networks (CNN), and Long Short-Term\nMemory networks (LSTM), which achieved accuracies of 89%, 86%, and 84%,\nrespectively. The Transformer-based approach outperforms the D&ML-based\napproach with a 6\\% increase in accuracy by processing actual discussions,\ni.e., without data augmentation. We applied SHAP (SHapley Additive\nexPlanations) for model interpretability, revealing how linguistic features\ndrive predictions and enhancing transparency in classification. These findings\ncan help quantum vendors and forums better organize discussions for improved\naccess and readability. However,empirical evaluation studies with actual\ndevelopers and vendors are needed.", "AI": {"tldr": "The paper analyzes quantum computing Q&A posts and successfully classifies them into challenge types using state-of-the-art transformer models, achieving higher accuracy and better interpretability than traditional methods, potentially improving the organization of quantum developer discussions online.", "motivation": "Quantum software engineering (QSE) faces distinct technical challenges, yet Stack Overflow and similar forums do not categorize discussions well according to these specific challenge types. Better classification can aid in understanding and addressing common issues faced by quantum developers.", "method": "The authors collected and analyzed 2829 Q&A posts tagged with quantum-related concepts from online platforms. Posts were categorized based on frequent QSE challenges using content analysis and grounded theory. Annotations were validated by both humans and ChatGPT. Multiple machine learning models, including fine-tuned transformers (BERT, DistilBERT, RoBERTa) and traditional neural networks (FNN, CNN, LSTM), were trained to automatically classify these challenges. SHAP was used for interpreting model predictions.", "result": "Transformer-based models, especially BERT and DistilBERT, achieved superior accuracy (95%) compared to traditional D&ML models (FNN: 89%, CNN: 86%, LSTM: 84%). The transformer approach outperformed others by 6% and worked well without data augmentation. SHAP analysis provided insights into feature influence on predictions.", "conclusion": "Classifying quantum software engineering questions by challenge type using transformer models is highly effective and interpretable. This improves organization and accessibility of QSE discussions online. Future developer-centered empirical studies are recommended for further validation."}}
{"id": "2509.21170", "categories": ["cs.SE", "cs.AI", "D.2.3; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.21170", "abs": "https://arxiv.org/abs/2509.21170", "authors": ["Yongda Yu", "Guohao Shi", "Xianwei Wu", "Haochuan He", "XueMing Gu", "Qianqian Zhao", "Kui Liu", "Qiushi Wang", "Zhao Tian", "Haifeng Shen", "Guoping Rong"], "title": "Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach", "comment": "22 pages", "summary": "Large Language Models (LLMs) have shown great potential in supporting\nautomated code review due to their impressive capabilities in context\nunderstanding and reasoning. However, these capabilities are still limited\ncompared to human-level cognition because they are heavily influenced by the\ntraining data. Recent research has demonstrated significantly improved\nperformance through fine-tuning LLMs with code review data. However, compared\nto human reviewers who often simultaneously analyze multiple dimensions of code\nreview to better identify issues, the full potential of these methods is\nhampered by the limited or vague information used to fine-tune the models. This\npaper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that\ntrains LLMs with an impressive reasoning ability to analyze multiple dimensions\nof code review by harnessing long COT techniques to provide rich structured\ninformation. To address context loss and reasoning logic loss issues that\nfrequently occur when LLMs process long COT prompts, we propose a solution that\ncombines the Maximum Entropy (ME) modeling principle with pre-defined reasoning\npathways in MelcotCR to enable more effective utilization of in-context\nknowledge within long COT prompts while strengthening the logical tightness of\nthe reasoning process. Empirical evaluations on our curated MelcotCR dataset\nand the public CodeReviewer dataset reveal that a low-parameter base model,\nsuch as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art\nmethods in terms of the accuracy of detecting and describing code issues, with\nits performance remarkably on par with that of the 671B DeepSeek-R1 model.", "AI": {"tldr": "MelcotCR is a novel fine-tuning method that equips LLMs with advanced multidimensional reasoning for code review, allowing smaller models to outperform or equal much larger ones using structured chain-of-thought prompts and improved logic retention.", "motivation": "Large Language Models (LLMs) exhibit great promise for automated code review, but still fall short compared to human reviewers in multidimensional analysis, partly due to limited or vague fine-tuning data. The authors are motivated to enhance LLMs\u2019 capabilities by improving their reasoning and context management during code review.", "method": "The paper introduces MelcotCR, a chain-of-thought (COT) fine-tuning approach that leverages long COT techniques for richer, structured input, and combines Maximum Entropy modeling with pre-defined reasoning paths to address context and logical reasoning loss.", "result": "Experiments on both a new MelcotCR dataset and the public CodeReviewer dataset show that a relatively small LLM, such as the 14B Qwen2.5 model fine-tuned with MelcotCR, outperforms existing state-of-the-art methods and matches the performance of a much larger 671B model (DeepSeek-R1) in identifying and describing code issues.", "conclusion": "MelcotCR significantly boosts LLM reasoning for code review, enabling low-parameter models to match or surpass much larger models in code issue detection and description accuracy by leveraging robust COT fine-tuning and advanced reasoning techniques."}}
{"id": "2509.21292", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21292", "abs": "https://arxiv.org/abs/2509.21292", "authors": ["Ronivaldo Ferreira", "Guilherme da Silva", "Carla Rocha", "Gustavo Pinto"], "title": "Semantic Clustering of Civic Proposals: A Case Study on Brazil's National Participation Platform", "comment": "12 pages, in Portuguese language", "summary": "Promoting participation on digital platforms such as Brasil Participativo has\nemerged as a top priority for governments worldwide. However, due to the sheer\nvolume of contributions, much of this engagement goes underutilized, as\norganizing it presents significant challenges: (1) manual classification is\nunfeasible at scale; (2) expert involvement is required; and (3) alignment with\nofficial taxonomies is necessary. In this paper, we introduce an approach that\ncombines BERTopic with seed words and automatic validation by large language\nmodels. Initial results indicate that the generated topics are coherent and\ninstitutionally aligned, with minimal human effort. This methodology enables\ngovernments to transform large volumes of citizen input into actionable data\nfor public policy.", "AI": {"tldr": "A novel topic-modeling approach using BERTopic, seed words, and LLMs can efficiently organize citizen input from digital platforms, making it easier for governments to use this data for policy decisions with minimal manual effort.", "motivation": "Governments want to promote citizen participation through digital platforms, but face challenges in organizing and utilizing the large volume of contributions. Manual classification is not scalable, requires expert involvement, and must be in line with official taxonomies.", "method": "The authors propose an approach that combines BERTopic (a topic modeling technique), seed words, and automatic validation using large language models to classify and organize contributions from digital platforms.", "result": "The initial results suggest that the proposed method produces coherent topics that align well with institutional needs, and significantly reduces the need for human effort.", "conclusion": "The methodology allows governments to efficiently convert vast amounts of citizen input into actionable data for policymaking, addressing key scalability and alignment challenges."}}
