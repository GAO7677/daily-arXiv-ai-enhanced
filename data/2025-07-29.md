<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 31]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [AccessGuru: Leveraging LLMs to Detect and Correct Web Accessibility Violations in HTML Code](https://arxiv.org/abs/2507.19549)
*Nadeen Fathallah,Daniel Hernández,Steffen Staab*

Main category: cs.SE

TL;DR: This paper introduces AccessGuru, a method using LLMs and existing tools to automatically detect and fix web accessibility violations, achieving much higher correction rates than previous approaches.


<details>
  <summary>Details</summary>
Motivation: Web pages often do not meet accessibility guidelines, preventing people with diverse abilities from using them. Manual corrections are costly and require expertise, so there is a need for automated methods to detect and fix accessibility issues in HTML code.

Method: The authors introduce a taxonomy of accessibility violations (Syntactic, Semantic, Layout) and propose AccessGuru, a method combining existing testing tools and Large Language Models (LLMs). Taxonomy-driven prompting strategies are used for detecting and correcting violations. An evaluation benchmark is created to assess performance based on compliance and expert comparison.

Result: AccessGuru achieves up to 84% reduction in average violation scores, surpassing previous methods which reach at most 50%.

Conclusion: AccessGuru, by integrating LLMs and existing tools using a novel taxonomy, effectively automates the detection and correction of a broad range of web accessibility issues, outperforming prior solutions.

Abstract: The vast majority of Web pages fail to comply with established Web
accessibility guidelines, excluding a range of users with diverse abilities
from interacting with their content. Making Web pages accessible to all users
requires dedicated expertise and additional manual efforts from Web page
providers. To lower their efforts and promote inclusiveness, we aim to
automatically detect and correct Web accessibility violations in HTML code.
While previous work has made progress in detecting certain types of
accessibility violations, the problem of automatically detecting and correcting
accessibility violations remains an open challenge that we address. We
introduce a novel taxonomy classifying Web accessibility violations into three
key categories - Syntactic, Semantic, and Layout. This taxonomy provides a
structured foundation for developing our detection and correction method and
redefining evaluation metrics. We propose a novel method, AccessGuru, which
combines existing accessibility testing tools and Large Language Models (LLMs)
to detect violations and applies taxonomy-driven prompting strategies to
correct all three categories. To evaluate these capabilities, we develop a
benchmark of real-world Web accessibility violations. Our benchmark quantifies
syntactic and layout compliance and judges semantic accuracy through
comparative analysis with human expert corrections. Evaluation against our
benchmark shows that AccessGuru achieves up to 84% average violation score
decrease, significantly outperforming prior methods that achieve at most 50%.

</details>


### [2] [LastMerge: A language-agnostic structured tool for code integration](https://arxiv.org/abs/2507.19687)
*Joao Pedro Duarte,Paulo Borba,Guilherme Cavalcanti*

Main category: cs.SE

TL;DR: LastMerge, a generic structured merge tool, offers similar accuracy and performance as language-specific tools, with some advantages, making structured merge feasible for more programming languages and increasing its practical adoption.


<details>
  <summary>Details</summary>
Motivation: Unstructured line-based merge tools are widely used but have limited accuracy. Structured AST-based (Abstract Syntax Tree) merge tools improve accuracy but are typically language-specific, difficult to develop, and not available for many programming languages. There is a need for a more generic, broadly applicable structured merge tool to increase merge accuracy across different languages.

Method: The authors propose LastMerge, a generic structured merge tool configurable via a thin interface to reduce setup cost for new languages. They analyze the impact of generic structured merge on merge accuracy and runtime by comparing four tools (jDime and Spork for Java, and their generic counterparts LastMerge and Mergiraf), replaying merge scenarios from a large dataset, and collecting metrics on runtime, behavioral differences, and merge accuracy.

Result: The experiment found that generic structured merge does not significantly hurt merge accuracy. While there is a roughly 10% difference rate between language-specific and generic tools, most differences are due to implementation details. LastMerge had 15% fewer false positives than jDime, and Mergiraf had 42% fewer false negatives than Spork. Both generic tools had runtime performance comparable to language-specific tools.

Conclusion: Generic structured merge tools like LastMerge can match the accuracy and performance of language-specific tools and therefore could replace them, enabling wider and easier adoption of structured merge in the software industry.

Abstract: Unstructured line-based merge tools are widely used in practice. Structured
AST-based merge tools show significantly improved merge accuracy, but are
rarely used in practice because they are language specific and costly,
consequently not being available for many programming languages. To improve
merge accuracy for a wide range of languages, we propose LastMerge, a generic
structured merge tool that can be configured through a thin interface that
significantly reduces the effort of supporting structured merge. To understand
the impact that generic structured merge might have on merge accuracy and
performance, we run an experiment with four structured merge tools: two Java
specific tools, jDime and Spork, and their generic counterparts, respectively
LastMerge and Mergiraf. Using each tool, we replay merge scenarios from a
significant dataset, and collect data on runtime, behavioral divergences, and
merge accuracy. Our results show no evidence that generic structured merge
significantly impacts merge accuracy. Although we observe a difference rate of
approximately 10% between the Java specific tools and their generic
counterparts, most of the differences stem from implementation details and
could be avoided. We find that LastMerge reports 15% fewer false positives than
jDime while Mergiraf misses 42% fewer false negatives than Spork. Both generic
tools exhibit comparable runtime performance to the state of the art language
specific implementations. These results suggest that generic structured merge
tools can effectively replace language-specific ones, paving the way for
broader adoption of structured merge in industry.

</details>


### [3] [Refactoring $\neq$ Bug-Inducing: Improving Defect Prediction with Code Change Tactics Analysis](https://arxiv.org/abs/2507.19714)
*Feifei Niu,Junqian Shao,Christoph Mayr-Dorn,Liguo Huang,Wesley K. G. Assunção,Chuanyi Li,Jidong Ge,Alexander Egyed*

Main category: cs.SE

TL;DR: Ignoring refactoring in just-in-time defect prediction leads to biased results and lower model performance. By categorizing and integrating refactoring information, the authors improve labeling and significantly boost model recall and F1-scores, urging its inclusion in future JIT-DP research and evaluation.


<details>
  <summary>Details</summary>
Motivation: Prior work in just-in-time defect prediction (JIT-DP) improved accuracy using code metrics and semantic features, but largely ignored the role of code refactoring, even though refactoring is common and can be intertwined with bug-fixing or inducing changes. Neglecting refactoring may introduce bias into model learning and evaluation.

Method: The paper investigates the impact of refactoring and its propagation on six state-of-the-art JIT-DP models. It introduces Code chAnge Tactics (CAT) analysis to categorize refactoring and its propagation, and refines dataset labeling. Experiments assess the performance of various models with and without consideration of refactoring.

Result: Including refactoring improves labeling accuracy in the JIT-Defects4J dataset by 13.7%. Ignoring refactoring can substantially reduce model F1-score (by 18.6% and 37.3% for semantic-based models). Integrating refactoring enhances model recall and F1-score, with improvements up to 43.2% and 32.5%, respectively.

Conclusion: Refactoring should be incorporated into both methodology and evaluation of JIT-DP, as it significantly affects model accuracy and assessment. The proposed CAT analysis has broader applications for software maintenance beyond defect prediction.

Abstract: Just-in-time defect prediction (JIT-DP) aims to predict the likelihood of
code changes resulting in software defects at an early stage. Although code
change metrics and semantic features have enhanced prediction accuracy, prior
research has largely ignored code refactoring during both the evaluation and
methodology phases, despite its prevalence. Refactoring and its propagation
often tangle with bug-fixing and bug-inducing changes within the same commit
and statement. Neglecting refactoring can introduce bias into the learning and
evaluation of JIT-DP models. To address this gap, we investigate the impact of
refactoring and its propagation on six state-of-the-art JIT-DP approaches. We
propose Code chAnge Tactics (CAT) analysis to categorize code refactoring and
its propagation, which improves labeling accuracy in the JIT-Defects4J dataset
by 13.7%. Our experiments reveal that failing to consider refactoring
information in the dataset can diminish the performance of models, particularly
semantic-based models, by 18.6% and 37.3% in F1-score. Additionally, we propose
integrating refactoring information to enhance six baseline approaches,
resulting in overall improvements in recall and F1-score, with increases of up
to 43.2% and 32.5%, respectively. Our research underscores the importance of
incorporating refactoring information in the methodology and evaluation of
JIT-DP. Furthermore, our CAT has broad applicability in analyzing refactoring
and its propagation for software maintenance.

</details>


### [4] [Clean Code In Practice: Challenges and Opportunities](https://arxiv.org/abs/2507.19721)
*Dapeng Yan,Wenjie Yang,Kui Liu,Zhiming Liu,Zhikuang Cai*

Main category: cs.SE

TL;DR: Integrating security and safety metrics with traditional reliability measurements creates a stronger framework for predicting and ensuring software robustness. The paper suggests actionable steps for practitioners to improve reliability prediction by considering security and safety together.


<details>
  <summary>Details</summary>
Motivation: With the increasing complexity of software systems, traditional reliability prediction metrics may fall short in accounting for intertwined security and safety concerns. There is a growing need for methods that integrate these factors to better ensure system robustness.

Method: The paper conducts a comprehensive analysis of existing industry metrics and measurement techniques for software reliability prediction. It also introduces a framework for threat estimation that factors in both security and safety aspects.

Result: The study identifies key threats affecting software reliability and demonstrates that integrating security and safety considerations into reliability metrics leads to more robust software systems. Actionable guidelines for improving reliability prediction are provided.

Conclusion: Incorporating security and safety considerations into software reliability prediction enhances the effectiveness of reliability modeling and strengthens system robustness. The proposed guidelines can help practitioners improve both reliability and overall system security and safety.

Abstract: Reliability prediction is crucial for ensuring the safety and security of
software systems, especially in the context of industry practices. While
various metrics and measurements are employed to assess software reliability,
the complexity of modern systems necessitates a deeper understanding of how
these metrics interact with security and safety concerns. This paper explores
the interplay between software reliability, safety, and security, offering a
comprehensive analysis of key metrics and measurement techniques used in the
industry for reliability prediction. We identify critical threats to software
reliability and provide a threat estimation framework that incorporates both
safety and security aspects. Our findings suggest that integrating reliability
metrics with safety and security considerations can enhance the robustness of
software systems. Furthermore, we propose a set of actionable guidelines for
practitioners to improve their reliability prediction models while
simultaneously addressing the security and safety challenges of contemporary
software applications.

</details>


### [5] [Defining ethically sourced code generation](https://arxiv.org/abs/2507.19743)
*Zhuolin Xu,Chenglin Li,Qiushi Li,Shin Hwei Tan*

Main category: cs.SE

TL;DR: The paper introduces the concept of Ethically Sourced Code Generation (ES-CodeGen), reviews 803 papers and surveys 32 practitioners to define 11 dimensions for ensuring ethical AI code generation. It finds social issues are often neglected and urges greater attention to comprehensive ethical practices in this emerging domain.


<details>
  <summary>Details</summary>
Motivation: There is an increasing need to address ethical issues in AI-assisted code generation, such as uncertain licensing, privacy, fairness, and environmental impacts, to promote responsible AI use. Other domains like speech and image generation have seen efforts to source data ethically, but code generation has lacked a comprehensive approach.

Method: The authors conducted a two-phase literature review, analyzing 803 papers (including those specifically on AI code generation), and identified 71 relevant papers. They built a taxonomy with 10 initial dimensions for ethical code generation. They surveyed 32 practitioners, including developers affected by ethical concerns, to refine these dimensions and understand their real-world effects.

Result: The study identified 11 dimensions (expanding from the original 10, adding code quality) crucial to ethically sourced code generation (ES-CodeGen). The research also highlights the gap where practitioners often overlook socially related ethical dimensions, despite their importance. The study provides consequences, artifacts, and development stages relevant to ES-CodeGen.

Conclusion: There is a strong need for more awareness and attention to ethical considerations in the development and deployment of code generation models. The research introduces a comprehensive taxonomy and calls on the community to address gaps, particularly in social aspects, to ensure truly ethical AI code generation.

Abstract: Several code generation models have been proposed to help reduce time and
effort in solving software-related tasks. To ensure responsible AI, there are
growing interests over various ethical issues (e.g., unclear licensing,
privacy, fairness, and environment impact). These studies have the overarching
goal of ensuring ethically sourced generation, which has gained growing
attentions in speech synthesis and image generation. In this paper, we
introduce the novel notion of Ethically Sourced Code Generation (ES-CodeGen) to
refer to managing all processes involved in code generation model development
from data collection to post-deployment via ethical and sustainable practices.
To build a taxonomy of ES-CodeGen, we perform a two-phase literature review
where we read 803 papers across various domains and specific to AI-based code
generation. We identified 71 relevant papers with 10 initial dimensions of
ES-CodeGen. To refine our dimensions and gain insights on consequences of
ES-CodeGen, we surveyed 32 practitioners, which include six developers who
submitted GitHub issues to opt-out from the Stack dataset (these impacted users
have real-world experience of ethically sourcing issues in code generation
models). The results lead to 11 dimensions of ES-CodeGen with a new dimension
on code quality as practitioners have noted its importance. We also identified
consequences, artifacts, and stages relevant to ES-CodeGen. Our post-survey
reflection showed that most practitioners tend to ignore social-related
dimensions despite their importance. Most practitioners either agreed or
strongly agreed that our survey help improve their understanding of ES-CodeGen.
Our study calls for attentions of various ethical issues towards ES-CodeGen.

</details>


### [6] [From Few-Label to Zero-Label: An Approach for Cross-System Log-Based Anomaly Detection with Meta-Learning](https://arxiv.org/abs/2507.19806)
*Xinlong Zhao,Tong Jia,Minghua He,Yihan Wu,Ying Li,Gang Huang*

Main category: cs.SE

TL;DR: FreeLog is a meta-learning based method for log anomaly detection that works across software systems without any labeled target data, achieving results similar to leading methods that require some labeled data.


<details>
  <summary>Details</summary>
Motivation: Log anomaly detection is crucial for maintaining software stability, but current methods require many labeled logs, which are hard to obtain. Reducing the labeling requirement, particularly for different software systems (cross-system), is an important research goal.

Method: The paper introduces FreeLog, a system-agnostic representation meta-learning approach. FreeLog enables log anomaly detection across systems without needing any labeled logs from the target system (zero-label setting).

Result: FreeLog performs comparably to state-of-the-art cross-system log anomaly detection methods that rely on a small amount of labeled data from the target system.

Conclusion: FreeLog eliminates the need for labeled target logs in cross-system log anomaly detection and effectively solves the cold-start problem, broadening the applicability of anomaly detection in real-world scenarios.

Abstract: Log anomaly detection plays a critical role in ensuring the stability and
reliability of software systems. However, existing approaches rely on large
amounts of labeled log data, which poses significant challenges in real-world
applications. To address this issue, cross-system transfer has been identified
as a key research direction. State-of-the-art cross-system approaches achieve
promising performance with only a few labels from the target system. However,
their reliance on labeled target logs makes them susceptible to the cold-start
problem when labeled logs are insufficient. To overcome this limitation, we
explore a novel yet underexplored setting: zero-label cross-system log anomaly
detection, where the target system logs are entirely unlabeled. To this end, we
propose FreeLog, a system-agnostic representation meta-learning method that
eliminates the need for labeled target system logs, enabling cross-system log
anomaly detection under zero-label conditions. Experimental results on three
public log datasets demonstrate that FreeLog achieves performance comparable to
state-of-the-art methods that rely on a small amount of labeled data from the
target system.

</details>


### [7] [LLM-Based Repair of Static Nullability Errors](https://arxiv.org/abs/2507.20674)
*Nima Karimipour,Michael Pradel,Martin Kellogg,Manu Sridharan*

Main category: cs.SE

TL;DR: NullRepair uses LLMs within a structured workflow to automatically fix tough nullability errors in Java code. It resolves most errors left after annotation inference, with minimal impact on program functionality.


<details>
  <summary>Details</summary>
Motivation: Null-pointer exceptions are a common and difficult problem in Java codebases. While static analysis tools that track nullness as a type property can help, integrating them into large, existing projects results in many persistent errors that are hard to address manually. Naive application of large language models (LLMs) does not solve the problem due to lacking context and appropriate code edits. There is a need for a systematic and automated approach to resolve nullability errors using LLMs effectively.

Method: The authors present NullRepair, a system that integrates LLMs into a structured workflow for resolving nullability errors as identified by static analysis. The workflow uses a decision process guided by a flowchart based on manual study of 200 real-world errors. NullRepair employs static analysis to differentiate safe and unsafe regions for null usage and augments LLM prompts with relevant, error-free code usage examples. The LLM communicates iteratively to generate patches, incorporating broad project context and guided decision logic.

Result: NullRepair was evaluated on 12 real-world Java projects. It successfully resolved an average of 72% of the errors left after state-of-the-art annotation inference. Additionally, application of the edits produced by NullRepair preserved program behavior in most cases: all unit tests passed in 10 out of 12 projects, and at least 98% of tests passed in the remaining two.

Conclusion: NullRepair provides a practical and effective system for resolving hard-to-fix nullability errors in Java by combining static analysis and structured LLM interaction. It significantly automates and improves upon current error-correction efforts while maintaining program correctness.

Abstract: Modern Java projects increasingly adopt static analysis tools that prevent
null-pointer exceptions by treating nullness as a type property. However,
integrating such tools into large, existing codebases remains a significant
challenge. While annotation inference can eliminate many errors automatically,
a subset of residual errors -- typically a mix of real bugs and false positives
-- often persist and can only be resolved via code changes. Manually addressing
these errors is tedious and error-prone. Large language models (LLMs) offer a
promising path toward automating these repairs, but naively-prompted LLMs often
generate incorrect, contextually-inappropriate edits. Resolving a nullability
error demands a deep understanding of how a symbol is used across the codebase,
often spanning methods, classes, and packages. We present NullRepair, a system
that integrates LLMs into a structured workflow for resolving the errors from a
nullability checker. NullRepair's decision process follows a flowchart derived
from manual analysis of 200 real-world errors. It leverages static analysis to
identify safe and unsafe usage regions of symbols, using error-free usage
examples to contextualize model prompts. Patches are generated through an
iterative interaction with the LLM that incorporates project-wide context and
decision logic. Our evaluation on 12 real-world Java projects shows that
NullRepair resolves an average of 72% of the errors that remain after applying
a state-of-the-art annotation inference technique. Unlike a naively-prompted
LLM, NullRepair also largely preserves program semantics, with all unit tests
passing in 10/12 projects after applying every edit proposed by NullRepair, and
98% or more tests passing in the remaining two projects.

</details>


### [8] [A Cooperative Approach for Knowledge-based Business Process Design in a Public Authority](https://arxiv.org/abs/2507.19842)
*Mohammad Azarijafari,Luisa Mich,Michele Missikoff,Oleg Missikoff*

Main category: cs.SE

TL;DR: The paper presents an accessible, step-by-step knowledge-based method that helps business experts, especially in SMEs, design business processes efficiently—no prior technical expertise required—and promotes collaborative workflow creation.


<details>
  <summary>Details</summary>
Motivation: Enterprises, especially small and medium-sized ones, need to adapt to the digital transformation to stay competitive. There is a demand for innovative, process-oriented production models, and a barrier exists as business process design often requires expertise in Knowledge Engineering, which many business experts lack.

Method: The paper introduces a knowledge-based method that assists business experts in designing business processes without needing prior Knowledge Engineering expertise. The method provides a structured, step-by-step approach, beginning with simple text-based knowledge artefacts and gradually building towards formal, structured process representations. It also emphasizes facilitating collaboration among all stakeholders involved in the business process design.

Result: The method enables business experts to create diagrammatic workflows for target processes efficiently, making process-oriented production models more accessible. It simplifies and shares the process design approach, supporting collaboration among diverse stakeholders.

Conclusion: The proposed knowledge-based method successfully supports business experts in designing business processes, reduces the entry barrier by eliminating the need for prior Knowledge Engineering expertise, and fosters a shared, collaborative approach.

Abstract: Enterprises are currently undergoing profound transformations due to the
unpostponable digital transformation. Then, to remain competitive, enterprises
must adapt their organisational structures and operations. This organisational
shift is also important for small and medium-sized enterprises. A key
innovation frontier is the adoption of process-oriented production models. This
paper presents a knowledge-based method to support business experts in
designing business processes. The method requires no prior expertise in
Knowledge Engineering and guides designers through a structured sequence of
steps to produce a diagrammatic workflow of the target process. The
construction of the knowledge base starts from simple, text-based, knowledge
artefacts and then progresses towards more structured, formal representations.
The approach has been conceived to allow a shared approach for all stakeholders
and actors who participate in the BP design.

</details>


### [9] [AgentMesh: A Cooperative Multi-Agent Generative AI Framework for Software Development Automation](https://arxiv.org/abs/2507.19902)
*Sourena Khanzadeh*

Main category: cs.SE

TL;DR: AgentMesh leverages multiple specialized AI agents to automate the software development process, improving over single-agent approaches by dividing tasks among Planner, Coder, Debugger, and Reviewer agents. This multi-agent method enhances code quality and efficiency but also faces challenges in scaling and error management.


<details>
  <summary>Details</summary>
Motivation: Software development is traditionally a complex and collaborative process, requiring input from people with varied expertise. Automating this process with AI could streamline development and make it more efficient, but previous single-agent approaches have limitations. The motivation is to leverage multiple specialized AI agents working together to improve automation, code quality, and efficiency in software engineering tasks.

Method: The authors propose AgentMesh, a Python-based framework that orchestrates multiple cooperative LLM-powered agents: Planner, Coder, Debugger, and Reviewer. Each agent is assigned a specialized role in the software development workflow such as task planning, code generation, debugging, and code review. Communication, prompt strategies, and workflow orchestration among agents are described, as well as a case study to demonstrate AgentMesh's effectiveness.

Result: A case study shows that AgentMesh can handle a complex software development request through stepwise planning, code generation, iterative debugging, and code review by specialized cooperating agents. The approach demonstrates how responsibility division and agent collaboration harness LLM strengths while addressing the limitations seen in single-agent systems. The study also highlights ongoing challenges such as error propagation and context scaling.

Conclusion: Dividing software development tasks among multiple LLM-powered agents within AgentMesh improves automation potential and addresses some issues of single-agent systems. The multi-agent framework presents a step forward toward robust AI-supported software engineering, though challenges remain in error handling and scaling.

Abstract: Software development is a complex, multi-phase process traditionally
requiring collaboration among individuals with diverse expertise. We propose
AgentMesh, a Python-based framework that uses multiple cooperating LLM-powered
agents to automate software development tasks. In AgentMesh, specialized agents
- a Planner, Coder, Debugger, and Reviewer - work in concert to transform a
high-level requirement into fully realized code. The Planner agent first
decomposes user requests into concrete subtasks; the Coder agent implements
each subtask in code; the Debugger agent tests and fixes the code; and the
Reviewer agent validates the final output for correctness and quality. We
describe the architecture and design of these agents and their communication,
and provide implementation details including prompt strategies and workflow
orchestration. A case study illustrates AgentMesh handling a non-trivial
development request via sequential task planning, code generation, iterative
debugging, and final code review. We discuss how dividing responsibilities
among cooperative agents leverages the strengths of large language models while
mitigating single-agent limitations. Finally, we examine current limitations -
such as error propagation and context scaling - and outline future work toward
more robust, scalable multi-agent AI systems for software engineering
automation.

</details>


### [10] [CrossPL: Evaluating Large Language Models on Cross Programming Language Code Generation](https://arxiv.org/abs/2507.19904)
*Zhanhang Xiong,Dongxia Wang,Yuekang Li,Xinyuan An,Wenhai Wang*

Main category: cs.SE

TL;DR: CrossPL is a new benchmark for testing LLMs' ability to generate interoperable code across programming languages. Evaluations show existing LLMs struggle with these tasks, indicating the need for specialized improvements.


<details>
  <summary>Details</summary>
Motivation: There is a lack of systematic evaluation for large language models' (LLMs) ability to generate code supporting cross-programming-language (CPL) interoperability, a vital skill for building software systems integrating multiple languages through mechanisms like inter-process communication (IPC).

Method: The authors introduce CrossPL, a benchmark comprising 1,982 tasks focused on IPC, spanning six programming languages and seven CPL strategies. The benchmark was constructed by analyzing 19,169 multi-language GitHub repositories using 156 designed finite state machines (FSMs), alongside an LLM-based extraction pipeline to gather code snippets, generate tasks, and validate correctness. They evaluated 20 LLMs (14 general-purpose, 6 code-based) using FSM-driven validation.

Result: The results show that even the best current LLMs underperform in generating correct CPL-interoperating code, revealing significant gaps in their interoperability capabilities.

Conclusion: CrossPL reveals the inability of current LLMs to reliably generate code for cross-language interoperability, highlighting a critical area in need of further research and development.

Abstract: As large language models (LLMs) become increasingly embedded in software
engineering workflows, a critical capability remains underexplored: generating
correct code that enables cross-programming-language (CPL) interoperability.
This skill is essential for building complex systems that integrate components
written in multiple languages via mechanisms like inter-process communication
(IPC). To bridge this gap, we present CrossPL, the first benchmark designed to
systematically evaluate LLMs' ability to generate CPL-interoperating code.
CrossPL comprises 1,982 tasks centered around IPC, covering six widely-used
programming languages and seven representative CPL techniques. We construct
this benchmark by (i) analyzing 19,169 multi-language GitHub repositories using
156 hand-crafted finite state machines (FSMs), and (ii) developing an LLM-based
pipeline that automatically extracts CPL code snippets, generates task
instructions, and validates functional correctness. We evaluate 14
state-of-the-art general-purpose LLMs and 6 code-oriented LLMs released in the
past three years on CrossPL via FSM-based validation. Results reveal that even
the best-performing models struggle with CPL scenarios, underscoring the need
for more targeted research in this space. Our benchmark and code are available
at: https://anonymous.4open.science/r/crosspl-2814.

</details>


### [11] [The Impact of Fine-tuning Large Language Models on Automated Program Repair](https://arxiv.org/abs/2507.19909)
*Roman Macháček,Anastasiia Grishina,Max Hort,Leon Moonen*

Main category: cs.SE

TL;DR: Parameter-efficient fine-tuning outperforms full fine-tuning for adapting LLMs to Automated Program Repair, avoiding overfitting and achieving better benchmark results with lower computational costs.


<details>
  <summary>Details</summary>
Motivation: Automated Program Repair (APR) is increasingly utilizing Large Language Models (LLMs) for better code correction, but training these models from scratch is computationally expensive. Fine-tuning pre-trained LLMs offers a cost-effective approach to enhance APR capabilities, yet the optimal fine-tuning strategy is unclear.

Method: The study empirically examines the impact of different fine-tuning techniques on LLMs when applied to APR tasks. It evaluates six state-of-the-art code-pretrained LLMs on three well-known APR benchmarks (QuixBugs, Defects4J, HumanEval-Java), testing three training regimens: no fine-tuning, full fine-tuning, and parameter-efficient fine-tuning (LoRA and IA3).

Result: Full fine-tuning often reduces model performance on benchmarks due to overfitting and data distribution discrepancies. In contrast, parameter-efficient fine-tuning approaches limit the number of trainable parameters and yield superior performance results.

Conclusion: Parameter-efficient fine-tuning frameworks are more effective than full fine-tuning for adapting LLMs to APR tasks, mitigating issues such as overfitting and mismatched data distributions.

Abstract: Automated Program Repair (APR) uses various tools and techniques to help
developers achieve functional and error-free code faster. In recent years,
Large Language Models (LLMs) have gained popularity as components in APR tool
chains because of their performance and flexibility. However, training such
models requires a significant amount of resources. Fine-tuning techniques have
been developed to adapt pre-trained LLMs to specific tasks, such as APR, and
enhance their performance at far lower computational costs than training from
scratch. In this study, we empirically investigate the impact of various
fine-tuning techniques on the performance of LLMs used for APR. Our experiments
provide insights into the performance of a selection of state-of-the-art LLMs
pre-trained on code. The evaluation is done on three popular APR benchmarks
(i.e., QuixBugs, Defects4J and HumanEval-Java) and considers six different LLMs
with varying parameter sizes (resp. CodeGen, CodeT5, StarCoder, DeepSeekCoder,
Bloom, and CodeLlama-2). We consider three training regimens: no fine-tuning,
full fine-tuning, and parameter-efficient fine-tuning (PEFT) using LoRA and
IA3. We observe that full fine-tuning techniques decrease the benchmarking
performance of various models due to different data distributions and
overfitting. By using parameter-efficient fine-tuning methods, we restrict
models in the amount of trainable parameters and achieve better results.
  Keywords: large language models, automated program repair,
parameter-efficient fine-tuning, AI4Code, AI4SE, ML4SE.

</details>


### [12] [Prometheus: Unified Knowledge Graphs for Issue Resolution in Multilingual Codebases](https://arxiv.org/abs/2507.19942)
*Zimin Chen,Yue Pan,Siyu Lu,Jiayi Xu,Claire Le Goues,Martin Monperrus,He Ye*

Main category: cs.SE

TL;DR: Prometheus is a multi-agent system that turns codebases into knowledge graphs to resolve real-world, multi-language repository issues. It surpasses previous LM agents by working on seven languages and solving unique issues with cost efficiency, and is available open source.


<details>
  <summary>Details</summary>
Motivation: Existing language model agents for automated issue resolution are restricted to Python-only issues and require pre-constructed benchmark settings, limiting their applicability to real-world, multi-language code repositories. There is a need for systems that can operate on real-world, multi-language projects for broader utility.

Method: The proposed system, Prometheus, is a multi-agent framework that converts an entire code repository into a knowledge graph, encoding files, abstract syntax trees, and natural language into a graph with typed nodes and five kinds of edges. Prometheus uses Neo4j for graph storage and is integrated with the DeepSeek-V3 model, allowing context-aware reasoning for issue resolution across multiple programming languages.

Result: Prometheus resolves 28.67% of issues on SWE-bench Lite and 13.7% on SWE-bench Multilingual, outperforms prior work by solving 10 additional unique issues, and is the first to be effective in seven programming languages. It demonstrates capability on real-world GitHub issues in major repositories, with average API costs per issue.

Conclusion: Prometheus advances automated issue resolution by addressing multi-language and real-world settings, going beyond Python-only or benchmark-limited solutions, and demonstrates broad applicability and cost-effective performance. The system is open-sourced for further research and adoption.

Abstract: Language model (LM) agents, such as SWE-agent and OpenHands, have made
progress toward automated issue resolution. However, existing approaches are
often limited to Python-only issues and rely on pre-constructed containers in
SWE-bench with reproduced issues, restricting their applicability to real-world
and work for multi-language repositories. We present Prometheus, designed to
resolve real-world issues beyond benchmark settings. Prometheus is a
multi-agent system that transforms an entire code repository into a unified
knowledge graph to guide context retrieval for issue resolution. Prometheus
encodes files, abstract syntax trees, and natural language text into a graph of
typed nodes and five general edge types to support multiple programming
languages. Prometheus uses Neo4j for graph persistence, enabling scalable and
structured reasoning over large codebases. Integrated by the DeepSeek-V3 model,
Prometheus resolves 28.67% and 13.7% of issues on SWE-bench Lite and SWE-bench
Multilingual, respectively, with an average API cost of $0.23 and $0.38 per
issue. Prometheus resolves 10 unique issues not addressed by prior work and is
the first to demonstrate effectiveness across seven programming languages.
Moreover, it shows the ability to resolve real-world GitHub issues in the
LangChain and OpenHands repositories. We have open-sourced Prometheus at:
https://github.com/Pantheon-temple/Prometheus

</details>


### [13] [PDLogger: Automated Logging Framework for Practical Software Development](https://arxiv.org/abs/2507.19951)
*Shengcheng Duan,Yihua Xu,Sheng Zhang,Shen Wang,Yue Duan*

Main category: cs.SE

TL;DR: PDLogger is an end-to-end automated log generation technique that outperforms prior methods in precision, accuracy, and message quality for real-world, multi-log scenarios in software, leveraging LLMs and advanced code analysis.


<details>
  <summary>Details</summary>
Motivation: Developers often struggle with effective software logging—deciding where and how to add logs. Existing automated techniques only address isolated log parameters and create incomplete or superficial logs, without considering real-world multi-log scenarios or deep code dependencies.

Method: PDLogger is introduced as a three-phase, end-to-end log generation technique. First, it predicts log positions using structured prompts to a large language model (LLM). Second, it generates full log statements using program slicing for context and an expanded variable extractor, with the LLM building the complete log (position, level, message, variables). Third, it refines logs by correcting levels and deduplicating contexts to remove redundancy and false positives.

Result: PDLogger was evaluated on 3,113 log statements from two popular Java projects, significantly outperforming previous methods in log position precision (139.0% improvement), F1 score (69.2% improvement), log level accuracy (82.3% improvement), variable precision (131.8% improvement), and message quality (BERTScore; 65.7% improvement). Its robustness is verified across different LLMs.

Conclusion: PDLogger provides a robust, generalizable, and significantly more effective approach for automatic, real-world logging in software, improving all major metrics and offering open-source implementation for further research and use.

Abstract: Logging is indispensable for maintaining the reliability and diagnosability
of modern software, yet developers still struggle to decide where and how to
log effectively. Existing automated logging techniques focus on isolated
sub-tasks - predicting a single log position, level, or message - and therefore
cannot produce complete, high-quality log statements that reflect real-world
practice in which multiple logs often appear inside one method. They also
neglect deeper semantic dependencies among methods and consider only a narrow
set of candidate variables, leading to superficial or incomplete logs. In this
paper, we present PDLogger, the first end-to-end log generation technique
expressly designed for practical, multi-log scenarios. PDLogger operates in
three phases. (1) Log position prediction: block-type-aware structured prompts
guide a large language model (LLM) to suggest candidate positions across all
control-flow blocks of a method. (2) Log generation: backward program slicing
supplies precise inter-procedural control and data-dependency context, while an
expanded variable extractor captures both member and external function
expressions; the enriched prompt enables the LLM to emit a full log statement
(position, level, message, variables). (3) Log refinement: level correction and
context-sensitive deduplication prune false positives and redundant logs. We
evaluate PDLogger on 3,113 log statements drawn from two widely used Java
projects. Compared with the strongest prior systems, PDLogger improves
log-position precision by 139.0 percent, F1 by 69.2 percent, level accuracy by
82.3 percent, variable precision by 131.8 percent, and message quality
(BERTScore) by 65.7 percent. The framework consistently performs well with
different mainstream LLMs, demonstrating robustness and generality. PDLogger's
implementation is available as open source to foster future research and
adoption.

</details>


### [14] [The Effect of Pointer Analysis on Semantic Conflict Detection](https://arxiv.org/abs/2507.20081)
*Matheus Barbosa,Paulo Borba,Rodrigo Bonifácio,Victor Lira,Galileu Santos*

Main category: cs.SE

TL;DR: Adding pointer analysis to semantic conflict detection in merge tools reduces some errors but introduces serious new ones. The study finds that neither approach alone is ideal, and future work should explore hybrid analyses to balance accuracy and practicality.


<details>
  <summary>Details</summary>
Motivation: Current merge tools fail to detect semantic conflicts, which may result from textually integrated but semantically incompatible changes made by different developers. Existing static analyses for detecting such conflicts have high false positive rates. The paper aims to explore whether incorporating pointer analysis can reduce these false positives.

Method: The authors conduct an empirical study by implementing semantic conflict detection analysis both with and without pointer analysis. They run these implementations on two datasets, comparing differences in the results, accuracy, and computational performance, specifically measuring false positives, false negatives, recall, and F1-score.

Result: Incorporating pointer analysis into semantic conflict detection resulted in fewer timeouts and false positives. However, it also led to a significant increase in false negatives, which caused a notable drop in recall and F1-score, potentially making the tool less reliable for practical use.

Conclusion: While pointer analysis helps reduce some issues like timeouts and false positives in static semantic conflict detection, it introduces severe drawbacks, including more false negatives and reduced recall/F1-score. The paper suggests that hybrid approaches combining the strengths of both methods may offer a better solution.

Abstract: Current merge tools don't detect semantic conflicts, which occur when changes
from different developers are textually integrated but semantically interfere
with each other. Although researchers have proposed static analyses for
detecting semantic conflicts, these analyses suffer from significant false
positive rates. To understand whether such false positives could be reduced by
using pointer analysis in the implementation of semantic conflict static
analyses, we conduct an empirical study. We implement the same analysis with
and without pointer analysis, run them on two datasets, observe how often they
differ, and compare their accuracy and computational performance. Although
pointer analysis is known to improve precision in static analysis, we find that
its effect on semantic conflict detection can be drastic: we observe a
significant reduction in timeouts and false positives, but also a significant
increase in false negatives, with prohibitive drops in recall and F1-score.
These results suggest that, in the context of semantic conflict detection, we
should explore hybrid analysis techniques, combining aspects of both
implementations we compare in our study.

</details>


### [15] [From First Use to Final Commit: Studying the Evolution of Multi-CI Service Adoption](https://arxiv.org/abs/2507.20095)
*Nitika Chopra,Taher A. Ghaleb*

Main category: cs.SE

TL;DR: The paper studies the adoption of multiple CI services in nearly 19,000 GitHub Java projects over 16 years, finding that using and migrating between CI services is common and motivates future research and tools on supporting these transitions.


<details>
  <summary>Details</summary>
Motivation: CI services are crucial for modern software development, but prior research mainly focuses on individual services. There is limited understanding of how projects use, switch, or maintain multiple CI services over time. The authors want to clarify these patterns to help developers and researchers.

Method: A large-scale historical analysis of 18,924 Java projects on GitHub, from 2008 to 2024, was conducted. The study tracked the adoption of eight popular CI services and assessed how often services are used together, replaced, or impact project maintenance.

Result: Nearly 20% of projects co-adopted or migrated between different CI services, indicating that using multiple CI services is common. Maintenance activity varies across services, and migration between CI systems is a significant trend.

Conclusion: Multi-CI service adoption is widespread, posing challenges and opportunities. There is a need for improved tools and strategies for service selection, coordination, and migration. This research provides foundational insights for further studies.

Abstract: Continuous Integration (CI) services, such as GitHub Actions and Travis CI,
are widely adopted in open-source development to automate testing and
deployment. Though existing research often examines individual services in
isolation, it remains unclear how projects adopt and transition between
multiple services over time. To understand how CI adoption is evolving across
services, we present a preliminary study analyzing the historical CI adoption
of 18,924 Java projects hosted on GitHub between January 2008 and December
2024, adopting at least one of eight CI services, namely Travis CI, AppVeyor,
CircleCI, Azure Pipelines, GitHub Actions, Bitbucket, GitLab CI, and Cirrus CI.
Specifically, we investigate: (1) how frequently CI services are co-adopted or
replaced, and (2) how maintenance activity varies across different services.
Our analysis shows that the use of multiple CI services within the same project
is a recurring pattern observed in nearly one in five projects, often
reflecting migration across CI services. Our study is among the first to
examine multi-CI adoption in practice, offering new insights for future
research and highlighting the need for strategies and tools to support service
selection, coordination, and migration in evolving CI environments.

</details>


### [16] [Learning to Align Human Code Preferences](https://arxiv.org/abs/2507.20109)
*Xin Yin,Chao Ni,Liushan Chen,Xiaohu Yang*

Main category: cs.SE

TL;DR: This paper introduces Adaptive Preference Optimization (APO), a new adaptive training strategy for code-preference alignment in LLMs. APO dynamically tailors model response amplification and suppression during training, providing superior or comparable performance to traditional strategies (SFT or SFT+DPO) across multiple code preference tasks. The study offers theory and guidance for choosing training approaches based on task characteristics.


<details>
  <summary>Details</summary>
Motivation: Existing training strategies for aligning large language models (LLMs) with human code preferences—specifically Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO)—lack clarity on which approach is best suited for different coding scenarios. There is a need to systematically determine optimal approaches for maximizing model performance in scenarios with varying degrees of solution objectivity.

Method: The paper conducts both theoretical analysis and comprehensive empirical studies. It introduces a new dynamic training strategy called Adaptive Preference Optimization (APO), which adaptively amplifies, suppresses, and encourages exploration of model responses during training. The approach is validated through extensive experiments across six code preference tasks.

Result: The experiments show that SFT is best when there are objectively verifiable optimal solutions, while a combination of SFT and DPO helps in open-ended scenarios without clear optimal answers. APO, the proposed method, consistently matches or outperforms existing strategies for all tested code preference tasks.

Conclusion: APO provides a dynamic and adaptive approach to aligning LLMs with code preferences, outperforming traditional SFT and S&D (SFT + DPO) strategies in various scenarios. The paper offers theoretical and practical insights for choosing the best training method according to the nature of the coding task.

Abstract: Large Language Models (LLMs) have demonstrated remarkable potential in
automating software development tasks. While recent advances leverage
Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to align
models with human preferences, the optimal training strategy remains unclear
across diverse code preference scenarios. This paper systematically
investigates the roles of SFT and DPO in aligning LLMs with different code
preferences. Through both theoretical analysis and empirical observation, we
hypothesize that SFT excels in scenarios with objectively verifiable optimal
solutions, while applying SFT followed by DPO (S&D) enables models to explore
superior solutions in scenarios without objectively verifiable optimal
solutions. Based on the analysis and experimental evidence, we propose Adaptive
Preference Optimization (APO), a dynamic integration approach that adaptively
amplifies preferred responses, suppresses dispreferred ones, and encourages
exploration of potentially superior solutions during training. Extensive
experiments across six representative code preference tasks validate our
theoretical hypotheses and demonstrate that APO consistently matches or
surpasses the performance of existing SFT and S&D strategies. Our work provides
both theoretical foundations and practical guidance for selecting appropriate
training strategies in different code preference alignment scenarios.

</details>


### [17] [From Prompt to Pipeline: Large Language Models for Scientific Workflow Development in Bioinformatics](https://arxiv.org/abs/2507.20122)
*Khairul Alam,Banani Roy*

Main category: cs.SE

TL;DR: LLMs like GPT-4o, Gemini 2.5 Flash, and DeepSeek-V3 can help non-programmers build bioinformatics workflows on systems like Galaxy and Nextflow. The best results come from pairing the right LLM with the right system and using smart prompting techniques, making workflow creation more accessible and reproducible.


<details>
  <summary>Details</summary>
Motivation: The growing complexity of bioinformatics data analysis requires scalable and user-friendly workflow systems, but domain experts without programming skills struggle to create and understand workflows. The paper aims to explore whether recent large language models (LLMs) can assist in generating accurate, usable workflows for these systems.

Method: The study evaluates three advanced LLMs—GPT-4o, Gemini 2.5 Flash, and DeepSeek-V3—across a range of representative bioinformatics tasks (e.g., SNP analysis, RNA-seq, DNA methylation, data retrieval), using both graphical (Galaxy) and script-based (Nextflow) workflow platforms. The experiments also assess the effect of different prompting strategies. Expert reviewers compare LLM-generated workflows to community standards from the Galaxy Training Network and nf-core.

Result: Gemini 2.5 Flash performs best in generating workflows for Galaxy, while DeepSeek-V3 excels for Nextflow. Prompting strategies, particularly role-based and chain-of-thought prompts, significantly improve performance. GPT-4o works better with structured inputs, and DeepSeek-V3 produces detailed but sometimes verbose workflows.

Conclusion: Modern LLMs can significantly lower the barrier for developing bioinformatics workflows, improving reproducibility and accessibility. Their utility is maximized when paired with effective prompt engineering, enabling domain experts to participate in workflow creation without advanced programming knowledge.

Abstract: The increasing complexity of bioinformatics data analysis has made Scientific
Workflow Systems (SWSs) like Galaxy and Nextflow essential for enabling
scalable, reproducible, and automated workflows. However, creating and
understanding these workflows remains challenging, particularly for domain
experts without programming expertise. This study investigates whether modern
Large Language Models (LLMs), GPT-4o, Gemini 2.5 Flash, and DeepSeek-V3, can
support the generation of accurate, complete, and usable bioinformatics
workflows, and examines which prompting strategies most effectively guide this
process. We evaluate these models using diverse tasks such as SNP analysis,
RNA-seq, DNA methylation, and data retrieval, spanning both graphical (Galaxy)
and script-based (Nextflow) platforms. Expert reviewers assess the generated
workflows against community-curated baselines from the Galaxy Training Network
and nf-core repositories. The results show that Gemini 2.5 Flash excels in
generating Galaxy workflows, while DeepSeek-V3 performs strongly in Nextflow.
Prompting strategies significantly impact quality, with role-based and
chain-of-thought prompts improving completeness and correctness. While GPT-4o
benefits from structured inputs, DeepSeek-V3 offers rich technical detail,
albeit with some verbosity. Overall, the findings highlight the potential of
LLMs to lower the barrier for workflow development, improve reproducibility,
and democratize access to computational tools in bioinformatics, especially
when combined with thoughtful prompt engineering.

</details>


### [18] [Relating System Safety and Machine Learnt Model Performance](https://arxiv.org/abs/2507.20135)
*Ganesh Pai*

Main category: cs.SE

TL;DR: The paper proposes a systematic method to derive machine learning model performance requirements and metrics directly from system safety objectives in aeronautical applications, using an emergency braking system as a case study.


<details>
  <summary>Details</summary>
Motivation: There is currently a gap between high-level system safety objectives in aeronautical applications and the quantitative metrics used to measure the performance of machine-learned models, making it unclear how model-level requirements align with overall safety goals.

Method: The paper uses an example of an aircraft emergency braking system that includes a machine-learned component for object detection and alerting. It abstracts the required behavior of this component and proposes a method to derive minimum safety-related performance requirements, corresponding metrics, and their target values to ensure the machine-learned component meets the system's safety objectives.

Result: The proposed method provides a systematic approach to tracing system safety objectives down to the selection of model performance metrics and their minimum acceptable values. This is demonstrated through a case study and supported by rationale, a clarification of assumptions, constraints on applicability, and discussion of verification implications.

Conclusion: A method is provided to connect safety objectives from system-level safety assessment to explicit model performance requirements for machine-learned components, facilitating more rigorous, safety-conscious integration in safety-critical contexts like aviation.

Abstract: The prediction quality of machine learnt models and the functionality they
ultimately enable (e.g., object detection), is typically evaluated using a
variety of quantitative metrics that are specified in the associated model
performance requirements. When integrating such models into aeronautical
applications, a top-down safety assessment process must influence both the
model performance metrics selected, and their acceptable range of values.
Often, however, the relationship of system safety objectives to model
performance requirements and the associated metrics is unclear. Using an
example of an aircraft emergency braking system containing a machine learnt
component (MLC) responsible for object detection and alerting, this paper first
describes a simple abstraction of the required MLC behavior. Then, based on
that abstraction, an initial method is given to derive the minimum
safety-related performance requirements, the associated metrics, and their
targets for the both MLC and its underlying deep neural network, such that they
meet the quantitative safety objectives obtained from the safety assessment
process. We give rationale as to why the proposed method should be considered
valid, also clarifying the assumptions made, the constraints on applicability,
and the implications for verification.

</details>


### [19] [Strategic Motivators for Ethical AI System Development: An Empirical and Holistic Model](https://arxiv.org/abs/2507.20218)
*Muhammad Azeem Akbar,Arif Ali Khan,Saima Rafi,Damian Kedziora,Sami Hyrynsalmi*

Main category: cs.SE

TL;DR: The paper identifies and ranks crucial motivators for ethical AI development, finding Human Resource and Coordination as the most influential, and recommends organizations incorporate these motivators into their governance and frameworks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to determine and prioritize the key drivers for the ethical development of Artificial Intelligence, recognizing the need to prevent unintended consequences as AI transforms industries and society.

Method: The study conducted a Multivocal Literature Review (MLR) and a questionnaire survey to gather data, then used Interpretive Structure Modeling (ISM) to analyze relationships between motivator categories, MICMAC analysis to assess their driving and dependence power, and Fuzzy TOPSIS to rank their importance.

Result: Twenty key motivators were identified and grouped into eight categories. 'Human Resource' and 'Coordination' are most influential over others. Several categories (Human Resource, Coordination, Stakeholders, Strategy & Matrices) have high driving but low dependence power. Top motivators include promoting team diversity, governance bodies, oversight leaders, and data privacy.

Conclusion: Organizations should integrate the identified key motivators into their strategies, policies, and development frameworks to support ethical AI adoption.

Abstract: Artificial Intelligence (AI) presents transformative opportunities for
industries and society, but its responsible development is essential to prevent
unintended consequences. Ethically sound AI systems demand strategic planning,
strong governance, and an understanding of the key drivers that promote
responsible practices. This study aims to identify and prioritize the
motivators that drive the ethical development of AI systems. A Multivocal
Literature Review (MLR) and a questionnaire-based survey were conducted to
capture current practices in ethical AI. We applied Interpretive Structure
Modeling (ISM) to explore the relationships between motivator categories,
followed by MICMAC analysis to classify them by their driving and dependence
power. Fuzzy TOPSIS was used to rank these motivators by importance. Twenty key
motivators were identified and grouped into eight categories: Human Resource,
Knowledge Integration, Coordination, Project Administration, Standards,
Technology Factor, Stakeholders, and Strategy & Matrices. ISM results showed
that 'Human Resource' and 'Coordination' heavily influence other factors.
MICMAC analysis placed categories like Human Resource (CA1), Coordination
(CA3), Stakeholders (CA7), and Strategy & Matrices (CA8) in the independent
cluster, indicating high driving but low dependence power. Fuzzy TOPSIS ranked
motivators such as promoting team diversity, establishing AI governance bodies,
appointing oversight leaders, and ensuring data privacy as most critical. To
support ethical AI adoption, organizations should align their strategies with
these motivators and integrate them into their policies, governance models, and
development frameworks.

</details>


### [20] [Beyond Binary Moderation: Identifying Fine-Grained Sexist and Misogynistic Behavior on GitHub with Large Language Models](https://arxiv.org/abs/2507.20358)
*Tanni Dev,Sayma Sultana,Amiangshu Bosu*

Main category: cs.SE

TL;DR: This paper presents a new approach to detecting nuanced sexist and misogynistic comments in technical communities like GitHub using a fine-grained, multi-class classification framework based on instruction-tuned LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing moderation tools are insufficient for capturing subtle and context-dependent sexist behaviors, leading to exclusion and attrition among minority developers. More nuanced detection methods are needed to better protect online technical communities.

Method: The researchers trained and iteratively refined prompts for a large language model over 20 iterations, tested on 1,440 GitHub comments categorized into 12 types of sexism/misogyny. Performance was evaluated using metrics like F1-score, precision, recall, and MCC.

Result: The best model (GPT-4o with Prompt 19) achieved an MCC of 0.501, outperforming baselines with low false positive rates. However, challenges remain in accurately detecting context-dependent, nuanced sexism.

Conclusion: Instruction-tuned LLMs, when paired with well-crafted prompts and structured outputs, can significantly improve the accuracy and clarity of detecting sexism and misogyny, supporting more effective moderation.

Abstract: Background: Sexist and misogynistic behavior significantly hinders inclusion
in technical communities like GitHub, causing developers, especially
minorities, to leave due to subtle biases and microaggressions. Current
moderation tools primarily rely on keyword filtering or binary classifiers,
limiting their ability to detect nuanced harm effectively.
  Aims: This study introduces a fine-grained, multi-class classification
framework that leverages instruction-tuned Large Language Models (LLMs) to
identify twelve distinct categories of sexist and misogynistic comments on
GitHub.
  Method: We utilized an instruction-tuned LLM-based framework with systematic
prompt refinement across 20 iterations, evaluated on 1,440 labeled GitHub
comments across twelve sexism/misogyny categories. Model performances were
rigorously compared using precision, recall, F1-score, and the Matthews
Correlation Coefficient (MCC).
  Results: Our optimized approach (GPT-4o with Prompt 19) achieved an MCC of
0.501, significantly outperforming baseline approaches. While this model had
low false positives, it struggled to interpret nuanced, context-dependent
sexism and misogyny reliably.
  Conclusion: Well-designed prompts with clear definitions and structured
outputs significantly improve the accuracy and interpretability of sexism
detection, enabling precise and practical moderation on developer platforms
like GitHub.

</details>


### [21] [CIgrate: Automating CI Service Migration with Large Language Models](https://arxiv.org/abs/2507.20402)
*Md Nazmul Hossain,Taher A. Ghaleb*

Main category: cs.SE

TL;DR: This paper introduces CIgrate, a framework leveraging large language models to improve automation and accuracy in migrating CI configurations between services, and compares its performance and usability to existing rule-based methods.


<details>
  <summary>Details</summary>
Motivation: Continuous Integration (CI) service migration is often needed in open-source development due to evolving requirements or limitations, but current rule-based automation methods like CIMig are not accurate enough. Manual migration is difficult and error-prone, motivating the search for better automated solutions.

Method: The paper proposes CIgrate, a framework using Large Language Models (LLMs) to automatically migrate CI configurations. The study compares CIgrate to the rule-based baseline CIMig using zero-shot/few-shot prompting and fine-tuning techniques, and also collects developer feedback on output quality and usability.

Result: The study is designed to evaluate whether LLMs can improve the accuracy and usability of automated CI migration compared to existing rule-based systems. It aims to produce and analyze comparative performance data and user feedback.

Conclusion: The expected findings are that LLMs can provide a more accurate and generalizable solution for CI migration, offering the first LLM-powered approach and deeper insights into supporting configuration evolution with AI.

Abstract: Continuous Integration (CI) configurations often need to be migrated between
services (e.g., Travis CI to GitHub Actions) as projects evolve, due to changes
in service capabilities, usage limits, or service deprecation. Previous studies
reported that migration across CI services is a recurring need in open-source
development. However, manual migration can be time-consuming and error-prone.
The state-of-the-art approach, CIMig, addresses this challenge by analyzing
past migration examples to create service-specific rules and produce equivalent
configurations across CI services. However, its relatively low accuracy raises
concerns about the overall feasibility of automated CI migration using
rule-based techniques alone. Meanwhile, Large Language Models (LLMs) have
demonstrated strong capabilities in code generation and transformation tasks,
suggesting potential to improve the automation, usability, and generalizability
of CI configuration migration. This registered report presents a study in which
we aim to assess whether CI migration can be improved using LLMs. To this end,
we propose CIgrate, an LLM-based framework for automatically migrating CI
configurations. We plan to evaluate the performance of CIgrate compared to
CIMig as a baseline, in different setups (a) zero-shot/few-shot prompting of
LLMs for configuration migration and (b) fine-tuning an LLM on a dataset of
already established CI service migrations. We will also seek developer feedback
on the quality and usability of the generated configurations. We formulate
research questions focusing on the accuracy of LLM-generated migrations versus
ground truth and the output of CIMig. The expected contributions include the
first LLM-powered approach for CI service migration, a comparative evaluation
of its effectiveness compared to rule-based approaches, and insight into
leveraging LLMs to support software configuration evolution.

</details>


### [22] [Testing Is Not Boring: Characterizing Challenge in Software Testing Tasks](https://arxiv.org/abs/2507.20407)
*Davi Gama Hardman,Cesar França,Brody Stuart-Verner,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: Software testing requires creativity and adaptability, not just repetitive skills. Challenging tasks motivate professionals, while lack of challenge or excessive demands cause frustration. Striking the right balance in task complexity is key to sustaining engagement in testing roles.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the misconception that software testing is a repetitive and low-skill activity by highlighting the creativity, problem-solving, and adaptability involved in testing work. The authors aim to better understand the experiences of software testing professionals, especially regarding challenging tasks.

Method: The authors conducted a study involving software testing professionals to explore the nature of challenging tasks in their work and their effects.

Result: The study reveals that tasks involving creativity, continuous learning, and time pressure are generally perceived as motivating and rewarding by testing professionals. However, tasks lacking challenge or with overwhelming demands can result in frustration and disengagement.

Conclusion: The paper concludes that balancing task complexity is crucial in maintaining motivation among testing professionals and upholds software testing as a dynamic and intellectually engaging discipline.

Abstract: As software systems continue to grow in complexity, testing has become a
fundamental part of ensuring the quality and reliability of software products.
Yet, software testing is still often perceived, both in industry and academia,
as a repetitive, low-skill activity. This perception fails to recognize the
creativity, problem-solving, and adaptability required in testing work. Tasks
such as designing complex test cases, automating testing processes, and
handling shifting requirements illustrate the challenges testing professionals
regularly face. To better understand these experiences, we conducted a study
with software testing professionals to explore the nature of challenging tasks
in software testing and how they affect these professionals. Our findings show
that tasks involving creativity, ongoing learning, and time pressure are often
seen as motivating and rewarding. On the other hand, a lack of challenge or
overwhelming demands can lead to frustration and disengagement. These findings
demonstrate the importance of balancing task complexity to sustain motivation
and present software testing as a dynamic and intellectually engaging field.

</details>


### [23] [When Prompts Go Wrong: Evaluating Code Model Robustness to Ambiguous, Contradictory, and Incomplete Task Descriptions](https://arxiv.org/abs/2507.20439)
*Maya Larbi,Amal Akli,Mike Papadakis,Rihab Bouyousfi,Maxime Cordy,Federica Sarro,Yves Le Traon*

Main category: cs.SE

TL;DR: Even top-performing LLMs for code generation struggle with ambiguous, incomplete, or contradictory task descriptions—common in real-world scenarios. The paper shows that current models are fragile under such conditions, calls for improvement in model robustness, and suggests changes to evaluation frameworks to better reflect actual usage.


<details>
  <summary>Details</summary>
Motivation: Current code generation benchmarks use idealized, clear, and unambiguous task descriptions, whereas real-world developer instructions are often unclear, incomplete, or contradictory. The motivation is to understand how state-of-the-art code generation models perform under realistic, messy conditions and to identify their robustness to ambiguous requirements.

Method: The authors systematically altered (mutated) task descriptions in established code generation benchmarks (HumanEval and MBPP) to introduce ambiguity, incompleteness, and contradictions, simulating real-world developer instructions. They evaluated several large language models (LLMs) of different sizes and architectures by measuring their performance and analyzing error types on these flawed benchmarks.

Result: The study found that LLMs experience significant performance degradation when task descriptions are even slightly unclear. Contradictory descriptions in particular lead to many logical errors. Although larger models are somewhat more resilient to these flaws, none are immune. Error patterns correlate with the level of description clarity.

Conclusion: There is a critical need to improve LLM robustness to real-world, imperfect task descriptions. Advances in model training, better evaluation benchmarks, and considerations for deployment in software development should focus on handling the inherent ambiguities of natural user instructions.

Abstract: Large Language Models (LLMs) have demonstrated impressive performance in code
generation tasks under idealized conditions, where task descriptions are clear
and precise. However, in practice, task descriptions frequently exhibit
ambiguity, incompleteness, or internal contradictions. In this paper, we
present the first empirical study examining the robustness of state-of-the-art
code generation models when faced with such unclear task descriptions. We
extend the HumanEval and MBPP benchmarks by systematically introducing
realistic task descriptions flaws through guided mutation strategies, producing
a dataset that mirrors the messiness of informal developer instructions. We
evaluate multiple LLMs of varying sizes and architectures, analyzing their
functional correctness and failure modes across task descriptions categories.
Our findings reveal that even minor imperfections in task description phrasing
can cause significant performance degradation, with contradictory task
descriptions resulting in numerous logical errors. Moreover, while larger
models tend to be more resilient than smaller variants, they are not immune to
the challenges posed by unclear requirements. We further analyze semantic error
patterns and identify correlations between description clarity, model behavior,
and error types. Our results underscore the critical need for developing LLMs
that are not only powerful but also robust to the imperfections inherent in
natural user tasks, highlighting important considerations for improving model
training strategies, designing more realistic evaluation benchmarks, and
ensuring reliable deployment in practical software development environments.

</details>


### [24] [Distinguishing Quantum Software Bugs from Hardware Noise: A Statistical Approach](https://arxiv.org/abs/2507.20475)
*Ahmik Virani,Devraj,Anirudh Suresh,Lei Zhang,M V Panduranga Rao*

Main category: cs.SE

TL;DR: The paper introduces a statistical method to help developers tell apart quantum software bugs from hardware noise in NISQ computers, and demonstrates its effectiveness through experiments on key quantum algorithms.


<details>
  <summary>Details</summary>
Motivation: In the NISQ era, quantum computers are prone to both software bugs and hardware-induced noise, making it difficult to identify the root cause of unexpected behavior in quantum programs. Classical debugging approaches are inadequate due to quantum computation's probabilistic nature and noise.

Method: The authors propose a statistical approach using probabilistic metrics to differentiate between quantum software bugs and hardware-induced noise. Their methodology is empirically evaluated on well-known quantum algorithms like Grover's, Deutsch-Jozsa, and Simon's algorithms.

Result: The experimental results show that the proposed statistical approach is effective and practically applicable in distinguishing between software bugs and hardware noise in quantum programs.

Conclusion: This statistical methodology offers quantum software developers a reliable tool for analyzing and classifying unexpected behaviors, helping to improve debugging in noisy intermediate-scale quantum computers.

Abstract: Quantum computing in the Noisy Intermediate-Scale Quantum (NISQ) era presents
significant challenges in differentiating quantum software bugs from hardware
noise. Traditional debugging techniques from classical software engineering
cannot directly resolve this issue due to the inherently stochastic nature of
quantum computation mixed with noises from NISQ computers. To address this gap,
we propose a statistical approach leveraging probabilistic metrics to
differentiate between quantum software bugs and hardware noise. We evaluate our
methodology empirically using well-known quantum algorithms, including Grover's
algorithm, Deutsch-Jozsa algorithm, and Simon's algorithm. Experimental results
demonstrate the efficacy and practical applicability of our approach, providing
quantum software developers with a reliable analytical tool to identify and
classify unexpected behavior in quantum programs.

</details>


### [25] [VDGraph: A Graph-Theoretic Approach to Unlock Insights from SBOM and SCA Data](https://arxiv.org/abs/2507.20502)
*Howell Xia,Jonah Gluck,Sevval Simsek,David Sastre Medina,David Starobinski*

Main category: cs.SE

TL;DR: This paper presents VDGraph, a knowledge graph framework that unifies SBOM and SCA data to analyze vulnerabilities in software supply chains. By implementing and applying VDGraph to real Java projects, the authors reveal that most severe vulnerabilities reside deep in dependency trees and demonstrate how the tool improves risk analysis and visibility.


<details>
  <summary>Details</summary>
Motivation: Modern software supply chains are highly complex, making it difficult to manage component dependencies and identify vulnerabilities effectively. While tools like SBOM and SCA exist, their integration is limited, and there is no unified way to view the relationships between dependencies and vulnerabilities.

Method: The paper introduces VDGraph, a knowledge graph-based methodology that integrates vulnerability (from SCA) and dependency (from SBOM) data into a comprehensive graph. The authors provide a formal analysis of VDGraph, implement a proof-of-concept using CycloneDX Maven plugin and OSV-Scanner, and apply it to 21 Java projects to evaluate its effectiveness.

Result: The application of VDGraph enabled the discovery of concentrated risk points—vulnerable components with high severity reachable through multiple paths. It was also found that vulnerabilities mainly occur at three or more levels of dependency depth, while direct and secondary dependencies are generally more secure.

Conclusion: VDGraph offers a graph-based approach that enhances visibility into vulnerability propagation through complex software dependencies. The proof-of-concept, combining open source tools and graph technology, demonstrates scalability and potential for automated real-world analysis.

Abstract: The high complexity of modern software supply chains necessitates tools such
as Software Bill of Materials (SBOMs) to manage component dependencies, and
Software Composition Analysis (SCA) tools to identify vulnerabilities. While
there exists limited integration between SBOMs and SCA tools, a unified view of
complex dependency-vulnerability relationships remains elusive. In this paper,
we introduce VDGraph, a novel knowledge graph-based methodology for integrating
vulnerability and dependency data into a holistic view. VDGraph consolidates
SBOM and SCA outputs into a graph representation of software projects'
dependencies and vulnerabilities. We provide a formal description and analysis
of the theoretical properties of VDGraph and present solutions to manage
possible conflicts between the SBOM and SCA data. We further introduce and
evaluate a practical, proof-of-concept implementation of VDGraph using two
popular SBOM and SCA tools, namely CycloneDX Maven plugin and Google's
OSV-Scanner. We apply VDGraph on 21 popular Java projects. Through the
formulation of appropriate queries on the graphs, we uncover the existence of
concentrated risk points (i.e., vulnerable components of high severity
reachable through numerous dependency paths). We further show that
vulnerabilities predominantly emerge at a depth of three dependency levels or
higher, indicating that direct or secondary dependencies exhibit lower
vulnerability density and tend to be more secure. Thus, VDGraph contributes a
graph-theoretic methodology that improves visibility into how vulnerabilities
propagate through complex, transitive dependencies. Moreover, our
implementation, which combines open SBOM and SCA standards with Neo4j, lays a
foundation for scalable and automated analysis across real-world projects.

</details>


### [26] [GeoJSEval: An Automated Evaluation Framework for Large Language Models on JavaScript-Based Geospatial Computation and Visualization Code Generation](https://arxiv.org/abs/2507.20553)
*Guanyu Chen,Haoyue Jiao,Shuyang Hou,Ziqi Liu,Lutong Xie,Shaowen Wu,Huayi Wu,Xuefeng Guan,Zhipeng Gui*

Main category: cs.SE

TL;DR: The paper introduces GeoJSEval, the first automatic multimodal evaluation framework for LLM-generated geospatial JavaScript code. Covering hundreds of tasks and test cases across key libraries and data types, GeoJSEval enables detailed, robust measurement of model performance. The evaluation of 18 LLMs reveals key strengths and weaknesses, providing a standardized and extensible tool for advancing geospatial code generation research and applications.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the increasing use of large language models (LLMs) for code generation in geospatial tasks, particularly in JavaScript environments, where orchestrating multiple libraries and handling complex geospatial data poses unique challenges. There is an urgent need for systematic and robust evaluation methods for the performance of LLMs in these contexts.

Method: To address this, the authors propose GeoJSEval, a multimodal, function-level automatic evaluation framework that consists of a standardized test suite (GeoJSEval-Bench), a code submission engine, and an evaluation module. The framework provides 432 function-level tasks and 2,071 structured test cases across five major JavaScript geospatial libraries and 25 geospatial data types. It enables multidimensional evaluation along several metrics including accuracy, output stability, execution efficiency, resource consumption, and error types, also incorporating boundary testing for enhanced robustness.

Result: Using GeoJSEval, the authors conducted a comprehensive analysis of 18 state-of-the-art LLMs. The evaluation revealed significant performance differences and exposed bottlenecks in spatial semantic understanding, code reliability, and function invocation accuracy in generated geospatial code.

Conclusion: GeoJSEval offers a standardized, extensible, and practical toolkit for assessing and improving LLMs in geospatial code generation. It provides a crucial foundational methodology and resource for future research and real-world applications in this domain.

Abstract: With the widespread adoption of large language models (LLMs) in code
generation tasks, geospatial code generation has emerged as a critical frontier
in the integration of artificial intelligence and geoscientific analysis. This
trend underscores the urgent need for systematic evaluation methodologies to
assess LLMs generation capabilities in geospatial contexts. In particular,
geospatial computation and visualization tasks in JavaScript environments rely
heavily on orchestrating diverse frontend libraries and ecosystems, placing
elevated demands on a model's semantic understanding and code synthesis
abilities. To address this challenge, we propose GeoJSEval--the first
multimodal, function-level automatic evaluation framework for LLMs in
JavaScript-based geospatial code generation. GeoJSEval comprises three core
components: a standardized test suite (GeoJSEval-Bench), a code submission
engine, and an evaluation module. It includes 432 function-level tasks and
2,071 structured test cases spanning five widely used JavaScript geospatial
libraries and 25 mainstream geospatial data types. GeoJSEval enables
multidimensional quantitative evaluation across metrics such as accuracy,
output stability, execution efficiency, resource consumption, and error type
distribution, and integrates boundary testing mechanisms to enhance robustness
and coverage. We conduct a comprehensive evaluation of 18 state-of-the-art LLMs
using GeoJSEval, revealing significant performance disparities and bottlenecks
in spatial semantic understanding, code reliability, and function invocation
accuracy. GeoJSEval provides a foundational methodology, evaluation resource,
and practical toolkit for the standardized assessment and optimization of
geospatial code generation models, with strong extensibility and applicability
in real-world scenarios.

</details>


### [27] [Intention-Driven Generation of Project-Specific Test Cases](https://arxiv.org/abs/2507.20619)
*Binhang Qi,Yun Lin,Xinyi Weng,Yuhuan Huang,Chenyan Liu,Hailong Sun,Jin Song Dong*

Main category: cs.SE

TL;DR: Automated test generation methods focused only on code coverage are insufficient for real-world usage. IntentionTest leverages structured validation intentions and domain-specific knowledge to guide test generation, producing significantly more correct and usable tests than state-of-the-art methods, as validated on thousands of open-source project cases.


<details>
  <summary>Details</summary>
Motivation: Existing automated test generation approaches mostly focus on code coverage or translating code directly into tests. However, these approaches overlook developers' specific validation intentions and project-specific knowledge, which are crucial for creating practical and useful tests that can pass code review and be integrated into real software projects.

Method: The paper proposes 'IntentionTest', a technique that generates project-specific test cases using structured descriptions of validation intentions. It retrieves relevant existing tests from the project to guide the new test generation. The approach frames test generation as an editing task on the retrieved test code, aligning it with the specified validation intention, and aims to produce executable and semantically correct tests including both setup (prefix) and assertion (oracle). Its effectiveness is evaluated against state-of-the-art baselines using 4,146 test cases from 13 open-source projects.

Result: IntentionTest outperformed existing methods, specifically generating significantly more semantically correct tests. It improved mutation scores by 39.03% and coverage overlap with ground-truth tests by 40.14% compared to the ChatTester baseline, as well as generating 21.30% more successful passing tests.

Conclusion: Incorporating validation intentions and reusing project-specific domain knowledge enables more practical and effective automated test generation. IntentionTest demonstrates superior ability to generate semantically correct and executable tests that meet real development needs, outperforming existing test generation techniques based solely on code coverage or code-to-test translation.

Abstract: Test cases are valuable assets for maintaining software quality. While
numerous automated techniques have been proposed for generating tests (either
by maximizing code coverage or by translating focal code into test code),
practical tests are seldom driven by coverage alone. In real projects, each
test reflects a developer's validation intention for a specific behaviour and
embodies rich, project-specific knowledge: which specific APIs to call and what
assertions truly matter. Without considering such knowledge, tests can hardly
pass code review and be integrated into the software product.
  In this work, we propose IntentionTest, which generates project-specific
tests with validation intention as a structured description. Our design is
motivated by two insights: (1) a description of validation intention, compared
to coverage and focal code, carries more crucial information about what to
test; and (2) practical tests exhibit high code duplication, indicating that
domain knowledge is highly reusable for writing new tests. Given a focal code
and a description of validation intention (in the form of either an informal
comment or a formal test plan), IntentionTest retrieves a referable test in the
project to guide test generation. Moreover, IntentionTest reduces the test
generation problem into an editing problem on the test code regarding the
validation intention. It generates a test including both test prefix and
oracle, which aims to be executable and semantically correct.
  We evaluate IntentionTest against state-of-the-art baselines on 4,146 test
cases from 13 open-source projects. Specifically, compared to ChatTester,
IntentionTest can (1) generate significantly more semantically correct tests,
improving common mutation scores by 39.03% and coverage overlap with
ground-truth tests by 40.14%; (2) generate 21.30% more successful passing
tests.

</details>


### [28] [Client--Library Compatibility Testing with API Interaction Snapshots](https://arxiv.org/abs/2507.20814)
*Gustave Monce,Thomas Degueule,Jean-Rémy Falleri,Romain Robbes*

Main category: cs.SE

TL;DR: Relying only on traditional regression tests leaves client applications exposed to subtle, hard-to-detect breaking changes in third-party libraries. This paper proposes and validates a method—implemented in the tool Gilesi—to record and compare actual API interactions (snapshots) during testing, which successfully catches these breaking changes where standard methods fail.


<details>
  <summary>Details</summary>
Motivation: Traditional regression tests for software that rely on third-party libraries often fail to detect silent, behavioral breaking changes (BBCs) introduced by library updates. This is due to limited test coverage or weak assertions in client tests, leaving applications vulnerable to undetected run-time errors.

Method: The authors propose a compatibility testing approach that records actual API interactions (inputs, outputs, exceptions, etc.) at the client-library boundary during client test execution. These recorded interactions form 'snapshots' which represent the contract expected by the client. When the library evolves, new snapshots are compared with the original ones to identify changes, potentially flagging BBCs. This technique is implemented in a Java framework called Gilesi, which automates API instrumentation, snapshot recording, and comparison.

Result: The prototype tool, Gilesi, was evaluated in a preliminary case study on several client-library pairs where BBCs were artificially inserted. Gilesi successfully detected BBCs that were missed by the client test suites, demonstrating its effectiveness in revealing subtle, contract-breaking changes.

Conclusion: The paper concludes that snapshot-based API interaction recording and comparison is a practical and effective way to flag behavioral breaking changes that would evade traditional client-side regression testing. The approach is automated, works with existing client tests, and increases resilience to silent incompatibilities between evolving libraries and their clients.

Abstract: Modern software development heavily relies on third-party libraries to speed
up development and enhance quality. As libraries evolve, they may break the
tacit contract established with their clients by introducing behavioral
breaking changes (BBCs) that alter run-time behavior and silently break client
applications without being detected at compile time. Traditional regression
tests on the client side often fail to detect such BBCs, either due to limited
library coverage or weak assertions that do not sufficiently exercise the
library's expected behavior. To address this issue, we propose a novel approach
to client--library compatibility testing that leverages existing client tests
in a novel way. Instead of relying on developer-written assertions, we propose
recording the actual interactions at the API boundary during the execution of
client tests (protocol, input and output values, exceptions, etc.). These
sequences of API interactions are stored as snapshots which capture the exact
contract expected by a client at a specific point in time. As the library
evolves, we compare the original and new snapshots to identify perturbations in
the contract, flag potential BBCs, and notify clients. We implement this
technique in our prototype tool Gilesi, a Java framework that automatically
instruments library APIs, records snapshots, and compares them. Through a
preliminary case study on several client--library pairs with artificially
seeded BBCs, we show that Gilesi reliably detects BBCs missed by client test
suites.

</details>


### [29] [Search-Based Fuzzing For RESTful APIs That Use MongoDB](https://arxiv.org/abs/2507.20848)
*Hernan Ghianni,Man Zhang,Juan P. Galeotti,Andrea Arcuri*

Main category: cs.SE

TL;DR: Enhancing white-box testing for RESTful APIs using MongoDB by dynamically analyzing and inserting database states during test generation, leading to up to 18% higher code coverage compared to existing techniques.


<details>
  <summary>Details</summary>
Motivation: Testing RESTful APIs that interact with NoSQL databases requires consideration of the dynamic database state, which is often neglected by existing white-box testing techniques. Achieving high code coverage and fault detection is more challenging when the database state is not easily or effectively manipulated during test generation.

Method: The authors propose novel search-based software test generation techniques for RESTful APIs that interact with NoSQL databases (specifically MongoDB). These techniques dynamically analyze the runtime state of the database through automated code instrumentation and allow direct insertion of NoSQL data from within test cases. The approach is implemented as an extension to EvoMaster, an open-source white-box fuzzing tool.

Result: In experiments conducted on six RESTful APIs, the proposed methods led to significant improvements in code coverage, with up to 18% higher coverage compared to previous white-box approaches. The new techniques also outperformed four state-of-the-art black-box fuzzers.

Conclusion: Taking the dynamic state of NoSQL databases into account and allowing direct data insertion from test cases can greatly improve the effectiveness of white-box test generation for RESTful APIs. The authors’ extension to EvoMaster demonstrates a substantial increase in code coverage and shows clear advantages over existing tools.

Abstract: In RESTful APIs, interactions with a database are a common and crucial
aspect. When generating whitebox tests, it is essential to consider the
database's state (i.e., the data contained in the database) to achieve higher
code coverage and uncover more hidden faults. This article presents novel
techniques to enhance search-based software test generation for RESTful APIs
interacting with NoSQL databases. Specifically, we target the popular MongoDB
database, by dynamically analyzing (via automated code instrumentation) the
state of the database during the test generation process. Additionally, to
achieve better results, our novel approach allows inserting NoSQL data directly
from test cases. This is particularly beneficial when generating the correct
sequence of events to set the NoSQL database in an appropriate state is
challenging or time-consuming. This method is also advantageous for testing
read-only microservices. Our novel techniques are implemented as an extension
of EvoMaster, the only open-source tool for white-box fuzzing RESTful APIs.
Experiments conducted on six RESTful APIs demonstrated significant improvements
in code coverage, with increases of up to 18% compared to existing white-box
approaches. To better highlight the improvements of our novel techniques,
comparisons are also carried out with four state-of-the-art black-box fuzzers.

</details>


### [30] [Enhancing Project-Specific Code Completion by Inferring Internal API Information](https://arxiv.org/abs/2507.20888)
*Le Deng,Xiaoxue Ren,Chao Ni,Ming Liang,David Lo,Zhongxin Liu*

Main category: cs.SE

TL;DR: Current code completion models struggle with internal APIs lacking explicit imports. This paper presents a way to infer and represent such APIs for LLM-based completion, yielding large performance gains on new and existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Project-specific code completion struggles when internal APIs aren't explicitly imported, which hinders code accuracy in large, modular projects. Existing LLMs with RAG can't easily incorporate these unseen APIs during code generation, creating a gap in performance.

Method: The proposed method infers internal API information even when imports are missing. This is done by constructing usage examples and semantic descriptions for APIs to create a knowledge base. LLMs use this enriched context for improved code completions. The authors also introduce ProjBench, a real-world project benchmark designed to avoid information leakage from import statements.

Result: Experiments on ProjBench and CrossCodeEval benchmarks show the method significantly outperforms previous approaches, raising code exact match by 22.72% and identifier exact match by 18.31%. Integrating the method with baselines further boosts code match by 47.80% and identifier match by 35.55%.

Conclusion: Inferring and enriching internal API representations without relying on imports results in much more accurate project-specific code completion. Using knowledge bases constructed from examples and semantic information allows LLMs to address the internal API gap effectively, as shown on rigorous new benchmarks.

Abstract: Project-specific code completion is a critical task that leverages context
from a project to generate accurate code. State-of-the-art methods use
retrieval-augmented generation (RAG) with large language models (LLMs) and
project information for code completion. However, they often struggle to
incorporate internal API information, which is crucial for accuracy, especially
when APIs are not explicitly imported in the file.
  To address this, we propose a method to infer internal API information
without relying on imports. Our method extends the representation of APIs by
constructing usage examples and semantic descriptions, building a knowledge
base for LLMs to generate relevant completions. We also introduce ProjBench, a
benchmark that avoids leaked imports and consists of large-scale real-world
projects.
  Experiments on ProjBench and CrossCodeEval show that our approach
significantly outperforms existing methods, improving code exact match by
22.72% and identifier exact match by 18.31%. Additionally, integrating our
method with existing baselines boosts code match by 47.80% and identifier match
by 35.55%.

</details>


### [31] [Repairing vulnerabilities without invisible hands. A differentiated replication study on LLMs](https://arxiv.org/abs/2507.20977)
*Maria Camporese,Fabio Massacci*

Main category: cs.SE

TL;DR: This paper questions whether LLMs' impressive results in automated vulnerability repair are genuine or inflated by hidden factors, by experimentally disturbing fault localization information in repair tasks and rigorously assessing the impact on LLM patching performance.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to investigate whether the impressive performance of large language models (LLMs) in Automated Vulnerability Repair (AVR) is genuinely due to their capabilities, or if it is influenced by hidden biases such as training-data leakage or overly accurate fault localization in benchmarks.

Method: The paper uses a controlled experiment where errors are deliberately introduced to the reported location of vulnerabilities in AVR prompts. Utilizing the Vul4J and VJTrans benchmarks, LLMs are tasked with generating and reviewing patches with fault locations shifted by various numbers of lines from the actual site. The effectiveness and accuracy of the repairs are then validated using regression and proof-of-vulnerability tests, followed by manual auditing and estimation of error rates via the Agresti-Coull-Wilson method.

Result: The results will reveal whether LLM-based AVR performance diminishes when the fault localization is less accurate, thereby assessing if LLMs are simply memorizing existing fixes or can generalize repairs away from ground truth fault positions. The specific outcome of the experiment is not detailed in the abstract.

Conclusion: The conclusion aims to clarify whether current LLM AVR benchmarks reflect true repair capability or are overly optimistic due to underlying experimental or data biases. The study's outcome will inform best practices for future AVR benchmarking and development.

Abstract: Background: Automated Vulnerability Repair (AVR) is a fast-growing branch of
program repair. Recent studies show that large language models (LLMs)
outperform traditional techniques, extending their success beyond code
generation and fault detection.
  Hypothesis: These gains may be driven by hidden factors -- "invisible hands"
such as training-data leakage or perfect fault localization -- that let an LLM
reproduce human-authored fixes for the same code.
  Objective: We replicate prior AVR studies under controlled conditions by
deliberately adding errors to the reported vulnerability location in the
prompt. If LLMs merely regurgitate memorized fixes, both small and large
localization errors should yield the same number of correct patches, because
any offset should divert the model from the original fix.
  Method: Our pipeline repairs vulnerabilities from the Vul4J and VJTrans
benchmarks after shifting the fault location by n lines from the ground truth.
A first LLM generates a patch, a second LLM reviews it, and we validate the
result with regression and proof-of-vulnerability tests. Finally, we manually
audit a sample of patches and estimate the error rate with the
Agresti-Coull-Wilson method.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [32] [Development and Evaluation of Adaptive LearningSupport System Based on Ontology of MultipleProgramming Languages](https://arxiv.org/abs/2507.19728)
*Lalita Na Nongkhai,Jingyun Wang,Takahiko Mendori*

Main category: cs.PL

TL;DR: An adaptive programming exercise system using an ontology and the Elo Rating System significantly improves learning outcomes over random exercise selection. The system personalizes tasks by assessing skills and adapting recommendations, leading to more correct answers and conceptual mastery.


<details>
  <summary>Details</summary>
Motivation: There is a need to personalize programming education to individual skill levels, as learners benefit from exercises that are tailored to their current understanding and abilities. Existing systems may not sufficiently leverage domain knowledge or adapt exercise difficulty accurately.

Method: The paper proposes an ontology-based adaptive learning system called ADVENTURE. This system uses an ontology (CONTINUOUS) that covers concepts common to multiple programming languages. ADVENTURE visualizes these concepts, offers hints, and recommends new topics, adjusting difficulty in real time using an educational adaptation of the Elo Rating System. The system's effectiveness was evaluated through an experimental study with 1,186 code submissions, comparing adaptive and random assignment modes across six features.

Result: The study found significant differences in four out of six features between adaptive and random modes. In particular, the adaptive mode led to a higher submission of correct answers and more pass concepts, suggesting better learning outcomes.

Conclusion: The adaptive, ontology-driven approach supports programming learners more effectively than a non-adaptive (random) approach, as indicated by improved outcomes in key metrics. Personalization via ontologies and adaptive difficulty mechanisms enhances practice in programming exercises.

Abstract: This paper introduces an ontology-based approach within an adaptive learning
support system for computer programming. This system (named ADVENTURE) is
designed to deliver personalized programming exercises that are tailored to
individual learners' skill levels. ADVENTURE utilizes an ontology, named
CONTINUOUS, which encompasses common concepts across multiple programming
languages. The system leverages this ontology not only to visualize programming
concepts but also to provide hints during practice programming exercises and
recommend subsequent programming concepts. The adaptive mechanism is driven by
the Elo Rating System, applied in an educational context to dynamically
estimate the most appropriate exercise difficulty for each learner. An
experimental study compared two instructional modes, adaptive and random, based
on six features derived from 1,186 code submissions across all the experimental
groups. The results indicate significant differences in four of six analyzed
features between these two modes. Notably, the adaptive mode demonstrates a
significant difference over the random mode in two features, the submission of
correct answers and the number of pass concepts. Therefore, these results
underscore that this adaptive learning support system may support learners in
practicing programming exercises.

</details>


### [33] [The Power of Negation in Higher-Order Datalog](https://arxiv.org/abs/2507.20251)
*Angelos Charalambidis,Babis Kostopoulos,Christos Nomikos,Panos Rondogiannis*

Main category: cs.PL

TL;DR: This paper maps out how Higher-Order Datalog with negation relates to computational complexity, showing higher orders align with higher complexity classes under different semantics, and revealing trade-offs between program order and non-determinism in logic programming.


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand the expressive power of Higher-Order Datalog with negation under different semantics, particularly how it relates to established computational complexity classes. This helps clarify the capabilities and limitations of logic programming languages in representing complex computational tasks.

Method: They analyze Higher-Order Datalog$^\neg$ under both the well-founded and stable model semantics, and establish connections to complexity classes (k-EXP, k-NEXP, co-(k-NEXP)). They use logical and theoretical techniques to demonstrate these equivalences, including existential predicate variables, partially applied relations, and stratified fragments.

Result: Under well-founded semantics, $(k+1)$-Order Datalog$^\neg$ captures k-EXP for all $k\geq 1$. Under stable model semantics, $(k+1)$-Order Datalog$^\neg$ captures co-(k-NEXP) with cautious reasoning and k-NEXP with brave reasoning. These results also hold for certain stratified and augmented fragments. A hierarchy of expressive power is shown, with higher order increasing capabilities over lower order plus non-determinism.

Conclusion: Higher-Order Datalog$^\neg$ exhibits a structured hierarchy in expressive power depending on both semantic choices and program order. Increasing order can provide more expressive power than increasing non-determinism, illuminating a trade-off rooted in logic programming paradigms.

Abstract: We investigate the expressive power of Higher-Order Datalog$^\neg$ under both
the well-founded and the stable model semantics, establishing tight connections
with complexity classes. We prove that under the well-founded semantics, for
all $k\geq 1$, $(k+1)$-Order Datalog$^\neg$ captures k-EXP, a result that holds
without explicit ordering of the input database. The proof of this fact can be
performed either by using the powerful existential predicate variables of the
language or by using partially applied relations and relation enumeration.
Furthermore, we demonstrate that this expressive power is retained within a
stratified fragment of the language. Under the stable model semantics, we show
that $(k+1)$-Order Datalog$^\neg$ captures co-(k-NEXP) using cautious reasoning
and k-NEXP using brave reasoning, again with analogous results for the
stratified fragment augmented with choice rules. Our results establish a
hierarchy of expressive power, highlighting an interesting trade-off between
order and non-determinism in the context of higher-order logic programming:
increasing the order of programs under the well-founded semantics can surpass
the expressive power of lower-order programs under the stable model semantics.

</details>
