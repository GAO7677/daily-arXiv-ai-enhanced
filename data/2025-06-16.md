<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 68]
- [cs.PL](#cs.PL) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Application Modernization with LLMs: Addressing Core Challenges in Reliability, Security, and Quality](https://arxiv.org/abs/2506.10984)
*Ahilan Ayyachamy Nadar Ponnusamy*

Main category: cs.SE

TL;DR: This paper proposes and tests a framework that blends large language models' code reasoning/generation abilities with essential human oversight to improve AI-assisted application modernization, demonstrating its value through a real-world case study and providing an open-source implementation.


<details>
  <summary>Details</summary>
Motivation: AI-assisted code generation tools have improved software development efficiency, but face critical challenges like security vulnerabilities, reliability issues, and inconsistent output. Addressing these issues is essential for realizing the technology's full potential, especially as existing models still leave gaps in trustworthy, high-quality code.

Method: The paper builds on prior research using large language models (LLMs) for application modernization. It proposes a framework that integrates two main LLM capabilities—code reasoning and code generation—with active human involvement to boost effectiveness and trustworthiness. The framework's application is demonstrated via a real-world case study with detailed step-by-step analysis and comparisons to alternative methods.

Result: The framework effectively combines human expertise with LLM capabilities to address application modernization challenges, improving quality and trustworthiness. The practical utility of this approach is demonstrated in a real-world scenario, with detailed methodology provided. A reference implementation is available for public use on GitHub.

Conclusion: AI-assisted code generation can be made more reliable and trustworthy by merging advanced LLM features with human guidance. This combined approach addresses existing limitations and serves as a strong foundation for future work in AI-driven application modernization.

Abstract: AI-assisted code generation tools have revolutionized software development,
offering unprecedented efficiency and scalability. However, multiple studies
have consistently highlighted challenges such as security vulnerabilities,
reliability issues, and inconsistencies in the generated code. Addressing these
concerns is crucial to unlocking the full potential of this transformative
technology. While advancements in foundational and code-specialized language
models have made notable progress in mitigating some of these issues,
significant gaps remain, particularly in ensuring high-quality, trustworthy
outputs.
  This paper builds upon existing research on leveraging large language models
(LLMs) for application modernization. It explores an opinionated approach that
emphasizes two core capabilities of LLMs: code reasoning and code generation.
The proposed framework integrates these capabilities with human expertise to
tackle application modernization challenges effectively. It highlights the
indispensable role of human involvement and guidance in ensuring the success of
AI-assisted processes.
  To demonstrate the framework's utility, this paper presents a detailed case
study, walking through its application in a real-world scenario. The analysis
includes a step-by-step breakdown, assessing alternative approaches where
applicable. This work aims to provide actionable insights and a robust
foundation for future research in AI-driven application modernization. The
reference implementation created for this paper is available on GitHub.

</details>


### [2] [Collaboration Tools and their Role in Agile Software Projects](https://arxiv.org/abs/2506.10985)
*Raman Mohammed Hussein,Bryar A. Hassan*

Main category: cs.SE

TL;DR: This review highlights that using Slack, Microsoft Teams, and Confluence boosts productivity in Agile software projects by improving coordination, communication, and alignment with Agile principles, particularly for remote teams.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the challenges teams face in effective collaboration and communication while working remotely within agile and software projects.

Method: This is a review paper that examines how specific collaboration tools (Slack, Microsoft Teams, and Confluence) align with Agile principles and support agile practices in project environments.

Result: The paper finds that Slack, Microsoft Teams, and Confluence are essential for better task coordination and knowledge sharing, supporting agile values in cross-functional teams.

Conclusion: Collaboration tools like Slack, Microsoft Teams, and Confluence are crucial for successful Agile project execution, enabling better organization, openness, and interaction, and ultimately enhancing productivity.

Abstract: The purpose of this review is to understand the importance of collaboration
tools which are Slack, Microsoft Teams, Confluence in Agile and software
projects. Agile methodologies rely on flexibility, using cycles and integration
throughout various levels of developing cycles. However, it is still a great
problem for many teams to collaborate and communicate even if staff members and
teams are working remotely. In terms of collaboration, the applications and
technologies mean better organization of work, increased mutually
understandable openness and fast and efficient inter team and interpersonal
interactions to enhance results of projects into productivity. This paper
examines how these tools fit the Agile principles, how they facilitate
iterative development, and encouraging effective initiation and tracking of
tasks in small and large projects. The insights focus on how Slack, Microsoft
Teams, and Confluence are essential for gaining better task coordination,
supporting knowledge sharing, and adopting agile values across cross-functional
contexts.

</details>


### [3] [CoMRAT: Commit Message Rationale Analysis Tool](https://arxiv.org/abs/2506.10986)
*Mouna Dhaouadi,Bentley James Oakes,Michalis Famelis*

Main category: cs.SE

TL;DR: CoMRAT is a tool that analyzes rationale in Github commit messages, helping researchers and developers understand the reasoning behind code changes. Early evaluation shows it is effective and user-friendly.


<details>
  <summary>Details</summary>
Motivation: Rationale for code changes is crucial in collaborative open-source development, and commit messages often contain this rationale. However, current research on extracting and analyzing rationale from commit messages is lacking.

Method: The paper introduces CoMRAT, a tool designed to analyze decision and rationale sentences in Github commit messages. It allows both researchers and developers to extract metrics and check the rationale content in any Github repository's commit messages.

Result: The preliminary evaluation of CoMRAT indicates it is useful and usable for both researchers analyzing rationale and developers checking rationale quantity in commit messages.

Conclusion: CoMRAT offers a practical solution for extracting and analyzing rationale from commit messages, supporting both research and software development needs.

Abstract: In collaborative open-source development, the rationale for code changes is
often captured in commit messages, making them a rich source of valuable
information. However, research on rationale in commit messages remains limited.
In this paper, we present CoMRAT, a tool for analyzing decision and rationale
sentences rationale in commit messages. CoMRAT enables a) researchers to
produce metrics and analyses on rationale information in any Github module, and
b) developers to check the amount of rationale in their commit messages. A
preliminary evaluation suggests the tool's usefulness and usability in both
these research and development contexts.

</details>


### [4] [Chain of Draft for Software Engineering: Challenges in Applying Concise Reasoning to Code Tasks](https://arxiv.org/abs/2506.10987)
*Shaoyi Yang*

Main category: cs.SE

TL;DR: Chain of Draft (CoD) prompting methods can make large language models for coding tasks far more efficient, cutting computation needs by 45% while keeping over 90% of code quality compared to the popular Chain of Thought (CoT). Use CoD variants to save costs in software engineering without a big loss in accuracy.


<details>
  <summary>Details</summary>
Motivation: Large language models are widely used for software development, but complex code tasks require extensive intermediate reasoning, leading to high computation cost and latency. The paper aims to find more efficient reasoning methods without compromising solution quality.

Method: The study extends the Chain of Draft (CoD) prompting method for software engineering tasks, creating and evaluating multiple CoD variants on 300 samples from the SWE-bench benchmark. Token usage and code quality were assessed and compared against the traditional Chain of Thought (CoT) approach across multidimensional metrics.

Result: All CoD variants use significantly fewer tokens than CoT, with the most efficient variant using only 55.4% as many tokens. Code quality remains high, with over 90% of CoT's performance across correctness, compatibility, and maintainability. Efficiency gains were notable but less dramatic than in prior work on mathematical tasks, due to the complexity of software engineering problems.

Conclusion: CoD prompting strategies substantially reduce token usage, processing time, and API costs while maintaining most of the code quality, making them suitable for real-world software development. Choice of prompting strategy should be domain-specific, and the paper provides a decision framework for optimizing LLM-based workflows.

Abstract: Large language models (LLMs) have become vital tools for software
development, but they often require verbose intermediate reasoning for complex
code tasks, leading to high latency and costs. This research extends the Chain
of Draft (CoD) method to software engineering, designing and evaluating
multiple CoD variants tailored for code tasks. Through comprehensive
experiments on all 300 samples from the SWE-bench benchmark, we found that all
CoD variants used significantly fewer tokens than Chain of Thought (CoT), with
Baseline CoD being most efficient at 55.4% of CoT's tokens. While this
represents substantial efficiency gains - translating to approximately 45%
reduction in processing time and API costs - it differs from the extreme 7.6%
reported in the original CoD paper for mathematical reasoning. This difference
stems from the inherent complexity and context-dependency of software tasks,
which require more detailed reasoning to maintain solution quality. Our
multi-dimensional quality assessment revealed that CoD variants maintain over
90% of CoT's code quality across key metrics including correctness,
compatibility, and maintainability, making them practical alternatives for
real-world development scenarios where efficiency matters. This research
demonstrates how domain-specific characteristics influence prompting strategy
effectiveness and provides a framework for balancing efficiency with solution
quality in software engineering applications. Our findings offer practical
guidance for optimizing LLM-based development workflows through appropriate
prompting strategy selection based on project requirements.

</details>


### [5] [You Only Train Once: A Flexible Training Framework for Code Vulnerability Detection Driven by Vul-Vector](https://arxiv.org/abs/2506.10988)
*Bowen Tian,Zhengyang Xu,Mingqiang Wu,Songning Lai,Yutai Yue*

Main category: cs.SE

TL;DR: YOTO is a novel framework that integrates multiple vulnerability detection models without joint retraining, addressing inefficiencies in current deep learning-based approaches, and enabling fast, resource-efficient updates as new vulnerabilities emerge.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenges faced by deep learning-based code vulnerability detection tools, particularly their reliance on large labeled datasets, long training times, and the frequent need to retrain models as new vulnerabilities appear. These challenges make traditional approaches inefficient and less applicable in fast-evolving environments.

Method: The paper introduces YOTO (You Only Train Once), a framework that allows the integration of multiple vulnerability detection models by parameter fusion. This approach does not require joint training, enabling models to be updated or extended without retraining from scratch.

Result: YOTO allows for rapid adaptation to new vulnerabilities, greatly reducing the time and computational resources required for updates. The framework eliminates the need for frequent retraining, improving applicability in emerging scenarios.

Conclusion: The YOTO framework offers a resource-efficient and flexible solution for code vulnerability detection by allowing integration of different models via parameter fusion and removing the necessity for repeated training.

Abstract: With the pervasive integration of computer applications across industries,
the presence of vulnerabilities within code bases poses significant risks. The
diversity of software ecosystems coupled with the intricate nature of modern
software engineering has led to a shift from manual code vulnerability
identification towards the adoption of automated tools. Among these, deep
learning-based approaches have risen to prominence due to their superior
accuracy; however, these methodologies encounter several obstacles. Primarily,
they necessitate extensive labeled datasets and prolonged training periods, and
given the rapid emergence of new vulnerabilities, the frequent retraining of
models becomes a resource-intensive endeavor, thereby limiting their
applicability in cutting-edge scenarios. To mitigate these challenges, this
paper introduces the \underline{\textbf{YOTO}}--\underline{\textbf{Y}}ou
\underline{\textbf{O}}nly \underline{\textbf{T}}rain \underline{\textbf{O}}nce
framework. This innovative approach facilitates the integration of multiple
types of vulnerability detection models via parameter fusion, eliminating the
need for joint training. Consequently, YOTO enables swift adaptation to newly
discovered vulnerabilities, significantly reducing both the time and
computational resources required for model updates.

</details>


### [6] [Prompt engineering and framework: implementation to increase code reliability based guideline for LLMs](https://arxiv.org/abs/2506.10989)
*Rogelio Cruz,Jonatan Contreras,Francisco Guerrero,Ezequiel Rodriguez,Carlos Valdez,Citlali Carrillo*

Main category: cs.SE

TL;DR: A new prompt template significantly improves Python code generation by LLMs, achieving higher test pass rates and lower token usage than existing methods like CoT and zero-shot prompting.


<details>
  <summary>Details</summary>
Motivation: Existing techniques for generating Python code with Large Language Models (LLMs), such as zero-shot and Chain-of-Thought (CoT) prompting, have notable limitations in terms of code correctness and computational efficiency. There is a need to improve the quality, reliability, and resource usage of code generated by LLMs.

Method: The paper proposes a novel prompt template specifically designed for LLMs to enhance Python code generation. This template is evaluated by comparing it with zero-shot and Chain-of-Thought prompting methods using the HumanEval benchmark dataset. Two state-of-the-art LLMs are tested, focusing on the ability of generated code to pass functional tests.

Result: The proposed prompting method outperforms both zero-shot and CoT approaches on the Pass@k metric, indicating higher rates of code correctness. Additionally, it achieves this with significantly reduced token usage compared to the CoT method.

Conclusion: Tailored prompting strategies can meaningfully improve the accuracy, efficiency, and environmental footprint of LLM-driven code generation. This opens up new opportunities for efficient and reliable AI-driven programming.

Abstract: In this paper, we propose a novel prompting approach aimed at enhancing the
ability of Large Language Models (LLMs) to generate accurate Python code.
Specifically, we introduce a prompt template designed to improve the quality
and correctness of generated code snippets, enabling them to pass tests and
produce reliable results. Through experiments conducted on two state-of-the-art
LLMs using the HumanEval dataset, we demonstrate that our approach outperforms
widely studied zero-shot and Chain-of-Thought (CoT) methods in terms of the
Pass@k metric. Furthermore, our method achieves these improvements with
significantly reduced token usage compared to the CoT approach, making it both
effective and resource-efficient, thereby lowering the computational demands
and improving the eco-footprint of LLM capabilities. These findings highlight
the potential of tailored prompting strategies to optimize code generation
performance, paving the way for broader applications in AI-driven programming
tasks.

</details>


### [7] [On the Effectiveness of the 'Follow-the-Sun' Strategy in Mitigating the Carbon Footprint of AI in Cloud Instances](https://arxiv.org/abs/2506.10990)
*Roberto Vergallo,Luís Cruz,Alessio Errico,Luca Mainetti*

Main category: cs.SE

TL;DR: This paper provides evidence that the Follow-the-Sun (FtS) strategy can reduce the carbon emissions of AI training by up to 16.3% without increasing training time, using a benchmark of AI algorithms and real-world carbon intensity data across Europe.


<details>
  <summary>Details</summary>
Motivation: Artificial Intelligence (AI) training consumes significant power, leading to increased carbon emissions. Given the ongoing debate on AI's environmental impact, strategies to minimize the carbon footprint are needed. Although 'Follow-the-Sun' (FtS) has been theorized as a way to reduce emissions by moving workloads to regions with cleaner energy, there is a lack of scientific evidence supporting its effectiveness for AI workloads.

Method: The authors conducted an experiment using a partial synthetic scenario. They benchmarked four AI algorithms in the anomaly detection domain and compared carbon emissions in four scenarios: no strategy, FtS, and two previously known strategies ('Flexible Start' and 'Pause and Resume'). Historical carbon intensity data from 2021 for seven European cities were used to simulate the scenarios.

Result: The experiment demonstrated that using the FtS strategy resulted in average carbon emission reductions of up to 14.6%, with peaks of 16.3%. Additionally, FtS helped to maintain the training time required for AI models.

Conclusion: FtS is an effective strategy to reduce the carbon footprint of AI model training without negatively impacting training time. The results provide the first scientific evidence supporting FtS for reducing AI-related carbon emissions.

Abstract: 'Follow-the-Sun' (FtS) is a theoretical computational model aimed at
minimizing the carbon footprint of computer workloads. It involves dynamically
moving workloads to regions with cleaner energy sources as demand increases and
energy production relies more on fossil fuels. With the significant power
consumption of Artificial Intelligence (AI) being a subject of extensive
debate, FtS is proposed as a strategy to mitigate the carbon footprint of
training AI models. However, the literature lacks scientific evidence on the
advantages of FtS to mitigate the carbon footprint of AI workloads. In this
paper, we present the results of an experiment conducted in a partial synthetic
scenario to address this research gap. We benchmarked four AI algorithms in the
anomaly detection domain and measured the differences in carbon emissions in
four cases: no strategy, FtS, and two strategies previously introduced in the
state of the art, namely Flexible Start and Pause and Resume. To conduct our
experiment, we utilized historical carbon intensity data from the year 2021 for
seven European cities. Our results demonstrate that the FtS strategy not only
achieves average reductions of up to 14.6% in carbon emissions (with peaks of
16.3%) but also helps in preserving the time needed for training.

</details>


### [8] [What is Business Process Automation Anyway?](https://arxiv.org/abs/2506.10991)
*Hoang Vu,Henrik Leopold,Han van der Aa*

Main category: cs.SE

TL;DR: This paper analyzes and categorizes the business process automation capabilities of 18 major vendors, showing industry solutions go well beyond Robotic Process Automation and outlining promising future directions.


<details>
  <summary>Details</summary>
Motivation: Many organizations are increasingly seeking to automate business processes. While traditional automation focused on physical work, digital automation is now most common—especially in business processes involving human-computer interactions. However, academic research has mostly focused on Robotic Process Automation (RPA), even as industry offerings go beyond RPA. This paper aims to fill that gap by investigating the broader spectrum of business process automation capabilities available from industry vendors.

Method: The authors conducted a structured market analysis of 18 major business process automation vendors, as identified by Gartner. This involved systematically examining and comparing the automation capabilities offered by these vendors.

Result: The paper delivers a comprehensive overview of current business process automation solutions in the industry. It categorizes different types and facets of automation available, going beyond just RPA. The study also highlights which aspects of current solutions are promising areas for future development.

Conclusion: The study concludes that business process automation is a multifaceted field with capabilities extending far beyond Robotic Process Automation. By mapping out the current landscape, the paper provides valuable direction for both future research and practical advancements in automation technologies.

Abstract: Many organizations strive to increase the level of automation in their
business processes. While automation historically was mainly concerned with
automating physical labor, current automation efforts mostly focus on
automation in a digital manner, thus targeting work that is related to the
interaction between humans and computers. This type of automation, commonly
referred to as business process automation, has many facets. Yet, academic
literature mainly focuses on Robotic Process Automation, a specific automation
capability. Recognizing that leading vendors offer automation capabilities
going way beyond that, we use this paper to develop a detailed understanding of
business process automation in industry. To this end, we conduct a structured
market analysis of the 18 predominant vendors of business process automation
solutions as identified by Gartner. As a result, we provide a comprehensive
overview of the business process automation capabilities currently offered by
industrial vendors. We show which types and facets of automation exist and
which aspects represent promising directions for the future.

</details>


### [9] [Towards a Theory on Process Automation Effects](https://arxiv.org/abs/2506.10992)
*Hoang Vu,Jennifer Haase,Henrik Leopold,Jan Mendling*

Main category: cs.SE

TL;DR: The paper reviews human-automation interaction literature to analyze the post-implementation effects of process automation in businesses, proposes an engagement model for stakeholders, and offers recommendations to optimize automation use.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the research gap regarding how process automation impacts organizations after it is operational, specifically focusing on the interaction between humans and automation.

Method: A literature review of human-automation interaction studies from various domains is conducted to inform propositions about process automation effects.

Result: The analysis highlights how humans perceive and interact with automation technologies in business processes, leading to the development of an effective engagement model among technology, process participants, process managers, and software developers. The paper also identifies new research questions for the field.

Conclusion: The paper concludes that understanding human perceptions and interactions with automation is essential for optimizing process automation outcomes. Recommendations and a conceptual engagement model are provided to guide organizations in effectively leveraging process automation.

Abstract: Process automation is a crucial strategy for improving business processes,
but little attention has been paid to the effects that automation has once it
is operational. This paper addresses this research problem by reviewing the
literature on human-automation interaction. Although many of the studies in
this field have been conducted in different domains, they provide a foundation
for developing propositions about process automation effects. Our analysis
focuses on how humans perceive automation technology when working within a
process, allowing us to propose an effective engagement model between
technology, process participants, process managers, and software developers.
This paper offers insights and recommendations that can help organizations
optimize their use of process automation. We further derive novel research
questions for a discourse within the process automation community.

</details>


### [10] [Contract-based Verification of Digital Twins](https://arxiv.org/abs/2506.10993)
*Muhammad Naeem,Cristina Seceleanu*

Main category: cs.SE

TL;DR: This paper presents a new, automated method for verifying neural network-based digital twins in Simulink using system-level contracts and model checking via UPPAAL, successfully detecting errors in a real-world case study without needing access to the model's details.


<details>
  <summary>Details</summary>
Motivation: Verifying digital twin models is challenging due to large datasets and complex model architectures, especially when the models use neural networks and function as black boxes. Existing verification methods are insufficient, prompting the need for more effective, scalable solutions.

Method: The proposed methodology combines black-box verification of neural network-based digital twin models with model checking. System-level contracts, expressed as timed automata in UPPAAL, are used to verify if the models' outputs meet the system's requirements. Automated simulation and contract verification are performed without needing details of the digital twin's inner structure.

Result: The methodology is successfully applied to a boiler system case study, uncovering scenarios where the digital twin model fails the specified contracts, demonstrating the practical effectiveness of the approach.

Conclusion: Integrating model checking with digital twin models significantly enhances their verification process and enables the identification of prediction errors without requiring access to model internals.

Abstract: Digital twins are becoming powerful tools in industrial applications,
offering virtual representations of cyber-physical systems. However,
verification of these models remains a significant challenge due to the
potentially large datasets used by the digital twin. This paper introduces an
innovative methodology for verifying neural network-based digital twin models,
in a black-box fashion, by integrating model checking into the process. The
latter relies on defining and applying system-level contracts that capture the
system's requirements, to verify the behavior of digital twin models,
implemented in Simulink. We develop an automated solution that simulates the
digital twin model for certain inputs, and feeds the predicted outputs together
with the inputs to the contract model described as a network of timed automata
in the UPPAAL model checker. The latter verifies whether the predicted outputs
fulfill the specified contracts. This approach allows us to identify scenarios
where the digital twin's behavior fails to meet the contracts, without
requiring the digital twin's design technicalities. We apply our method to a
boiler system case study for which we identify prediction errors via contract
verification. Our work demonstrates the effectiveness of integrating model
checking with digital twin models for continuous improvement.

</details>


### [11] [Improving Software Team Communication Through Social Interventions in Project Management Tools](https://arxiv.org/abs/2506.10994)
*April Clarke*

Main category: cs.SE

TL;DR: The paper investigates how social network analysis can improve communication and teamwork in student SE group projects by building and testing new project management tool features to help students recognize and address collaboration issues.


<details>
  <summary>Details</summary>
Motivation: Many software engineering teams lack effective communication and balanced contributions, leading to project failure. University project-based courses offer a chance for students to improve these skills, but there is limited knowledge on how to consistently guide and enhance student communication and coordination.

Method: The authors propose evaluating the suitability of social network analysis techniques for identifying weaknesses in team communication. Subsequently, they will develop and integrate tool features into a project management system to help students identify and address these issues. Finally, these new features will be evaluated in actual software engineering student group projects.

Result: The expected result is the creation and validation of project management tool features—based on social network analysis—that can effectively point out communication issues and guide students toward better teamwork.

Conclusion: This study aims to offer practical insights and tools for improving communication and coordination in student software engineering teams by leveraging social network analyses within project management software.

Abstract: Productive software engineering teams require effective communication and
balanced contributions between team members. However, teams are often
ineffective at these skills, which is detrimental to project success.
Project-based university courses are an opportunity for students to practise
these skills, but we have yet to establish how we can guide students towards
improving their communication and coordination. We aim to develop project
management tool features, informed by social network analysis, that nudge
students in software engineering group projects towards beneficial behaviours.
To do this, we will first evaluate the suitability of social network analysis
techniques for identifying areas of improvement in teams' communication. Then,
we will develop features in a project management tool that aid students in
identifying and addressing these areas of improvement, and evaluate them in the
context of a software engineering group project.

</details>


### [12] [Evaluating Small-Scale Code Models for Code Clone Detection](https://arxiv.org/abs/2506.10995)
*Jorge Martinez-Gil*

Main category: cs.SE

TL;DR: Recent small code models are generally effective in detecting code clones across multiple benchmarks, but struggle with cases where similar-looking code performs different functions, highlighting an ongoing limitation in code clone detection.


<details>
  <summary>Details</summary>
Motivation: Code clone detection is crucial for software maintenance and refactoring but remains a challenge, especially when structural similarity does not imply functional equivalence. Recent advances in code models motivate a systematic evaluation of their effectiveness in detecting code clones.

Method: The study systematically evaluates the code clone detection performance of six small code models (CodeBERT, GraphCodeBERT, Salesforce T5, UniXCoder, PLBART, and Polycoder) using five datasets (BigCloneBench, CodeJam, Karnalim, POJ104, PoolC). Standard performance metrics such as accuracy, precision, recall, and F1-score are computed.

Result: Most code models perform well across the evaluated metrics. However, detecting clones remains a challenge when code is structurally similar but functionally different.

Conclusion: While state-of-the-art code models are effective in code clone detection in most cases, distinguishing code with high structural similarity but different functionality is still a challenging aspect requiring further research.

Abstract: Detecting code clones is relevant to software maintenance and code
refactoring. This challenge still presents unresolved cases, mainly when
structural similarity does not reflect functional equivalence, though recent
code models show promise. Therefore, this research aims to systematically
measure the performance of several newly introduced small code models in
classifying code pairs as clones or non-clones. The evaluation is based on five
datasets: BigCloneBench, CodeJam, Karnalim, POJ104, and PoolC, as well as six
code models: CodeBERT, GraphCodeBERT, Salesforce T5, UniXCoder, PLBART, and
Polycoder. Most models performed well across standard metrics, including
accuracy, precision, recall, and F1-score. However, a marginal fraction of
clones remains challenging to detect, especially when the code looks similar
but performs different operations. The source code that illustrates our
approach is available at:
https://github.com/jorge-martinez-gil/small-code-models

</details>


### [13] [Evaluating LLMs for Visualization Tasks](https://arxiv.org/abs/2506.10996)
*Saadiq Rauf Khan,Vinit Chandak,Sougata Mukherjea*

Main category: cs.SE

TL;DR: The paper evaluates how well popular LLMs can generate visualization code and interpret common visualizations from simple prompts/questions. While LLMs show promising abilities, they also face several limitations, and insights from this work may help improve future LLMs and visualization tools.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the emergence of Large Language Models (LLMs) and their promising abilities in diverse tasks, particularly exploring their potential role in the domain of Information Visualization, where generating code and understanding visualizations are valuable skills.

Method: The authors conduct a study using various popular LLMs, prompting them to generate code for information visualizations and to answer simple questions about common types of visualizations, in order to assess their performance and capabilities.

Result: The results show that while LLMs can generate code for some visualizations and answer basic questions about them, they also exhibit notable limitations in both tasks.

Conclusion: The paper concludes that, despite observed limitations, insights obtained from evaluating LLMs can be utilized to enhance both LLM development and Information Visualization systems.

Abstract: Information Visualization has been utilized to gain insights from complex
data. In recent times, Large Language Models (LLMs) have performed very well in
many tasks. In this paper, we showcase the capabilities of different popular
LLMs to generate code for visualization based on simple prompts. We also
analyze the power of LLMs to understand some common visualizations by answering
simple questions. Our study shows that LLMs could generate code for some
visualizations as well as answer questions about them. However, LLMs also have
several limitations. We believe that our insights can be used to improve both
LLMs and Information Visualization systems.

</details>


### [14] [A Theory-driven Interpretation and Elaboration of Verification and Validation](https://arxiv.org/abs/2506.10997)
*Hanumanthrao Kannan,Alejandro Salado*

Main category: cs.SE

TL;DR: This paper introduces a formal theory using dynamic epistemic modal logic to clearly define and contextualize verification and validation (V&V) in systems engineering. The approach resolves ambiguities in traditional practices and provides a structured, rigorous framework for V&V as knowledge-building processes.


<details>
  <summary>Details</summary>
Motivation: Verification and validation (V&V) in systems engineering often suffer from conceptual ambiguities, lacking a structured and precise theoretical foundation. The motivation of the paper is to address these ambiguities and to redefine V&V as core knowledge-building activities within systems engineering.

Method: The paper employs dynamic epistemic modal logic to formally define verification and validation. This approach models epistemic states, evidence, and reasoning processes to create a theoretical structure and enables the derivation of theorems illustrating the relationships and underpinnings of V&V.

Result: The authors deliver precise definitions of verification and validation, clarify their respective roles in knowledge generation, and formalize the interplay between knowledge states, evidence, and reasoning. Their framework enables greater rigor and can resolve ambiguities found in conventional V&V practices.

Conclusion: By providing a formal, logic-based theoretical foundation for V&V, the paper enhances the consistency and precision of systems engineering methodologies. This enriches both academic understanding and practical implementation of V&V as essential activities for generating engineering knowledge.

Abstract: This paper presents a formal theory of verification and validation (V&V)
within systems engineering, grounded in the axiom that V&V are fundamentally
knowledge-building activities. Using dynamic epistemic modal logic, we develop
precise definitions of verification and validation, articulating their roles in
confirming and contextualizing knowledge about systems. The theory formalizes
the interplay between epistemic states, evidence, and reasoning processes,
allowing for the derivation of theorems that clarify the conceptual
underpinnings of V&V. By providing a formal foundation, this work addresses
ambiguities in traditional V&V practices, offering a structured framework to
enhance precision and consistency in systems engineering methodologies. The
insights gained have implications for both academic research and practical
applications, fostering a deeper understanding of V&V as critical components of
engineering knowledge generation.

</details>


### [15] [Towards Automated Formal Verification of Backend Systems with LLMs](https://arxiv.org/abs/2506.10998)
*Kangping Xu,Yifan Luo,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.SE

TL;DR: This paper introduces a novel AI-assisted framework that translates Scala code into formal proofs, automates verification of backend logic, and can halve manual testing costs and efforts, marking a scalable breakthrough for software QA.


<details>
  <summary>Details</summary>
Motivation: Automated software testing is limited by issues such as test locality, lack of general reliability, and failure to capture business logic, hindering its ability to match human engineers' effectiveness. There is a need for more robust, reliable, and scalable testing solutions that can reduce manual workload.

Method: The paper proposes a framework that translates Scala backend code into formal Lean representations using functional programming and type systems. This pipeline automatically generates formal theorems describing the system's intended behavior and uses LLM-based provers to attempt to verify these theorems. If proven, the component is confirmed correct; proven negation confirms a bug; otherwise, human intervention is necessary.

Result: The framework was evaluated on realistic backend systems, successfully verifying over 50% of all test requirements automatically. The average cost of LLM-based verification per API is just $2.19, making it much cheaper and more scalable than manual testing, and supporting parallel execution.

Conclusion: The proposed method can potentially automate up to half of a testing engineer's workload, offering a scalable, cost-effective, and productive approach to software testing as LLMs improve. This indicates strong potential for AI-driven, formal verification-based testing in real-world backend systems.

Abstract: Software testing plays a critical role in ensuring that systems behave as
intended. However, existing automated testing approaches struggle to match the
capabilities of human engineers due to key limitations such as test locality,
lack of general reliability, and business logic blindness. In this work, we
propose a novel framework that leverages functional programming and type
systems to translate Scala backend code into formal Lean representations. Our
pipeline automatically generates theorems that specify the intended behavior of
APIs and database operations, and uses LLM-based provers to verify them. When a
theorem is proved, the corresponding logic is guaranteed to be correct and no
further testing is needed. If the negation of a theorem is proved instead, it
confirms a bug. In cases where neither can be proved, human intervention is
required. We evaluate our method on realistic backend systems and find that it
can formally verify over 50% of the test requirements, which suggests that half
of a testing engineer's workload can be automated. Additionally, with an
average cost of only $2.19 per API, LLM-based verification is significantly
more cost-effective than manual testing and can be scaled easily through
parallel execution. Our results indicate a promising direction for scalable,
AI-powered software testing, with the potential to greatly improve engineering
productivity as models continue to advance.

</details>


### [16] [Automated Validation of COBOL to Java Transformation](https://arxiv.org/abs/2506.10999)
*Atul Kumar,Diptikalyan Saha,Toshikai Yasue,Kohichi Ono,Saravanan Krishnan,Sandeep Hans,Fumiko Satoh,Gerald Mitchell,Sachin Kumar*

Main category: cs.SE

TL;DR: This paper presents a testing framework to validate and improve LLM-generated translations of COBOL to Java, by generating and running equivalent unit tests on both original and translated code to check for semantic equivalence and repair errors.


<details>
  <summary>Details</summary>
Motivation: Large Language Model (LLM)-based AI can now translate legacy enterprise code (like COBOL) to modern languages (such as Java or Python), but the automatically generated code may not be semantically equivalent or trustworthy. There is a need to validate and improve the correctness of such code translations.

Method: The authors propose a framework and tool that uses symbolic execution to automatically generate unit tests for COBOL programs, including mocking external resource calls. Equivalent JUnit test cases are generated for the translated Java code, and these are run to check for semantic equivalence between the original and translated programs.

Result: The generated tools successfully validate whether the COBOL and Java programs are semantically equivalent. If discrepancies are found, the tool can help repair the translated code and provide feedback for improving the LLM-based translation process.

Conclusion: The proposed framework enhances trust and reliability in LLM-based code translation by systematically validating semantic equivalence and facilitating iterative improvements through code repair and feedback.

Abstract: Recent advances in Large Language Model (LLM) based Generative AI techniques
have made it feasible to translate enterpriselevel code from legacy languages
such as COBOL to modern languages such as Java or Python. While the results of
LLM-based automatic transformation are encouraging, the resulting code cannot
be trusted to correctly translate the original code. We propose a framework and
a tool to help validate the equivalence of COBOL and translated Java. The
results can also help repair the code if there are some issues and provide
feedback to the AI model to improve. We have developed a
symbolic-execution-based test generation to automatically generate unit tests
for the source COBOL programs which also mocks the external resource calls. We
generate equivalent JUnit test cases with equivalent mocking as COBOL and run
them to check semantic equivalence between original and translated programs.

</details>


### [17] [Ever-Improving Test Suite by Leveraging Large Language Models](https://arxiv.org/abs/2506.11000)
*Ketai Qiu*

Main category: cs.SE

TL;DR: E-Test uses LLMs to analyze production behavior and incrementally add untested or error-prone cases to test suites, outperforming current methods in both coverage and quality.


<details>
  <summary>Details</summary>
Motivation: Test suites need to evolve to reflect how software is used in production, as this is essential for maintaining software quality over time. Traditional test suites may miss real-world behaviors that emerge after deployment.

Method: The paper introduces E-Test, an incremental test suite augmentation approach. E-Test leverages Large Language Models (LLMs) to analyze execution scenarios in production, classify them as already-tested, not-yet-tested, or error-prone, and then generate and add relevant test cases to the suite.

Result: E-Test was evaluated experimentally and shown to outperform leading state-of-the-art methods in identifying inadequately tested behaviors and optimizing test suites.

Conclusion: E-Test effectively augments test suites by identifying and adding tests for previously untested or error-prone behaviors found in production. Using LLMs in this process leads to improved coverage and quality compared to existing techniques.

Abstract: Augmenting test suites with test cases that reflect the actual usage of the
software system is extremely important to sustain the quality of long lasting
software systems. In this paper, we propose E-Test, an approach that
incrementally augments a test suite with test cases that exercise behaviors
that emerge in production and that are not been tested yet. E-Test leverages
Large Language Models to identify already-tested, not-yet-tested, and
error-prone unit execution scenarios, and augment the test suite accordingly.
Our experimental evaluation shows that E-Test outperforms the main
state-of-the-art approaches to identify inadequately tested behaviors and
optimize test suites.

</details>


### [18] [Rethinking Technological Readiness in the Era of AI Uncertainty](https://arxiv.org/abs/2506.11001)
*S. Tucker Browne,Mark M. Bailey*

Main category: cs.SE

TL;DR: Traditional technology assessments miss key AI issues in military systems. The new AI Readiness Framework helps evaluate AI maturity, trustworthiness, and readiness, giving military leaders better confidence before deployment.


<details>
  <summary>Details</summary>
Motivation: Current technology readiness assessments are inadequate for evaluating AI-specific challenges in military systems, potentially leading to deployment risks.

Method: The authors propose a new AI Readiness Framework, which adapts the concept of Technology Readiness Levels (TRL) for AI-enabled military systems, and demonstrate its feasibility using current data evaluation tools and testing practices.

Result: The framework can be practically implemented in the near term, offering a more comprehensive assessment of AI systems' maturity, reliability, and safety for combat use.

Conclusion: A specialized AI Readiness Framework enables more accurate evaluation of AI components in military systems, enhancing decision-making and risk management for defense technology deployment.

Abstract: Artificial intelligence (AI) is poised to revolutionize military combat
systems, but ensuring these AI-enabled capabilities are truly mission-ready
presents new challenges. We argue that current technology readiness assessments
fail to capture critical AI-specific factors, leading to potential risks in
deployment. We propose a new AI Readiness Framework to evaluate the maturity
and trustworthiness of AI components in military systems. The central thesis is
that a tailored framework - analogous to traditional Technology Readiness
Levels (TRL) but expanded for AI - can better gauge an AI system's reliability,
safety, and suitability for combat use. Using current data evaluation tools and
testing practices, we demonstrate the framework's feasibility for near-term
implementation. This structured approach provides military decision-makers with
clearer insight into whether an AI-enabled system has met the necessary
standards of performance, transparency, and human integration to be deployed
with confidence, thus advancing the field of defense technology management and
risk assessment.

</details>


### [19] [Notes On Writing Effective Empirical Software Engineering Papers: An Opinionated Primer](https://arxiv.org/abs/2506.11002)
*Roberto Verdecchia,Justus Bogner*

Main category: cs.SE

TL;DR: The paper offers subjective yet practical writing guidance for students and researchers in Empirical Software Engineering, aiming to fill a gap in explicit training on good scientific writing in the field.


<details>
  <summary>Details</summary>
Motivation: Good scientific writing practices are critical in Empirical Software Engineering (ESE), but they are rarely discussed or documented systematically, despite being important evaluation criteria in the field.

Method: The paper takes a pragmatic, educational-first approach, presenting subjective and opinionated writing advice specifically intended for ESE researchers, particularly students (BSc, MSc, PhD).

Result: The paper provides a guide for better ESE paper writing, based on the authors' own experience and perspectives, intended to be practical and helpful, especially for students but also for other researchers.

Conclusion: Although not claiming full objectivity or generalizability, the authors believe their writing approach has been effective for them and propose that it will be helpful for others engaged in ESE research.

Abstract: While mastered by some, good scientific writing practices within Empirical
Software Engineering (ESE) research appear to be seldom discussed and
documented. Despite this, these practices are implicit or even explicit
evaluation criteria of typical software engineering conferences and journals.
In this pragmatic, educational-first document, we want to provide guidance to
those who may feel overwhelmed or confused by writing ESE papers, but also
those more experienced who still might find an opinionated collection of
writing advice useful. The primary audience we had in mind for this paper were
our own BSc, MSc, and PhD students, but also students of others. Our documented
advice therefore reflects a subjective and personal vision of writing ESE
papers. By no means do we claim to be fully objective, generalizable, or
representative of the whole discipline. With that being said, writing papers in
this way has worked pretty well for us so far. We hope that this guide can at
least partially do the same for others.

</details>


### [20] [EmbedAgent: Benchmarking Large Language Models in Embedded System Development](https://arxiv.org/abs/2506.11003)
*Ruiyang Xu,Jialun Cao,Mingyuan Wu,Wenliang Zhong,Yaojie Lu,Ben He,Xianpei Han,Shing-Chi Cheung,Le Sun*

Main category: cs.SE

TL;DR: The paper introduces Embedbench and EmbedAgent to benchmark LLMs for embedded system programming, exposing their weaknesses and improvement methods, with new strategies significantly boosting performance on challenging tasks.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) show potential for various tasks but lack specific benchmarks to evaluate their effectiveness in the domain of embedded system development. There is a need to simulate real-world engineering roles and assess LLMs' capacity to bridge digital and physical systems.

Method: The authors introduced EmbedAgent, a role-based simulation paradigm for embedded system development, and created Embedbench, a comprehensive benchmark with 126 cases across programming, circuit design, and cross-platform migration for 9 components and 3 platforms. Ten mainstream LLMs were evaluated, and improvement strategies (retrieval augmented generation and compiler feedback) were tested.

Result: LLMs perform inconsistently across tasks: DeepSeek-R1 achieves only 55.6% pass@1 (schematic provided) and 50.0% (self-generated schematics). Performance is better on MicroPython/Raspberry Pi Pico (up to 73.8%), but poor on ESP-IDF (29.4%). General LLMs struggle to apply relevant knowledge, and reasoning LLMs overthink simple tasks. The proposed enhancement strategies improved DeepSeek-R1 to 65.1% (with schematics) and Arduino to ESP32 migration accuracy from 21.4% to 27.8%.

Conclusion: Embedbench provides a novel and comprehensive way to assess LLMs in embedded system-related tasks, revealing their current limitations and highlighting the potential of augmentation strategies for improved performance. Significant domain-specific gaps remain, especially for less common platforms and migration tasks.

Abstract: Large Language Models (LLMs) have shown promise in various tasks, yet few
benchmarks assess their capabilities in embedded system development.In this
paper, we introduce EmbedAgent, a paradigm designed to simulate real-world
roles in embedded system development, such as Embedded System Programmer,
Architect, and Integrator. This paradigm enables LLMs to be tested in tasks
that bridge the gap between digital and physical systems, allowing for a more
comprehensive assessment of their capabilities. To evaluate LLMs on these
tasks, we propose Embedbench, the first comprehensive benchmark for embedded
system programming, circuit design, and cross-platform migration.Embedbench
consists of 126 cases, covering 9 electronic components across 3 hardware
platforms. Through extensive experiments on 10 mainstream LLMs, we uncover
several key findings. Surprisingly, despite the simplicity of the cases,
DeepSeek-R1 achieves only a 55.6% pass@1 rate when provided with schematic
information, and 50.0% when tasked with generating the schematics itself. In
the cross-platform migration tasks, LLMs show relatively strong performance
with MicroPython on the Raspberry Pi Pico (with the top model achieving 73.8%
pass@1), but perform poorly on ESP-IDF, where the best model reaches only 29.4%
pass@1.Interestingly, we observe that general-purpose chat LLMs like
DeepSeek-V3 often fail to utilize relevant pre-trained knowledge in this
domain, while reasoning LLMs tend to overthink and overlook efficient knowledge
during pretraining. Based on these insights, we propose two strategies:
retrieval augmented generation and compiler feedback-to enhance LLM
performance. These strategies result in significant improvements, with
Deepseek-R1 reaching a 65.1% pass@1 with correct schematics, and 53.1% without.
Additionally, the accuracy of the Arduino to ESP32 migration task improves from
21.4% to 27.8%.

</details>


### [21] [Automated Extraction and Analysis of Developer's Rationale in Open Source Software](https://arxiv.org/abs/2506.11005)
*Mouna Dhaouadi,Bentley Oakes,Michalis Famelis*

Main category: cs.SE

TL;DR: The paper presents an automated system that uses Large Language Models to extract and analyze decision rationale in open source projects, helping developers avoid conflicting changes and maintain project coherence.


<details>
  <summary>Details</summary>
Motivation: Contributors to open source projects need to understand project history and rationale behind past decisions, but manually analyzing related changes is time-consuming and currently lacks automated support.

Method: The authors propose an automated approach for rationale analysis based on instantiating Kantara, a high-level rationale extraction and management architecture. This implementation uses pre-trained models, Large Language Models, and structure-based mechanisms to extract decision rationale and detect conflicts. The approach is tested on the OOM-Killer module of the Linux Kernel as well as five other active open source projects.

Result: The automated approach demonstrated reasonable performance in supporting rationale analysis, successfully identifying relationships, potential conflicts, and reasoning issues. The system also effectively extracted decision and rationale sentences, suggesting promise for generalization to other open source projects.

Conclusion: The proposed automated system for rationale extraction and conflict detection can assist open source developers in detecting hidden conflicts and reasoning problems, improving decision coherence and reducing design erosion, and is feasible for generalization across various projects.

Abstract: Contributors to open source software must deeply understand a project's
history to make coherent decisions which do not conflict with past reasoning.
However, inspecting all related changes to a proposed contribution requires
intensive manual effort, and previous research has not yet produced an
automated mechanism to expose and analyze these conflicts. In this article, we
propose such an automated approach for rationale analyses, based on an
instantiation of Kantara, an existing high-level rationale extraction and
management architecture. Our implementation leverages pre-trained models and
Large Language Models, and includes structure-based mechanisms to detect
reasoning conflicts and problems which could cause design erosion in a project
over time. We show the feasibility of our extraction and analysis approach
using the OOM-Killer module of the Linux Kernel project, and investigate the
approach's generalization to five other highly active open source projects. The
results confirm that our automated approach can support rationale analyses with
reasonable performance, by finding interesting relationships and to detect
potential conflicts and reasoning problems. We also show the effectiveness of
the automated extraction of decision and rationale sentences and the prospects
for generalizing this to other open source projects. This automated approach
could therefore be used by open source software developers to proactively
address hidden issues and to ensure that new changes do not conflict with past
decisions.

</details>


### [22] [Test code generation at Ericsson using Program Analysis Augmented Fine Tuned LLMs](https://arxiv.org/abs/2506.11006)
*Sai Krishna,Balvinder Singh,Sujoy Roychowdhury,Giriprasad Sridhara,Sourav Mazumdar,Magnus Sandelin,Dimitris Rentas,Maciej Nalepa,Karol Sawicki,Jakub Gajda*

Main category: cs.SE

TL;DR: The paper presents an approach for generating accurate Java test code from natural language test steps using LLMs enhanced with retrieval (RAG), prompt engineering, and model fine-tuning, resulting in significant improvements in code accuracy and efficiency, even with smaller models.


<details>
  <summary>Details</summary>
Motivation: Generating test code from natural language is valuable but challenging due to LLMs inventing functions/methods not present in the codebase. There is a need to reliably translate test steps in English to precise, workable Java code based on existing code context.

Method: The authors use Retrieval Augmented Generation (RAG) and prompt engineering to provide additional context from the codebase to the LLM, augmented with static program analysis. They also fine-tune the LLM using custom prompt templates containing pre-dependent classes, their methods, and exemplar RAG outputs.

Result: Their fine-tuned LLM improves correspondence with developer-written test code, as measured by F1-score for method usage. The fine-tuned 8x7b MoE model improves F1 by 8% over the base model and achieves scores close to the much larger 8x22b MoE model.

Conclusion: Combining RAG, prompt engineering, and custom fine-tuning significantly improves LLM performance in generating contextually appropriate test code from natural language, making smaller models perform comparably to larger models while increasing accuracy and adherence to the codebase.

Abstract: We describe test code generation using Large Language Models (LLMs) in
Ericsson. Our input is a test step in natural language (English) and our output
is code (Java) which accomplishes the test step. We describe how straight
forward prompting does not suffice and results in LLM assuming functions and
signatures which are not present in the code repository. We then show how we
alleviate the problem by a combination of Retrieval Augmented Generation (RAG)
along with prompt engineering that expanded the simple prompt with additional
contextual information using static program analysis. We then describe further
improvements that we obtained by fine-tuning the underlying LLM. The fine
tuning is done based on a custom designed prompt template which has
pre-dependent classes, their public methods as well two exemplar outputs
obtained from RAG. Our results establish that our fine tuned models help
improve the correspondence or conformity with the original developer written
test code as measured by the traditional metrics of F1-score based on the
methods used in the generated code. Fine tuning of a 8x7b Mixture of Experts
(MoE) model leads to an average improvement of 8\% over the base model and is
comparable to the scores on a much larger 8x22b MoE model.

</details>


### [23] [Impact of Comments on LLM Comprehension of Legacy Code](https://arxiv.org/abs/2506.11007)
*Rock Sabetto,Emily Escamilla,Devesh Agarwal,Sujay Kandwal,Justin F. Brunelle,Scott Rosen,Nitin Naik,Samruddhi Thaker,Eric O. Scott,Jacob Zimmer,Amit Madan,Arun Sridharan,Doug Wendt,Michael Doyle,Christopher Glasz,Jasper Phillips,William Macke,Colin Diggs,Michael Bartholf,Zachary Robin,Paul Ursino*

Main category: cs.SE

TL;DR: This paper explores how well large language models understand legacy programming code, especially when documentation is missing or inaccurate. Using multiple-choice questions to test comprehension, the authors find that documentation matters and suggest future research directions.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) are becoming increasingly important for software engineering tasks, but their ability to understand code in legacy programming languages is still uncertain. Real-world legacy systems often lack proper or accurate documentation, making it even harder to evaluate LLMs' performance on such code.

Method: The study uses multiple-choice question answering (MCQA) as a quantitative and efficient evaluation method. MCQA is used to assess how well LLMs understand legacy code and how the presence or inaccuracy of comments and documentation affect this comprehension.

Result: Preliminary findings indicate that the presence and quality of code documentation do impact LLM comprehension of legacy code, though detailed quantitative results are not specified in the abstract. The study also identifies areas for further investigation.

Conclusion: The research highlights the importance of proper code documentation for improving LLM comprehension of legacy languages and establishes MCQA as a promising evaluation methodology. It also sets forth objectives for further research in this area.

Abstract: Large language models (LLMs) have been increasingly integrated into software
engineering and maintenance tasks due to their high performance with software
engineering tasks and robust understanding of modern programming languages.
However, the ability of LLMs to comprehend code written with legacy languages
remains a research gap challenged by real-world legacy systems lacking or
containing inaccurate documentation that may impact LLM comprehension. To
assess LLM comprehension of legacy languages, there is a need for objective LLM
evaluation. In order to objectively measure LLM comprehension of legacy
languages, we need an efficient, quantitative evaluation method. We leverage
multiple-choice question answering (MCQA), an emerging LLM evaluation
methodology, to evaluate LLM comprehension of legacy code and the impact of
comment prevalence and inaccurate comments. In this work, we present
preliminary findings on the impact of documentation on LLM comprehension of
legacy code and outline strategic objectives for future work.

</details>


### [24] [Encoding Software For Perpetuity: A Compact Representation Of Apollo 11 Guidance Code](https://arxiv.org/abs/2506.11008)
*David Noever*

Main category: cs.SE

TL;DR: The paper presents a method to encode Apollo 11 Lunar Module guidance code into a single, scannable QR code using compression and selective preservation, making historic software instantly accessible through mobile devices without the need for internet or special hardware.


<details>
  <summary>Details</summary>
Motivation: There is a need to make historically significant software, such as Apollo 11 Lunar Module guidance computer code, accessible and preservable, especially through modern mobile devices, without dependence on specialized hardware or internet.

Method: The authors developed a novel encoding technique that uses tokenization, selective content preservation, and minimal HTML/JavaScript to compress the assembly code into a single 3KB QR code. They evaluated various compression strategies based on size, readability, and preservation of historical content.

Result: A successful demonstration of encoding key components of the Apollo 11 guidance computer assembly code into a compact, easily scannable QR code. The resulting digital artifact is both shareable and preservable, accessible through mobile devices without internet.

Conclusion: This method offers a new, accessible way to digitally preserve and share landmark computing artifacts, complementing traditional archival methods, and advances the field of software heritage preservation by leveraging contemporary mobile technology.

Abstract: This brief note presents a novel method for encoding historic Apollo 11 Lunar
Module guidance computer code into a single, compact Quick Response Code (QR
code) format, creating an accessible digital artifact for transmission and
archival purposes. By applying tokenization, selective content preservation,
and minimal HTML/JavaScript techniques, we successfully compressed key
components of the original Assembly Language Code (AGC) into a shareable,
preservable, and scannable 3 kilobyte (KB) image. We evaluate multiple
compression strategies and their tradeoffs in terms of size, readability, and
historical significance. This method addresses the challenge of making
historically significant software artifacts available through modern mobile
devices without requiring specialized hardware or internet connectivity. While
numerous digital preservation methods exist for historic software, this
approach balances accessibility with historical significance, offering a
complementary method to traditional archival techniques. This work contributes
to the broader field of computing heritage preservation by demonstrating how
landmark software can be made accessible instantly through contemporary mobile
technologies.

</details>


### [25] [Human-In-The-Loop Software Development Agents: Challenges and Future Directions](https://arxiv.org/abs/2506.11009)
*Jirat Pasuksmit,Wannita Takerngsaksiri,Patanamon Thongtanunam,Chakkrit Tantithamthavorn,Ruixiong Zhang,Shiyan Wang,Fan Jiang,Jing Li,Evan Cook,Kun Chen,Ming Wu*

Main category: cs.SE

TL;DR: The paper discusses deploying LLM-driven agents at Atlassian to automate Jira work item resolution, evaluating their code via functional tests and GPT-based scoring. It finds issues in testing costs and evaluation consistency, suggesting better assessment methods are needed.


<details>
  <summary>Details</summary>
Motivation: There is increasing interest in using multi-agent LLM-driven systems to boost software development productivity. Evaluating the effectiveness and practicality of these systems is crucial, particularly within enterprise environments like Atlassian.

Method: Human-in-the-Loop Software Development Agents were deployed to address Jira work items. The quality of code generated by these agents was assessed via functional correctness tests and similarity scoring by GPT models.

Result: Two primary challenges were identified: (1) high computational cost associated with unit testing, and (2) inconsistent outcomes in LLM-based evaluation approaches.

Conclusion: Future work should focus on developing improved and more efficient evaluation frameworks for Human-in-the-Loop software development systems.

Abstract: Multi-agent LLM-driven systems for software development are rapidly gaining
traction, offering new opportunities to enhance productivity. At Atlassian, we
deployed Human-in-the-Loop Software Development Agents to resolve Jira work
items and evaluated the generated code quality using functional correctness
testing and GPT-based similarity scoring. This paper highlights two major
challenges: the high computational costs of unit testing and the variability in
LLM-based evaluations. We also propose future research directions to improve
evaluation frameworks for Human-In-The-Loop software development tools.

</details>


### [26] [Enhancing Inventory Management with Progressive Web Applications (PWAs): A Scalable Solution for Small and Large Enterprises](https://arxiv.org/abs/2506.11011)
*Abhi Desai*

Main category: cs.SE

TL;DR: This paper presents a PWA that improves inventory management with cross-device access, barcode/QR code scanning, and geolocation features, offering a flexible and affordable solution. It discusses both advantages and limitations, serving as a guide for developers and enterprises considering PWAs for inventory systems.


<details>
  <summary>Details</summary>
Motivation: Enterprises need efficient inventory management to streamline operations and reduce costs, but existing solutions may lack cross-device accessibility, offline capabilities, and may be expensive or platform-dependent.

Method: The paper develops and implements a Progressive Web Application (PWA) with functionalities such as barcode/QR code scanning, geolocation-based warehouse identification, and cross-device access. It analyzes both technical implementation and performance considerations, comparing PWA to native apps.

Result: The PWA proved to be scalable, cost-effective, responsive, and capable of running offline across diverse platforms. However, the study notes that PWAs have some limitations in performance compared to native applications. Insights are given for future enterprise-level PWA development.

Conclusion: PWA technology offers an effective, adaptable, and affordable approach for inventory management, although some trade-offs exist in performance relative to native solutions. The paper provides valuable guidance for future adoption in enterprise environments.

Abstract: Efficient inventory management is crucial for both small and large
enterprises to optimize operational workflows and reduce overhead costs. This
paper explores the development and implementation of a Progressive Web
Application (PWA) designed to enhance the inventory management experience. The
application integrates key functionalities such as barcode and QR code
scanning, geolocation-based warehouse identification, and cross-device
accessibility. By leveraging PWA technology, the solution ensures offline
capabilities, responsive user experience, and seamless adaptability across
various platforms. The study discusses the challenges and benefits of
implementing PWA in inventory management systems, including its limitations in
performance compared to native applications. Insights from the development
process provide a roadmap for future developers looking to integrate PWA
technology into enterprise applications. This research contributes to the
growing domain of web-based inventory solutions, offering a scalable and
cost-effective alternative to traditional inventory management software.

</details>


### [27] [Toward a Brazilian Research Agenda in Quantum Software Engineering: A Systematic Mapping Study](https://arxiv.org/abs/2506.11013)
*Filipe Fernandes,Cláudia Werner*

Main category: cs.SE

TL;DR: This paper systematically maps current Quantum Software Engineering research, revealing fragmentation, a shortage of empirical studies, and little involvement from Brazil. It proposes a Brazilian research agenda to address gaps and foster greater participation in this emerging field.


<details>
  <summary>Details</summary>
Motivation: There is fragmented knowledge in QSE, with a lack of unified methodologies, tools, and guidelines tailored for quantum software, along with limited involvement from certain countries like Brazil. This fragmentary state hampers progress in the discipline.

Method: The study uses a systematic mapping approach, selecting and analyzing relevant papers according to set inclusion and exclusion criteria. Studies were categorized by study type, research focus, and alignment with recognized software engineering knowledge areas (SWEBOK).

Result: Research in QSE is mostly conceptual and technical, focusing on models, architecture, and software testing, but lacks empirical validation. Most research originates from outside Brazil, with scarce contributions from Brazilian scholars.

Conclusion: Quantum Software Engineering (QSE) is a promising but still immature field, requiring more standardized practices, empirical validation, and broader global participation. Brazilian involvement remains limited, pointing to the need for a national research agenda.

Abstract: Context: Quantum Software Engineering (QSE) has emerged as a key field to
support the development of reliable, maintainable, and scalable quantum
applications, bridging advances in quantum computing with established practices
in software engineering. Problem: Despite its growth, the field still suffers
from fragmented knowledge, with a lack of standardized methodologies, tools,
and guidelines tailored to the unique features of the quantum paradigm.
Additionally, countries like Brazil have had limited participation in the
development of this emerging domain. Objective: This study aims to map the
state of the art in QSE by identifying current research trends, recurring
contributions, and existing gaps that can guide future investigations and
strategic initiatives. Methodology: A systematic mapping study was conducted
analyzing selected publications based on inclusion and exclusion criteria.
Articles were categorized by study type, research type, and alignment with the
SWEBOK knowledge areas. Results: Most of the reviewed studies are primary
research articles written in English, with a strong focus on Software
Engineering Models and Methods, Software Architecture, and Software Testing.
Conceptual proposals and technical solutions predominate, while empirical
validations remain limited. Conclusions: Findings confirm that QSE is a
promising but still maturing field. The standardization of practices, expansion
of empirical studies, and inclusion of researchers from developing countries
are crucial for advancing the discipline. Additionally, Brazilian contributions
are still scarce, highlighting the urgent need to establish a national research
agenda. As a main contribution, this study proposes a Brazilian Research Agenda
in QSE, outlining priority areas and opportunities to foster a local scientific
community and accelerate progress in this emerging field.

</details>


### [28] [MultiMind: A Plug-in for the Implementation of Development Tasks Aided by AI Assistants](https://arxiv.org/abs/2506.11014)
*Benedetta Donato,Leonardo Mariani,Daniela Micucci,Oliviero Riganelli,Marco Somaschini*

Main category: cs.SE

TL;DR: MultiMind is a VSCode plugin framework that makes it easy for developers to integrate, use, and experiment with AI-powered coding assistants without complex IDE modifications, as demonstrated in two real-world use cases.


<details>
  <summary>Details</summary>
Motivation: Integrating AI assistants into software development, especially within IDEs, is challenging despite the demonstrated effectiveness of LLMs in coding tasks. Existing tools often require complex customizations to support new AI-powered interactions.

Method: The authors introduce MultiMind, a Visual Studio Code plug-in designed as a modular and extensible framework. It simplifies the development and integration of AI-assisted tasks into the IDE by enabling easy invocation and interaction with multiple AI assistants, handling output processing, and presenting feedback seamlessly.

Result: MultiMind was tested on two use cases: automatic generation of code comments and the definition of AI-powered chat, demonstrating its flexibility and utility for integrating AI into developer workflows.

Conclusion: MultiMind provides a practical, cost-effective solution for integrating and experimenting with AI-powered development tools in IDEs, lowering the barrier to entry for developers to leverage AI assistance.

Abstract: The integration of AI assistants into software development workflows is
rapidly evolving, shifting from automation-assisted tasks to collaborative
interactions between developers and AI. Large Language Models (LLMs) have
demonstrated their effectiveness in several development activities, including
code completion, test case generation, and documentation production. However,
embedding AI-assisted tasks within Integrated Development Environments (IDEs)
presents significant challenges. It requires designing mechanisms to invoke AI
assistants at the appropriate time, coordinate interactions with multiple
assistants, process the generated outputs, and present feedback in a way that
seamlessly integrates with the development workflow. To address these issues,
we introduce MultiMind, a Visual Studio Code plug-in that streamlines the
creation of AI-assisted development tasks. MultiMind provides a modular and
extensible framework, enabling developers to cost-effectively implement and
experiment with new AI-powered interactions without the need for complex IDE
customizations. MultiMind has been tested in two use cases: one for the
automatic generation of code comments and the other about the definition of
AI-powered chat.

</details>


### [29] [ZjsComponent: A Pragmatic Approach to Modular, Reusable UI Fragments for Web Development](https://arxiv.org/abs/2506.11016)
*Lelanthran Manickum*

Main category: cs.SE

TL;DR: ZjsComponent is a dependency-free, framework-agnostic web component for easily building modular, reusable UI elements directly in HTML, without the need for builds or special ecosystems.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to simplify the creation of modular and reusable UI elements, reducing developer overhead and eliminating dependencies, build steps, or complex setup.

Method: The method involves designing and implementing ZjsComponent, a framework-agnostic web component that can be used purely from HTML by leveraging standard browser support for JavaScript and Web Components.

Result: ZjsComponent enables dynamic loading and isolation of HTML and JS code fragments, provides dependency-free operation, and supports reusable interfaces with simple lifecycle hooks, along with code and DOM isolation.

Conclusion: ZjsComponent offers a straightforward, lightweight solution for creating reusable UI components without any build process or ecosystem requirements, making UI development simpler and more accessible.

Abstract: In this paper, I present ZjsComponent, a lightweight and framework-agnostic
web component designed for creating modular, reusable UI elements with minimal
developer overhead. ZjsComponent is an example implementation of an approach to
creating components and object instances that can be used purely from HTML.
Unlike traditional approaches to components, the approach implemented by
ZjsComponent does not require build-steps, transpiling, pre-compilation, any
specific ecosystem or any other dependency. All that is required is that the
browser can load and execute Javascript as needed by Web Components.
ZjsComponent allows dynamic loading and isolation of HTML+JS fragments,
offering developers a simple way to build reusable interfaces with ease. This
approach is dependency-free, provides significant DOM and code isolation, and
supports simple lifecycle hooks as well as traditional methods expected of an
instance of a class.

</details>


### [30] [Formation of requirements traceability in the process of information systems design](https://arxiv.org/abs/2506.11018)
*Grigory Tsiperman*

Main category: cs.SE

TL;DR: Tracing requirements is hard to integrate into design; the authors use a new clustering method connecting artifacts across abstraction levels to make traceability easier and less developer-dependent.


<details>
  <summary>Details</summary>
Motivation: Requirements traceability is crucial for software quality, enabling validation and verification, but is challenging to integrate smoothly into the design process of information systems.

Method: The paper proposes using the Adaptive Clustering Method (ACM), developed by the author, which offers seamless system architecture and explicit interconnection among project artifacts at different abstraction levels.

Result: Using ACM facilitates the integration of traceability into the system design process, improving traceability and making the system less dependent on individual developers.

Conclusion: The application of ACM effectively addresses the integration challenge of requirements traceability in the system design process by providing a clear and seamless connection between project artifacts.

Abstract: The traceability of requirements in the information system design process is
considered an essential property of the project, one of its quality
characteristics. The point here is that traceability provides the methods of
validation and verification of software systems, and that the system model
based on requirements traceability reduces the system's dependence on
developers and, in general, makes it as straightforward as possible. One of the
challenges of the traceability process, dubbed "The grand challenge of
traceability" among traceability researchers, is its integration into the
design process. In this paper, to achieve this goal, we propose the application
of the Adaptive Clustering Method (ACM) of Information Systems developed by the
author, which is based on the idea of a seamless system architecture that
provides explicit interconnection of project artifacts of different levels of
abstraction.

</details>


### [31] [Mind the Metrics: Patterns for Telemetry-Aware In-IDE AI Application Development using the Model Context Protocol (MCP)](https://arxiv.org/abs/2506.11019)
*Vincent Koc,Jacques Verre,Douglas Blank,Abigail Morgan*

Main category: cs.SE

TL;DR: The paper introduces telemetry-aware IDEs powered by the Model Context Protocol, integrating real-time AI telemetry into developer workflows. With the open-source Opik server, the authors demonstrate improved LLM development practices, setting a foundation for future AI observability tools and research.


<details>
  <summary>Details</summary>
Motivation: There is a growing need for AI development environments that can provide real-time observability, such as telemetry, prompt traces, and evaluation feedback, to streamline and improve the developer workflow for AI and LLM applications.

Method: The authors propose telemetry-aware IDEs enabled by the Model Context Protocol (MCP), which connects the IDE with prompt metrics, trace logs, and version control for real-time refinement. They present design patterns for prompt iteration, CI-based optimization, and the use of autonomous agents that adapt to telemetry. The architecture supports integration with existing frameworks and is demonstrated through Opik, an open source MCP server.

Result: The paper demonstrates the feasibility of the MCP architecture through Opik and shows its compatibility with popular LLMOps frameworks. This approach enables real-time prompt refinement, telemetry-driven development, and paves the way for advanced tools in AI workflows.

Conclusion: This work establishes a foundational architecture for observability-first AI development platforms, facilitating prompt optimization, rich developer feedback, and empirical benchmarking within AI workflows. It also highlights future directions for research in telemetry-rich AI development and the LLMOps ecosystem.

Abstract: AI development environments are evolving into observability first platforms
that integrate real time telemetry, prompt traces, and evaluation feedback into
the developer workflow. This paper introduces telemetry aware integrated
development environments (IDEs) enabled by the Model Context Protocol (MCP), a
system that connects IDEs with prompt metrics, trace logs, and versioned
control for real time refinement. We present design patterns for local prompt
iteration, CI based optimization, and autonomous agents that adapt behavior
using telemetry. Rather than focusing on a single algorithm, we describe an
architecture that supports integration with frameworks like DSPy, PromptWizard,
and Prompts as Programs. We demonstrate this through Opik, an open source MCP
server for LLM telemetry, and position our approach within the emerging LLMOps
ecosystem. This work lays a foundation for future research on prompt
optimization, IDE agent tooling, and empirical benchmarking in telemetry rich
AI development workflows.

</details>


### [32] [Extracting Knowledge Graphs from User Stories using LangChain](https://arxiv.org/abs/2506.11020)
*Thayná Camargo da Silva*

Main category: cs.SE

TL;DR: This thesis presents an automated method to generate knowledge graphs from user stories using LLMs and LangChain, enabling better visualization and alignment of requirements, and supporting more effective user-centric software development.


<details>
  <summary>Details</summary>
Motivation: Extracting knowledge from user stories is crucial for aligning software functionality with user needs, but manual extraction is tedious and error-prone.

Method: A User Story Graph Transformer module utilizing Large Language Models (LLMs) with the LangChain framework was developed to extract nodes and relationships from user stories, automate knowledge graph creation, and evaluate results using an annotated dataset.

Result: The approach successfully automated the extraction and evaluation of knowledge graphs from user stories, improving the visualization and understanding of user requirements and domain concepts.

Conclusion: The methodology enables fully automated, accurate knowledge graph generation from user stories, fostering improved alignment between software and user expectations and supporting more user-centric software development.

Abstract: This thesis introduces a novel methodology for the automated generation of
knowledge graphs from user stories by leveraging the advanced capabilities of
Large Language Models. Utilizing the LangChain framework as a basis, the User
Story Graph Transformer module was developed to extract nodes and relationships
from user stories using an LLM to construct accurate knowledge graphs.This
innovative technique was implemented in a script to fully automate the
knowledge graph extraction process. Additionally, the evaluation was automated
through a dedicated evaluation script, utilizing an annotated dataset for
assessment. By enhancing the visualization and understanding of user
requirements and domain concepts, this method fosters better alignment between
software functionalities and user expectations, ultimately contributing to more
effective and user-centric software development processes.

</details>


### [33] [Eliminating Hallucination-Induced Errors in LLM Code Generation with Functional Clustering](https://arxiv.org/abs/2506.11021)
*Chaitanya Ravuri,Saman Amarasinghe*

Main category: cs.SE

TL;DR: Functional clustering is a black-box method that drastically reduces hallucination errors in code-generation LLMs, providing tunable reliability without modifying models. It offers a significant step toward safe, autonomous code generation and is readily usable by the community.


<details>
  <summary>Details</summary>
Motivation: Current large language models (LLMs) for code generation can solve many programming tasks but often introduce subtle, hard-to-catch bugs, making their outputs unreliable for unsupervised use.

Method: The paper introduces 'functional clustering,' a black-box wrapper that samples many code outputs from the model, executes them with auto-generated tests, and clusters outputs by identical input/output (I/O) behavior. The largest cluster's size provides a confidence estimate, which users can threshold to balance reliability and coverage.

Result: Functional clustering dramatically reduces the proportion of hallucination-induced errors. On the LiveCodeBench benchmark, it keeps the correct answer rate similar to the baseline but reduces the error rate from approximately 65% to 2%. Setting a conservative confidence threshold eliminates all errors in 15.6% of cases. Manual review shows that remaining faults are due to ambiguous prompts, narrowing the future challenge to prompt specification.

Conclusion: Functional clustering offers a practical and model-agnostic way to make code-generation LLMs reliable for autonomous deployment. The approach is robust, does not require model internals, and can be used with any model or API. Code is publicly available for use and further research.

Abstract: Modern code-generation LLMs can already solve a large fraction of programming
problems, yet they still hallucinate subtle bugs that make their outputs unsafe
for autonomous deployment. We present functional clustering, a black-box
wrapper that eliminates nearly all hallucination-induced errors while providing
a tunable confidence score. The wrapper samples many candidate programs,
executes each on a self-generated test suite, and clusters candidates whose I/O
behavior is identical; the empirical mass of the largest cluster serves as an
exact confidence estimate. A single scalar threshold on this estimate lets
users trade coverage for reliability with exponential guarantees. On
LiveCodeBench our verifier preserves baseline pass@1 on solvable tasks yet
slashes the error rate of returned answers from ~65% to 2%, and drives it to 0%
at a conservative threshold while still answering 15.6% of prompts. Manual
audits show that the few residual mistakes stem from prompt misinterpretation,
not random generation noise, narrowing future work to specification clarity.
Because the method requires only sampling and sandbox execution, it applies
unchanged to closed-source APIs and future models, offering a practical path
toward dependable, autonomous code generation. Our code is available on Github
(https://github.com/20ChaituR/functional-clustering).

</details>


### [34] [Security Degradation in Iterative AI Code Generation -- A Systematic Analysis of the Paradox](https://arxiv.org/abs/2506.11022)
*Shivani Shukla,Himanshu Joshi,Romilla Syed*

Main category: cs.SE

TL;DR: Repeatedly refining LLM-generated code can make it less secure, not more. Human review is vital to prevent security issues from accumulating during code iteration.


<details>
  <summary>Details</summary>
Motivation: The motivation is to investigate how security vulnerabilities change during iterative feedback and refinement of code generated by Large Language Models (LLMs), a topic previously underexplored despite the widespread use of LLMs in software development.

Method: The study conducted a controlled experiment using 400 code samples subjected to 40 rounds of iterative improvements via four distinct prompting strategies. The security of the code samples was analyzed after each round to observe any evolution in vulnerabilities.

Result: A 37.6% increase in critical security vulnerabilities was observed after just five iterations. Different prompting strategies led to distinct patterns in the emergence of vulnerabilities, suggesting that not all refinement strategies are equally secure.

Conclusion: Iterative refinement of LLM-generated code does not necessarily improve—and may actually decrease—security. Human expertise remains essential, and robust human validation should be emphasized between LLM iterations to avoid unintentionally introducing new security risks.

Abstract: The rapid adoption of Large Language Models(LLMs) for code generation has
transformed software development, yet little attention has been given to how
security vulnerabilities evolve through iterative LLM feedback. This paper
analyzes security degradation in AI-generated code through a controlled
experiment with 400 code samples across 40 rounds of "improvements" using four
distinct prompting strategies. Our findings show a 37.6% increase in critical
vulnerabilities after just five iterations, with distinct vulnerability
patterns emerging across different prompting approaches. This evidence
challenges the assumption that iterative LLM refinement improves code security
and highlights the essential role of human expertise in the loop. We propose
practical guidelines for developers to mitigate these risks, emphasizing the
need for robust human validation between LLM iterations to prevent the
paradoxical introduction of new security issues during supposedly beneficial
code "improvements".

</details>


### [35] [Software Security Mapping Framework: Operationalization of Security Requirements](https://arxiv.org/abs/2506.11051)
*Sung Une Lee,Liming Dong,Zhenchang Xing,Muhammad Ejaz Ahmed,Stefan Avgoustakis*

Main category: cs.SE

TL;DR: This paper presents a comprehensive framework and toolkit that translates high-level software supply chain security requirements into practical, trackable development practices, addressing real-world risks and facilitating compliance and automation in organizations.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of software development environments has raised concerns about supply chain security. Existing frameworks fail to effectively translate high-level security concepts into actionable developer practices, leaving a gap in practical implementation.

Method: The authors developed the Software Security Mapping Framework through collaboration with academic experts and industry practitioners. They systematically mapped 131 security requirements to over 400 operational steps using the KAOS goal modeling methodology, and validated the framework in a real-world case study. They also provided a web tool and a machine-readable OSCAL Catalog Model for interactive use and automation.

Result: The framework enables traceable and practical mapping from high-level regulatory standards to technical activities, structured around four security goals. The Log4j case study demonstrated its effectiveness in generating an actionable, best practices checklist. The OSCAL Catalog Model and web tool further facilitate adoption and automation for security and compliance.

Conclusion: The Software Security Mapping Framework bridges the gap between abstract security mandates and actionable development practices, providing organizations with a comprehensive, structured approach to operationalizing supply chain security requirements and enhancing software security management.

Abstract: The escalating complexity of modern software development environments has
heightened concerns around supply chain security. However, existing frameworks
often fall short in translating abstract security principles into concrete,
actionable practices. This paper introduces the Software Security Mapping
Framework, a structured solution designed to operationalize security
requirements across hierarchical levels -- from high-level regulatory standards
(e.g., ISM, Australia cybersecurity standard published by the Australian
Signals Directorate), through mid-level frameworks (e.g., NIST SSDF, the U.S.
Secure Software Development Framework), to fine-grained technical activities
(e.g., SLSA, a software supply chain security framework). Developed through
collaborative research with academic experts and industry practitioners, the
framework systematically maps 131 refined security requirements to over 400
actionable operational steps spanning the software development lifecycle. It is
grounded in four core security goals: Secure Software Environment, Secure
Software Development, Software Traceability, and Vulnerability Management. Our
approach leverages the KAOS goal modeling methodology to establish traceable
linkages between strategic goals and tactical operations, enhancing clarity,
accountability, and practical implementation. To facilitate adoption, we
provide a web-based navigation tool for interactive exploration of the
framework. A real-world case study based on the Log4j vulnerability illustrates
the framework's utility by generating a tailored checklist aligned with
industry best practices. Additionally, we offer a structured, machine-readable
OSCAL Catalog Model of the Software Security Mapping Framework, enabling
organizations to automate implementation, streamline compliance processes, and
respond effectively to evolving security risks.

</details>


### [36] [Refactoring Codebases through Library Design](https://arxiv.org/abs/2506.11058)
*Ziga Kovacic,Celine Lee,Justin Chiu,Wenting Zhao,Kevin Ellis*

Main category: cs.SE

TL;DR: Librarian is a new method that helps code agents refactor and consolidate code into reusable libraries, with a new benchmark (Minicode) proving it outperforms existing solutions in both code efficiency and correctness.


<details>
  <summary>Details</summary>
Motivation: Modern software development demands maintainable and general solutions, but transitioning specialized code into reusable components is challenging. This need is heightened as code agents excel at solving isolated tasks but struggle with broader software engineering goals such as refactoring for reusability.

Method: The authors propose a sample-and-rerank method called Librarian for generating reusable libraries and introduce a new benchmark, Minicode, where code agents must refactor and consolidate multiple solutions into a single library. The methodology involves evaluating code agents on their ability to minimize code and enhance reusability while maintaining correctness.

Result: Librarian significantly outperforms state-of-the-art code agents both in terms of code compression (achieving 1.6-2x better rates) and correctness on the Minicode benchmark.

Conclusion: Librarian, along with the Minicode benchmark, advances the field of AI-assisted code refactoring for reusability, showing that automated agents can be improved for software engineering tasks beyond isolated problem-solving.

Abstract: Maintainable and general software allows developers to build robust
applications efficiently, yet achieving these qualities often requires
refactoring specialized solutions into reusable components. This challenge
becomes particularly relevant as code agents become increasingly accurate at
solving isolated programming problems. We investigate code agents' capacity to
refactor code in ways supporting growth and reusability. We present both a
method and a benchmark for refactoring: Librarian, a sample-and-rerank method
for generating reusable libraries, and Minicode, a benchmark where code agents
must minimize and refactor multiple independent solutions into a joint library.
Compared to state-of-the-art code agents, Librarian achieves strong results on
both compression and correctness on Minicode, obtaining compression rates
1.6-2x better than coding agents while also improving correctness. We
open-source our code and benchmark at https://code-refactor.github.io/.

</details>


### [37] [CodeMirage: A Multi-Lingual Benchmark for Detecting AI-Generated and Paraphrased Source Code from Production-Level LLMs](https://arxiv.org/abs/2506.11059)
*Hanxi Guo,Siyuan Cheng,Kaiyuan Zhang,Guangyu Shen,Xiangyu Zhang*

Main category: cs.SE

TL;DR: The paper introduces CodeMirage, a large benchmark addressing the shortcomings of existing datasets for detecting AI-generated code. Spanning ten languages and multiple LLMs, it provides rigorous evaluation of detectors, revealing important insights and guiding the development of better detection tools.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the growing prevalence of AI-generated source code by large language models (LLMs) in modern software development, which introduces risks such as plagiarism, license violations, and insecure code. Existing benchmarks for detecting AI-generated code are limited and do not adequately reflect real-world scenarios.

Method: The authors developed CodeMirage, a benchmark designed to evaluate AI-generated code detectors more rigorously. CodeMirage covers ten popular programming languages, includes both original and paraphrased code samples, and features outputs from ten state-of-the-art LLMs. They used CodeMirage to test ten different detectors across four methodological paradigms, using four evaluation settings and three metrics.

Result: The analysis led to nine key findings on the performance of current code detectors, highlighting both their strengths and shortcomings, and pinpointing important challenges for future research.

Conclusion: CodeMirage serves as a comprehensive and practical benchmark that will facilitate the advancement of robust and generalizable detectors for AI-generated code.

Abstract: Large language models (LLMs) have become integral to modern software
development, producing vast amounts of AI-generated source code. While these
models boost programming productivity, their misuse introduces critical risks,
including code plagiarism, license violations, and the propagation of insecure
programs. As a result, robust detection of AI-generated code is essential. To
support the development of such detectors, a comprehensive benchmark that
reflects real-world conditions is crucial. However, existing benchmarks fall
short -- most cover only a limited set of programming languages and rely on
less capable generative models. In this paper, we present CodeMirage, a
comprehensive benchmark that addresses these limitations through three major
advancements: (1) it spans ten widely used programming languages, (2) includes
both original and paraphrased code samples, and (3) incorporates outputs from
ten state-of-the-art production-level LLMs, including both reasoning and
non-reasoning models from six major providers. Using CodeMirage, we evaluate
ten representative detectors across four methodological paradigms under four
realistic evaluation configurations, reporting results using three
complementary metrics. Our analysis reveals nine key findings that uncover the
strengths and weaknesses of current detectors, and identify critical challenges
for future work. We believe CodeMirage offers a rigorous and practical testbed
to advance the development of robust and generalizable AI-generated code
detectors.

</details>


### [38] [Code Researcher: Deep Research Agent for Large Systems Code and Commit History](https://arxiv.org/abs/2506.11060)
*Ramneet Singh,Sathvik Joel,Abhav Mehrotra,Nalin Wadhwa,Ramakrishna B Bairi,Aditya Kanade,Nagarajan Natarajan*

Main category: cs.SE

TL;DR: Code Researcher is a new LLM-based coding agent designed to patch crashes in large systems codebases by deeply gathering code context and commit history. It outperforms previous agents, particularly in complex settings like the Linux kernel, underscoring the need for comprehensive context understanding in automated code repair.


<details>
  <summary>Details</summary>
Motivation: LLM-based coding agents have shown promise, but their effectiveness in the complex domain of systems code (such as making changes to large, intricate codebases like the Linux kernel) is not well understood. Successfully patching systems code requires synthesizing a lot of contextual information, which is a significant challenge for both humans and AI agents.

Method: The authors introduce 'Code Researcher,' a deep research agent for code. It performs multi-step reasoning over code semantics, coding patterns, and historical commits, storing gathered context in a structured memory to synthesize patches. The agent's performance is evaluated on the kBenchSyz benchmark (Linux kernel crashes) and on open-source multimedia software to test generalizability.

Result: Code Researcher significantly outperforms baselines like the SWE-agent, achieving a 58% crash-resolution rate (vs. 37.5% for SWE-agent) on Linux kernel patching tasks. It explores notably more files per trajectory, demonstrating deeper exploration and superior context gathering. Code Researcher also generalizes to other codebases beyond the kernel.

Conclusion: Global context gathering and multi-step, multi-faceted reasoning are crucial for effectively patching large systems codebases; Code Researcher demonstrates strong capabilities in these aspects and sets a new state-of-the-art on related benchmarks.

Abstract: Large Language Model (LLM)-based coding agents have shown promising results
on coding benchmarks, but their effectiveness on systems code remains
underexplored. Due to the size and complexities of systems code, making changes
to a systems codebase is a daunting task, even for humans. It requires
researching about many pieces of context, derived from the large codebase and
its massive commit history, before making changes. Inspired by the recent
progress on deep research agents, we design the first deep research agent for
code, called Code Researcher, and apply it to the problem of generating patches
for mitigating crashes reported in systems code. Code Researcher performs
multi-step reasoning about semantics, patterns, and commit history of code to
gather sufficient context. The context is stored in a structured memory which
is used for synthesizing a patch. We evaluate Code Researcher on kBenchSyz, a
benchmark of Linux kernel crashes, and show that it significantly outperforms
strong baselines, achieving a crash-resolution rate of 58%, compared to 37.5%
by SWE-agent. On an average, Code Researcher explores 10 files in each
trajectory whereas SWE-agent explores only 1.33 files, highlighting Code
Researcher's ability to deeply explore the codebase. Through another experiment
on an open-source multimedia software, we show the generalizability of Code
Researcher. Our experiments highlight the importance of global context
gathering and multi-faceted reasoning for large codebases.

</details>


### [39] [CoQuIR: A Comprehensive Benchmark for Code Quality-Aware Information Retrieval](https://arxiv.org/abs/2506.11066)
*Jiahui Geng,Fengyu Cai,Shaobo Cui,Qing Li,Liangwei Chen,Chenyang Lyu,Haonan Li,Derui Zhu,Walter Pretschner,Heinz Koeppl,Fakhri Karray*

Main category: cs.SE

TL;DR: The paper introduces CoQuIR, a comprehensive benchmark for evaluating code retrieval systems on code quality, not just functional relevance. Results show current models often miss low-quality code, but new quality-aware approaches perform better, emphasizing the need for quality integration in future tools.


<details>
  <summary>Details</summary>
Motivation: Current code retrieval benchmarks focus mainly on functional relevance, overlooking essential software quality aspects such as correctness, efficiency, security, and maintainability. This creates a gap in the evaluation and development of retrieval systems that need to recognize and prioritize high-quality code.

Method: The paper introduces CoQuIR, a multilingual benchmark with 42,725 queries and 134,907 code snippets annotated for four code quality dimensions across 11 programming languages. Two new evaluation metrics, Pairwise Preference Accuracy and Margin-based Ranking Score, are proposed. The authors evaluate 23 code retrieval models and explore training methods using synthetic datasets to enhance quality-aware code retrieval.

Result: Existing top-performing retrieval models often cannot distinguish between insecure or buggy code and higher quality alternatives. The experiments with quality-aware training methods show promising improvements in quality sensitive metrics while maintaining semantic retrieval relevance, and the approach is also validated through downstream code generation tasks.

Conclusion: Integrating code quality signals into code retrieval systems is both necessary and effective, demonstrating improved trustworthiness and robustness in retrieval and generation tasks. CoQuIR sets a foundation for research in quality-aware software tools.

Abstract: Code retrieval is essential in modern software development, as it boosts code
reuse and accelerates debugging. However, current benchmarks primarily
emphasize functional relevance while neglecting critical dimensions of software
quality. Motivated by this gap, we introduce CoQuIR, the first large-scale,
multilingual benchmark specifically designed to evaluate quality-aware code
retrieval across four key dimensions: correctness, efficiency, security, and
maintainability. CoQuIR provides fine-grained quality annotations for 42,725
queries and 134,907 code snippets in 11 programming languages, and is
accompanied by two quality-centric evaluation metrics: Pairwise Preference
Accuracy and Margin-based Ranking Score. Using CoQuIR, we benchmark 23
retrieval models, covering both open-source and proprietary systems, and find
that even top-performing models frequently fail to distinguish buggy or
insecure code from their more robust counterparts. Furthermore, we conduct
preliminary investigations into training methods that explicitly encourage
retrievers to recognize code quality. Using synthetic datasets, we demonstrate
promising improvements in quality-aware metrics across various models, without
sacrificing semantic relevance. Downstream code generation experiments further
validate the effectiveness of our approach. Overall, our work highlights the
importance of integrating quality signals into code retrieval systems, laying
the groundwork for more trustworthy and robust software development tools.

</details>


### [40] [DCE-LLM: Dead Code Elimination with Large Language Models](https://arxiv.org/abs/2506.11076)
*Minyu Chen,Guoqiang Li,Ling-I Wu,Ruibang Liu*

Main category: cs.SE

TL;DR: DCE-LLM is a new automated framework for detecting and fixing dead code by combining CodeBERT and LLMs, outperforming existing tools and GPT-4o by a large margin in both accuracy and language support.


<details>
  <summary>Details</summary>
Motivation: Dead code in software complicates maintenance, increases binary size, introduces security risks for LLM-based code tasks, and can be exploited by malware. Current elimination tools have limited effectiveness and require much manual intervention, necessitating a more comprehensive automated solution.

Method: The paper proposes DCE-LLM, a framework that uses a lightweight CodeBERT model with an attribution-based line selector to identify suspicious dead code lines. Large Language Models are then employed to generate explanations and patches, trained on a large annotated dead code dataset. The approach automates classification, location, explanation, and correction of dead code.

Result: DCE-LLM achieves over 94% F1 scores in detecting unused and unreachable code and significantly surpasses GPT-4o by 30%. It offers advanced features including detection of complex dead code patterns, automated corrections, and multi-language support.

Conclusion: DCE-LLM provides a comprehensive and automated solution for dead code elimination, outperforming state-of-the-art tools and reducing the need for manual effort in handling dead code.

Abstract: Dead code introduces several challenges in software development, such as
increased binary size and maintenance difficulties. It can also obscure logical
errors and be exploited for obfuscation in malware. For LLM-based code-related
tasks, dead code introduces vulnerabilities that can mislead these models,
raising security concerns. Although modern compilers and IDEs offer dead code
elimination, sophisticated patterns can bypass these tools. A universal
approach that includes classification, location, explanation, and correction is
needed, yet current tools often require significant manual effort. We present
DCE-LLM, a framework for automated dead code elimination using a small CodeBERT
model with an attribution-based line selector to efficiently locate suspect
code. LLMs then generate judgments and explanations, fine-tuned on a
large-scale, annotated dead code dataset to provide detailed explanations and
patches. DCE-LLM outperforms existing tools, with advanced unreachability
detection, automated correction, and support for multiple programming
languages. Experimental results show DCE-LLM achieves over 94% F1 scores for
unused and unreachable code, significantly surpassing GPT-4o by 30%.

</details>


### [41] [Research and Analysis of Employers' Opinion on the Necessary Skills that Students in the Field of Web Programming Should Possess](https://arxiv.org/abs/2506.11084)
*Yordan Kalmukov*

Main category: cs.SE

TL;DR: This paper surveys IT employers to determine which Web Programming skills are most important for new graduates, weighing the value of foundational coding knowledge versus proficiency with modern tools and frameworks in a rapidly automating industry.


<details>
  <summary>Details</summary>
Motivation: In the AI era, programming is increasingly automated using frameworks, libraries, and APIs, prompting a reevaluation of what skills are most valuable for IT graduates. There is a debate between focusing on teaching students to use these tools or on foundational programming skills.

Method: The paper analyzes the results of a survey conducted among IT employers to identify the technical skills they consider essential for graduating students in Web Programming to quickly and effectively integrate into company work.

Result: The survey results highlight employers' perspectives on the specific skills and knowledge that new IT graduates should have in the area of Web Programming, helping clarify current industry expectations.

Conclusion: Understanding employer requirements can inform educational priorities, ensuring that curriculum in Web Programming aligns with the rapidly changing demands of the IT industry in the AI and automation age.

Abstract: In the era of artificial intelligence (AI) and chatbots, based on large
language models that can generate programming code in any language, write texts
and summarize information, it is obvious that the requirements of employers for
graduating students have already changed. The modern IT world offers
significant automation of programming through software frameworks and a huge
set of third-party libraries and application programming interfaces (APIs). All
these tools provide most of the necessary functionality out of the box (already
implemented), and quite naturally the question arises as to what is more useful
for students - to teach how to use these ready-made tools or the basic
principles of working and development of web applications from scratch. This
paper analyzes the results of a survey conducted among IT employers, aimed to
identify what, in their opinion, are the necessary technical skills that
graduating students in the field of Web Programming should possess in order to
join the company's work as quickly and effectively as possible.

</details>


### [42] [LeanExplore: A search engine for Lean 4 declarations](https://arxiv.org/abs/2506.11085)
*Justin Asher*

Main category: cs.SE

TL;DR: LeanExplore is a new semantic search engine for Lean 4 libraries, combining AI-driven and traditional search approaches. It makes finding definitions and theorems easier, is accessible via web or API, can be self-hosted, integrates with AI tools, and improves both human and automated research workflows.


<details>
  <summary>Details</summary>
Motivation: The growing Lean 4 ecosystem has made it difficult for users to effectively find and navigate relevant declarations and statements within its large libraries. Consequently, there's a need for a robust, user-friendly search engine that accommodates both formal and informal queries, improving accessibility and usability for mathematicians, researchers, and AI applications.

Method: The paper introduces LeanExplore, which uses a hybrid ranking strategy for semantic search. This strategy combines: (1) multi-source semantic embedding models for conceptual understanding of Lean code, docstrings, AI-generated informal translations, and declaration titles; (2) BM25+ for keyword-based lexical matching; and (3) a PageRank-based score to reflect the importance and connectivity of declarations. The system is accessible via a web interface and Python API and can be self-hosted. It also provides easy integration with LLMs via the model context protocol (MCP).

Result: LeanExplore enables effective semantic and lexical search across major Lean 4 packages, enhancing discoverability of declarations. It is available as a web-based tool, Python API, and downloadable database, and can be integrated with AI and theorem-proving agents. The paper describes its architecture, data processing pipeline, and its integration with LLMs and user workflows.

Conclusion: LeanExplore represents an important tool for the Lean 4 community, improving accessibility, navigation, and searchability in the expanding ecosystem. Its hybrid ranking and integration with AI assistants facilitate more efficient research and theorem proving, promising to benefit both humans and automated agents.

Abstract: The expanding Lean 4 ecosystem poses challenges for navigating its vast
libraries. This paper introduces LeanExplore, a search engine for Lean 4
declarations. LeanExplore enables users to semantically search for statements,
both formally and informally, across select Lean 4 packages (including
Batteries, Init, Lean, Mathlib, PhysLean, and Std). This search capability is
powered by a hybrid ranking strategy, integrating scores from a multi-source
semantic embedding model (capturing conceptual meaning from formal Lean code,
docstrings, AI-generated informal translations, and declaration titles), BM25+
for keyword-based lexical relevance, and a PageRank-based score reflecting
declaration importance and interconnectedness. The search engine is accessible
via a dedicated website (https://www.leanexplore.com/) and a Python API
(https://github.com/justincasher/lean-explore). Furthermore, the database can
be downloaded, allowing users to self-host the service. LeanExplore integrates
easily with LLMs via the model context protocol (MCP), enabling users to chat
with an AI assistant about Lean declarations or utilize the search engine for
building theorem-proving agents. This work details LeanExplore's architecture,
data processing, functionalities, and its potential to enhance Lean 4 workflows
and AI-driven mathematical research

</details>


### [43] [Denoising Programming Knowledge Tracing with a Code Graph-based Tuning Adaptor](https://arxiv.org/abs/2506.11107)
*Weibo Gao,Qi Liu,Rui Li,Yuze Zhao,Hao Wang,Linan Yre,Fangzhou Yao,Zheng Zhang*

Main category: cs.SE

TL;DR: Coda uses code graphs and GCN to reduce noise from unrelated or minor coding activities in programming knowledge tracking, leading to improved, model-agnostic tracking accuracy on real coding data.


<details>
  <summary>Details</summary>
Motivation: Existing PKT models struggle with noise in learners' programming data — especially unrelated submissions and small code tweaks — which hampers performance and practical application.

Method: Coda introduces a code graph to structure learners' sequential code submissions, applies cluster-aware Graph Convolutional Networks (GCN) for better identification of signal types, and integrates a lightweight adaptor with noise-sensitive constraints and regularization to correct misdiagnosed knowledge states. The framework operates independently of specific PKT models.

Result: Coda significantly improves PKT performance on noisy programming datasets, outperforming typical baseline models across four real-world datasets.

Conclusion: Coda provides an effective, model-agnostic approach for enhancing PKT by identifying and mitigating noise, offering superior knowledge tracking under realistic conditions.

Abstract: Programming Knowledge Tracking (PKT) aims to dynamically diagnose learners'
mastery levels of programming knowledge based on their coding activities,
facilitating more effective and personalized programming education. However,
current PKT studies primarily focus on the implicit relationship between code
content and knowledge assessment, often overlooking two types of noise signals
in long-term programming activities: unwanted signals from unrelated
submissions and weak signals from minor modifications. This practical challenge
significantly limits model performance and application. To address this issue,
we propose Coda, a Code graph-based tuning adaptor designed to enhance existing
PKT models by identifying and mitigating the impact of noise. Specifically,
Coda first transforms the loose code sequences submitted by each learner into a
compact code graph. By leveraging this code graph, unwanted signals can be
identified from a semantic similarity perspective. We then apply a
cluster-aware GCN to the code graph, which improves the discrimination of weak
signals and enables their clustering for identification. Finally, a lightweight
yet effective adaptor is incorporated into the PKT task through optimization
with two noise feature-based constraints and a navigational regularization
term, to correct knowledge states affected by noise. It is worth mentioning
that the Coda framework is model-agnostic and can be adapted to most existing
PKT solutions. Extensive experimental results on four real-world datasets
demonstrate that Coda effectively performs the PKT task in the presence of
noisy programming records, outperforming typical baselines.

</details>


### [44] [From over-reliance to smart integration: using Large-Language Models as translators between specialized modeling and simulation tools](https://arxiv.org/abs/2506.11141)
*Philippe J. Giabbanelli,John Beverley,Istvan David,Andreas Tolk*

Main category: cs.SE

TL;DR: The paper suggests using LLMs as smart middleware in modeling and simulation, connecting existing tools more effectively. This improves workflow efficiency and reliability without replacing specialized tools, thanks to careful integration strategies.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) can simplify Modeling & Simulation (M&S) workflows, but their unrestrained use introduces risks such as logical errors and hallucinations.

Method: The paper proposes using LLMs as middleware or translators that connect specialized M&S tools. It discusses structured workflow integration, focusing on efficient software architecture using Low-Rank Adaptation-based approaches.

Result: LLMs, when used as translators, improve interoperability across diverse M&S systems and tools, reducing complexity without introducing performance issues.

Conclusion: Integrating LLMs as complementary middleware, not replacements, leads to more robust, high-quality M&S processes. Structured integration and task-specific adaptations are essential.

Abstract: Large Language Models (LLMs) offer transformative potential for Modeling &
Simulation (M&S) through natural language interfaces that simplify workflows.
However, over-reliance risks compromising quality due to ambiguities, logical
shortcuts, and hallucinations. This paper advocates integrating LLMs as
middleware or translators between specialized tools to mitigate complexity in
M&S tasks. Acting as translators, LLMs can enhance interoperability across
multi-formalism, multi-semantics, and multi-paradigm systems. We address two
key challenges: identifying appropriate languages and tools for modeling and
simulation tasks, and developing efficient software architectures that
integrate LLMs without performance bottlenecks. To this end, the paper explores
LLM-mediated workflows, emphasizes structured tool integration, and recommends
Low-Rank Adaptation-based architectures for efficient task-specific
adaptations. This approach ensures LLMs complement rather than replace
specialized tools, fostering high-quality, reliable M&S processes.

</details>


### [45] [Mutual-Supervised Learning for Sequential-to-Parallel Code Translation](https://arxiv.org/abs/2506.11153)
*Changxin Ke,Rui Zhang,Shuo Wang,Li Ding,Guangli Li,Yuanbo Wen,Shuoming Zhang,Ruiyuan Xu,Jin Qin,Jiaming Guo,Chenxi Wang,Ling Li,Qi Guo,Yunji Chen*

Main category: cs.SE

TL;DR: The paper introduces MuSL, a mutual-supervised framework for translating sequential to parallel code. By having translation and testing models teach each other, MuSL ensures functionally correct output, significantly boosting performance against previous methods and achieving results comparable to leading AI models.


<details>
  <summary>Details</summary>
Motivation: Despite the proliferation of GPU-based high-performance computing (HPC) and CUDA, the complexity of parallel programming calls for automated tools to translate sequential code to parallel code. However, limited data for training and challenges in ensuring functional equivalence impede current machine learning-based methods.

Method: The authors propose a Mutual-Supervised Learning (MSL) framework, which comprises two models: a Translator (for code generation) and a Tester (for generating unit tests). The system operates in an iterative loop with 'Co-verify' and 'Co-evolve' steps, enabling the Translator and Tester to mutually generate and refine data to enhance each other's performance.

Result: When applied to Qwen2.5-Coder, MuSL improves the Pass@1 metric by up to 28.91%, enhances Tester performance by 68.90%, and surpasses previous top methods like CodeRosetta in BLEU and CodeBLEU scores, while being competitive with advanced models such as DeepSeek-R1 and GPT-4.1.

Conclusion: The Mutual-Supervised Learning (MSL) framework effectively addresses functional equivalence in sequential-to-parallel code translation, resulting in significant improvements over prior state-of-the-art techniques.

Abstract: The rise of GPU-based high-performance computing (HPC) has driven the
widespread adoption of parallel programming models such as CUDA. Yet, the
inherent complexity of parallel programming creates a demand for the automated
sequential-to-parallel approaches. However, data scarcity poses a significant
challenge for machine learning-based sequential-to-parallel code translation.
Although recent back-translation methods show promise, they still fail to
ensure functional equivalence in the translated code. In this paper, we propose
a novel Mutual-Supervised Learning (MSL) framework for sequential-to-parallel
code translation to address the functional equivalence issue. MSL consists of
two models, a Translator and a Tester. Through an iterative loop consisting of
Co-verify and Co-evolve steps, the Translator and the Tester mutually generate
data for each other and improve collectively. The Tester generates unit tests
to verify and filter functionally equivalent translated code, thereby evolving
the Translator, while the Translator generates translated code as augmented
input to evolve the Tester. Experimental results demonstrate that MuSL
significantly enhances the performance of the base model: when applied to
Qwen2.5-Coder, it not only improves Pass@1 by up to 28.91% and boosts Tester
performance by 68.90%, but also outperforms the previous state-of-the-art
method CodeRosetta by 1.56 and 6.92 in BLEU and CodeBLEU scores, while
achieving performance comparable to DeepSeek-R1 and GPT-4.1. Our code is
available at https://github.com/kcxain/musl.

</details>


### [46] [Model Discovery and Graph Simulation: A Lightweight Alternative to Chaos Engineering](https://arxiv.org/abs/2506.11176)
*Anatoly A. Krasnovsky,Alexander Zorkin*

Main category: cs.SE

TL;DR: Automated extraction of system dependency graphs from trace data enables fast, accurate resilience predictions for microservices, minimizing the need for expensive failure experiments.


<details>
  <summary>Details</summary>
Motivation: Microservice applications often suffer from cascading failures due to complex inter-service dependencies. Traditional approaches to ensure resilience require resource-intensive fault-injection experiments in production-like environments.

Method: The authors propose an automated model discovery technique integrated into CI/CD pipelines, which extracts a live dependency graph from trace data. They use this lightweight graph to simulate failures via Monte Carlo methods and then validate predictions through chaos experiments on the DeathStarBench Social Network application.

Result: The automatically discovered graph model predicts system resilience with high accuracy. For example, the predicted and observed resilience values are very close, especially in replicated setups (mean absolute error ≤ 0.0004).

Conclusion: A simple, automatically discovered dependency graph can accurately estimate microservice availability, reducing the need for costly and time-consuming real-world failure tests.

Abstract: Microservice applications are prone to cascading failures because of dense
inter-service dependencies. Ensuring resilience usually demands fault-injection
experiments in production-like setups. We propose \textit{model discovery} --
an automated CI/CD step that extracts a live dependency graph from trace data
-- and show that this lightweight representation is sufficient for accurate
resilience prediction. Using the DeathStarBench Social Network, we build the
graph, simulate failures via Monte-Carlo, and run matching chaos experiments on
the real system. The graph model closely matches reality: with no replication,
16 trials yield an observed resilience of 0.186 versus a predicted 0.161; with
replication, both observed and predicted values converge to 0.305 (mean
absolute error \leq 0.0004). These results indicate that even a simple,
automatically discovered graph can estimate microservice availability with high
fidelity, offering rapid design-time insight without full-scale failure
testing.

</details>


### [47] [Beyond Formal Semantics for Capabilities and Skills: Model Context Protocol in Manufacturing](https://arxiv.org/abs/2506.11180)
*Luis Miguel Vieira da Silva,Aljosha Köcher,Felix Gehlhoff*

Main category: cs.SE

TL;DR: Instead of relying on difficult, manual modeling of skills and capabilities for LLMs, the authors show that the Model Context Protocol allows LLMs to flexibly access and use resources in industrial automation. Their tests with a prototype system indicate that it is feasible and effective, opening new directions for LLM-driven automation without explicit semantics.


<details>
  <summary>Details</summary>
Motivation: Current approaches to modeling capabilities and skills in industrial automation are labor-intensive and often incompatible with Large Language Models (LLMs), limiting flexibility and scalability.

Method: The authors introduce and evaluate the Model Context Protocol (MCP) as a means for exposing system functionality via a standardized interface, directly accessible by LLM-based agents. They test this in a lab-scale manufacturing environment, where resource functions are accessed through MCP, and an LLM plans and executes a multi-step process, including managing constraints and invoking resource functions.

Result: The approach demonstrates that LLMs can flexibly plan and execute complex tasks in industrial automation settings without the need for detailed, explicit semantic models. MCP facilitates this interaction with external tools/functions.

Conclusion: MCP offers a promising alternative to manual semantic modeling for LLM-driven production systems, enabling easier integration of external functionalities and flexible automation. This could pave the way for broader adoption of LLMs in industry.

Abstract: Explicit modeling of capabilities and skills -- whether based on ontologies,
Asset Administration Shells, or other technologies -- requires considerable
manual effort and often results in representations that are not easily
accessible to Large Language Models (LLMs). In this work-in-progress paper, we
present an alternative approach based on the recently introduced Model Context
Protocol (MCP). MCP allows systems to expose functionality through a
standardized interface that is directly consumable by LLM-based agents. We
conduct a prototypical evaluation on a laboratory-scale manufacturing system,
where resource functions are made available via MCP. A general-purpose LLM is
then tasked with planning and executing a multi-step process, including
constraint handling and the invocation of resource functions via MCP. The
results indicate that such an approach can enable flexible industrial
automation without relying on explicit semantic models. This work lays the
basis for further exploration of external tool integration in LLM-driven
production systems.

</details>


### [48] [LLM-as-a-Judge for Reference-less Automatic Code Validation and Refinement for Natural Language to Bash in IT Automation](https://arxiv.org/abs/2506.11237)
*Ngoc Phuoc An Vo,Brent Paulovicks,Vadim Sheinin*

Main category: cs.SE

TL;DR: This paper introduces improved LLM-based evaluation for automatically selecting and refining Bash code in IT automation, achieving better accuracy and supporting robust, reference-less code validation and refinement.


<details>
  <summary>Details</summary>
Motivation: Automatic incident remediation in IT automation requires high-quality code generation. Evaluating and selecting the best models for this task is challenging, as existing evaluation methods either focus only on syntactic similarity or require execution-based judgments that may be costly or limited. There is a need for more effective, automated, and reliable evaluation methods.

Method: The paper explores three approaches for code evaluation: surface form similarity metrics, execution-based evaluation, and LLM-as-a-Judge. The focus is on enhancing LLM-as-a-Judge by introducing bidirectional functionality matching and logic representation for automatic, reference-less validation of Bash code. Execution-based evaluation is used as ground-truth for validating the proposed LLM-as-a-Judge metrics. Reflection code agents are developed to leverage these metrics for automated code refinement.

Result: The enhanced LLM-as-a-Judge method demonstrates high accuracy and strong agreement with execution-based evaluation, outperforming the baseline by up to 8%. The Reflection code agents, utilizing this evaluation approach, achieve up to a 24% increase in accuracy for automatic code refinement tasks.

Conclusion: The proposed enhancements to LLM-as-a-Judge make it possible to automatically and accurately validate and select models for Bash code generation in IT automation, without relying on reference code. The Reflection code agents significantly improve code quality and robustness in automatic incident remediation.

Abstract: In an effort to automatically evaluate and select the best model and improve
code quality for automatic incident remediation in IT Automation, it is crucial
to verify if the generated code for remediation action is syntactically and
semantically correct and whether it can be executed correctly as intended.
There are three approaches: 1) conventional methods use surface form similarity
metrics (token match, exact match, etc.) which have numerous limitations, 2)
execution-based evaluation focuses more on code functionality based on
pass/fail judgments for given test-cases, and 3) LLM-as-a-Judge employs LLMs
for automated evaluation to judge if it is a correct answer for a given problem
based on pre-defined metrics. In this work, we focused on enhancing
LLM-as-a-Judge using bidirectional functionality matching and logic
representation for reference-less automatic validation and refinement for Bash
code generation to select the best model for automatic incident remediation in
IT Automation. We used execution-based evaluation as ground-truth to evaluate
our LLM-as-a-Judge metrics. Results show high accuracy and agreement with
execution-based evaluation (and up to 8% over baseline). Finally, we built
Reflection code agents to utilize judgments and feedback from our evaluation
metrics which achieved significant improvement (up to 24% increase in accuracy)
for automatic code refinement.

</details>


### [49] [Invocable APIs derived from NL2SQL datasets for LLM Tool-Calling Evaluation](https://arxiv.org/abs/2506.11266)
*Benjamin Elder,Anupama Murthi,Jungkoo Kang,Ankita Rajaram Naik,Kiran Kate,Kinjal Basu,Danish Contractor*

Main category: cs.SE

TL;DR: The paper introduces a method to convert natural language to SQL datasets into API-call datasets, creating real-world-like benchmarks for evaluating LLMs on complex tool use. Testing on 10 LLMs reveals that current models struggle with API selection and sequencing, achieving low task completion rates. Enhanced evaluation datasets and new benchmarks are needed to advance tool-using LLMs.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) are increasingly used as agents in environments where they must interact with complex, large collections of APIs, similar to enterprise settings. There is a lack of datasets that adequately reflect these real-world, large-scale API scenarios, limiting the ability to evaluate and improve LLM performance for tool use.

Method: The authors propose a novel data generation pipeline that converts existing NL2SQL (natural language to SQL) datasets into NL2API datasets. They do this by translating SQL queries into functionally equivalent API call sequences. This pipeline was applied to the BIRD-SQL dataset, resulting in the creation of a large API pool and paired natural language to API sequences. They then evaluated 10 public LLMs on this data, including studying effects of variable tool numbers and tool/slot-name obfuscation via ablation studies.

Result: All tested LLMs showed significant difficulty in determining the correct API tools and constructing proper call sequences, achieving low task completion rates (7-47%, depending on conditions), which increased to 50% with ReACT agent interaction. None of the LLMs approached desirable performance for general-purpose tool-calling situations. Additional studies highlight models' reliance on SQL compared to APIs and the challenges posed by larger toolsets and naming obfuscations.

Conclusion: Current state-of-the-art tool-calling LLMs are far from effective in handling complex API selection and composition tasks, especially in enterprise-like environments. There is substantial room for improvement. Converting NL2SQL datasets into NL2API datasets is a practical step to create challenging benchmarks and drive progress.

Abstract: Large language models (LLMs) are routinely deployed as agentic systems, with
access to tools that interact with live environments to accomplish tasks. In
enterprise deployments these systems need to interact with API collections that
can be extremely large and complex, often backed by databases. In order to
create datasets with such characteristics, we explore how existing NL2SQL
(Natural Language to SQL query) datasets can be used to automatically create
NL2API datasets. Specifically, this work describes a novel data generation
pipeline that exploits the syntax of SQL queries to construct a functionally
equivalent sequence of API calls. We apply this pipeline to one of the largest
NL2SQL datasets, BIRD-SQL to create a collection of over 2500 APIs that can be
served as invocable tools or REST-endpoints. We pair natural language queries
from BIRD-SQL to ground-truth API sequences based on this API pool. We use this
collection to study the performance of 10 public LLMs and find that all models
struggle to determine the right set of tools (consisting of tasks of intent
detection, sequencing with nested function calls, and slot-filling). We find
that models have extremely low task completion rates (7-47 percent - depending
on the dataset) which marginally improves to 50 percent when models are
employed as ReACT agents that interact with the live API environment. The best
task completion rates are far below what may be required for effective
general-use tool-calling agents, suggesting substantial scope for improvement
in current state-of-the-art tool-calling LLMs. We also conduct detailed
ablation studies, such as assessing the impact of the number of tools available
as well as the impact of tool and slot-name obfuscation. We compare the
performance of models on the original SQL generation tasks and find that
current models are sometimes able to exploit SQL better than APIs.

</details>


### [50] [A Tale of Two Systems: Characterizing Architectural Complexity on Machine Learning-Enabled Systems](https://arxiv.org/abs/2506.11295)
*Renato Cordeiro Ferreira*

Main category: cs.SE

TL;DR: This paper proposes a metrics-based architectural model to manage and characterize the complexity of ML-enabled systems, demonstrated through two case studies, aiming to support better architectural decisions.


<details>
  <summary>Details</summary>
Motivation: Managing the complexity of machine learning-enabled systems (MLES) is challenging and crucial for their effective development and growth.

Method: The research introduces a metrics-based architectural model by analyzing the architectures of two case study systems, SPIRA and Ocean Guard, to characterize and measure complexity in MLES.

Result: A metrics-based architectural model is presented, along with comparative architecture representations of SPIRA and Ocean Guard to demonstrate the model's application.

Conclusion: The proposed model helps guide architectural decisions and supports the inception and growth of ML-enabled systems by providing a clearer understanding of their complexity.

Abstract: How can the complexity of ML-enabled systems be managed effectively? The goal
of this research is to investigate how complexity affects ML-Enabled Systems
(MLES). To address this question, this research aims to introduce a
metrics-based architectural model to characterize the complexity of MLES. The
goal is to support architectural decisions, providing a guideline for the
inception and growth of these systems. This paper brings, side-by-side, the
architecture representation of two systems that can be used as case studies for
creating the metrics-based architectural model: the SPIRA and the Ocean Guard
MLES.

</details>


### [51] [A Step-by-Step Guide to Creating a Robust Autonomous Drone Testing Pipeline](https://arxiv.org/abs/2506.11400)
*Yupeng Jiang,Yao Deng,Sebastian Schroder,Linfeng Liang,Suhaas Gambhir,Alice James,Avishkar Seth,James Pirrie,Yihao Zhang,Xi Zheng*

Main category: cs.SE

TL;DR: This paper introduces a comprehensive, multi-stage testing pipeline for autonomous drones, combining proven and emerging techniques to achieve safer, more reliable, and efficient systems ready for critical real-world tasks.


<details>
  <summary>Details</summary>
Motivation: As autonomous drones become increasingly deployed in critical applications, there is a crucial need for robust methods to ensure their safety, reliability, and operational efficiency.

Method: The paper introduces a step-by-step validation and testing pipeline for autonomous drones, incorporating Software-in-the-Loop (SIL) Simulation Testing, Hardware-in-the-Loop (HIL) Testing, Controlled Real-World Testing, and In-Field Testing. It demonstrates these stages with practical scenarios such as marker-based autonomous landing systems, and discusses emerging testing trends like Neurosymbolic integration, Large Language Models (LLMs), co-simulation, and Digital Twin-enabled techniques.

Result: Applying this systematic pipeline allows for comprehensive testing, identification of integration challenges, and optimization of drone behavior, reducing the risks associated with real-world deployment. New trends enhance the rigor and fidelity of drone testing environments.

Conclusion: The proposed stepwise pipeline enables developers and researchers to thoroughly validate autonomous drones, optimize performance, and ensure safer, more reliable deployment across diverse applications.

Abstract: Autonomous drones are rapidly reshaping industries ranging from aerial
delivery and infrastructure inspection to environmental monitoring and disaster
response. Ensuring the safety, reliability, and efficiency of these systems is
paramount as they transition from research prototypes to mission-critical
platforms. This paper presents a step-by-step guide to establishing a robust
autonomous drone testing pipeline, covering each critical stage:
Software-in-the-Loop (SIL) Simulation Testing, Hardware-in-the-Loop (HIL)
Testing, Controlled Real-World Testing, and In-Field Testing. Using practical
examples, including the marker-based autonomous landing system, we demonstrate
how to systematically verify drone system behaviors, identify integration
issues, and optimize performance. Furthermore, we highlight emerging trends
shaping the future of drone testing, including the integration of Neurosymbolic
and LLMs, creating co-simulation environments, and Digital Twin-enabled
simulation-based testing techniques. By following this pipeline, developers and
researchers can achieve comprehensive validation, minimize deployment risks,
and prepare autonomous drones for safe and reliable real-world operations.

</details>


### [52] [ReVeal: Self-Evolving Code Agents via Iterative Generation-Verification](https://arxiv.org/abs/2506.11442)
*Yiyang Jin,Kunzhao Xu,Hang Li,Xueting Han,Yanmin Zhou,Cheng Li,Jing Bai*

Main category: cs.SE

TL;DR: ReVeal is a new RL framework that interleaves code generation with self-verification and external tool evaluations, enabling LLMs to improve reasoning and reliability. It outperforms previous benchmarks and demonstrates scalable, robust AI agent development.


<details>
  <summary>Details</summary>
Motivation: Current RL methods for enhancing large language model reasoning are limited by weak verification signals and a lack of explicit self-verification optimization. This results in unreliable outputs that restrict the reasoning abilities of LLMs, especially in realistic environments where robust verification is crucial.

Method: The authors propose ReVeal, a multi-turn RL framework that integrates code generation with explicit self-verification and tool-based evaluation. ReVeal trains LLMs to autonomously create test cases and use external tools for feedback, optimizing performance with a custom RL algorithm that provides dense, per-turn rewards.

Result: ReVeal demonstrates significant gains in Pass@k on the LiveCodeBench benchmark and achieves better results as the number of inference turns increases, ultimately outperforming DeepSeek-R1-Zero-Qwen-32B. The model effectively learns to co-evolve its generation and verification skills, leading to more robust and reliable outputs.

Conclusion: ReVeal offers a scalable and effective approach for developing AI agents with enhanced reasoning and self-verification abilities, setting a new standard for robust and autonomous performance in complex environments.

Abstract: Recent advances in reinforcement learning (RL) with verifiable outcome
rewards have significantly improved the reasoning capabilities of large
language models (LLMs), especially when combined with multi-turn tool
interactions. However, existing methods lack both meaningful verification
signals from realistic environments and explicit optimization for verification,
leading to unreliable self-verification. To address these limitations, we
propose ReVeal, a multi-turn reinforcement learning framework that interleaves
code generation with explicit self-verification and tool-based evaluation.
ReVeal enables LLMs to autonomously generate test cases, invoke external tools
for precise feedback, and improves performance via a customized RL algorithm
with dense, per-turn rewards. As a result, ReVeal fosters the co-evolution of a
model's generation and verification capabilities through RL training, expanding
the reasoning boundaries of the base model, demonstrated by significant gains
in Pass@k on LiveCodeBench. It also enables test-time scaling into deeper
inference regimes, with code consistently evolving as the number of turns
increases during inference, ultimately surpassing DeepSeek-R1-Zero-Qwen-32B.
These findings highlight the promise of ReVeal as a scalable and effective
paradigm for building more robust and autonomous AI agents.

</details>


### [53] [Understanding the Issue Types in Open Source Blockchain-based Software Projects with the Transformer-based BERTopic](https://arxiv.org/abs/2506.11451)
*Md Nahidul Islam Opu,Md Shahidul Islam,Sara Rouhani,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: The paper analyzes nearly 500,000 issues from blockchain GitHub projects, finding Wallet and UI issues most common and hardest to resolve. It highlights the need for tailored maintenance tools to address both general and blockchain-specific challenges.


<details>
  <summary>Details</summary>
Motivation: Blockchain technology is being rapidly adopted across numerous fields, but there is a lack of comprehensive knowledge about the unique and general development challenges faced in blockchain-based software systems.

Method: The authors conducted a large-scale empirical study of 497,742 issues from 1,209 open-source blockchain projects on GitHub, utilizing BERTopic, a transformer-based topic modeling technique, to identify and categorize the most common issue types.

Result: The study found that general software issues and blockchain-specific concerns occur in almost equal measure. Wallet Management and UI Enhancement are the most prominent issue topics. Wallet-related issues appear most frequently and take the longest to resolve, while Mechanisms issues are resolved fastest. Issue numbers grew rapidly after 2016 with the popularity of Ethereum, but decreased after 2022.

Conclusion: Both general and blockchain-specific issues are major challenges in blockchain software. Maintenance practices should be informed by these insights to improve the development and upkeep of blockchain systems. Specialized tools and improved procedures are needed for robustness and maintainability.

Abstract: Blockchain-based software systems are increasingly deployed across diverse
domains, yet a systematic understanding of their development challenges remains
limited. This paper presents a large-scale empirical study of 497,742 issues
mined from 1,209 open-source blockchain projects hosted on GitHub. Employing
BERTopic, a transformer-based topic modeling technique, we identify 49 distinct
issue topics and organize them hierarchically into 11 major subcategories. Our
analysis reveals that both general software development issues and
blockchain-specific concerns are nearly equally represented, with Wallet
Management and UI Enhancement emerging as the most prominent topics. We further
examine the temporal evolution of issue categories and resolution times,
finding that Wallet issues not only dominate in frequency but also exhibit the
longest resolution time. Conversely, Mechanisms issues are resolved
significantly faster. Issue frequency surged after 2016 with the rise of
Ethereum and decentralized applications, but declined after 2022. These
findings enhance our understanding of blockchain software maintenance,
informing the development of specialized tools and practices to improve
robustness and maintainability.

</details>


### [54] [VulStamp: Vulnerability Assessment using Large Language Model](https://arxiv.org/abs/2506.11484)
*Haoshen,Ming Hu,Xiaofei Xie,Jiaye Li,Mingsong Chen*

Main category: cs.SE

TL;DR: Manual descriptions of vulnerabilities are often unreliable for assessing severity. VulStamp uses static analysis and LLMs to automatically extract code intention and leverages RL-based prompt tuning for improved, description-free vulnerability assessment, optimizing remediation efforts and reducing wasted development resources.


<details>
  <summary>Details</summary>
Motivation: Modern vulnerability detection tools identify many security vulnerabilities, leading to high development costs when all are remediated. Not all vulnerabilities are equally severe, so accurate severity assessment is important. Current methods rely on manually written descriptions, which are subjective and inconsistent, limiting their performance.

Method: The paper proposes VulStamp, a new framework that does not require vulnerability descriptions for assessment. VulStamp uses static analysis and Large Language Models (LLMs) to extract intention information from code. It then leverages a prompt-tuned model for vulnerability severity assessment. In addition, VulStamp addresses data imbalance in vulnerability types by incorporating Reinforcement Learning (RL)-based prompt tuning during training.

Result: VulStamp can accurately assess vulnerability severity without relying on potentially low-quality or inconsistent human-written descriptions. The system leverages code intention information and deals with class imbalance, leading to improved assessment performance.

Conclusion: VulStamp provides an effective, description-free framework for assessing vulnerability severity. This approach optimizes developer resources by accurately evaluating which vulnerabilities require remediation, thus improving software development efficiency.

Abstract: Although modern vulnerability detection tools enable developers to
efficiently identify numerous security flaws, indiscriminate remediation
efforts often lead to superfluous development expenses. This is particularly
true given that a substantial portion of detected vulnerabilities either
possess low exploitability or would incur negligible impact in practical
operational environments. Consequently, vulnerability severity assessment has
emerged as a critical component in optimizing software development efficiency.
Existing vulnerability assessment methods typically rely on manually crafted
descriptions associated with source code artifacts. However, due to variability
in description quality and subjectivity in intention interpretation, the
performance of these methods is seriously limited. To address this issue, this
paper introduces VulStamp, a novel intention-guided framework, to facilitate
description-free vulnerability assessment. Specifically, VulStamp adopts static
analysis together with Large Language Model (LLM) to extract the intention
information of vulnerable code. Based on the intention information, VulStamp
uses a prompt-tuned model for vulnerability assessment. Furthermore, to
mitigate the problem of imbalanced data associated with vulnerability types,
VulStamp integrates a Reinforcement Learning (RL)-based prompt-tuning method to
train the assessment model.

</details>


### [55] [A Procedural Framework for Assessing the Desirability of Process Deviations](https://arxiv.org/abs/2506.11525)
*Michael Grohs,Nadine Cordes,Jana-Rebecca Rehse*

Main category: cs.SE

TL;DR: This paper introduces and validates a procedural framework that helps analysts systematically assess whether process deviations are problematic, acceptable, or beneficial, making deviation evaluation more consistent and actionable.


<details>
  <summary>Details</summary>
Motivation: Traditional conformance checking shows where processes deviate from models, but does not assess whether deviations are problematic, acceptable, or beneficial. Current desirability assessments are usually manual and inconsistent, creating the need for a more systematic approach.

Method: The authors present a procedural framework, built from literature review and interviews with practitioners and researchers, to guide analysts in systematically assessing the desirability of process deviations.

Result: The proposed framework was evaluated through a desirability assessment task with practitioners, who found it effective in streamlining and improving the quality and consistency of deviation assessments.

Conclusion: The framework provides a step-by-step, replicable method for categorizing process deviations by desirability and linking them to action recommendations, addressing subjectivity and inefficiency in current practices.

Abstract: Conformance checking techniques help process analysts to identify where and
how process executions deviate from a process model. However, they cannot
determine the desirability of these deviations, i.e., whether they are
problematic, acceptable or even beneficial for the process. Such desirability
assessments are crucial to derive actions, but process analysts typically
conduct them in a manual, ad-hoc way, which can be time-consuming, subjective,
and irreplicable. To address this problem, this paper presents a procedural
framework to guide process analysts in systematically assessing deviation
desirability. It provides a step-by-step approach for identifying which input
factors to consider in what order to categorize deviations into mutually
exclusive desirability categories, each linked to action recommendations. The
framework is based on a review and conceptualization of existing literature on
deviation desirability, which is complemented by empirical insights from
interviews with process analysis practitioners and researchers. We evaluate the
framework through a desirability assessment task conducted with practitioners,
indicating that the framework effectively enables them to streamline the
assessment for a thorough yet concise evaluation.

</details>


### [56] [Augmenting the Generality and Performance of Large Language Models for Software Engineering](https://arxiv.org/abs/2506.11548)
*Fabian C. Peña*

Main category: cs.SE

TL;DR: This paper explores and improves how large language models handle non-code software engineering tasks, providing new models, benchmarks, and methods to detect misinformation, and shows early promising results.


<details>
  <summary>Details</summary>
Motivation: While LLMs are proving transformative in code-related tasks in software engineering, their effectiveness in broader non-code SE activities such as conceptualization and design has not been sufficiently studied. There is also a need to better understand their reliability as knowledge sources and to detect when they produce inaccurate ('hallucinated') information.

Method: The research advances understanding by: (1) evaluating LLMs—varied in type and characteristics—on non-code SE tasks, (2) assessing their value as repositories of foundational software engineering knowledge, and (3) creating and applying methods for hallucination detection. This involves training and evaluating various LLMs using SE-specific datasets and creating new benchmarks.

Result: Initial results demonstrate promising improvements in LLM performance for non-code SE tasks. The project also develops multiple LLMs, new benchmarks, and hallucination detection methods.

Conclusion: The study expands the application of LLMs in software engineering to include non-code tasks, establishes their potential as knowledge providers, and provides tools to identify inaccuracies, thus enhancing reliability.

Abstract: Large Language Models (LLMs) are revolutionizing software engineering (SE),
with special emphasis on code generation and analysis. However, their
applications to broader SE practices including conceptualization, design, and
other non-code tasks, remain partially underexplored. This research aims to
augment the generality and performance of LLMs for SE by (1) advancing the
understanding of how LLMs with different characteristics perform on various
non-code tasks, (2) evaluating them as sources of foundational knowledge in SE,
and (3) effectively detecting hallucinations on SE statements. The expected
contributions include a variety of LLMs trained and evaluated on
domain-specific datasets, new benchmarks on foundational knowledge in SE, and
methods for detecting hallucinations. Initial results in terms of performance
improvements on various non-code tasks are promising.

</details>


### [57] [Leveraging GPT-4 for Vulnerability-Witnessing Unit Test Generation](https://arxiv.org/abs/2506.11559)
*Gábor Antal,Dénes Bán,Martin Isztin,Rudolf Ferenc,Péter Hegedűs*

Main category: cs.SE

TL;DR: GPT-4 can automatically generate vulnerability-focused unit test templates for software code about two-thirds of the time. While only a small percentage are fully correct automatically, the generated tests significantly reduce developer effort for creating vulnerability witnesses.


<details>
  <summary>Details</summary>
Motivation: Manual creation of software tests for vulnerability detection is complex and resource-intensive. Automating this process could help developers and security experts improve software quality and security faster and at lower cost.

Method: The authors evaluated GPT-4's ability to automatically generate unit tests targeting software vulnerabilities, using a portion of the VUL4J dataset that includes real vulnerability cases and their fixes. They assessed the syntactic and semantic correctness of GPT-4-generated tests, investigated the influence of code context, examined self-correction features, and conducted subjective usability evaluations of the test cases produced.

Result: GPT-4 generated syntactically correct unit tests 66.5% of the time without requiring domain-specific training. Only 7.5% of the semantic correctness cases were automatically validated. Subjective evaluation indicated that the generated test templates are useful and can be easily adapted to become fully functional vulnerability tests with minimal manual work.

Conclusion: GPT-4 is a promising tool for partially automating the creation of vulnerability-focused tests in software, producing helpful test templates even if some manual intervention is still required.

Abstract: In the life-cycle of software development, testing plays a crucial role in
quality assurance. Proper testing not only increases code coverage and prevents
regressions but it can also ensure that any potential vulnerabilities in the
software are identified and effectively fixed. However, creating such tests is
a complex, resource-consuming manual process. To help developers and security
experts, this paper explores the automatic unit test generation capability of
one of the most widely used large language models, GPT-4, from the perspective
of vulnerabilities. We examine a subset of the VUL4J dataset containing real
vulnerabilities and their corresponding fixes to determine whether GPT-4 can
generate syntactically and/or semantically correct unit tests based on the code
before and after the fixes as evidence of vulnerability mitigation. We focus on
the impact of code contexts, the effectiveness of GPT-4's self-correction
ability, and the subjective usability of the generated test cases. Our results
indicate that GPT-4 can generate syntactically correct test cases 66.5\% of the
time without domain-specific pre-training. Although the semantic correctness of
the fixes could be automatically validated in only 7. 5\% of the cases, our
subjective evaluation shows that GPT-4 generally produces test templates that
can be further developed into fully functional vulnerability-witnessing tests
with relatively minimal manual effort.
  Therefore, despite the limited data, our initial findings suggest that GPT-4
can be effectively used in the generation of vulnerability-witnessing tests. It
may not operate entirely autonomously, but it certainly plays a significant
role in a partially automated process.

</details>


### [58] [Identifying Helpful Context for LLM-based Vulnerability Repair: A Preliminary Study](https://arxiv.org/abs/2506.11561)
*Gábor Antal,Bence Bogenfürst,Rudolf Ferenc,Péter Hegedűs*

Main category: cs.SE

TL;DR: GPT-4o, when aided by contextual prompts (especially CVE and manual code context), outperforms previous baselines in automated software vulnerability repair, though it sometimes lags behind GPT-4 per prompt. Combining top prompts and contextual information enables GPT-4o to repair more vulnerabilities in zero-shot scenarios.


<details>
  <summary>Details</summary>
Motivation: Advancements in large language models (LLMs) like GPT-4o show potential for automating vulnerability repair in software, but there is limited understanding of how contextual information and different prompts affect the repair performance in real-world datasets.

Method: The authors evaluate GPT-4o on the Vul4J Java vulnerability dataset, comparing its repair abilities with GPT-4 using identical prompts. Additionally, they design nine new prompts incorporating various contextual cues such as CWE/CVE information and manual code contexts, executing each prompt three times per vulnerability and validating repairs with an automated test suite.

Result: GPT-4o performed 11.9% worse than GPT-4 on average with the same prompt, but it succeeded in fixing 10.5% more unique vulnerabilities across repeated trials. Including CVE information significantly boosted repair success, while the length of task descriptions was less important. The best results were achieved by combining CVE guidance with manual code context, and using ensemble (top-3) prompts GPT-4o repaired 62% of vulnerabilities, surpassing previous baselines.

Conclusion: Contextual information, especially CVE guidance and diverse prompt strategies, significantly enhances LLM-based automated vulnerability repair. Ensemble prompting yields higher fix rates than previous one-shot approaches, making it a promising direction for future AVR research.

Abstract: Recent advancements in large language models (LLMs) have shown promise for
automated vulnerability detection and repair in software systems. This paper
investigates the performance of GPT-4o in repairing Java vulnerabilities from a
widely used dataset (Vul4J), exploring how different contextual information
affects automated vulnerability repair (AVR) capabilities. We compare the
latest GPT-4o's performance against previous results with GPT-4 using identical
prompts. We evaluated nine additional prompts crafted by us that contain
various contextual information such as CWE or CVE information, and manually
extracted code contexts. Each prompt was executed three times on 42
vulnerabilities, and the resulting fix candidates were validated using Vul4J's
automated testing framework.
  Our results show that GPT-4o performed 11.9\% worse on average than GPT-4
with the same prompt, but was able to fix 10.5\% more distinct vulnerabilities
in the three runs together. CVE information significantly improved repair
rates, while the length of the task description had minimal impact. Combining
CVE guidance with manually extracted code context resulted in the best
performance. Using our \textsc{Top}-3 prompts together, GPT-4o repaired 26
(62\%) vulnerabilities at least once, outperforming both the original baseline
(40\%) and its reproduction (45\%), suggesting that ensemble prompt strategies
could improve vulnerability repair in zero-shot settings.

</details>


### [59] [MBSR at Work: Perspectives from an Instructor and Software Developers](https://arxiv.org/abs/2506.11588)
*Simone Romano,Alberto Conforti,Gloria Guidetti,Sara Viotti,Rachele Ceschin,Giuseppe Scanniello*

Main category: cs.SE

TL;DR: This study used interviews to explore software developers' experiences with mindfulness training. Developers saw personal benefits but found it hard to consistently apply mindfulness techniques at work.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore how Mindfulness-Based Stress Reduction (MBSR), which has been studied in other fields but not in software development, is perceived and experienced by software developers and instructors within a multinational company's SD context.

Method: The study uses a qualitative research approach, specifically semi-structured interviews, to gather insights from software developers who participated in an MBSR program and the instructor who led it.

Result: Developers reported personal improvements after MBSR practice, despite initial skepticism. However, seamlessly integrating MBSR techniques into the actual work context proved to be challenging.

Conclusion: MBSR can offer personal benefits to software developers even if its practical, ongoing integration into the workplace is not straightforward. The qualitative approach provides unique insights that quantitative studies might miss.

Abstract: In this paper, we present the preliminary findings from a qualitative study
(i.e., semi-structured interviews) on how a Mindfulness-Based Stress Reduction
(MBSR) program, carried out in the Software Development (SD) working context,
is perceived by the software developers of a multinational company who
participated in the MBSR program and by the instructor who led it. MBSR is a
deeply personal and experiential practice in helping individuals manage stress,
particularly in high-pressure environments such as workplaces, healthcare
settings, education, and other demanding professional or personal situations.
Although MBSR has been experimented in different working contexts;
surprisingly, it has never been studied in the SD working context where there
are several stress factors that developers experience (e.g., time pressure and
uncertainty about the content of a particular task and its outcome). In this
respect, qualitative research can generate valuable insights into the
application of MBSR in the SD working context that cannot be captured by
standardized quantitative measures. Being MBSR instructors and software
developers the key stakeholders in delivering an MBSR program in the SD working
context, understanding their first-hand experiences can provide a more detailed
picture of the investigated phenomenon. The most important takeaway result of
our research can be summarized as follows: despite initial skepticism, the
developers recognized personal improvements due to the MBSR practice, though
the integration of MBSR techniques in the working context remained challenging.

</details>


### [60] [Retrieval-Augmented Code Review Comment Generation](https://arxiv.org/abs/2506.11591)
*Hyunsun Hong,Jongmoon Baik*

Main category: cs.SE

TL;DR: This paper introduces a retrieval-augmented approach for automatic code review comment generation, combining strengths from both generation-based and retrieval-based methods, and shows notable improvements in accuracy and rare token handling on standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: Automated code review comment generation assists developers by generating natural language feedback for code modifications. Existing methods are either generation-based, which struggle with rare but significant tokens, or retrieval-based, which lack flexibility for new contexts. Bridging these complementary strengths is needed for improved performance.

Method: The paper proposes using retrieval-augmented generation (RAG) for code review comment generation. The approach conditions pretrained language models on relevant retrieved examples of past code reviews, aiming to combine the flexibility of generation-based models with the rare token accuracy of retrieval-based models.

Result: On the Tufano et al. benchmark, RAG-based comment generation outperforms both generation-only and retrieval-only approaches. It achieves up to 1.67% higher exact match and 4.25% higher BLEU score compared to generation models, and improves rare token generation by up to 24.01%. More retrieved examples further boost performance.

Conclusion: Leveraging retrieval-augmented generation significantly enhances automated code review comment generation, effectively merging the strengths of both generation and retrieval approaches. The system yields better overall accuracy, greater coverage of semantically important tokens, and benefits further from increased relevant exemplar retrieval.

Abstract: Automated code review comment generation (RCG) aims to assist developers by
automatically producing natural language feedback for code changes. Existing
approaches are primarily either generation-based, using pretrained language
models, or information retrieval-based (IR), reusing comments from similar past
examples. While generation-based methods leverage code-specific pretraining on
large code-natural language corpora to learn semantic relationships between
code and natural language, they often struggle to generate low-frequency but
semantically important tokens due to their probabilistic nature. In contrast,
IR-based methods excel at recovering such rare tokens by copying from existing
examples but lack flexibility in adapting to new code contexts-for example,
when input code contains identifiers or structures not found in the retrieval
database. To bridge the gap between generation-based and IR-based methods, this
work proposes to leverage retrieval-augmented generation (RAG) for RCG by
conditioning pretrained language models on retrieved code-review exemplars. By
providing relevant examples that illustrate how similar code has been
previously reviewed, the model is better guided to generate accurate review
comments. Our evaluation on the Tufano et al. benchmark shows that RAG-based
RCG outperforms both generation-based and IR-based RCG. It achieves up to
+1.67% higher exact match and +4.25% higher BLEU scores compared to
generation-based RCG. It also improves the generation of low-frequency
ground-truth tokens by up to 24.01%. We additionally find that performance
improves as the number of retrieved exemplars increases.

</details>


### [61] [Further Evidence on a Controversial Topic about Human-Based Experiments: Professionals vs. Students](https://arxiv.org/abs/2506.11597)
*Simone Romano,Francesco Paolo Sferratore,Giuseppe Scanniello*

Main category: cs.SE

TL;DR: Students outperformed professionals in a bug-fixing experiment, challenging assumptions about the external validity of using students in software engineering research and calling for more realistic experimental settings in future studies.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the ongoing debate regarding the external validity of using students as participants in software engineering (SE) human-based controlled experiments. There is concern about whether results from student participants generalize to real-world software industry contexts.

Method: The authors conduct a controlled experiment comparing two groups performing the same bug-fixing task on a Java program: 62 students from a Computer Science Bachelor's program and 42 professional software engineers from multinational companies. There were some differences in the experimental environments, with professionals facing a more realistic setting including distractions and interruptions.

Result: Contrary to previous empirical findings, the data indicates that students outperformed professionals in the bug-fixing task. The divergent result is highlighted, with an emphasis on the differences in experimental settings.

Conclusion: The study does not provide definitive conclusions but highlights that the use of students in SE experiments requires further scrutiny. The results catalyze discussion and call for deeper inquiry into the factors influencing SE tasks, advocating for experiments that closely reflect real-world scenarios.

Abstract: Most Software Engineering (SE) human-based controlled experiments rely on
students as participants, raising concerns about their external validity.
Specifically, the realism of results obtained from students and their
applicability to the software industry remains in question. In this short
paper, we bring further evidence on this controversial point. To do so, we
compare 62 students and 42 software professionals on a bug-fixing task on the
same Java program. The students were enrolled in a Bachelor's program in
Computer Science, while the professionals were employed by two multinational
companies (for one of them, the professionals were from two offices). Some
variations in the experimental settings of the two groups (students and
professionals) were present. For instance, the experimental environment of the
experiment with professionals was more realistic; i.e., they faced some stress
factors such as interruptions during the bug-fixing task. Considering the
differences between the two groups of participants, the gathered data show that
the students outperformed the professionals in fixing bugs. This diverges to
some extent from past empirical evidence. Rather than presenting definitive
conclusions, our results aim to catalyze the discussion on the use of students
in experiments and pave the way for future investigations. Specifically, our
results encourage us to examine the complex factors influencing SE tasks,
making experiments as more realistic as possible.

</details>


### [62] [Understanding API Usage and Testing: An Empirical Study of C Libraries](https://arxiv.org/abs/2506.11598)
*Ahmed Zaki,Cristian Cadar*

Main category: cs.SE

TL;DR: The paper reveals that open-source C/C++ libraries often under-test the APIs most used by real-world clients. By analyzing client codebases and test suites with LibProbe, the authors show that integrating client tests can significantly improve library test coverage and ensure testing aligns with actual usage trends.


<details>
  <summary>Details</summary>
Motivation: Library developers need to make data-driven decisions regarding bug reports, feature requests, and testing, and understanding real-world API usage is crucial for prioritizing their development efforts. However, it is unclear whether current testing practices align with API usage in the field.

Method: The authors conducted an empirical study analyzing API usage across 21 popular open-source C libraries, examining a total of 3,061 C/C++ client projects. They compared actual API usage by clients to the extent these APIs are exercised by existing library test suites, utilizing a new framework called LibProbe to automate the analysis and generate metrics.

Result: The study found that there is a disconnect between API usage by clients and API testing efforts in libraries. Frequently used APIs are often under-tested. For example, 45% of LMDB APIs used by clients are not covered by library tests. Using client test suites can improve library test coverage—LMDB's coverage could be increased by 14.7% by leveraging client tests, with the added benefit that these tests reflect real-world API usage.

Conclusion: Library developers often do not test widely used APIs adequately, missing opportunities to align their testing priorities with actual usage in the field. Incorporating tests from client projects can substantially improve test coverage and make testing more representative of real-world usage patterns.

Abstract: For library developers, understanding how their Application Programming
Interfaces (APIs) are used in the field can be invaluable. Knowing how clients
are using their APIs allows for data-driven decisions on prioritising bug
reports, feature requests, and testing activities. For example, the priority of
a bug report concerning an API can be partly determined by how widely that API
is used.
  In this paper, we present an empirical study in which we analyse API usage
across 21 popular open-source C libraries, such as OpenSSL and SQLite, with a
combined total of 3,061 C/C++ clients. We compare API usage by clients with how
well library test suites exercise the APIs to offer actionable insights for
library developers. To our knowledge, this is the first study that compares API
usage and API testing at scale for the C/C++ ecosystem. Our study shows that
library developers do not prioritise their effort based on how clients use
their API, with popular APIs often poorly tested. For example, in LMDB, a
popular key-value store, 45% of the APIs are used by clients but not tested by
the library test suite. We further show that client test suites can be
leveraged to improve library testing e.g., improving coverage in LMDB by 14.7%
with the important advantage that those tests are representative of how the
APIs are used in the field.
  For our empirical study, we have developed LibProbe, a framework that can be
used to analyse a large corpus of clients for a given library and produce
various metrics useful to library developers.

</details>


### [63] [Accelerating Delta Debugging through Probabilistic Monotonicity Assessment](https://arxiv.org/abs/2506.11614)
*Yonggang Tao,Jingling Xue*

Main category: cs.SE

TL;DR: The paper presents a new approach (PMA) to make delta debugging much more efficient by probabilistically assessing monotonicity, which sharply cuts down processing time and redundant testing while maintaining or improving the quality of program reduction.


<details>
  <summary>Details</summary>
Motivation: Delta debugging relies on a monotonicity assumption, which isn't always valid. This limitation can cause inefficiencies during debugging, as unnecessary tests may be performed.

Method: The paper proposes Probabilistic Monotonicity Assessment (PMA), an algorithm that models and evaluates the monotonicity of the search space in real-time during the debugging process, using a confidence function based on prior test outcomes to drive efficient pruning of search subsets.

Result: PMA significantly reduces redundant tests, processing time, and the size of reduced programs compared to state-of-the-art tools. Specifically, it reduces processing time by 59.2% (vs. CHISEL), increases reduction speed by 3.32x, and reduces final program size by 6.7%. Against ProbDD, PMA further shows improved processing time (22% reduction), speed (1.34x), and result size (3% reduction).

Conclusion: PMA enhances the efficiency and effectiveness of delta debugging by enabling probabilistic exclusion of non-failure-inducing subsets, thus reducing redundant testing while achieving equal or better program reduction.

Abstract: Delta debugging assumes search space monotonicity: if a program causes a
failure, any supersets of that program will also induce the same failure,
permitting the exclusion of subsets of non-failure-inducing programs. However,
this assumption does not always hold in practice. This paper introduces
Probabilistic Monotonicity Assessment (PMA), enhancing the efficiency of
DDMIN-style algorithms without sacrificing effectiveness. PMA dynamically
models and assesses the search space's monotonicity based on prior tests tried
during the debugging process and uses a confidence function to quantify
monotonicity, thereby enabling the probabilistic exclusion of subsets of
non-failure-inducing programs. Our approach significantly reduces redundant
tests that would otherwise be performed, without compromising the quality of
the reduction.
  We evaluated PMA against two leading DDMIN-style tools, CHISEL and ProbDD.
Our findings indicate that PMA cuts processing time by 59.2% compared to
CHISEL, accelerates the reduction process (i.e., the number of tokens deleted
per second) by 3.32x, and decreases the sizes of the final reduced programs by
6.7%. Against ProbDD, PMA reduces processing time by 22.0%, achieves a 1.34x
speedup in the reduction process, and further decreases the sizes of the final
reduced programs by 3.0%. These findings affirm PMA's role in significantly
improving delta debugging's efficiency while maintaining or enhancing its
effectiveness.

</details>


### [64] [An Empirical study on LLM-based Log Retrieval for Software Engineering Metadata Management](https://arxiv.org/abs/2506.11659)
*Simin Sun,Yuchuan Jin,Miroslaw Staron*

Main category: cs.SE

TL;DR: The paper presents an LLM-powered API that enables intuitive, reliable searches of complex ADS log data by combining signals and video, thus simplifying scenario retrieval for developers and outperforming standard SQL approaches.


<details>
  <summary>Details</summary>
Motivation: Developers of autonomous driving systems (ADSs) face difficulties locating specific driving scenarios within vast, high-frequency log data. The complexity of vehicle signals, lack of signal domain knowledge, and limitations of traditional SQL querying (which requires expertise and yields hard-to-verify results) exacerbate the problem.

Method: The paper proposes an approach that utilizes Large Language Models (LLMs) to enable natural language scenario searches in log data. It fuses signal logs with video data, employs scenario distance graphs and relative gap indicators for reliability evaluation, and implements the solution as an API for efficient querying and record retrieval with video-based visualization.

Result: Evaluation on an open industrial dataset demonstrated that the approach improves scenario retrieval efficiency and reliability, offering quantifiable metrics for result trustworthiness and removing the need for specialized database or signal knowledge.

Conclusion: The proposed LLM-based API solution enhances the process of retrieving specific driving scenarios from complex autonomous vehicle log data by making it accessible via natural language and offering visual validation, outperforming traditional SQL-based methods.

Abstract: Developing autonomous driving systems (ADSs) involves generating and storing
extensive log data from test drives, which is essential for verification,
research, and simulation. However, these high-frequency logs, recorded over
varying durations, pose challenges for developers attempting to locate specific
driving scenarios. This difficulty arises due to the wide range of signals
representing various vehicle components and driving conditions, as well as
unfamiliarity of some developers' with the detailed meaning of these signals.
Traditional SQL-based querying exacerbates this challenge by demanding both
domain expertise and database knowledge, often yielding results that are
difficult to verify for accuracy.
  This paper introduces a Large Language Model (LLM)-supported approach that
combines signal log data with video recordings from test drives, enabling
natural language based scenario searches while reducing the need for
specialized knowledge. By leveraging scenario distance graphs and relative gap
indicators, it provides quantifiable metrics to evaluate the reliability of
query results. The method is implemented as an API for efficient database
querying and retrieval of relevant records, paired with video frames for
intuitive visualization. Evaluation on an open industrial dataset demonstrates
improved efficiency and reliability in scenario retrieval, eliminating
dependency on a single data source and conventional SQL.

</details>


### [65] [SoK: Automated Vulnerability Repair: Methods, Tools, and Assessments](https://arxiv.org/abs/2506.11697)
*Yiwei Hu,Zhen Li,Kedie Shu,Shenghua Guan,Deqing Zou,Shouhuai Xu,Bin Yuan,Hai Jin*

Main category: cs.SE

TL;DR: The paper reviews and systematizes current Automated Vulnerability Repair (AVR) methods, creates a new C/C++ vulnerability benchmark (Vul4C), and uses it to evaluate several AVR tools, offering a snapshot of the field and key future directions.


<details>
  <summary>Details</summary>
Motivation: The complexity of software is increasing, which leads to more vulnerabilities. Manual vulnerability repair is both labor-intensive and time-consuming, necessitating more efficient and automated solutions.

Method: This paper systematizes Automated Vulnerability Repair (AVR) methods based on a three-step workflow: vulnerability analysis, patch generation, and patch validation. It assesses existing AVR tools for C/C++ and Java, constructs a new benchmark dataset Vul4C for C/C++ vulnerabilities, and evaluates AVR tools using Vul4C and the Vul4J dataset.

Result: The authors created Vul4C, the first comprehensive C/C++ benchmark dataset with 144 vulnerabilities, their exploits, and patches. They evaluated seven AVR tools for C/C++ and two for Java using standardized datasets, contributing new comparative insights.

Conclusion: The paper provides a structured evaluation of AVR methods and tools, introduces a valuable new benchmark for C/C++ repair research, and highlights areas and directions for future work in automated software vulnerability repair.

Abstract: The increasing complexity of software has led to the steady growth of
vulnerabilities. Vulnerability repair investigates how to fix software
vulnerabilities. Manual vulnerability repair is labor-intensive and
time-consuming because it relies on human experts, highlighting the importance
of Automated Vulnerability Repair (AVR). In this SoK, we present the
systematization of AVR methods through the three steps of AVR workflow:
vulnerability analysis, patch generation, and patch validation. We assess AVR
tools for C/C++ and Java programs as they have been widely studied by the
community. Since existing AVR tools for C/C++ programs are evaluated with
different datasets, which often consist of a few vulnerabilities, we construct
the first C/C++ vulnerability repair benchmark dataset, dubbed Vul4C, which
contains 144 vulnerabilities as well as their exploits and patches. We use
Vul4C to evaluate seven AVR tools for C/C++ programs and use the third-party
Vul4J dataset to evaluate two AVR tools for Java programs. We also discuss
future research directions.

</details>


### [66] [Classification of Quality Characteristics in Online User Feedback using Linguistic Analysis, Crowdsourcing and LLMs](https://arxiv.org/abs/2506.11722)
*Eduard C. Groen,Fabiano Dalpiaz,Martijn van Vliet,Boris Winter,Joerg Doerr,Sjaak Brinkkemper*

Main category: cs.SE

TL;DR: This paper compares three low-data approaches for extracting software quality insights from mobile app user reviews. Crowdsourcing and LLM methods offer strong classification accuracy, showing promise for both classification tasks and for bootstrapping training datasets, while simple keyword methods are less effective.


<details>
  <summary>Details</summary>
Motivation: There is a need to automatically identify software quality characteristics (e.g., usability, reliability) in mobile app user feedback, as user reviews provide vital quality-related insights. However, feedback is heterogenous and lacks sufficient annotated data, making supervised machine learning approaches difficult to apply.

Method: The paper investigates and compares three alternative methods suitable for low-data scenarios: (1) language patterns using quality-related keyword lists (LPs), (2) crowdsourced micro-task instructions, and (3) prompts for large language models (LLMs). The feasibility and classification accuracy of each method for multiclass quality characteristic identification are evaluated.

Result: The LP-based (keyword) approach showed highly variable precision (0.38-0.92 depending on the quality characteristic) and generally low recall. Crowdsourcing delivered the best average accuracy across two phases (0.63, 0.72), which was closely matched by the best LLM configuration (0.66) and an LLM majority vote ensemble (0.68).

Conclusion: In low-data environments, crowdsourcing and LLM-based methods (without expert involvement) can accurately classify quality-related feedback in user reviews, while the LP keyword-based approach is limited. Crowdsourcing and LLMs also show potential for constructing labeled training datasets.

Abstract: Software qualities such as usability or reliability are among the strongest
determinants of mobile app user satisfaction and constitute a significant
portion of online user feedback on software products, making it a valuable
source of quality-related feedback to guide the development process. The
abundance of online user feedback warrants the automated identification of
quality characteristics, but the online user feedback's heterogeneity and the
lack of appropriate training corpora limit the applicability of supervised
machine learning. We therefore investigate the viability of three approaches
that could be effective in low-data settings: language patterns (LPs) based on
quality-related keywords, instructions for crowdsourced micro-tasks, and large
language model (LLM) prompts. We determined the feasibility of each approach
and then compared their accuracy. For the complex multiclass classification of
quality characteristics, the LP-based approach achieved a varied precision
(0.38-0.92) depending on the quality characteristic, and low recall;
crowdsourcing achieved the best average accuracy in two consecutive phases
(0.63, 0.72), which could be matched by the best-performing LLM condition
(0.66) and a prediction based on the LLMs' majority vote (0.68). Our findings
show that in this low-data setting, the two approaches that use crowdsourcing
or LLMs instead of involving experts achieve accurate classifications, while
the LP-based approach has only limited potential. The promise of crowdsourcing
and LLMs in this context might even extend to building training corpora.

</details>


### [67] [A Short Survey on Formalising Software Requirements using Large Language Models](https://arxiv.org/abs/2506.11874)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: A survey of thirty-five papers reviews how large language models help convert natural language requirements into formal software specifications, providing key insights and future research directions.


<details>
  <summary>Details</summary>
Motivation: Transforming natural language software requirements into formal specifications is a challenge in software engineering. Large language models (LLMs) offer potential to aid this process, but systematic understanding of their usefulness is needed.

Method: A focused literature survey was conducted, examining thirty-five key papers identified from multiple academic databases. The AI-assisted tool Elicit was used for initial paper selection, followed by manual screening for final inclusion. The survey includes illustrative examples for Dafny, C, and Java.

Result: The survey synthesizes current research on how LLMs aid in specifying software, offering examples and a comprehensive overview. It identifies valuable insights as well as future directions for research and tool development in this domain.

Conclusion: LLMs show promise in supporting the transition from natural language requirements to formal specifications. The survey highlights the state of the field, practical examples, and avenues for improving LLM-assisted formalization of software requirements.

Abstract: This paper presents a focused literature survey on the use of large language
models (LLM) to assist in writing formal specifications for software. A summary
of thirty-five key papers is presented, including examples for specifying
programs written in Dafny, C and Java. This paper arose from the project
VERIFAI - Traceability and verification of natural language requirements that
addresses the challenges in writing formal specifications from requirements
that are expressed in natural language. Our methodology employed multiple
academic databases to identify relevant research. The AI-assisted tool Elicit
facilitated the initial paper selection, which were manually screened for final
selection. The survey provides valuable insights and future directions for
utilising LLMs while formalising software requirements.

</details>


### [68] [LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?](https://arxiv.org/abs/2506.11928)
*Zihan Zheng,Zerui Cheng,Zeyu Shen,Shang Zhou,Kaiyuan Liu,Hansen He,Dongruixuan Li,Stanley Wei,Hangyi Hao,Jianzhu Yao,Peiyao Sheng,Zixuan Wang,Wenhao Chai,Aleksandra Korolova,Peter Henderson,Sanjeev Arora,Pramod Viswanath,Jingbo Shang,Saining Xie*

Main category: cs.SE

TL;DR: Despite reports, leading language models do not yet match expert humans in competitive programming, especially on hard problems requiring deep reasoning. A new human-annotated benchmark shows LLMs are strong at implementation but weak on complex, nuanced tasks. There is still a big gap to grandmaster-level human performance.


<details>
  <summary>Details</summary>
Motivation: Recent claims suggest large language models (LLMs) outperform elite humans in competitive programming. This paper seeks to rigorously reevaluate these claims, identifying differences between LLMs and human experts and characterizing the current limitations of LLMs in this domain.

Method: The authors develop LiveCodeBench Pro, a new benchmark comprising continuously updated competitive programming problems from Codeforces, ICPC, and IOI, with careful annotation by Olympiad medalists. They analyze model performance, including detailed error analysis through expert annotation and comparison to human performance.

Result: The best current LLM achieves only 53% pass@1 on medium problems and 0% on hard problems without external tools, while expert humans excel at these. LLMs perform better on implementation-heavy tasks but perform poorly on problems requiring nuanced algorithmic reasoning. Errors often feature confidently incorrect justifications. LLM high performance is largely due to implementation skills and external tool usage, not reasoning.

Conclusion: There remains a significant gap between LLMs and human grandmasters in competitive programming, particularly in algorithmic reasoning. The new benchmark provides granular diagnostics to guide further LLM development in code reasoning.

Abstract: Recent reports claim that large language models (LLMs) now outperform elite
humans in competitive programming. Drawing on knowledge from a group of
medalists in international algorithmic contests, we revisit this claim,
examining how LLMs differ from human experts and where limitations still
remain. We introduce LiveCodeBench Pro, a benchmark composed of problems from
Codeforces, ICPC, and IOI that are continuously updated to reduce the
likelihood of data contamination. A team of Olympiad medalists annotates every
problem for algorithmic categories and conducts a line-by-line analysis of
failed model-generated submissions. Using this new data and benchmark, we find
that frontier models still have significant limitations: without external
tools, the best model achieves only 53% pass@1 on medium-difficulty problems
and 0% on hard problems, domains where expert humans still excel. We also find
that LLMs succeed at implementation-heavy problems but struggle with nuanced
algorithmic reasoning and complex case analysis, often generating confidently
incorrect justifications. High performance appears largely driven by
implementation precision and tool augmentation, not superior reasoning.
LiveCodeBench Pro thus highlights the significant gap to human grandmaster
levels, while offering fine-grained diagnostics to steer future improvements in
code-centric LLM reasoning.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [69] [A Performance Model for Warp Specialization Kernels](https://arxiv.org/abs/2506.11209)
*Zhengyang Liu,Vinod Grover*

Main category: cs.PL

TL;DR: A new performance model for GPU warp specialization kernels is introduced, accurately predicting execution time using differential equations. This model is validated experimentally and helps optimize GPU performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide an accurate performance model for warp specialization kernels, addressing how various factors (warp size, tiling size, matrix size, bandwidth, thread divergence) affect performance and aiding in GPU application optimization.

Method: The paper develops a performance model using differential equations. The model is validated through simulations and experiments, focusing on predicting the execution time of warp specialization kernels.

Result: The performance model accurately predicts execution time given different kernel parameters, validated by experiments and simulations.

Conclusion: The model deepens the understanding of warp specialization and provides practical tools for optimizing GPU-accelerated applications via compiler, tuning, and algorithm design.

Abstract: This paper presents a performance model tailored for warp specialization
kernels, focusing on factors such as warp size, tilling size, input matrix
size, memory bandwidth, and thread divergence. Our model offers accurate
predictions of execution time by leveraging differential equations validated
through simulations and experiments. The insights gained from this model not
only enhance our understanding of warp specialization techniques but also have
practical implications for optimizing GPU-accelerated applications through
compiler optimizations, kernel parameter tuning, and algorithm design.

</details>


### [70] [PermRust: A Token-based Permission System for Rust](https://arxiv.org/abs/2506.11701)
*Lukas Gehring,Sebastian Rehms,Florian Tschorsch*

Main category: cs.PL

TL;DR: PermRust provides library-level permission control within Rust by leveraging its type system, enabling fine-grained and efficient security beyond traditional process-level OS permissions.


<details>
  <summary>Details</summary>
Motivation: Traditional OS-level permission systems can only restrict resources access at the process level, which does not provide fine-grained control, especially when third-party libraries are involved. There is a need for permission systems that work at the library level to improve security.

Method: The paper adapts concepts from capability systems and builds a theoretical foundation for permission systems at the programming language level. It introduces PermRust, a token-based permission system using the Rust language's type system as a zero-cost abstraction.

Result: PermRust enables resource access management at the granularity of individual libraries within Rust applications, without incurring extra runtime costs.

Conclusion: The approach demonstrates that permission systems can be effectively integrated into the type system of a programming language (Rust), allowing fine-grained, library-level access control and enhanced security.

Abstract: Permission systems which restrict access to system resources are a
well-established technology in operating systems, especially for smartphones.
However, as such systems are implemented in the operating system they can at
most manage access on the process-level. Since moderns software often (re)uses
code from third-parties libraries, a permission system for libraries can be
desirable to enhance security. In this short-paper, we adapt concepts from
capability systems building a novel theoretical foundation for permission
system at the level of the programming language. This leads to PermRust, a
token-based permission system for the Rust programming language as a zero cost
abstraction on top of its type-system. With it access to system resources can
be managed per library.

</details>


### [71] [ALEA IACTA EST: A Declarative Domain-Specific Language for Manually Performable Random Experiments](https://arxiv.org/abs/2506.11794)
*Baltasar Trancón y Widemann,Markus Lepper*

Main category: cs.PL

TL;DR: Alea is a new, easy-to-use language for specifying and analyzing random experiments, aimed at students and game designers, with ongoing tool development.


<details>
  <summary>Details</summary>
Motivation: Random experiments are central to teaching elementary stochastics and feature in games, but there is a lack of simple tools for specifying and analyzing these experiments accessible to non-experts.

Method: The authors propose Alea, a domain-specific language tailored for specifying random experiments. Alea supports both static analysis to obtain probability distributions and execution with pseudo-randomness for simulations. It borrows concepts from functional programming and basic mathematics and targets beginners in stochastics and game players/designers.

Result: A prototype of the Alea language has been developed to allow specification, analysis, and simulation of random experiments, though both the language design and runtime implementation are still under development.

Conclusion: Alea offers a beginner-friendly way to specify and analyze random experiments, helping students and game enthusiasts more easily interact with stochastic concepts; further development is ongoing.

Abstract: Random experiments that are simple and clear enough to be performed by human
agents feature prominently in the teaching of elementary stochastics as well as
in games. We present Alea, a domain-specific language for the specification of
random experiments. Alea code can either be analyzed statically to obtain and
inspect probability distributions of outcomes, or be executed with a source
pseudo-randomness for simulation or as a game assistant. The language is
intended for ease of use by non-expert programmers, in particular students of
elementary stochastics, and players and designers of games of chance, by
focusing on concepts common to functional programming and basic mathematics.
Both the design of the language and the implementation of runtime environments
are work in progress.

</details>
