{"id": "2508.18587", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18587", "abs": "https://arxiv.org/abs/2508.18587", "authors": ["Bar\u0131\u015f Bayaz\u0131t", "Yao Li", "Xujie Si"], "title": "A Case Study on the Effectiveness of LLMs in Verification with Proof Assistants", "comment": "Accepted by LMPL 2025", "summary": "Large language models (LLMs) can potentially help with verification using\nproof assistants by automating proofs. However, it is unclear how effective\nLLMs are in this task. In this paper, we perform a case study based on two\nmature Rocq projects: the hs-to-coq tool and Verdi. We evaluate the\neffectiveness of LLMs in generating proofs by both quantitative and qualitative\nanalysis. Our study finds that: (1) external dependencies and context in the\nsame source file can significantly help proof generation; (2) LLMs perform\ngreat on small proofs but can also generate large proofs; (3) LLMs perform\ndifferently on different verification projects; and (4) LLMs can generate\nconcise and smart proofs, apply classical techniques to new definitions, but\ncan also make odd mistakes.", "AI": {"tldr": "This paper case-studies LLMs' effectiveness at generating formal proofs for verification projects, finding they excel with small proofs, benefit from context, can produce innovative results, but still make odd mistakes and show varying performance across projects.", "motivation": "The motivation behind this paper is to investigate the practical effectiveness of large language models (LLMs) in automating proof generation with proof assistants, addressing the gap in understanding their real-world utility for formal verification tasks.", "method": "The authors conducted a case study using two mature Rocq projects (hs-to-coq tool and Verdi) to assess LLMs' proof generation abilities. They applied both quantitative and qualitative analyses to evaluate LLM-generated proofs.", "result": "The study found that: (1) incorporating external dependencies and relevant context from the same file significantly improves proof generation; (2) LLMs perform very well on smaller proofs but are also capable of generating larger proofs; (3) performance varies across different verification projects; (4) LLMs can create concise proofs and successfully apply classical methods to new definitions but sometimes make peculiar errors.", "conclusion": "LLMs demonstrate strong potential in assisting with proof generation for verification projects, particularly for smaller proofs, but their performance varies with context, project type, and complexity, and they are prone to occasional unusual mistakes."}}
{"id": "2508.18370", "categories": ["cs.SE", "cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18370", "abs": "https://arxiv.org/abs/2508.18370", "authors": ["Terry Yue Zhuo", "Dingmin Wang", "Hantian Ding", "Varun Kumar", "Zijian Wang"], "title": "Training Language Model Agents to Find Vulnerabilities with CTF-Dojo", "comment": null, "summary": "Large language models (LLMs) have demonstrated exceptional capabilities when\ntrained within executable runtime environments, notably excelling at software\nengineering tasks through verified feedback loops. Yet, scalable and\ngeneralizable execution-grounded environments remain scarce, limiting progress\nin training more capable ML agents. We introduce CTF-Dojo, the first\nlarge-scale executable runtime tailored for training LLMs with verifiable\nfeedback, featuring 658 fully functional Capture-The-Flag (CTF)-style\nchallenges containerized in Docker with guaranteed reproducibility. To enable\nrapid scaling without manual intervention, we develop CTF-Forge, an automated\npipeline that transforms publicly available artifacts into ready-to-use\nexecution environments in minutes, eliminating weeks of expert configuration\ntraditionally required. We trained LLM-based agents on just 486 high-quality,\nexecution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute\ngains over strong baselines across three competitive benchmarks: InterCode-CTF,\nNYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1,\nestablishing a new open-weight state-of-the-art that rivals frontier models\nlike DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a\nbenchmark for executable-agent learning, CTF-Dojo demonstrates that\nexecution-grounded training signals are not only effective but pivotal in\nadvancing high-performance ML agents without dependence on costly proprietary\nsystems.", "AI": {"tldr": "CTF-Dojo provides a large-scale, automated, and reproducible environment for training LLM agents through verifiable execution feedback, leading to state-of-the-art results in CTF-based software engineering benchmarks that approach the performance of proprietary frontier models.", "motivation": "Current training of LLMs with executable environments is limited by the scarcity and inflexibility of such platforms. There is a need for scalable, reproducible environments for verifiable feedback to develop more capable machine learning agents without heavy manual effort or proprietary data.", "method": "The authors developed CTF-Dojo, an executable runtime comprising 658 Docker-based CTF challenges, and CTF-Forge, an automated tool for transforming public artifacts into execution environments. They trained LLM agents with execution-verified feedback and evaluated them against established CTF benchmarks.", "result": "Training on 486 high-quality trajectories from CTF-Dojo yielded up to 11.6% absolute improvement over strong baselines in three benchmarks, with the top model achieving 31.9% Pass@1. This performance rivals that of leading closed-source models, demonstrating the platform's effectiveness.", "conclusion": "Execution-grounded training signals using CTF-style challenges can significantly improve the performance of large language model agents, achieving results competitive with frontier proprietary models."}}
{"id": "2508.18431", "categories": ["cs.SE", "cs.ET", "cs.HC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.18431", "abs": "https://arxiv.org/abs/2508.18431", "authors": ["K\u00e9rian Fiter", "Louis Malassign\u00e9-Onfroy", "Bentley Oakes"], "title": "DTInsight: A Tool for Explicit, Interactive, and Continuous Digital Twin Reporting", "comment": null, "summary": "With Digital Twin (DT) construction and evolution occurring over time,\nstakeholders require tools to understand the current characteristics and\nconceptual architecture of the system at any time. We introduce DTInsight, a\nsystematic and automated tool and methodology for producing continuous\nreporting for DTs. DTInsight offers three key features: (a) an interactive\nconceptual architecture visualization of DTs; (b) generation of summaries of DT\ncharacteristics based on ontological data; and (c) integration of these outputs\ninto a reporting page within a continuous integration and continuous deployment\n(CI/CD) pipeline. Given a modeled description of the DT aligning to our DT\nDescription Framework (DTDF), DTInsight enables up-to-date and detailed reports\nfor enhanced stakeholder understanding.", "AI": {"tldr": "DTInsight is a tool for automatically and continuously reporting the state of Digital Twin systems, featuring visualization, summary generation, and CI/CD integration, thus helping stakeholders maintain up-to-date understanding.", "motivation": "As Digital Twin systems evolve over time, stakeholders need effective tools to track their current characteristics and architecture for better understanding and decision-making.", "method": "The paper introduces DTInsight, which systematically and automatically produces continuous reports for Digital Twins using a DT Description Framework. DTInsight features interactive architecture visualization, summary generation using ontological data, and integration with CI/CD pipelines.", "result": "DTInsight provides stakeholders with real-time, detailed, and easily accessible reports that improve their understanding of Digital Twin systems.", "conclusion": "DTInsight effectively supports stakeholders by delivering timely, thorough insights into evolving Digital Twin systems through automated reporting and visualization."}}
{"id": "2508.18452", "categories": ["cs.SE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.18452", "abs": "https://arxiv.org/abs/2508.18452", "authors": ["Pierre-Emmanuel Goffi", "Rapha\u00ebl Tremblay", "Bentley Oakes"], "title": "Engineering a Digital Twin for the Monitoring and Control of Beer Fermentation Sampling", "comment": "Accepted for EDTconf 2025", "summary": "Successfully engineering interactive industrial DTs is a complex task,\nespecially when implementing services beyond passive monitoring. We present\nhere an experience report on engineering a safety-critical digital twin (DT)\nfor beer fermentation monitoring, which provides continual sampling and reduces\nmanual sampling time by 91%. We document our systematic methodology and\npractical solutions for implementing bidirectional DTs in industrial\nenvironments. This includes our three-phase engineering approach that\ntransforms a passive monitoring system into an interactive Type 2 DT with\nreal-time control capabilities for pressurized systems operating at seven bar.\nWe contribute details of multi-layered safety protocols, hardware-software\nintegration strategies across Arduino controllers and Unity visualization, and\nreal-time synchronization solutions. We document specific engineering\nchallenges and solutions spanning interdisciplinary integration, demonstrating\nhow our use of the constellation reporting framework facilitates cross-domain\ncollaboration. Key findings include the critical importance of safety-first\ndesign, simulation-driven development, and progressive implementation\nstrategies. Our work thus provides actionable guidance for practitioners\ndeveloping DTs requiring bidirectional control in safety-critical applications.", "AI": {"tldr": "This paper reports on engineering an interactive, safety-critical digital twin for beer fermentation, reducing manual work by 91%. It details a stepwise methodology and integration strategies, emphasizing safety, simulation, and collaboration. Findings offer practical guidance for developing real-time, controllable DTs in complex industrial settings.", "motivation": "The motivation is to address the complexity of engineering interactive digital twins (DTs) in industrial settings, particularly beyond mere passive monitoring, and to enhance operational efficiency and safety for processes such as beer fermentation.", "method": "A systematic methodology was used, featuring a three-phase engineering approach that transitions from passive monitoring to an interactive Type 2 DT. This system integrates hardware (Arduino controllers) and software (Unity visualization), employs multi-layered safety protocols, and applies real-time synchronization and the constellation reporting framework for cross-domain collaboration.", "result": "The developed DT system achieved a 91% reduction in manual sampling time and successfully implemented real-time control capabilities for a high-pressure system. The project overcame challenges related to interdisciplinary integration and demonstrated the importance of safety-first and simulation-driven approaches.", "conclusion": "The study offers practical, actionable strategies for creating bidirectional, safety-critical DTs, emphasizing the value of safety, simulation, and progressive implementation. The experience and methodology provide guidance for practitioners developing similar systems with real-time interactive controls in industrial environments."}}
{"id": "2508.18547", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18547", "abs": "https://arxiv.org/abs/2508.18547", "authors": ["Youssef Abdelsalam", "Norman Peitek", "Anna-Maria Maurer", "Mariya Toneva", "Sven Apel"], "title": "How do Humans and LLMs Process Confusing Code?", "comment": null, "summary": "Already today, humans and programming assistants based on large language\nmodels (LLMs) collaborate in everyday programming tasks. Clearly, a\nmisalignment between how LLMs and programmers comprehend code can lead to\nmisunderstandings, inefficiencies, low code quality, and bugs.\n  A key question in this space is whether humans and LLMs are confused by the\nsame kind of code. This would not only guide our choices of integrating LLMs in\nsoftware engineering workflows, but also inform about possible improvements of\nLLMs.\n  To this end, we conducted an empirical study comparing an LLM to human\nprogrammers comprehending clean and confusing code. We operationalized\ncomprehension for the LLM by using LLM perplexity, and for human programmers\nusing neurophysiological responses (in particular, EEG-based fixation-related\npotentials).\n  We found that LLM perplexity spikes correlate both in terms of location and\namplitude with human neurophysiological responses that indicate confusion. This\nresult suggests that LLMs and humans are similarly confused about the code.\nBased on these findings, we devised a data-driven, LLM-based approach to\nidentify regions of confusion in code that elicit confusion in human\nprogrammers.", "AI": {"tldr": "The study shows that humans and LLMs get confused by the same code regions, suggesting LLMs can help identify confusing code for human programmers.", "motivation": "The motivation behind this paper is to understand whether human programmers and large language models (LLMs) experience confusion in the same parts of source code. This has important implications for integrating LLMs into software engineering workflows, improving code quality, and enhancing LLMs' design.", "method": "The authors conducted an empirical study comparing code comprehension between LLMs and human programmers. Comprehension in LLMs was measured by examining perplexity scores, while comprehension in humans was measured using neurophysiological data (EEG-based fixation-related potentials). They analyzed the alignment of confusion indicators from both sources when processing clean and confusing code.", "result": "The study found that spikes in LLM perplexity strongly correlate in both location and amplitude with human neurophysiological signals associated with confusion. In other words, both LLMs and humans tend to be confused by the same kinds of code.", "conclusion": "LLMs and humans exhibit similar confusion patterns when reading code, which means LLMs could be valuable in identifying confusing code for humans. This insight can guide the integration of LLMs in software engineering processes as well as improvements in LLM technology."}}
{"id": "2508.18636", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18636", "abs": "https://arxiv.org/abs/2508.18636", "authors": ["Yan Wang", "Xinyi Hou", "Yanjie Zhao", "Weiguo Lin", "Haoyu Wang", "Junjun Si"], "title": "LaQual: A Novel Framework for Automated Evaluation of LLM App Quality", "comment": null, "summary": "LLM app stores are quickly emerging as platforms that gather a wide range of\nintelligent applications based on LLMs, giving users many choices for content\ncreation, coding support, education, and more. However, the current methods for\nranking and recommending apps in these stores mostly rely on static metrics\nlike user activity and favorites, which makes it hard for users to efficiently\nfind high-quality apps. To address these challenges, we propose LaQual, an\nautomated framework for evaluating the quality of LLM apps. LaQual consists of\nthree main stages: first, it labels and classifies LLM apps in a hierarchical\nway to accurately match them to different scenarios; second, it uses static\nindicators, such as time-weighted user engagement and functional capability\nmetrics, to filter out low-quality apps; and third, it conducts a dynamic,\nscenario-adaptive evaluation, where the LLM itself generates scenario-specific\nevaluation metrics, scoring rules, and tasks for a thorough quality assessment.\nExperiments on a popular LLM app store show that LaQual is effective. Its\nautomated scores are highly consistent with human judgments (with Spearman's\nrho of 0.62 and p=0.006 in legal consulting, and rho of 0.60 and p=0.009 in\ntravel planning). By effectively screening, LaQual can reduce the pool of\ncandidate LLM apps by 66.7% to 81.3%. User studies further confirm that LaQual\nsignificantly outperforms baseline systems in decision confidence, comparison\nefficiency (with average scores of 5.45 compared to 3.30), and the perceived\nvalue of its evaluation reports (4.75 versus 2.25). Overall, these results\ndemonstrate that LaQual offers a scalable, objective, and user-centered\nsolution for finding and recommending high-quality LLM apps in real-world use\ncases.", "AI": {"tldr": "LaQual is an automated framework that improves ranking and recommendation in LLM app stores via hierarchical classification, advanced filtering, and dynamic, scenario-based evaluation. Experiments show its results align with human judgments and outperform current baselines in efficiency and user confidence, offering a better way to surface high-quality LLM apps.", "motivation": "Current LLM app stores use static metrics (like user activity and favorites) for ranking and recommendations, which do not efficiently help users find high-quality apps. There is a need for a more robust and intelligent quality evaluation framework for LLM-based apps.", "method": "The paper introduces LaQual, a three-stage automated framework: (1) hierarchically labeling and classifying LLM apps for scenario matching; (2) filtering out low-quality apps using advanced static indicators; (3) applying dynamic, scenario-adaptive evaluation where LLMs generate specific metrics, scoring rules, and tasks tailored to each scenario.", "result": "LaQual's automated quality scores closely match human assessments (Spearman's rho 0.62 and 0.60 with significant p-values in the evaluated domains). It can reduce the app candidate pool by 66.7% to 81.3%. User studies show LaQual outperforms baselines in decision confidence, efficiency, and perceived value of evaluations.", "conclusion": "LaQual provides a scalable, objective, and user-centered framework for evaluating and recommending high-quality LLM apps in app stores, addressing limitations of current static-metric-based approaches."}}
{"id": "2508.18675", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18675", "abs": "https://arxiv.org/abs/2508.18675", "authors": ["Xu Lu", "Weisong Sun", "Yiran Zhang", "Ming Hu", "Cong Tian", "Zhi Jin", "Yang Liu"], "title": "Requirements Development and Formalization for Reliable Code Generation: A Multi-Agent Vision", "comment": null, "summary": "Automated code generation has long been considered the holy grail of software\nengineering. The emergence of Large Language Models (LLMs) has catalyzed a\nrevolutionary breakthrough in this area. However, existing methods that only\nrely on LLMs remain inadequate in the quality of generated code, offering no\nguarantees of satisfying practical requirements. They lack a systematic\nstrategy for requirements development and modeling. Recently, LLM-based agents\ntypically possess powerful abilities and play an essential role in facilitating\nthe alignment of LLM outputs with user requirements. In this paper, we envision\nthe first multi-agent framework for reliable code generation based on\n\\textsc{re}quirements \\textsc{de}velopment and \\textsc{fo}rmalization, named\n\\textsc{ReDeFo}. This framework incorporates three agents, highlighting their\naugmentation with knowledge and techniques of formal methods, into the\nrequirements-to-code generation pipeline to strengthen quality assurance. The\ncore of \\textsc{ReDeFo} is the use of formal specifications to bridge the gap\nbetween potentially ambiguous natural language requirements and precise\nexecutable code. \\textsc{ReDeFo} enables rigorous reasoning about correctness,\nuncovering hidden bugs, and enforcing critical properties throughout the\ndevelopment process. In general, our framework aims to take a promising step\ntoward realizing the long-standing vision of reliable, auto-generated software.", "AI": {"tldr": "ReDeFo is a multi-agent framework that integrates formal methods with LLM-based agents to reliably generate code from requirements, bridging the gap between ambiguous natural language and precise executable code for improved quality assurance.", "motivation": "Automated code generation is a highly desired goal in software engineering, but current approaches using only Large Language Models (LLMs) fall short in generating reliable code that meets practical requirements. These methods lack a systematic process for developing and modeling requirements.", "method": "The paper introduces ReDeFo, a multi-agent framework that combines LLM-based agents with formal methods. The framework features three agents working together to transform natural language requirements into formal specifications, which are then used for code generation and verification, ensuring higher quality and correctness.", "result": "ReDeFo uses formal specifications to rigorously reason about code correctness, uncover hidden bugs, and enforce critical properties. This strengthens quality assurance across the development pipeline and improves the reliability of auto-generated code.", "conclusion": "The framework represents a significant step toward achieving reliable automated software generation, addressing the long-standing challenges of requirement ambiguity and lack of formal verification in LLM-generated code."}}
{"id": "2508.18721", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18721", "abs": "https://arxiv.org/abs/2508.18721", "authors": ["Yunrui Pei", "Hongshu Wang", "Wenjie Zhang", "Yun Lin", "Weiyu Kong", "Jin song Dong"], "title": "LLM as an Execution Estimator: Recovering Missing Dependency for Practical Time-travelling Debugging", "comment": null, "summary": "Dynamic data dependency, answering \"why a variable has this value?\", is\ncritical for debugging. Given a program step `s` reading a variable `v`,\nfinding the dynamic definition of `v` is challenging. Traditional methods\nrequire either (1) exhaustive instrumentation of all possible definitions of\n`v` in one run or (2) replicating the run to re-examine reads/writes - both\ncostly. If `v` is defined in a library, instrumentation becomes expensive; for\nnon-deterministic programs, replication is infeasible.\n  We propose RecovSlicing, which computes dynamic data dependency in a single\nrun with partial instrumentation. We leverage LLMs to infer program behavior\nfrom a partially recorded trace and code context. Given a trace and a slicing\ncriterion (step `s` and variable `v`), RecovSlicing estimates the runtime\ndefinition of `v` by recovering the missing execution.It also supports implicit\nvariables, such as those in `list.get(i)`. Technically, RecovSlicing tackles:\n(1) recovering runtime values and structures, and (2) aligning recovered\nvariables with recorded memory to analyze definitions.\n  We evaluate RecovSlicing on 8,300 data dependencies across three slicing\nbenchmarks, comparing it with Slicer4J, ND-Slicer, LLM Slicer, and re-execution\nSlicer. RecovSlicing achieves accuracy of 80.3%, 91.1%, and 98.3%,\noutperforming the best baseline (39.0%, 82.0%, 59.9%), and also leads in recall\n(91.1%, 91.1%, 98.3% vs. 53.4%, 79.1%, 87.1%). Integrated into a regression bug\nlocalizer, it enables finding 16% more regressions.", "AI": {"tldr": "RecovSlicing is a new method for tracking dynamic data dependencies for debugging, using LLMs and partial instrumentation. It offers higher accuracy and recall than existing tools, requires only a single program run, and enables finding more bugs, making debugging more efficient and feasible in challenging scenarios.", "motivation": "Dynamic data dependency analysis is crucial for debugging: it helps answer why a variable holds a particular value during execution. Existing methods either require broad, invasive instrumentation of all possible variable definitions or multiple program runs, which is inefficient, especially in large, non-deterministic, or library-heavy codebases.", "method": "The paper proposes RecovSlicing, a novel approach using partial instrumentation and leveraging LLMs (large language models) to infer unrecorded program behaviors. Given a program execution trace and a particular slicing criterion (a program step and variable of interest), RecovSlicing approximates how a variable's value was defined by reconstructing missing executions and aligning inferred variables to memory states. It also supports tracing implicit variables, such as those inside data structures like list.get(i).", "result": "The authors evaluated RecovSlicing on 8,300 data dependencies using three established slicing benchmarks. RecovSlicing achieved high accuracy (80.3%, 91.1%, and 98.3%) compared to current state-of-the-art tools, significantly outperforming them. Its recall was also higher (91.1%, 91.1%, 98.3%). Furthermore, when integrated with a regression bug localizer, RecovSlicing facilitated the identification of 16% more regressions.", "conclusion": "RecovSlicing substantially improves dynamic data dependency analysis by allowing effective debugging with partial instrumentation, reducing overhead, increasing accuracy and recall, and making it practical for non-deterministic or library-intensive programs. Its integration into bug localization further demonstrates its practical value."}}
{"id": "2508.18771", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18771", "abs": "https://arxiv.org/abs/2508.18771", "authors": ["Kexin Sun", "Hongyu Kuang", "Sebastian Baltes", "Xin Zhou", "He Zhang", "Xiaoxing Ma", "Guoping Rong", "Dong Shao", "Christoph Treude"], "title": "Does AI Code Review Lead to Code Changes? A Case Study of GitHub Actions", "comment": null, "summary": "AI-based code review tools automatically review and comment on pull requests\nto improve code quality. Despite their growing presence, little is known about\ntheir actual impact. We present a large-scale empirical study of 16 popular\nAI-based code review actions for GitHub workflows, analyzing more than 22,000\nreview comments in 178 repositories. We investigate (1) how these tools are\nadopted and configured, (2) whether their comments lead to code changes, and\n(3) which factors influence their effectiveness. We develop a two-stage\nLLM-assisted framework to determine whether review comments are addressed, and\nuse interpretable machine learning to identify influencing factors. Our\nfindings show that, while adoption is growing, effectiveness varies widely.\nComments that are concise, contain code snippets, and are manually triggered,\nparticularly those from hunk-level review tools, are more likely to result in\ncode changes. These results highlight the importance of careful tool design and\nsuggest directions for improving AI-based code review systems.", "AI": {"tldr": "AI code review tools are being adopted more, but not all comments lead to code changes. Concise, code-containing, and manually triggered comments\u2014especially from hunk-level tools\u2014are most effective. Better tool design and understanding of key factors can improve these systems.", "motivation": "AI-based code review tools are increasingly used in software development to improve code quality, but there is limited knowledge about their real-world impact and effectiveness.", "method": "The authors conducted a large-scale empirical study analyzing over 22,000 review comments from 16 popular AI-based code review tools used in GitHub workflows across 178 repositories. They used a two-stage LLM-assisted framework to classify whether comments led to code changes and employed interpretable machine learning to identify factors influencing effectiveness.", "result": "Adoption of AI-based code review tools is increasing, but their effectiveness in prompting code changes varies. Comments that are concise, include code snippets, and are manually triggered, especially those from hunk-level review tools, are more likely to lead to code changes.", "conclusion": "Careful design of AI-based code review tools is crucial for effectiveness. The study provides insights into which factors and design choices can enhance the impact of such systems and suggests directions for future improvements."}}
{"id": "2508.18816", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18816", "abs": "https://arxiv.org/abs/2508.18816", "authors": ["Sabato Nocera", "Davide Fucci", "Giuseppe Scanniello"], "title": "Dealing with SonarQube Cloud: Initial Results from a Mining Software Repository Study", "comment": "Accepted for ESEM25 NIER track", "summary": "Background: Static Code Analysis (SCA) tools are widely adopted to enforce\ncode quality standards. However, little is known about how open-source projects\nuse and customize these tools. Aims: This paper investigates how GitHub\nprojects use and customize a popular SCA tool, namely SonarQube Cloud. Method:\nWe conducted a mining study of GitHub projects that are linked through GitHub\nActions to SonarQube Cloud projects. Results: Among 321 GitHub projects using\nSonarQube Cloud, 81% of them are correctly connected to SonarQube Cloud\nprojects, while others exhibit misconfigurations or restricted access. Among\n265 accessible SonarQube Cloud projects, 75% use the organization's default\nquality gate, i.e., a set of conditions that deployed source code must meet to\npass automated checks. While 55% of the projects use the built-in quality gate\nprovided by SonarQube Cloud, 45% of them customize their quality gate with\ndifferent conditions. Overall, the most common quality conditions align with\nSonarQube Cloud's \"Clean as You Code\" principle and enforce security,\nmaintainability, reliability, coverage, and a few duplicates on newly added or\nmodified source code. Conclusions: Many projects rely on predefined\nconfigurations, yet a significant portion customize their configurations to\nmeet specific quality goals. Building on our initial results, we envision a\nfuture research agenda linking quality gate configurations to actual software\noutcomes (e.g., improvement of software security). This would enable\nevidence-based recommendations for configuring SCA tools like SonarQube Cloud\nin various contexts.", "AI": {"tldr": "Most GitHub projects using SonarQube Cloud rely on default code quality settings, but many also customize them to enforce specific goals. There is potential for future research to directly link these customizations to improvements in software quality, allowing for better configuration guidelines.", "motivation": "Static Code Analysis tools like SonarQube Cloud are increasingly used to ensure code quality, but there is limited understanding of how open-source projects actually use and customize these tools.", "method": "A mining study was conducted on GitHub projects linked to SonarQube Cloud via GitHub Actions, investigating how these projects connect to and configure SonarQube Cloud.", "result": "Out of 321 GitHub projects, 81% were properly connected to SonarQube Cloud. Of 265 accessible projects, 75% used the organization's default quality gate; 55% relied on the built-in (default) quality gate, while 45% customized their configurations. Most enforcements focus on security, maintainability, reliability, and code coverage, especially for new or changed code.", "conclusion": "While many projects stick with predefined SonarQube Cloud configurations, a substantial number customize settings to fit their unique quality goals. The study suggests future work should explore how these configuration choices impact real-world software quality outcomes, enabling evidence-based tool recommendations."}}
{"id": "2508.18955", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18955", "abs": "https://arxiv.org/abs/2508.18955", "authors": ["Yunbo Ni", "Shaohua Li"], "title": "Interleaving Large Language Models for Compiler Testing", "comment": null, "summary": "Testing compilers with AI models, especially large language models (LLMs),\nhas shown great promise. However, current approaches struggle with two key\nproblems: The generated programs for testing compilers are often too simple,\nand extensive testing with the LLMs is computationally expensive. In this\npaper, we propose a novel compiler testing framework that decouples the testing\nprocess into two distinct phases: an offline phase and an online phase. In the\noffline phase, we use LLMs to generate a collection of small but feature-rich\ncode pieces. In the online phase, we reuse these code pieces by strategically\ncombining them to build high-quality and valid test programs, which are then\nused to test compilers.\n  We implement this idea in a tool, LegoFuzz, for testing C compilers. The\nresults are striking: we found 66 bugs in GCC and LLVM, the most widely used C\ncompilers. Almost half of the bugs are miscompilation bugs, which are serious\nand hard-to-find bugs that none of the existing LLM-based tools could find. We\nbelieve this efficient design opens up new possibilities for using AI models in\nsoftware testing beyond just C compilers.", "AI": {"tldr": "A new framework splits compiler testing into LLM-driven snippet generation and strategic recombination, yielding better test programs, finding tough bugs, and reducing computational effort.", "motivation": "Current AI-driven compiler testing methods suffer from overly simple generated test programs and high computational costs during extensive testing.", "method": "A two-phase framework is proposed: (1) Offline LLMs generate small, feature-rich code snippets; (2) Online these snippets are recombined to create complex, valid test cases for compiler testing.", "result": "The approach was implemented in LegoFuzz for C compilers, leading to the discovery of 66 bugs in GCC and LLVM, including many serious, hard-to-find miscompilation bugs.", "conclusion": "Decoupling test generation and execution using LLMs greatly enhances compiler bug discovery efficiency and could benefit broader software testing applications."}}
{"id": "2508.18993", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18993", "abs": "https://arxiv.org/abs/2508.18993", "authors": ["Ziyi Ni", "Huacan Wang", "Shuo Zhang", "Shuo Lu", "Ziyang He", "Wang You", "Zhenheng Tang", "Yuntao Du", "Bill Sun", "Hongzhang Liu", "Sen Hu", "Ronghao Chen", "Bo Li", "Xin Li", "Chen Hu", "Binxing Jiao", "Daxin Jiang", "Pin Lyu"], "title": "GitTaskBench: A Benchmark for Code Agents Solving Real-World Tasks Through Code Repository Leveraging", "comment": "Highly practical, Well-motivated, Actionable", "summary": "Beyond scratch coding, exploiting large-scale code repositories (e.g.,\nGitHub) for practical tasks is vital in real-world software development, yet\ncurrent benchmarks rarely evaluate code agents in such authentic,\nworkflow-driven scenarios. To bridge this gap, we introduce GitTaskBench, a\nbenchmark designed to systematically assess this capability via 54 realistic\ntasks across 7 modalities and 7 domains. Each task pairs a relevant repository\nwith an automated, human-curated evaluation harness specifying practical\nsuccess criteria. Beyond measuring execution and task success, we also propose\nthe alpha-value metric to quantify the economic benefit of agent performance,\nwhich integrates task success rates, token cost, and average developer\nsalaries. Experiments across three state-of-the-art agent frameworks with\nmultiple advanced LLMs show that leveraging code repositories for complex task\nsolving remains challenging: even the best-performing system, OpenHands+Claude\n3.7, solves only 48.15% of tasks. Error analysis attributes over half of\nfailures to seemingly mundane yet critical steps like environment setup and\ndependency resolution, highlighting the need for more robust workflow\nmanagement and increased timeout preparedness. By releasing GitTaskBench, we\naim to drive progress and attention toward repository-aware code reasoning,\nexecution, and deployment -- moving agents closer to solving complex,\nend-to-end real-world tasks. The benchmark and code are open-sourced at\nhttps://github.com/QuantaAlpha/GitTaskBench.", "AI": {"tldr": "The paper introduces GitTaskBench, a new benchmark and evaluation harness for assessing code agents on realistic repository-driven software tasks. Results show that advanced agents still struggle with practical workflow management, solving less than half of the tasks. The work provides a challenging new testbed and proposes a metric (alpha-value) to guide future research toward more robust, real-world code agent systems.", "motivation": "Existing benchmarks for code agents do not adequately evaluate their performance in realistic, workflow-driven scenarios involving practical use of large-scale code repositories, such as those on GitHub, which are critical in real-world software development.", "method": "The authors introduce GitTaskBench, a benchmark featuring 54 realistic coding tasks spanning 7 modalities and 7 domains, each paired with a relevant repository and an automated evaluation harness defining practical success criteria. They also propose a new metric called alpha-value to quantify the economic benefit of agent performance by integrating success rates, computational costs, and developer salaries. They evaluate the benchmark using three state-of-the-art agent frameworks paired with several advanced LLMs.", "result": "The experiments reveal that current advanced code agents struggle with realistic repository-centric code tasks, with the best-performing system (OpenHands+Claude 3.7) solving only 48.15% of tasks. Over half of task failures are due to basic but crucial steps like environment setup and dependency resolution.", "conclusion": "GitTaskBench fills a critical evaluation gap for code agents by focusing on authentic, repository-driven tasks and practical workflow obstacles. The benchmark highlights significant challenges that remain for code agents to solve end-to-end real-world software development problems. The open-source release aims to foster research on repository-aware code reasoning and task execution."}}
{"id": "2508.19056", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.19056", "abs": "https://arxiv.org/abs/2508.19056", "authors": ["S. Panda", "D. Munjal", "D. P. Mohapatra"], "title": "A Slice-Based Change Impact Analysis for Regression Test Case Prioritization of Object-Oriented Programs", "comment": null, "summary": "Test case prioritization focuses on finding a suitable order of execution of\nthe test cases in a test suite to meet some performance goals like detecting\nfaults early. It is likely that some test cases execute the program parts that\nare more prone to errors and will detect more errors if executed early during\nthe testing process. Finding an optimal order of execution for the selected\nregression test cases saves time and cost of retesting. This paper presents a\nstatic approach to prioritizing the test cases by computing the affected\ncomponent coupling (ACC) of the affected parts of object-oriented programs. We\nconstruct a graph named affected slice graph (ASG) to represent these affected\nprogram parts.We determine the fault-proneness of the nodes of ASG by computing\ntheir respective ACC values. We assign higher priority to those test cases that\ncover the nodes with higher ACC values. Our analysis with mutation faults shows\nthat the test cases executing the fault-prone program parts have a higher\nchance to reveal faults earlier than other test cases in the test suite. The\nresult obtained from seven case studies justifies that our approach is feasible\nand gives acceptable performance in comparison to some existing techniques.", "AI": {"tldr": "The paper introduces a static method for prioritizing test cases based on affected component coupling in object-oriented software. By focusing on fault-prone areas (identified via ACC and ASG), the approach helps reveal faults earlier and shows competitive performance compared to other methods, based on results from multiple case studies.", "motivation": "Test case prioritization in software testing is important to detect faults early and reduce retesting time and cost. The motivation is to improve the efficiency of regression testing by ordering test cases based on their potential to detect errors.", "method": "The authors propose a static approach for prioritizing test cases by calculating the affected component coupling (ACC) of program parts in object-oriented software. They construct an affected slice graph (ASG) to represent these parts, calculate ACC values for ASG nodes, and assign higher priority to tests covering nodes with higher ACC.", "result": "The approach was analyzed using mutation faults and applied to seven case studies. Results show that test cases covering fault-prone program parts (with higher ACC) tend to reveal faults earlier. The proposed technique performs acceptably compared to existing methods.", "conclusion": "The static ACC-based prioritization approach is feasible and improves fault detection performance, justifying its use over some current techniques."}}
