{"id": "2509.04936", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.04936", "abs": "https://arxiv.org/abs/2509.04936", "authors": ["Andrea Gilot", "Tobias Wrigstad", "Eva Darulova"], "title": "A Large-Scale Study of Floating-Point Usage in Statically Typed Languages", "comment": null, "summary": "Reasoning about floating-point arithmetic is notoriously hard. While static\nand dynamic analysis techniques or program repair have made significant\nprogress, more work is still needed to make them relevant to real-world code.\nOn the critical path to that goal is understanding what real-world\nfloating-point code looks like. To close that knowledge gap, this paper\npresents the first large-scale empirical study of floating-point arithmetic\nusage in statically typed languages across public GitHub repositories. We\nfollow state-of the art mining practices including random sampling and\nfiltering based on only intrinsic properties to avoid bias, and identify\nfloating-point usage by searching for keywords in the source code, and\nprogramming language constructs (e.g., loops) by parsing the code. Our\nevaluation supports the claim often made in papers that floating-point\narithmetic is widely used. Comparing statistics such as size and usage of\ncertain constructs and functions, we find that benchmarks used in literature to\nevaluate automated reasoning techniques for floating-point arithmetic are in\ncertain aspects representative of 'real-world' code, but not in all. We aim for\nour study and dataset to help future techniques for floating-point arithmetic\nto be designed and evaluated to match actual users' expectations.", "AI": {"tldr": "This paper presents a comprehensive empirical study of floating-point usage in public GitHub repositories, confirming its widespread use and highlighting differences between benchmarks and real-world code to guide future research tools.", "motivation": "Reasoning about floating-point arithmetic is difficult, and current techniques are not fully relevant to real-world code because the characteristics of actual floating-point code are not well understood.", "method": "The paper performs a large-scale empirical study of floating-point arithmetic usage by mining public GitHub repositories written in statically typed languages. It uses random sampling, intrinsic filtering to avoid bias, keyword searches, and code parsing for identifying floating-point usage and language constructs.", "result": "The study confirms that floating-point arithmetic is widely used in real-world code. It finds that while some aspects of benchmark code used in research are representative of real-world usage, there are notable differences in other aspects.", "conclusion": "The results and released dataset can guide the development and evaluation of future static and dynamic analysis techniques for floating-point arithmetic, helping them better address users' real-world needs."}}
{"id": "2509.05160", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.05160", "abs": "https://arxiv.org/abs/2509.05160", "authors": ["Steven Smyth", "Daniel Busch", "Moez Ben Haj Hmida", "Edward A. Lee", "Bernhard Steffen"], "title": "AI-Assisted Modeling: DSL-Driven AI Interactions", "comment": "7 pages, 4 figures", "summary": "AI-assisted programming greatly increases software development performance.\nWe enhance this potential by integrating transparency through domain-specific\nmodeling techniques and providing instantaneous, graphical visualizations that\naccurately represent the semantics of AI-generated code. This approach\nfacilitates visual inspection and formal verification, such as model checking.\n  Formal models can be developed using programming, natural language prompts,\nvoice commands, and stage-wise refinement, with immediate feedback after each\ntransformation step. This support can be tailored to specific domains or\nintended purposes, improving both code generation and subsequent validation\nprocesses.\n  To demonstrate the effectiveness of this approach, we have developed a\nprototype as a Visual Studio Code extension for the Lingua Franca language.\nThis prototype showcases the potential for novel domain-specific modeling\npractices, offering an advancement in how models are created, visualized, and\nverified.", "AI": {"tldr": "The paper presents a VS Code extension that enhances AI programming by enabling immediate, visual, and formal validation of models through domain-specific graphical representations, supporting trustworthiness and efficiency in code development.", "motivation": "AI-assisted programming enhances software development, but transparency and trustworthy validation are lacking. Integrating formal verification and making AI outputs more understandable are key challenges addressed.", "method": "The authors propose the integration of domain-specific modeling techniques with AI programming tools, delivering instantaneous, graphical visualizations of the AI-generated code. This includes interactive support for programming, natural language, voice, and step-wise refinement with immediate feedback. They implemented a Visual Studio Code extension for Lingua Franca as a prototype.", "result": "The resulting prototype enables the creation, visualization, and verification of formal models within a development environment, demonstrating instantaneous visual feedback and domain-specific adaptation. It facilitates both intuitive modeling and rigorous validation.", "conclusion": "Combining AI code generation with transparent, domain-specific visualizations and formal verification methods significantly improves both the efficiency and reliability of software development and validation workflows."}}
{"id": "2509.05293", "categories": ["cs.PL", "cs.CL", "cs.SE", "D.3; F.3"], "pdf": "https://arxiv.org/pdf/2509.05293", "abs": "https://arxiv.org/abs/2509.05293", "authors": ["Julien Vanegue", "Jules Villard", "Peter O'Hearn", "Azalea Raad"], "title": "Non-Termination Proving: 100 Million LoC and Beyond", "comment": "14 pages, 4 figures", "summary": "We report on our tool, Pulse Infinite, that uses proof techniques to show\nnon-termination (divergence) in large programs. Pulse Infinite works\ncompositionally and under-approximately: the former supports scale, and the\nlatter ensures soundness for proving divergence. Prior work focused on small\nbenchmarks in the tens or hundreds of lines of code (LoC), and scale limits\ntheir practicality: a single company may have tens of millions, or even\nhundreds of millions of LoC or more. We report on applying Pulse Infinite to\nover a hundred million lines of open-source and proprietary software written in\nC, C++, and Hack, identifying over 30 previously unknown issues, establishing a\nnew state of the art for detecting divergence in real-world codebases.", "AI": {"tldr": "Pulse Infinite is a scalable proof-based tool that detects non-termination in massive software projects, finding many new issues and setting a new benchmark for real-world applicability.", "motivation": "Detecting non-termination (divergence) bugs in large-scale software is challenging, and previous techniques only worked on small codebases, limiting their practical use for industry-scale software.", "method": "The authors introduce Pulse Infinite, a compositional and under-approximate proof-based tool designed to identify non-termination in large codebases. The compositional approach supports scalability, and under-approximation ensures soundness in proving divergence.", "result": "Pulse Infinite was applied to over a hundred million lines of code in various programming languages (C, C++, Hack), finding over 30 previously unknown non-termination issues in real-world open-source and proprietary software.", "conclusion": "Pulse Infinite establishes a new state of the art for detecting divergence in real-world, large-scale codebases, overcoming the scalability limitations of prior work."}}
{"id": "2509.04644", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.04644", "abs": "https://arxiv.org/abs/2509.04644", "authors": ["Subhang Boorlagadda", "Nitya Naga Sai Atluri", "Muhammet Mustafa Olmez", "Edward F. Gehringer"], "title": "Comparative Evaluation of Large Language Models for Test-Skeleton Generation", "comment": "Forthcoming in Frontiers in Education (FIE 2025), Nashville,\n  Tennessee, USA, Nov 2-5, 2025", "summary": "This paper explores the use of Large Language Models (LLMs) to automate the\ngeneration of test skeletons -- structural templates that outline unit test\ncoverage without implementing full test logic. Test skeletons are especially\nimportant in test-driven development (TDD), where they provide an early\nframework for systematic verification. Traditionally authored manually, their\ncreation can be time-consuming and error-prone, particularly in educational or\nlarge-scale development settings. We evaluate four LLMs -- GPT-4,\nDeepSeek-Chat, Llama4-Maverick, and Gemma2-9B -- on their ability to generate\nRSpec skeletons for a real-world Ruby class developed in a university software\nengineering course. Each model's output is assessed using static analysis and a\nblind expert review to measure structural correctness, clarity,\nmaintainability, and conformance to testing best practices. The study reveals\nkey differences in how models interpret code structure and testing conventions,\noffering insights into the practical challenges of using LLMs for automated\ntest scaffolding. Our results show that DeepSeek generated the most\nmaintainable and well-structured skeletons, while GPT-4 produced more complete\nbut conventionally inconsistent output. The study reveals prompt design and\ncontextual input as key quality factors.", "AI": {"tldr": "Evaluated four LLMs for generating Ruby test skeletons; DeepSeek delivered the most maintainable results, while GPT-4 was complete but inconsistent. Prompt design is crucial for quality output.", "motivation": "Manual creation of test skeletons for TDD is time-consuming and prone to errors, especially for students and large development teams. Automating this process using LLMs could improve efficiency and consistency.", "method": "The study evaluates four LLMs (GPT-4, DeepSeek-Chat, Llama4-Maverick, Gemma2-9B) on their ability to generate RSpec test skeletons for a Ruby class. Outputs were assessed via static analysis and blind expert review on criteria such as correctness, clarity, maintainability, and best practice alignment.", "result": "DeepSeek-Chat generated the most maintainable and well-structured skeletons. GPT-4 produced outputs that were more complete but less consistent with conventions. Differences in code structure interpretation and prompt/context effects were observed across models.", "conclusion": "Key factors influencing LLM output quality for test skeleton generation are prompt design and contextual input. While some models (DeepSeek) excel in maintainability, others (GPT-4) may require further refinement for consistent best practice adherence."}}
{"id": "2509.04721", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04721", "abs": "https://arxiv.org/abs/2509.04721", "authors": ["Abhishek Dey", "Saurabh Srivastava", "Gaurav Singh", "Robert G. Pettit"], "title": "Real-Time Performance Benchmarking of TinyML Models in Embedded Systems (PICO: Performance of Inference, CPU, and Operations)", "comment": null, "summary": "This paper presents PICO-TINYML-BENCHMARK, a modular and platform-agnostic\nframework for benchmarking the real-time performance of TinyML models on\nresource-constrained embedded systems. Evaluating key metrics such as inference\nlatency, CPU utilization, memory efficiency, and prediction stability, the\nframework provides insights into computational trade-offs and platform-specific\noptimizations. We benchmark three representative TinyML models -- Gesture\nClassification, Keyword Spotting, and MobileNet V2 -- on two widely adopted\nplatforms, BeagleBone AI64 and Raspberry Pi 4, using real-world datasets.\nResults reveal critical trade-offs: the BeagleBone AI64 demonstrates consistent\ninference latency for AI-specific tasks, while the Raspberry Pi 4 excels in\nresource efficiency and cost-effectiveness. These findings offer actionable\nguidance for optimizing TinyML deployments, bridging the gap between\ntheoretical advancements and practical applications in embedded systems.", "AI": {"tldr": "The paper introduces a new benchmarking framework for TinyML on embedded devices, showing how two popular hardware platforms compare in key performance metrics. The results guide practical TinyML deployment decisions.", "motivation": "There is a growing need to evaluate TinyML models on resource-constrained embedded systems, but existing benchmarks may lack modularity and platform independence. This paper aims to provide a benchmark framework specifically for real-time TinyML performance on embedded devices.", "method": "The authors propose and implement PICO-TINYML-BENCHMARK, a modular, platform-agnostic benchmarking framework. They evaluate three TinyML models\u2014Gesture Classification, Keyword Spotting, and MobileNet V2\u2014across two platforms (BeagleBone AI64 and Raspberry Pi 4), using real-world datasets and metrics such as inference latency, CPU usage, memory efficiency, and prediction stability.", "result": "Benchmarking results show that the BeagleBone AI64 platform performs best for consistent inference latency in AI tasks, while the Raspberry Pi 4 provides superior resource efficiency and cost effectiveness. The collected metrics reveal important trade-offs between platforms.", "conclusion": "The provided benchmarking framework effectively highlights the strengths and weaknesses of different embedded platforms for TinyML deployment. It helps bridge the gap between theory and practical application by giving actionable recommendations for optimizing TinyML on real-world embedded systems."}}
{"id": "2509.04763", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.04763", "abs": "https://arxiv.org/abs/2509.04763", "authors": ["Tiancheng Jin", "Shangzhou Xia", "Jianjun Zhao"], "title": "NovaQ: Improving Quantum Program Testing through Diversity-Guided Test Case Generation", "comment": "5 pages", "summary": "Quantum programs are designed to run on quantum computers, leveraging quantum\ncircuits to solve problems that are intractable for classical machines. As\nquantum computing advances, ensuring the reliability of quantum programs has\nbecome increasingly important. This paper introduces NovaQ, a diversity-guided\ntesting framework for quantum programs. NovaQ combines a distribution-based\ntest case generator with a novelty-driven evaluation module. The generator\nproduces diverse quantum state inputs by mutating circuit parameters, while the\nevaluator quantifies behavioral novelty based on internal circuit state\nmetrics, including magnitude, phase, and entanglement. By selecting inputs that\nmap to infrequently covered regions in the metric space, NovaQ effectively\nexplores under-tested program behaviors. We evaluate NovaQ on quantum programs\nof varying sizes and complexities. Experimental results show that NovaQ\nconsistently achieves higher test input diversity and detects more bugs than\nexisting baseline approaches.", "AI": {"tldr": "NovaQ is a novel quantum program testing framework that enhances bug detection and test diversity by generating and prioritizing novel quantum circuit inputs, outperforming traditional methods.", "motivation": "As quantum computing progresses, reliable quantum programs are more critical, but testing them thoroughly is a challenge due to their unique behaviors and state spaces.", "method": "NovaQ is proposed as a testing framework combining a distribution-based test case generator (mutates quantum circuit parameters for diverse states) and a novelty-driven evaluator (measures behavioral novelty via magnitude, phase, and entanglement metrics). It prioritizes test inputs that explore rarely tested behavioral regions.", "result": "NovaQ achieves higher diversity in test cases and detects more bugs compared to existing baseline testing approaches, as demonstrated on various quantum programs.", "conclusion": "NovaQ provides a more effective method for uncovering bugs and increasing behavioral coverage in quantum programs by focusing on diversity and novelty in test input selection."}}
{"id": "2509.04810", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04810", "abs": "https://arxiv.org/abs/2509.04810", "authors": ["Yogev Cohen", "Dudi Ohayon", "Romy Somkin", "Yehudit Aperstein", "Alexander Apartsin"], "title": "Code Review Without Borders: Evaluating Synthetic vs. Real Data for Review Recommendation", "comment": "4 pages, 1 figure", "summary": "Automating the decision of whether a code change requires manual review is\nvital for maintaining software quality in modern development workflows.\nHowever, the emergence of new programming languages and frameworks creates a\ncritical bottleneck: while large volumes of unlabelled code are readily\navailable, there is an insufficient amount of labelled data to train supervised\nmodels for review classification. We address this challenge by leveraging Large\nLanguage Models (LLMs) to translate code changes from well-resourced languages\ninto equivalent changes in underrepresented or emerging languages, generating\nsynthetic training data where labelled examples are scarce. We assume that\nalthough LLMs have learned the syntax and semantics of new languages from\navailable unlabelled code, they have yet to fully grasp which code changes are\nconsidered significant or review-worthy within these emerging ecosystems. To\novercome this, we use LLMs to generate synthetic change examples and train\nsupervised classifiers on them. We systematically compare the performance of\nthese classifiers against models trained on real labelled data. Our experiments\nacross multiple GitHub repositories and language pairs demonstrate that\nLLM-generated synthetic data can effectively bootstrap review recommendation\nsystems, narrowing the performance gap even in low-resource settings. This\napproach provides a scalable pathway to extend automated code review\ncapabilities to rapidly evolving technology stacks, even in the absence of\nannotated data.", "AI": {"tldr": "This paper shows that LLMs can generate synthetic code review data to train classifiers for new programming languages, helping automate code review when there isn\u2019t enough labelled data.", "motivation": "There is a lack of labelled data in emerging programming languages, which creates a bottleneck for automating code review decisions, despite the availability of large volumes of unlabelled code.", "method": "The paper leverages Large Language Models (LLMs) to translate code changes from well-resourced languages into underrepresented or emerging languages, generating synthetic training data for supervised classifier training. Supervised classifiers are trained on this data and compared with models trained on real labelled data.", "result": "LLM-generated synthetic data can effectively bootstrap review recommendation systems and narrow the performance gap between low-resource and well-resourced settings.", "conclusion": "Automated code review systems can be extended to new languages using LLM-generated synthetic training data, providing a scalable solution even when annotated data is minimal or unavailable."}}
{"id": "2509.04877", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.04877", "abs": "https://arxiv.org/abs/2509.04877", "authors": ["Maryam Khan", "Muhammad Azeem Akbar", "Jussi Kasurinen"], "title": "Integrating Large Language Models in Software Engineering Education: A Pilot Study through GitHub Repositories Mining", "comment": null, "summary": "Context: Large Language Models (LLMs) such as ChatGPT are increasingly\nadopted in software engineering (SE) education, offering both opportunities and\nchallenges. Their adoption requires systematic investigation to ensure\nresponsible integration into curricula. Objective: This doctoral research aims\nto develop a validated framework for integrating LLMs into SE education through\na multi-phase process, including taxonomies development, empirical\ninvestigation, and case studies. This paper presents the first empirical step.\nMethod: We conducted a pilot repository mining study of 400 GitHub projects,\nanalyzing README files and issues discussions to identify the presence of\nmotivator and demotivator previously synthesized in our literature review [ 8]\nstudy. Results: Motivators such as engagement and motivation (227 hits),\nsoftware engineering process understanding (133 hits), and programming\nassistance and debugging support (97 hits) were strongly represented.\nDemotivators, including plagiarism and IP concerns (385 hits), security,\nprivacy and data integrity (87 hits), and over-reliance on AI in learning (39\nhits), also appeared prominently. In contrast, demotivators such as challenges\nin evaluating learning outcomes and difficulty in curriculum redesign recorded\nno hits across the repositories. Conclusion: The study provides early empirical\nvalidation of motivators/demotivators taxonomies with respect to their themes,\nhighlights research practice gaps, and lays the foundation for developing a\ncomprehensive framework to guide the responsible adoption of LLMs in SE\neducation.", "AI": {"tldr": "This paper empirically validates key motivators and demotivators of using LLMs in SE education by analyzing 400 GitHub repositories. It confirms the importance of engagement, motivation, and programming support as motivators, and highlights concerns like plagiarism and security as major demotivators. Some concerns seen in literature did not appear in practice. The findings help shape a future framework for responsible LLM use in education.", "motivation": "The paper is motivated by the growing adoption of Large Language Models (LLMs) like ChatGPT in software engineering education. With the increasing use, there is a need to systematically investigate the responsible and effective integration of LLMs into educational curricula, understanding both the opportunities and the challenges they present.", "method": "The authors conducted a pilot repository mining study, analyzing 400 GitHub project repositories. They focused on README files and issue discussions to find evidence of motivators and demotivators previously identified in their literature review.", "result": "The study identified strong motivators for LLM adoption, including engagement and motivation, process understanding, and programming assistance (with numerous mentions for each). Prominent demotivators were also found, particularly plagiarism/IP concerns, as well as security and privacy issues. However, some demotivators, such as challenges in learning outcome evaluation or curriculum redesign, were not observed in the repositories.", "conclusion": "This empirical study provides early validation for the motivators and demotivators taxonomy reflecting real-world usage themes. It exposes gaps in research and sets the groundwork for a comprehensive framework to responsibly guide LLM integration into software engineering education curricula."}}
{"id": "2509.04967", "categories": ["cs.SE", "cs.CR", "D.2.5"], "pdf": "https://arxiv.org/pdf/2509.04967", "abs": "https://arxiv.org/abs/2509.04967", "authors": ["Kai Feng", "Jeremy Singer", "Angelos K Marnerides"], "title": "FuzzRDUCC: Fuzzing with Reconstructed Def-Use Chain Coverage", "comment": null, "summary": "Binary-only fuzzing often struggles with achieving thorough code coverage and\nuncovering hidden vulnerabilities due to limited insight into a program's\ninternal dataflows. Traditional grey-box fuzzers guide test case generation\nprimarily using control flow edge coverage, which can overlook bugs not easily\nexposed through control flow analysis alone. We argue that integrating dataflow\nanalysis into the fuzzing process can enhance its effectiveness by revealing\nhow data propagates through the program, thereby enabling the exploration of\nexecution paths that control flow-based methods might miss. In this context, we\nintroduce FuzzRDUCC, a novel fuzzing framework that employs symbolic execution\nto reconstruct definition-use (def-use) chains directly from binary\nexecutables. FuzzRDUCC identifies crucial dataflow paths and exposes security\nvulnerabilities without incurring excessive computational overhead, due to a\nnovel heuristic algorithm that selects relevant def-use chains without\naffecting the thoroughness of the fuzzing process. We evaluate FuzzRDUCC using\nthe binutils benchmark and demonstrate that it can identify unique crashes not\nfound by state-of-the-art fuzzers. Hence, establishing FuzzRDUCC as a feasible\nsolution for next generation vulnerability detection and discovery mechanisms.", "AI": {"tldr": "FuzzRDUCC is a new binary-only fuzzing framework that uses symbolic execution for dataflow analysis, exposing vulnerabilities missed by control flow-guided fuzzers, and demonstrates effectiveness by finding unique crashes in benchmarks.", "motivation": "Binary-only fuzzing is limited in code coverage and vulnerability detection because it mainly uses control flow edge coverage, missing bugs related to dataflow.", "method": "FuzzRDUCC integrates dataflow analysis with fuzzing by employing symbolic execution to extract definition-use chains from binary executables. It uses a heuristic algorithm to select relevant def-use chains, maintaining efficiency and fuzzing thoroughness.", "result": "FuzzRDUCC identifies crucial dataflow paths and uncovers security vulnerabilities that traditional fuzzers miss. Evaluation on binutils benchmark showed it found unique crashes that existing fuzzers did not.", "conclusion": "FuzzRDUCC advances vulnerability detection for binaries by integrating efficient dataflow tracking into the fuzzing process, making it a strong candidate for future security testing tools."}}
{"id": "2509.05112", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05112", "abs": "https://arxiv.org/abs/2509.05112", "authors": ["Denesa Zyberaj", "Lukasz Mazur", "Nenad Petrovic", "Pankhuri Verma", "Pascal Hirmer", "Dirk Slama", "Xiangwei Cheng", "Alois Knoll"], "title": "GenAI-based test case generation and execution in SDV platform", "comment": null, "summary": "This paper introduces a GenAI-driven approach for automated test case\ngeneration, leveraging Large Language Models and Vision-Language Models to\ntranslate natural language requirements and system diagrams into structured\nGherkin test cases. The methodology integrates Vehicle Signal Specification\nmodeling to standardize vehicle signal definitions, improve compatibility\nacross automotive subsystems, and streamline integration with third-party\ntesting tools. Generated test cases are executed within the digital.auto\nplayground, an open and vendor-neutral environment designed to facilitate rapid\nvalidation of software-defined vehicle functionalities. We evaluate our\napproach using the Child Presence Detection System use case, demonstrating\nsubstantial reductions in manual test specification effort and rapid execution\nof generated tests. Despite significant automation, the generation of test\ncases and test scripts still requires manual intervention due to current\nlimitations in the GenAI pipeline and constraints of the digital.auto platform.", "AI": {"tldr": "GenAI (LLMs and VLMs) are used to automate the creation of vehicle software test cases from requirements and diagrams, reducing manual labor, but some human oversight is still needed due to platform and AI limitations.", "motivation": "The motivation for this paper lies in reducing manual effort and increasing efficiency in generating structured test cases for software-defined vehicles by harnessing the capabilities of GenAI, particularly Large Language Models and Vision-Language Models.", "method": "The paper presents a methodology that leverages GenAI technologies to translate natural language requirements and system diagrams into Gherkin test cases, integrates Vehicle Signal Specification modeling, and executes these test cases within a vendor-neutral digital.auto playground.", "result": "The proposed approach is evaluated using a Child Presence Detection System use case, resulting in significant reductions in manual test specification efforts and enabling rapid execution of the generated tests.", "conclusion": "While the GenAI-driven solution substantially automates the test case generation and execution process, some manual intervention remains necessary due to current limitations of the pipeline and the constraints of the digital.auto platform."}}
{"id": "2509.05197", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.05197", "abs": "https://arxiv.org/abs/2509.05197", "authors": ["Naimeng Ye", "Xiao Yu", "Ruize Xu", "Tianyi Peng", "Zhou Yu"], "title": "AI Agents for Web Testing: A Case Study in the Wild", "comment": null, "summary": "Automated web testing plays a critical role in ensuring high-quality user\nexperiences and delivering business value. Traditional approaches primarily\nfocus on code coverage and load testing, but often fall short of capturing\ncomplex user behaviors, leaving many usability issues undetected. The emergence\nof large language models (LLM) and AI agents opens new possibilities for web\ntesting by enabling human-like interaction with websites and a general\nawareness of common usability problems. In this work, we present WebProber, a\nprototype AI agent-based web testing framework. Given a URL, WebProber\nautonomously explores the website, simulating real user interactions,\nidentifying bugs and usability issues, and producing a human-readable report.\nWe evaluate WebProber through a case study of 120 academic personal websites,\nwhere it uncovered 29 usability issues--many of which were missed by\ntraditional tools. Our findings highlight agent-based testing as a promising\ndirection while outlining directions for developing next-generation,\nuser-centered testing frameworks.", "AI": {"tldr": "WebProber, an LLM and AI agent-based web testing tool, simulates human interactions to detect usability issues missed by traditional methods, showing the potential for smarter, user-centered automated testing frameworks.", "motivation": "Traditional web testing misses complex user behaviors and usability issues, as it mainly focuses on code coverage and load testing. With the rise of large language models (LLMs) and AI agents, there is an opportunity to improve web testing by simulating human interactions and identifying more nuanced usability problems.", "method": "The authors introduce WebProber, an AI agent-based web testing framework that uses LLM-powered agents to autonomously explore websites, simulate user interactions, detect bugs and usability issues, and generate human-readable reports. The prototype is evaluated via a case study on 120 academic personal websites.", "result": "WebProber was able to uncover 29 usability issues across 120 tested websites. Many of these issues were not detected by traditional web testing tools, suggesting the superiority of the agent-based approach in identifying nuanced usability problems.", "conclusion": "The study demonstrates that agent-based, LLM-powered web testing can identify usability issues that traditional methods often miss. This approach offers a promising direction for future, user-centered web testing frameworks that can better ensure high-quality user experiences."}}
