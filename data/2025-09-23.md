<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 4]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Digging Into the Internal: Causality-Based Analysis of LLM Function Calling](https://arxiv.org/abs/2509.16268)
*Zhenlan Ji,Daoyuan Wu,Wenxuan Wang,Pingchuan Ma,Shuai Wang,Lei Ma*

Main category: cs.SE

TL;DR: Function calling (FC) is not only a mechanism for LLMs to interact with external systems but also greatly boosts instruction compliance and safety. Causal analysis reveals FC’s substantial impact on model behavior. Experiments show FC outperforms regular prompts by 135% in detecting malicious inputs, highlighting FC’s strong potential to improve LLM robustness and reliability.


<details>
  <summary>Details</summary>
Motivation: Function calling (FC) is a promising method for improving how large language models (LLMs) execute tasks and interact with user instructions, but its influence on model behavior and effectiveness has not been thoroughly studied.

Method: The study applies causal analysis at both layer and token levels within LLMs to dissect how FC affects the models’ internal computational mechanisms and responses. The researchers also compare FC-based instruction methods with conventional prompting across four mainstream LLMs using two benchmark datasets.

Result: FC substantially improves the detection of malicious inputs and overall safety robustness of LLMs. Experimental results indicate that FC-based instructions offer an average performance increase of about 135% over conventional methods.

Conclusion: Function calling significantly enhances LLM compliance with user instructions and boosts their capability, especially in safety-critical applications, compared to traditional prompting techniques.

Abstract: Function calling (FC) has emerged as a powerful technique for facilitating
large language models (LLMs) to interact with external systems and perform
structured tasks. However, the mechanisms through which it influences model
behavior remain largely under-explored. Besides, we discover that in addition
to the regular usage of FC, this technique can substantially enhance the
compliance of LLMs with user instructions. These observations motivate us to
leverage causality, a canonical analysis method, to investigate how FC works
within LLMs. In particular, we conduct layer-level and token-level causal
interventions to dissect FC's impact on the model's internal computational
logic when responding to user queries. Our analysis confirms the substantial
influence of FC and reveals several in-depth insights into its mechanisms. To
further validate our findings, we conduct extensive experiments comparing the
effectiveness of FC-based instructions against conventional prompting methods.
We focus on enhancing LLM safety robustness, a critical LLM application
scenario, and evaluate four mainstream LLMs across two benchmark datasets. The
results are striking: FC shows an average performance improvement of around
135% over conventional prompting methods in detecting malicious inputs,
demonstrating its promising potential to enhance LLM reliability and capability
in practical applications.

</details>


### [2] [Constrained Co-evolutionary Metamorphic Differential Testing for Autonomous Systems with an Interpretability Approach](https://arxiv.org/abs/2509.16478)
*Hossein Yousefizadeh,Shenghui Gu,Lionel C. Briand,Ali Nasr*

Main category: cs.SE

TL;DR: CoCoMagic is a novel, interpretable test generation system for autonomous systems. It finds many more critical behavioral differences between software versions than previous methods, helping developers improve safety and debugging.


<details>
  <summary>Details</summary>
Motivation: Autonomous systems like self-driving cars undergo frequent updates, which risks introducing unintended behavioral degradations. Testing these systems is difficult due to the large scenario space, lack of reliable test oracles, and the need for test cases that are relevant and interpretable.

Method: The authors propose CoCoMagic, an automated test generation framework that combines metamorphic testing, differential testing, and search-based techniques. They formulate test generation as a constrained cooperative co-evolutionary search that evolves both source scenarios and metamorphic perturbations to uncover behavioral differences across system versions. Additional constraints and initialization strategies keep the generated scenarios realistic, and an interpretability module helps diagnose divergences.

Result: Experiments on the InterFuser autonomous driving stack (within the Carla simulator) demonstrate that CoCoMagic outperforms baseline approaches, finding up to 287% more distinct, high-severity behavioral differences while ensuring scenario realism. Interpretability tools help developers understand and debug root causes of those differences.

Conclusion: CoCoMagic is an efficient, effective, and interpretable test generation framework that significantly enhances the differential testing of evolving autonomous systems, enabling better safety assessment and debugging.

Abstract: Autonomous systems, such as autonomous driving systems, evolve rapidly
through frequent updates, risking unintended behavioral degradations. Effective
system-level testing is challenging due to the vast scenario space, the absence
of reliable test oracles, and the need for practically applicable and
interpretable test cases. We present CoCoMagic, a novel automated test case
generation method that combines metamorphic testing, differential testing, and
advanced search-based techniques to identify behavioral divergences between
versions of autonomous systems. CoCoMagic formulates test generation as a
constrained cooperative co-evolutionary search, evolving both source scenarios
and metamorphic perturbations to maximize differences in violations of
predefined metamorphic relations across versions. Constraints and population
initialization strategies guide the search toward realistic, relevant
scenarios. An integrated interpretability approach aids in diagnosing the root
causes of divergences. We evaluate CoCoMagic on an end-to-end ADS, InterFuser,
within the Carla virtual simulator. Results show significant improvements over
baseline search methods, identifying up to 287\% more distinct high-severity
behavioral differences while maintaining scenario realism. The interpretability
approach provides actionable insights for developers, supporting targeted
debugging and safety assessment. CoCoMagic offers an efficient, effective, and
interpretable way for the differential testing of evolving autonomous systems
across versions.

</details>


### [3] [Causal Fuzzing for Verifying Machine Unlearning](https://arxiv.org/abs/2509.16525)
*Anna Mazhar,Sainyam Galhotra*

Main category: cs.SE

TL;DR: CAF'É is a new causality-based framework for verifying machine unlearning in black-box models, detecting more subtle influences than existing methods and doing so efficiently.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the verification of machine unlearning, which is essential for enhancing adaptability, fairness, and privacy in machine learning models, especially when retraining is expensive. Current verification methods provide limited insights and often miss indirect influences.

Method: The paper proposes CAF'É, a causality-based framework that unifies datapoint and feature-level unlearning verification for black-box ML models. CAF'É analyzes both direct and indirect effects using causal dependencies, enabling fine-grained and actionable evaluation.

Result: CAF'É was evaluated on five datasets and three model architectures, demonstrating its ability to detect residual influence that baseline approaches missed, while maintaining computational efficiency.

Conclusion: CAF'É provides a more effective and efficient verification method for machine unlearning by considering both direct and indirect causal effects, outperforming existing techniques and offering fine-grained insights.

Abstract: As machine learning models become increasingly embedded in decision-making
systems, the ability to "unlearn" targeted data or features is crucial for
enhancing model adaptability, fairness, and privacy in models which involves
expensive training. To effectively guide machine unlearning, a thorough testing
is essential. Existing methods for verification of machine unlearning provide
limited insights, often failing in scenarios where the influence is indirect.
In this work, we propose CAF\'E, a new causality based framework that unifies
datapoint- and feature-level unlearning for verification of black-box ML
models. CAF\'E evaluates both direct and indirect effects of unlearning targets
through causal dependencies, providing actionable insights with fine-grained
analysis. Our evaluation across five datasets and three model architectures
demonstrates that CAF\'E successfully detects residual influence missed by
baselines while maintaining computational efficiency.

</details>


### [4] [Is Measurement Enough? Rethinking Output Validation in Quantum Program Testing](https://arxiv.org/abs/2509.16595)
*Jiaming Ye,Xiongfei Wu,Shangzhou Xia,Fuyuan Zhang,Jianjun Zhao*

Main category: cs.SE

TL;DR: Quantum program testing mostly uses measurement-based validation, but this method struggles with complex programs due to quantum probabilism. The study shows measurement-based validation works for simple tasks, while statevector-based methods are better for complex assessments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the quality assurance of quantum programs, which is a critical challenge due to the probabilistic nature of quantum computing and the reliance on measurement-based testing methods.

Method: The authors conducted an empirical study of recent research on quantum program testing. They analyzed the literature, categorizing measurement-based validation methods into distribution-level and output-value-level groups, and compared these with statevector-based validation methods.

Result: The study found that measurement-based validation is suitable for simple tasks like checking for the presence of certain output values. In contrast, statevector-based validation is more effective for assessing complex program behaviors.

Conclusion: Measurement-based validation has inherent limitations for quantum program testing, especially for complex behaviors, and statevector-based validation offers advantages in these situations.

Abstract: As quantum computing continues to emerge, ensuring the quality of quantum
programs has become increasingly critical. Quantum program testing has emerged
as a prominent research area within the scope of quantum software engineering.
While numerous approaches have been proposed to address quantum program quality
assurance, our analysis reveals that most existing methods rely on
measurement-based validation in practice. However, due to the inherently
probabilistic nature of quantum programs, measurement-based validation methods
face significant limitations.
  To investigate these limitations, we conducted an empirical study of recent
research on quantum program testing, analyzing measurement-based validation
methods in the literature. Our analysis categorizes existing measurement-based
validation methods into two groups: distribution-level validation and
output-value-level validation. We then compare measurement-based validation
with statevector-based validation methods to evaluate their pros and cons. Our
findings demonstrate that measurement-based validation is suitable for
straightforward assessments, such as verifying the existence of specific output
values, while statevector-based validation proves more effective for
complicated tasks such as assessing the program behaviors.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [5] [VerilogMonkey: Exploring Parallel Scaling for Automated Verilog Code Generation with LLMs](https://arxiv.org/abs/2509.16246)
*Juxin Niu,Yuxin Du,Dan Niu,Xi Wang,Zhe Jiang,Nan Guan*

Main category: cs.PL

TL;DR: Sampling many outputs in parallel from LLMs greatly improves automated Verilog code generation, making it faster, cheaper, and more effective than previous approaches, even without extra training techniques.


<details>
  <summary>Details</summary>
Motivation: Automated Verilog generation is an important, under-explored task. Recent success in LLM-based code generation can potentially be leveraged, but the effectiveness of parallel scaling for this specific domain is not well studied.

Method: The authors conduct an empirical study investigating the effect of parallel scaling on automated Verilog code generation using LLMs. They sample many outputs in parallel across multiple benchmarks and LLMs, analyzing the cost (time/money), performance, and the impact of output randomness.

Result: Parallel scaling to hundreds of samples improves Verilog generation performance, being cost-effective and surpassing previous results, even without additional methods such as post-training or agentic enhancements.

Conclusion: Parallel sampling with LLMs is highly effective for automated Verilog generation, resulting in better performance, efficiency, and providing insight into the underlying impact of LLM output randomness.

Abstract: We present VerilogMonkey, an empirical study of parallel scaling for the
under-explored task of automated Verilog generation. Parallel scaling improves
LLM performance by sampling many outputs in parallel. Across multiple
benchmarks and mainstream LLMs, we find that scaling to hundreds of samples is
cost-effective in both time and money and, even without any additional
enhancements such as post-training or agentic methods, surpasses prior results
on LLM-based Verilog generation. We further dissect why parallel scaling
delivers these gains and show how output randomness in LLMs affects its
effectiveness.

</details>
