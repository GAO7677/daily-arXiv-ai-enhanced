{"id": "2506.12084", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.FL", "cs.NE"], "pdf": "https://arxiv.org/pdf/2506.12084", "abs": "https://arxiv.org/abs/2506.12084", "authors": ["Michele Alberti", "Fran√ßois Bobot", "Julien Girard-Satabin", "Alban Grastien", "Aymeric Varasse", "Zakaria Chihani"], "title": "The CAISAR Platform: Extending the Reach of Machine Learning Specification and Verification", "comment": null, "summary": "The formal specification and verification of machine learning programs saw\nremarkable progress in less than a decade, leading to a profusion of tools.\nHowever, diversity may lead to fragmentation, resulting in tools that are\ndifficult to compare, except for very specific benchmarks. Furthermore, this\nprogress is heavily geared towards the specification and verification of a\ncertain class of property, that is, local robustness properties. But while\nprovers are becoming more and more efficient at solving local robustness\nproperties, even slightly more complex properties, involving multiple neural\nnetworks for example, cannot be expressed in the input languages of winners of\nthe International Competition of Verification of Neural Networks VNN-Comp. In\nthis tool paper, we present CAISAR, an open-source platform dedicated to\nmachine learning specification and verification. We present its specification\nlanguage, suitable for modelling complex properties on neural networks, support\nvector machines and boosted trees. We show on concrete use-cases how\nspecifications written in this language are automatically translated to queries\nto state-of-the-art provers, notably by using automated graph editing\ntechniques, making it possible to use their off-the-shelf versions. The\nartifact to reproduce the paper claims is available at the following DOI:\nhttps://doi.org/10.5281/zenodo.15209510"}
{"id": "2506.12111", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.12111", "abs": "https://arxiv.org/abs/2506.12111", "authors": ["Oscar Boullosa Dapena"], "title": "Quantum-Inspired Differentiable Integral Neural Networks (QIDINNs): A Feynman-Based Architecture for Continuous Learning Over Streaming Data", "comment": null, "summary": "Real-time continuous learning over streaming data remains a central challenge\nin deep learning and AI systems. Traditional gradient-based models such as\nbackpropagation through time (BPTT) face computational and stability\nlimitations when dealing with temporally unbounded data. In this paper, we\nintroduce a novel architecture, Quantum-Inspired Differentiable Integral Neural\nNetworks (QIDINNs), which leverages the Feynman technique of differentiation\nunder the integral sign to formulate neural updates as integrals over\nhistorical data. This reformulation allows for smoother, more stable learning\ndynamics that are both physically interpretable and computationally tractable.\nInspired by Feynman's path integral formalism and compatible with quantum\ngradient estimation frameworks, QIDINNs open a path toward hybrid\nclassical-quantum neural computation. We demonstrate our model's effectiveness\non synthetic and real-world streaming tasks, and we propose directions for\nquantum extensions and scalable implementations."}
{"id": "2506.12278", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.12278", "abs": "https://arxiv.org/abs/2506.12278", "authors": ["Zheyuan Yang", "Zexi Kuang", "Xue Xia", "Yilun Zhao"], "title": "Can LLMs Generate High-Quality Test Cases for Algorithm Problems? TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure", "comment": "ACL 2025", "summary": "We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs\nin test-case generation. TestCase-Eval includes 500 algorithm problems and\n100,000 human-crafted solutions from the Codeforces platform. It focuses on two\npivotal tasks: (1) Fault Coverage, which measures how well LLM-generated test\nsets probe diverse input scenarios and cover a wide range of potential failure\nmodes. (2) Fault Exposure, which evaluates whether LLMs can craft a tailored\ntest input that reveals a specific incorrect code implementation. We provide a\ncomprehensive assessment of 19 state-of-the-art open-source and proprietary\nLLMs on TestCase-Eval, offering insights into their strengths and limitations\nin generating effective test cases for algorithm problems."}
{"id": "2506.12320", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12320", "abs": "https://arxiv.org/abs/2506.12320", "authors": ["Weipeng Jiang", "Xiaoyu Zhang", "Xiaofei Xie", "Jiongchi Yu", "Yuhan Zhi", "Shiqing Ma", "Chao Shen"], "title": "The Foundation Cracks: A Comprehensive Study on Bugs and Testing Practices in LLM Libraries", "comment": null, "summary": "Large Language Model (LLM) libraries have emerged as the foundational\ninfrastructure powering today's AI revolution, serving as the backbone for LLM\ndeployment, inference optimization, fine-tuning, and production serving across\ndiverse applications. Despite their critical role in the LLM ecosystem, these\nlibraries face frequent quality issues and bugs that threaten the reliability\nof AI systems built upon them. To address this knowledge gap, we present the\nfirst comprehensive empirical investigation into bug characteristics and\ntesting practices in modern LLM libraries. We examine 313 bug-fixing commits\nextracted across two widely-adopted LLM libraries: HuggingFace Transformers and\nvLLM.Through rigorous manual analysis, we establish comprehensive taxonomies\ncategorizing bug symptoms into 5 types and root causes into 14 distinct\ncategories.Our primary discovery shows that API misuse has emerged as the\npredominant root cause (32.17%-48.19%), representing a notable transition from\nalgorithm-focused defects in conventional deep learning frameworks toward\ninterface-oriented problems. Additionally, we examine 7,748 test functions to\nidentify 7 distinct test oracle categories employed in current testing\napproaches, with predefined expected outputs (such as specific tensors and text\nstrings) being the most common strategy. Our assessment of existing testing\neffectiveness demonstrates that the majority of bugs escape detection due to\ninadequate test cases (41.73%), lack of test drivers (32.37%), and weak test\noracles (25.90%). Drawing from these findings, we offer some recommendations\nfor enhancing LLM library quality assurance."}
{"id": "2506.12202", "categories": ["cs.PL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.12202", "abs": "https://arxiv.org/abs/2506.12202", "authors": ["Stephen Mell", "Botong Zhang", "David Mell", "Shuo Li", "Ramya Ramalingam", "Nathan Yu", "Steve Zdancewic", "Osbert Bastani"], "title": "A Fast, Reliable, and Secure Programming Language for LLM Agents with Code Actions", "comment": null, "summary": "Modern large language models (LLMs) are often deployed as agents, calling\nexternal tools adaptively to solve tasks. Rather than directly calling tools,\nit can be more effective for LLMs to write code to perform the tool calls,\nenabling them to automatically generate complex control flow such as\nconditionals and loops. Such code actions are typically provided as Python\ncode, since LLMs are quite proficient at it; however, Python may not be the\nideal language due to limited built-in support for performance, security, and\nreliability. We propose a novel programming language for code actions, called\nQuasar, which has several benefits: (1) automated parallelization to improve\nperformance, (2) uncertainty quantification to improve reliability and mitigate\nhallucinations, and (3) security features enabling the user to validate\nactions. LLMs can write code in a subset of Python, which is automatically\ntranspiled to Quasar. We evaluate our approach on the ViperGPT visual question\nanswering agent, applied to the GQA dataset, demonstrating that LLMs with\nQuasar actions instead of Python actions retain strong performance, while\nreducing execution time when possible by 42%, improving security by reducing\nuser approval interactions when possible by 52%, and improving reliability by\napplying conformal prediction to achieve a desired target coverage level."}
{"id": "2506.12347", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.12347", "abs": "https://arxiv.org/abs/2506.12347", "authors": ["Aayush Kumar", "Yasharth Bajpai", "Sumit Gulwani", "Gustavo Soares", "Emerson Murphy-Hill"], "title": "How Developers Use AI Agents: When They Work, When They Don't, and Why", "comment": null, "summary": "Software Engineering Agents (SWE agents) can autonomously perform development\ntasks on benchmarks like SWE Bench, but still face challenges when tackling\ncomplex and ambiguous real-world tasks. Consequently, SWE agents are often\ndesigned to allow interactivity with developers, enabling collaborative\nproblem-solving. To understand how developers collaborate with SWE agents and\nthe communication challenges that arise in such interactions, we observed 19\ndevelopers using an in-IDE agent to resolve 33 open issues in repositories to\nwhich they had previously contributed. Participants successfully resolved about\nhalf of these issues, with participants solving issues incrementally having\ngreater success than those using a one-shot approach. Participants who actively\ncollaborated with the agent and iterated on its outputs were also more\nsuccessful, though they faced challenges in trusting the agent's responses and\ncollaborating on debugging and testing. These results have implications for\nsuccessful developer-agent collaborations, and for the design of more effective\nSWE agents."}
{"id": "2506.12212", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2506.12212", "abs": "https://arxiv.org/abs/2506.12212", "authors": ["Grant VanDomelen", "Gan Shen", "Lindsey Kuper", "Yao Li"], "title": "Freer Arrows and Why You Need Them in Haskell", "comment": "In submission to the Haskell Symposium 2025", "summary": "Freer monads are a useful structure commonly used in various domains due to\ntheir expressiveness. However, a known issue with freer monads is that they are\nnot amenable to static analysis. This paper explores freer arrows, a relatively\nexpressive structure that is amenable to static analysis. We propose several\nvariants of freer arrows. We conduct a case study on choreographic programming\nto demonstrate the usefulness of freer arrows in Haskell."}
{"id": "2506.12590", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.12590", "abs": "https://arxiv.org/abs/2506.12590", "authors": ["Breno Alves de Andrade", "Rodrigo Siqueira", "Lidiane Gomes", "Antonio Oliveira", "Danilo Monteiro Ribeiro"], "title": "A Mapping Study About Training in Industry Context in Software Engineering", "comment": null, "summary": "Context: Corporate training plays a strategic role in the continuous\ndevelopment of professionals in the software engineering industry. However,\nthere is a lack of systematized understanding of how training initiatives are\ndesigned, implemented, and evaluated within this domain.\n  Objective: This study aims to map the current state of research on corporate\ntraining in software engineering in industry settings, using Eduardo Salas'\ntraining framework as an analytical lens.\n  Method: A systematic mapping study was conducted involving the selection and\nanalysis of 26 primary studies published in the field. Each study was\ncategorized according to Salas' four key areas: Training Needs Analysis,\nAntecedent Training Conditions, Training Methods and Instructional Strategies,\nand Post-Training Conditions.\n  Results: The findings show a predominance of studies focusing on Training\nMethods and Instructional Strategies. Significant gaps were identified in other\nareas, particularly regarding Job/Task Analysis and Simulation-based Training\nand Games. Most studies were experience reports, lacking methodological rigor\nand longitudinal assessment.\n  Conclusions: The study offers a structured overview of how corporate training\nis approached in software engineering, revealing underexplored areas and\nproposing directions for future research. It contributes to both academic and\npractical communities by highlighting challenges, methodological trends, and\nopportunities for designing more effective training programs in industry."}
{"id": "2506.13383", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2506.13383", "abs": "https://arxiv.org/abs/2506.13383", "authors": ["Jules Jacobs", "Nate Foster", "Tobias Kapp√©", "Dexter Kozen", "Lily Saada", "Alexandra Silva", "Jana Wagemaker"], "title": "StacKAT: Infinite State Network Verification", "comment": null, "summary": "We develop StacKAT, a network verification language featuring loops, finite\nstate variables, nondeterminism, and - most importantly - access to a stack\nwith accompanying push and pop operations. By viewing the variables and stack\nas the (parsed) headers and (to-be-parsed) contents of a network packet,\nStacKAT can express a wide range of network behaviors including parsing, source\nrouting, and telemetry. These behaviors are difficult or impossible to model\nusing existing languages like NetKAT. We develop a decision procedure for\nStacKAT program equivalence, based on finite automata. This decision procedure\nprovides the theoretical basis for verifying network-wide properties and is\nable to provide counterexamples for inequivalent programs. Finally, we provide\nan axiomatization of StacKAT equivalence and establish its completeness."}
{"id": "2506.12616", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.12616", "abs": "https://arxiv.org/abs/2506.12616", "authors": ["Debasish Jana", "Pinakpani Pal", "Pawan Kumar"], "title": "Real-Time Agile Software Management for Edge and Fog Computing Based Smart City Infrastructure", "comment": "The paper has been published at the Fifth International Conference on\n  Computing and Communication Networks (ICCCN 2025), Volume 1", "summary": "The evolution of smart cities demands scalable, secure, and energy-efficient\narchitectures for real-time data processing. With the number of IoT devices\nexpected to exceed 40 billion by 2030, traditional cloud-based systems are\nincreasingly constrained by bandwidth, latency, and energy limitations. This\npaper leverages the ROOF (Real-time Onsite Operations Facilitation) framework\nwith decentralized computing at intermediary fog and peripheral edge network\nlayers to reduce latency by processing data near its point of origin. ROOF\nfeatures fog caching to avoid redundancy, ultra-low-power wireless transmission\nfor energy savings, and AI-driven resource allocation for efficiency. Security\nis enhanced through TLS encryption, blockchain-based authentication, and\nedge-level access control. Case studies from Bhubaneswar, Barcelona and\nCopenhagen validate the use of ROOF in traffic systems and environmental\nmonitoring. The paper concludes by outlining key challenges and prospects of\nAI-driven analytics in smart urban infrastructure."}
{"id": "2506.12643", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.12643", "abs": "https://arxiv.org/abs/2506.12643", "authors": ["Prachnachai Meakpaiboonwattana", "Warittha Tarntong", "Thai Mekratanavorakul", "Chaiyong Ragkhitwetsagul", "Pattaraporn Sangaroonsilp", "Raula Kula", "Morakot Choetkiertikul", "Kenichi Matsumoto", "Thanwadee Sunetnanta"], "title": "Social Media Reactions to Open Source Promotions: AI-Powered GitHub Projects on Hacker News", "comment": null, "summary": "Social media platforms have become more influential than traditional news\nsources, shaping public discourse and accelerating the spread of information.\nWith the rapid advancement of artificial intelligence (AI), open-source\nsoftware (OSS) projects can leverage these platforms to gain visibility and\nattract contributors. In this study, we investigate the relationship between\nHacker News, a social news site focused on computer science and\nentrepreneurship, and the extent to which it influences developer activity on\nthe promoted GitHub AI projects.\n  We analyzed 2,195 Hacker News (HN) stories and their corresponding comments\nover a two-year period. Our findings reveal that at least 19\\% of AI developers\npromoted their GitHub projects on Hacker News, often receiving positive\nengagement from the community. By tracking activity on the associated 1,814\nGitHub repositories after they were shared on Hacker News, we observed a\nsignificant increase in forks, stars, and contributors. These results suggest\nthat Hacker News serves as a viable platform for AI-powered OSS projects, with\nthe potential to gain attention, foster community engagement, and accelerate\nsoftware development."}
{"id": "2506.12669", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.12669", "abs": "https://arxiv.org/abs/2506.12669", "authors": ["Anrafel Fernandes Pereira", "Marcos Kalinowski", "Maria Teresa Baldassarre", "J√ºrgen B√∂rstler", "Nauman bin Ali", "Daniel Mendez"], "title": "Towards Lean Research Inception: Assessing Practical Relevance of Formulated Research Problems", "comment": "Accepted for publication at EASE 2025", "summary": "[Context] The lack of practical relevance in many Software Engineering (SE)\nresearch contributions is often rooted in oversimplified views of industrial\npractice, weak industry connections, and poorly defined research problems.\nClear criteria for evaluating SE research problems can help align their value,\nfeasibility, and applicability with industrial needs. [Goal] In this paper, we\nintroduce the Lean Research Inception (LRI) framework, designed to support the\nformulation and assessment of practically relevant research problems in SE. We\ndescribe its initial evaluation strategy conducted in a workshop with a network\nof SE researchers experienced in industry-academia collaboration and report the\nevaluation of its three assessment criteria (valuable, feasible, and\napplicable) regarding their importance in assessing practical relevance.\n[Method] We applied LRI retroactively to a published research paper, engaging\nworkshop participants in discussing and assessing the research problem by\napplying the proposed criteria using a semantic differential scale.\nParticipants provided feedback on the criteria's importance and completeness,\ndrawn from their own experiences in industry-academia collaboration. [Results]\nThe findings reveal an overall agreement on the importance of the three\ncriteria - valuable (83.3%), feasible (76.2%), and applicable (73.8%) - for\naligning research problems with industrial needs. Qualitative feedback\nsuggested adjustments in terminology with a clearer distinction between\nfeasible and applicable, and refinements for valuable by more clearly\nconsidering business value, ROI, and originality. [Conclusion] While LRI\nconstitutes ongoing research and requires further evaluation, our results\nstrengthen our confidence that the three criteria applied using the semantic\ndifferential scale can already help the community assess the practical\nrelevance of SE research problems."}
{"id": "2506.12691", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.12691", "abs": "https://arxiv.org/abs/2506.12691", "authors": ["Bianca Trinkenreich", "Fabio Calefato", "Geir Hanssen", "Kelly Blincoe", "Marcos Kalinowski", "Mauro Pezz√®", "Paolo Tell", "Margaret-Anne Storey"], "title": "Get on the Train or be Left on the Station: Using LLMs for Software Engineering Research", "comment": "Accepted for publication at the 1st Workshop on Human-Centered AI for\n  SE (Human AISE) held at the 33rd ACM International Conference on the\n  Foundations of Software Engineering (FSE Companion '25), June 23-28, 2025,\n  Trondheim, Norway", "summary": "The adoption of Large Language Models (LLMs) is not only transforming\nsoftware engineering (SE) practice but is also poised to fundamentally disrupt\nhow research is conducted in the field. While perspectives on this\ntransformation range from viewing LLMs as mere productivity tools to\nconsidering them revolutionary forces, we argue that the SE research community\nmust proactively engage with and shape the integration of LLMs into research\npractices, emphasizing human agency in this transformation. As LLMs rapidly\nbecome integral to SE research - both as tools that support investigations and\nas subjects of study - a human-centric perspective is essential. Ensuring human\noversight and interpretability is necessary for upholding scientific rigor,\nfostering ethical responsibility, and driving advancements in the field.\nDrawing from discussions at the 2nd Copenhagen Symposium on Human-Centered AI\nin SE, this position paper employs McLuhan's Tetrad of Media Laws to analyze\nthe impact of LLMs on SE research. Through this theoretical lens, we examine\nhow LLMs enhance research capabilities through accelerated ideation and\nautomated processes, make some traditional research practices obsolete,\nretrieve valuable aspects of historical research approaches, and risk reversal\neffects when taken to extremes. Our analysis reveals opportunities for\ninnovation and potential pitfalls that require careful consideration. We\nconclude with a call to action for the SE research community to proactively\nharness the benefits of LLMs while developing frameworks and guidelines to\nmitigate their risks, to ensure continued rigor and impact of research in an\nAI-augmented future."}
{"id": "2506.12713", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.12713", "abs": "https://arxiv.org/abs/2506.12713", "authors": ["Xiangyang Li", "Xiaopeng Li", "Kuicai Dong", "Quanhu Zhang", "Rongju Ruan", "Xinyi Dai", "Xiaoshuang Liu", "Shengchun Xu", "Yasheng Wang", "Ruiming Tang"], "title": "Humanity's Last Code Exam: Can Advanced LLMs Conquer Human's Hardest Code Competition?", "comment": null, "summary": "Code generation is a core capability of large language models (LLMs), yet\nmainstream benchmarks (e.g., APPs and LiveCodeBench) contain questions with\nmedium-level difficulty and pose no challenge to advanced LLMs. To better\nreflected the advanced reasoning and code generation ability, We introduce\nHumanity's Last Code Exam (HLCE), comprising 235 most challenging problems from\nthe International Collegiate Programming Contest (ICPC World Finals) and the\nInternational Olympiad in Informatics (IOI) spanning 2010 - 2024. As part of\nHLCE, we design a harmonized online-offline sandbox that guarantees fully\nreproducible evaluation. Through our comprehensive evaluation, we observe that\neven the strongest reasoning LLMs: o4-mini(high) and Gemini-2.5 Pro, achieve\npass@1 rates of only 15.9% and 11.4%, respectively. Meanwhile, we propose a\nnovel \"self-recognition\" task to measure LLMs' awareness of their own\ncapabilities. Results indicate that LLMs' self-recognition abilities are not\nproportionally correlated with their code generation performance. Finally, our\nempirical validation of test-time scaling laws reveals that current advanced\nLLMs have substantial room for improvement on complex programming tasks. We\nexpect HLCE to become a milestone challenge for code generation and to catalyze\nadvances in high-performance reasoning and human-AI collaborative programming.\nOur code and dataset are also public\navailable(https://github.com/Humanity-s-Last-Code-Exam/HLCE)."}
{"id": "2506.12728", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.12728", "abs": "https://arxiv.org/abs/2506.12728", "authors": ["Yibo Wang", "Zhihao Peng", "Ying Wang", "Zhao Wei", "Hai Yu", "Zhiliang Zhu"], "title": "MCTS-Refined CoT: High-Quality Fine-Tuning Data for LLM-Based Repository Issue Resolution", "comment": null, "summary": "LLMs demonstrate strong performance in auto-mated software engineering,\nparticularly for code generation and issue resolution. While proprietary models\nlike GPT-4o achieve high benchmarks scores on SWE-bench, their API dependence,\ncost, and privacy concerns limit adoption. Open-source alternatives offer\ntransparency but underperform in complex tasks, especially sub-100B parameter\nmodels. Although quality Chain-of-Thought (CoT) data can enhance reasoning,\ncurrent methods face two critical flaws: (1) weak rejection sampling reduces\ndata quality, and (2) inadequate step validation causes error accumulation.\nThese limitations lead to flawed reasoning chains that impair LLMs'ability to\nlearn reliable issue resolution. The paper proposes MCTS-REFINE, an enhanced\nMonte Carlo Tree Search (MCTS)-based algorithm that dynamically validates and\noptimizes intermediate reasoning steps through a rigorous rejection sampling\nstrategy, generating high-quality CoT data to improve LLM performance in issue\nresolution tasks. Key innovations include: (1) augmenting MCTS with a\nreflection mechanism that corrects errors via rejection sampling and\nrefinement, (2) decomposing issue resolution into three subtasks-File\nLocalization, Fault Localization, and Patch Generation-each with clear\nground-truth criteria, and (3) enforcing a strict sampling protocol where\nintermediate outputs must exactly match verified developer patches, ensuring\ncorrectness across reasoning paths. Experiments on SWE-bench Lite and SWE-bench\nVerified demonstrate that LLMs fine-tuned with our CoT dataset achieve\nsubstantial improvements over baselines.Notably, Qwen2.5-72B- Instruct achieves\n28.3%(Lite) and 35.0%(Verified) resolution rates, surpassing SOTA baseline\nSWE-Fixer-Qwen-72B with the same parameter scale, which only reached\n24.7%(Lite) and 32.8%(Verified)."}
{"id": "2506.12760", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.12760", "abs": "https://arxiv.org/abs/2506.12760", "authors": ["Lantian Li", "Yejian Liang", "Zhongxing Yu"], "title": "IDOL: Improved Different Optimization Levels Testing for Solidity Compilers", "comment": "Accepted by QRS 2025 (Fast Abstracts track)", "summary": "As blockchain technology continues to evolve and mature, smart contracts have\nbecome a key driving force behind the digitization and automation of\ntransactions. Smart contracts greatly simplify and refine the traditional\nbusiness transaction processes, and thus have had a profound impact on various\nindustries such as finance and supply chain management. However, because smart\ncontracts cannot be modified once deployed, any vulnerabilities or design flaws\nwithin the contract cannot be easily fixed, potentially leading to significant\nfinancial losses or even legal issues. The compiler, as a critical component in\nthe development process, directly affects the quality and security of smart\ncontracts. This paper innovatively proposes a method, known as the Improved\nDifferent Optimization Levels (IDOL), for testing the Solidity compiler. The\nkey idea behind IDOL is to perform reverse optimization transformations (i.e.,\nchange optimized form into unoptimized form) to generate semantically\nequivalent variants of the smart contracts under test, aiming to maximize the\nopportunities to trigger the optimization logic of compilers. We conducted a\npreliminary evaluation of IDOL and three confirmed compiler optimization bugs\nhave been uncovered at the time of writing."}
{"id": "2506.12858", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.12858", "abs": "https://arxiv.org/abs/2506.12858", "authors": ["Nick Battle", "Peter Gorm Larsen"], "title": "Towards Operation Proof Obligation Generation for VDM", "comment": "Presented at the 23rd Overture workshop, June 2025\n  (arXiv:cs/2506.08680)", "summary": "All formalisms have the ability to ensure that their models are internally\nconsistent. Potential inconsistencies are generally highlighted by assertions\ncalled proof obligations, and the generation of these obligations is an\nimportant role of the tools that support the method. This capability has been\navailable for VDM tools for many years. However, support for obligation\ngeneration for explicit operation bodies has always been limited. This work\ndescribes the current state of work to address this, showing the capabilities\nso far and highlighting the work remaining."}
{"id": "2506.13114", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.13114", "abs": "https://arxiv.org/abs/2506.13114", "authors": ["Yanzhou Mu", "Rong Wang", "Juan Zhai", "Chunrong Fang", "Xiang Chen", "Jiacong Wu", "An Guo", "Jiawei Shen", "Bingzhuo Li", "Zhenyu Chen"], "title": "Designing Deep Learning Frameworks for LLMs:Challenges, Expectations, and Opportunities", "comment": "12 pages, 2 figures", "summary": "Large language models (LLMs) drive significant advancements in real industry\napplications. LLMs rely on DL frameworks for efficient model construction,\ndistributed execution, and optimized deployment. Their large parameter scale\nand long execution cycles place extreme demands on DL frameworks in terms of\nscalability, stability, and efficiency. Therefore, poor usability, limited\nfunctionality, and subtle bugs in DL frameworks may hinder development\nefficiency and cause severe failures or resource waste. However, a fundamental\nquestion remains underinvestigated, i.e., What challenges do DL frameworks face\nin supporting LLMs? To seek an answer, we investigate these challenges through\na large-scale analysis of issue reports from three major DL frameworks\n(MindSpore, PyTorch, TensorFlow) and eight associated LLM toolkits (e.g.,\nMegatron). We construct a taxonomy of LLM-centric bugs, requirements, and user\nquestions and enrich it through interviews with 11 LLM users and eight DL\nframework developers, uncovering key technical challenges and misalignments\nbetween user needs and developer priorities. Our contributions are threefold:\n(1) we develop a comprehensive taxonomy comprising four question themes (nine\nsub-themes), four requirement themes (15 sub-themes), and ten bug themes (45\nsub-themes); (2) we assess the perceived importance and priority of these\nchallenges based on practitioner insights; and (3) we identify five key\nfindings across the LLM development and propose five actionable recommendations\nto improve the reliability, usability, and testability of DL frameworks. Our\nresults highlight critical limitations in current DL frameworks and offer\nconcrete guidance for advancing their support for the next generation of LLM\nconstruction and applications."}
{"id": "2506.13171", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13171", "abs": "https://arxiv.org/abs/2506.13171", "authors": ["Lukasz Mazur", "Nenad Petrovic", "James Pontes Miranda", "Ansgar Radermacher", "Robert Rasche", "Alois Knoll"], "title": "Querying Large Automotive Software Models: Agentic vs. Direct LLM Approaches", "comment": null, "summary": "Large language models (LLMs) offer new opportunities for interacting with\ncomplex software artifacts, such as software models, through natural language.\nThey present especially promising benefits for large software models that are\ndifficult to grasp in their entirety, making traditional interaction and\nanalysis approaches challenging. This paper investigates two approaches for\nleveraging LLMs to answer questions over software models: direct prompting,\nwhere the whole software model is provided in the context, and an agentic\napproach combining LLM-based agents with general-purpose file access tools. We\nevaluate these approaches using an Ecore metamodel designed for timing analysis\nand software optimization in automotive and embedded domains. Our findings show\nthat while the agentic approach achieves accuracy comparable to direct\nprompting, it is significantly more efficient in terms of token usage. This\nefficiency makes the agentic approach particularly suitable for the automotive\nindustry, where the large size of software models makes direct prompting\ninfeasible, establishing LLM agents as not just a practical alternative but the\nonly viable solution. Notably, the evaluation was conducted using small LLMs,\nwhich are more feasible to be executed locally - an essential advantage for\nmeeting strict requirements around privacy, intellectual property protection,\nand regulatory compliance. Future work will investigate software models in\ndiverse formats, explore more complex agent architectures, and extend agentic\nworkflows to support not only querying but also modification of software\nmodels."}
{"id": "2506.13182", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13182", "abs": "https://arxiv.org/abs/2506.13182", "authors": ["Anh Ho", "Thanh Le-Cong", "Bach Le", "Christine Rizkallah"], "title": "From Empirical Evaluation to Context-Aware Enhancement: Repairing Regression Errors with LLMs", "comment": null, "summary": "[...] Since then, various APR approaches, especially those leveraging the\npower of large language models (LLMs), have been rapidly developed to fix\ngeneral software bugs. Unfortunately, the effectiveness of these advanced\ntechniques in the context of regression bugs remains largely unexplored. This\ngap motivates the need for an empirical study evaluating the effectiveness of\nmodern APR techniques in fixing real-world regression bugs.\n  In this work, we conduct an empirical study of APR techniques on Java\nregression bugs. To facilitate our study, we introduce RegMiner4APR, a\nhigh-quality benchmark of Java regression bugs integrated into a framework\ndesigned to facilitate APR research. The current benchmark includes 99\nregression bugs collected from 32 widely used real-world Java GitHub\nrepositories. We begin by conducting an in-depth analysis of the benchmark,\ndemonstrating its diversity and quality. Building on this foundation, we\nempirically evaluate the capabilities of APR to regression bugs by assessing\nboth traditional APR tools and advanced LLM-based APR approaches. Our\nexperimental results show that classical APR tools fail to repair any bugs,\nwhile LLM-based APR approaches exhibit promising potential. Motivated by these\nresults, we investigate impact of incorporating bug-inducing change information\ninto LLM-based APR approaches for fixing regression bugs. Our results highlight\nthat this context-aware enhancement significantly improves the performance of\nLLM-based APR, yielding 1.8x more successful repairs compared to using\nLLM-based APR without such context."}
{"id": "2506.13186", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.13186", "abs": "https://arxiv.org/abs/2506.13186", "authors": ["Jiajun Sun", "Fengjie Li", "Xinzhu Qi", "Hongyu Zhang", "Jiajun Jiang"], "title": "Empirical Evaluation of Large Language Models in Automated Program Repair", "comment": null, "summary": "The increasing prevalence of software bugs has made automated program repair\n(APR) a key research focus. Large language models (LLMs) offer new\nopportunities for APR, but existing studies mostly rely on smaller,\nearlier-generation models and Java benchmarks. The repair capabilities of\nmodern, large-scale LLMs across diverse languages and scenarios remain\nunderexplored. To address this, we conduct a comprehensive empirical study of\nfour open-source LLMs, CodeLlama, LLaMA, StarCoder, and DeepSeek-Coder,\nspanning 7B to 33B parameters, diverse architectures, and purposes. We evaluate\nthem across two bug scenarios (enterprise-grades and algorithmic), three\nlanguages (Java, C/C++, Python), and four prompting strategies, analyzing over\n600K generated patches on six benchmarks. Key findings include: (1) model\nspecialization (e.g., CodeLlama) can outperform larger general-purpose models\n(e.g., LLaMA); (2) repair performance does not scale linearly with model size;\n(3) correct patches often appear early in generation; and (4) prompts\nsignificantly affect results. These insights offer practical guidance for\ndesigning effective and efficient LLM-based APR systems."}
{"id": "2506.13273", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.13273", "abs": "https://arxiv.org/abs/2506.13273", "authors": ["Charaka Geethal Kapugama"], "title": "Isolating Noisy Labelled Test Cases in Human-in-the-Loop Oracle Learning", "comment": "2025 International Research Conference on Smart Computing and Systems\n  Engineering (SCSE)", "summary": "Incorrectly labelled test cases can adversely affect the training process of\nhuman-in-the-loop oracle learning tech-niques. This paper introduces ISONOISE,\na technique designed to identify such mislabelled test cases introduced during\nhuman-in-the-loop oracle learning. This technique can be applied to programs\ntaking numeric inputs. Given a compromised automatic test oracle and its\ntraining test suite, ISONOISE first isolates thetest cases suspected of being\nmislabelled. This task is performed based on the level of disagreement of a\ntest case with respect to the others. An intermediate automatic test oracle is\ntrained based on the slightly disagreeing test cases. Based on the predictions\nof this intermediate oracle, the test cases suspected of being mislabelled are\nsystematically presented for relabelling. When mislabelled test cases are\nfound, the intermediate test oracle is updated. This process repeats until no\nmislabelled test case is found in relabelling. ISONOISE was evaluated within\nthe human-in-the-loop oracle learning method used in LEARN2FIX. Experimental\nresults demonstrate that ISONOISE can identify mislabelled test cases\nintroduced by the human in LEARN2FIX with over 67% accuracy, while requiring\nonly a small number of relabelling queries. These findings highlight the\npotential of ISONOISE to enhance the reliability of human-in-the-loop oracle\nlearning."}
{"id": "2506.13303", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.13303", "abs": "https://arxiv.org/abs/2506.13303", "authors": ["Julian Frattini", "Anja Frattini"], "title": "Adopting Use Case Descriptions for Requirements Specification: an Industrial Case Study", "comment": null, "summary": "Context: Use case (UC) descriptions are a prominent format for specifying\nfunctional requirements. Existing literature abounds with recommendations on\nhow to write high-quality UC descriptions but lacks insights into (1) their\nreal-world adoption, (2) whether these recommendations correspond to actual\nquality, and (3) which factors influence the quality of UCs. Objectives: We aim\nto contribute empirical evidence about the adoption of UC descriptions in a\nlarge, globally distributed case company. Methods: We surveyed 1188 business\nrequirements of a case company that were elicited from 2020-01-01 until\n2024-12-31 and contained 1192 UCs in various forms. Among these, we manually\nevaluated the 273 template-style UC descriptions against established quality\nguidelines. We generated descriptive statistics of the format's adoption over\nthe surveyed time frame. Furthermore, we used inferential statistics to\ndetermine (a) how properties of the requirements engineering process affected\nthe UC quality and (b) how UC quality affects subsequent software development\nactivities. Results and Conclusions: Our descriptive results show how the\nadoption of UC descriptions in practice deviates from textbook recommendations.\nHowever, our inferential results suggest that only a few phenomena like\nsolution-orientation show an actual impact in practice. These results can steer\nUC quality research into a more relevant direction."}
{"id": "2506.13538", "categories": ["cs.SE", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.13538", "abs": "https://arxiv.org/abs/2506.13538", "authors": ["Mohammed Mehedi Hasan", "Hao Li", "Emad Fallahzadeh", "Bram Adams", "Ahmed E. Hassan"], "title": "Model Context Protocol (MCP) at First Glance: Studying the Security and Maintainability of MCP Servers", "comment": null, "summary": "Although Foundation Models (FMs), such as GPT-4, are increasingly used in\ndomains like finance and software engineering, reliance on textual interfaces\nlimits these models' real-world interaction. To address this, FM providers\nintroduced tool calling-triggering a proliferation of frameworks with distinct\ntool interfaces. In late 2024, Anthropic introduced the Model Context Protocol\n(MCP) to standardize this tool ecosystem, which has become the de facto\nstandard with over eight million weekly SDK downloads. Despite its adoption,\nMCP's AI-driven, non-deterministic control flow introduces new risks to\nsustainability, security, and maintainability, warranting closer examination.\n  Towards this end, we present the first large-scale empirical study of MCP.\nUsing state-of-the-art health metrics and a hybrid analysis pipeline, combining\na general-purpose static analysis tool with an MCP-specific scanner, we\nevaluate 1,899 open-source MCP servers to assess their health, security, and\nmaintainability. Despite MCP servers demonstrating strong health metrics, we\nidentify eight distinct vulnerabilities-only three overlapping with traditional\nsoftware vulnerabilities. Additionally, 7.2% of servers contain general\nvulnerabilities and 5.5% exhibit MCP-specific tool poisoning. Regarding\nmaintainability, while 66% exhibit code smells, 14.4% contain ten bug patterns\noverlapping prior research. These findings highlight the need for MCP-specific\nvulnerability detection techniques while reaffirming the value of traditional\nanalysis and refactoring practices."}
{"id": "2506.13663", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.13663", "abs": "https://arxiv.org/abs/2506.13663", "authors": ["Yunnong Chen", "Shixian Ding", "YingYing Zhang", "Wenkai Chen", "Jinzhou Du", "Lingyun Sun", "Liuqing Chen"], "title": "DesignCoder: Hierarchy-Aware and Self-Correcting UI Code Generation with Large Language Models", "comment": "11 pages,6 figures", "summary": "Multimodal large language models (MLLMs) have streamlined front-end interface\ndevelopment by automating code generation. However, these models also introduce\nchallenges in ensuring code quality. Existing approaches struggle to maintain\nboth visual consistency and functional completeness in the generated\ncomponents. Moreover, they lack mechanisms to assess the fidelity and\ncorrectness of the rendered pages. To address these issues, we propose\nDesignCoder, a novel hierarchical-aware and self-correcting automated code\ngeneration framework. Specifically, we introduce UI Grouping Chains, which\nenhance MLLMs' capability to understand and predict complex nested UI\nhierarchies. Subsequently, DesignCoder employs a hierarchical\ndivide-and-conquer approach to generate front-end code. Finally, we incorporate\na self-correction mechanism to improve the model's ability to identify and\nrectify errors in the generated code. Extensive evaluations on a dataset of UI\nmockups collected from both open-source communities and industry projects\ndemonstrate that DesignCoder outperforms state-of-the-art baselines in React\nNative, a widely adopted UI framework. Our method achieves a 37.63%, 9.52%,\n12.82% performance increase in visual similarity metrics (MSE, CLIP, SSIM) and\nsignificantly improves code structure similarity in terms of TreeBLEU,\nContainer Match, and Tree Edit Distance by 30.19%, 29.31%, 24.67%. Furthermore,\nwe conducted a user study with professional developers to assess the quality\nand practicality of the generated code. Results indicate that DesignCoder\naligns with industry best practices, demonstrating high usability, readability,\nand maintainability. Our approach provides an efficient and practical solution\nfor agile front-end development, enabling development teams to focus more on\ncore functionality and product innovation."}
