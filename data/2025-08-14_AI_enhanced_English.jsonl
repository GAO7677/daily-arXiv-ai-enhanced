{"id": "2508.09856", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.09856", "abs": "https://arxiv.org/abs/2508.09856", "authors": ["Mathieu Boespflug", "Arnaud Spiwack"], "title": "Invertible Syntax without the Tuples (Functional Pearl)", "comment": null, "summary": "In the seminal paper Functional unparsing, Olivier Danvy used continuation\npassing to reanalyse printf-like format strings as combinators. In the\nintervening decades, the conversation shifted towards a concurrent line of work\n-- applicative, monadic or arrow-based combinator libraries -- in an effort to\nfind combinators for invertible syntax descriptions that simultaneously\ndetermine a parser as well as a printer, and with more expressive power, able\nto handle inductive structures such as lists and trees. Along the way,\ncontinuation passing got lost. This paper argues that Danvy's insight remains\nas relevant to the general setting as it was to the restricted setting of his\noriginal paper. Like him, we present three solutions that exploit\ncontinuation-passing style as an alternative to both dependent types and\nmonoidal aggregation via nested pairs, in our case to parse and print\nstructured data with increasing expressive power.", "AI": {"tldr": "This paper revisits the idea of using continuation-passing style (CPS) for format-string combinators, arguing that CPS techniques are still highly relevant and effective for invertible parsing and printing of structured data, including complex inductive types. The authors present three CPS-based methods that increase expressive power and offer alternatives to newer but sometimes less elegant techniques.", "motivation": "The motivation of this paper is to revisit and generalize Olivier Danvy's original continuation-passing approach for handling printf-like format strings, aiming to recover lost techniques amidst the shift towards more modern combinator libraries for invertible syntax descriptions. The authors argue that the continuation-passing style (CPS) remains relevant and powerful for parsing and printing structured data, including more complex inductive structures, which newer methods may not handle as elegantly.", "method": "The paper presents three alternative solutions that leverage the continuation-passing style (CPS) for parsing and printing structured data. These approaches are positioned as alternatives to both dependent type techniques and conventional monoidal aggregation through nested pairs. Each solution demonstrates increasing expressive power, addressing more complex data such as lists and trees.", "result": "The results show that applying CPS to syntax descriptions not only parallels Danvy\u2019s original findings but extends their applicability to more general and expressive settings. The paper successfully demonstrates how CPS-based combinators can be used for both parsing and printing, achieving invertibility and expressiveness for structured data.", "conclusion": "The conclusion is that the continuation-passing style approach remains valuable and relevant for invertible syntax descriptions, offering expressive solutions for both parsing and printing structured data, and should not be overlooked in favor of purely applicative, monadic, or arrow-based combinator libraries."}}
{"id": "2508.09332", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09332", "abs": "https://arxiv.org/abs/2508.09332", "authors": ["Anshul Khairnar", "Aarya Rajoju", "Edward F. Gehringer"], "title": "Teaching Code Refactoring Using LLMs", "comment": "Accepted for presentation at the Frontiers in Education Conference,\n  Nashville, Tennessee, USA, 2-5 November 2025", "summary": "This Innovative Practice full paper explores how Large Language Models (LLMs)\ncan enhance the teaching of code refactoring in software engineering courses\nthrough real-time, context-aware feedback. Refactoring improves code quality\nbut is difficult to teach, especially with complex, real-world codebases.\nTraditional methods like code reviews and static analysis tools offer limited,\ninconsistent feedback. Our approach integrates LLM-assisted refactoring into a\ncourse project using structured prompts to help students identify and address\ncode smells such as long methods and low cohesion. Implemented in Spring 2025\nin a long-lived OSS project, the intervention is evaluated through student\nfeedback and planned analysis of code quality improvements. Findings suggest\nthat LLMs can bridge theoretical and practical learning, supporting a deeper\nunderstanding of maintainability and refactoring principles.", "AI": {"tldr": "The paper shows that using large language models to guide students in code refactoring activities improves their understanding and skills by providing real-time, tailored feedback, outperforming traditional teaching methods.", "motivation": "Refactoring is vital for code quality but challenging to teach, especially in complex, real-world codebases. Existing tools and methods provide limited and inconsistent feedback, creating a need for more effective educational interventions.", "method": "The study integrates LLM-assisted refactoring into a software engineering course using structured prompts, enabling students to identify and address code smells. The intervention was implemented in a real-world OSS project, and its effectiveness was evaluated through student feedback and plans for code quality analysis.", "result": "The findings indicate that LLMs can connect theoretical concepts with practical application, leading to a richer understanding of code maintainability and refactoring.", "conclusion": "LLMs offer valuable, context-aware support for teaching code refactoring, improving students' comprehension and ability to address code quality issues."}}
{"id": "2508.09366", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2508.09366", "abs": "https://arxiv.org/abs/2508.09366", "authors": ["Qiaolin Qin", "Xingfang Wu", "Heng Li", "Ettore Merlo"], "title": "Plug it and Play on Logs: A Configuration-Free Statistic-Based Log Parser", "comment": null, "summary": "Log parsing is an essential task in log analysis, and many tools have been\ndesigned to accomplish it. Existing log parsers can be categorized into\nstatistic-based and semantic-based approaches. In comparison to semantic-based\nparsers, existing statistic-based parsers tend to be more efficient, require\nlower computational costs, and be more privacy-preserving thanks to on-premise\ndeployment, but often fall short in their accuracy (e.g., grouping or parsing\naccuracy) and generalizability. Therefore, it became a common belief that\nstatistic-based parsers cannot be as effective as semantic-based parsers since\nthe latter could take advantage of external knowledge supported by pretrained\nlanguage models. Our work, however, challenges this belief with a novel\nstatistic-based parser, PIPLUP. PIPLUP eliminates the pre-assumption of the\nposition of constant tokens for log grouping and relies on data-insensitive\nparameters to overcome the generalizability challenge, allowing \"plug and play\"\non given log files. According to our experiments on an open-sourced large log\ndataset, PIPLUP shows promising accuracy and generalizability with the\ndata-insensitive default parameter set. PIPLUP not only outperforms the\nstate-of-the-art statistic-based log parsers, Drain and its variants, but also\nobtains a competitive performance compared to the best unsupervised\nsemantic-based log parser (i.e., LUNAR). Further, PIPLUP exhibits low time\nconsumption without GPU acceleration and external API usage; our simple,\nefficient, and effective approach makes it more practical in real-world\nadoptions, especially when costs and privacy are of major concerns.", "AI": {"tldr": "PIPLUP is a new statistical log parser that challenges prior assumptions by achieving near-semantic accuracy and generalizability, outperforming existing statistical tools and rivaling top semantic approaches, with greater efficiency and privacy.", "motivation": "Statistical log parsers are widely used due to their efficiency and privacy, but conventional wisdom suggests they cannot match the accuracy and generalizability of semantic-based parsers relying on pretrained language models.", "method": "This work introduces PIPLUP, a novel statistical log parser that removes assumptions about token positions and uses data-insensitive parameters. This design allows PIPLUP to be easily applied to various log files without tuning, aiming for better generalizability and accuracy.", "result": "Experiments on large, open-source log datasets show that PIPLUP achieves high accuracy and generalizability using default parameters. It outperforms state-of-the-art statistical parsers (Drain and variants) and is competitive with the top unsupervised semantic parser (LUNAR), all while requiring less computational resources and preserving privacy.", "conclusion": "PIPLUP casts doubt on previous beliefs, proving that statistical log parsers can achieve both accuracy and generalizability comparable to semantic-based methods, while remaining efficient and privacy-preserving."}}
{"id": "2508.09537", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09537", "abs": "https://arxiv.org/abs/2508.09537", "authors": ["Yanzhou Li", "Tianlin Li", "Yiran Zhang", "Shangqing Liu", "Aishan Liu", "Yang Liu"], "title": "Your Coding Intent is Secretly in the Context and You Should Deliberately Infer It Before Completion", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used for function completion in\nrepository-scale codebases. Prior studies demonstrate that when explicit\ninstructions--such as docstrings--are provided, these models can generate\nhighly accurate implementations. However, in real-world repositories, such\nannotations are frequently absent, and performance drops substantially without\nthem. To address this gap, we frame the task as a three-stage process. The\nfirst stage focuses on intent inference, where the model analyzes the code\npreceding the target function to uncover cues about the desired functionality.\nSuch preceding context often encodes subtle but critical information, and we\ndesign a reasoning-based prompting framework to guide the LLM through\nstep-by-step extraction and synthesis of these signals before any code is\ngenerated. The second stage introduces an optional interactive refinement\nmechanism to handle cases where preceding context alone is insufficient for\nintent recovery. In this stage, the model proposes a small set of candidate\nintentions, enabling the developer to select or edit them so that the inferred\nintent closely matches the actual requirement. Finally, in the third stage, the\nLLM generates the target function conditioned on the finalized intent. To\nsupport this pipeline, we curate a dataset of 40,000 examples annotated with\nintermediate reasoning traces and corresponding docstrings. Extensive\nexperiments on DevEval and ComplexCodeEval show that our approach consistently\nboosts multiple LLMs, achieving over 20\\% relative gains in both\nreference-based and execution-based metrics, with the interactive refinement\nstage delivering additional improvements beyond these gains.", "AI": {"tldr": "This paper introduces a reasoning and interaction-driven pipeline to enable LLMs to reliably generate code functions without explicit instructions, achieving substantial accuracy gains and providing tools for developer intervention when intent is unclear.", "motivation": "Previous methods for code completion with LLMs are effective only when explicit instructions like docstrings are present, but these are often missing in practical scenarios, leading to a significant performance drop.", "method": "A three-stage process: 1) Intent inference from preceding code context using reasoning-based prompting, 2) Optional interactive refinement allowing developer input on inferred intentions, and 3) Function generation conditioned on the finalized intent. They also curated a dataset with annotated reasoning traces and docstrings for training and evaluation.", "result": "The approach leads to consistent improvements across multiple LLMs, with over 20% relative gains in reference-based and execution-based metrics. The interactive refinement stage adds further improvements.", "conclusion": "The proposed approach significantly improves LLM performance for function completion in real-world code repositories lacking explicit instructions, with interactive refinement providing extra benefits."}}
{"id": "2508.09648", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.09648", "abs": "https://arxiv.org/abs/2508.09648", "authors": ["Taohong Zhu", "Lucas C. Cordeiro", "Youcheng Sun"], "title": "ReqInOne: A Large Language Model-Based Agent for Software Requirements Specification Generation", "comment": null, "summary": "Software Requirements Specification (SRS) is one of the most important\ndocuments in software projects, but writing it manually is time-consuming and\noften leads to ambiguity. Existing automated methods rely heavily on manual\nanalysis, while recent Large Language Model (LLM)-based approaches suffer from\nhallucinations and limited controllability. In this paper, we propose ReqInOne,\nan LLM-based agent that follows the common steps taken by human requirements\nengineers when writing an SRS to convert natural language into a structured\nSRS. ReqInOne adopts a modular architecture by decomposing SRS generation into\nthree tasks: summary, requirement extraction, and requirement classification,\neach supported by tailored prompt templates to improve the quality and\nconsistency of LLM outputs.\n  We evaluate ReqInOne using GPT-4o, LLaMA 3, and DeepSeek-R1, and compare the\ngenerated SRSs against those produced by the holistic GPT-4-based method from\nprior work as well as by entry-level requirements engineers. Expert evaluations\nshow that ReqInOne produces more accurate and well-structured SRS documents.\nThe performance advantage of ReqInOne benefits from its modular design, and\nexperimental results further demonstrate that its requirement classification\ncomponent achieves comparable or even better results than the state-of-the-art\nrequirement classification model.", "AI": {"tldr": "ReqInOne is a modular LLM-based system for automated SRS generation that outperforms existing LLM approaches and entry-level engineers in both accuracy and structure, benefiting from a task-specific architecture and improved classification.", "motivation": "Writing Software Requirements Specifications (SRS) manually is labor-intensive and prone to ambiguity. Existing automated and LLM-based methods present challenges such as heavy reliance on manual analysis or issues like hallucination and limited control.", "method": "The paper introduces ReqInOne, an LLM-based agent with a modular architecture. It breaks down SRS generation into three tasks: summary, requirement extraction, and requirement classification, each handled with specialized prompt templates. The system is evaluated using different LLMs (GPT-4o, LLaMA 3, DeepSeek-R1) and compared with both holistic LLM-based methods and entry-level human engineers.", "result": "ReqInOne produces SRS documents that are more accurate and well-structured compared to both holistic LLM approaches and entry-level engineers. Its modular design results in superior performance, and its requirement classification component matches or exceeds current state-of-the-art models.", "conclusion": "The modular, prompt-guided design of ReqInOne overcomes limitations of previous SRS automation approaches, delivering high-quality, consistent, and reliable SRS documents. It sets a new benchmark for automated SRS generation and requirement classification."}}
{"id": "2508.09676", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09676", "abs": "https://arxiv.org/abs/2508.09676", "authors": ["Vishal Khare", "Vijay Saini", "Deepak Sharma", "Anand Kumar", "Ankit Rana", "Anshul Yadav"], "title": "DeputyDev -- AI Powered Developer Assistant: Breaking the Code Review Logjam through Contextual AI to Boost Developer Productivity", "comment": "12 pages, 5 figures, 6 pages of supplementary materials", "summary": "This study investigates the implementation and efficacy of DeputyDev, an\nAI-powered code review assistant developed to address inefficiencies in the\nsoftware development process. The process of code review is highly inefficient\nfor several reasons, such as it being a time-consuming process, inconsistent\nfeedback, and review quality not being at par most of the time. Using our\ntelemetry data, we observed that at TATA 1mg, pull request (PR) processing\nexhibits significant inefficiencies, with average pick-up and review times of\n73 and 82 hours, respectively, resulting in a 6.2 day closure cycle. The review\ncycle was marked by prolonged iterative communication between the reviewing and\nsubmitting parties. Research from the University of California, Irvine\nindicates that interruptions can lead to an average of 23 minutes of lost\nfocus, critically affecting code quality and timely delivery. To address these\nchallenges, we developed DeputyDev's PR review capabilities by providing\nautomated, contextual code reviews. We conducted a rigorous double-controlled\nA/B experiment involving over 200 engineers to evaluate DeputyDev's impact on\nreview times. The results demonstrated a statistically significant reduction in\nboth average per PR (23.09%) and average per-line-of-code (40.13%) review\ndurations. After implementing safeguards to exclude outliers, DeputyDev has\nbeen effectively rolled out across the entire organisation. Additionally, it\nhas been made available to external companies as a Software-as-a-Service (SaaS)\nsolution, currently supporting the daily work of numerous engineering\nprofessionals. This study explores the implementation and effectiveness of\nAI-assisted code reviews in improving development workflow timelines and code.", "AI": {"tldr": "DeputyDev, an AI-powered code review tool, reduced review durations by over 23% per pull request and 40% per line of code in a large-scale experiment, streamlining development processes and now supports engineers both internally and as a SaaS product.", "motivation": "Code review processes in software development are often inefficient, leading to long review cycles, inconsistent feedback, and reduced code quality. At TATA 1mg, inefficiencies were observed, including long pull request pick-up and review times, resulting in extended closure cycles. The study aims to address these bottlenecks and improve workflow efficiency.", "method": "The researchers developed DeputyDev, an AI-powered code review assistant. They implemented it for automated, contextual review of pull requests (PRs). Its efficacy was tested using a double-controlled A/B experiment with over 200 engineers, and results were analyzed using statistical methods, with safeguards in place to exclude outliers.", "result": "The A/B experiment showed a statistically significant reduction in review times: 23.09% reduction in average review time per PR and a 40.13% reduction per line of code. DeputyDev was subsequently deployed organization-wide and offered externally as a SaaS, demonstrating practical benefits and adoption.", "conclusion": "AI-assisted code review, as demonstrated by DeputyDev, can significantly improve the efficiency and quality of software development by reducing review times and enhancing consistency. Its successful rollout at TATA 1mg and as a SaaS shows its effectiveness and broader applicability."}}
{"id": "2508.09680", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.09680", "abs": "https://arxiv.org/abs/2508.09680", "authors": ["Orvila Sarker", "Mona Jamshaid", "M. Ali Babar"], "title": "Inclusive Employment Pathways: Career Success Factors for Autistic Individuals in Software Engineering", "comment": null, "summary": "Research has highlighted the valuable contributions of autistic individuals\nin the Information and Communication Technology (ICT) sector, particularly in\nareas such as software development, testing, and cybersecurity. Their strengths\nin information processing, attention to detail, innovative thinking, and\ncommitment to high-quality outcomes in the ICT domain are well-documented.\nHowever, despite their potential, autistic individuals often face barriers in\nSoftware Engineering (SE) roles due to a lack of personalised tools, complex\nwork environments, non-inclusive recruitment practices, limited co-worker\nsupport, challenging social dynamics and so on. Motivated by the ethical\nframework of the neurodiversity movement and the success of pioneering\ninitiatives like the Dandelion program, corporate Diversity, Equity, and\nInclusion (DEI) in the ICT sector has increasingly focused on autistic talent.\nThis movement fundamentally reframes challenges not as individual deficits but\nas failures of environments designed for a neurotypical majority. Despite this\nprogress, there is no synthesis of knowledge reporting the full pathway from\nsoftware engineering education through to sustainable workplace inclusion. To\naddress this, we conducted a Systematic Review of 30 studies and identified 18\nsuccess factors grouped into four thematic categories: (1) Software Engineering\nEducation, (2) Career and Employment Training, (3) Work Environment, and (4)\nTools and Assistive Technologies. Our findings offer evidence-based\nrecommendations for educational institutions, employers, organisations, and\ntool developers to enhance the inclusion of autistic individuals in SE. These\ninclude strategies for inclusive meeting and collaboration practices,\naccessible and structured work environments, clear role and responsibility\ndefinitions, and the provision of tailored workplace accommodations.", "AI": {"tldr": "This paper systematically reviews research on the inclusion of autistic individuals in software engineering, identifies success factors, and provides practical recommendations for improving education, recruitment, and workplace support.", "motivation": "Autistic individuals have demonstrated important strengths in ICT, but face persistent barriers in software engineering roles due to non-inclusive environments, lack of support, and unsuitable tools. There is a need to understand how to better facilitate the inclusion of autistic talent throughout the SE field.", "method": "A systematic review of 30 studies was conducted to identify factors influencing successful inclusion of autistic individuals in software engineering.", "result": "The review identified 18 key success factors across four categories: SE education, career and employment training, work environment, and tools/assistive technologies. These factors inform actionable, evidence-based recommendations for stakeholders.", "conclusion": "Recommendations include implementing inclusive collaboration practices, accessible and structured workspaces, clear role definitions, and tailored accommodations. The findings aim to guide organizations and educators in promoting sustainable inclusion of autistic individuals in software engineering."}}
{"id": "2508.09791", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09791", "abs": "https://arxiv.org/abs/2508.09791", "authors": ["Junxiao Han", "Yarong Wang", "Xiaodong Gu", "Cuiyun Gao", "Yao Wan", "Song Han", "David Lo", "Shuiguang Deng"], "title": "LibRec: Benchmarking Retrieval-Augmented LLMs for Library Migration Recommendations", "comment": null, "summary": "In this paper, we propose LibRec, a novel framework that integrates the\ncapabilities of LLMs with retrieval-augmented generation(RAG) techniques to\nautomate the recommendation of alternative libraries. The framework further\nemploys in-context learning to extract migration intents from commit messages\nto enhance the accuracy of its recommendations. To evaluate the effectiveness\nof LibRec, we introduce LibEval, a benchmark designed to assess the performance\nin the library migration recommendation task. LibEval comprises 2,888 migration\nrecords associated with 2,368 libraries extracted from 2,324 Python\nrepositories. Each migration record captures source-target library pairs, along\nwith their corresponding migration intents and intent types. Based on LibEval,\nwe evaluated the effectiveness of ten popular LLMs within our framework,\nconducted an ablation study to examine the contributions of key components\nwithin our framework, explored the impact of various prompt strategies on the\nframework's performance, assessed its effectiveness across various intent\ntypes, and performed detailed failure case analyses.", "AI": {"tldr": "LibRec presents an automated framework that uses LLMs and RAG to recommend library migrations, extracting intent from commit messages and evaluated on a new benchmark dataset. The system is tested on ten LLMs, analyzed via ablation and prompt strategy studies, and shows strong results, paving the way for future improvements.", "motivation": "Developers frequently need to migrate from one software library to another, but finding suitable alternative libraries is challenging. Automating this process would save time and reduce errors.", "method": "LibRec integrates large language models (LLMs) with retrieval-augmented generation (RAG) techniques, using in-context learning to extract migration intents from commit messages. For evaluation, the authors introduce LibEval, a large benchmark dataset of migration records from Python repositories. They test ten LLMs, perform ablation studies, analyze prompt strategies, assess across intent types, and investigate failure cases.", "result": "LibRec shows strong performance in automated library recommendation and migration tasks. The evaluation reveals the contributions of key components like RAG and in-context learning, effectiveness of prompt strategies, and performance on different migration intent types. Detailed failure case analyses provide insights for future improvement.", "conclusion": "LibRec is a promising framework that effectively combines LLMs and RAG for automated library recommendation and migration tasks. Its performance is validated on a comprehensive benchmark, and failure case analysis identifies directions for enhancing the framework."}}
{"id": "2508.09828", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.09828", "abs": "https://arxiv.org/abs/2508.09828", "authors": ["Sebastiano Antonio Piccolo"], "title": "Fast and Accurate Heuristics for Bus-Factor Estimation", "comment": null, "summary": "The bus-factor is a critical risk indicator that quantifies how many key\ncontributors a project can afford to lose before core knowledge or\nfunctionality is compromised. Despite its practical importance, accurately\ncomputing the bus-factor is NP-Hard under established formalizations, making\nscalable analysis infeasible for large software systems.\n  In this paper, we model software projects as bipartite graphs of developers\nand tasks and propose two novel approximation heuristics, Minimum Coverage and\nMaximum Coverage, based on iterative graph peeling, for two influential\nbus-factor formalizations. Our methods significantly outperform the widely\nadopted degree-based heuristic, which we show can yield severely inflated\nestimates.\n  We conduct a comprehensive empirical evaluation on over $1\\,000$ synthetic\npower-law graphs and demonstrate that our heuristics provide tighter estimates\nwhile scaling to graphs with millions of nodes and edges in minutes. Our\nresults reveal that the proposed heuristics are not only more accurate but also\nrobust to structural variations in developer-task assignment graph. We release\nour implementation as open-source software to support future research and\npractical adoption.", "AI": {"tldr": "This paper introduces two scalable and accurate heuristics for estimating the bus-factor in large software projects, outperforming existing approaches and enabling practical risk assessment at scale.", "motivation": "Accurately determining the bus-factor\u2014the number of key contributors whose loss would compromise a software project\u2014is critical, but current methods are computationally infeasible for large projects due to NP-Hardness.", "method": "The authors model software projects as bipartite graphs of developers and tasks and propose two new approximation heuristics (Minimum Coverage and Maximum Coverage) based on iterative graph peeling to estimate the bus-factor.", "result": "The proposed heuristics deliver tighter and more accurate bus-factor estimates than the common degree-based heuristic, and they scale efficiently to very large graphs (millions of nodes and edges). These methods are also robust to different graph structures.", "conclusion": "The new heuristics are both accurate and scalable, making practical and precise bus-factor assessment possible for large and complex software projects. Their open-source release supports further study and real-world adoption."}}
{"id": "2508.09832", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09832", "abs": "https://arxiv.org/abs/2508.09832", "authors": ["Linh Nguyen", "Chunhua Liu", "Hong Yi Lin", "Patanamon Thongtanunam"], "title": "Exploring the Potential of Large Language Models in Fine-Grained Review Comment Classification", "comment": "Accepted at 2025 IEEE International Conference on Source Code\n  Analysis & Manipulation (SCAM)", "summary": "Code review is a crucial practice in software development. As code review\nnowadays is lightweight, various issues can be identified, and sometimes, they\ncan be trivial. Research has investigated automated approaches to classify\nreview comments to gauge the effectiveness of code reviews. However, previous\nstudies have primarily relied on supervised machine learning, which requires\nextensive manual annotation to train the models effectively. To address this\nlimitation, we explore the potential of using Large Language Models (LLMs) to\nclassify code review comments. We assess the performance of LLMs to classify 17\ncategories of code review comments. Our results show that LLMs can classify\ncode review comments, outperforming the state-of-the-art approach using a\ntrained deep learning model. In particular, LLMs achieve better accuracy in\nclassifying the five most useful categories, which the state-of-the-art\napproach struggles with due to low training examples. Rather than relying\nsolely on a specific small training data distribution, our results show that\nLLMs provide balanced performance across high- and low-frequency categories.\nThese results suggest that the LLMs could offer a scalable solution for code\nreview analytics to improve the effectiveness of the code review process.", "AI": {"tldr": "LLMs can classify code review comments better than previous deep learning models, especially in hard-to-learn and infrequent categories, offering a more scalable and balanced solution for code review analytics.", "motivation": "Traditional automated classification of code review comments relies on supervised machine learning, which demands extensive manual annotation, limiting scalability and adaptability, especially for comments in less-represented categories.", "method": "The study explores the use of Large Language Models (LLMs) for the automated classification of code review comments, specifically evaluating their accuracy across 17 categories and comparing results against a state-of-the-art deep learning model.", "result": "LLMs outperform the trained deep learning model, especially on the five most useful categories that initially suffer from low training examples. LLMs maintain balanced accuracy across both frequent and infrequent comment categories.", "conclusion": "LLMs present a scalable and effective alternative for code review comment classification, improving the potential for automated code review analytics and supporting more effective code review processes."}}
{"id": "2508.09875", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.09875", "abs": "https://arxiv.org/abs/2508.09875", "authors": ["Jinbao Chen", "Boyao Ding", "Yu Zhang", "Qingwei Li", "Fugen Tang"], "title": "An Empirical Study of CGO Usage in Go Projects -- Distribution, Purposes, Patterns and Critical Issues", "comment": "Accepted for publication in The Journal of Systems and Software", "summary": "Multilingual software development integrates multiple languages into a single\napplication, with the Foreign Function Interface (FFI) enabling seamless\ninteraction. While FFI boosts efficiency and extensibility, it also introduces\nrisks. Existing studies focus on FFIs in languages like Python and Java,\nneglecting CGO, the emerging FFI in Go, which poses unique risks.\n  To address these concerns, we conduct an empirical study of CGO usage across\n920 open-source Go projects. Our study aims to reveal the distribution,\npatterns, purposes, and critical issues associated with CGO, offering insights\nfor developers and the Go team. We develop CGOAnalyzer, a tool to efficiently\nidentify and quantify CGO-related features. Our findings reveal that: (1) 11.3%\nof analyzed Go projects utilize CGO, with usage concentrated in a subset of\nprojects; (2) CGO serves 4 primary purposes, including system-level\ninteractions and performance optimizations, with 15 distinct usage patterns\nobserved; (3) 19 types of CGO-related issues exist, including one critical\nissue involving unnecessary pointer checks that pose risks of runtime crashes\ndue to limitations in the current Go compilation toolchain; (4) a temporary\nsolution reduces unnecessary pointer checks, mitigating crash risks, and (5) we\nsubmitted a proposal to improve the Go toolchain for a permanent fix, which has\nbeen grouped within an accepted proposal for future resolution. Our findings\nprovide valuable insights for developers and the Go team, enhancing development\nefficiency and reliability while improving the robustness of the Go toolchain.", "AI": {"tldr": "This paper empirically studies CGO (Go's FFI) across 920 projects, revealing usage statistics, patterns, purposes, and 19 issue types. A critical runtime crash risk was found and mitigated with a temporary solution; a permanent toolchain fix is forthcoming based on the team's proposal. The findings support safer and more efficient multilingual development in Go.", "motivation": "Multilingual software development often relies on Foreign Function Interface (FFI) to integrate multiple programming languages. While many studies have analyzed FFI in languages such as Python and Java, CGO\u2014the FFI for Go\u2014remains underexplored despite its unique risks and growing adoption.", "method": "Researchers conducted an empirical study on 920 open-source Go projects to investigate the usage, patterns, purposes, and issues associated with CGO. They developed a specialized tool, CGOAnalyzer, to automate the identification and quantification of CGO-related features in these projects.", "result": "The study found 11.3% of Go projects use CGO, concentrated in a subset of projects. There are 4 primary purposes for its use and 15 distinct usage patterns. 19 types of CGO-related issues were identified, including a critical issue with unnecessary pointer checks, which can cause runtime crashes. A temporary solution was implemented, and a proposal for a permanent fix was submitted\u2014and included in an upcoming update of the Go toolchain.", "conclusion": "The research delivers actionable insights about CGO usage and its risks, helping Go developers and maintainers improve efficiency, reliability, and robustness in their projects. It also informs improvements to the Go compilation toolchain."}}
