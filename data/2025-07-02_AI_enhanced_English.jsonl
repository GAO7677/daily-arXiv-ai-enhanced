{"id": "2507.00347", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00347", "abs": "https://arxiv.org/abs/2507.00347", "authors": ["Sun Ding", "Ude Enebeli", "Atilhan", "Manay", "Ryan Pua", "Kamal Kotak"], "title": "VTS-Guided AI Interaction Workflow for Business Insights", "comment": null, "summary": "Modern firms face a flood of dense, unstructured reports. Turning these\ndocuments into usable insights takes heavy effort and is far from agile when\nquick answers are needed. VTS-AI tackles this gap. It integrates Visual\nThinking Strategies, which emphasize evidence-based observation, linking, and\nthinking, into AI agents, so the agents can extract business insights from\nunstructured text, tables, and images at scale. The system works in three tiers\n(micro, meso, macro). It tags issues, links them to source pages, and rolls\nthem into clear action levers stored in a searchable YAML file. In tests on an\n18-page business report, VTS-AI matched the speed of a one-shot ChatGPT prompt\nyet produced richer findings: page locations, verbatim excerpts, severity\nscores, and causal links. Analysts can accept or adjust these outputs in the\nsame IDE, keeping human judgment in the loop. Early results show VTS-AI spots\nthe direction of key metrics and flags where deeper number-crunching is needed.\nNext steps include mapping narrative tags to financial ratios, adding\nfinance-tuned language models through a Model-Context Protocol, and building a\nRisk & Safety Layer to stress-test models and secure data. These upgrades aim\nto make VTS-AI a production-ready, audit-friendly tool for rapid business\nanalysis.", "AI": {"tldr": "VTS-AI uses AI and Visual Thinking Strategies to extract actionable business insights from unstructured reports faster and with richer details than current methods, while keeping human feedback in the loop. Early results are promising, and further developments will enhance its reliability and security for business analysis.", "motivation": "Modern organizations struggle to rapidly extract actionable insights from dense, unstructured business reports. The current methods are labor-intensive and lack agility, especially when quick answers are required.", "method": "VTS-AI integrates Visual Thinking Strategies into AI agents, enabling extraction of business insights from unstructured text, tables, and images at scale. The system operates in three tiers (micro, meso, macro), tags issues, links them to source pages, and generates action items stored in a searchable YAML file. The workflow combines automated extraction with human judgment within an IDE.", "result": "In tests on an 18-page business report, VTS-AI delivered results as quickly as a one-shot ChatGPT prompt but with richer output, including page locations, verbatim excerpts, severity scores, and causal links. The system supports analyst oversight and highlights metric directions and areas needing deeper analysis.", "conclusion": "VTS-AI demonstrates promise as a scalable, agile tool for extracting and organizing insights from complex business reports. Planned enhancements\u2014such as mapping narrative tags to financial ratios and adding security layers\u2014aim to improve auditability and readiness for production use."}}
{"id": "2507.00352", "categories": ["cs.SE", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.00352", "abs": "https://arxiv.org/abs/2507.00352", "authors": ["Abanoub E. Abdelmalak", "Mohamed A. Elsayed", "David Abercrombie", "Ilhami Torunoglu"], "title": "An AST-guided LLM Approach for SVRF Code Synthesis", "comment": "9 Pages, 5 Figures, 2 Tables", "summary": "Standard Verification Rule Format (SVRF) is essential for semiconductor\napplications like Design Rule Check (DRC), Layout Versus Schematic (LVS), and\nOptical Proximity Correction (OPC) and it faces challenges as advancing nodes\ncreate complex design rules that renders traditional SVRF development\nineffective and highlight an expertise gap. This paper introduces a novel\nmethodology integrating Abstract Syntax Tree (AST) embedding and\nRetrieval-Augmented Generation (RAG) for enhanced SVRF code synthesis, ensuring\nsemantic accuracy and error minimization through structural validation with\ndomain-specific insights for precise code generation.\n  We evaluate different T5-based models and propose an innovative SVRF-specific\nscoring framework that complements standard metrics like BLEU and ROUGE-L. In\nour approach, AST provides rigorous structural validation, while RAG infuses\nrelevant domain knowledge, effectively enhancing the code generation workflow.\n  Testing on a comprehensive benchmark of 740 DRC rule implementations, our\nmethodology demonstrates up to a 40\\% improvement in code generation accuracy\ncompared to basic text-based fine-tuning process. This fusion of industry\nexpertise with advanced coding strategies not only optimizes SVRF development\nunder limited dataset constraints but also creates a more intuitive and\nefficient coding environment. Consequently, users can rapidly iterate through\ndesign cycles, reduce manual error correction, and significantly improve\noverall productivity.", "AI": {"tldr": "The paper presents a new method combining AST and RAG for generating SVRF code, achieving up to 40% higher accuracy over standard methods and streamlining semiconductor design workflows.", "motivation": "Traditional SVRF development is becoming ineffective due to increasingly complex semiconductor design rules with advancing manufacturing nodes, exposing an expertise gap and workflow challenges.", "method": "The authors introduce a methodology that combines Abstract Syntax Tree (AST) embedding and Retrieval-Augmented Generation (RAG) for SVRF code synthesis. They also use several T5-based models and present a specialized SVRF scoring framework alongside metrics like BLEU and ROUGE-L. Structural validation is provided by AST, while RAG contributes domain-specific information to enhance code generation accuracy.", "result": "Testing on a benchmark of 740 DRC rule implementations shows up to a 40% improvement in code generation accuracy with the proposed methodology compared to typical text-based fine-tuning approaches.", "conclusion": "Integrating domain expertise with advanced code generation strategies (AST and RAG) significantly enhances SVRF development, facilitating rapid design iteration, reducing manual errors, and boosting overall productivity, even with limited datasets."}}
{"id": "2507.00378", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00378", "abs": "https://arxiv.org/abs/2507.00378", "authors": ["Xikai Sun", "Fan Dang", "Kebin Liu", "Xin Miao", "Zihao Yang", "Haimo Lu", "Yawen Zheng", "Yunhao Liu"], "title": "iPanda: An Intelligent Protocol Testing and Debugging Agent for Conformance Testing", "comment": "14 pages, 6 figures", "summary": "Conformance testing is essential for ensuring that protocol implementations\ncomply with their specifications. However, traditional testing approaches\ninvolve manually creating numerous test cases and scripts, making the process\nlabor-intensive and inefficient. Recently, Large Language Models (LLMs) have\ndemonstrated impressive text comprehension and code generation abilities,\nproviding promising opportunities for automation. In this paper, we propose\niPanda, the first end-to-end framework that leverages LLMs to automate protocol\nconformance testing. Given a protocol specification document and its\nimplementation, iPanda first employs a keyword-based method to automatically\ngenerate comprehensive test cases. Then, it utilizes a code-based\nretrieval-augmented generation approach to effectively interpret the\nimplementation and produce executable test code. To further enhance code\nquality, iPanda incorporates an iterative self-correction mechanism to refine\ngenerated test scripts interactively. Finally, by executing and analyzing the\ngenerated tests, iPanda systematically verifies compliance between\nimplementations and protocol specifications. Comprehensive experiments on\nvarious protocols show that iPanda significantly outperforms pure LLM-based\napproaches, improving the success rate (Pass@1) of test-code generation by\nfactors ranging from 4.675 times to 10.751 times.", "AI": {"tldr": "iPanda leverages LLMs to fully automate protocol conformance testing, achieving much higher test-code generation success than pure LLM methods.", "motivation": "Conformance testing for protocol implementations is crucial but traditionally requires significant manual effort to create and maintain numerous test cases and scripts.", "method": "The paper introduces iPanda, a framework that uses Large Language Models (LLMs) to automate protocol conformance testing. It uses a keyword-based generation for test cases, retrieval-augmented generation to interpret code, and an iterative self-correction loop to refine generated scripts, concluding with automated execution and compliance verification.", "result": "Experiments demonstrate that iPanda significantly improves automated test-code generation, with Pass@1 success rates 4.675\u201310.751 times higher than pure LLM-based methods.", "conclusion": "iPanda is an effective, automated end-to-end solution for protocol conformance testing, greatly reducing manual workload and achieving superior performance over existing LLM-only techniques."}}
{"id": "2507.00413", "categories": ["cs.SE", "D.2.7"], "pdf": "https://arxiv.org/pdf/2507.00413", "abs": "https://arxiv.org/abs/2507.00413", "authors": ["Taiming Wang", "Hui Liu", "Yuxia Zhang", "Yanjie Jiang"], "title": "Recommending Variable Names for Extract Local Variable Refactorings", "comment": "Accepted by TOSEM", "summary": "Extract local variable is one of the most popular refactorings, and most IDEs\nand refactoring tools provide automated support for this refactoring. However,\nwe find approximately 70% of the names recommended by these IDEs are different\nfrom what developers manually constructed, adding additional renaming burdens\nto developers and providing limited assistance. In this paper, we introduce\nVarNamer, an automated approach designed to recommend variable names for\nextract local variable refactorings. Through a large-scale empirical study, we\nidentify key contexts that are useful for composing variable names. Leveraging\nthese insights, we developed a set of heuristic rules through program static\nanalysis techniques and employ data mining techniques to recommend variable\nnames effectively. Notably, some of our heuristic rules have been successfully\nintegrated into Eclipse, where they are now distributed with the latest\nreleases of the IDE. Evaluation demonstrates its superiority over\nstate-of-the-art IDEs. Specifically, VarNamer significantly increases the\nchance of exact match by 52.6% compared to Eclipse and 40.7% compared to\nIntelliJ IDEA. We also evaluated the proposed approach with real-world extract\nlocal variable refactorings conducted in C++ projects, and the results suggest\nthat the approach can achieve comparable performance on programming languages\nbesides Java. It may suggest the generalizability of VarNamer. Finally, we\ndesigned and conducted a user study and the results of the user study suggest\nthat our approach can speed up the refactoring by 27.8% and reduce 49.3% edits\non the recommended variable names.", "AI": {"tldr": "Automated IDE suggestions for variable names often miss the mark. VarNamer uses empirical and heuristic methods to recommend better names, outperforming state-of-the-art tools in accuracy and efficiency. It's been adopted in Eclipse and proven useful in both Java and C++ projects, streamlining the refactoring process for developers.", "motivation": "Automated refactoring tools often suggest variable names that don't match developers' preferences, resulting in extra renaming work and reduced usefulness of such tools. This paper wants to improve the quality of variable name recommendations to better assist developers during extract local variable refactorings.", "method": "The authors conducted a large-scale empirical study to identify important contexts for creating variable names. Based on this, they developed heuristic rules using static analysis and data mining for recommending variable names. They integrated some of these heuristics into Eclipse and evaluated the approach through empirical comparison and user studies.", "result": "VarNamer, the proposed tool, increased the exact match rate of suggested names by 52.6% over Eclipse and 40.7% over IntelliJ IDEA. User studies showed that VarNamer sped up refactoring by 27.8% and reduced naming edits by 49.3%. The method also showed comparable results on C++ projects, indicating potential generalizability beyond Java.", "conclusion": "VarNamer significantly outperforms existing IDEs in variable name recommendation for extract local variable refactoring, reducing developer effort and showing effectiveness across different programming languages. Some techniques have been adopted by major IDEs, and user studies confirm its practical usefulness."}}
{"id": "2507.00057", "categories": ["cs.PL", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.00057", "abs": "https://arxiv.org/abs/2507.00057", "authors": ["Thomas Valentin", "Ardi Madadi", "Gaetano Sapia", "Marcel B\u00f6hme"], "title": "Estimating Correctness Without Oracles in LLM-Based Code Generation", "comment": "8 pages + refs and appendix", "summary": "Generating code from natural language specifications is one of the most\nsuccessful applications of Large Language Models (LLMs). Yet, they hallucinate:\nLLMs produce outputs that may be grammatically correct but are factually\nincorrect. Without an existing, correct implementation (i.e., an oracle), can\nwe quantify how likely the generated program is correct?\n  In this paper, we propose a measure of incorrectness, called incoherence,\nthat can be estimated efficiently in the absence of an oracle and provides a\nlower bound on the error, i.e., the probability that the LLM-generated program\nfor that specification is incorrect. Our experiments demonstrate an\nextraordinary effectiveness. For the average code generation task, our\nincoherence-based methodology can automatically identify about two-thirds of\nincorrect programs without reports of false positives. In fact, an oracle-based\nevaluation of LLMs can be reliably replaced by an incoherence-based evaluation.\nIn particular, we find a very strong agreement between the ranking of LLMs by\nthe number of programs deemed correct via an oracle (pass@1) and the ranking of\nLLMs by the number of programs deemed correct via our incoherence.", "AI": {"tldr": "The paper proposes 'incoherence' as a new way to measure code incorrectness from LLM-generated code without needing a reference solution. This method efficiently detects most incorrect programs and closely matches traditional evaluation techniques.", "motivation": "Large Language Models (LLMs) are effective at generating code from natural language, but they sometimes produce code that is syntactically correct but semantically wrong ('hallucinations'). Currently, assessing code correctness requires a known correct implementation (an 'oracle'), which is often unavailable. There's a need for a way to estimate the correctness of generated code without an oracle.", "method": "The authors introduce a new metric called 'incoherence' to estimate code incorrectness. This incoherence metric can be calculated efficiently and does not rely on having an oracle (a correct reference implementation). The method is validated experimentally by comparing incoherence-based evaluation with traditional oracle-based assessment.", "result": "The proposed incoherence measure identifies about two-thirds of all incorrect code outputs without false positives. The incoherence-based approach shows a very high correlation with traditional oracle-based evaluation, enabling reliable code assessment even when an oracle is not available.", "conclusion": "Incoherence can serve as an effective proxy for oracle-based evaluations, enabling efficient and automated identification of incorrect code generated by LLMs without access to a ground truth implementation."}}
{"id": "2507.00421", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.00421", "abs": "https://arxiv.org/abs/2507.00421", "authors": ["Parthiv Katapara", "Anand Sharma"], "title": "Embedded DevOps: A Survey on the Application of DevOps Practices in Embedded Software and Firmware Development", "comment": "This paper present survey on DevOps practices which exists in\n  Embedded Software development", "summary": "The adoption of DevOps practices in embedded systems and firmware development\nis emerging as a response to the growing complexity of modern\nhardware--software co-designed products. Unlike cloud-native applications,\nembedded systems introduce challenges such as hardware dependency, real-time\nconstraints, and safety-critical requirements. This literature review\nsynthesizes findings from 20 academic and industrial sources to examine how\nDevOps principles--particularly continuous integration, continuous delivery,\nand automated testing--are adapted to embedded contexts. We categorize efforts\nacross tooling, testing strategies, pipeline automation, and security\npractices. The review highlights current limitations in deployment workflows\nand observability, proposing a roadmap for future research. This work offers\nresearchers and practitioners a consolidated understanding of Embedded DevOps,\nbridging fragmented literature with a structured perspective.", "AI": {"tldr": "DevOps is gradually being adapted for embedded systems, but faces significant challenges due to hardware and real-time constraints. This literature review organizes ongoing efforts, highlights current gaps, and suggests directions for future research.", "motivation": "The motivation stems from the increasing complexity of hardware-software co-designed products, which makes traditional development techniques insufficient. DevOps practices, while well established in cloud-native environments, face unique challenges in embedded systems due to factors like hardware dependency and safety-critical requirements.", "method": "The paper conducts a literature review, synthesizing 20 academic and industrial sources, categorizing efforts in tooling, testing, automation, and security related to adopting DevOps in embedded contexts.", "result": "The review finds that while DevOps principles such as CI/CD and automated testing are being adapted for embedded systems, significant limitations remain, particularly in deployment workflows and observability. It categorizes current research and practice, and identifies research gaps.", "conclusion": "The work consolidates fragmented information about Embedded DevOps, highlighting current limitations and providing a structured roadmap for future research, aimed at both researchers and practitioners."}}
{"id": "2507.00264", "categories": ["cs.PL", "cs.DC", "cs.PF", "cs.SE", "D.2.13; D.2.8; D.3.3; B.8"], "pdf": "https://arxiv.org/pdf/2507.00264", "abs": "https://arxiv.org/abs/2507.00264", "authors": ["Isabella Basso do Amaral", "Renato Cordeiro Ferreira", "Alfredo Goldman"], "title": "Rust vs. C for Python Libraries: Evaluating Rust-Compatible Bindings Toolchains", "comment": "10 pages, 27 figures (1 diagram, 4 graphs, 9 tables, 13 code\n  listings), submitted to SBAC-PAD 2025", "summary": "The Python programming language is best known for its syntax and scientific\nlibraries, but it is also notorious for its slow interpreter. Optimizing\ncritical sections in Python entails special knowledge of the binary\ninteractions between programming languages, and can be cumbersome to interface\nmanually, with implementers often resorting to convoluted third-party\nlibraries. This comparative study evaluates the performance and ease of use of\nthe PyO3 Python bindings toolchain for Rust against ctypes and cffi. By using\nRust tooling developed for Python, we can achieve state-of-the-art performance\nwith no concern for API compatibility.", "AI": {"tldr": "PyO3 enables high-performance Python extensions with Rust, offering better speed and easier integration than ctypes/cffi, and simplifies maintaining compatibility.", "motivation": "Python is widely used and popular for its easy syntax and scientific libraries, but is hampered by a slow interpreter. Optimizing Python's performance often requires complex and challenging integration with lower-level languages, which can be cumbersome using traditional tools or third-party libraries.", "method": "This paper conducts a comparative study, evaluating the performance and ease of use of PyO3 (a toolchain for binding Rust code to Python) versus the traditional Python C interface libraries like ctypes and cffi.", "result": "Using Rust with the PyO3 toolchain allows developers to achieve state-of-the-art performance improvements for critical Python code sections, while avoiding compatibility headaches associated with API changes and binary-level interactions.", "conclusion": "PyO3 presents a robust and user-friendly alternative for optimizing Python code using Rust, outperforming traditional tools (ctypes, cffi) in both performance and usability, and reducing maintenance and integration complexity."}}
{"id": "2507.00481", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.00481", "abs": "https://arxiv.org/abs/2507.00481", "authors": ["Philipp M. Z\u00e4hl", "Sabine Theis", "Martin R. Wolf"], "title": "The Influence of HEXACO Personality Traits on the Teamwork Quality in Software Teams -- A Preliminary Research Approach", "comment": null, "summary": "Although software engineering research has focused on optimizing processes\nand technology, there is a growing recognition that human factors, particularly\nteamwork, also significantly impact optimization. Recent research suggests that\ndeveloper personality has a strong influence on teamwork. In fact, personality\nconsiderations may have a greater impact on software development than processes\nand tools. This paper aims to design a study that measures the impact of HEXACO\npersonality traits on the Teamwork Quality (TWQ) of software teams. A\npreliminary data collection (n=54) was conducted for this purpose. The analysis\nshowed that several personality traits, as well as their composition, had a\nsignificant impact on TWQ. Additionally, other variables, such as the\nproportion of women and age distribution, also affected TWQ. The study's\ninitial results demonstrate the usefulness and validity of the study design.\nThe results also suggest several opportunities to improve teamwork in IT\norganizations and avenues for further research.", "AI": {"tldr": "Personality traits, measured using the HEXACO model, significantly affect teamwork quality in software teams. Preliminary findings suggest that these human factors may be more influential than processes or technology, emphasizing the need for IT organizations to consider personality composition in team formation. Demographic factors like gender ratio and age distribution also play a role.", "motivation": "While much of software engineering research concentrates on optimizing processes and technology, there is increasing awareness that human elements, especially teamwork and individual developer personalities, may be equally or more impactful. This paper is motivated by the need to understand how personality traits influence teamwork quality in software teams.", "method": "The authors designed a study investigating the relationship between HEXACO personality traits and Teamwork Quality (TWQ) in software teams. A preliminary data collection was carried out with 54 participants, analyzing how individual and collective personality traits, as well as demographic variables, affect TWQ.", "result": "The preliminary analysis found that specific personality traits and their group compositions significantly influenced Teamwork Quality. Demographic factors such as the proportion of women and age distribution also had measurable effects on TWQ.", "conclusion": "The initial results support the utility and validity of the study design, revealing that personality traits are a significant factor in teamwork quality. The study points to possible improvements for teamwork in IT organizations and highlights directions for future research."}}
{"id": "2507.00488", "categories": ["cs.PL", "D.3.3; D.2"], "pdf": "https://arxiv.org/pdf/2507.00488", "abs": "https://arxiv.org/abs/2507.00488", "authors": ["Lloyd Allison"], "title": "Have Object-Oriented Languages Missed a Trick with Class Function and its Subclasses?", "comment": null, "summary": "Compared to functions in mathematics, functions in programming languages seem\nto be under classified. Functional programming languages based on the lambda\ncalculus famously treat functions as first-class values. Object-oriented\nlanguages have adopted ``lambdas'', notably for call-back routines in\nevent-based programming. Typically a programming language has functions, a\nfunction has a type, and some functions act on other functions and/or return\nfunctions but there is generally a lack of (i) ``class Function'' in the OO\nsense of the word class and particularly (ii) subclasses of Function for\nfunctions having specific properties. Some such classes are presented here and\nprogrammed in some popular programming languages as an experimental\ninvestigation into OO languages missing this opportunity.", "AI": {"tldr": "Current programming languages don't fully classify functions like mathematics does. This paper explores creating a 'Function' class and subclasses for certain function types in popular OO languages, showing that better function classification is possible and beneficial.", "motivation": "The paper is motivated by the observation that functions in programming languages are less systematically categorized (or 'classified') than mathematical functions. Although functional programming and recent developments in OO languages allow functions as values, there is no 'class Function' or subclasses representing particular function properties, which is seen as a missed opportunity.", "method": "The paper proposes and implements various subclasses of a Function class, corresponding to functions with specific properties, in a selection of popular programming languages. This experimental approach examines how object-oriented languages could benefit from more explicit and structured classifications of functions.", "result": "By introducing subclasses of Function in object-oriented programming languages, the authors demonstrate that this richer classification system is both feasible and potentially advantageous. The experiments show opportunities for improved structure and expressiveness in handling functions within existing programming environments.", "conclusion": "Object-oriented programming languages could benefit from a more deliberate classification of functions, including the use of a base Function class and related subclasses describing specific function properties. Such an approach is implementable in current languages and may enrich the way programmers utilize and reason about functions."}}
{"id": "2507.00496", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.00496", "abs": "https://arxiv.org/abs/2507.00496", "authors": ["Hongjing Guo", "Chuanqi Tao", "Zhiqiu Huang", "Weiqin Zou"], "title": "Coverage-Guided Testing for Deep Learning Models: A Comprehensive Survey", "comment": null, "summary": "As Deep Learning (DL) models are increasingly applied in safety-critical\ndomains, ensuring their quality has emerged as a pressing challenge in modern\nsoftware engineering. Among emerging validation paradigms, coverage-guided\ntesting (CGT) has gained prominence as a systematic framework for identifying\nerroneous or unexpected model behaviors. Despite growing research attention,\nexisting CGT studies remain methodologically fragmented, limiting the\nunderstanding of current advances and emerging trends. This work addresses that\ngap through a comprehensive review of state-of-the-art CGT methods for DL\nmodels, including test coverage analysis, coverage-guided test input\ngeneration, and coverage-guided test input optimization. This work provides\ndetailed taxonomies to organize these methods based on methodological\ncharacteristics and application scenarios. We also investigate evaluation\npractices adopted in existing studies, including the use of benchmark datasets,\nmodel architectures, and evaluation aspects. Finally, open challenges and\nfuture directions are highlighted in terms of the correlation between\nstructural coverage and testing objectives, method generalizability across\ntasks and models, practical deployment concerns, and the need for standardized\nevaluation and tool support. This work aims to provide a roadmap for future\nacademic research and engineering practice in DL model quality assurance.", "AI": {"tldr": "This paper reviews and categorizes the latest coverage-guided testing methods for deep learning models, highlighting trends, evaluation practices, and ongoing challenges, and proposes directions for future research to ensure model quality in safety-critical applications.", "motivation": "Deep learning models are increasingly used in critical applications where errors can have serious consequences. However, ensuring the quality of these models is challenging, and existing coverage-guided testing (CGT) research is fragmented, making it hard to track progress and identify trends.", "method": "The paper performs a comprehensive review of current CGT methods for deep learning models. It analyzes state-of-the-art techniques within three main areas: test coverage analysis, test input generation, and input optimization. The paper constructs taxonomies to classify these methods and studies evaluation practices such as benchmark datasets and model architectures used.", "result": "The review organizes and categorizes existing CGT approaches, providing a structured understanding of methodological characteristics and application contexts. It identifies current evaluation practices and gaps, and outlines open challenges including standardization, tool support, method generalizability, and links between coverage and testing objectives.", "conclusion": "The paper lays out a comprehensive roadmap for further research and practical improvements in quality assurance for deep learning models using coverage-guided testing, highlighting areas needing future work and standardization."}}
{"id": "2507.00686", "categories": ["cs.SE", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.00686", "abs": "https://arxiv.org/abs/2507.00686", "authors": ["Ronny Seiger", "Daniel Locher", "Marco Kaufmann", "Aaron F. Kurz"], "title": "A Domain-specific Language and Architecture for Detecting Process Activities from Sensor Streams in IoT", "comment": "Submitted to Internet of Things (ISSN 2542-6605)", "summary": "Modern Internet of Things (IoT) systems are equipped with a plethora of\nsensors providing real-time data about the current operations of their\ncomponents, which is crucial for the systems' internal control systems and\nprocesses. However, these data are often too fine-grained to derive useful\ninsights into the execution of the larger processes an IoT system might be part\nof. Process mining has developed advanced approaches for the analysis of\nbusiness processes that may also be used in the context of IoT. Bringing\nprocess mining to IoT requires an event abstraction step to lift the low-level\nsensor data to the business process level. In this work, we aim to empower\ndomain experts to perform this step using a newly developed domain-specific\nlanguage (DSL) called Radiant. Radiant supports the specification of patterns\nwithin the sensor data that indicate the execution of higher level process\nactivities. These patterns are translated to complex event processing (CEP)\napplications to be used for detecting activity executions at runtime. We\npropose a corresponding software architecture for online event abstraction from\nIoT sensor streams using the CEP applications. We evaluate these applications\nto monitor activity executions using IoT sensors in smart manufacturing and\nsmart healthcare. The evaluation method and results inform the domain expert\nabout the quality of activity detections and potential for improvement.", "AI": {"tldr": "The paper presents Radiant, a DSL that lets experts transform raw IoT sensor data into meaningful business process events using pattern definitions. This enables real-time monitoring and process mining in IoT systems, demonstrated in manufacturing and healthcare.", "motivation": "Sensor data in IoT systems is often too detailed (fine-grained) to provide insights at the business process level. Process mining can analyze high-level processes, but bridging the gap from raw sensor data requires abstraction.", "method": "The authors developed a domain-specific language (DSL) called Radiant. Radiant allows domain experts to specify patterns in sensor data that correlate with higher-level process activities. These patterns are converted into complex event processing (CEP) applications, which are then executed in a proposed software architecture to abstract and monitor events in real-time. The approach is evaluated in smart manufacturing and healthcare scenarios.", "result": "Radiant successfully enables online abstraction and monitoring of business process activities from IoT sensor data. The evaluation demonstrates that activities can be detected and monitored at runtime, and the results help domain experts assess and improve the quality of detections.", "conclusion": "Introducing Radiant enables domain experts to bridge the gap between low-level IoT sensor data and high-level business process insights, supporting effective process mining in IoT contexts. The method is viable in smart manufacturing and healthcare, and can inform further refinements in detection quality."}}
{"id": "2507.00699", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.00699", "abs": "https://arxiv.org/abs/2507.00699", "authors": ["Guoliang Duan", "Mingwei Liu", "Yanlin Wang", "Chong Wang", "Xin Peng", "Zibin Zheng"], "title": "A Hierarchical and Evolvable Benchmark for Fine-Grained Code Instruction Following with Multi-Turn Feedback", "comment": null, "summary": "Large language models (LLMs) have advanced significantly in code generation,\nyet their ability to follow complex programming instructions with layered and\ndiverse constraints remains underexplored. Existing benchmarks often prioritize\nfunctional correctness, overlooking the nuanced requirements found in\nreal-world development. We introduce MultiCodeIF, a comprehensive benchmark\ndesigned to evaluate instruction-following in code generation across multiple\ndimensions: constraint type, hierarchical levels, and iterative refinement.\nBuilt upon a structured taxonomy of 9 categories and 27 constraint types,\nMultiCodeIF enables granular assessment of both functional and non-functional\ninstruction adherence. Using an automated pipeline, ConstraGen, we synthesize\nand evolve 2,021 code tasks sourced from 14 programming languages, supporting\nmulti-turn evaluation through feedback-driven task variants. Empirical\nevaluation of six state-of-the-art LLMs uncovers substantial performance\ndisparities. The top-performing model, Claude-3-7-Sonnet, achieves 63.0%\naverage constraint satisfaction, while smaller models like Qwen3-1.7B fall to\n44.8%. Models perform well on explicit constraints, but struggle with implicit\nor abstract constraints. Tasks with multiple hierarchical constraints\nsignificantly reduce model success rates, from 54.5% in single-level to just\n18.8% in multi-level scenarios. However, structured feedback enables\nprogressive improvement: average constraint satisfaction rises from 63.0% to\n83.4% over four iterative refinement rounds. MultiCodeIF provides a scalable,\nconstraint-aware, and feedback-sensitive framework to benchmark LLMs under\nrealistic code generation scenarios, bridging the gap between synthetic\nevaluations and real-world instruction complexity. The full benchmark dataset,\nevaluation pipeline, and source code are available at\nhttps://github.com/SYSUSELab/MultiCodeIF.", "AI": {"tldr": "MultiCodeIF is a new, multi-dimensional benchmark for testing how well LLMs follow complex coding instructions involving layered and non-functional constraints. Results reveal large gaps in model performance, especially with multi-level or implicit constraints, but also show that structured feedback can notably boost model performance through iterative refinement. The dataset and tools are publicly released.", "motivation": "While large language models excel at code generation, their ability to follow layered and complex instructions with diverse constraints\u2014especially beyond functional correctness\u2014remains insufficiently studied. Current benchmarks mainly focus on whether generated code works, not on whether all requirements, including subtle or non-functional ones, are properly followed. There is thus a need for a more comprehensive evaluation framework.", "method": "The authors introduce MultiCodeIF, a new benchmark designed to assess instruction-following in code generation under multiple types and levels of constraints. The benchmark leverages a taxonomy organizing constraints into 9 categories and 27 types. Using an automatic pipeline called ConstraGen, they synthesize and evolve over 2,000 code tasks in 14 programming languages, supporting multi-turn evaluation with feedback-driven refinement. Six leading LLMs are evaluated on these tasks regarding their adherence to both functional and non-functional instructions.", "result": "Results show that there are significant differences in how well LLMs adhere to instructions, especially under complex constraints. The best model scored 63% on constraint satisfaction, dropping steeply for tasks with hierarchical, multi-level constraints. Models perform better on explicit constraints than implicit or abstract ones. However, the ability to iteratively refine outputs via feedback significantly improves LLM performance, raising satisfaction rates up to 83.4% after several rounds.", "conclusion": "MultiCodeIF sets a new standard for evaluating LLM code generation under realistic, constraint-rich instructions, identifying major gaps in current model capabilities (especially with complex or abstract requirements). It also demonstrates the importance of feedback-driven refinement for improving adherence. The benchmark and tools are publicly available for broader use."}}
{"id": "2507.00786", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.00786", "abs": "https://arxiv.org/abs/2507.00786", "authors": ["Jukka Ruohonen", "Qusai Ramadan"], "title": "Snaps: Bloated and Outdated?", "comment": "Submitted as a \"poster paper\" to APSEC", "summary": "Snap is an alternative software packaging system developed by Canonical and\nprovided by default in the Ubuntu Linux distribution. Given the heterogeneity\nof various Linux distributions and their various releases, Snap allows an\ninteroperable delivery of software directly to users. However, concerns and\ncriticism have also been frequently expressed. Regarding this criticism, the\npaper shows that currently distributed snap packages are indeed on average\nbloated in terms of their sizes and outdated in terms updating frequencies.\nWith these empirical observations, this short paper contributes to the research\ndomain of software packaging, software packages, and package managers.", "AI": {"tldr": "Snap packages, while designed for cross-distribution compatibility, tend to be larger and less frequently updated than ideal, raising concerns about efficiency and freshness in software delivery.", "motivation": "Snap was developed to handle software packaging challenges across diverse Linux distributions, aiming for more interoperable software delivery. However, there have been ongoing concerns and criticisms regarding the effectiveness of Snap.", "method": "The paper conducts empirical analyses on currently distributed Snap packages, specifically evaluating their size (bloat) and update frequencies (timeliness).", "result": "The analysis found that Snap packages are on average larger (bloated) and less frequently updated (outdated) than expected.", "conclusion": "The findings highlight current weaknesses in Snap's implementation\u2014specifically package bloat and slow update cycles, contributing empirical evidence to the ongoing debates in the software packaging community."}}
{"id": "2507.00788", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00788", "abs": "https://arxiv.org/abs/2507.00788", "authors": ["Markus Borg", "Dave Hewett", "Nadim Hagatulah", "Noric Couderc", "Emma S\u00f6derberg", "Donald Graham", "Uttam Kini", "Dave Farley"], "title": "Echoes of AI: Investigating the Downstream Effects of AI Assistants on Software Maintainability", "comment": "Preprint of study preregistered at ICSME 2025 with In-Principal\n  Acceptance.\n  https://conf.researchr.org/track/icsme-2024/icsme-2024-registered-reports-track", "summary": "[Context] AI assistants, like GitHub Copilot and Cursor, are transforming\nsoftware engineering. While several studies highlight productivity\nimprovements, their impact on maintainability requires further investigation.\n[Objective] This study investigates whether co-development with AI assistants\naffects software maintainability, specifically how easily other developers can\nevolve the resulting source code. [Method] We conducted a two-phase controlled\nexperiment involving 151 participants, 95% of whom were professional\ndevelopers. In Phase 1, participants added a new feature to a Java web\napplication, with or without AI assistance. In Phase 2, a randomized controlled\ntrial, new participants evolved these solutions without AI assistance.\n[Results] AI-assisted development in Phase 1 led to a modest speedup in\nsubsequent evolution and slightly higher average CodeHealth. Although neither\ndifference was significant overall, the increase in CodeHealth was\nstatistically significant when habitual AI users completed Phase 1. For Phase\n1, we also observed a significant effect that corroborates previous\nproductivity findings: using an AI assistant yielded a 30.7% median decrease in\ntask completion time. Moreover, for habitual AI users, the mean speedup was\n55.9%. [Conclusions] Our study adds to the growing evidence that AI assistants\ncan effectively accelerate development. Moreover, we did not observe warning\nsigns of degraded code-level maintainability. We recommend that future research\nfocus on risks such as code bloat from excessive code generation and the\nbuild-up of cognitive debt as developers invest less mental effort during\nimplementation.", "AI": {"tldr": "Using AI assistants like GitHub Copilot can make professional developers faster without making code harder to maintain, especially for those who regularly use such tools. However, further study is needed on long-term maintainability issues like code bloat and cognitive debt.", "motivation": "AI assistants like GitHub Copilot are known to improve productivity in software engineering, but their impact on code maintainability\u2014specifically, how easily others can modify and evolve the code\u2014has not been fully explored. This paper addresses that gap.", "method": "The researchers conducted a two-phase controlled experiment involving 151 mostly professional developers. In Phase 1, participants developed new features for a Java web app, with or without AI assistance. In Phase 2, different participants attempted to evolve the previously written solutions, all without AI assistance.", "result": "AI-assisted development led to a modest speedup in subsequent code evolution and slightly increased CodeHealth for the resulting codebase, particularly for habitual AI users. However, these improvements were not statistically significant overall, except the CodeHealth increase for habitual users. Notably, AI assistance provided a significant productivity boost, especially for users accustomed to AI assistants.", "conclusion": "AI assistants can accelerate software development without introducing observable negative effects on code maintainability. There are potential risks, such as code bloat and cognitive debt, which future research should investigate."}}
{"id": "2507.00803", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.00803", "abs": "https://arxiv.org/abs/2507.00803", "authors": ["Gillian Daniel", "Chris Hall", "Per Hammer", "Alec-Angus Macdonald", "Hollie Marwick-Best", "Emma McKenzie", "George Popa", "Derek Somerville", "Tim Storer"], "title": "Out of the Day Job: Perspectives of Industry Practitioners in Co-Design and Delivery of Software Engineering Courses", "comment": null, "summary": "Over more than two decades, The University of Glasgow has co-designed and\ndelivered numerous software engineering focused courses with industry partners,\ncovering both technical and discipline specific professional skills. Such\ncollaborations are not unique and many of the benefits are well recognised in\nthe literature. These include enhancing the real-world relevance of curricula,\ndeveloping student professional networks ahead of graduation and easing\nrecruitment opportunities for employers.\n  However, there is relatively little scholarship on the perspectives of\nindustry practitioners who participate in course design and delivery. This gap\nis significant, since the effort invested by practitioners is often substantial\nand may require ongoing support from both the industry partner and academic\ninstitution. Understanding the motivations, expectations and experiences of\npractitioners who engage in course delivery can guide the formation of future\npartnerships and ensure their long-term sustainability.\n  We begin to address this gap by reporting on the outcomes of a retrospective\nconducted amongst the practitioner coauthors of this paper, with the academic\ncoauthors acting as facilitators. All coauthors have participated in the recent\nco-design and delivery of software engineering courses, but we choose to focus\nexplicitly on the perspectives of the practitioners. We report on the themes\nthat emerged from the discussions and our resulting recommendations for future\ncollaborations.", "AI": {"tldr": "This paper analyzes the perspectives of industry practitioners involved in co-designing and delivering software engineering courses with the University of Glasgow, revealing their motivations and offering recommendations to sustain such industry-academic partnerships.", "motivation": "Although industry-academic collaborations in software engineering education are common and recognized as beneficial, there is limited research on the perspectives of industry practitioners who participate in co-design and delivery of courses. This paper seeks to understand their motivations, expectations, and experiences, as their substantial investment needs to be supported for sustainable partnerships.", "method": "The study conducts a retrospective analysis among the practitioner coauthors, facilitated by academic coauthors. All coauthors have experience in co-design and delivery of software engineering courses at the University of Glasgow, but the analysis focuses on thematic discussions from the practitioners' viewpoint.", "result": "The paper identifies key themes that emerged from the practitioners' discussions, outlining their experiences, motivations, and challenges in course co-design and delivery. The study culminates in practical recommendations to guide future collaborations and support the sustainability of industry-academic partnerships.", "conclusion": "The perspectives of industry practitioners are under-explored but crucial for maintaining effective and sustainable collaborations in software engineering education. By capturing these viewpoints, the paper provides insights and actionable recommendations that can improve future course co-design efforts."}}
