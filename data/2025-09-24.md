<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 7]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [CoRaCMG: Contextual Retrieval-Augmented Framework for Commit Message Generation](https://arxiv.org/abs/2509.18337)
*Bo Xiong,Linghao Zhang,Chong Wang,Peng Liang*

Main category: cs.SE

TL;DR: CoRaCMG boosts LLM-generated commit messages by retrieving and incorporating similar example pairs, leading to substantial quality improvements across several metrics, with diminishing returns after three examples.


<details>
  <summary>Details</summary>
Motivation: Commit messages are essential for documenting code changes but often lack quality, making them less useful. Improving automated commit message generation can reduce developer effort and enhance clarity.

Method: The paper proposes CoRaCMG, a framework that augments LLMs with context by retrieving similar diff-message pairs and feeding them as structured prompts to the model. The process involves retrieval of examples, prompt augmentation, and message generation using LLMs.

Result: CoRaCMG significantly improves LLM performance on CMG across four benchmark metrics (BLEU, Rouge-L, METEOR, CIDEr), with relative improvements up to 89%. Gains plateau after three retrieved examples, suggesting diminishing returns. Improvements are traced to the model's better use of project-specific terminology and writing style.

Conclusion: Incorporating retrieved diff-message pairs into LLM prompts consistently enhances the generation of precise and informative commit messages, helping models better learn from human-written examples.

Abstract: Commit messages play a key role in documenting the intent behind code
changes. However, they are often low-quality, vague, or incomplete, limiting
their usefulness. Commit Message Generation (CMG) aims to automatically
generate descriptive commit messages from code diffs to reduce developers'
effort and improve message quality. Although recent advances in LLMs have shown
promise in automating CMG, their performance remains limited. This paper aims
to enhance CMG performance by retrieving similar diff-message pairs to guide
LLMs to generate commit messages that are more precise and informative. We
proposed CoRaCMG, a Contextual Retrieval-augmented framework for Commit Message
Generation, structured in three phases: (1) Retrieve: retrieving the similar
diff-message pairs; (2) Augment: combining them with the query diff into a
structured prompt; and (3) Generate: generating commit messages corresponding
to the query diff via LLMs. CoRaCMG enables LLMs to learn project-specific
terminologies and writing styles from the retrieved diff-message pairs, thereby
producing high-quality commit messages. We evaluated our method on various
LLMs, including closed-source GPT models and open-source DeepSeek models.
Experimental results show that CoRaCMG significantly boosts LLM performance
across four metrics (BLEU, Rouge-L, METEOR, and CIDEr). Specifically,
DeepSeek-R1 achieves relative improvements of 76% in BLEU and 71% in CIDEr when
augmented with a single retrieved example pair. After incorporating the single
example pair, GPT-4o achieves the highest improvement rate, with BLEU
increasing by 89%. Moreover, performance gains plateau after more than three
examples are used, indicating diminishing returns. Further analysis shows that
the improvements are attributed to the model's ability to capture the
terminologies and writing styles of human-written commit messages from the
retrieved example pairs.

</details>


### [2] [Reading Between the Lines: Scalable User Feedback via Implicit Sentiment in Developer Prompts](https://arxiv.org/abs/2509.18361)
*Daye Nam,Malgorzata Salawa,Satish Chandra*

Main category: cs.SE

TL;DR: The paper shows that analyzing the sentiment in developer prompts using automated sentiment analysis can capture significantly more implicit feedback about satisfaction with conversational AI assistants than explicit methods, offering a scalable approach to improve understanding of developer experience.


<details>
  <summary>Details</summary>
Motivation: User studies give deep insights into developer satisfaction but are not scalable, while large-scale quantitative measures (like logs or product ratings) are often shallow or unreliable. There is a need for scalable, reliable signals of developer satisfaction with conversational AI assistants.

Method: The authors propose leveraging sentiment analysis on developer prompts as an implicit measure of user satisfaction. They applied sentiment analysis to industrial usage logs from 372 professional developers and assessed the utility and accuracy of this signal relative to explicit feedback.

Result: Sentiment analysis detected satisfaction signals in about 8% of all interactions—over 13 times more than explicit user feedback—with reasonable accuracy, even when using an off-the-shelf sentiment analysis method.

Conclusion: Sentiment analysis of developer prompts can serve as a scalable, practical complement to explicit feedback mechanisms, enabling more comprehensive and reliable insights into developer satisfaction with conversational AI assistants.

Abstract: Evaluating developer satisfaction with conversational AI assistants at scale
is critical but challenging. User studies provide rich insights, but are
unscalable, while large-scale quantitative signals from logs or in-product
ratings are often too shallow or sparse to be reliable. To address this gap, we
propose and evaluate a new approach: using sentiment analysis of developer
prompts to identify implicit signals of user satisfaction. With an analysis of
industrial usage logs of 372 professional developers, we show that this
approach can identify a signal in ~8% of all interactions, a rate more than 13
times higher than explicit user feedback, with reasonable accuracy even with an
off-the-shelf sentiment analysis approach. This new practical approach to
complement existing feedback channels would open up new directions for building
a more comprehensive understanding of the developer experience at scale.

</details>


### [3] [SC2Tools: StarCraft II Toolset and Dataset API](https://arxiv.org/abs/2509.18454)
*Andrzej Białecki,Piotr Białecki,Piotr Sowiński,Mateusz Budziak,Jan Gajewski*

Main category: cs.SE

TL;DR: SC2Tools is a modular toolkit that eases the creation and handling of large esports datasets, specifically in StarCraft 2, and provides user-friendly APIs to support gaming research, making it more accessible and standardized for the scientific community.


<details>
  <summary>Details</summary>
Motivation: Gaming and esports research increasingly relies on RL, AI, and ML, but faces challenges in data collection, preprocessing, and technical implementation. There is a need for streamlined tooling to support researchers and enable the creation of large datasets.

Method: The authors introduce SC2Tools, a modular toolset with submodules designed for dataset creation and management. The toolkit supports both StarCraft 2 and other data types, and offers APIs in PyTorch and PyTorch Lightning for easy data access.

Result: SC2Tools was instrumental in producing one of the largest StarCraft 2 tournament datasets. The modular structure allows future extension, and the provided APIs simplify data utilization for research.

Conclusion: SC2Tools reduces the technical and developmental burden on researchers, making esports and gaming studies more accessible. It helps standardize experiment workflows in StarCraft 2 and potentially other domains.

Abstract: Computer games, as fully controlled simulated environments, have been
utilized in significant scientific studies demonstrating the application of
Reinforcement Learning (RL). Gaming and esports are key areas influenced by the
application of Artificial Intelligence (AI) and Machine Learning (ML) solutions
at scale. Tooling simplifies scientific workloads and is essential for
developing the gaming and esports research area.
  In this work, we present ``SC2Tools'', a toolset containing multiple
submodules responsible for working with, and producing larger datasets. We
provide a modular structure of the implemented tooling, leaving room for future
extensions where needed. Additionally, some of the tools are not StarCraft~2
exclusive and can be used with other types of data for dataset creation.
  The tools we present were leveraged in creating one of the largest
StarCraft~2 tournament datasets to date with a separate PyTorch and PyTorch
Lightning application programming interface (API) for easy access to the data.
  We conclude that alleviating the burden of data collection, preprocessing,
and custom code development is essential for less technically proficient
researchers to engage in the growing gaming and esports research area. Finally,
our solution provides some foundational work toward normalizing experiment
workflow in StarCraft~2

</details>


### [4] [Locking Down Science Gateways](https://arxiv.org/abs/2509.18548)
*Steven R Brandt,Max Morris,Patrick Diehl,Christopher Bowen,Jacob Tucker,Lauren Bristol,Golden G. Richard III*

Main category: cs.SE

TL;DR: Landlock in modern Linux kernels enables applications to shed resource access during execution. The authors adapted several scientific codes to use Landlock, successfully building a more secure science gateway that relies on Landlock for protection, instead of traditional authentication.


<details>
  <summary>Details</summary>
Motivation: Science Gateway applications require network access during startup but need to minimize security risks by dropping unnecessary privileges before processing user input. Existing security mechanisms like Seccomp inspired new features like Landlock.

Method: The authors evaluated the practicality of Landlock by integrating it into three scientific codes—The Einstein Toolkit, Octo-Tiger, and FUKA—then developed a science gateway for FUKA that uses Landlock for security instead of traditional user authentication.

Result: Landlock could effectively restrict resource access after startup, thereby enhancing security for scientific applications running as Science Gateways. A functional FUKA science gateway was implemented utilizing Landlock.

Conclusion: Landlock is a useful new Linux kernel feature for restricting application privileges at runtime, providing an alternative or complement to existing application security models in scientific gateways.

Abstract: The most recent Linux kernels have a new feature for securing applications:
Landlock. Like Seccomp before it, Landlock makes it possible for a running
process to give up access to resources. For applications running as Science
Gateways, network access is required while starting up MPI, but for the sake of
security, it should be taken away prior to the reading of user-supplied
parameter files. We explore the usefulness of Landlock by modifying and locking
down three mature scientific codes: The Einstein Toolkit (a code that studies
the dynamics of relativistic astrophysics, e.g. neutron star collisions),
Octo-Tiger (a code for studying the dynamics of non-relativistic astrophysics,
e.g. white dwarfs), and FUKA (an initial data solver for relativistic codes).
Finally, we implement a fully-functioning FUKA science gateway that relies on
Landlock (instead of user authentication) for security.

</details>


### [5] [SR-Eval: Evaluating LLMs on Code Generation under Stepwise Requirement Refinement](https://arxiv.org/abs/2509.18808)
*Zexun Zhan,Shuzheng Gao,Ruida Hu,Cuiyun Gao*

Main category: cs.SE

TL;DR: SR-Eval is a new benchmark that tests LLMs on multi-turn, evolving code generation tasks, closely mirroring real-world software development. State-of-the-art LLMs perform poorly on these tasks, revealing a gap between current benchmarks and practical requirements, and indicating that both model development and prompting require significant improvement.


<details>
  <summary>Details</summary>
Motivation: Existing code generation benchmarks focus on single-turn, static tasks, not reflecting the iterative and evolving nature of real-world software development. This limits our understanding of how well large language models (LLMs) can support actual development workflows with changing requirements.

Method: The authors introduce SR-Eval, a new benchmark for evaluating LLMs in iterative code generation scenarios with stepwise requirements refinement. SR-Eval includes both function-level and repository-level tasks in Python and Java and utilizes a multi-agent-based approach to simulate requirement evolution, along with a semantic-aware, discriminative test case generator for consistent evaluation at each step.

Result: SR-Eval contains 443 multi-turn tasks and 1,857 questions. When tested on 11 LLMs using three different prompting strategies, the best model achieved only 22.67% success on function-level and 20.00% on repository-level tasks, demonstrating that iterative code generation with evolving requirements is still a significant challenge for current LLMs. Prompting strategy also had a noticeable impact on performance.

Conclusion: The study shows that current LLMs struggle with iterative code generation tasks reflecting real development processes. SR-Eval offers a comprehensive and challenging benchmark for this scenario, and highlights the urgent need for improved LLM methods and better prompting strategies to support real-world, multi-turn software development workflows.

Abstract: Large language models (LLMs) have achieved remarkable progress in code
generation. However, existing benchmarks mainly formalize the task as a static,
single-turn problem, overlooking the stepwise requirement changes and iterative
workflows in real-world software development. This mismatch limits the
understanding of how well LLMs can support real-world development workflows.
Constructing such iterative benchmarks is challenging due to the lack of public
interaction traces and the difficulty of creating discriminative, turn-specific
test cases.
  To bridge this gap, we present SR-Eval, a benchmark specifically designed to
assess LLMs on iterative code generation under Stepwise requirements
Refinement. SR-Eval spans both function-level and repository-level tasks in
Python and Java, enabling fine-grained and progressive evaluation across
evolving requirements. The construction of SR-Eval follows a carefully designed
pipeline that first leverages a multi-agent-based requirement generation method
to simulate the development process and recover the multi-round interaction
process from final requirements, then employs a semantic-aware discriminative
test case generation component to ensure discriminative and consistent
evaluation at each turn. SR-Eval comprises 443 multi-turn tasks and 1,857
questions at both function and repository levels. Using SR-Eval, we evaluate 11
representative LLMs with three prompting strategies that simulate different
usage patterns. Results show that iterative code generation under stepwise
requirement refinement remains highly challenging: the best-performing model
achieves only 22.67% completion rate on function-level tasks and 20.00% on
repository-level tasks. We further observe that prompting strategies
substantially influence performance, highlighting the need for the development
of advanced methods.

</details>


### [6] [On the Soundness and Consistency of LLM Agents for Executing Test Cases Written in Natural Language](https://arxiv.org/abs/2509.19136)
*Sébastien Salva,Redha Taguelmimt*

Main category: cs.SE

TL;DR: This paper shows that while using LLMs to execute natural language GUI test cases is promising, ambiguity and inconsistency still pose challenges. With guardrails and specialized agents, top-performing LLMs (like Meta Llama 3.1 70B) can reliably run such tests, but general adoption requires refinement. Tools and experimental data are released.


<details>
  <summary>Details</summary>
Motivation: Manually written executable test scripts for GUI applications are costly and hard to maintain. Using natural language (NL) test cases, executed directly by large language model (LLM) agents, could reduce these burdens, but NL test cases can be ambiguous and unreliable. Therefore, it is crucial to investigate their soundness and consistency.

Method: The authors propose an algorithm with guardrail mechanisms and specialized agents for verifying each step of NL test case execution. They introduce specific measures to assess LLMs' execution capabilities and consistency, and define 'weak unsoundness' in the context of industrial quality standards (Six Sigma). Experimental evaluation is performed using eight LLMs (3B to 70B parameters) on publicly available test suites.

Result: The experiments reveal that current LLM agents have both promise and limitations for executing NL test cases in GUI testing. Notably, Meta Llama 3.1 70B achieves high consistency in test execution (above 3-sigma level), suggesting it is suitable for practical use. Prototype tools and result datasets are provided.

Conclusion: Direct execution of NL test cases with LLM agents is feasible, provided soundness and consistency are adequately addressed. The proposed approach with guardrails and step-by-step validation improves reliability, and some modern LLMs surpass necessary industrial consistency benchmarks. However, further improvements are needed for broader applicability.

Abstract: The use of natural language (NL) test cases for validating graphical user
interface (GUI) applications is emerging as a promising direction to manually
written executable test scripts, which are costly to develop and difficult to
maintain. Recent advances in large language models (LLMs) have opened the
possibility of the direct execution of NL test cases by LLM agents. This paper
investigates this direction, focusing on the impact on NL test case unsoundness
and on test case execution consistency. NL test cases are inherently unsound,
as they may yield false failures due to ambiguous instructions or unpredictable
agent behaviour. Furthermore, repeated executions of the same NL test case may
lead to inconsistent outcomes, undermining test reliability. To address these
challenges, we propose an algorithm for executing NL test cases with guardrail
mechanisms and specialised agents that dynamically verify the correct execution
of each test step. We introduce measures to evaluate the capabilities of LLMs
in test execution and one measure to quantify execution consistency. We propose
a definition of weak unsoundness to characterise contexts in which NL test case
execution remains acceptable, with respect to the industrial quality levels Six
Sigma. Our experimental evaluation with eight publicly available LLMs, ranging
from 3B to 70B parameters, demonstrates both the potential and current
limitations of current LLM agents for GUI testing. Our experiments show that
Meta Llama 3.1 70B demonstrates acceptable capabilities in NL test case
execution with high execution consistency (above the level 3-sigma). We provide
prototype tools, test suites, and results.

</details>


### [7] [An Empirical Study of Testing Practices in Open Source AI Agent Frameworks and Agentic Applications](https://arxiv.org/abs/2509.19185)
*Mohammed Mehedi Hasan,Hao Li,Emad Fallahzadeh,Gopi Krishnan Rajbahadur,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: Empirical study of AI agent testing reveals developers favor traditional methods and neglect FM-specific/prompt testing. Most testing targets deterministic parts, leaving prompts as a blind spot. The paper urges better adoption of modern testing techniques to improve agent reliability.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the rapid adoption of foundation model (FM)-based AI agents and highlights the challenge that their non-determinism and non-reproducibility present for testing and quality assurance. There is a lack of understanding about how developers ensure the internal correctness of these agents during development.

Method: The authors conducted a large-scale empirical study, analyzing 39 open-source agent frameworks and 439 agentic applications. They identified and classified different testing patterns, mapped these to the architectural components of agent frameworks/applications, and analyzed the distribution of testing efforts.

Result: The study identified ten testing patterns, found that agent-specific methods like DeepEval are rarely used (~1%), whereas traditional testing patterns are widely adapted. It was discovered that deterministic components consume over 70% of testing effort, while FM-based components receive much less attention. Most notably, prompts (Trigger component) are tested in only ~1% of cases, indicating a critical testing blind spot.

Conclusion: Current testing practices in FM-based agent frameworks are rational but incomplete, with significant neglect of FM-specific and prompt-testing methods. The authors recommend that framework and application developers, as well as researchers, should enhance support for novel testing methods and prompt regression testing to improve robustness of AI agents.

Abstract: Foundation model (FM)-based AI agents are rapidly gaining adoption across
diverse domains, but their inherent non-determinism and non-reproducibility
pose testing and quality assurance challenges. While recent benchmarks provide
task-level evaluations, there is limited understanding of how developers verify
the internal correctness of these agents during development.
  To address this gap, we conduct the first large-scale empirical study of
testing practices in the AI agent ecosystem, analyzing 39 open-source agent
frameworks and 439 agentic applications. We identify ten distinct testing
patterns and find that novel, agent-specific methods like DeepEval are seldom
used (around 1%), while traditional patterns like negative and membership
testing are widely adapted to manage FM uncertainty. By mapping these patterns
to canonical architectural components of agent frameworks and agentic
applications, we uncover a fundamental inversion of testing effort:
deterministic components like Resource Artifacts (tools) and Coordination
Artifacts (workflows) consume over 70% of testing effort, while the FM-based
Plan Body receives less than 5%. Crucially, this reveals a critical blind spot,
as the Trigger component (prompts) remains neglected, appearing in around 1% of
all tests.
  Our findings offer the first empirical testing baseline in FM-based agent
frameworks and agentic applications, revealing a rational but incomplete
adaptation to non-determinism. To address it, framework developers should
improve support for novel testing methods, application developers must adopt
prompt regression testing, and researchers should explore barriers to adoption.
Strengthening these practices is vital for building more robust and dependable
AI agents.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [8] [A Verified Compiler for Quantum Simulation](https://arxiv.org/abs/2509.18583)
*Liyi Li,Fenfen An,Federico Zahariev,Zhi Xiang Chong,Amr Sabry,Mark Gordon*

Main category: cs.PL

TL;DR: QBlue is a high-level, fully verified compiler framework for Hamiltonian simulations using second quantization. It ensures safety, expressiveness, and correctness in quantum programming—surpassing the limitations of Pauli-based compilers—and is the first to achieve end-to-end formal verification for these simulations.


<details>
  <summary>Details</summary>
Motivation: Current Hamiltonian simulation compilers are based on low-level Pauli operator representations, which restrict programmability and lack formal correctness guarantees. There is a need for higher-level, more expressive, and formally verified compilation frameworks.

Method: The authors introduce QBlue, a high-level framework for compiling Hamiltonian simulations. It leverages the second quantization formalism for describing quantum systems, employs a type system for safety and correctness, supports compilation to both digital and analog quantum circuits, and is fully mechanized and verified within the Rocq proof framework.

Result: QBlue provides formal guarantees of correctness across the compilation pipeline, supports expressive modeling using second quantization, enforces structural and type constraints, and is the first end-to-end verified compiler for Hamiltonian simulation using this formalism.

Conclusion: QBlue advances the field of quantum simulation compilation by offering a high-level, formally verified, and expressive approach that bridges programmability with correctness, filling a major gap in current quantum compiler technology.

Abstract: Hamiltonian simulation is a central application of quantum computing, with
significant potential in modeling physical systems and solving complex
optimization problems. Existing compilers for such simulations typically focus
on low-level representations based on Pauli operators, limiting programmability
and offering no formal guarantees of correctness across the compilation
pipeline. We introduce QBlue, a high-level, formally verified framework for
compiling Hamiltonian simulations. QBlue is based on the formalism of second
quantization, which provides a natural and expressive way to describe quantum
particle systems using creation and annihilation operators. To ensure safety
and correctness, QBlue includes a type system that tracks particle types and
enforces Hermitian structure. The framework supports compilation to both
digital and analog quantum circuits and captures multiple layers of semantics,
from static constraints to dynamic evolution. All components of QBlue,
including its language design, type system, and compilation correctness, are
fully mechanized in the Rocq proof framework, making it the first end-to-end
verified compiler for second-quantized Hamiltonian simulation.

</details>
