{"id": "2507.22069", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22069", "abs": "https://arxiv.org/abs/2507.22069", "authors": ["Tobias Sesterhenn", "Ian Berlot-Attwell", "Janis Zenkner", "Christian Bartelt"], "title": "A Compute-Matched Re-Evaluation of TroVE on MATH", "comment": null, "summary": "Reusing established theorems and formulas is central to mathematical problem\nsolving, serving as essential building blocks for tackling increasingly complex\nchallenges. Recent work, TroVE, argues that code-generating Large Language\nModels (LLMs) can benefit similarly on the MATH benchmark by inducing and\nreusing higher-level toolboxes. By allocating computational budget across an\nensemble of three modes -- directly generating code, creating tools, and\nreusing tools -- TroVE claims to outperform a PRIMITIVE baseline that only\nperforms direct generation. However, recent analysis (Berlot-Attwell et al.,\n2024) casts doubt on these gains, noting that the tools created are often\ntrivial or rarely reused, suggesting that improvements may stem from\nself-consistency or self-correction. In this work, we re-evaluate TroVE on\nMATH, analyze the impact of each of its modes, and show that its benefit does\nnot come from these mechanisms, but simply from a higher computational budget\nspent for TroVE compared to PRIMITIVE. To this end, we also perform a small\ncorrection in the original implementation of TroVE's selection mechanism,\nboosting TroVE's performance on MATH by 3\\% in accuracy. After matching for\ncompute, the benefit of TroVE reduces to a marginal improvement of 1\\%,\nsuggesting that this toolbox approach does not provide a significant benefit on\nMATH.", "AI": {"tldr": "The paper finds that TroVE's toolbox-based approach does not meaningfully improve math problem solving by code-generating LLMs on the MATH benchmark\u2014its marginal gains come mainly from using more computational resources, not from genuinely effective tool reuse.", "motivation": "Mathematical problem-solving often relies on reusing previously established theorems and formulas. TroVE, a recent approach, claims that Large Language Models (LLMs) can similarly improve performance on the MATH benchmark by leveraging reusable toolboxes rather than just direct problem-solving. There is skepticism regarding whether the gains are from tool reuse or other factors like increased computation.", "method": "The authors conduct a re-evaluation of the TroVE approach on the MATH benchmark, analyzing the contributions of its three modes (direct code generation, tool creation, tool reuse). They correct a minor flaw in TroVE's selection mechanism and perform comparisons between TroVE and a direct-code-generation (PRIMITIVE) baseline, controlling for computational budget.", "result": "The authors find that most of TroVE's claimed improvements originate from a higher computational budget relative to the baseline, not from actual tool reuse or toolbox mechanisms. With computational budget controlled, TroVE's measured benefit drops to a marginal 1% improvement. A minor correction to the tool selection slightly increases TroVE's performance by 3%.", "conclusion": "TroVE's toolbox mechanism does not provide a substantial benefit for mathematical problem-solving on the MATH benchmark beyond what can be explained by a greater computation budget."}}
{"id": "2507.22065", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.22065", "abs": "https://arxiv.org/abs/2507.22065", "authors": ["Xiaotao Feng", "Xiaogang Zhu", "Kun Hu", "Jincheng Wang", "Yingjie Cao", "Guang Gong", "Jianfeng Pan"], "title": "Fuzzing: Randomness? Reasoning! Efficient Directed Fuzzing via Large Language Models", "comment": null, "summary": "Fuzzing is highly effective in detecting bugs due to the key contribution of\nrandomness. However, randomness significantly reduces the efficiency of\nfuzzing, causing it to cost days or weeks to expose bugs. Even though directed\nfuzzing reduces randomness by guiding fuzzing towards target buggy locations,\nthe dilemma of randomness still challenges directed fuzzers. Two critical\ncomponents, which are seeds and mutators, contain randomness and are closely\ntied to the conditions required for triggering bugs. Therefore, to address the\nchallenge of randomness, we propose to use large language models (LLMs) to\nremove the randomness in seeds and reduce the randomness in mutators. With\ntheir strong reasoning and code generation capabilities, LLMs can be used to\ngenerate reachable seeds that target pre-determined locations and to construct\nbug-specific mutators tailored for specific bugs. We propose RandLuzz, which\nintegrates LLMs and directed fuzzing, to improve the quality of seeds and\nmutators, resulting in efficient bug exposure. RandLuzz analyzes function call\nchain or functionality to guide LLMs in generating reachable seeds. To\nconstruct bug-specific mutators, RandLuzz uses LLMs to perform bug analysis,\nobtaining information such as bug causes and mutation suggestions, which\nfurther help generate code that performs bug-specific mutations. We evaluate\nRandLuzz by comparing it with four state-of-the-art directed fuzzers, AFLGo,\nBeacon, WindRanger, and SelectFuzz. With RandLuzz-generated seeds, the fuzzers\nachieve an average speedup ranging from 2.1$\\times$ to 4.8$\\times$ compared to\nusing widely-used initial seeds. Additionally, when evaluated on individual\nbugs, RandLuzz achieves up to a 2.7$\\times$ speedup compared to the\nsecond-fastest exposure. On 8 bugs, RandLuzz can even expose them within 60\nseconds.", "AI": {"tldr": "RandLuzz uses LLMs to create more targeted seeds and mutators for fuzzing, greatly accelerating bug discovery (up to 4.8\u00d7 faster) compared to existing tools by removing much of the randomness that slows down traditional fuzzing.", "motivation": "Fuzzing is effective at exposing software bugs due to its use of randomness, but this same randomness leads to inefficiency and slow bug discovery. Even directed fuzzers still struggle with the negative effects of randomness, particularly in the choice of input seeds and mutator strategies, which are closely related to whether bugs are triggered.", "method": "The paper proposes RandLuzz, a system that leverages large language models (LLMs) to reduce randomness in fuzzing. RandLuzz uses LLMs to generate high-quality, targeted seeds by analyzing function call chains or program functionality, and to construct bug-specific mutators by analyzing bugs and making mutation suggestions. This integration aims to create seeds and mutators that are more likely to expose bugs quickly.", "result": "When evaluated against four state-of-the-art directed fuzzers (AFLGo, Beacon, WindRanger, SelectFuzz), RandLuzz significantly improved efficiency, with fuzzers using RandLuzz-generated seeds achieving 2.1\u00d7 to 4.8\u00d7 speedup over traditional seeds. On specific bugs, RandLuzz achieved up to 2.7\u00d7 faster exposure than the second-best tool, and it was able to expose 8 bugs within 60 seconds.", "conclusion": "Integrating LLMs with directed fuzzing as done in RandLuzz substantially improves the efficiency of bug exposure by reducing randomness in seed and mutator selection. This demonstrates the potential of LLMs to guide fuzzing in a targeted and highly efficient manner."}}
{"id": "2507.22070", "categories": ["cs.SE", "cs.CE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.22070", "abs": "https://arxiv.org/abs/2507.22070", "authors": ["Y. Du"], "title": "Automated Test Data Generation for Enterprise Protobuf Systems: A Metaclass-Enhanced Statistical Approach", "comment": "7 pages", "summary": "Large-scale enterprise systems utilizing Protocol Buffers (protobuf) present\nsignificant challenges for performance testing, particularly when targeting\nintermediate business interfaces with complex nested data structures.\nTraditional test data generation approaches are inadequate for handling the\nintricate hierarchical and graph-like structures inherent in enterprise\nprotobuf schemas. This paper presents a novel test data generation framework\nthat leverages Python's metaclass system for dynamic type enhancement and\nstatistical analysis of production logs for realistic value domain extraction.\nOur approach combines automatic schema introspection, statistical value\ndistribution analysis, and recursive descent algorithms for handling deeply\nnested structures. Experimental evaluation on three real-world enterprise\nsystems demonstrates up to 95\\% reduction in test data preparation time and\n80\\% improvement in test coverage compared to existing approaches. The\nframework successfully handles protobuf structures with up to 15 levels of\nnesting and generates comprehensive test suites containing over 100,000 test\ncases within seconds.", "AI": {"tldr": "This paper proposes a Python-based framework combining dynamic typing and statistical analysis to automate test data generation for complex protobuf structures in enterprise systems, achieving significant gains in speed and coverage over existing methods.", "motivation": "Performance testing of large-scale enterprise systems using Protocol Buffers (protobuf) is challenging, especially with complex, deeply nested data structures found in business interfaces. Traditional test data generation methods are not effective for these intricate schemas.", "method": "The paper introduces a novel framework that uses Python's metaclass system for dynamic type manipulation, analyzes production logs to extract realistic value domains, employs automatic schema introspection, performs statistical value distribution analysis, and uses recursive descent algorithms to manage deeply nested protobuf structures.", "result": "Experiments on three real-world enterprise systems showed that the new framework reduced test data preparation time by up to 95% and improved test coverage by 80% compared to traditional approaches. It effectively handled protobuf schemas with up to 15 levels of nesting and could generate over 100,000 test cases in seconds.", "conclusion": "The proposed framework dramatically improves both the efficiency and effectiveness of test data generation for complex enterprise protobuf systems, making it a practical solution for challenging performance testing needs."}}
{"id": "2507.22086", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.22086", "abs": "https://arxiv.org/abs/2507.22086", "authors": ["Honghua Dong", "Jiacheng Yang", "Xun Deng", "Yuhe Jiang", "Gennady Pekhimenko", "Fan Long", "Xujie Si"], "title": "TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories", "comment": null, "summary": "Type inference for dynamic languages like Python is a persistent challenge in\nsoftware engineering. While large language models (LLMs) have shown promise in\ncode understanding, their type inference capabilities remain underexplored. We\nintroduce TypyBench, a benchmark designed to evaluate LLMs' type inference\nacross entire Python repositories. TypyBench features two novel metrics:\nTypeSim, which captures nuanced semantic relationships between predicted and\nground truth types, and TypeCheck, which assesses type consistency across\ncodebases. Our evaluation of various LLMs on a curated dataset of 50\nhigh-quality Python repositories reveals that, although LLMs achieve decent\nTypeSim scores, they struggle with complex nested types and exhibit significant\ntype consistency errors. These findings suggest that future research should\nshift focus from improving type similarity to addressing repository-level\nconsistency. TypyBench provides a foundation for this new direction, offering\ninsights into model performance across different type complexities and usage\ncontexts. Our code and data are available at\nhttps://github.com/typybench/typybench.", "AI": {"tldr": "TypyBench is a new benchmark for evaluating LLMs' type inference across whole Python repositories. It shows that while LLMs are okay at predicting similar types, they often fail at being consistent with types project-wide, especially with complex or nested types. Improving repository-level type consistency should be a key focus going forward.", "motivation": "Type inference in dynamic languages, particularly Python, is difficult, and while large language models show promise in code understanding, their abilities at type inference (especially at the repository scale) are not well evaluated.", "method": "The authors introduce TypyBench, a specialized benchmark to evaluate LLMs' type inference abilities on full Python repositories. It includes two metrics: TypeSim (measuring semantic similarity of predicted and ground truth types) and TypeCheck (assessing type consistency at repository level). They apply TypyBench to various LLMs using a dataset of 50 high-quality Python repos.", "result": "LLMs perform fairly well on TypeSim (showing decent similarity between predicted and actual types) but struggle with complex/nested types and with maintaining type consistency across entire repositories, leading to significant errors.", "conclusion": "Current LLMs are limited in repository-level type consistency, even if they achieve reasonable type similarity on simpler cases. Future research should focus more on ensuring type consistency across projects rather than only improving type similarity. TypyBench provides an effective way to evaluate and encourage this shift."}}
{"id": "2507.22063", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22063", "abs": "https://arxiv.org/abs/2507.22063", "authors": ["Wenjie Jacky Mo", "Qin Liu", "Xiaofei Wen", "Dongwon Jung", "Hadi Askari", "Wenxuan Zhou", "Zhe Zhao", "Muhao Chen"], "title": "RedCoder: Automated Multi-Turn Red Teaming for Code LLMs", "comment": null, "summary": "Large Language Models (LLMs) for code generation (i.e., Code LLMs) have\ndemonstrated impressive capabilities in AI-assisted software development and\ntesting. However, recent studies have shown that these models are prone to\ngenerating vulnerable or even malicious code under adversarial settings.\nExisting red-teaming approaches rely on extensive human effort, limiting their\nscalability and practicality, and generally overlook the interactive nature of\nreal-world AI-assisted programming, which often unfolds over multiple turns. To\nbridge these gaps, we present RedCoder, a red-teaming agent that engages victim\nmodels in multi-turn conversation to elicit vulnerable code. The pipeline to\nconstruct RedCoder begins with a multi-agent gaming process that simulates\nadversarial interactions, yielding a set of prototype conversations and an\narsenal of reusable attack strategies. We then fine-tune an LLM on these\nprototype conversations to serve as the backbone of RedCoder. Once deployed,\nRedCoder autonomously engages Code LLMs in multi-turn conversations,\ndynamically retrieving relevant strategies from the arsenal to steer the\ndialogue toward vulnerability-inducing outputs. Experiments across multiple\nCode LLMs show that our approach outperforms prior single-turn and multi-turn\nred-team methods in inducing vulnerabilities in code generation, offering a\nscalable and effective tool for evaluating the security boundaries of modern\ncode-generation systems.", "AI": {"tldr": "RedCoder is an automated, multi-turn red-teaming agent that effectively uncovers vulnerabilities in code LLMs, surpassing prior methods in scalability and vulnerability induction through dynamic, realistic adversarial dialogue.", "motivation": "Existing red-teaming approaches for code LLMs require significant human effort and typically focus on single-turn interactions, neglecting the multi-turn, interactive nature of real-world AI-assisted programming. This limits their scalability and effectiveness for identifying vulnerabilities in code generated by LLMs.", "method": "The proposed method, RedCoder, is a red-teaming agent designed to autonomously engage code LLMs in multi-turn conversations to elicit vulnerable code. RedCoder is built by first simulating adversarial, multi-agent interactions to produce prototype conversations and reusable attack strategies, then fine-tuning an LLM on these prototypes. During deployment, RedCoder dynamically applies relevant attack strategies in conversations to induce vulnerabilities.", "result": "Experiments show that RedCoder outperforms existing single-turn and multi-turn red-team methods in generating vulnerable code from several Code LLMs, demonstrating superior effectiveness and scalability.", "conclusion": "RedCoder represents a scalable, effective tool for robustly evaluating and probing the security boundaries of code-generation LLMs through realistic, multi-turn adversarial interactions."}}
{"id": "2507.22064", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22064", "abs": "https://arxiv.org/abs/2507.22064", "authors": ["Michael Cohoon", "Debbie Furman"], "title": "Machine Learning Experiences: A story of learning AI for use in enterprise software testing that can be used by anyone", "comment": null, "summary": "This paper details the machine learning (ML) journey of a group of people\nfocused on software testing. It tells the story of how this group progressed\nthrough a ML workflow (similar to the CRISP-DM process). This workflow consists\nof the following steps and can be used by anyone applying ML techniques to a\nproject: gather the data; clean the data; perform feature engineering on the\ndata; splitting the data into two sets, one for training and one for testing;\nchoosing a machine learning model; training the model; testing the model and\nevaluating the model performance. By following this workflow, anyone can\neffectively apply ML to any project that they are doing.", "AI": {"tldr": "This paper shows how following a structured, CRISP-DM-like workflow makes it easier for anyone to apply machine learning to software testing projects successfully.", "motivation": "The paper aims to guide people interested in leveraging machine learning for software testing by demonstrating a practical workflow.", "method": "A step-by-step process, mirroring the CRISP-DM methodology, is used to detail ML project execution: data gathering, cleaning, feature engineering, train-test split, model selection, training, testing, and evaluation.", "result": "By using this systematic workflow, practitioners can successfully implement ML solutions in any project.", "conclusion": "A well-defined, repeatable workflow enables effective use of ML techniques in software testing and potentially other domains."}}
{"id": "2507.22066", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.22066", "abs": "https://arxiv.org/abs/2507.22066", "authors": ["Dylan Manuel", "Paul Rad"], "title": "CodableLLM: Automating Decompiled and Source Code Mapping for LLM Dataset Generation", "comment": null, "summary": "The generation of large, high-quality datasets for code understanding and\ngeneration remains a significant challenge, particularly when aligning\ndecompiled binaries with their original source code. To address this, we\npresent CodableLLM, a Python framework designed to automate the creation and\ncuration of datasets by mapping decompiled functions to their corresponding\nsource functions. This process enhances the alignment between decompiled and\nsource code representations, facilitating the development of large language\nmodels (LLMs) capable of understanding and generating code across multiple\nabstraction levels. CodableLLM supports multiple programming languages and\nintegrates with existing decompilers and parsers to streamline dataset\ngeneration. This paper presents the design and implementation of CodableLLM,\nevaluates its performance in dataset creation, and compares it to existing\ntools in the field. The results demonstrate that CodableLLM offers a robust and\nefficient solution for generating datasets tailored for code-focused LLMS.", "AI": {"tldr": "CodableLLM automates the alignment of decompiled and source code, making high-quality dataset generation for code language models easier and more effective than existing tools.", "motivation": "Generating large, high-quality datasets for code understanding and generation is challenging, especially when trying to accurately align decompiled binaries to their original source code, which is crucial for training effective code-focused language models.", "method": "The authors introduce CodableLLM, a Python framework that automates the mapping of decompiled functions to their source counterparts. The framework supports various programming languages, integrates with standard decompilers and parsers, and streamlines the data creation process for code LLMs.", "result": "CodableLLM is shown to perform well in automating dataset creation for code-language models, providing better alignment between decompiled and source code compared to existing tools.", "conclusion": "CodableLLM offers a robust and efficient solution for generating high-quality, well-aligned datasets, supporting the development of advanced code-focused LLMs."}}
{"id": "2507.22071", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.22071", "abs": "https://arxiv.org/abs/2507.22071", "authors": ["Niels Glodny"], "title": "Analyzing and Evaluating the Behavior of Git Diff and Merge", "comment": "Bachelor's thesis", "summary": "Despite being widely used, the algorithms that enable collaboration with Git\nare not well understood. The diff and merge algorithms are particularly\ninteresting, as they could be applied in other contexts. In this thesis, I\ndocument the main functionalities of Git: how diffs are computed, how they are\nused to run merges, and how merges enable more complex operations. In the\nprocess, I show multiple unexpected behaviors in Git, including the following:\nThe histogram diff algorithm has pathological cases where a single-line change\ncan cause the entire rest of the file to be marked as changed. The default\nmerge strategy (ort) can result in merges requiring exponential time in the\nnumber of commits in the history. Merges and rebases are not commutative, and\neven when merges do not result in a conflict, the result is not specified but\ndepends on the diff algorithm used. And finally, sometimes when two sides of a\nmerge add different lines at the same position, the result is not a conflict,\nbut a merge containing both changes after each other, in arbitrary order.", "AI": {"tldr": "The paper thoroughly investigates Git's diff and merge algorithms, revealing several unexpected behaviors, such as excessive changes flagged by histogram diff, expensive merge computations, non-commutative operations, and unpredictable merge results. These findings emphasize the complexity and sometimes surprising outcomes when using Git for collaboration.", "motivation": "Git's diff and merge algorithms are widely used but not thoroughly understood, and their behaviors can occasionally be counterintuitive or problematic. By investigating these algorithms, the paper aims to expose their complexities and weaknesses, which is valuable for developers and researchers who may want to use or improve upon them in other contexts.", "method": "The paper documents and analyzes the main functionalities of Git, particularly how diffs are computed and used in merges. It details observed behaviors through examples and possibly experimental investigation, focusing on revealing unexpected or pathological scenarios.", "result": "The study identifies several non-obvious behaviors: the histogram diff algorithm can flag extensive changes due to single-line edits; Git's default merge strategy (ort) can be computationally expensive in certain histories; merges and rebases in Git are not commutative; and some merge results depend on the diff algorithm. Additionally, some merge conflicts do not trigger warnings but merge both changes in no guaranteed order.", "conclusion": "Git's diff and merge algorithms exhibit unexpected behaviors and limitations that are not commonly documented or understood. Such insights are crucial for users relying on these tools and for developers aiming to design better collaborative systems or improve Git itself."}}
{"id": "2507.22080", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22080", "abs": "https://arxiv.org/abs/2507.22080", "authors": ["Qiushi Sun", "Jinyang Gong", "Lei Li", "Qipeng Guo", "Fei Yuan"], "title": "CodeEvo: Interaction-Driven Synthesis of Code-centric Data through Hybrid and Iterative Feedback", "comment": "Work in progress", "summary": "Acquiring high-quality instruction-code pairs is essential for training Large\nLanguage Models (LLMs) for code generation. Manually curated data is expensive\nand inherently limited in scale, motivating the development of code-centric\nsynthesis methods. Yet, current approaches either focus on augmenting existing\ncode or rely on predefined heuristics, both lacking rigorous data validation,\nwhich results in synthetic data that is ungrounded, repetitive, or overly\nsimplistic. Inspired by collaborative programming practices, we propose\nCodeEvo, a framework that synthesizes code data through iterative interactions\nbetween two LLM agents: a Coder, which generates candidate code and test cases\nbased on given instructions, and a Reviewer, which guides the synthesis process\nby producing new instructions and feedback. We further introduce a hybrid\nfeedback mechanism that combines compiler determinism with the generative\nflexibility of agents, enabling automatic quality control throughout synthesis.\nExtensive experiments demonstrate that models fine-tuned on CodeEvo data\nsignificantly outperform established baselines across code generation\nbenchmarks with various difficulties. In-depth analyses further provide\ninsights from multiple perspectives into effective code-centric data synthesis.", "AI": {"tldr": "CodeEvo uses two interacting LLM agents and hybrid feedback to generate superior code data, resulting in better code generation performance than existing methods.", "motivation": "High-quality instruction-code pair data is vital for training LLMs for code generation, but manual curation is expensive and difficult to scale. Existing synthetic data methods rely on augmentation or heuristics and suffer from poor validation, resulting in low-quality and ungrounded data.", "method": "The paper introduces CodeEvo, a novel framework where two LLM agents (a Coder and a Reviewer) interact iteratively. The Coder generates code and test cases from instructions, while the Reviewer provides new instructions and feedback. The process incorporates a hybrid feedback system that merges compiler checks with LLM-based feedback for automated quality control.", "result": "Models trained with data synthesized through CodeEvo outperform those trained on existing baselines across a range of code generation tasks. Detailed analyses show improved diversity, grounding, and effectiveness in synthesized datasets.", "conclusion": "CodeEvo offers a scalable, automated framework for producing high-quality instruction-code pairs by leveraging agent interaction and a hybrid feedback mechanism. This results in superior performance and better-quality training data for code generation LLMs."}}
{"id": "2507.22085", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.22085", "abs": "https://arxiv.org/abs/2507.22085", "authors": ["Vaani Goenka", "Aalok D. Thakkar"], "title": "BOOP: Write Right Code", "comment": null, "summary": "Novice programmers frequently adopt a syntax-specific and test-case-driven\napproach, writing code first and adjusting until programs compile and test\ncases pass, rather than developing correct solutions through systematic\nreasoning. AI coding tools exacerbate this challenge by providing syntactically\ncorrect but conceptually flawed solutions. In this paper, we introduce BOOP\n(Blueprint, Operations, OCaml, Proof), a structured framework requiring four\nmandatory phases: formal specification, language-agnostic algorithm\ndevelopment, implementation, and correctness proof. This shifts focus from\n``making code work'' to understanding why code is correct.\n  BOOP was implemented at our institution using a VS Code extension and\npreprocessor that enforces constraints and identifies counterproductive\npatterns. Initial evaluation shows improved algorithmic reasoning and reduced\ntrial-and-error debugging. Students reported better edge case understanding and\nproblem decomposition, though some initially found the format verbose.\nInstructors observed stronger foundational skills compared to traditional\napproaches.", "AI": {"tldr": "BOOP is a structured programming framework that improves students' algorithmic reasoning and code correctness by enforcing a four-phase process, showing better results than traditional trial-and-error methods.", "motivation": "Novice programmers often focus on making code work via repeated trial-and-error rather than developing correct solutions through structured reasoning. The use of AI coding tools can worsen this by providing code that is syntactically correct but conceptually incorrect.", "method": "The paper introduces BOOP, a four-phase structured programming framework: (1) formal specification, (2) language-agnostic algorithm development, (3) implementation, and (4) correctness proof. The framework was implemented via a VS Code extension and preprocessor, which enforce the phases and monitor coding patterns. Effectiveness was evaluated at the authors' institution.", "result": "Initial results demonstrate that students using BOOP show improved algorithmic reasoning, understand edge cases better, and are less reliant on trial-and-error debugging. Instructors noticed stronger foundational skills, although some students found the framework initially verbose.", "conclusion": "The BOOP framework successfully shifts the focus from making code simply work to understanding code correctness, enhancing foundational skills in programming education, despite some initial resistance to the framework's verbosity."}}
{"id": "2507.22223", "categories": ["cs.SE", "D.2; D.2.4; D.4.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.22223", "abs": "https://arxiv.org/abs/2507.22223", "authors": ["Kiana Kiashemshaki", "Mohammad Jalili Torkamani", "Negin Mahmoudi"], "title": "Secure coding for web applications: Frameworks, challenges, and the role of LLMs", "comment": "11 pages, 5 figures, 3 tables, 6 listings", "summary": "Secure coding is a critical yet often overlooked practice in software\ndevelopment. Despite extensive awareness efforts, real-world adoption remains\ninconsistent due to organizational, educational, and technical barriers. This\npaper provides a comprehensive review of secure coding practices across major\nframeworks and domains, including web development, DevSecOps, and cloud\nsecurity. It introduces a structured framework comparison and categorizes\nthreats aligned with the OWASP Top 10. Additionally, we explore the rising role\nof Large Language Models (LLMs) in evaluating and recommending secure code,\npresenting a reproducible case study across four major vulnerability types.\nThis paper offers practical insights for researchers, developers, and educators\non integrating secure coding into real-world development processes.", "AI": {"tldr": "This paper reviews secure coding practices across domains, categorizes threats, compares frameworks, and shows how LLMs can evaluate secure code\u2014offering actionable insights for improving real-world secure development.", "motivation": "Despite awareness of secure coding, real-world uptake is poor due to organizational, educational, and technical challenges. There's a need for practical guidance and new tools to boost adoption and effectiveness.", "method": "A comprehensive review and framework comparison were conducted, integrating threat classification (aligned with OWASP Top 10) and a case study using LLMs to evaluate secure code across major vulnerabilities.", "result": "The study provides a detailed categorization of secure coding practices across frameworks and domains, offers a comparison framework, and demonstrates, through a case study, how LLMs can help in analyzing and recommending secure code.", "conclusion": "The paper concludes that a structured evaluation of secure coding practices, framework comparison, and threat categorization can help bridge existing adoption gaps. It emphasizes LLMs\u2019 emerging potential in advancing secure coding."}}
{"id": "2507.22324", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22324", "abs": "https://arxiv.org/abs/2507.22324", "authors": ["Cameron S. Movassaghi", "Amanda Momenzadeh", "Jesse G. Meyer"], "title": "From Articles to Code: On-Demand Generation of Core Algorithms from Scientific Publications", "comment": null, "summary": "Maintaining software packages imposes significant costs due to dependency\nmanagement, bug fixes, and versioning. We show that rich method descriptions in\nscientific publications can serve as standalone specifications for modern large\nlanguage models (LLMs), enabling on-demand code generation that could supplant\nhuman-maintained libraries. We benchmark state-of-the-art models\n(GPT-o4-mini-high, Gemini Pro 2.5, Claude Sonnet 4) by tasking them with\nimplementing a diverse set of core algorithms drawn from original publications.\nOur results demonstrate that current LLMs can reliably reproduce package\nfunctionality with performance indistinguishable from conventional libraries.\nThese findings foreshadow a paradigm shift toward flexible, on-demand code\ngeneration and away from static, human-maintained packages, which will result\nin reduced maintenance overhead by leveraging published articles as sufficient\ncontext for the automated implementation of analytical workflows.", "AI": {"tldr": "LLMs can now generate high-quality code just from detailed scientific descriptions, matching traditional library performance, which could dramatically cut software maintenance needs.", "motivation": "Software package maintenance is expensive due to complex factors like dependencies, bug fixes, and versioning. The research explores if using scientific publications with thorough method descriptions can reduce this burden by empowering LLMs to generate code on demand.", "method": "The authors benchmarked leading large language models (GPT-o4-mini-high, Gemini Pro 2.5, Claude Sonnet 4) by asking them to implement various core algorithms directly from published method descriptions, and then compared the quality and performance of these generated codes to that of traditional software packages.", "result": "State-of-the-art LLMs were able to accurately reproduce the functionality of established software libraries, achieving similar performance based solely on rich method descriptions from scientific publications.", "conclusion": "Modern LLMs, given high-quality scientific method descriptions, can generate code with reliability comparable to hand-maintained libraries, indicating a shift toward automated, on-demand code creation and reduced reliance on static, manually maintained packages."}}
{"id": "2507.22414", "categories": ["cs.SE", "D.2; I.2"], "pdf": "https://arxiv.org/pdf/2507.22414", "abs": "https://arxiv.org/abs/2507.22414", "authors": ["Sungmin Kang", "Haifeng Ruan", "Abhik Roychoudhury"], "title": "AutoCodeSherpa: Symbolic Explanations in AI Coding Agents", "comment": null, "summary": "Large Language Model (LLM) agents autonomously use external tools on top of\none or more LLMs to accomplish specific tasks. Lately LLM agents for software\nengineering tasks have become popular. These agents can benefit from the use of\nprogram analysis tools working on program representations. This is demonstrated\nby existing agentic AI solutions such as AutoCodeRover or SpecRover which\nperform automated program repair. Specifically the goal of these works is to\nuse program analysis to improve the patch quality. These agents are currently\nbeing used to automatically fix static analysis issues from the widely used\nSonarQube static analyzer.\n  Nevertheless, for the agents to be deployed in a production environment,\nagents need to suggest software artifacts, such as patches, with evidence and\nwith high confidence. In this work, we provide a workflow where an agent\nprovides explanations of the bug in the form of symbolic formulae. The\nexplanations are in the form of input conditions, infection conditions and\noutput conditions, implemented as property based tests (PBT) and\nprogram-internal symbolic expressions. These can help in human developer\ncognition of the agent outputs as well as in achieving completely automated\nagentic workflows for software. The human developer can benefit from the input\ncondition, represented as a PBT, to generate various concrete inputs showing a\ngiven issue. Furthermore, since the PBTs are executable, our explanations are\nexecutable as well. We can thus also use the explanations in a completely\nautomated issue resolution environment for accepting or rejecting the patches\nthat are suggested by patching agents such as AutoCodeRover. Finally, as\nagentic AI approaches continue to develop, the program analysis driven\nexplanations can be provided to other LLM-based repair techniques such as\nAgentless to improve their output.", "AI": {"tldr": "This paper presents a workflow for LLM code repair agents that generates symbolic, executable explanations for software bugs and fixes, improving developer trust and enabling more reliable, automated patch acceptance in production environments.", "motivation": "Large Language Model (LLM) agents have become popular in software engineering for tasks such as automated program repair. However, for deployment in real-world production environments, these agents need to provide trustworthy and explainable software patches. There is a need for approaches that increase developer confidence by providing clear explanations for bug fixes and automating issue resolution more effectively.", "method": "The authors introduce a workflow in which an LLM agent produces explanations about code bugs through symbolic formulae, specifically input, infection, and output conditions. These are implemented using property-based tests (PBT) and symbolic program expressions. The explanations, being executable, serve both human developers and automated systems in assessing the validity of proposed patches.", "result": "The workflow enables LLM agents to generate executable and human-understandable explanations for code fixes. Human developers can use these explanations to test and better understand issues, while automated systems can utilize them for patch evaluation in a fully autonomous pipeline. The approach is also compatible with other agentic AI repair techniques beyond the studied systems, potentially improving overall patch quality.", "conclusion": "The proposed workflow enhances both the transparency and reliability of LLM-driven automated code repair systems. By providing symbolic, executable explanations, agentic workflows can aid human understanding and drive further automation. This benefits both manual and automated acceptance of patches, improving the viability of LLM agents in production environments."}}
{"id": "2507.22442", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.22442", "abs": "https://arxiv.org/abs/2507.22442", "authors": ["Yukai Zhao", "Shaohua Wang", "Jue Wang", "Xing Hu", "Xin Xia"], "title": "Ensemble Fuzzing with Dynamic Resource Scheduling and Multidimensional Seed Evaluation", "comment": "first submit", "summary": "Fuzzing is widely used for detecting bugs and vulnerabilities, with various\ntechniques proposed to enhance its effectiveness. To combine the advantages of\nmultiple technologies, researchers proposed ensemble fuzzing, which integrates\nmultiple base fuzzers. Despite promising results, state-of-the-art ensemble\nfuzzing techniques face limitations in resource scheduling and performance\nevaluation, leading to unnecessary resource waste. In this paper, we propose\nLegion, a novel ensemble fuzzing framework that dynamically schedules resources\nduring the ensemble fuzzing campaign. We designed a novel resource scheduling\nalgorithm based on the upper confidence bound algorithm to reduce the resource\nconsumption of ineffective base fuzzers. Additionally, we introduce a\nmultidimensional seed evaluation strategy, which considers multiple metrics to\nachieve more comprehensive fine-grained performance evaluation. We implemented\nLegion as a prototype tool and evaluated its effectiveness on Google's\nfuzzer-test-suite as well as real-world open-source projects. Results show that\nLegion outperforms existing state-of-the-art base fuzzers and ensemble fuzzing\ntechniques, detecting 20 vulnerabilities in real-world open-source\nprojects-five previously unknown and three classified as CVEs.", "AI": {"tldr": "Legion is a new ensemble fuzzing framework that dynamically manages resources using a novel algorithm and better performance evaluation, outperforming previous tools by finding more vulnerabilities in tests and real-world projects.", "motivation": "Ensemble fuzzing aims to improve bug and vulnerability detection by combining multiple fuzzers. However, existing approaches struggle with effective resource allocation and comprehensive performance evaluation, causing resource inefficiencies.", "method": "The authors present Legion, a new ensemble fuzzing framework. Legion uses a resource scheduling algorithm based on the upper confidence bound to reduce resource waste and introduces a multidimensional seed evaluation strategy for finer performance assessment.", "result": "Legion was implemented and tested on Google's fuzzer-test-suite and real-world open-source projects. It outperformed current state-of-the-art base fuzzers and ensemble fuzzing techniques, discovering 20 vulnerabilities, including five previously unknown and three CVEs.", "conclusion": "Legion provides significant improvements over existing ensemble fuzzing methods by dynamically optimizing resource allocation and refining performance evaluation, resulting in better bug and vulnerability detection."}}
{"id": "2507.22538", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.22538", "abs": "https://arxiv.org/abs/2507.22538", "authors": ["Matilde Gargiani", "Robin Sieber", "Philip Pawlowsky", "John Lygeros"], "title": "Inside madupite: Technical Design and Performance", "comment": null, "summary": "In this work, we introduce and benchmark madupite, a newly proposed\nhigh-performance solver designed for large-scale discounted infinite-horizon\nMarkov decision processes with finite state and action spaces. After a brief\noverview of the class of mathematical optimization methods on which madupite\nrelies, we provide details on implementation choices, technical design and\ndeployment. We then demonstrate its scalability and efficiency by showcasing\nits performance on the solution of Markov decision processes arising from\ndifferent application areas, including epidemiology and classical control.\nMadupite sets a new standard as, to the best of our knowledge, it is the only\nsolver capable of efficiently computing exact solutions for large-scale Markov\ndecision processes, even when these exceed the memory capacity of modern\nlaptops and operate in near-undiscounted settings. This is possible as madupite\ncan work in a fully distributed manner and therefore leverage the memory\nstorage and computation capabilities of modern high-performance computing\nclusters. This key feature enables the solver to efficiently handle problems of\nmedium to large size in an exact manner instead of necessarily resorting to\nfunction approximations. Moreover, madupite is unique in allowing users to\ncustomize the solution algorithm to better exploit the specific structure of\ntheir problem, significantly accelerating convergence especially in\nlarge-discount factor settings. Overall, madupite represents a significant\nadvancement, offering unmatched scalability and flexibility in solving\nlarge-scale Markov decision processes.", "AI": {"tldr": "Madupite is a new, scalable solver for large-scale Markov decision processes that computes exact solutions even on problems too big for standard computers, thanks to distributed computing and customizability. It is more flexible and efficient than previous approaches and marks a major step forward in solving such problems.", "motivation": "There is a need for scalable and efficient solvers that can compute exact solutions to large-scale discounted infinite-horizon Markov decision processes (MDPs), especially when such problems exceed the memory capacity of standard hardware. Current methods often require function approximation or cannot fully exploit problem structure, limiting their effectiveness.", "method": "The paper introduces 'madupite', a high-performance solver for large-scale discounted infinite-horizon MDPs with finite state and action spaces. The authors detail the solver\u2019s mathematical optimization foundation, its technical implementation, and its ability to run in a distributed manner across high-performance computing clusters. Madupite\u2019s algorithm can also be customized based on specific problem structures to improve convergence speed.", "result": "Madupite is demonstrated to be both scalable and efficient across MDPs from diverse fields such as epidemiology and control. It uniquely enables exact solutions even for problems exceeding the memory of typical laptops, leveraging distributed computation. It also accelerates convergence for problems with high discount factors by allowing algorithm customization.", "conclusion": "Madupite marks a new standard for solving large-scale MDPs, providing unmatched scalability and flexibility. It handles medium to large problems exactly, avoids relying on approximations, and works efficiently in near-undiscounted settings, thus representing a significant advancement in the field."}}
{"id": "2507.22580", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22580", "abs": "https://arxiv.org/abs/2507.22580", "authors": ["Marcos Fuster-Pena", "David de-Fitero-Dominguez", "Antonio Garcia-Cabot", "Eva Garcia-Lopez"], "title": "RePaCA: Leveraging Reasoning Large Language Models for Static Automated Patch Correctness Assessment", "comment": null, "summary": "Automated Program Repair (APR) seeks to automatically correct software bugs\nwithout requiring human intervention. However, existing tools tend to generate\npatches that satisfy test cases without fixing the underlying bug, those are\nknown as overfitting patches. To address this issue, Automated Patch\nCorrectness Assessment (APCA) attempts to identify overfitting patches\ngenerated by APR tools. It can be solved as a static approach, meaning that no\nadditional information is needed beyond the original and fixed code snippets.\nCurrent static techniques often struggle with reliability, flexibility and\ntransparency. To address these issues, we introduce RePaCA, a novel static APCA\ntechnique that leverages Large Language Models (LLMs) specialized in thinking\ntasks. Our model is prompted with both buggy and fixed code snippets and guided\nto generate a Chain of Thought that analyses code differences, reasons about\nhow the patch addresses the root cause, and ultimately provides a binary\nclassification: correct or overfitting. To enhance these reasoning capabilities\nfor the APCA task specifically, the LLM is finetuned using Reinforcement\nLearning with the Group Relative Policy Optimization algorithm. When evaluated\non a standard Defects4J-derived test, our approach achieves state-of-the-art\nperformance, with 83.1% accuracy and an 84.8% F1-score. Furthermore, our model\ndemonstrates superior generalization capabilities when trained on different\ndatasets, outperforming the leading technique. This reasoning capability also\nprovides enhanced explainability for the patch assessment. These findings\nunderscore the considerable promise of finetuned, reasoning LLMs to advance\nstatic APCA by enhancing accuracy, generalization, and explainability.", "AI": {"tldr": "This paper presents RePaCA, a novel method using finetuned reasoning LLMs for more accurate, generalizable, and explainable automated assessment of program repair patch correctness. It achieves state-of-the-art results on benchmarks.", "motivation": "Automated Program Repair (APR) tools often produce overfitting patches that only satisfy test cases without fixing the real bug. Current static methods to automatically assess the correctness of these patches (APCA) suffer from poor reliability, flexibility, and transparency.", "method": "The authors introduce RePaCA, a new static APCA technique that uses specialized Large Language Models (LLMs) prompted with buggy and fixed code snippets. The model generates a Chain of Thought reasoning trace and classifies patches as correct or overfitting. The LLM is finetuned for this specific task using Reinforcement Learning with the Group Relative Policy Optimization algorithm.", "result": "RePaCA achieves state-of-the-art results on a standard APCA benchmark (Defects4J-derived), reaching 83.1% accuracy and an 84.8% F1-score. The model also generalizes better than previous techniques when trained on different datasets, and provides more explainable patch assessments.", "conclusion": "Finetuned reasoning LLMs like RePaCA significantly improve the accuracy, generalization, and explainability of static APCA, showcasing the potential of advanced LLMs for automated patch correctness assessment."}}
{"id": "2507.22610", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22610", "abs": "https://arxiv.org/abs/2507.22610", "authors": ["Ali Asgari", "Milan de Koning", "Pouria Derakhshanfar", "Annibale Panichella"], "title": "Metamorphic Testing of Deep Code Models: A Systematic Literature Review", "comment": null, "summary": "Large language models and deep learning models designed for code intelligence\nhave revolutionized the software engineering field due to their ability to\nperform various code-related tasks. These models can process source code and\nsoftware artifacts with high accuracy in tasks such as code completion, defect\ndetection, and code summarization; therefore, they can potentially become an\nintegral part of modern software engineering practices. Despite these\ncapabilities, robustness remains a critical quality attribute for deep-code\nmodels as they may produce different results under varied and adversarial\nconditions (e.g., variable renaming). Metamorphic testing has become a widely\nused approach to evaluate models' robustness by applying semantic-preserving\ntransformations to input programs and analyzing the stability of model outputs.\nWhile prior research has explored testing deep learning models, this systematic\nliterature review focuses specifically on metamorphic testing for deep code\nmodels. By studying 45 primary papers, we analyze the transformations,\ntechniques, and evaluation methods used to assess robustness. Our review\nsummarizes the current landscape, identifying frequently evaluated models,\nprogramming tasks, datasets, target languages, and evaluation metrics, and\nhighlights key challenges and future directions for advancing the field.", "AI": {"tldr": "A literature review of 45 papers reveals how metamorphic testing is used to evaluate the robustness of deep code models, outlining common practices, challenges, and opportunities for improving reliability in software engineering AI.", "motivation": "Large language models for code are highly accurate in code-related tasks but may lack robustness when subjected to adversarial conditions or semantic-preserving changes. Ensuring their consistent performance is crucial for their integration into software engineering.", "method": "A systematic literature review was conducted, focusing on 45 primary papers that applied metamorphic testing to deep code models. The review analyzed the types of semantic-preserving transformations, robustness evaluation techniques, and assessment methods used in these studies.", "result": "The review identified commonly evaluated models, tasks, datasets, programming languages, and metrics. It mapped the state of robustness assessment for deep code models, summarizing the methods, highlighting key trends, and noting challenges that remain in robust evaluation.", "conclusion": "Metamorphic testing is actively used to evaluate and improve the robustness of code intelligence models, but the field faces ongoing challenges. The paper provides a comprehensive summary of current practices and offers guidance for future research directions."}}
