<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 35]
- [cs.PL](#cs.PL) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A benchmark for vericoding: formally verified program synthesis](https://arxiv.org/abs/2509.22908)
*Sergiu Bursuc,Theodore Ehrenborg,Shaowei Lin,Lacramioara Astefanoaei,Ionel Emilian Chiosa,Jure Kukovec,Alok Singh,Oliver Butterley,Adem Bizid,Quinn Dougherty,Miranda Zhao,Max Tan,Max Tegmark*

Main category: cs.SE

TL;DR: A new large benchmark for LLM-generated, formally verified code shows high verification rates in Dafny, moderate in Verus/Rust, and low in Lean. Performance has improved notably for Dafny. Natural language prompts offer little added value.


<details>
  <summary>Details</summary>
Motivation: The aim is to shift from generating potentially buggy code via natural language prompts to generating provably correct code directly from formal specifications using LLMs, assessing the current capability and progress of LLMs in this area.

Method: The authors created and tested a large benchmark consisting of 12,504 formal specifications in Dafny, Verus/Rust, and Lean. They evaluated off-the-shelf LLMs for their ability to generate formally verified code and measured success rates. They also analyzed performance improvements over time and the effects of adding natural-language descriptions.

Result: Success rates for vericoding with LLMs were 82% for Dafny, 44% for Verus/Rust, and 27% for Lean. Adding natural language had negligible effect. LLM performance in Dafny verification has risen from 68% to 96% in the past year. The dataset and results have been made publicly available.

Conclusion: LLMs can generate formally verified code from formal specifications, but success rates vary significantly across languages and benchmarks. Recent progress in LLMs has greatly improved verification rates for Dafny, but natural-language descriptions offer little performance boost.

Abstract: We present and test the largest benchmark for vericoding, LLM-generation of
formally verified code from formal specifications - in contrast to vibe coding,
which generates potentially buggy code from a natural language description. Our
benchmark contains 12,504 formal specifications, with 3,029 in Dafny, 2,334 in
Verus/Rust and 7,141 in Lean. Of these, 6,174 are new unseen problems. We find
vericoding success rates of 27% in Lean, 44% in Verus/Rust and 82% in Dafny
using off-the-shelf LLMs. Adding natural-language descriptions does not
significantly improve performance. We also find that LLM progress has improved
progress on pure Dafny verification from 68% to 96% over the past year. The
benchmark and vericoding results are shared at
https://github.com/Beneficial-AI-Foundation/vericoding-benchmark

</details>


### [2] [Towards Human-interpretable Explanation in Code Clone Detection using LLM-based Post Hoc Explainer](https://arxiv.org/abs/2509.22978)
*Teeradaj Racharak,Chaiyong Ragkhitwetsagul,Chayanee Junplong,Akara Supratak*

Main category: cs.SE

TL;DR: This paper proposes using ChatGPT-4 to explain GraphCodeBERT code clone detection results, achieving high accuracy and usefulness. The study suggests LLMs are effective post hoc explainers and highlights directions for refinement and broader application in software engineering.


<details>
  <summary>Details</summary>
Motivation: ML-based code clone detectors accurately identify and classify code clones, particularly semantic clones, but are often opaque and difficult to interpret. Current post hoc explainers are limited by the need for white-box access or heavy computational demands, leaving a gap for interpretable, practical solutions.

Method: The authors use large language models, specifically ChatGPT-4, to explain the predictions of the GraphCodeBERT code clone detector, leveraging in-context learning to act as a post hoc explainer. They conduct a study to evaluate the effectiveness of this approach in providing understandable and correct explanations.

Result: The approach achieved up to 98% accuracy in providing correct explanations and 95% in delivering good explanations. Setting the LLM temperature to zero further improved explanation accuracy. The usefulness of the explanations and code line examples varied depending on the case.

Conclusion: Integrating large language models as post hoc explainers is promising for interpreting ML-based code clone detectors, addressing challenges of interpretability, and paving the way for their use in other software engineering tasks. Additional insights were identified for future improvement.

Abstract: Recent studies highlight various machine learning (ML)-based techniques for
code clone detection, which can be integrated into developer tools such as
static code analysis. With the advancements brought by ML in code
understanding, ML-based code clone detectors could accurately identify and
classify cloned pairs, especially semantic clones, but often operate as black
boxes, providing little insight into the decision-making process. Post hoc
explainers, on the other hand, aim to interpret and explain the predictions of
these ML models after they are made, offering a way to understand the
underlying mechanisms driving the model's decisions. However, current post hoc
techniques require white-box access to the ML model or are computationally
expensive, indicating a need for advanced post hoc explainers. In this paper,
we propose a novel approach that leverages the in-context learning capabilities
of large language models to elucidate the predictions made by the ML-based code
clone detectors. We perform a study using ChatGPT-4 to explain the code clone
results inferred by GraphCodeBERT. We found that our approach is promising as a
post hoc explainer by giving the correct explanations up to 98% and offering
good explanations 95% of the time. However, the explanations and the code line
examples given by the LLM are useful in some cases. We also found that lowering
the temperature to zero helps increase the accuracy of the explanation. Lastly,
we list the insights that can lead to further improvements in future work. This
study paves the way for future studies in using LLMs as a post hoc explainer
for various software engineering tasks.

</details>


### [3] [Agentic Specification Generator for Move Programs](https://arxiv.org/abs/2509.24515)
*Yu-Fu Fu,Meng Xu,Taesoo Kim*

Main category: cs.SE

TL;DR: MSG is a new LLM-based tool for generating specifications for Move smart contracts. It shows that LLMs work well with less-common languages, achieves high coverage, creates more useful clauses than existing approaches, and its design and feedback mechanisms significantly boost specification quality.


<details>
  <summary>Details</summary>
Motivation: While LLM-based specification generation is becoming popular, such tools mainly target widely used programming languages, neglecting emerging, verification-focused languages like Move used in smart contracts. Addressing this gap could extend these tools’ benefits to new ecosystems.

Method: The authors introduce MSG, an automated tool that applies LLMs for specification generation specifically for the Move programming language. MSG leverages an agentic, modular architecture, incorporates explicit use of specification language features, and integrates feedback from the verification toolchain.

Result: MSG generated verifiable specifications for 84% of tested Move functions, exceeded expert-written clauses in some cases, produced 57% more verifiable clauses through its modular design, and achieved a 30% increase in verifiable specifications by using verification toolchain feedback.

Conclusion: LLMs can be successfully leveraged for specification generation in non-mainstream, verification-oriented languages like Move. MSG’s agentic, modular approach and feedback incorporation not only attains strong coverage but also enhances the quality and usefulness of generated specifications.

Abstract: While LLM-based specification generation is gaining traction, existing tools
primarily focus on mainstream programming languages like C, Java, and even
Solidity, leaving emerging and yet verification-oriented languages like Move
underexplored. In this paper, we introduce MSG, an automated specification
generation tool designed for Move smart contracts. MSG aims to highlight key
insights that uniquely present when applying LLM-based specification generation
to a new ecosystem. Specifically, MSG demonstrates that LLMs exhibit robust
code comprehension and generation capabilities even for non-mainstream
languages. MSG successfully generates verifiable specifications for 84% of
tested Move functions and even identifies clauses previously overlooked by
experts. Additionally, MSG shows that explicitly leveraging specification
language features through an agentic, modular design improves specification
quality substantially (generating 57% more verifiable clauses than conventional
designs). Incorporating feedback from the verification toolchain further
enhances the effectiveness of MSG, leading to a 30% increase in generated
verifiable specifications.

</details>


### [4] [The Matthew Effect of AI Programming Assistants: A Hidden Bias in Software Evolution](https://arxiv.org/abs/2509.23261)
*Fei Gu,Zi Liang,Hongzong LI,Jiahao MA*

Main category: cs.SE

TL;DR: LLMs in programming tend to succeed more with popular languages and frameworks, risking further entrenchment of dominant tools and reducing ecosystem diversity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the broader impact of AI-assisted programming, specifically by large language models (LLMs), on the iterative dynamics and ecosystem of software engineering, beyond just code generation quality.

Method: The authors conducted large-scale experiments involving thousands of algorithmic programming tasks and hundreds of framework selection tasks to systematically analyze LLM interactions with the software ecosystem.

Result: The results quantitatively show a Matthew effect: LLMs generate code with higher success rates for more popular programming languages and frameworks, potentially reinforcing existing popularity hierarchies.

Conclusion: AI systems powered by LLMs may accelerate convergence around dominant languages and frameworks, which can hinder diversity and innovation in programming ecosystems.

Abstract: AI-assisted programming is rapidly reshaping software development, with large
language models (LLMs) enabling new paradigms such as vibe coding and agentic
coding. While prior works have focused on prompt design and code generation
quality, the broader impact of LLM-driven development on the iterative dynamics
of software engineering remains underexplored. In this paper, we conduct
large-scale experiments on thousands of algorithmic programming tasks and
hundreds of framework selection tasks to systematically investigate how
AI-assisted programming interacts with the software ecosystem. Our analysis
reveals \textbf{a striking Matthew effect: the more popular a programming
language or framework, the higher the success rate of LLM-generated code}. The
phenomenon suggests that AI systems may reinforce existing popularity
hierarchies, accelerating convergence around dominant tools while hindering
diversity and innovation. We provide a quantitative characterization of this
effect and discuss its implications for the future evolution of programming
ecosystems.

</details>


### [5] [Code Arcades: 3d Visualization of Classes, Dependencies and Software Metrics](https://arxiv.org/abs/2509.23297)
*Anthony Savidis,Christos Vasilopoulos*

Main category: cs.SE

TL;DR: This paper presents a novel software visualization approach featuring configurable grouping, multi-level metrics, and interactive viewing, collectively advancing flexible and deep source code understanding.


<details>
  <summary>Details</summary>
Motivation: Software visualization aims to aid in the comprehension, analysis, maintenance, and evolution of source code by representing software artifacts in graphical form, addressing the challenges of understanding large and complex software systems.

Method: The paper introduces three main contributions: (1) a configurable grouping mechanism allowing flexible organization of code elements according to arbitrary relationships; (2) integration of both fine-grained and coarse-grained software metrics to achieve a multi-level perspective; and (3) an interactive visualization engine enabling real-time adjustment of visualization attributes.

Result: The proposed system enables software engineers to identify structural patterns, complexity hotspots, and infer system behaviors, significantly improving the accessibility and depth of source code comprehension.

Conclusion: By providing more adaptable and insightful software visualizations, the authors' approach helps developers better understand large-scale systems, facilitating improvements in analysis, maintenance, and evolution tasks.

Abstract: Software visualization seeks to represent software artifacts graphical-ly in
two or three dimensions, with the goal of enhancing comprehension, anal-ysis,
maintenance, and evolution of the source code. In this context, visualiza-tions
employ graphical forms such as dependency structures, treemaps, or time-lines
that incorporate repository histories. These visualizations allow software
engineers to identify structural patterns, detect complexity hotspots, and
infer system behaviors that are difficult to perceive directly from source
text. By adopting metaphor-based approaches, visualization tools provide
macroscopic overviews while enabling focused inspection of specific program
elements, thus offering an accessible means of understanding large-scale
systems. The contri-bution of our work lies in three areas. First, we introduce
a configurable group-ing mechanism that supports flexible organization of code
elements based on arbitrary relationships. Second, we combine fine-grained and
coarse-grained software metrics to provide a multi-level perspective on system
properties. Third, we present an interactive visualization engine that allows
developers to dynamically adjust rendering attributes. Collectively, these
advances provide a more adaptable and insightful approach to source code
comprehension.

</details>


### [6] [Methods for evaluating software accessibility](https://arxiv.org/abs/2509.23469)
*Mykola Kuz,Ivan Yaremiy,Hanna Yaremii,Mykola Pikuliak,Ihor Lazarovych,Mykola Kozlenko,Denys Vekeryk*

Main category: cs.SE

TL;DR: The paper proposes a new, detailed method for evaluating software accessibility, applies it to a university website, and recommends improvements for visually impaired users, advancing inclusivity beyond standard methods.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the challenge of evaluating software accessibility effectively to ensure equal access for users with diverse needs. Existing methods are often too generalized and do not cater to specific user categories or interaction modes.

Method: The authors propose developing a classification and mathematical model for accessibility. They create a method for assessing the 'Accessibility' subcharacteristic within the broader 'Usability' quality metric, focusing on metrics tailored to users with visual impairments.

Result: A more detailed and practical methodology for software accessibility assessment is developed and applied to analyze the inclusivity of Vasyl Stefanyk Precarpathian National University's website, with specific improvements recommended.

Conclusion: The developed assessment method is more thorough and practical than standardized approaches, enabling more effective evaluation and improvement of web accessibility, especially for visually impaired users.

Abstract: The development and enhancement of methods for evaluating software
accessibility is a relevant challenge in modern software engineering, as
ensuring equal access to digital services is a key factor in improving their
efficiency and inclusivity. The increasing digitalization of society
necessitates the creation of software that complies with international
accessibility standards such as ISO/IEC 25023 and WCAG. Adhering to these
standards helps eliminate barriers to software use for individuals with diverse
physical, sensory, and cognitive needs. Despite advancements in regulatory
frameworks, existing accessibility evaluation methodologies are often
generalized and fail to account for the specific needs of different user
categories or the unique ways they interact with digital systems. This
highlights the need for the development of new, more detailed methods for
defining metrics that influence the quality of user interaction with software
products. Building a classification and mathematical model and developing
accessibility assessment methods for software based on it. A method for
assessing the quality subcharacteristic "Accessibility", which is part of the
"Usability" quality characteristic, has been developed. This enabled the
analysis of a website's inclusivity for individuals with visual impairments,
and the formulation of specific recommendations for further improvements, which
is a crucial step toward creating an inclusive digital environment. Comparing
to standardized approaches, a more detailed and practically oriented
accessibility assessment methodology has been proposed. Using this methodology,
an analysis of the accessibility of the main pages of Vasyl Stefanyk
Precarpathian National University's website was conducted, and improvements
were suggested to enhance its inclusivity.

</details>


### [7] [Improving the Efficiency of LLM Agent Systems through Trajectory Reduction](https://arxiv.org/abs/2509.23586)
*Yuan-An Xiao,Pengfei Gao,Chao Peng,Yingfei Xiong*

Main category: cs.SE

TL;DR: The paper introduces AgentDiet, a method that reduces computational cost for multi-turn LLM agents by automatically removing redundant and expired information from their input trajectories, achieving significant efficiency gains without losing performance.


<details>
  <summary>Details</summary>
Motivation: Multi-turn LLM agent systems are efficient for software engineering but suffer from high computational costs due to large input token trajectories. Previous research has largely overlooked efficiency concerns.

Method: The authors analyzed agent trajectories to identify redundant, useless, and expired information. They developed AgentDiet, an inference-time trajectory reduction technique that automatically prunes wasteful information from agent inputs. AgentDiet was implemented on a top coding agent and tested on two LLMs and two benchmarks.

Result: AgentDiet reduced input tokens by 39.9% to 59.7%, and overall computational cost by 21.1% to 35.9%, without affecting agent performance.

Conclusion: Trajectory reduction, as implemented by AgentDiet, is a promising approach to improving computational efficiency in multi-turn LLM agent systems for software engineering without sacrificing effectiveness.

Abstract: Multi-turn agent systems based on Large Language Models (LLMs) have been
increasingly popular for software engineering tasks. While LLM agents show
decent effectiveness, the high computational cost of input tokens due to the
ever-growing trajectory remains an efficiency concern for their applications.
Efficiency is largely neglected in existing studies and agent products, and
this paper fills the gap by introducing an inference-time trajectory reduction
approach to reduce the cost of agents.
  Through analyzing existing agent trajectories, we demonstrate that useless,
redundant, and expired information is widespread in all trajectories, which can
be identified and reduced without harming the agent's performance. We then
design a simple yet effective trajectory reduction approach, AgentDiet, which
automatically removes such waste information. We implement AgentDiet on a
top-performing coding agent, and the evaluation on two LLMs and two benchmarks
shows that AgentDiet can reduce input tokens by 39.9% ~ 59.7%, or the final
computational cost by 21.1% ~ 35.9%, while maintaining the same agent
performance. This indicates that trajectory reduction is a promising direction
for agent systems.

</details>


### [8] [Similarity-Based Assessment of Computational Reproducibility in Jupyter Notebooks](https://arxiv.org/abs/2509.23645)
*A S M Shahadat Hossain,Colin Brown,David Koop,Tanu Malik*

Main category: cs.SE

TL;DR: This paper introduces the Similarity-based Reproducibility Index (SRI), a metric and tool for quantifying how reproducible Jupyter Notebook outputs are using similarity metrics. SRI delivers detailed scores and insights per output cell, and a case study demonstrates its utility in assessing reproducibility.


<details>
  <summary>Details</summary>
Motivation: Although Jupyter Notebooks are widely used for sharing computational experiments, rerunning them does not always yield the same results due to factors such as randomness or changes in the computational environment, making reproducibility challenging to gauge.

Method: The paper introduces the Similarity-based Reproducibility Index (SRI), which uses novel similarity metrics tailored to different Python object types to quantitatively and qualitatively evaluate the reproducibility of Jupyter Notebook outputs. Each output cell is scored between 0 and 1 based on how similar the rerun result is to the original. The paper also presents a case study applying SRI to multiple notebooks.

Result: SRI provides both quantitative (a score between 0 and 1 for each output cell) and qualitative insights on reproducibility in Jupyter Notebooks. The case study shows that SRI can effectively use different similarity metrics to measure and compare computational reproducibility across notebooks.

Conclusion: The Similarity-based Reproducibility Index is an effective tool for assessing and quantifying the reproducibility of computational experiments in Jupyter Notebooks, helping users understand and improve reproducibility with precise, output-specific feedback.

Abstract: Computational reproducibility refers to obtaining consistent results when
rerunning an experiment. Jupyter Notebook, a web-based computational notebook
application, facilitates running, publishing, and sharing computational
experiments along with their results. However, rerunning a Jupyter Notebook may
not always generate identical results due to various factors, such as
randomness, changes in library versions, or variations in the computational
environment. This paper introduces the Similarity-based Reproducibility Index
(SRI) -- a metric for assessing the reproducibility of results in Jupyter
Notebooks. SRI employs novel methods developed based on similarity metrics
specific to different types of Python objects to compare rerun outputs against
original outputs. For every cell generating an output in a rerun notebook, SRI
reports a quantitative score in the range [0, 1] as well as some qualitative
insights to assess reproducibility. The paper also includes a case study in
which the proposed metric is applied to a set of Jupyter Notebooks,
demonstrating how various similarity metrics can be leveraged to quantify
computational reproducibility.

</details>


### [9] [PAT-Agent: Autoformalization for Model Checking](https://arxiv.org/abs/2509.23675)
*Xinyue Zuo,Yifan Zhang,Hongshu Wang,Yufan Cai,Zhe Hou,Jing Sun,Jin Song Dong*

Main category: cs.SE

TL;DR: PAT-Agent uses LLMs to automatically create and repair formal models from natural language, verifies them, and helps users (including non-experts) build correct models more efficiently than baseline approaches.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) have the potential to automate formal methods. However, using LLMs for formal verification is difficult due to complex specification languages, possible hallucinations, and a gap between natural language and formal logic.

Method: The authors introduce PAT-Agent, an end-to-end framework that leverages LLMs for natural language autoformalization and formal model repair. PAT-Agent uses a Planning LLM to extract modeling elements and generate a plan, guiding a Code Generation LLM to produce formal models. PAT-Agent integrates model checking via Process Analysis Toolkit and includes a Repair Loop that iteratively corrects models using counterexamples, all accessible through a user-friendly web interface.

Result: PAT-Agent outperforms existing baselines across 40 systems, achieving higher verification success and efficiency. Ablation studies confirm that planning and repair are essential components, while user studies show that the system enables non-experts to effectively perform formal modeling and verification.

Conclusion: PAT-Agent successfully automates formal model construction and repair, making formal verification more accessible and efficient, especially for users without expertise in formal methods.

Abstract: Recent advances in large language models (LLMs) offer promising potential for
automating formal methods. However, applying them to formal verification
remains challenging due to the complexity of specification languages, the risk
of hallucinated output, and the semantic gap between natural language and
formal logic. We introduce PAT-Agent, an end-to-end framework for natural
language autoformalization and formal model repair that combines the generative
capabilities of LLMs with the rigor of formal verification to automate the
construction of verifiable formal models. In PAT-Agent, a Planning LLM first
extracts key modeling elements and generates a detailed plan using semantic
prompts, which then guides a Code Generation LLM to synthesize syntactically
correct and semantically faithful formal models. The resulting code is verified
using the Process Analysis Toolkit (PAT) model checker against user-specified
properties, and when discrepancies occur, a Repair Loop is triggered to
iteratively correct the model using counterexamples. To improve flexibility, we
built a web-based interface that enables users, particularly non-FM-experts, to
describe, customize, and verify system behaviors through user-LLM interactions.
Experimental results on 40 systems show that PAT-Agent consistently outperforms
baselines, achieving high verification success with superior efficiency. The
ablation studies confirm the importance of both planning and repair components,
and the user study demonstrates that our interface is accessible and supports
effective formal modeling, even for users with limited formal methods
experience.

</details>


### [10] [Satellite: Detecting and Analyzing Smart Contract Vulnerabilities caused by Subcontract Misuse](https://arxiv.org/abs/2509.23679)
*Zeqin Liao,Yuhong Nan,Zixu Gao,Henglong Liang,Sicheng Hao,Jiajing Wu,Zibin Zheng*

Main category: cs.SE

TL;DR: Satellite is a new static analysis tool for detecting vulnerabilities introduced by subcontract reuse in smart contracts at the bytecode level. It uses transfer learning and method-level feature analysis, exhibiting high precision and recall, and has uncovered new security issues in real-world contracts.


<details>
  <summary>Details</summary>
Motivation: Smart contract developers often reuse subcontracts to boost efficiency, but this practice can unintentionally introduce or spread vulnerabilities. Most smart contracts are distributed in bytecode form, making it hard to analyze issues due to obscured class-level and semantic information after compilation.

Method: The paper proposes Satellite, a bytecode-level static analysis framework designed to detect subcontract misuse vulnerabilities (SMVs) in smart contracts. Satellite leverages transfer learning to recover inherited methods, extracts method-level features, compares them to identify reused parts, and summarizes SMV indicators by type. The framework is evaluated using a dataset of real-world SMVs and patterns.

Result: Satellite demonstrated strong performance, achieving a precision rate of 84.68% and recall rate of 92.11% for SMV detection. It also identified 14 previously unknown SMVs in over 10,000 smart contracts, impacting digital assets amounting to more than $200,000.

Conclusion: Satellite is an effective static analysis tool for detecting subcontract misuse vulnerabilities in smart contracts at the bytecode level, even uncovering previously unknown security issues in real-world deployments.

Abstract: Developers of smart contracts pervasively reuse subcontracts to improve
development efficiency. Like any program language, such subcontract reuse may
unexpectedly include, or introduce vulnerabilities to the end-point smart
contract. Unfortunately, automatically detecting such issues poses several
unique challenges. Particularly, in most cases, smart contracts are compiled as
bytecode, whose class-level information (e.g., inheritance, virtual function
table), and even semantics (e.g., control flow and data flow) are fully
obscured as a single smart contract after compilation.
  In this paper, we propose Satellite, a new bytecode-level static analysis
framework for subcontract misuse vulnerability (SMV) detection in smart
contracts. Satellite incorporates a series of novel designs to enhance its
overall effectiveness.. Particularly, Satellite utilizes a transfer learning
method to recover the inherited methods, which are critical for identifying
subcontract reuse in smart contracts. Further, Satellite extracts a set of
fine-grained method-level features and performs a method-level comparison, for
identifying the reuse part of subcontract in smart contracts. Finally,
Satellite summarizes a set of SMV indicators according to their types, and
hence effectively identifies SMVs. To evaluate Satellite, we construct a
dataset consisting of 58 SMVs derived from real-world attacks and collect
additional 56 SMV patterns from SOTA studies. Experiment results indicate that
Satellite exhibits good performance in identifying SMV, with a precision rate
of 84.68% and a recall rate of 92.11%. In addition, Satellite successfully
identifies 14 new/unknown SMV over 10,011 real-world smart contracts, affecting
a total amount of digital assets worth 201,358 USD.

</details>


### [11] [Influence-Guided Concolic Testing of Transformer Robustness](https://arxiv.org/abs/2509.23806)
*Chih-Duo Hong,Yu Wang,Yao-Chen Chang,Fang Yu*

Main category: cs.SE

TL;DR: This paper introduces SHAP-guided concolic testing tailored for Transformers. By ranking path predicates by their influence, and redesigning attention mechanisms for SMT solvers, they efficiently find adversarial examples and demonstrate scalability to deeper models. The method could help in debugging and auditing neural classifiers.


<details>
  <summary>Details</summary>
Motivation: Concolic testing for neural networks is promising but struggles with deeper architectures like Transformers due to constraint growth and inefficient search for adversarial inputs.

Method: They developed an 'influence-guided' concolic tester for Transformer classifiers that ranks path predicates using SHAP estimates to bias the search. They also designed solver-compatible Python semantics for multi-head self-attention and scheduling heuristics to control constraint growth.

Result: Influence-guided concolic testing finds adversarial inputs (label flips) more efficiently than baseline FIFO search, maintains steady search progress for deeper models, and SHAP-based critical path analysis shows shared decision logic among attack instances.

Conclusion: Influence signals (via SHAP) effectively guide symbolic exploration in concolic testing. Solver-friendly attention semantics and lightweight scheduling make concolic testing viable for real Transformer models, which can aid debugging and auditing. This approach is more efficient and scalable for deep architectures like Transformers.

Abstract: Concolic testing for deep neural networks alternates concrete execution with
constraint solving to search for inputs that flip decisions. We present an
{influence-guided} concolic tester for Transformer classifiers that ranks path
predicates by SHAP-based estimates of their impact on the model output. To
enable SMT solving on modern architectures, we prototype a solver-compatible,
pure-Python semantics for multi-head self-attention and introduce practical
scheduling heuristics that temper constraint growth on deeper models. In a
white-box study on compact Transformers under small $L_0$ budgets, influence
guidance finds label-flip inputs more efficiently than a FIFO baseline and
maintains steady progress on deeper networks. Aggregating successful attack
instances with a SHAP-based critical decision path analysis reveals recurring,
compact decision logic shared across attacks. These observations suggest that
(i) influence signals provide a useful search bias for symbolic exploration,
and (ii) solver-friendly attention semantics paired with lightweight scheduling
make concolic testing feasible for contemporary Transformer models, offering
potential utility for debugging and model auditing.

</details>


### [12] [Navigating the Labyrinth: Path-Sensitive Unit Test Generation with Large Language Models](https://arxiv.org/abs/2509.23812)
*Dianshu Liao,Xin Yin,Shidong Pan,Chao Ni,Zhenchang Xing,Xiaoyu Sun*

Main category: cs.SE

TL;DR: JUnitGenie is a new framework for automating unit test generation in Java. It combines code understanding and LLMs to produce better tests, achieves much higher coverage, and helps find real bugs, outperforming previous approaches.


<details>
  <summary>Details</summary>
Motivation: Unit testing is crucial for ensuring software quality, but manual creation and upkeep are costly and error-prone. Existing automated methods, relying on heuristics or LLMs, do not adequately cover complex control-flow paths in code.

Method: The authors propose JUnitGenie, a path-sensitive framework that combines code knowledge extraction from Java projects with semantic capabilities of large language models. This framework uses structured prompts to guide context-aware, high-coverage unit test generation.

Result: JUnitGenie is evaluated on 2,258 complex Java methods across ten real-world projects. It produces valid tests and increases branch coverage by 29.60% and line coverage by 31.00% compared to heuristic and LLM-only baselines. The tests generated also help identify actual bugs that developers subsequently fix.

Conclusion: JUnitGenie effectively improves automated unit test generation by leveraging both code knowledge and LLM semantics, leading to superior coverage and the discovery of real-world bugs compared to prior methods.

Abstract: Unit testing is essential for software quality assurance, yet writing and
maintaining tests remains time-consuming and error-prone. To address this
challenge, researchers have proposed various techniques for automating unit
test generation, including traditional heuristic-based methods and more recent
approaches that leverage large language models (LLMs). However, these existing
approaches are inherently path-insensitive because they rely on fixed
heuristics or limited contextual information and fail to reason about deep
control-flow structures. As a result, they often struggle to achieve adequate
coverage, particularly for deep or complex execution paths. In this work, we
present a path-sensitive framework, JUnitGenie, to fill this gap by combining
code knowledge with the semantic capabilities of LLMs in guiding context-aware
unit test generation. After extracting code knowledge from Java projects,
JUnitGenie distills this knowledge into structured prompts to guide the
generation of high-coverage unit tests. We evaluate JUnitGenie on 2,258 complex
focal methods from ten real-world Java projects. The results show that
JUnitGenie generates valid tests and improves branch and line coverage by
29.60% and 31.00% on average over both heuristic and LLM-based baselines. We
further demonstrate that the generated test cases can uncover real-world bugs,
which were later confirmed and fixed by developers.

</details>


### [13] [SolContractEval: A Benchmark for Evaluating Contract-Level Solidity Code Generation](https://arxiv.org/abs/2509.23824)
*Zhifan Ye,Jiachi Chen,Zhenzhe Shao,Lingfeng Bao,Xiaohu Yang,Zhongxin Liu*

Main category: cs.SE

TL;DR: The paper introduces SolContractEval, a benchmark for evaluating LLMs on real-world Solidity smart contract generation. Results show LLMs lag behind their performance with general-purpose languages, particularly with complex logic and Solidity-specific features, highlighting the need for further improvement.


<details>
  <summary>Details</summary>
Motivation: The popularity of blockchain has increased the need for effective smart contract generation tools, but current large language models (LLMs) are underexplored in their ability to generate Solidity code, which is critical for smart contracts. Existing evaluations are insufficient as they focus on isolated functions and synthetic tasks, not real-world contract development.

Method: The authors introduce SolContractEval, a contract-level benchmark for Solidity code generation, consisting of 124 tasks from actual on-chain contracts across various domains. Tasks include complete context, structured frameworks, and annotated guidance. They also create an automated evaluation framework based on historical transaction replay for functional correctness. Six mainstream LLMs are systematically assessed with this new benchmark.

Result: Claude-3.7-Sonnet achieved the best overall performance among the tested LLMs, but all evaluated models performed worse on Solidity compared to general-purpose language tasks. Models handled standard patterns better than complex logic and inter-contract dependencies, and demonstrated limited understanding of Solidity-specific features and contextual requirements.

Conclusion: While LLMs show promise for Solidity code generation, they are not yet as effective as they are for general-purpose programming languages, particularly in handling complex smart contract logic and unique Solidity features. SolContractEval provides a foundation for more accurate model evaluations and future advancements.

Abstract: The rise of blockchain has brought smart contracts into mainstream use,
creating a demand for smart contract generation tools. While large language
models (LLMs) excel at generating code in general-purpose languages, their
effectiveness on Solidity, the primary language for smart contracts, remains
underexplored. Solidity constitutes only a small portion of typical LLM
training data and differs from general-purpose languages in its
version-sensitive syntax and limited flexibility. These factors raise concerns
about the reliability of existing LLMs for Solidity code generation.
Critically, existing evaluations, focused on isolated functions and synthetic
inputs, fall short of assessing models' capabilities in real-world contract
development.
  To bridge this gap, we introduce SolContractEval, the first contract-level
benchmark for Solidity code generation. It comprises 124 tasks drawn from real
on-chain contracts across nine major domains. Each task input, consisting of
complete context dependencies, a structured contract framework, and a concise
task prompt, is independently annotated and cross-validated by experienced
developers. To enable precise and automated evaluation of functional
correctness, we also develop a dynamic evaluation framework based on historical
transaction replay. Building on SolContractEval, we perform a systematic
evaluation of six mainstream LLMs. We find that Claude-3.7-Sonnet achieves the
highest overall performance, though evaluated models underperform relative to
their capabilities on class-level generation tasks in general-purpose
programming languages. Second, current models perform better on tasks that
follow standard patterns but struggle with complex logic and inter-contract
dependencies. Finally, they exhibit limited understanding of Solidity-specific
features and contextual dependencies.

</details>


### [14] [HFuzzer: Testing Large Language Models for Package Hallucinations via Phrase-based Fuzzing](https://arxiv.org/abs/2509.23835)
*Yukai Zhao,Menghan Wu,Xing Hu,Xin Xia*

Main category: cs.SE

TL;DR: HFUZZER is a new fuzzing framework for detecting package hallucinations in code-generating LLMs. It finds more hallucinated packages and task diversity than previous methods, showing widespread vulnerabilities in popular models, especially during coding and configuration.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are extensively used for code generation, but they suffer from package hallucinations, where non-existent packages are recommended, posing severe security risks in production. These hallucinations can be exploited for supply chain attacks.

Method: The authors propose HFUZZER, a novel phrase-based fuzzing framework to systematically test LLMs for package hallucinations. HFUZZER uses fuzzing technology driven by phrases extracted from package information or coding tasks, increasing the diversity and relevance of generated code tasks.

Result: HFUZZER triggers package hallucinations in all evaluated LLMs. It outperforms mutational fuzzing frameworks, identifying 2.60x more unique hallucinated packages and producing more diverse tasks. For GPT-4o, it discovers 46 unique hallucinated packages and finds hallucinations occur during both code generation and environment configuration.

Conclusion: HFUZZER effectively tests for package hallucinations, uncovering significant vulnerabilities across mainstream LLMs. Its phrase-based fuzzing approach is more effective than traditional mutational fuzzing, highlighting the need for robust testing and mitigation strategies before LLMs are used in production.

Abstract: Large Language Models (LLMs) are widely used for code generation, but they
face critical security risks when applied to practical production due to
package hallucinations, in which LLMs recommend non-existent packages. These
hallucinations can be exploited in software supply chain attacks, where
malicious attackers exploit them to register harmful packages. It is critical
to test LLMs for package hallucinations to mitigate package hallucinations and
defend against potential attacks. Although researchers have proposed testing
frameworks for fact-conflicting hallucinations in natural language generation,
there is a lack of research on package hallucinations. To fill this gap, we
propose HFUZZER, a novel phrase-based fuzzing framework to test LLMs for
package hallucinations. HFUZZER adopts fuzzing technology and guides the model
to infer a wider range of reasonable information based on phrases, thereby
generating enough and diverse coding tasks. Furthermore, HFUZZER extracts
phrases from package information or coding tasks to ensure the relevance of
phrases and code, thereby improving the relevance of generated tasks and code.
We evaluate HFUZZER on multiple LLMs and find that it triggers package
hallucinations across all selected models. Compared to the mutational fuzzing
framework, HFUZZER identifies 2.60x more unique hallucinated packages and
generates more diverse tasks. Additionally, when testing the model GPT-4o,
HFUZZER finds 46 unique hallucinated packages. Further analysis reveals that
for GPT-4o, LLMs exhibit package hallucinations not only during code generation
but also when assisting with environment configuration.

</details>


### [15] [Learning-Based Testing for Deep Learning: Enhancing Model Robustness with Adversarial Input Prioritization](https://arxiv.org/abs/2509.23961)
*Sheikh Md Mushfiqur Rahman,Nasir Eisty*

Main category: cs.SE

TL;DR: The paper introduces a Learning-Based Testing approach that, augmented with hypothesis and mutation testing, prioritizes adversarial test cases for DNNs. The approach is architecture-agnostic, improves fault detection speed and robustness, and is effective across various datasets and attack types.


<details>
  <summary>Details</summary>
Motivation: Deep Neural Networks (DNNs) are used in critical applications that require robustness against adversarial inputs. Existing test prioritization methods are inefficient at quickly identifying inputs that reveal model faults, which reduces their effectiveness in practical scenarios.

Method: The paper proposes integrating Learning-Based Testing (LBT) with hypothesis and mutation testing to prioritize adversarial test cases. The method selects a subset of adversarial inputs likely to reveal faults, without relying on specific model architectures or formal verification, making it broadly applicable.

Result: The proposed LBT method outperforms baseline approaches by consistently prioritizing fault-revealing inputs and speeding up fault detection. It organizes test cases efficiently, uncovering faults faster across different datasets, models, and adversarial attacks.

Conclusion: The method not only improves fault detection but also preserves input diversity and aids in model retraining, boosting DNN robustness. This makes it a practical and powerful solution for adversarial test prioritization in real-world applications.

Abstract: Context: Deep Neural Networks (DNNs) are increasingly deployed in critical
applications, where resilience against adversarial inputs is paramount.
However, whether coverage-based or confidence-based, existing test
prioritization methods often fail to efficiently identify the most
fault-revealing inputs, limiting their practical effectiveness. Aims: This
project aims to enhance fault detection and model robustness in DNNs by
integrating Learning-Based Testing (LBT) with hypothesis and mutation testing
to efficiently prioritize adversarial test cases. Methods: Our method selects a
subset of adversarial inputs with a high likelihood of exposing model faults,
without relying on architecture-specific characteristics or formal
verification, making it adaptable across diverse DNNs. Results: Our results
demonstrate that the proposed LBT method consistently surpasses baseline
approaches in prioritizing fault-revealing inputs and accelerating fault
detection. By efficiently organizing test permutations, it uncovers all
potential faults significantly faster across various datasets, model
architectures, and adversarial attack techniques. Conclusion: Beyond improving
fault detection, our method preserves input diversity and provides effective
guidance for model retraining, further enhancing robustness. These advantages
establish our approach as a powerful and practical solution for adversarial
test prioritization in real-world DNN applications.

</details>


### [16] [SandCell: Sandboxing Rust Beyond Unsafe Code](https://arxiv.org/abs/2509.24032)
*Jialun Zhang,Merve Gülmez,Thomas Nyman,Gang Tan*

Main category: cs.SE

TL;DR: SandCell is a Rust isolation system that gives programmers flexible control to sandbox code, including both safe and unsafe portions. It uses simple annotations and novel data transfer methods to maintain low overhead and is proven effective at preventing vulnerabilities, offering a practical improvement over existing fixed-boundary Rust isolation tools.


<details>
  <summary>Details</summary>
Motivation: Rust provides memory safety via ownership and borrowing, but 'unsafe' code can introduce vulnerabilities. Existing isolation methods in Rust only support fixed boundaries and cannot flexibly sandbox both safe and unsafe code, limiting security policies.

Method: The paper develops SandCell, a system for flexible, lightweight isolation in Rust. It uses syntactic boundaries to allow developers to specify and sandbox components with minimal annotations, supporting fine-grained control. SandCell also introduces techniques to efficiently transfer data between sandboxes, minimizing overhead.

Result: SandCell is shown to effectively prevent vulnerabilities in a range of Rust applications. The evaluation indicates it can do so while keeping performance overheads within reasonable levels.

Conclusion: SandCell enables fine-grained and expressive isolation of both safe and unsafe Rust code via minimal programmer effort, providing better security without substantially impacting performance.

Abstract: Rust is a modern systems programming language that ensures memory safety by
enforcing ownership and borrowing rules at compile time. While the unsafe
keyword allows programmers to bypass these restrictions, it introduces
significant risks. Various approaches for isolating unsafe code to protect safe
Rust from vulnerabilities have been proposed, yet these methods provide only
fixed isolation boundaries and do not accommodate expressive policies that
require sandboxing both safe and unsafe code. This paper presents SandCell for
flexible and lightweight isolation in Rust by leveraging existing syntactic
boundaries. SandCell allows programmers to specify which components to sandbox
with minimal annotation effort, enabling fine-grained control over isolation.
The system also introduces novel techniques to minimize overhead when
transferring data between sandboxes. Our evaluation demonstrates SandCell's
effectiveness in preventing vulnerabilities across various Rust applications
while maintaining reasonable performance overheads.

</details>


### [17] [PerfBench: Can Agents Resolve Real-World Performance Bugs?](https://arxiv.org/abs/2509.24091)
*Spandan Garg,Roshanak Zilouchian Moghaddam*

Main category: cs.SE

TL;DR: PerfBench introduces a benchmark for performance bug-fixing tasks and shows current coding agents struggle to optimize code (~3–20% success rate). Adding performance-aware tools improves results, but much progress remains possible.


<details>
  <summary>Details</summary>
Motivation: Performance bugs, although non-functional, waste resources and are difficult to detect/fix. Existing benchmarks focus only on functional correctness and overlook performance bugs, limiting the evaluation of automated bug-fixing agents.

Method: Authors introduce PerfBench, a benchmark with 81 real-world performance bug-fixing tasks from .NET GitHub repositories. PerfBench uses a new evaluation system where agents generate and validate their own benchmarks by comparing execution metrics for both developer and agent fixes. Tasks are derived from verified developer fixes, ensuring real-world relevance.

Result: State-of-the-art coding agents perform poorly on performance bug tasks: the baseline OpenHands achieves ~3% success, but a tailored agent, OpenHands-Perf-Agent, with performance-aware tooling/instructions, reaches ~20% success. Enhanced benchmarking and output processing significantly improve agent outcomes.

Conclusion: PerfBench is a rigorous test set that exposes gaps in current agents’ abilities to fix performance bugs and lays the foundation for further advancements by highlighting the importance of dedicated evaluation and performance-aware agent design.

Abstract: Performance bugs are inefficiencies in software that waste computational
resources without causing functional failures, making them particularly
challenging to detect and fix. While recent advances in Software Engineering
agents have shown promise in automated bug fixing, existing benchmarks
primarily focus on functional correctness and fail to evaluate agents'
abilities to identify and resolve non-functional issues like performance bugs.
We introduce PerfBench, a benchmark comprising 81 real-world performance
bug-fixing tasks from popular .NET repositories on GitHub. Unlike existing
benchmarks that rely on pre-existing test suites, PerfBench features a novel
evaluation harness that allows agents to generate their own performance
benchmarks and validates fixes by comparing execution metrics collected for
developer fix and agent fix. Each task in PerfBench is derived from actual
developer fixes linked to performance-related issues, which are then verified
by human experts, ensuring real-world relevance. Our evaluation reveals that
current state-of-the-art coding agents struggle with performance optimization
tasks, with baseline OpenHands agent achieving only a ~3% success rate on our
benchmark. We develop OpenHands-Perf-Agent, which incorporates
performance-aware tooling and instructions and achieves a ~20% success rate on
the benchmark. We show that by ensuring the agent has proper instructions to
benchmark its changes and tooling for benchmark output processing, we can
improve the agent performance significantly, but room for improvement still
remains. PerfBench provides a challenging test set for furthering the
capabilities of agents in fixing performance issues.

</details>


### [18] [TENET: Leveraging Tests Beyond Validation for Code Generation](https://arxiv.org/abs/2509.24148)
*Yiran Hu,Nan Jiang,Shanchao Liang,Yi Wu,Lin Tan*

Main category: cs.SE

TL;DR: TENET is a new agent for LLM-based, TDD-driven code generation that addresses key challenges in selecting test suites, retrieving context, and refining code. It outperforms previous approaches and provides the first in-depth analysis of test-driven code generation with repository-level context.


<details>
  <summary>Details</summary>
Motivation: With the rise of 'vibe coding', where developers use LLMs to generate code from high-level intentions, TDD becomes more essential to define and verify desired functionality, going beyond vague natural-language instructions. However, key challenges exist such as selecting effective test suites, retrieving relevant context, and refining code effectively via test feedback.

Method: The authors introduce TENET, an LLM agent for TDD-based code generation in large repositories. TENET has three main components: (1) a test harness for concise, diverse test suite selection, (2) a specialized retrieval and debugging agent toolset, and (3) a reflection-based workflow that iteratively refines code using test feedback.

Result: TENET achieves state-of-the-art results with 69.08% Pass@1 on RepoCod and 81.77% on RepoEval benchmarks, outperforming best agentic baselines by 9.49 and 2.17 percentage points, respectively. The study also systematically analyzes the impact of test suite design on code generation performance in repository-level context.

Conclusion: TENET significantly enhances the effectiveness of LLM-based code generation under TDD in real-world repositories, surpassing existing agentic methods and providing new insights into the relationship between test suite features and agent performance.

Abstract: Test-Driven Development (TDD) is a widely adopted software engineering
practice that requires developers to create and execute tests alongside code
implementation, ensuring that software behavior is continuously validated and
refined. In the era of vibe coding, where developers increasingly delegate code
writing to large language models (LLMs) by specifying high-level intentions,
TDD becomes even more crucial, as test cases serve as executable specifications
that explicitly define and verify intended functionality beyond what
natural-language descriptions and code context can convey. While vibe coding
under TDD is promising, there are three main challenges: (1) selecting a small
yet effective test suite to improve the generation accuracy and control the
execution workload, (2) retrieving context such as relevant code effectively,
and (3) systematically using test feedback for effective code refinement. To
address these challenges, we introduce TENET, an LLM agent for generating
functions in complex real-world repositories under the TDD setting. TENET
features three components: (1) a novel test harness mechanism that selects a
concise test suite to maximize diversity of target usage scenarios; (2) a
tailored agent toolset that performs efficient retrieval of relevant code with
interactive debugging; and (3) a reflection-based refinement workflow that
iteratively analyzes failures, replenishes context, and applies code
refinement. TENET achieves 69.08% and 81.77% Pass@1 on RepoCod and RepoEval
benchmarks, outperforming the best agentic baselines by 9.49 and 2.17
percentage points, respectively. In addition, this is the first study of
test-driven code generation with repository-level context, examining how
different aspects of test suites affect the performance of LLM agents under the
TDD setting.

</details>


### [19] [Metamorphic Testing for Audio Content Moderation Software](https://arxiv.org/abs/2509.24215)
*Wenxuan Wang,Yongjiang Wu,Junyuan Zhang,Shuqing Li,Yun Peng,Wenting Chen,Shuai Wang,Michael R. Lyu*

Main category: cs.SE

TL;DR: Audio moderation systems on popular platforms are vulnerable to adversarially modified toxic content. The proposed MTAM framework generates such adversarial examples, revealing that commercial and academic moderation tools frequently fail to detect them, with error finding rates up to 51.1%. Robustness of current moderation solutions must be improved.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper lies in the increasing misuse of audio-centric platforms (like WhatsApp and Twitter) to spread harmful audio content, such as hate speech and deceptive ads. Current moderation tools can often be evaded via subtle audio modifications, and their robustness against such tactics is not well-studied. This paper aims to address this gap.

Method: The paper proposes MTAM, a Metamorphic Testing framework for Audio moderation software, defining 14 metamorphic relations (across audio features-based and heuristic perturbations). MTAM systematically generates adversarial test cases from toxic audio by introducing modifications that preserve harmfulness but are more likely to bypass moderation. The framework is then used to test commercial and academic moderation tools against three toxic content types.

Result: MTAM was applied to five commercial moderation systems (Gladia, Assembly AI, Baidu, Nextdata, Tencent) and an academic model, finding high error rates (up to 51.1% EFR for Tencent, and up to 45.7% for the academic model), demonstrating that contemporary audio moderation tools are vulnerable to adversarial attacks generated by MTAM.

Conclusion: The study concludes that current commercial and academic audio moderation tools are insufficiently robust against adversarially modified toxic audio. MTAM effectively reveals these weaknesses, highlighting an urgent need for developing more resilient moderation systems.

Abstract: The rapid growth of audio-centric platforms and applications such as WhatsApp
and Twitter has transformed the way people communicate and share audio content
in modern society. However, these platforms are increasingly misused to
disseminate harmful audio content, such as hate speech, deceptive
advertisements, and explicit material, which can have significant negative
consequences (e.g., detrimental effects on mental health). In response,
researchers and practitioners have been actively developing and deploying audio
content moderation tools to tackle this issue. Despite these efforts, malicious
actors can bypass moderation systems by making subtle alterations to audio
content, such as modifying pitch or inserting noise. Moreover, the
effectiveness of modern audio moderation tools against such adversarial inputs
remains insufficiently studied. To address these challenges, we propose MTAM, a
Metamorphic Testing framework for Audio content Moderation software.
Specifically, we conduct a pilot study on 2000 audio clips and define 14
metamorphic relations across two perturbation categories: Audio Features-Based
and Heuristic perturbations. MTAM applies these metamorphic relations to toxic
audio content to generate test cases that remain harmful while being more
likely to evade detection. In our evaluation, we employ MTAM to test five
commercial textual content moderation software and an academic model against
three kinds of toxic content. The results show that MTAM achieves up to 38.6%,
18.3%, 35.1%, 16.7%, and 51.1% error finding rates (EFR) when testing
commercial moderation software provided by Gladia, Assembly AI, Baidu,
Nextdata, and Tencent, respectively, and it obtains up to 45.7% EFR when
testing the state-of-the-art algorithms from the academy.

</details>


### [20] [Comparing Open-Source and Commercial LLMs for Domain-Specific Analysis and Reporting: Software Engineering Challenges and Design Trade-offs](https://arxiv.org/abs/2509.24344)
*Theo Koraag,Niklas Wagner,Felix Dobslaw,Lucas Gren*

Main category: cs.SE

TL;DR: The paper explores the use of both open-source and commercial LLMs for automating financial report analysis. While LLMs show strong promise, especially cloud-based models for fluency, practical deployment faces integration, privacy, and reliability challenges. Tailored engineering and validation are crucial for success in financial contexts.


<details>
  <summary>Details</summary>
Motivation: Research in applying LLMs to domain-specific areas such as Finance is limited; automating financial report analysis is important for efficiency and accuracy, but presents technical engineering challenges.

Method: Design Science Research methodology was used in an exploratory case study, designing and evaluating two LLM-based systems—one with open-source local models in a multi-agent workflow and another with commercial GPT-4o. Expert evaluation was conducted on real-world financial reporting use cases.

Result: LLMs have high potential for automating financial reporting tasks. Cloud-based models (GPT-4o) are more fluent and usable but pose data privacy concerns, while open-source local models offer better data control and compliance at the cost of higher engineering effort. Key challenges include prompt design, contextual dependency, and integration trade-offs.

Conclusion: While LLMs are promising for financial reporting automation, successful integration requires architectural care, robust prompt design, and reliable systems. Domain-specific validation and tailored engineering strategies are necessary to balance accuracy, control, and compliance.

Abstract: Context: Large Language Models (LLMs) enable automation of complex natural
language processing across domains, but research on domain-specific
applications like Finance remains limited. Objectives: This study explored
open-source and commercial LLMs for financial report analysis and commentary
generation, focusing on software engineering challenges in implementation.
Methods: Using Design Science Research methodology, an exploratory case study
iteratively designed and evaluated two LLM-based systems: one with local
open-source models in a multi-agent workflow, another using commercial GPT-4o.
Both were assessed through expert evaluation of real-world financial reporting
use cases. Results: LLMs demonstrated strong potential for automating financial
reporting tasks, but integration presented significant challenges. Iterative
development revealed issues including prompt design, contextual dependency, and
implementation trade-offs. Cloud-based models offered superior fluency and
usability but raised data privacy and external dependency concerns. Local
open-source models provided better data control and compliance but required
substantially more engineering effort for reliability and usability.
Conclusion: LLMs show strong potential for financial reporting automation, but
successful integration requires careful attention to architecture, prompt
design, and system reliability. Implementation success depends on addressing
domain-specific challenges through tailored validation mechanisms and
engineering strategies that balance accuracy, control, and compliance.

</details>


### [21] [Efficient Decomposition Identification of Deterministic Finite Automata from Examples](https://arxiv.org/abs/2509.24347)
*Junjie Meng,Jie An,Yong Li,Andrea Turrini,Fanjiang Xu,Naijun Zhan,Miaomiao Zhang*

Main category: cs.SE

TL;DR: By switching from APTA to compact 3DFA representations for SAT encoding, the authors dramatically enhance the scalability and efficiency of automata decomposition learning, enabling practical solutions to complex DFA identification tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional deterministic finite automata (DFA) learning often produces large, monolithic automata that lack simplicity and modularity, which are important for understanding and managing complex systems. Previous attempts to decompose DFA models (DFA-DIPs) were hampered by unscalable SAT encodings due to redundancy in Augmented Prefix Tree Acceptors (APTAs).

Method: The paper proposes a new framework that replaces the use of APTA with a more compact 3-valued DFA (3DFA) representation, derived directly from labeled examples, to serve as the basis for SAT encodings. This approach is applied to both the classic Pareto-optimal DFA decomposition identification problem (DIP) and a new variant, the states-optimal DIP prioritizing minimal state numbers.

Result: The 3DFA-based encoding significantly reduces the number of variables in the SAT-based approach, making the method more scalable and efficient than previous APTA-based techniques.

Conclusion: The new 3DFA-based framework improves scalability and efficiency in solving DFA decomposition identification problems, particularly for the Pareto-optimal and states-optimal variants. It outperforms previous methods relying on APTA-based encoding.

Abstract: The identification of deterministic finite automata (DFAs) from labeled
examples is a cornerstone of automata learning, yet traditional methods focus
on learning monolithic DFAs, which often yield a large DFA lacking simplicity
and interoperability. Recent work addresses these limitations by exploring DFA
decomposition identification problems (DFA-DIPs), which model system behavior
as intersections of multiple DFAs, offering modularity for complex tasks.
However, existing DFA-DIP approaches depend on SAT encodings derived from
Augmented Prefix Tree Acceptors (APTAs), incurring scalability limitations due
to their inherent redundancy.
  In this work, we advance DFA-DIP research through studying two variants: the
traditional Pareto-optimal DIP and the novel states-optimal DIP, which
prioritizes a minimal number of states. We propose a novel framework that
bridges DFA decomposition with recent advancements in automata representation.
One of our key innovations replaces APTA with 3-valued DFA (3DFA) derived
directly from labeled examples. This compact representation eliminates
redundancies of APTA, thus drastically reducing variables in the improved SAT
encoding. Experimental results demonstrate that our 3DFA-based approach
achieves significant efficiency gains for the Pareto-optimal DIP while enabling
a scalable solution for the states-optimal DIP.

</details>


### [22] [Walk the Talk: Is Your Log-based Software Reliability Maintenance System Really Reliable?](https://arxiv.org/abs/2509.24352)
*Minghua He,Tong Jia,Chiming Duan,Pei Xiao,Lingzhe Zhang,Kangjin Wang,Yifan Wu,Ying Li,Gang Huang*

Main category: cs.SE

TL;DR: Current deep learning log anomaly detectors are black boxes that providers can't trust. This paper introduces a new trust metric and method (FaithLog) with mechanisms for better explainability, achieving top performance in faithfulness on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Deep learning-based log anomaly detection is effective but lacks interpretability, preventing service providers from trusting and deploying these systems in critical environments. There is a need for more trustworthy, explainable anomaly detection methods.

Method: The paper introduces a new metric, 'diagnostic faithfulness', to assess trustworthiness of anomaly detection models. They design two evaluation tasks: attention-based root cause localization and event perturbation. FaithLog, a new system, leverages a causality-guided attention mechanism and adversarial consistency learning to improve faithfulness.

Result: Empirical studies show current methods perform poorly in diagnostic faithfulness. FaithLog, tested on two public and one industrial dataset, achieves state-of-the-art results for diagnostic faithfulness.

Conclusion: FaithLog establishes a new benchmark for trustworthy, explainable log-based anomaly detection, addressing a key barrier to real-world deployment and operator trust.

Abstract: Log-based software reliability maintenance systems are crucial for sustaining
stable customer experience. However, existing deep learning-based methods
represent a black box for service providers, making it impossible for providers
to understand how these methods detect anomalies, thereby hindering trust and
deployment in real production environments. To address this issue, this paper
defines a trustworthiness metric, diagnostic faithfulness, for models to gain
service providers' trust, based on surveys of SREs at a major cloud provider.
We design two evaluation tasks: attention-based root cause localization and
event perturbation. Empirical studies demonstrate that existing methods perform
poorly in diagnostic faithfulness. Consequently, we propose FaithLog, a
faithful log-based anomaly detection system, which achieves faithfulness
through a carefully designed causality-guided attention mechanism and
adversarial consistency learning. Evaluation results on two public datasets and
one industrial dataset demonstrate that the proposed method achieves
state-of-the-art performance in diagnostic faithfulness.

</details>


### [23] [United We Stand: Towards End-to-End Log-based Fault Diagnosis via Interactive Multi-Task Learning](https://arxiv.org/abs/2509.24364)
*Minghua He,Chiming Duan,Pei Xiao,Tong Jia,Siyu Yu,Lingzhe Zhang,Weijie Hong,Jin Han,Yifan Wu,Ying Li,Gang Huang*

Main category: cs.SE

TL;DR: Chimera is an end-to-end log-based fault diagnosis method that bridges anomaly detection and root cause localization using interactive multi-task learning, yielding notable accuracy improvements and practical deployment success.


<details>
  <summary>Details</summary>
Motivation: Log-based fault diagnosis is crucial for software system reliability, but current approaches do not connect the tasks of anomaly detection and root cause localization. This disconnection causes diagnostic bias, reliance on costly data, and ignores collaboration between tasks.

Method: The paper proposes Chimera, a novel end-to-end log-based fault diagnosis method using interactive multi-task learning. Chimera enables bidirectional interaction and knowledge transfer between anomaly detection and root cause localization at data, feature, and result levels within a unified framework.

Result: On two public and one industrial dataset, Chimera shows significant performance improvements: 2.92%-5.00% higher in anomaly detection, and 19.01%-37.09% higher in root cause localization compared to existing techniques. It has also been deployed on an industrial cloud platform.

Conclusion: Chimera effectively closes the gap between anomaly detection and root cause localization, leading to better diagnostic performance and successful real-world deployment.

Abstract: Log-based fault diagnosis is essential for maintaining software system
availability. However, existing fault diagnosis methods are built using a
task-independent manner, which fails to bridge the gap between anomaly
detection and root cause localization in terms of data form and diagnostic
objectives, resulting in three major issues: 1) Diagnostic bias accumulates in
the system; 2) System deployment relies on expensive monitoring data; 3) The
collaborative relationship between diagnostic tasks is overlooked. Facing this
problems, we propose a novel end-to-end log-based fault diagnosis method,
Chimera, whose key idea is to achieve end-to-end fault diagnosis through
bidirectional interaction and knowledge transfer between anomaly detection and
root cause localization. Chimera is based on interactive multi-task learning,
carefully designing interaction strategies between anomaly detection and root
cause localization at the data, feature, and diagnostic result levels, thereby
achieving both sub-tasks interactively within a unified end-to-end framework.
Evaluation on two public datasets and one industrial dataset shows that Chimera
outperforms existing methods in both anomaly detection and root cause
localization, achieving improvements of over 2.92% - 5.00% and 19.01% - 37.09%,
respectively. It has been successfully deployed in production, serving an
industrial cloud platform.

</details>


### [24] [Agentic Services Computing](https://arxiv.org/abs/2509.24380)
*Shuiguang Deng,Hailiang Zhao,Ziqi Wang,Guanjie Cheng,Peng Chen,Wenzhuo Qian,Zhiwei Ling,Jianwei Yin,Albert Y. Zomaya,Schahram Dustdar*

Main category: cs.SE

TL;DR: This survey redefines services computing for the era of LLM-powered agents, introducing Agentic Service Computing (ASC), a lifecycle-based framework merging classical and modern approaches. It systematically reviews perception, autonomy, collaboration, and trust across the service lifecycle, and offers a reference for building future intelligent, adaptive, and trustworthy agent-based services.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the fundamental transformation occurring in services computing, driven by the rise of large language model (LLM)-powered agents. Traditional request-response architectures are shifting toward dynamic, goal-oriented, and autonomous multi-agent ecosystems, necessitating new paradigms and frameworks to develop and manage these intelligent services.

Method: The method adopted in this paper is a comprehensive survey and the proposal of a lifecycle-driven framework for Agentic Service Computing (ASC). The framework is organized around four phases—Design, Deployment, Operation, and Evolution—and analyzes ASC through four research dimensions: perception and environment modeling, autonomous decision-making, multi-agent collaboration, and evaluation/trustworthiness.

Result: The paper synthesizes current research and identifies that agentic services are orchestrated rather than simply assembled. Key findings include the importance of contextual awareness in deployment, autonomous reasoning for operations, emergence and evolution of collaborative structures, and the need for ongoing trust and value alignment. The review also highlights emerging trends and integrates classical service principles with modern multi-agent system advances.

Conclusion: Agentic Service Computing (ASC) is a promising, holistic paradigm that combines classic services computing with LLM-based multi-agent systems. The paper provides a foundational reference for designing adaptive, trustworthy, and human-centric intelligent services, guiding researchers and practitioners in the development and management of such systems.

Abstract: The rise of LLM-powered agents is driving a fundamental transformation in
services computing: from static, request-response functions to dynamic,
goal-oriented, and autonomous multi-agent ecosystems. In response to this
shift, we introduce Agentic Service Computing (ASC), a new paradigm that
reimagines services as intelligent, self-adaptive, and socially embedded
entities. This comprehensive survey presents a lifecycle-driven framework for
ASC, structured around four core phases: Design, Deployment, Operation, and
Evolution. We systematically analyze ASC through four foundational research
dimensions: (1) Perception, Context, and Environment Modeling, (2) Autonomous
Decision-Making and Task Execution, (3) Multi-Agent Collaboration and
Organization, and (4) Evaluation, Value Alignment, and Trustworthiness. We
examine how these dimensions are instantiated, integrated, and continuously
adapted across the service lifecycle. Our synthesis reveals that agentic
services are not merely assembled but orchestrated: contextual awareness
enables robust deployment; autonomous reasoning supports real-time operation;
collaborative structures emerge and evolve through interaction; and
trustworthiness must be upheld as a cross-cutting, lifelong imperative. We
further identify and discuss emerging trends shaping the future of ASC. By
integrating classical principles of services computing with advances in
LLM-based multi-agent systems, this work establishes a holistic and
forward-looking foundation for ASC. It provides a unified reference for
researchers and practitioners aiming to develop adaptive, accountable, and
human-centered intelligent services.

</details>


### [25] [Unit Test Update through LLM-Driven Context Collection and Error-Type-Aware Refinement](https://arxiv.org/abs/2509.24419)
*Yuanhe Zhang,Zhiquan Yang,Shengyi Pan,Zhongxin Liu*

Main category: cs.SE

TL;DR: Maintaining unit tests manually is slow and error-prone. TESTUPDATER, an LLM-based system, automates both repair and enhancement of tests for updated code, outperforming existing solutions in correctness and test coverage, as shown on a new real-world benchmark.


<details>
  <summary>Details</summary>
Motivation: Unit testing is important for software quality, but manual maintenance is inefficient and can miss necessary fixes. Existing automated approaches mostly focus on repairing broken tests and rarely address enhancing tests to cover new features, while also struggling with accuracy due to rigid context collection methods.

Method: The authors propose TESTUPDATER, a large language model (LLM) based system that analyzes code changes, extracts and filters relevant context, uses step-by-step prompting for various code change types, and employs an error-type-aware iterative refinement process to repair and enhance unit tests just-in-time. The method also introduces a new benchmark, UPDATES4J.

Result: TESTUPDATER achieves a compilation pass rate of 94.4%, test pass rate of 86.7%, and outperforms SYNTER (the state-of-the-art) by 15.9% and 20.0% respectively. It also delivers 12.9% higher branch coverage and 15.2% greater line coverage than SYNTER.

Conclusion: TESTUPDATER is an effective automated tool for just-in-time updating, repairing, and enhancing unit tests in response to production code changes. It improves both the correctness and coverage of test suites over current methods.

Abstract: Unit testing is critical for ensuring software quality and software system
stability. The current practice of manually maintaining unit tests suffers from
low efficiency and the risk of delayed or overlooked fixes. Therefore, an
automated approach is required to instantly update unit tests, with the
capability to both repair and enhance unit tests. However, existing automated
test maintenance methods primarily focus on repairing broken tests, neglecting
the scenario of enhancing existing tests to verify new functionality.
Meanwhile, due to their reliance on rule-based context collection and the lack
of verification mechanisms, existing approaches struggle to handle complex code
changes and often produce test cases with low correctness. To address these
challenges, we propose TESTUPDATER, a novel LLM based approach that enables
automated just-in-time test updates in response to production code changes.
TESTUPDATER first leverages the LLM to analyze code changes and identify
relevant context, which it then extracts and filters. Then, through carefully
designed prompts, TESTUPDATER guides the LLM step by step to handle various
types of code changes and introduce new dependencies, enabling both test repair
and enhancement. Finally, we introduce an error-type-aware iterative refinement
mechanism that executes the LLM-updated tests and repairs failures, which
significantly improves the overall correctness of test updates. Since existing
test repair datasets lack scenarios of test enhancement, we further construct a
new benchmark, UPDATES4J, with 195 real-world samples from 7 projects.
Experimental results show that TESTUPDATER achieves a compilation pass rate of
94.4% and a test pass rate of 86.7%, outperforming the state-of-the-art method
SYNTER by 15.9% and 20.0%, respectively. Furthermore, TESTUPDATER exhibits
12.9% higher branch coverage and 15.2% greater line coverage than SYNTER.

</details>


### [26] [Towards Shift-Up: A Framework and a Prestudy on High-Value Activities in GenAI Native Software Development](https://arxiv.org/abs/2509.24485)
*Vlad Stirbu,Mateen Ahmed Abbasi,Teerath Das,Jesse Haimi,Niko Iljin,Pyry Kotilainen,Petrus Lipsanen,Niko Mäkitalo,Maiju Sipilä,Venla Veijalainen,Tommi Mikkonen*

Main category: cs.SE

TL;DR: The paper introduces 'shift-up,' a GenAI-focused framework to help software teams optimize work by leveraging AI agents, supported by a preliminary study and setting the stage for future research.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the evolving role of GenAI in software engineering, exploring how AI-powered specialized agents are changing traditional development practices and how teams can adapt to focus on high-value tasks.

Method: The authors propose a new framework called 'shift-up' for GenAI native development and conduct a preliminary study using current GenAI tools to test its feasibility and impact.

Result: A framework called 'shift-up' is presented, aiming to help software teams leverage GenAI for increased focus on valuable work, supported by preliminary findings from experimental use of GenAI tools.

Conclusion: GenAI is fundamentally transforming software engineering, and the 'shift-up' framework offers a structured approach for teams to maximize productivity by reallocating focus to higher-value tasks. The paper encourages further research to validate and refine this approach.

Abstract: Generative AI (GenAI) has significantly influenced software engineering.
Associated tools have created a shift in software engineering, where
specialized agents, based on user-provided prompts, are replacing human
developers. In this paper, we propose a framework for GenAI native development
that we call \textit{shift-up}, which helps software teams focus on high-value
work while being supported by GenAI. Furthermore, we also present a preliminary
study testing these ideas with current GenAI tools. Towards the end of the
paper, we propose future research goals to study shift-up in more detail.

</details>


### [27] [JSProtect: A Scalable Obfuscation Framework for Mini-Games in WeChat](https://arxiv.org/abs/2509.24498)
*Zhihao Li,Chaozheng Wang,Zongjie Li,Xinyong Peng,Zelin Su,Qun Xia,Haochuan Lu,Ting Xiong,Man Ho Lam,Shuzheng Gao,Yuchong Xie,Cuiyun Gao,Shuai Wang,Yuetang Deng,Huafeng Ma*

Main category: cs.SE

TL;DR: JSProtect is a new, fast, and scalable JavaScript obfuscation framework for WeChat mini-games that avoids huge code inflation and performance loss; it delivers strong security with minimal impact on runtime and code size, outperforming existing tools by a wide margin.


<details>
  <summary>Details</summary>
Motivation: The WeChat mini-game ecosystem experiences intellectual property theft through secondary development on other platforms. Existing JavaScript obfuscation tools cannot efficiently protect large-scale applications due to slow processing speeds, runtime performance issues, and excessive code size growth.

Method: This paper introduces JSProtect, a parallelized obfuscation framework using the Parallel-Aware Scope Analysis (PASA) algorithm. This algorithm supports independent code partitioning for multi-core processing and efficient namespace management that reuses short identifiers to minimize code bloat.

Result: JSProtect can obfuscate 20MB JavaScript codebases in minutes, maintains 100% semantic equivalence, limits code size inflation to around 20% (far better than over 1,000% with traditional tools), and keeps runtime performance near-native. It also effectively defends against static analysis and large language models.

Conclusion: JSProtect provides a new paradigm for large-scale JavaScript protection, achieving a balance between enhanced security, scalability, and high performance.

Abstract: The WeChat mini-game ecosystem faces rampant intellectual property theft to
other platforms via secondary development, yet existing JavaScript obfuscation
tools are ill-equipped for large-scale applications, suffering from prohibitive
processing times, severe runtime performance degradation, and unsustainable
code size inflation. This paper introduces JSProtect, a high-throughput
parallelized obfuscation framework designed to overcome these fundamental
limitations. At the core of our framework is the Parallel-Aware Scope Analysis
(PASA) algorithm, which enables two key optimizations: independent code
partitioning for multi-core processing and independent namespace management
that aggressively reuses short identifiers to combat code bloat. Our evaluation
demonstrates that JSProtectprocesses 20MB codebases in minutes, maintaining
100\% semantic equivalence while controlling code size inflation to as low as
20\% compared to over 1,000\% with baseline tools. Furthermore, it preserves
near-native runtime performance and provides superior security effectiveness
against both static analysis tools and large language models. This work
presents a new paradigm for industrial-scale JavaScript protection that
effectively balances robust security with high performance and scalability.

</details>


### [28] [SemGuard: Real-Time Semantic Evaluator for Correcting LLM-Generated Code](https://arxiv.org/abs/2509.24507)
*Qinglin Wang,Zhihong Sun,Ruyun Wang,Tao Huang,Zhi Jin,Ge Li,Chen Lyu*

Main category: cs.SE

TL;DR: SemGuard introduces real-time, line-level semantic supervision for LLM code generation, catching errors early without running code or needing test cases. It achieves substantial accuracy improvements over prior methods across multiple models and datasets.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs often produce semantic errors, which are difficult to catch and fix during code generation. Post-hoc repair approaches are inefficient and can miss faults due to incomplete test suites. Improving error detection during generation, rather than after execution, is crucial for reducing semantic defects in LLM-generated code.

Method: The paper introduces SemGuard, a framework that embeds a semantic evaluator into the LLM's decoding process for real-time, line-level semantic supervision. It uses SemDiff, a new dataset with fine-grained annotations marking lines where correct and incorrect implementations diverge, to train the evaluator. The evaluator flags and rolls back partial-code deviations during generation, guiding regeneration without executing code or relying on test cases.

Result: SemGuard substantially outperforms existing methods. It lowers semantic error rates by 19.86% compared to ROCODE and improves Pass@1 by 48.92% on LiveCodeBench using CodeLlama-7B. Significant gains are also shown for StarCoder2-7B on MBPP and DeepSeekCoder-6.7B on SemDiff-Java, indicating cross-model and cross-language effectiveness.

Conclusion: Early, semantic-level supervision during code generation is highly effective at reducing semantic errors from LLMs. SemGuard achieves state-of-the-art results on multiple benchmarks and is both model- and language-agnostic, showing promise for robust, error-minimized code generation.

Abstract: Large Language Models (LLMs) can translate natural language requirements into
code, yet empirical analyses of representative models reveal that semantic
errors-programs that compile but behave incorrectly-constitute the majority of
observed faults (e.g., >60% on DeepSeek-Coder-6.7B and QwenCoder-7B). Post-hoc
repair pipelines detect such faults only after execution, incurring latency,
relying on incomplete test suites, and often mis-localizing the defect. Since
semantic drift originates in the autoregressive decoding process, intervening
while the code is being generated is a direct way to stop error propagation.
Constrained-decoding approaches such as ROCODE attempt this, but still wait
until the entire program runs to obtain feedback and use entropy heuristics
that do not truly capture semantics. A more effective solution must inject
semantic signals-early and precisely-into the decoding process.We present
SemGuard, a semantic-evaluator-driven framework that performs real-time,
line-level semantic supervision. To train the evaluator, we build SemDiff, the
first dataset with fine-grained annotations that mark the exact line where a
correct and an incorrect implementation diverge. The evaluator, once embedded
in the LLM's decoder, flags deviations on partial code, rolls back to the
faulty line, and guides regeneration-without executing the program or requiring
test cases. Across four benchmarks, SemGuard consistently outperforms
state-of-the-art baselines. It lowers the semantic error rate by 19.86% on
SemDiff relative to ROCODE, and lifts Pass@1 by 48.92% on the real-world
LiveCodeBench with CodeLlama-7B. Similar gains hold for StarCoder2-7B on MBPP
and for DeepSeekCoder-6.7B on the Java benchmark SemDiff-Java, demonstrating
model- and language-agnostic effectiveness.

</details>


### [29] [Bridging Developer Instructions and Code Completion Through Instruction-Aware Fill-in-the-Middle Paradigm](https://arxiv.org/abs/2509.24637)
*Zhensu Sun,Chengran Yang,Chao Peng,Pengfei Gao,Xiaoning Du,Li Li,David Lo*

Main category: cs.SE

TL;DR: IFIM is a new training method for code completion LLMs that makes them better at following developer instructions without hurting their performance on normal fill-in-the-middle tasks. Applied to two popular models, it vastly improves instruction adherence, solving the usual trade-off with minimal downsides.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) improve code completion but often falter when a developer's intent is unclear. Developers typically add natural language instructions (comments) to clarify intent, but current code LLMs are not optimized to utilize these effectively, as standard fill-in-the-middle (FIM) training lacks instruction-like data. Moreover, conventional instruction-tuning improves instruction following but degrades FIM, causing a trade-off. This motivates the need for a method that enhances both instruction-following and FIM performance.

Method: The paper introduces Instruction-aware Fill-in-the-Middle (IFIM), a training method that incorporates explicit instruction sections into FIM objectives. IFIM trains models on (prefix, instruction, suffix) triplets, enabling them to leverage developer directives while maintaining FIM capabilities. A large-scale dataset with intent-focused instructions was created using GPT-4o for code infilling examples. The approach was applied to Deepseek-Coder and Qwen2.5-Coder and evaluated on HumanEval-infilling and RepoMasterEval benchmarks.

Result: IFIM significantly improves instruction-following capabilities, increasing the Pass@1 score from 84.6% to 93.6% on HumanEval-infilling. Importantly, the enhancement does not degrade the base models' performance on standard FIM code completion when no instructions are given.

Conclusion: Instruction-aware Fill-in-the-Middle (IFIM) bridges the gap between instruction-following and infilling abilities in code LLMs. By integrating explicit instruction sections in the training objective, IFIM enables models to utilize developer intent instructions in code completion without sacrificing performance on standard FIM tasks.

Abstract: Large Language Models (LLMs) have significantly advanced code completion, yet
they often fail when the developer's intent is underspecified in the code
context. To address this, developers usually add natural language instructions
(e.g., comments) into the code context to clarify their intent. However,
existing code LLMs applied for code completion systems merely undergo a
fill-in-the-middle (FIM) pre-training, which struggles to leverage this
information effectively due to the lack of instruction-like training data.
Existing instruction-tuning techniques, which improve instruction-following in
general code generation, paradoxically degrade FIM performance, forcing a
trade-off between instruction-following and infilling capabilities. To address
this gap, we introduce Instruction-aware Fill-in-the-Middle (IFIM), an
instruction-tuning method specifically designed to enhance FIM code completion
models. IFIM extends the conventional FIM training objective by incorporating
an explicit instruction section into the input, enabling the model to learn
from (prefix, instruction, suffix) triplets. This approach allows the model to
effectively leverage developer-provided directives while preserving its core
completion abilities when no instructions are present. To facilitate this, we
constructed a large-scale dataset by using GPT-4o to generate concise,
intent-focused instructions for code infilling examples. We evaluated IFIM by
applying it to two popular base models, Deepseek-Coder and Qwen2.5-Coder, on
the benchmarks derived from HumanEval-infilling and RepoMasterEval. The results
demonstrate that IFIM significantly improves instruction-following
capabilities, boosting the Pass@1 score from 84.6% to 93.6% on
HumanEval-infilling. Moreover, this enhancement does not compromise the models'
original performance on FIM code completion tasks with no instructions
provided.

</details>


### [30] [CoTune: Co-evolutionary Configuration Tuning](https://arxiv.org/abs/2509.24694)
*Gangda Xiong,Tao Chen*

Main category: cs.SE

TL;DR: CoTune is a novel configuration tuner that leverages co-evolution of explicit and auxiliary performance requirements, significantly exceeding the performance and efficiency of current tuning methods by robustly meeting target requirements.


<details>
  <summary>Details</summary>
Motivation: Existing system configuration tuners often overlook the explicit presence and influence of complex performance requirements, such as target latency, which can lead to inefficient resource usage and sub-optimal tuning strategies.

Method: The paper introduces CoTune, a co-evolutionary tuning tool that leverages both the explicit target performance requirement and an auxiliary requirement evolved in parallel. This dual-guided evolution helps avoid pitfalls like strict convergence or premature stagnation by offering robustness and adaptability during tuning.

Result: CoTune was evaluated on 162 test cases across nine systems and 18 different requirements, where it achieved the best results in 90% of cases, compared to 0%-35% for competing tuners, and delivered up to 2.9x overall improvements with higher efficiency.

Conclusion: Incorporating both primary and auxiliary requirements through co-evolution, as accomplished in CoTune, makes automatic tuning for explicit performance requirements substantially more effective and efficient, outperforming state-of-the-art solutions.

Abstract: To automatically tune configurations for the best possible system performance
(e.g., runtime or throughput), much work has been focused on designing
intelligent heuristics in a tuner. However, existing tuner designs have mostly
ignored the presence of complex performance requirements (e.g., the latency
shall ideally be 2 seconds), but simply assume that better performance is
always more preferred. This would not only waste valuable information in a
requirement but might also consume extensive resources to tune for a goal with
little gain. Yet, prior studies have shown that simply incorporating the
requirement as a tuning objective is problematic since the requirement might be
too strict, harming convergence; or its highly diverse satisfactions might lead
to premature convergence. In this paper, we propose CoTune, a tool that takes
the information of a given target performance requirement into account through
co-evolution. CoTune is unique in the sense that it creates an auxiliary
performance requirement to be co-evolved with the configurations, which assists
the target performance requirement when it becomes ineffective or even
misleading, hence allowing the tuning to be guided by the requirement while
being robust to its harm. Experiment results on 162 cases (nine systems and 18
requirements) reveal that CoTune considerably outperforms existing tuners,
ranking as the best for 90% cases (against the 0%--35% for other tuners) with
up to 2.9x overall improvements, while doing so under a much better efficiency.

</details>


### [31] [Large language models for behavioral modeling: A literature survey](https://arxiv.org/abs/2509.24782)
*Muhammad Laiq*

Main category: cs.SE

TL;DR: This paper reviews 14 studies on LLM-assisted behavioral modeling, finding promising results for use case and sequence diagram generation. Most studies use GPT-based models and lack expert validation. Future work should broaden model variety and involve domain experts for evaluation.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) are increasingly applied to behavioral modeling tasks such as automatic generation of sequence diagrams, but there is no published overview of this work. Such a review can guide future research and inform practitioners and educators about LLM effectiveness in this domain.

Method: The study conducts a term-based search to filter and identify 14 relevant primary studies focused on using LLMs for behavioral modeling, specifically use case and sequence diagrams. The analysis synthesizes findings from these studies.

Result: LLMs have shown promising results in automatically generating use case and sequence diagrams. However, most studies rely on GPT-based models and typically do not involve expert-based evaluations of model outputs.

Conclusion: There is potential for LLMs in behavioral modeling, but future research should diversify the types of LLMs assessed and incorporate expert-based evaluations to provide a more robust assessment of their effectiveness.

Abstract: In recent years, large language models (LLMs) have been extensively utilized
for behavioral modeling, for example, to automatically generate sequence
diagrams. However, no overview of this work has been published yet. Such an
overview will help identify future research directions and inform practitioners
and educators about the effectiveness of LLMs in assisting behavioral modeling.
This study aims to provide an overview of the existing research on the use of
LLMs for behavioral modeling, particularly focusing on use case and sequence
diagrams. Through a term-based search, we filtered and identified 14 relevant
primary studies. Our analysis of the selected primary studies reveals that LLMs
have demonstrated promising results in automatically generating use case and
sequence diagrams. In addition, we found that most of the current literature
lacks expert-based evaluations and has mainly used GPT-based models. Therefore,
future work should evaluate a broader range of LLMs for behavioral modeling and
involve domain experts to evaluate the output of LLMs.

</details>


### [32] [Evaluating SAP Joule for Code Generation](https://arxiv.org/abs/2509.24828)
*Joshua Heisler,Johannes Reisinger,Andreas Fischer*

Main category: cs.SE

TL;DR: This paper presents the first comparative evaluation of SAP Joule's code generation abilities, showing it is a top performer (5th/30) for JavaScript tasks, with an accuracy of 80.49% on the HumanEval-X benchmark.


<details>
  <summary>Details</summary>
Motivation: To investigate and assess the code generation capabilities of SAP Joule, particularly in JavaScript, and to provide the first comprehensive comparative study of its performance.

Method: Comparative evaluation using the HumanEval-X JavaScript benchmark against 29 other generative models.

Result: SAP Joule achieves a strict accuracy of 80.49%, ranking fifth overall, indicating high competence in code generation tasks for JavaScript.

Conclusion: SAP Joule demonstrates strong performance in JavaScript code generation, ranking fifth among 30 models evaluated, with strict accuracy of 80.49%.

Abstract: SAP has released its own proprietary generative model SAP Joule, intended for
various generative tasks, including serving as a code assistant for software
engineers. While Joule is yet not focused on SAP-specific ABAP code generation,
it can be used for other common languages, including Javascript. This paper
compares SAP Joules Javascript coding capabilities against a total of 29 other
models using the HumanEval-X Javascript benchmark. SAP Joule achieves a strict
accuracy of 80.49% as the fifth best model in our evaluation. To the best of
our knowledge, this is the first comparative evaluation of SAP Joule code
generation capabilities.

</details>


### [33] [DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern](https://arxiv.org/abs/2509.24975)
*Lekang Yang,Yuetong Liu,Yitong Zhang,Jia Li*

Main category: cs.SE

TL;DR: DiffTester is a framework that speeds up unit test generation in software development by enabling diffusion LLMs to produce more tokens per step without sacrificing test quality. It works by detecting repetitive patterns in unit tests and adaptively increasing generation efficiency, and it generalizes well across languages and models.


<details>
  <summary>Details</summary>
Motivation: Unit Test Generation (UTG) is crucial for software development. The inefficiency of current LLMs in generating unit tests (as they produce one token per step) limits UTG's scalability and practicality.

Method: DiffTester is an acceleration framework tailored for diffusion LLMs (dLLMs) in UTG. It uses dynamic analysis of abstract syntax trees to identify repetitive structural patterns among unit tests targeting the same method. This enables adaptive token generation to improve efficiency without reducing test quality.

Result: DiffTester significantly accelerates unit test generation, while maintaining high test coverage and quality. Experiments across multiple languages (Python, Java, C++) and models show strong generalization and scalability.

Conclusion: DiffTester offers a practical, scalable solution for efficient UTG using dLLMs, overcoming the trade-off between generation speed and test quality through adaptive generation techniques. It generalizes well to various languages and models.

Abstract: Software development relies heavily on extensive unit testing, which makes
the efficiency of automated Unit Test Generation (UTG) particularly important.
However, most existing LLMs generate test cases one token at a time in each
forward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs)
have emerged, offering promising parallel generation capabilities and showing
strong potential for efficient UTG. Despite this advantage, their application
to UTG is still constrained by a clear trade-off between efficiency and test
quality, since increasing the number of tokens generated in each step often
causes a sharp decline in the quality of test cases. To overcome this
limitation, we present DiffTester, an acceleration framework specifically
tailored for dLLMs in UTG. The key idea of DiffTester is that unit tests
targeting the same focal method often share repetitive structural patterns. By
dynamically identifying these common patterns through abstract syntax tree
analysis during generation, DiffTester adaptively increases the number of
tokens produced at each step without compromising the quality of the output. To
enable comprehensive evaluation, we extend the original TestEval benchmark,
which was limited to Python, by introducing additional programming languages
including Java and C++. Extensive experiments on three benchmarks with two
representative models show that DiffTester delivers significant acceleration
while preserving test coverage. Moreover, DiffTester generalizes well across
different dLLMs and programming languages, providing a practical and scalable
solution for efficient UTG in software development. Code and data are publicly
available at https://github.com/wellbeingyang/DLM4UTG-open .

</details>


### [34] [Large Language Models for Software Testing: A Research Roadmap](https://arxiv.org/abs/2509.25043)
*Cristian Augusto,Antonia Bertolino,Guglielmo De Angelis,Francesca Lonetti,Jesús Morán*

Main category: cs.SE

TL;DR: This paper reviews and categorizes research on applying Large Language Models to software testing, providing a roadmap of current progress, trends, challenges, and future impacts.


<details>
  <summary>Details</summary>
Motivation: The rapid proliferation of research applying Large Language Models (LLMs) in software testing has made it challenging for researchers to stay updated and to understand the overall landscape and research trends.

Method: The authors conducted a semi-systematic literature review, collecting and categorizing relevant articles, analyzing the current and ongoing status, and identifying open challenges in LLM-based software testing.

Result: The contributions in LLM-based software testing were grouped into categories, with an overview of research trends and open challenges provided. Several expected long-term impacts of LLMs on the software testing field were outlined.

Conclusion: The paper provides a structured vision and roadmap of LLM-based software testing research, organizing existing work, identifying key research directions, and outlining future impacts and challenges.

Abstract: Large Language Models (LLMs) are starting to be profiled as one of the most
significant disruptions in the Software Testing field.
  Specifically, they have been successfully applied in software testing tasks
such as generating test code, or summarizing documentation.
  This potential has attracted hundreds of researchers, resulting in dozens of
new contributions every month, hardening researchers to
  stay at the forefront of the wave. Still, to the best of our knowledge, no
prior work has provided a structured vision of the progress
  and most relevant research trends in LLM-based testing. In this article, we
aim to provide a roadmap that illustrates its current state,
  grouping the contributions into different categories, and also sketching the
most promising and active research directions for the field.
  To achieve this objective, we have conducted a semi-systematic literature
review, collecting articles and mapping them into the most
  prominent categories, reviewing the current and ongoing status, and analyzing
the open challenges of LLM-based software testing.
  Lastly, we have outlined several expected long-term impacts of LLMs over the
whole software testing field.

</details>


### [35] [Towards Reliable Generation of Executable Workflows by Foundation Models](https://arxiv.org/abs/2509.25117)
*Sogol Masoumzadeh,Keheliya Gallaba,Dayi Lin,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: The paper presents an automated framework using static analysis to detect, classify, and fix errors in workflows generated by Foundation Models from natural language. With a new taxonomy of defects, a custom static analyzer (Timon), and integration with an FM tool (Pumbaa), the framework substantially improves the reliability of automated workflow generation.


<details>
  <summary>Details</summary>
Motivation: Foundation Models (FMs) can perform complex natural language tasks, but breaking down tasks into reliable executable workflows using domain-specific languages (DSLs) remains difficult, error-prone, and requires domain expertise. Manual workflow creation is laborious and FM-generated workflows often have defects.

Method: The authors introduce a framework that employs static analysis to detect and repair defects in FM-generated DSL workflows. They create a taxonomy categorizing 18 types of defects in such workflows. They develop a static analyzer tool called Timon for defect detection and couple it with Pumbaa, an FM-based tool, to repair the detected defects using static analysis feedback.

Result: The study finds that 87.27% of FM-generated DSL workflows contain at least one defect, highlighting the severity of the problem. Timon can effectively identify nine types of defects through static analysis, and providing feedback from Timon enables the FM tool Pumbaa to repair detected defects, systematically improving workflow reliability.

Conclusion: This paper demonstrates a significant advance toward automating the reliable generation of executable workflows from natural language. By systematically detecting and repairing common workflow defects through static analysis and FM feedback loops, the approach increases the practicality and accuracy of FM-generated software workflows.

Abstract: Recent advancements in Foundation Models (FMs) have demonstrated significant
progress in comprehending complex natural language to perform intricate tasks.
Successfully executing these tasks often requires orchestrating calls to FMs
alongside other software components. However, manually decomposing a task into
a coherent sequence of smaller, logically aggregated steps, commonly referred
to as workflows, demands considerable effort and specialized domain knowledge.
While FMs can assist in generating such workflows specified in domain-specific
languages (DSLs), achieving accuracy and reliability in this process remains a
challenge.
  This work introduces a framework that leverages static analysis feedback to
enable FMs to detect and repair defects in the DSL-based workflows they
generate. We begin by presenting the first-ever taxonomy of incidences of
defects in FM-generated DSL workflows, categorizing them into 18 distinct
types. Furthermore, we observe a high prevalence of defects across FM-generated
DSL workflows, with 87.27% of the studied instances containing at least one
defect. This, in turn, emphasizes the magnitude of the problem in practice and
underscores the necessity for implementing mitigation strategies. Following
this, we demonstrate that nine types of these defects can be effectively
identified through static analysis of the workflows. For this purpose, we
develop Timon, the first-of-its-kind static analyzer specifically designed for
FM-generated DSL workflows. Finally, we show that by incorporating feedback
from Timon, we can guide Pumbaa, an FM-based tool, to repair the detected
defect incidences. By systematically detecting and repairing defects, our work
provides a crucial step towards the reliable and automated generation of
executable workflows from natural language requirements.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [36] [Efficient Cost Bounds with Linear Maps](https://arxiv.org/abs/2509.22982)
*David M Kahn,Jan Hoffmann,Thomas Reps,Jessie Grosen*

Main category: cs.PL

TL;DR: This paper presents a new way to infer cost-free types in AARA using linear maps and matrix inequalities, enabling efficient inference (even for non-polynomial bounds) with linear programming tools and achieving much greater efficiency over previous algorithms.


<details>
  <summary>Details</summary>
Motivation: Current AARA algorithms for resource bound inference rely on cost-free types, which are necessary for composing types but costly to infer due to recursive dependencies. Existing heuristics only work for polynomial cost bounds, limiting applicability to more complex cases such as exponential bounds.

Method: The paper introduces a new approach representing cost-free types using linear maps. This representation supports reasoning with matrix inequalities and leverages linear-programming tools for efficient solution and inference, applicable to non-polynomial bounds.

Result: Experimental evaluation using a prototype implementation demonstrates that linear map inference is exponentially more efficient than the current state-of-the-art algorithm for cases where it applies.

Conclusion: The proposed linear map method enables more efficient inference of cost-free types in AARA, broadens the method’s applicability beyond polynomial bounds, and achieves significantly improved performance compared to existing approaches.

Abstract: The Automatic Amortized Resource Analysis (AARA) derives program-execution
cost bounds using types. To do so, AARA often makes use of cost-free types,
which are critical for the composition of types and cost bounds. However,
inferring cost-free types using the current state-of-the-art algorithm is
expensive due to recursive dependence on additional cost-free types.
Furthermore, that algorithm uses a heuristic only applicable to polynomial cost
bounds, and not, e.g., exponential bounds. This paper presents a new approach
to these problems by representing the cost-free types of a function in a new
way: with a linear map, which can stand for infinitely many cost-free types.
Such maps enable an algebraic flavor of reasoning about cost bounds (including
non-polynomial bounds) via matrix inequalities. These inequalities can be
solved with off-the-shelf linear-programming tools for many programs, so that
types can always be efficiently checked and often be efficiently inferred. An
experimental evaluation with a prototype implementation shows that-when it is
applicable-the inference of linear maps is exponentially more efficient than
the state-of-the-art algorithm.

</details>


### [37] [Local Success Does Not Compose: Benchmarking Large Language Models for Compositional Formal Verification](https://arxiv.org/abs/2509.23061)
*Xu Xu,Xin Li,Xingwei Qu,Jie Fu,Binhang Yuan*

Main category: cs.PL

TL;DR: The paper presents DafnyCOMP, a benchmark for testing LLMs on complex, multi-function programming tasks in Dafny. LLMs struggle significantly more with compositional code generation than with single-function tasks, highlighting a key area for future improvement.


<details>
  <summary>Details</summary>
Motivation: Previous benchmarks for language models mainly evaluated single-function tasks, which do not reflect the complexity of real-world programming involving multiple interacting functions. The paper is motivated by the need for a benchmark that tests LLMs on compositional reasoning.

Method: The authors introduce DafnyCOMP, a benchmark comprising 300 automatically synthesized multi-function Dafny programs. LLMs are evaluated on their ability to generate correct and verifiable specifications for these compositional tasks. Performance is compared to single-function benchmarks to assess the drop in effectiveness.

Result: State-of-the-art LLMs perform comparatively well on single-function verification, but their performance significantly decreases on compositional specification tasks. The analysis shows systematic problems in cross-functional reasoning, such as fragile specifications and misalignments between code and proofs.

Conclusion: DafnyCOMP exposes current limitations of LLMs in compositional code generation and serves as a diagnostic tool to track improvements in reliable and verifiable code synthesis across multiple components.

Abstract: We introduce DafnyCOMP, a benchmark for evaluating large language models
(LLMs) on compositional specification generation in Dafny. Unlike prior
benchmarks that focus on single-function tasks, DafnyCOMP targets programs
composed of multiple interacting functions with data dependencies, requiring
reasoning across component boundaries. The benchmark consists of 300
automatically synthesized multi-function programs. We evaluate several
state-of-the-art LLM families and find that, while they perform well on
single-function verification, their performance drops sharply on compositional
tasks. Analysis reveals systematic failures in cross-functional reasoning,
including fragile specifications, misalignment between implementations and
proofs, and unstable reasoning. DafnyCOMP thus provides a diagnostic tool for
measuring progress toward reliable, verifiable, and compositional code
generation with LLMs.

</details>


### [38] [Fine-Grained Reasoning About Container-Internal Pointers with Logical Pinning](https://arxiv.org/abs/2509.23229)
*Yawen Guan,Clément Pit-Claudel*

Main category: cs.PL

TL;DR: Logical pinning is a lightweight borrowing model for separation logic, allowing fine-grained pointer tracking for more precise program verification. It's practical, compatible with existing logics, proven via mechanization, and improves or subsumes existing specification and proof methods.


<details>
  <summary>Details</summary>
Motivation: Traditional separation logics obscure container-internal pointers for modularity, which complicates API specifications that temporarily expose those pointers and hinders verification of programs that use such APIs.

Method: The authors introduce 'logical pinning', a borrowing model generalizing the magic-wand operator, and mechanize their approach in the Rocq proof assistant using CFML. They verify small pointer-manipulating programs as case studies.

Result: Logical pinning subsumes standard proof patterns, simplifies complex proofs, and supports reasoning about program behaviors previously unsupported by traditional methods. It yields more precise container specifications and has been successfully mechanized.

Conclusion: Logical pinning enables more precise and flexible specification and verification of pointer-manipulating programs by allowing selective tracking of container-internal pointers, and is compatible with most separation logic variants.

Abstract: Most separation logics hide container-internal pointers for modularity. This
makes it difficult to specify container APIs that temporarily expose those
pointers to the outside, and to verify programs that use these APIs. We present
logical pinning, a lightweight borrowing model for sequential programs that
allows users to selectively track container-internal pointers at the logical
level. Our model generalizes the magic-wand operator, making it easy to write
and prove precise specifications, including pointer-stability properties.
Because it only changes how representation predicates and specifications are
written, our approach is compatible with most separation logic variants. We
demonstrate the practicality of logical pinning by verifying small but
representative pointer-manipulating programs, and deriving more precise
versions of common container specifications. In doing so, we show that our
approach subsumes some well-known proof patterns, simplifies some complex
proofs, and enables reasoning about program patterns not supported by
traditional specifications. All of our results are mechanized in the Rocq proof
assistant, using the CFML library.

</details>


### [39] [From Affine to Polynomial: Synthesizing Loops with Branches via Algebraic Geometry](https://arxiv.org/abs/2509.25114)
*Erdenebayar Bayarmagnai,Fatemeh Mohammadi,Rémi Prébet*

Main category: cs.PL

TL;DR: This paper tackles the challenge of generating polynomial invariants for software loops by instead synthesizing loops from those invariants, using tools from algebraic geometry and SMT solving. Their approach generalizes past work to allow complex guard conditions and update maps, offers a more efficient method for a new invariant class, and enables automated synthesis of nondeterministic branching loops relevant to program verification.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the fundamental challenge in ensuring software correctness through formal program verification. Specifically, the difficulty lies in generating polynomial invariants for loops, a crucial but generally undecidable task in loop analysis. Existing methods have limitations as they only address affine loops without guard conditions, motivating the need for a more general and robust approach.

Method: The paper introduces an approach that uses algebraic geometry tools to design and implement an algorithm for synthesizing loops based on given polynomial invariants. It allows loops to have polynomial update maps, inequations in the guard condition, and invariants of arbitrary polynomial form. The algorithm computes a finite set of polynomial equations to characterize all nondeterministic branching loops that satisfy the invariants. Additionally, a new class of invariants is defined, for which a more efficient algorithm is presented. The synthesis problem is ultimately reduced to solving multivariate polynomial systems with rational entries, which is handled using an SMT solver in their software.

Result: The authors successfully designed and implemented an algorithm capable of characterizing and synthesizing a broad class of nondeterministic branching loops from arbitrary polynomial invariants and guard conditions. They introduced a new class of invariants enabling more efficient synthesis, and demonstrated their method can solve the problem by reducing it to multivariate polynomial systems solvable by SMT solvers.

Conclusion: The paper extends invariant-based loop synthesis from restricted cases to general polynomial invariants and guards, enabling automated synthesis of complex loops. The use of algebraic geometry and SMT solvers makes the approach both general and computationally feasible, contributing significantly to formal program verification.

Abstract: Ensuring software correctness remains a fundamental challenge in formal
program verification. One promising approach relies on finding polynomial
invariants for loops. Polynomial invariants are properties of a program loop
that hold before and after each iteration. Generating such invariants is a
crucial task in loop analysis, but it is undecidable in the general case.
Recently, an alternative approach to this problem has emerged, focusing on
synthesizing loops from invariants. However, existing methods only synthesize
affine loops without guard conditions from polynomial invariants. In this
paper, we address a more general problem, allowing loops to have polynomial
update maps with a given structure, inequations in the guard condition, and
polynomial invariants of arbitrary form.
  We use algebraic geometry tools to design and implement an algorithm that
computes a finite set of polynomial equations whose solutions correspond to all
nondeterministic branching loops satisfying the given invariants. Furthermore,
we introduce a new class of invariants for which we present a significantly
more efficient algorithm. In other words, we reduce the problem of synthesizing
loops to find solutions of multivariate polynomial systems with rational
entries. This final step is handled in our software using an SMT solver.

</details>
