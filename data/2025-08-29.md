<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce Formula Generators](https://arxiv.org/abs/2508.20340)
*Maolin Sun,Yibiao Yang,Yuming Zhou*

Main category: cs.SE

TL;DR: Chimera uses LLMs to extract grammars and generate reusable formula generators for SMT solver testing, ensuring valid, diverse tests with minimal computational cost; it discovered 43 real bugs in leading solvers.


<details>
  <summary>Details</summary>
Motivation: SMT solvers are crucial in modern programming and verification, demanding correctness and robustness. Prior testing approaches struggle with the pace of solver advancement and robust test generation.

Method: Chimera, an LLM-assisted fuzzing framework, synthesizes CFGs from documentation and generates reusable Boolean term generators that ensure syntactically valid and semantically diverse test formulas. The LLM is only used once to minimize runtime overhead. Chimera populates skeletons from existing formulas with terms from these generators.

Result: Chimera identified 43 confirmed bugs in Z3 and cvc5 SMT solvers, 40 of which have been fixed.

Conclusion: Chimera is an effective, scalable approach to generating high-quality, valid formulas for SMT solver testing, combining the strengths of LLMs and grammar synthesis, and significantly reducing computational cost while uncovering real bugs.

Abstract: Satisfiability Modulo Theory (SMT) solvers are foundational to modern systems
and programming languages research, providing the foundation for tasks like
symbolic execution and automated verification. Because these solvers sit on the
critical path, their correctness is essential, and high-quality test formulas
are key to uncovering bugs. However, while prior testing techniques performed
well on earlier solver versions, they struggle to keep pace with rapidly
evolving features. Recent approaches based on Large Language Models (LLMs) show
promise in exploring advanced solver capabilities, but two obstacles remain:
nearly half of the generated formulas are syntactically invalid, and iterative
interactions with the LLMs introduce substantial computational overhead. In
this study, we present Chimera, a novel LLM-assisted fuzzing framework that
addresses both issues by shifting from direct formula generation to the
synthesis of reusable term (i.e., logical expression) generators. Particularly,
Chimera uses LLMs to (1) automatically extract context-free grammars (CFGs) for
SMT theories, including solver-specific extensions, from documentation, and (2)
synthesize composable Boolean term generators that adhere to these grammars.
During fuzzing, Chimera populates structural skeletons derived from existing
formulas with the terms iteratively produced by the LLM-synthesized generators.
This design ensures syntactic validity while promoting semantic diversity.
Notably, Chimera requires only one-time LLM interaction investment,
dramatically reducing runtime cost. We evaluated Chimera on two leading SMT
solvers: Z3 and cvc5. Our experiments show that Chimera has identified 43
confirmed bugs, 40 of which have already been fixed by developers.

</details>


### [2] [Evaluating LLMs on microservice-based applications: how complex is your specification?](https://arxiv.org/abs/2508.20119)
*Daniel M. Yellin*

Main category: cs.SE

TL;DR: LLMs can generate code for moderately complex microservices but fail on more difficult, real-world scenarios involving complex logic and integrations. The study identifies main challenges and suggests research directions to enhance LLM-based code synthesis.


<details>
  <summary>Details</summary>
Motivation: With the growing use of LLMs for code generation, it is crucial to assess their actual performance on complex, real-world programming tasks, especially in modern architectures like microservices. The motivation is to understand current limitations and drive future improvement.

Method: The authors defined a standard template for specifying microservice-based applications and proposed a metric to assess specification difficulty. They also developed an automated framework for testing LLM-generated code using unit tests, then ran experiments using various specification difficulties.

Result: Experimental results indicated that strong LLMs perform adequately on medium-difficulty tasks but struggle significantly with higher-difficulty specifications, which involve complex business logic, integration with external services, databases, and non-functional requirements like authentication.

Conclusion: LLMs currently face key challenges in synthesizing code for highly complex, real-world microservice specifications. Error analysis reveals the need for further research focused on overcoming obstacles in LLM code synthesis, particularly for applications with intricate requirements.

Abstract: In this paper we evaluate how far LLMs have advanced in generating code for
real-world problems. Specifically, we explore code synthesis for
microservice-based applications, a widely used architecture pattern. We define
a standard template for specifying these applications, and we propose a metric
for judging the difficulty level of a specification. The higher the score, the
more difficult it is to generate code for the specification. We develop a
framework to automate the process of testing LLM-synthesized code for a
microservice using unit tests. Our experimental results show that strong LLMs
(like GPT-3o-mini) do fairly well on medium difficulty specifications but do
very poorly on those of higher difficulty levels. This is due to more intricate
business logic, a greater use of external services, database integration and
inclusion of non-functional capabilities such as authentication. We analyzed
the errors in LLM-synthesized code and report on the key challenges LLMs face
in generating code for these specifications thereby suggesting future research
directions to improve code synthesis for real-world problems.

</details>


### [3] [Towards Better Correctness and Efficiency in Code Generation](https://arxiv.org/abs/2508.20124)
*Yunlong Feng,Yang Xu,Xiao Xu,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: They introduce a new reinforcement learning framework focused on code efficiency, identify key optimization bottlenecks, and propose solutions (dynamic exploration, error-insensitive RL, and a two-stage tuning method) that boost both correctness and speed of generated code.


<details>
  <summary>Details</summary>
Motivation: Current code-generating large language models produce code with poor runtime efficiency, making them less useful for performance-critical scenarios.

Method: They design a reinforcement learning framework with a novel reward for performance, utilizing dynamic exploration, robustness to errors, and a two-stage tuning approach for balanced optimization of correctness and efficiency.

Result: The proposed method improves code correctness by 10.18% and runtime efficiency by 7.75% on a 7B model, achieving performance comparable to much larger models.

Conclusion: The framework and tuning method effectively enhance code efficiency and correctness, making smaller models competitive with significantly larger ones for code generation tasks.

Abstract: While code large language models have demonstrated remarkable progress in
code generation, the generated code often exhibits poor runtime efficiency,
limiting its practical application in performance-sensitive scenarios. To
address this limitation, we propose an efficiency-oriented reinforcement
learning framework guided by a novel performance reward. Based on this
framework, we take a deeper dive into the code efficiency problem, identifying
then proposing methods to overcome key bottlenecks: (1) Dynamic exploration
overcomes the static data constraints of offline fine-tuning, enabling the
discovery of more efficient code implementations. (2) The error-insensitive
reinforcement learning method and high-contrast efficiency signals are crucial
for mitigating systematic errors and achieving effective optimization. (3)
Online exploration is most effective when starting from a high-correctness
baseline, as this allows for efficiency improvements without sacrificing
accuracy. With these discoveries, we finally propose a two-stage tuning method,
which achieves high and balanced performance across correctness and efficiency.
The results of experiments show the effectiveness of the method, which improves
code correctness by 10.18\% and runtime efficiency by 7.75\% on a 7B model,
achieving performance comparable to much larger model.

</details>


### [4] [Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought](https://arxiv.org/abs/2508.20370)
*Lingzhe Zhang,Tong Jia,Kangjin Wang,Weijie Hong,Chiming Duan,Minghua He,Ying Li*

Main category: cs.SE

TL;DR: Microservice failures are hard to diagnose with current methods. This paper presents RCLAgent, an LLM-driven, multi-agent system inspired by human SRE reasoning. It identifies root causes more accurately and efficiently, even from single requests, setting a new standard for reliability in microservices.


<details>
  <summary>Details</summary>
Motivation: Microservice systems are increasingly complex and prone to failures. Traditional root cause localization methods are either inflexible (relying on fixed schemas) or non-interpretable, leaving SREs confused. There is a need for a more adaptive, interpretable, and effective root cause localization technique.

Method: The paper proposes RCLAgent, an adaptive root cause localization method utilizing a multi-agent recursion-of-thought framework. It models the human SRE approach by employing recursive, multi-dimensional, and cross-modal reasoning, guiding a large language model (LLM) to synthesize information from different agents and automated tools.

Result: RCLAgent outperforms state-of-the-art methods on public datasets by localizing the root cause of failures using only a single request, rather than requiring aggregation over multiple requests. It improves both efficiency and accuracy of root cause localization.

Conclusion: RCLAgent offers an effective, interpretable, and adaptive approach to root cause localization in complex microservice systems by drawing on human reasoning strategies and leveraging advanced multi-agent LLM frameworks. It enables SREs to identify problems faster and more accurately.

Abstract: As contemporary microservice systems become increasingly popular and
complex-often comprising hundreds or even thousands of fine-grained,
interdependent subsystems-they are facing more frequent failures. Ensuring
system reliability thus demands accurate root cause localization. While traces
and metrics have proven to be effective data sources for this task, existing
methods either heavily rely on pre-defined schemas, which struggle to adapt to
evolving operational contexts, or lack interpretability in their reasoning
process, thereby leaving Site Reliability Engineers (SREs) confused. In this
paper, we conduct a comprehensive study on how SREs localize the root cause of
failures, drawing insights from multiple professional SREs across different
organizations. Our investigation reveals that human root cause analysis
exhibits three key characteristics: recursiveness, multi-dimensional expansion,
and cross-modal reasoning. Motivated by these findings, we introduce RCLAgent,
an adaptive root cause localization method for microservice systems that
leverages a multi-agent recursion-of-thought framework. RCLAgent employs a
novel recursion-of-thought strategy to guide the LLM's reasoning process,
effectively integrating data from multiple agents and tool-assisted analysis to
accurately pinpoint the root cause. Experimental evaluations on various public
datasets demonstrate that RCLAgent achieves superior performance by localizing
the root cause using only a single request-outperforming state-of-the-art
methods that depend on aggregating multiple requests. These results underscore
the effectiveness of RCLAgent in enhancing the efficiency and precision of root
cause localization in complex microservice environments.

</details>


### [5] [AI and Agile Software Development: A Research Roadmap from the XP2025 Workshop](https://arxiv.org/abs/2508.20563)
*Zheying Zhang,Tomas Herda,Victoria Pichler,Pekka Abrahamsson,Geir K. Hanssen,Joshua Kerievsky,Alex Polyakov,Mohit Chandna,Marius Irgens,Kai-Kristian Kemell,Ayman Asad Khan,Crystal Kwok,Evan Leybourn,Munish Malik,Dorota Mleczko,Morteza Moalagh,Christopher Morales,Yuliia Pieskova,Daniel Planötscher,Mika Saari,Anastasiia Tkalich,Karl Josef Gstettner,Xiaofeng Wang*

Main category: cs.SE

TL;DR: A workshop brought together experts from academia and industry to identify and analyze the challenges of incorporating Generative AI into agile software development. Key pain points were discussed, and a research roadmap was created to drive responsible, human-centered integration of GenAI in agile practices.


<details>
  <summary>Details</summary>
Motivation: There is a growing intersection between Generative Artificial Intelligence (GenAI) and agile software development, accompanied by numerous practical challenges, such as tool fragmentation, governance, data quality issues, and skills gaps related to AI literacy and prompt engineering.

Method: The researchers conducted a full-day workshop with over 30 academic researchers and industry practitioners, using structured, interactive breakout sessions to identify, analyze, and collaboratively address pain points and opportunities at the intersection of GenAI and agile development.

Result: The workshop resulted in the identification of key challenges (e.g., tool fragmentation, governance, data quality, skills gaps), analysis of underlying causes, and the co-creation of a multi-thematic research roadmap with both short-term and long-term actions for integrating GenAI into agile practices.

Conclusion: A collaboratively developed research agenda was established, guiding responsible and human-centered integration of GenAI into agile software development, addressing both immediate and future needs in the field.

Abstract: This paper synthesizes the key findings from a full-day XP2025 workshop on
"AI and Agile: From Frustration to Success", held in Brugg-Windisch,
Switzerland. The workshop brought together over 30 interdisciplinary academic
researchers and industry practitioners to tackle the concrete challenges and
emerging opportunities at the intersection of Generative Artificial
Intelligence (GenAI) and agile software development. Through structured,
interactive breakout sessions, participants identified shared pain points like
tool fragmentation, governance, data quality, and critical skills gaps in AI
literacy and prompt engineering. These issues were further analyzed, revealing
underlying causes and cross-cutting concerns. The workshop concluded by
collaboratively co-creating a multi-thematic research roadmap, articulating
both short-term, implementable actions and visionary, long-term research
directions. This cohesive agenda aims to guide future investigation and drive
the responsible, human-centered integration of GenAI into agile practices.

</details>


### [6] [Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol](https://arxiv.org/abs/2508.20737)
*Wei Ma,Yixiao Yang,Qiang Hu,Shi Ying,Zhi Jin,Bo Du,Zhenchang Xing,Tianlin Li,Junjie Shi,Yang Liu,Linxiao Jiang*

Main category: cs.SE

TL;DR: LLM applications require new approaches to quality assurance due to their complexity and dynamic behavior. This paper analyzes traditional vs. AI testing methods, presents a layered architecture, introduces practical strategies, and proposes a standardized protocol (AICL) to facilitate robust and trustworthy testing.


<details>
  <summary>Details</summary>
Motivation: As large language model (LLM) applications grow more complex, integrating retrieval, tool use, and multi-turn interactions, they present unique challenges for quality assurance that traditional software testing methods may not address. This motivates a systematic investigation of how to ensure the reliability and safety of such systems.

Method: The paper decomposes LLM applications into a three-layer architecture and analyzes the applicability of traditional software testing methods at each layer. It conducts a comparative analysis between software engineering testing and AI safety techniques, identifies structural disconnects and core challenges, and proposes collaborative strategies and a new protocol (AICL) to standardize testing for LLM applications.

Result: The analysis identifies four fundamental differences that lead to six core testing challenges in LLM systems. Four collaborative strategies—Retain, Translate, Integrate, and Runtime—are proposed to bridge the identified gaps. Additionally, the paper introduces the Agent Interaction Communication Language (AICL) protocol to facilitate test-oriented, agent-based communication and streamline standardization efforts.

Conclusion: A systematic approach is required to ensure the quality assurance of LLM-based applications, involving both adaptation and innovation in testing strategies. The proposed architecture analysis, protocol, and strategies aim to provide practical guidance and support the development of robust tooling and standards for LLM application testing.

Abstract: Applications of Large Language Models~(LLMs) have evolved from simple text
generators into complex software systems that integrate retrieval augmentation,
tool invocation, and multi-turn interactions. Their inherent non-determinism,
dynamism, and context dependence pose fundamental challenges for quality
assurance. This paper decomposes LLM applications into a three-layer
architecture: \textbf{\textit{System Shell Layer}}, \textbf{\textit{Prompt
Orchestration Layer}}, and \textbf{\textit{LLM Inference Core}}. We then assess
the applicability of traditional software testing methods in each layer:
directly applicable at the shell layer, requiring semantic reinterpretation at
the orchestration layer, and necessitating paradigm shifts at the inference
core. A comparative analysis of Testing AI methods from the software
engineering community and safety analysis techniques from the AI community
reveals structural disconnects in testing unit abstraction, evaluation metrics,
and lifecycle management. We identify four fundamental differences that
underlie 6 core challenges. To address these, we propose four types of
collaborative strategies (\emph{Retain}, \emph{Translate}, \emph{Integrate},
and \emph{Runtime}) and explore a closed-loop, trustworthy quality assurance
framework that combines pre-deployment validation with runtime monitoring.
Based on these strategies, we offer practical guidance and a protocol proposal
to support the standardization and tooling of LLM application testing. We
propose a protocol \textbf{\textit{Agent Interaction Communication Language}}
(AICL) that is used to communicate between AI agents. AICL has the
test-oriented features and is easily integrated in the current agent framework.

</details>


### [7] [From Law to Gherkin: A Human-Centred Quasi-Experiment on the Quality of LLM-Generated Behavioural Specifications from Food-Safety Regulations](https://arxiv.org/abs/2508.20744)
*Shabnam Hassani,Mehrdad Sabetzadeh,Daniel Amyot*

Main category: cs.SE

TL;DR: The paper shows that large language models can effectively automate the conversion of legal requirements into developer-friendly software specifications, reducing manual effort and maintaining high quality as judged by human evaluators.


<details>
  <summary>Details</summary>
Motivation: Legal requirements affect software design, but translating legal language into actionable software specifications is difficult, error-prone, and requires expertise. Manual methods are costly and slow. Advances in Generative AI offer new automation opportunities.

Method: The authors conducted a systematic human-subject study using a quasi-experimental design. Ten participants evaluated 60 Gherkin-style behavioral specifications generated by two LLMs (Claude and Llama) from food-safety legal texts. Each participant assessed 12 specifications against five criteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Mann-Whitney U tests were used for statistical analysis.

Result: Most specification ratings were in the top two categories for all criteria. Mann-Whitney U tests indicated no statistically significant differences across participants or models, though Llama slightly surpassed Claude in most criteria except Singularity. Feedback mentioned some hallucinations and omissions, but overall, the generated specifications were considered valuable.

Conclusion: LLMs are capable of producing high-quality, structured Gherkin specifications from legal texts. This can reduce manual work and assist in requirements engineering, software assurance, and test planning.

Abstract: Context: Laws and regulations increasingly affect software design and quality
assurance, but legal texts are written in technology-neutral language. This
creates challenges for engineers who must develop compliance artifacts such as
requirements and acceptance criteria. Manual creation is labor-intensive,
error-prone, and requires domain expertise. Advances in Generative AI (GenAI),
especially Large Language Models (LLMs), offer a way to automate deriving such
artifacts.
  Objective: We present the first systematic human-subject study of LLMs'
ability to derive behavioral specifications from legal texts using a
quasi-experimental design. These specifications translate legal requirements
into a developer-friendly form.
  Methods: Ten participants evaluated specifications generated from food-safety
regulations by Claude and Llama. Using Gherkin, a structured BDD language, 60
specifications were produced. Each participant assessed 12 across five
criteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Each
specification was reviewed by two participants, yielding 120 assessments.
  Results: For Relevance, 75% of ratings were highest and 20% second-highest.
Clarity reached 90% highest. Completeness: 75% highest, 19% second.
Singularity: 82% highest, 12% second. Time Savings: 68% highest, 24% second. No
lowest ratings occurred. Mann-Whitney U tests showed no significant differences
across participants or models. Llama slightly outperformed Claude in Clarity,
Completeness, and Time Savings, while Claude was stronger in Singularity.
Feedback noted hallucinations and omissions but confirmed the utility of the
specifications.
  Conclusion: LLMs can generate high-quality Gherkin specifications from legal
texts, reducing manual effort and providing structured artifacts useful for
implementation, assurance, and test generation.

</details>


### [8] [Towards an Architectural Perspective for Sustainability: Bundle the Needs from Industry](https://arxiv.org/abs/2508.20774)
*Markus Funke,Patricia Lago*

Main category: cs.SE

TL;DR: The paper introduces a framework to help software architects address sustainability during design by using tailored architectural perspectives. Through literature review and expert feedback, the approach is validated and shown to meet real-world industry needs.


<details>
  <summary>Details</summary>
Motivation: Sustainability is becoming an important quality in software systems, but software architects lack structured methods to address it during design. The authors aim to provide a framework to support sustainability considerations independently of existing architecture frameworks and across different contexts.

Method: The authors develop a 'sustainability perspective vision'—a framework based on architectural perspectives comprised of concerns, activities, tactics, pitfalls, and checklists. They use literature snowballing and a focus group with domain experts to gather evidence and refine the approach.

Result: The study confirms that the elements of the proposed architectural perspective are relevant and applicable in practice. Empirical findings highlight important considerations for adapting the perspective to industrial needs.

Conclusion: A specialized sustainability perspective can help systematically address sustainability in software architecture design. The approach is validated by expert input and literature, showing both its relevance and the practical implications of adopting such a perspective.

Abstract: Sustainability is increasingly recognized as an emerging quality property in
software-intensive systems, yet architects lack structured guidance to address
it effectively throughout the software design phase. Architectural
perspectives-an architectural knowledge artifact composed of concerns,
activities, tactics, pitfalls, and checklists-offer a promising approach to
tackle such emerging quality properties across architectural views and are also
independent of architecture frameworks and industry contexts. In this paper, we
present a sustainability perspective vision, i.e., a revised notion of
architectural perspective meant to be filled with its own elements to target
sustainability concerns. We formulate our sustainability perspective vision
through evidence from applying snowballing to seminal literature and from
conducting a focus group with experts in the field. Our findings confirm the
relevance of the different perspective elements in practice and highlight
implications for shaping a sustainability perspective that meets industrial
needs.

</details>


### [9] [Automated Test Oracles for Flaky Cyber-Physical System Simulators: Approach and Evaluation](https://arxiv.org/abs/2508.20902)
*Baharin A. Jodat,Khouloud Gaaloul,Mehrdad Sabetzadeh,Shiva Nejati*

Main category: cs.SE

TL;DR: Assertion-based test oracles, generated via genetic programming with Ochiai fitness, offer robust and accurate verdicts for CPS testing without needing simulator execution, thus reducing costs and overcoming simulator flakiness.


<details>
  <summary>Details</summary>
Motivation: Simulation-based testing of cyber-physical systems (CPS) is resource-intensive due to the slow execution and unreliability of simulators, which leads to high costs and inconsistent test outcomes. There is a need for automated test oracles that do not require actual system execution, are interpretable, and robust to simulator flakiness.

Method: The authors propose assertion-based test oracles, which are logical and arithmetic predicates over system inputs to predict test results without running the system. They introduce two methods for generating these oracles: (1) using genetic programming (GP) with spectrum-based fault localization (SBFL) formulas (Ochiai, Tarantula, Naish) as fitness functions; (2) using decision trees (DT) and decision rules (DR).

Result: GP with the Ochiai formula produced assertion-based test oracles that are significantly more accurate than those generated with Tarantula, Naish, DT, or DR. This accuracy advantage was consistent even when the CPS simulators exhibited flaky behavior. The accuracy varied by only 4% on average across different systems, showing robustness to simulator flakiness.

Conclusion: Assertion-based test oracles generated using GP and the Ochiai ranking formula are both accurate and robust for CPS testing, reducing the need for costly and unreliable simulation runs while being interpretable and resilient to simulator flakiness.

Abstract: Simulation-based testing of cyber-physical systems (CPS) is costly due to the
time-consuming execution of CPS simulators. In addition, CPS simulators may be
flaky, leading to inconsistent test outcomes and requiring repeated test
re-execution for reliable test verdicts. Automated test oracles that do not
require system execution are therefore crucial for reducing testing costs.
Ideally, such test oracles should be interpretable to facilitate human
understanding of test verdicts, and they must be robust against the potential
flakiness of CPS simulators. In this article, we propose assertion-based test
oracles for CPS as sets of logical and arithmetic predicates defined over the
inputs of the system under test. Given a test input, our assertion-based test
oracle determines, without requiring test execution, whether the test passes,
fails, or if the oracle is inconclusive in predicting a verdict. We describe
two methods for generating assertion-based test oracles: one using genetic
programming~(GP) that employs well-known spectrum-based fault localization
(SBFL) ranking formulas, namely Ochiai, Tarantula, and Naish, as fitness
functions; and the other using decision trees (DT) and decision rules (DR). We
evaluate our assertion-based test oracles through case studies in the domains
of aerospace, networking and autonomous driving. We show that test oracles
generated using GP with Ochiai are significantly more accurate than those
obtained using GP with Tarantula and Naish or using DT or DR. Moreover, this
accuracy advantage remains even when accounting for the flakiness of the system
under test. We further show that the assertion-based test oracles generated by
GP with Ochiai are robust against flakiness with only 4% average variation in
their accuracy results across four different network and autonomous driving
systems with flaky behaviours.

</details>


### [10] [Deep Learning Based Concurrency Bug Detection and Localization](https://arxiv.org/abs/2508.20911)
*Zuocheng Feng,Kaiwen Zhang,Miaomiao Wang,Yiming Cheng,Yuandao Cai,Xiaofeng Li,Guanjun Liu*

Main category: cs.SE

TL;DR: The paper introduces a new deep learning approach for concurrency bug detection and localization using a special code graph and interpretability tools. It provides a new dataset, improved bug detection accuracy, and precise localization—outperforming current methods.


<details>
  <summary>Details</summary>
Motivation: Concurrency bugs in multi-threaded or distributed systems are hard to detect and undermine reliability and security. Existing deep learning methods suffer from the lack of large, dedicated datasets, insufficient representation of concurrency semantics, and fail to provide fine-grained localization of bugs.

Method: The authors create a dedicated concurrency bug dataset and propose a new method integrating a pre-trained model with a heterogeneous graph neural network using a Concurrency-Aware Code Property Graph (CCPG) for richer semantic representation. SubgraphX, a GNN-based interpretability technique, is used for precise bug localization at the code line level.

Result: The proposed approach outperforms state-of-the-art methods with a 10% improvement in accuracy and precision, and a 26% increase in recall across various evaluations.

Conclusion: The novel method effectively detects and localizes concurrency bugs by addressing dataset, representation, and interpretability limitations, thereby enhancing debugging capabilities and outperforming prior work.

Abstract: Concurrency bugs, caused by improper synchronization of shared resources in
multi-threaded or distributed systems, are notoriously hard to detect and thus
compromise software reliability and security. The existing deep learning
methods face three main limitations. First, there is an absence of large and
dedicated datasets of diverse concurrency bugs for them. Second, they lack
sufficient representation of concurrency semantics. Third, binary
classification results fail to provide finer-grained debug information such as
precise bug lines. To address these problems, we propose a novel method for
effective concurrency bug detection as well as localization. We construct a
dedicated concurrency bug dataset to facilitate model training and evaluation.
We then integrate a pre-trained model with a heterogeneous graph neural network
(GNN), by incorporating a new Concurrency-Aware Code Property Graph (CCPG) that
concisely and effectively characterizes concurrency semantics. To further
facilitate debugging, we employ SubgraphX, a GNN-based interpretability method,
which explores the graphs to precisely localize concurrency bugs, mapping them
to specific lines of source code. On average, our method demonstrates an
improvement of 10\% in accuracy and precision and 26\% in recall compared to
state-of-the-art methods across diverse evaluation settings.

</details>


### [11] [ConfLogger: Enhance Systems' Configuration Diagnosability through Configuration Logging](https://arxiv.org/abs/2508.20977)
*Shiwen Shan,Yintong Huo,Yuxin Su,Zhining Wang,Dan Li,Zibin Zheng*

Main category: cs.SE

TL;DR: The paper presents ConfLogger, a tool combining static analysis and LLM-powered log generation to improve diagnosing configuration errors. Testing on multiple systems shows dramatic gains in error localization accuracy, coverage, and user troubleshooting speed compared to previous methods.


<details>
  <summary>Details</summary>
Motivation: Modern configurable systems enable extensive customization but come with increased risks of misconfigurations and software bugs. Existing diagnosability approaches primarily focus on analyzing software after failures occur, and do not consider whether the system logs provide enough information for diagnosing configuration problems.

Method: The paper introduces ConfLogger, a new tool that integrates configuration-aware static taint analysis with LLM-based log generation. The approach first identifies configuration-sensitive code through project-wide data flow tracing, then generates targeted diagnostic log statements by examining the context of configuration-relevant code.

Result: ConfLogger was evaluated on eight popular software systems. Results show that logs produced by ConfLogger enable a log-based misconfiguration diagnosis tool to localize errors with 100% accuracy in 30 silent misconfiguration scenarios, 80% of which were directly resolvable via the explicit configuration information exposed. ConfLogger covers 74% of existing logging points, outperforming baseline LLM-based loggers by 12% and 30%, and surpasses the current best method by 8.6% in precision, 79.3% in recall, and 26.2% in F1 for variable logging. A controlled user study demonstrated 1.25x faster diagnosis and a 251.4% improvement in troubleshooting accuracy.

Conclusion: ConfLogger significantly enhances the diagnosability of configuration-related issues in configurable software systems. By automatically exposing configuration-relevant information in logs, it considerably improves the effectiveness and accuracy of both automated and manual misconfiguration diagnosis compared to existing solutions.

Abstract: Modern configurable systems offer customization via intricate configuration
spaces, yet such flexibility introduces pervasive configuration-related issues
such as misconfigurations and latent softwarebugs. Existing diagnosability
supports focus on post-failure analysis of software behavior to identify
configuration issues, but none of these approaches look into whether the
software clue sufficient failure information for diagnosis. To fill in the
blank, we propose the idea of configuration logging to enhance existing logging
practices at the source code level. We develop ConfLogger, the first tool that
unifies configuration-aware static taint analysis with LLM-based log generation
to enhance software configuration diagnosability. Specifically, our method 1)
identifies configuration-sensitive code segments by tracing
configuration-related data flow in the whole project, and 2) generates
diagnostic log statements by analyzing configuration code contexts. Evaluation
results on eight popular software systems demonstrate the effectiveness of
ConfLogger to enhance configuration diagnosability. Specifically,
ConfLogger-enhanced logs successfully aid a log-based misconfiguration
diagnosis tool to achieve 100% accuracy on error localization in 30 silent
misconfiguration scenarios, with 80% directly resolvable through explicit
configuration information exposed. In addition, ConfLogger achieves 74%
coverage of existing logging points, outperforming baseline LLM-based loggers
by 12% and 30%. It also gains 8.6% higher in precision, 79.3% higher in recall,
and 26.2% higher in F1 compared to the state-of-the-art baseline in terms of
variable logging while also augmenting diagnostic value. A controlled user
study on 22 cases further validated its utility, speeding up diagnostic time by
1.25x and improving troubleshooting accuracy by 251.4%.

</details>


### [12] [Dynamics of Gender Bias in Software Engineering](https://arxiv.org/abs/2508.21050)
*Thomas J. Misa*

Main category: cs.SE

TL;DR: This paper reviews software engineering's history and gender bias, finds substantial evidence of women's exclusion in conference authorship, and calls for policy responses to gender inequity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to investigate the presence of gender biases within software engineering, especially given its roots in both engineering and computer science, where such biases may already exist.

Method: The paper surveys the historical development of software engineering, examines literature addressing professionalism and gender bias, profiles five leaders, and quantitatively analyzes the participation of women as research authors at the International Conference of Software Engineering from 1976 to 2010.

Result: The analysis found a dozen years with statistically significant gender exclusion among research authors at the conference, indicating persistent issues of gender bias in the field.

Conclusion: Gender bias has been a recurring issue in software engineering, as shown by historical and quantitative evidence; the paper suggests considering policy interventions to address these biases.

Abstract: The field of software engineering is embedded in both engineering and
computer science, and may embody gender biases endemic to both. This paper
surveys software engineering's origins and its long-running attention to
engineering professionalism, profiling five leaders; it then examines the
field's recent attention to gender issues and gender bias. It next
quantitatively analyzes women's participation as research authors in the
field's leading International Conference of Software Engineering (1976-2010),
finding a dozen years with statistically significant gender exclusion. Policy
dimensions of research on gender bias in computing are suggested.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [13] [Solvable Tuple Patterns and Their Applications to Program Verification](https://arxiv.org/abs/2508.20365)
*Naoki Kobayashi,Ryosuke Sato,Ayumi Shinohara,Ryo Yoshinaka*

Main category: cs.PL

TL;DR: This paper introduces STPs, a new method for efficiently inferring program invariants for list-like data structures using only positive samples. Integrating STP inference into a CHC solver greatly boosts verification performance, achieving state-of-the-art results in competition.


<details>
  <summary>Details</summary>
Motivation: Automated verification of programs that manipulate recursive data structures, such as lists, is still difficult, especially when trying to infer invariants efficiently and without requiring negative samples.

Method: The paper introduces 'solvable tuple patterns' (STPs) to express invariants of recursive data structures. It presents their basic properties, an algorithm to infer STPs from positive samples, and a method to validate STPs using SMT solvers supporting sequence theory. The approach is integrated into a CHC solver.

Result: The approach enables efficient inference of invariants for list-like data structures; only positive samples are needed. The enhanced CHC solver with STP inference achieved superior performance, winning the ADT-LIN category of CHC-COMP 2025 by a large margin.

Conclusion: STPs provide an effective way to infer and utilize inductive invariants for recursive data structures, improving the automation and efficacy of program verification tools.

Abstract: Despite the recent progress of automated program verification techniques,
fully automated verification of programs manipulating recursive data structures
remains a challenge. We introduce the notion of solvable tuple patterns (STPs)
to express invariants between list-like recursive data structures. A
distinguishing feature of STPs is that they can be efficiently inferred from
only a small number of positive samples; no negative samples are required. An
SMT solver that supports the sequence theory can be used to check that an
inferred STP is indeed an inductive invariant. After presenting basic
properties of STPs and an STP inference algorithm, we show how to incorporate
the STP inference into a CHC (Constrained Horn Clauses) solver supporting
list-like data structures, which serves as a uniform backend for automated
program verification tools. A CHC solver incorporating the STP inference has
won the ADT-LIN category of CHC-COMP 2025 by a big margin.

</details>


### [14] [Static Factorisation of Probabilistic Programs With User-Labelled Sample Statements and While Loops](https://arxiv.org/abs/2508.20922)
*Markus Böck,Jürgen Cito*

Main category: cs.PL

TL;DR: This paper extends graphical modeling to probabilistic programs with loops and dynamic sample statements, allowing static analysis and new optimizations, which are theoretically sound and shown to improve performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: While any Bayesian network can be implemented as a probabilistic program, the reverse is not clear—especially for programs with loops and dynamic features. The motivation is to bridge this gap by providing a way to represent such probabilistic programs graphically, thus allowing static analysis and optimization.

Method: They extend operational semantics to handle user-labelled sample statements and while loops in probabilistic programming languages. Then, they translate programs into control-flow graphs, enabling static analysis of random variable dependencies. They also develop a static program slicing technique for optimization purposes.

Result: A novel static graphical representation for probabilistic programs with loops or dynamic features is introduced, generalizing Bayesian networks. The proposed optimizations are shown to be both sound and practically beneficial, matching or surpassing current methods in empirical tests.

Conclusion: The paper shows that probabilistic programs with loops and dynamic labels can be statically analyzed to obtain a graphical representation that extends classic Bayesian network structures. This allows for the application of three static optimizations—variance reduction in variational inference, faster single-site Metropolis Hastings, and more efficient sequential Monte Carlo—which are proven and empirically validated.

Abstract: It is commonly known that any Bayesian network can be implemented as a
probabilistic program, but the reverse direction is not so clear. In this work,
we address the open question to what extent a probabilistic program with
user-labelled sample statements and while loops - features found in languages
like Gen, Turing, and Pyro - can be represented graphically. To this end, we
extend existing operational semantics to support these language features. By
translating a program to its control-flow graph, we define a sound static
analysis that approximates the dependency structure of the random variables in
the program. As a result, we obtain a static factorisation of the implicitly
defined program density, which is equivalent to the known Bayesian network
factorisation for programs without loops and constant labels, but constitutes a
novel graphical representation for programs that define an unbounded number of
random variables via loops or dynamic labels. We further develop a sound
program slicing technique to leverage this structure to statically enable three
well-known optimisations for the considered program class: we reduce the
variance of gradient estimates in variational inference and we speed up both
single-site Metropolis Hastings and sequential Monte Carlo. These optimisations
are proven correct and empirically shown to match or outperform existing
techniques.

</details>
