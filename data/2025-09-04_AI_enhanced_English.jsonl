{"id": "2509.03318", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2509.03318", "abs": "https://arxiv.org/abs/2509.03318", "authors": ["Eduard Kamburjan", "Vidar Norstein Klungre", "Yuanwei Qu", "Rudolf Schlatte", "Egor V. Kostylev", "Martin Giese", "Einar Broch Johnsen"], "title": "Semantically Reflected Programs", "comment": null, "summary": "This paper addresses the dichotomy between the formalization of structural\nand the formalization of behavioral knowledge by means of semantically lifted\nprograms, which explore an intuitive connection between programs and knowledge\ngraphs. While knowledge graphs and ontologies are eminently useful to represent\nformal knowledge about a system's individuals and universals, programming\nlanguages are designed to describe the system's evolution. To address this\ndichotomy, we introduce a semantic lifting of the program states of an\nexecuting program into a knowledge graph, for an object-oriented programming\nlanguage. The resulting graph is exposed as a semantic reflection layer within\nthe programming language, allowing programmers to leverage knowledge of the\napplication domain in their programs. In this paper, we formalize semantic\nlifting and semantic reflection for a small programming language, SMOL, explain\nthe operational aspects of the language, and consider type correctness and\nvirtualisation for runtime program queries through the semantic reflection\nlayer. We illustrate semantic lifting and semantic reflection through a case\nstudy of geological modelling and discuss different applications of the\ntechnique. The language implementation is open source and available online.", "AI": {"tldr": "This paper introduces, formalizes, and demonstrates 'semantic lifting'\u2014a technique to map program state into knowledge graphs, unifying structural and behavioral knowledge representation within programs. The approach is validated by a custom language, SMOL, and geological modeling case study, with open-source implementation provided.", "motivation": "There exists a dichotomy between structural knowledge (represented by knowledge graphs and ontologies) and behavioral knowledge (captured by programming languages). The motivation is to bridge this gap, enabling seamless integration and utilization of both knowledge types within software systems.", "method": "The paper introduces the concept of 'semantic lifting,' transforming program states in an object-oriented programming language into knowledge graphs. It provides a formalization of semantic lifting and semantic reflection in a custom language called SMOL. It then explains the operational behavior, considers type correctness and virtualization for runtime program queries, and demonstrates the approach through a geological modeling case study. The implementation is made available as open source.", "result": "The authors successfully formalize semantic lifting and reflection in the SMOL language, demonstrate its feasibility in practice with a case study, and provide an open-source implementation. The integration allows programmers to query and leverage structural (domain) knowledge in behavioral (programming) contexts.", "conclusion": "Semantic lifting provides a systematic way to bridge the dichotomy between structural and behavioral knowledge by combining the strengths of knowledge graphs and programming language semantics. This approach is practical, formally sound, and broadly applicable, as shown by a concrete case study and an open-source implementation."}}
{"id": "2509.02860", "categories": ["cs.SE", "cs.LO"], "pdf": "https://arxiv.org/pdf/2509.02860", "abs": "https://arxiv.org/abs/2509.02860", "authors": ["Connor Wojtak", "Darek Gajewski", "Tomas Cerny"], "title": "Vision: An Extensible Methodology for Formal Software Verification in Microservice Systems", "comment": "Accepted at MODELS 2025", "summary": "Microservice systems are becoming increasingly adopted due to their\nscalability, decentralized development, and support for continuous integration\nand delivery (CI/CD). However, this decentralized development by separate teams\nand continuous evolution can introduce miscommunication and incompatible\nimplementations, undermining system maintainability and reliability across\naspects from security policy to system architecture. We propose a novel\nmethodology that statically reconstructs microservice source code into a formal\nsystem model. From this model, a Satisfiability Modulo Theories (SMT)\nconstraint set can be derived, enabling formal verification. Our methodology is\nextensible, supporting software verification across multiple cross-cutting\nconcerns. We focus on applying the methodology to verify the system\narchitecture concern, presenting formal reasoning to validate the methodology's\ncorrectness and applicability for this concern. Additional concerns such as\nsecurity policy implementation are considered. Future directions are\nestablished to extend and evaluate the methodology.", "AI": {"tldr": "The paper introduces a static analysis methodology for reconstructing microservice source code into a formal model, enabling formal verification (via SMT) for system architecture and other concerns like security. This helps address maintainability and reliability challenges caused by decentralized development and supports further extensibility.", "motivation": "Microservices, while popular for their scalability, CI/CD support, and decentralized development, can suffer from miscommunication and incompatible implementations due to independent teams and continual evolution. This threatens maintainability and reliability.", "method": "The paper proposes a novel methodology that statically analyzes and reconstructs microservice source code into a formal system model. An SMT (Satisfiability Modulo Theories) constraint set is generated from this model, which is then used for formal verification of various cross-cutting concerns.", "result": "The methodology is demonstrated as extensible, capable of supporting formal verification across multiple concerns, with an emphasis on system architecture. Formal reasoning is provided to validate its correctness for this concern, and applicability to security policy implementation is noted.", "conclusion": "The proposed static reconstruction and verification approach can increase maintainability and reliability by systematically uncovering miscommunications and incompatibilities in microservice systems. The paper outlines future directions for expansion and evaluation of the methodology."}}
{"id": "2509.03093", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03093", "abs": "https://arxiv.org/abs/2509.03093", "authors": ["Fatih Pehlivan", "Ar\u00e7in \u00dclk\u00fc Erg\u00fczen", "Sahand Moslemi Yengejeh", "Mayasah Lami", "Anil Koyuncu"], "title": "Are We SOLID Yet? An Empirical Study on Prompting LLMs to Detect Design Principle Violations", "comment": "Accepted to ASE2025", "summary": "Traditional static analysis methods struggle to detect semantic design flaws,\nsuch as violations of the SOLID principles, which require a strong\nunderstanding of object-oriented design patterns and principles. Existing\nsolutions typically focus on individual SOLID principles or specific\nprogramming languages, leaving a gap in the ability to detect violations across\nall five principles in multi-language codebases. This paper presents a new\napproach: a methodology that leverages tailored prompt engineering to assess\nLLMs on their ability to detect SOLID violations across multiple languages. We\npresent a benchmark of four leading LLMs-CodeLlama, DeepSeekCoder, QwenCoder,\nand GPT-4o Mini-on their ability to detect violations of all five SOLID\nprinciples. For this evaluation, we construct a new benchmark dataset of 240\nmanually validated code examples. Using this dataset, we test four distinct\nprompt strategies inspired by established zero-shot, few-shot, and\nchain-of-thought techniques to systematically measure their impact on detection\naccuracy. Our emerging results reveal a stark hierarchy among models, with\nGPT-4o Mini decisively outperforming others, yet even struggles with\nchallenging principles like DIP. Crucially, we show that prompt strategy has a\ndramatic impact, but no single strategy is universally best; for instance, a\ndeliberative ENSEMBLE prompt excels at OCP detection while a hint-based EXAMPLE\nprompt is superior for DIP violations. Across all experiments, detection\naccuracy is heavily influenced by language characteristics and degrades sharply\nwith increasing code complexity. These initial findings demonstrate that\neffective, AI-driven design analysis requires not a single best model, but a\ntailored approach that matches the right model and prompt to the specific\ndesign context, highlighting the potential of LLMs to support maintainability\nthrough AI-assisted code analysis.", "AI": {"tldr": "This paper benchmarks multiple LLMs and prompt strategies for detecting SOLID design principle violations across several languages. Results show GPT-4o Mini leads but nobody model or prompt is best for all situations, and accuracy declines with code complexity. Tailoring model-prompt combinations to each context is crucial for effective, AI-driven code analysis and maintainability.", "motivation": "Traditional static analysis tools can hardly detect semantic design flaws related to the SOLID principles, especially in multi-language codebases. Existing research typically focuses on single principles or languages, leaving a significant gap in broad, effective detection across all SOLID principles.", "method": "The paper introduces a method that uses custom prompt engineering to evaluate large language models (LLMs) for their skill in detecting SOLID principle violations in code. The study benchmarks four LLMs (CodeLlama, DeepSeekCoder, QwenCoder, GPT-4o Mini) using a new dataset of 240 manually validated code examples. Four different prompt strategies\u2014zero-shot, few-shot, chain-of-thought, and ensemble\u2014are applied and their impact on detection performance is systematically analyzed.", "result": "GPT-4o Mini outperforms other models in overall detection but still struggles with complex principles (e.g., DIP). No single prompt strategy works best for all cases; for instance, ENSEMBLE excels at OCP detection, while EXAMPLE prompts are better for DIP. Language characteristics and code complexity heavily affect detection accuracy, with performance dropping as complexity rises.", "conclusion": "AI-driven code design analysis can't rely on a one-size-fits-all approach. Optimal detection demands careful pairing of model and prompt strategy to the specific coding context and principle. Tailored combinations offer the most potential for maintaining quality and supporting developers."}}
{"id": "2509.03270", "categories": ["cs.SE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.03270", "abs": "https://arxiv.org/abs/2509.03270", "authors": ["Martin Skoglund", "Fredrik Warg", "Aria Mirzai", "Anders Thorsen", "Karl Lundgren", "Peter Folkesson", "Bastian Havers-zulka"], "title": "AI Safety Assurance in Electric Vehicles: A Case Study on AI-Driven SOC Estimation", "comment": "12 pages, 9 figures, EVS38,\n  https://evs38-program.org/en/evs-38-proceedings/all", "summary": "Integrating Artificial Intelligence (AI) technology in electric vehicles (EV)\nintroduces unique challenges for safety assurance, particularly within the\nframework of ISO 26262, which governs functional safety in the automotive\ndomain. Traditional assessment methodologies are not geared toward evaluating\nAI-based functions and require evolving standards and practices. This paper\nexplores how an independent assessment of an AI component in an EV can be\nachieved when combining ISO 26262 with the recently released ISO/PAS 8800,\nwhose scope is AI safety for road vehicles. The AI-driven State of Charge (SOC)\nbattery estimation exemplifies the process. Key features relevant to the\nindependent assessment of this extended evaluation approach are identified. As\npart of the evaluation, robustness testing of the AI component is conducted\nusing fault injection experiments, wherein perturbed sensor inputs are\nsystematically introduced to assess the component's resilience to input\nvariance.", "AI": {"tldr": "The paper presents a method to independently assess the safety of AI components in electric vehicles by combining ISO 26262 and ISO/PAS 8800 standards, using robustness testing to evaluate resilience against faulty sensor inputs.", "motivation": "AI technologies are increasingly being integrated into electric vehicles (EVs), but current safety standards like ISO 26262 are insufficient for evaluating AI-based functions, necessitating updated assessment practices.", "method": "The paper proposes combining ISO 26262 with ISO/PAS 8800 for safety assessment of AI components in EVs. It uses the example of an AI-driven State of Charge (SOC) battery estimation and performs robustness testing via fault injection to test the system's resilience to sensor input perturbations.", "result": "The study identifies key aspects needed for independent assessment under the combined standards and demonstrates how AI component robustness can be evaluated through systematic fault injection experiments.", "conclusion": "Integrating traditional functional safety standards with new AI safety standards and employing robustness testing can enhance the independent safety assessment of AI components in EVs."}}
{"id": "2509.03331", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.03331", "abs": "https://arxiv.org/abs/2509.03331", "authors": ["Weizhe Wang", "Wei Ma", "Qiang Hu", "Yao Zhang", "Jianfei Sun", "Bin Wu", "Yang Liu", "Guangquan Xu", "Lingxiao Jiang"], "title": "VulnRepairEval: An Exploit-Based Evaluation Framework for Assessing Large Language Model Vulnerability Repair Capabilities", "comment": null, "summary": "The adoption of Large Language Models (LLMs) for automated software\nvulnerability patching has shown promising outcomes on carefully curated\nevaluation sets. Nevertheless, existing datasets predominantly rely on\nsuperficial validation methods rather than exploit-based verification, leading\nto overestimated performance in security-sensitive applications. This paper\nintroduces VulnRepairEval, an evaluation framework anchored in functional\nProof-of-Concept (PoC) exploits. Our framework delivers a comprehensive,\ncontainerized evaluation pipeline that enables reproducible differential\nassessment, where repair success requires the original exploit to fail\nexecution against the modified code. The benchmark construction involved\nextensive data curation: we processed over 400 CVEs and approximately 2,500\npotential sources to extract a collection of authentic vulnerability instances\n(23 Python CVEs) amenable to automated testing with working PoCs. Through\nVulnRepairEval, we conduct a comprehensive evaluation of 12 popular LLMs and\nobserve a significant performance deficit: even the top-performing model\nsuccessfully addresses merely 5/23 instances (about 21.7%), exposing critical\nweaknesses in security-focused applications. Our failure analysis reveals that\nmost unsuccessful attempts stem from imprecise vulnerability identification and\npatches containing syntactic or semantic errors. Enhanced prompting strategies\nand multi-agent approaches yield minimal improvements, with overall\neffectiveness remaining largely unaffected. This work contributes a stringent,\npractical evaluation framework for LLM-driven vulnerability remediation and\nunderscores the necessity for assessment protocols that authentically reflect\nreal-world exploitation scenarios.", "AI": {"tldr": "VulnRepairEval tests LLMs on real exploit-based vulnerability patching and finds that even the best models succeed on only ~22% of cases, exposing major weaknesses and the need for tougher, more realistic evaluation in security settings.", "motivation": "Current evaluation sets for LLM-based automated software vulnerability patching are not reliable because they use superficial validation, not real exploit-based verification. This leads to overestimated effectiveness, which is dangerous for real-world security applications.", "method": "The authors introduce VulnRepairEval, an evaluation framework that uses real Proof-of-Concept exploits to test patched vulnerabilities. They curated a dataset from over 400 CVEs, extracting 23 Python vulnerabilities with working PoCs. The framework benchmarks 12 popular LLMs in a reproducible, containerized pipeline, requiring that the exploit fails after a patch for it to be considered successful.", "result": "Only 5 out of 23 vulnerabilities were successfully patched by the best-performing LLM (about 21.7%). Most failures came from poor vulnerability identification or syntactic/semantic errors in patches. Improved prompting and multi-agent methods didn\u2019t significantly help.", "conclusion": "The paper provides a robust evaluation framework that reflects real exploit conditions, revealing critical limitations in current LLMs for vulnerability patching. Stronger and more realistic assessment methods are urgently needed for security-focused AI applications."}}
{"id": "2509.03463", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.03463", "abs": "https://arxiv.org/abs/2509.03463", "authors": ["Parham Khamsepour", "Mark Cole", "Ish Ashraf", "Sandeep Puri", "Mehrdad Sabetzadeh", "Shiva Nejati"], "title": "The Impact of Critique on LLM-Based Model Generation from Natural Language: The Case of Activity Diagrams", "comment": null, "summary": "Large Language Models (LLMs) show strong potential for automating the\ngeneration of models from natural-language descriptions. A common approach is\nan iterative generate-critique-refine loop, where candidate models are\nproduced, evaluated, and updated based on detected issues. This process needs\nto address: (1) structural correctness - compliance with well-formedness rules\n- and (2) semantic alignment - accurate reflection of the intended meaning in\nthe source text. We present LADEX (LLM-based Activity Diagram Extractor), a\npipeline for deriving activity diagrams from natural-language process\ndescriptions using an LLM-driven critique-refine process. Structural checks in\nLADEX can be performed either algorithmically or by an LLM, while alignment\nchecks are always performed by an LLM. We design five ablated variants of LADEX\nto study: (i) the impact of the critique-refine loop itself, (ii) the role of\nLLM-based semantic checks, and (iii) the comparative effectiveness of\nalgorithmic versus LLM-based structural checks.\n  To evaluate LADEX, we compare the generated activity diagrams with\nexpert-created ground truths using trace-based operational semantics. This\nenables automated measurement of correctness and completeness. Experiments on\ntwo datasets indicate that: (1) the critique-refine loop improves structural\nvalidity, correctness, and completeness compared to single-pass generation; (2)\nalgorithmic structural checks eliminate inconsistencies that LLM-based checks\nfail to detect, improving correctness by an average of 17.81% and completeness\nby 13.24% over LLM-only checks; and (3) combining algorithmic structural checks\nwith LLM-based semantic checks, implemented using the reasoning-focused O4\nMini, achieves the best overall performance - yielding average correctness of\nup to 86.37% and average completeness of up to 88.56% - while requiring fewer\nthan five LLM calls on average.", "AI": {"tldr": "The paper presents LADEX, a system for extracting activity diagrams from text using a critique-refine loop between LLMs and algorithms. While LLMs alone often miss structural errors, adding algorithmic checks significantly boosts accuracy. The best results come from combining these approaches, achieving over 86% correctness and 88% completeness with few LLM calls.", "motivation": "Automating the extraction of structured models (such as activity diagrams) from natural-language process descriptions remains challenging, especially regarding both structural correctness and semantic alignment. Existing LLM-based approaches often struggle with maintaining model quality and accurately reflecting intended meanings.", "method": "The authors introduce LADEX, a pipeline that uses a generate-critique-refine loop powered by LLMs to derive activity diagrams from natural-language descriptions. Structural correctness is checked algorithmically or by the LLM, while semantic alignment relies on LLM-based checks. Five ablated versions of LADEX are designed to study the roles of critique-refine loops, LLM-based semantic checking, and the comparative advantage of algorithmic structural checks. Evaluation is performed via automated comparisons with expert-generated ground truths on two datasets.", "result": "Experiments show that the critique-refine loop enhances structural validity, correctness, and completeness over single-pass approaches. Algorithmic structural checks outperform LLM-only checks by improving correctness by 17.81% and completeness by 13.24%. The combination of algorithmic structural and LLM-based semantic checks (O4 Mini) yields the highest performance: up to 86.37% correctness and 88.56% completeness, with under five LLM calls per instance on average.", "conclusion": "Integrating algorithmic structural checks with LLM-based semantic checking in an iterative critique-refine loop significantly improves the quality and efficiency of model extraction from natural language. The LADEX pipeline, particularly when using a hybrid approach, offers strong performance in generating accurate and complete process models with minimal LLM resource consumption."}}
