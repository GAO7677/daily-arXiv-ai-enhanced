<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 10]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [The State of Computational Science in Fission and Fusion Energy](https://arxiv.org/abs/2507.08061)
*Andrea Morales Coto,Aditi Verma*

Main category: cs.SE

TL;DR: A 2024 survey of 103 computational scientists in fusion and fission energy reveals a shift toward modern, open-source, modular software tools, reduced reliance on FORTRAN, and increased investment in code development, highlighting the future direction of nuclear engineering software.


<details>
  <summary>Details</summary>
Motivation: Software has become the dominant tool in fusion and fission energy engineering. The paper is motivated to understand what problems computational scientists are tackling with their codes and the tools and experiences involved, amidst a rapidly changing landscape.

Method: A survey was conducted in 2024 with 103 computational scientists working in fusion and fission energy. The survey asked them about the problems they are addressing, the tools they use, and their overall developer experience.

Result: Results show a significant shift in software preferences: there is increasing adoption of modern programming languages, open-source, and modular software, while usage of languages like FORTRAN is declining. The trend also highlights growing investment in code development and a move toward multiphysics codes.

Conclusion: Nuclear engineering is witnessing a transformation in computational practices, with trends toward modular design, smaller compute requirements, and organizational prioritization of software development. These insights likely reflect the direction of the field over the next 5 to 10 years, especially within US labs and universities.

Abstract: The tools used to engineer something are just as important as the thing that
is actually being engineered. In fact, in many cases, the tools can indeed
determine what is engineerable. In fusion and fission1 energy engineering,
software has become the dominant tool for design. For that reason, in 2024, for
the first time ever, we asked 103 computational scientists developing the codes
used in fusion and fission energy about the problems they are attempting to
solve with their codes, the tools available to them to solve them, and their
end to end developer experience with said tools.
  The results revealed a changing tide in software tools in fusion and fission,
with more and more computational scientists preferring modern programming
languages, open-source codes, and modular software. These trends represent a
peek into what will happen 5 to 10 years in the future of nuclear engineering.
Since the majority of our respondents belonged to US national labs and
universities, these results hint at the most cutting-edge trends in the
industry. The insights included in the State of Computational Science in
Fission and Fusion Energy indicate a dramatic shift toward multiphysics codes,
a drop-off in the use of FORTRAN in favor of more modern languages like Python
and C++, and ever-rising budgets for code development, at times reaching $50M
in a single organization.
  Our survey paints a future of nuclear engineering codes that is modular in
nature, small in terms of compute, and increasingly prioritized by
organizations. Access to our results in web form are available online.

</details>


### [2] [Code with Me or for Me? How Increasing AI Automation Transforms Developer Workflows](https://arxiv.org/abs/2507.08149)
*Valerie Chen,Ameet Talwalkar,Robert Brennan,Graham Neubig*

Main category: cs.SE

TL;DR: The paper compares current copilot tools and emerging autonomous coding agents, finds agents can outdo copilots and lessen user workload, but broader adoption is limited by challenges like user trust and workflow integration.


<details>
  <summary>Details</summary>
Motivation: While copilot-like AI aids are widely adopted by developers, more autonomous 'coding agents' are emerging. However, most evaluations of these agents lack real human interaction and rely on benchmarks. This study aims to fill this gap and explore their actual impact on developer productivity and experience.

Method: The authors conducted an academic user study comparing the developer experience with two leading AI coding tools: a copilot (GitHub Copilot) and a more autonomous agentic assistant (OpenHands). They recruited regular Copilot users and observed their interaction with both systems.

Result: Agentic coding tools demonstrated greater assistance potential, sometimes achieving tasks beyond human or copilot capabilities and requiring less user effort. However, a lack of user understanding of agent behavior poses challenges to wider agent adoption.

Conclusion: Coding agents can surpass traditional copilots in supporting developers, but their successful integration demands addressing usability and transparency concerns. The study highlights new workflow patterns and calls for better design and understanding to facilitate broader adoption.

Abstract: Developers now have access to a growing array of increasingly autonomous AI
tools to support software development. While numerous studies have examined
developer use of copilots, which can provide chat assistance or code
completions, evaluations of coding agents, which can automatically write files
and run code, still largely rely on static benchmarks without
humans-in-the-loop. In this work, we conduct the first academic study to
explore developer interactions with coding agents and characterize how more
autonomous AI tools affect user productivity and experience, compared to
existing copilots. We evaluate two leading copilot and agentic coding
assistants, GitHub Copilot and OpenHands, recruiting participants who regularly
use the former. Our results show agents have the potential to assist developers
in ways that surpass copilots (e.g., completing tasks that humans might not
have accomplished before) and reduce the user effort required to complete
tasks. However, there are challenges involved in enabling their broader
adoption, including how to ensure users have an adequate understanding of agent
behaviors. Our results not only provide insights into how developer workflows
change as a result of coding agents but also highlight how user interactions
with agents differ from those with existing copilots, motivating a set of
recommendations for researchers building new agents. Given the broad set of
developers who still largely rely on copilot-like systems, our work highlights
key challenges of adopting more agentic systems into developer workflows.

</details>


### [3] [The Impact of Generative AI on Code Expertise Models: An Exploratory Study](https://arxiv.org/abs/2507.08160)
*Otávio Cury,Guilherme Avelino*

Main category: cs.SE

TL;DR: The paper shows that using AI-generated code in projects can distort traditional ways of measuring developer expertise, suggesting a need to revise these metrics as GenAI tools become more common.


<details>
  <summary>Details</summary>
Motivation: The widespread adoption of Generative AI tools in coding has improved productivity but raised concerns about developers' decreasing understanding of generated code and the potential effects on expert identification models.

Method: The authors collected data on ChatGPT-generated code integrated into public GitHub projects and simulated different scenarios by varying the level of GenAI contributions to analyze the effects on a source code knowledge model and on a Truck Factor algorithm.

Result: The analysis showed that, under most simulated scenarios, increased use of GenAI-generated code measurably affected the outputs of expertise metrics, making them less reliable.

Conclusion: As GenAI becomes more prevalent in software development, existing metrics for assessing developer expertise and ownership (e.g., knowledge models and Truck Factor) may become less trustworthy due to the reduced link between code contributions and individual understanding.

Abstract: Generative Artificial Intelligence (GenAI) tools for source code generation
have significantly boosted productivity in software development. However, they
also raise concerns, particularly the risk that developers may rely heavily on
these tools, reducing their understanding of the generated code. We hypothesize
that this loss of understanding may be reflected in source code knowledge
models, which are used to identify developer expertise. In this work, we
present an exploratory analysis of how a knowledge model and a Truck Factor
algorithm built upon it can be affected by GenAI usage. To investigate this, we
collected statistical data on the integration of ChatGPT-generated code into
GitHub projects and simulated various scenarios by adjusting the degree of
GenAI contribution. Our findings reveal that most scenarios led to measurable
impacts, indicating the sensitivity of current expertise metrics. This suggests
that as GenAI becomes more integrated into development workflows, the
reliability of such metrics may decrease.

</details>


### [4] [Leveraging Large Language Models for Classifying App Users' Feedback](https://arxiv.org/abs/2507.08250)
*Yasaman Abedini,Abbas Heydarnoori*

Main category: cs.SE

TL;DR: LLMs such as GPT-4 and Llama3-70b can be used to augment labeled datasets for app user feedback classification, improving the performance of existing BERT-based models and reducing the reliance on costly human annotation.


<details>
  <summary>Details</summary>
Motivation: Classifying app user feedback is essential for app improvement, but building large, high-quality labeled datasets for supervised machine learning is costly and time-consuming. The study seeks to find more scalable, generalizable methods for handling this problem.

Method: The paper evaluates four large language models (LLMs) — GPT-3.5-Turbo, GPT-4, Flan-T5, and Llama3-70b — on eight existing, carefully labeled datasets of user feedback drawn from app stores, social platforms, and forums. The models are assessed for their abilities to classify feedback into both specific (fine-grained) and general (coarse-grained) categories. Additionally, the researchers leverage LLMs as annotation tools to expand (augment) labeled datasets, which are then used to train BERT-based classification models.

Result: LLMs, when directed by high-quality prompts, accurately classify user feedback at the coarse-grained level. Furthermore, using LLM-labeled augmented data to expand training sets meaningfully improves the performance of state-of-the-art BERT classifiers.

Conclusion: Well-guided LLMs can serve as effective tools for classifying and labeling user feedback data, helping to overcome the challenge of limited labeled datasets. LLM-augmented data enhances the classification performance of existing machine learning models.

Abstract: In recent years, significant research has been conducted into classifying
application (app) user feedback, primarily relying on supervised machine
learning algorithms. However, fine-tuning more generalizable classifiers based
on existing labeled datasets remains an important challenge, as creating large
and accurately labeled datasets often requires considerable time and resources.
In this paper, we evaluate the capabilities of four advanced LLMs, including
GPT-3.5-Turbo, GPT-4, Flan-T5, and Llama3-70b, to enhance user feedback
classification and address the challenge of the limited labeled dataset. To
achieve this, we conduct several experiments on eight datasets that have been
meticulously labeled in prior research. These datasets include user reviews
from app stores, posts from the X platform, and discussions from the public
forums, widely recognized as representative sources of app user feedback. We
analyze the performance of various LLMs in identifying both fine-grained and
coarse-grained user feedback categories. Given the substantial volume of daily
user feedback and the computational limitations of LLMs, we leverage these
models as an annotation tool to augment labeled datasets with general and
app-specific data. This augmentation aims to enhance the performance of
state-of-the-art BERT-based classification models. Our findings indicate that
LLMs when guided by well-crafted prompts, can effectively classify user
feedback into coarse-grained categories. Moreover, augmenting the training
dataset with datasets labeled using the consensus of LLMs can significantly
enhance classifier performance.

</details>


### [5] [Computing Floating-Point Errors by Injecting Perturbations](https://arxiv.org/abs/2507.08467)
*Youshuai Tan,Zhanwei Zhang,Jinfu Chen,Zishuo Ding,Jifeng Xuan,Weiyi Shang*

Main category: cs.SE

TL;DR: PI-detector is a new tool that efficiently and accurately detects floating-point errors in programs by injecting small perturbations and comparing outcomes, outperforming existing solutions in both speed and correctness.


<details>
  <summary>Details</summary>
Motivation: Floating-point errors in programs can result in serious consequences in fields like safety-critical systems and engineering. However, not all inputs trigger significant errors, and current methods to detect such errors are either hard to implement, slow, or prone to false positives.

Method: The paper proposes PI-detector, a novel approach that injects small perturbations into the operands of atomic operations (like addition and subtraction) in floating-point programs. By comparing the outcome of the original and perturbed executions, the method estimates floating-point errors. The approach is then evaluated with standard datasets and a complex linear system program.

Result: The experimental results show that PI-detector achieves both efficiency and accuracy in computing floating-point errors, addressing the limitations of existing tools like ATOMU (false positives) and FPCC (slow performance).

Conclusion: PI-detector is an effective and efficient solution for detecting significant floating-point errors in programs, overcoming the shortcomings of previous solutions by precisely and swiftly identifying errors for specific inputs.

Abstract: Floating-point programs form the foundation of modern science and
engineering, providing the essential computational framework for a wide range
of applications, such as safety-critical systems, aerospace engineering, and
financial analysis. Floating-point errors can lead to severe consequences.
Although floating-point errors widely exist, only a subset of inputs may
trigger significant errors in floating-point programs. Therefore, it is crucial
to determine whether a given input could produce such errors. Researchers tend
to take the results of high-precision floating-point programs as oracles for
detecting floating-point errors, which introduces two main limitations: (1)
difficulty of implementation and (2) prolonged execution time. The two recent
tools, ATOMU and FPCC, can partially address these issues. However, ATOMU
suffers from false positives; while FPCC, though eliminating false positives,
operates at a considerably slower speed.
  To address these two challenges, we propose a novel approach named
PI-detector to computing floating-point errors effectively and efficiently. Our
approach is based on the observation that floating-point errors stem from large
condition numbers in atomic operations (such as addition and subtraction),
which then propagate and accumulate. PI-detector injects small perturbations
into the operands of individual atomic operations within the program and
compares the outcomes of the original program with the perturbed version to
compute floating-point errors. We evaluate PI-detector with datasets from ATOMU
and HSED, as well as a complex linear system-solving program. Experimental
results demonstrate that PI-detector can perform efficient and accurate
floating-point error computation.

</details>


### [6] [InferLog: Accelerating LLM Inference for Online Log Parsing via ICL-oriented Prefix Caching](https://arxiv.org/abs/2507.08523)
*Yilun Wang,Pengfei Chen,Haiyu Huang,Zilong He,Gou Tan,Chuanfu Zhang,Jingkai He,Zibin Zheng*

Main category: cs.SE

TL;DR: InferLog is a new method to optimize LLM inference for log parsing, greatly improving inference speed while maintaining accuracy and addressing production deployment issues such as latency, throughput, and privacy.


<details>
  <summary>Details</summary>
Motivation: Modern software systems produce vast amounts of runtime logs. Efficient and accurate log parsing is crucial for tasks like anomaly detection and root cause analysis. Existing LLM-based parsers succeed in accuracy but struggle with production deployment due to privacy concerns with commercial LLMs and challenges meeting strict latency and throughput demands in large-scale environments.

Method: The paper introduces InferLog, an LLM inference optimization technique for online log parsing. InferLog includes: (1) A Prefix-aware In-Context Learning (ICL) Refinement policy for better prefix caching, and (2) a meta-learning-based configuration tuning pipeline for rapid, optimal LLM scheduling according to dynamic workloads.

Result: Experiments using the Loghub dataset and vLLM show that InferLog surpasses existing inference optimization methods. It greatly speeds up state-of-the-art LLM-based log parsers, all without sacrificing parsing accuracy.

Conclusion: InferLog addresses the main bottleneck in online LLM-based log parsing— inference efficiency— and demonstrates significant improvements over previous solutions in speed while retaining accuracy.

Abstract: Modern software systems generate massive volumes of runtime logs,
necessitating efficient and accurate log parsing to enable critical downstream
tasks such as anomaly detection and root cause analysis. Recently, large
language models (LLMs) have achieved advanced accuracy on log parsing, but
their deployment in production environments faces two major limitations: (1)
the privacy risks associated with commercial LLMs, driving the adoption of
local deployment, and (2) the stringent latency and throughput requirements
imposed by high-volume log streams, which existing LLM-based parsers fail to
meet. Although recent efforts have reduced the number of LLM queries, they
overlook the high latency of the LLM invocations, where concurrent log parsing
requests can cause serve performance degradation of LLM inference system.
  In this study, we present InferLog, the first LLM inference optimization
method for online log parsing. Our key insight is that the inference efficiency
emerges as the vital bottleneck in LLM-based online log parsing, rather than
parsing accuracy. InferLog accelerates inference by designing (1) A
Prefix-aware ICL Refinement policy to refine the examples and permutation of
in-context learning to improve the prefix caching efficiency. (2) A rapid and
task-specific configuration tuning pipeline based on meta-learning to find the
optimal LLM scheduling-related configuration for dynamic log parsing workloads.
The experimental results based on Loghub dataset and vLLM demonstrate that
InferLog significantly outperforms existing inference optimization methods and
markedly accelerates the state-of-the-art LLM-based log parser without
compromising parsing accuracy.

</details>


### [7] [Generating Proto-Personas through Prompt Engineering: A Case Study on Efficiency, Effectiveness and Empathy](https://arxiv.org/abs/2507.08594)
*Fernando Ayach,Vitor Lameirão,Raul Leão,Jerfferson Felizardo,Rafael Sobrinho,Vanessa Borges,Patrícia Matsubara,Awdren Fontão*

Main category: cs.SE

TL;DR: This paper shows that using Generative AI to create proto-personas during product discovery can save time and improve quality, but attention is needed to avoid overly generic results and ensure domain relevance and empathy.


<details>
  <summary>Details</summary>
Motivation: Manual creation of proto-personas during early-stage Product Discovery is time-consuming, cognitively demanding, and prone to bias. The paper seeks to address these challenges by leveraging Generative AI for persona generation.

Method: A prompt engineering-based approach using Generative AI was proposed. The authors conducted a case study with 19 participants engaged in a real Lean Inception process, employing both qualitative and quantitative research methods.

Result: The GenAI-supported approach reduced time and effort required for persona creation, improved the quality and reusability of personas in subsequent phases, and was generally well accepted regarding usefulness and ease of use. However, limitations related to persona generalization and lack of domain specificity were noted. Empathy support was high for cognitive empathy but varied for affective and behavioral empathy.

Conclusion: GenAI can be efficiently and effectively integrated into software Product Discovery for generating proto-personas, offering significant advantages in productivity and quality, but challenges remain—especially regarding generalization, domain specificity, and eliciting different types of empathy. Future efforts should address these limitations to enhance hybrid design processes.

Abstract: Proto-personas are commonly used during early-stage Product Discovery, such
as Lean Inception, to guide product definition and stakeholder alignment.
However, the manual creation of proto-personas is often time-consuming,
cognitively demanding, and prone to bias. In this paper, we propose and
empirically investigate a prompt engineering-based approach to generate
proto-personas with the support of Generative AI (GenAI). Our goal is to
evaluate the approach in terms of efficiency, effectiveness, user acceptance,
and the empathy elicited by the generated personas. We conducted a case study
with 19 participants embedded in a real Lean Inception, employing a qualitative
and quantitative methods design. The results reveal the approach's efficiency
by reducing time and effort and improving the quality and reusability of
personas in later discovery phases, such as Minimum Viable Product (MVP)
scoping and feature refinement. While acceptance was generally high, especially
regarding perceived usefulness and ease of use, participants noted limitations
related to generalization and domain specificity. Furthermore, although
cognitive empathy was strongly supported, affective and behavioral empathy
varied significantly across participants. These results contribute novel
empirical evidence on how GenAI can be effectively integrated into software
Product Discovery practices, while also identifying key challenges to be
addressed in future iterations of such hybrid design processes.

</details>


### [8] [NL in the Middle: Code Translation with LLMs and Intermediate Representations](https://arxiv.org/abs/2507.08627)
*Chi-en Amy Tai,Pengyu Nie,Lukasz Golab,Alexander Wong*

Main category: cs.SE

TL;DR: Using chain-of-thought prompting with natural language summaries as intermediate steps significantly increases the accuracy of code translation by large language models compared to standard prompting approaches.


<details>
  <summary>Details</summary>
Motivation: Large language models often generate buggy code translations. The motivation is to improve translation accuracy, potentially by introducing intermediate representations to guide the model’s understanding.

Method: The study explores the effect of integrating intermediate representations, specifically natural language (NL) summaries and abstract syntax trees (ASTs), into LLM code translation tasks. Different integration techniques, such as one-shot prompting and chain-of-thought (CoT) prompting, are tested. Experiments are conducted using Open Gpt4 8X7B, StarCoder, and CodeGen on established code translation benchmarks (CodeNet and AVATAR).

Result: The results reveal that chain-of-thought prompting with an intermediate NL summary achieves the highest performance, boosting successful translations by 13.8% and 6.7% for Open Gpt4 8X7B, compared to zero-shot prompting.

Conclusion: Incorporating intermediate natural language summaries using chain-of-thought prompting significantly improves LLM code translation performance. This approach offers a promising direction for better code translation outcomes with large language models.

Abstract: Studies show that large language models (LLMs) produce buggy code
translations. One avenue to improve translation accuracy is through
intermediate representations, which could provide structured insights to guide
the model's understanding. We explore whether code translation using LLMs can
benefit from intermediate representations via natural language (NL) and
abstract syntax trees (ASTs). Since prompt engineering greatly affects LLM
performance, we consider several ways to integrate these representations, from
one-shot to chain-of-thought (CoT) prompting. Using Open Gpt4 8X7B and
specialized StarCoder and CodeGen models on popular code translation benchmarks
(CodeNet and AVATAR), we find that CoT with an intermediate NL summary performs
best, with an increase of 13.8% and 6.7%, respectively, in successful
translations for the best-performing model (Open Gpt4 8X7B) compared to the
zero-shot prompt.

</details>


### [9] [LLMCup: Ranking-Enhanced Comment Updating with LLMs](https://arxiv.org/abs/2507.08671)
*Hua Ge,Juan Zhai,Minxue Pan,Fusen He,Ziyue Tan*

Main category: cs.SE

TL;DR: This paper introduces LLMCup, a novel framework using large language models and strategic prompting for automatic code comment updating. It significantly outperforms previous baseline methods and can sometimes exceed human-written updates, demonstrating the promise of LLM-based solutions for code documentation tasks.


<details>
  <summary>Details</summary>
Motivation: Comments are crucial for code readability and maintainability, but developers often neglect updating them when modifying code, leading to inconsistent documentation that hampers future maintenance.

Method: The paper proposes LLMCup, a framework that utilizes multiple prompting strategies to generate diverse comment update candidates with a large language model (LLM). It then employs a ranking model (CupRank) to select the best candidate for the final comment update.

Result: LLMCup outperforms existing methods (CUP and HebCup) in several evaluation metrics: 49.0%-116.9% in Accuracy, 10.8%-20% in BLEU-4, 4.6% in METEOR, 0.9%-1.9% in F1, and 2.1%-3.4% in SentenceBert similarity. User studies also indicate LLMCup can sometimes produce better outputs than human updates.

Conclusion: LLMCup improves the accuracy and quality of automated comment updating, addressing the limitations of prior approaches and even rivaling or surpassing human performance in some cases. Human assessment remains important for evaluating comment quality.

Abstract: While comments are essential for enhancing code readability and
maintainability in modern software projects, developers are often motivated to
update code but not comments, leading to outdated or inconsistent documentation
that hinders future understanding and maintenance. Recent approaches such as
CUP and HebCup have attempted automatic comment updating using neural
sequence-to-sequence models and heuristic rules, respectively. However, these
methods can miss or misinterpret crucial information during comment updating,
resulting in inaccurate comments, and they often struggle with complex update
scenarios. Given these challenges, a promising direction lies in leveraging
large language models (LLMs), which have shown impressive performance in
software engineering tasks such as comment generation, code synthesis, and
program repair. This suggests their strong potential to capture the logic
behind code modifications - an ability that is crucial for the task of comment
updating. Nevertheless, selecting an appropriate prompt strategy for an LLM on
each update case remains challenging. To address this, we propose a novel
comment updating framework, LLMCup, which first uses multiple prompt strategies
to provide diverse candidate updated comments via an LLM, and then employs a
ranking model, CupRank, to select the best candidate as final updated comment.
Experimental results demonstrate the effectiveness of LLMCup, with improvements
over state-of-the-art baselines (CUP and HebCup) by 49.0%-116.9% in Accuracy,
10.8%-20% in BLEU-4, 4.6% in METEOR, 0.9%-1.9% in F1, and 2.1%-3.4% in
SentenceBert similarity. Furthermore, a user study shows that comments updated
by LLMCup sometimes surpass human-written updates, highlighting the importance
of incorporating human evaluation in comment quality assessment.

</details>


### [10] [Dually Hierarchical Drift Adaptation for Online Configuration Performance Learning](https://arxiv.org/abs/2507.08730)
*Zezhen Xiang,Jingzhi Gong,Tao Chen*

Main category: cs.SE

TL;DR: DHDA is a new framework for online configuration performance learning that adaptively handles both global and local performance drifts in dynamic software systems, outperforming existing methods in accuracy and adaptability while keeping computational costs reasonable.


<details>
  <summary>Details</summary>
Motivation: Modern configurable software systems face the challenge of unpredictable and dynamic environments, where changes in workload, hardware, or system updates cause shifts ('concept drifts') in how configurations impact performance. Existing approaches struggle to adapt in real-time to these global and local drifts, making effective configuration performance prediction difficult.

Method: The paper proposes DHDA, an online adaptive learning framework that addresses both global and local concept drifts in configuration performance prediction. DHDA uses a hierarchical adaptation scheme: at the upper level, it repartitions the data and retrains local models to respond to detected global drifts; at the lower level, local models detect and asynchronously adapt to local drifts. DHDA balances efficiency and adaptability through a mix of incremental updates and periodic full retraining, minimizing unnecessary computation.

Result: DHDA was evaluated on eight different software systems and compared with state-of-the-art methods. The results show DHDA achieves much higher prediction accuracy and adapts more effectively to concept drifts (up to 2x improvement), with reasonable computational overhead and improved handling of local drifts.

Conclusion: The proposed DHDA framework effectively addresses both global and local concept drifts in configurable software systems, providing better accuracy and adaptability for configuration performance prediction in dynamic environments, with manageable overhead.

Abstract: Modern configurable software systems need to learn models that correlate
configuration and performance. However, when the system operates in dynamic
environments, the workload variations, hardware changes, and system updates
will inevitably introduce concept drifts at different levels - global drifts,
which reshape the performance landscape of the entire configuration space; and
local drifts, which only affect certain sub-regions of that space. As such,
existing offline and transfer learning approaches can struggle to adapt to
these implicit and unpredictable changes in real-time, rendering configuration
performance learning challenging. To address this, we propose DHDA, an online
configuration performance learning framework designed to capture and adapt to
these drifts at different levels. The key idea is that DHDA adapts to both the
local and global drifts using dually hierarchical adaptation: at the upper
level, we redivide the data into different divisions, within each of which the
local model is retrained, to handle global drifts only when necessary. At the
lower level, the local models of the divisions can detect local drifts and
adapt themselves asynchronously. To balance responsiveness and efficiency, DHDA
combines incremental updates with periodic full retraining to minimize
redundant computation when no drifts are detected. Through evaluating eight
software systems and against state-of-the-art approaches, we show that DHDA
achieves considerably better accuracy and can effectively adapt to drifts with
up to 2x improvements, while incurring reasonable overhead and is able to
improve different local models in handling concept drift.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [11] [Dependent Multiplicities in Dependent Linear Type Theory](https://arxiv.org/abs/2507.08759)
*Maximilian Doré*

Main category: cs.PL

TL;DR: This paper proposes a dependent linear type theory allowing variable usage multiplicity to depend on other variables, improving resource annotations for higher-order functions. The approach works by embedding linear logic into dependent type theory and is implemented in Agda.


<details>
  <summary>Details</summary>
Motivation: Traditional type systems cannot adequately express resource usage (multiplicity) dependent on other variables, especially for higher-order functions. The paper aims to address this gap by allowing a variable's multiplicity to be dependent on other variables for more precise resource annotations.

Method: The authors embed linear logic into dependent type theory and define the interaction between the embedded logic and the host theory. They use natural number types to represent quantitative multiplicities, characterize the semantics using standard models, and implement their theory in Agda.

Result: The new type theory successfully expresses dependent multiplicities and refines resource typing for higher-order functions. The framework is generic enough to be integrated into any dependently typed language and is validated through an Agda implementation.

Conclusion: The paper introduces a new dependent linear type theory that supports variables whose usage multiplicity can depend on other variables, enabling more precise resource management in higher-order functions. The system is shown to be compatible with existing dependently typed languages and has been implemented in Agda.

Abstract: We present a novel dependent linear type theory in which the multiplicity of
some variable - i.e., the number of times the variable can be used in a program
- can depend on other variables. This allows us to give precise resource
annotations to many higher-order functions that cannot be adequately typed in
any other system. Inspired by the Dialectica translation, our typing discipline
is obtained by embedding linear logic into dependent type theory and specifying
how the embedded logic interacts with the host theory. We can then use a
standard natural numbers type to obtain a quantitative typing system with
dependent multiplicities. We characterise the semantics for our theory as a
combination of standard models of dependent type theory and linear logic. Our
system can be added to any dependently typed language, which we demonstrate
with an implementation in Agda.

</details>


### [12] [Filter Equivariant Functions: A symmetric account of length-general extrapolation on lists](https://arxiv.org/abs/2507.08796)
*Owen Lewis,Neil Ghani,Andrew Dudzik,Christos Perivolaropoulos,Razvan Pascanu,Petar Veličković*

Main category: cs.PL

TL;DR: The authors define and study filter equivariant functions, which predictably handle input lists even when elements are removed. They connect this concept to map equivariant functions, give geometric interpretations, and present an algorithm for perfect extrapolation based on sublists.


<details>
  <summary>Details</summary>
Motivation: The motivation is to determine what characteristics make a function a 'good' extrapolator—specifically, how functions should behave beyond the set of known input/output pairs. Since any function matching known examples could be deemed correct, the goal is to set meaningful criteria for extrapolation, focusing on rule-following behavior in list functions.

Method: The authors introduce and formalize the concept of filter equivariant functions—functions whose outputs are predictably consistent even when some list elements are removed (modeled via filter operations in functional programming). The approach includes defining this class, exploring its mathematical properties, connecting it to map equivariant functions, and analyzing it from a geometric (simplicial structures) perspective. They culminate in proposing an algorithm—the amalgamation algorithm—to build outputs based on sublist analysis.

Result: The paper shows that filter equivariant functions form an interesting and mathematically rich class. The authors prove foundational theorems, draw connections to established function classes, provide a geometric interpretation, and present the amalgamation algorithm, which enables perfect extrapolation using sublist behavior.

Conclusion: The paper establishes the practical and theoretical significance of filter equivariant functions, both as an analytic tool for function behavior under element removal and as a basis for developing extrapolating algorithms. Their amalgamation algorithm exemplifies an optimal approach to constructing filter-equivariant extrapolants.

Abstract: What should a function that extrapolates beyond known input/output examples
look like? This is a tricky question to answer in general, as any function
matching the outputs on those examples can in principle be a correct
extrapolant. We argue that a "good" extrapolant should follow certain kinds of
rules, and here we study a particularly appealing criterion for rule-following
in list functions: that the function should behave predictably even when
certain elements are removed. In functional programming, a standard way to
express such removal operations is by using a filter function. Accordingly, our
paper introduces a new semantic class of functions -- the filter equivariant
functions. We show that this class contains interesting examples, prove some
basic theorems about it, and relate it to the well-known class of map
equivariant functions. We also present a geometric account of filter
equivariants, showing how they correspond naturally to certain simplicial
structures. Our highlight result is the amalgamation algorithm, which
constructs any filter-equivariant function's output by first studying how it
behaves on sublists of the input, in a way that extrapolates perfectly.

</details>
