{"id": "2507.18726", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18726", "abs": "https://arxiv.org/abs/2507.18726", "authors": ["Sadia Afrin Mim"], "title": "Exploring the Landscape of Fairness Interventions in Software Engineering", "comment": null, "summary": "Current developments in AI made it broadly significant for reducing human\nlabor and expenses across several essential domains, including healthcare and\nfinance. However, the application of AI in the actual world poses multiple\nrisks and disadvantages due to potential risk factors in data (e.g., biased\ndataset). Practitioners developed a number of fairness interventions for\naddressing these kinds of problems. The paper acts as a survey, summarizing the\nvarious studies and approaches that have been developed to address fairness\nissues", "AI": {"tldr": "The paper surveys efforts to improve fairness in AI by summarizing existing studies and solutions aimed at reducing bias and other risks in real-world applications.", "motivation": "AI is increasingly used in critical sectors like healthcare and finance, but brings challenges due to potential risks such as biased data, making fairness a major concern.", "method": "The paper conducts a survey, compiling and summarizing previous research and different interventions developed to address fairness issues in AI systems.", "result": "It provides an overview of the various studies and approaches that tackle fairness problems in AI, outlining the landscape of current solutions.", "conclusion": "This survey paper offers a comprehensive summary of the methods adopted to mitigate fairness-related risks in AI applications."}}
{"id": "2507.18755", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.18755", "abs": "https://arxiv.org/abs/2507.18755", "authors": ["Chandra Maddila", "Adam Tait", "Claire Chang", "Daniel Cheng", "Nauman Ahmad", "Vijayaraghavan Murali", "Marshall Roch", "Arnaud Avondet", "Aaron Meltzer", "Victor Montalvao", "Michael Hopko", "Chris Waterson", "Parth Thakkar", "Renuka Fernandez", "Kristian Kristensen", "Sivan Barzily", "Sherry Chen", "Rui Abreu", "Nachiappan Nagappan", "Payam Shodjai", "Killian Murphy", "James Everingham", "Aparna Ramani", "Peter C. Rigby"], "title": "Agentic Program Repair from Test Failures at Scale: A Neuro-symbolic approach with static analysis and test execution feedback", "comment": null, "summary": "Aim: With the advent of LLMs, sophisticated agentic program repair has become\nviable at large organizations with large codebases. In this work, we develop an\nEngineering Agent that fixes the source code based on test failures at scale\nacross diverse software offerings internally.\n  Method: Using Llama as the base, we employ the ReAct harness to develop an\nagent. We start with a test failure that was triaged by a rule-based test\nfailure bot. We then set up an agentic harness and allow the agent to reason\nand run a set of 15 actions from reading a file to generating a patch. We\nprovide feedback to the agent through static analysis and test failures so it\ncan refine its solution. We leverage an LLM-as-a-Judge to ensure that the patch\nconforms to the standards followed by a human review to land fixes.\n  Benchmark Findings: We curated offline benchmarks for our patch generator,\nthe Engineering Agent loop, and the LLM-as-a-Judge. In offline evaluations we\nfound that a specialized 70B model is highly competitive with the much larger\nbut vanilla Llama-405B. In an ablation study, we found that the ReAct harness\n(neural model) benefited from the symbolic information from static analysis\ntools and test execution traces. A model that strikes a balance between the\nsolve rate and error rate vs the cost and latency has a benchmark solve rate of\n42.3% using an average 11.8 feedback iterations.\n  Production Findings: In a three month period, 80% of the generated fixes were\nreviewed, of which 31.5% were landed (25.5% of the total number of generated\nfixes).\n  Feedback from Engineers: We used open coding to extract qualitative themes\nfrom engineers' feedback. We saw positive feedback in the form of quick\napprovals, gratitude, and surprise. We also found mixed feedback when the\nEngineering Agent's solution was partially correct and it served as a good\nstarting point.", "AI": {"tldr": "This paper presents an agentic LLM-based system for automated program repair in large organizations, showing competitive model performance and practical fix acceptance rates using a combination of neural and symbolic feedback mechanisms.", "motivation": "Large organizations with vast codebases face challenges in efficiently repairing code at scale based on test failures. The advent of large language models (LLMs) has opened up new opportunities for automating sophisticated program repair, motivating this study to develop and assess an agentic system for automatic code fixing using LLMs.", "method": "The authors developed an 'Engineering Agent' using Llama as the foundational LLM and the ReAct framework for agentic reasoning and action. The system operates by starting from test failures triaged by a rule-based bot, then employing 15 different agentic actions (from file reading to patch generation). Feedback mechanisms include static analysis and test execution results, allowing iterative solution refinement. Patch quality is validated using a combination of an LLM-as-a-Judge and human review.", "result": "Benchmark tests demonstrate that a specialized 70B model can compete with a larger vanilla Llama-405B model. The ReAct harness benefits when augmented with symbolic information from static analyzers and test traces. The agent achieved a solve rate of 42.3% with an average of 11.8 feedback loops per task. In production, 80% of fixes were reviewed, and 31.5% of those were accepted (overall, 25.5% of all generated fixes landed). Engineer feedback ranged from positive (quick approvals, surprises) to mixed (partially correct solutions used as starting points).", "conclusion": "Automated program repair using LLM-driven Engineering Agents is feasible and can efficiently address code issues at scale in large organizations, yielding a significant proportion of accepted code fixes and positive developer engagement. Combining neural reasoning with symbolic tools further enhances repair effectiveness."}}
{"id": "2507.18812", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18812", "abs": "https://arxiv.org/abs/2507.18812", "authors": ["Yiping Jia", "Zhen Ming Jiang", "Shayan Noei", "Ying Zou"], "title": "MemoCoder: Automated Function Synthesis using LLM-Supported Agents", "comment": null, "summary": "With the widespread adoption of Large Language Models (LLMs) such as GitHub\nCopilot and ChatGPT, developers increasingly rely on AI-assisted tools to\nsupport code generation. While LLMs can generate syntactically correct\nsolutions for well-structured programming tasks, they often struggle with\nchallenges that require iterative debugging, error handling, or adaptation to\ndiverse problem structures. Existing approaches such as fine-tuning or\nself-repair strategies either require costly retraining or lack mechanisms to\naccumulate and reuse knowledge from previous attempts.\n  To address these limitations, we propose MemoCoder, a multi-agent framework\nthat enables collaborative problem solving and persistent learning from past\nfixes. At the core of MemoCoder is a Fixing Knowledge Set, which stores\nsuccessful repairs and supports retrieval for future tasks. A central Mentor\nAgent supervises the repair process by identifying recurring error patterns and\nrefining high-level fixing strategies, providing a novel supervisory role that\nguides the self-repair loop. We evaluate MemoCoder across three public\nbenchmarks -- MBPP, HumanEval, and LiveCodeBench -- spanning a range of problem\ncomplexities. Experimental results show that MemoCoder consistently outperforms\nboth zero-shot prompting and a Self-Repair strategy, with improvements ranging\nfrom 3.1% to 12.1% in Pass@10 and from 1.4% to 14.5% in Pass@50, demonstrating\nits effectiveness in iterative refinement and knowledge-guided code generation.", "AI": {"tldr": "MemoCoder is a collaborative, multi-agent framework that helps LLMs improve code generation and debugging by learning iteratively from past fixes and deploying supervisory agents. Experiments show it outperforms existing approaches on major benchmarks.", "motivation": "Large Language Models help with code generation but struggle with iterative debugging and adapting to diverse or complex problem structures. Existing methods like fine-tuning or self-repair are either too expensive or do not allow for accumulating and leveraging past fixes.", "method": "The paper introduces MemoCoder, a multi-agent framework. It features a Fixing Knowledge Set for storing and retrieving successful code repairs and a central Mentor Agent that supervises the repair process by identifying patterns and refining strategies. MemoCoder is evaluated on different code generation benchmarks (MBPP, HumanEval, LiveCodeBench).", "result": "MemoCoder outperforms baseline methods including zero-shot prompting and Self-Repair across multiple benchmarks, showing significant improvement in Pass@10 and Pass@50 metrics.", "conclusion": "MemoCoder enables more effective and persistent learning from code repairs, leading to better code generation performance by leveraging iterative refinement and accumulated knowledge."}}
{"id": "2507.18833", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18833", "abs": "https://arxiv.org/abs/2507.18833", "authors": ["Wenyuan Jiang", "Diany Pressato", "Harsh Darji", "Thibaud Lutellier"], "title": "Exploring the Jupyter Ecosystem: An Empirical Study of Bugs and Vulnerabilities", "comment": null, "summary": "Background. Jupyter notebooks are one of the main tools used by data\nscientists. Notebooks include features (configuration scripts, markdown,\nimages, etc.) that make them challenging to analyze compared to traditional\nsoftware. As a result, existing software engineering models, tools, and studies\ndo not capture the uniqueness of Notebook's behavior. Aims. This paper aims to\nprovide a large-scale empirical study of bugs and vulnerabilities in the\nNotebook ecosystem. Method. We collected and analyzed a large dataset of\nNotebooks from two major platforms. Our methodology involved quantitative\nanalyses of notebook characteristics (such as complexity metrics, contributor\nactivity, and documentation) to identify factors correlated with bugs.\nAdditionally, we conducted a qualitative study using grounded theory to\ncategorize notebook bugs, resulting in a comprehensive bug taxonomy. Finally,\nwe analyzed security-related commits and vulnerability reports to assess risks\nassociated with Notebook deployment frameworks. Results. Our findings highlight\nthat configuration issues are among the most common bugs in notebook documents,\nfollowed by incorrect API usage. Finally, we explore common vulnerabilities\nassociated with popular deployment frameworks to better understand risks\nassociated with Notebook development. Conclusions. This work highlights that\nnotebooks are less well-supported than traditional software, resulting in more\ncomplex code, misconfiguration, and poor maintenance.", "AI": {"tldr": "Jupyter notebooks pose unique challenges compared to traditional software, leading to more bugs\u2014mainly configuration problems\u2014and higher security risks. These findings suggest a need for tailored software engineering tools and improved practices specific to notebooks.", "motivation": "Jupyter notebooks are widely used by data scientists, but their unique features (such as the integration of scripts, markdown, and images) make them tougher to analyze compared to traditional software, leading to gaps in existing software engineering tools and studies.", "method": "The study collected a large dataset of Jupyter notebooks from two major platforms and conducted quantitative analyses on their characteristics. Qualitative research using grounded theory was performed to create a bug taxonomy. Additionally, security-related commits and vulnerability reports were analyzed to assess deployment risks.", "result": "Configuration issues are the most common bugs in notebooks, followed by incorrect API usage. Popular deployment frameworks for notebooks also exhibit notable vulnerabilities.", "conclusion": "Jupyter notebooks lack the support typical of traditional software, resulting in more complex code, frequent misconfigurations, and poorer maintenance practices."}}
