<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 22]
- [cs.PL](#cs.PL) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Blueprint First, Model Second: A Framework for Deterministic LLM Workflow](https://arxiv.org/abs/2508.02721)
*Libin Qiu,Yuhang Ye,Zhirong Gao,Xide Zou,Junfu Chen,Ziming Gui,Weizhi Huang,Xiaobo Xue,Wenkai Qiu,Kun Zhao*

Main category: cs.SE

TL;DR: By separating deterministic workflow logic from the non-deterministic capabilities of LLMs, the Source Code Agent framework achieves superior reliability and efficiency for autonomous agents operating under strict procedural constraints, setting a new state-of-the-art on a challenging benchmark.


<details>
  <summary>Details</summary>
Motivation: Large language model (LLM) agents are powerful, but their inherent non-determinism makes them unsuitable for environments requiring strict procedural reliability and predictability. The challenge is that current systems merge high-level planning and low-level execution, leading to unpredictability.

Method: The paper introduces the Source Code Agent framework, which separates workflow logic from generative models. Expert-defined procedures are encoded as source code (Execution Blueprint) and executed deterministically, while the LLM is used only for specific complex sub-tasks rather than workflow decisions.

Result: On the tau-bench benchmark, the Source Code Agent framework outperformed the strongest baseline by 10.1 percentage points on the average Pass^1 score and improved execution efficiency significantly.

Conclusion: Decoupling procedural logic from generative models allows for verifiable, reliable, and efficient deployment of autonomous agents in environments that demand strict adherence to operational procedures.

Abstract: While powerful, the inherent non-determinism of large language model (LLM)
agents limits their application in structured operational environments where
procedural fidelity and predictable execution are strict requirements. This
limitation stems from current architectures that conflate probabilistic,
high-level planning with low-level action execution within a single generative
process. To address this, we introduce the Source Code Agent framework, a new
paradigm built on the "Blueprint First, Model Second" philosophy. Our framework
decouples the workflow logic from the generative model. An expert-defined
operational procedure is first codified into a source code-based Execution
Blueprint, which is then executed by a deterministic engine. The LLM is
strategically invoked as a specialized tool to handle bounded, complex
sub-tasks within the workflow, but never to decide the workflow's path. We
conduct a comprehensive evaluation on the challenging tau-bench benchmark,
designed for complex user-tool-rule scenarios. Our results demonstrate that the
Source Code Agent establishes a new state-of-the-art, outperforming the
strongest baseline by 10.1 percentage points on the average Pass^1 score while
dramatically improving execution efficiency. Our work enables the verifiable
and reliable deployment of autonomous agents in applications governed by strict
procedural logic.

</details>


### [2] [Interpreting Performance Profiles with Deep Learning](https://arxiv.org/abs/2508.02729)
*Zhuoran Liu*

Main category: cs.SE

TL;DR: This paper presents a system that enhances Java profilers by integrating AI-generated code summaries, making it easier for users to understand and optimize code performanceâ€”even if they didn't write the code themselves.


<details>
  <summary>Details</summary>
Motivation: Traditional profiling tools require users to interpret complex performance data and manually link inefficiencies to code semantics, which is particularly difficult for those who are not code authors. This limitation hampers the practical use of profilers for actionable optimizations.

Method: The proposed approach integrates performance profiles from Async Profiler with code summaries generated by a fine-tuned CodeBERT-based deep learning model. These enriched profiles are displayed via a graphical user interface, allowing users to see semantic information for selected call paths.

Result: The integrated system effectively assists users in analyzing and optimizing Java programs, as demonstrated on various Java benchmarks.

Conclusion: Combining deep learning-based code summaries with performance profiling streamlines the identification and optimization of program bottlenecks, making profiling tools more accessible and actionable, especially for non-authors of the analyzed code.

Abstract: Profiling tools (also known as profilers) play an important role in
understanding program performance at runtime, such as hotspots, bottlenecks,
and inefficiencies. While profilers have been proven to be useful, they give
extra burden to software engineers. Software engineers, as the users, are
responsible to interpret the complex performance data and identify actionable
optimization in program source code. However, it can be challenging for users
to associate inefficiencies with the program semantics, especially if the users
are not the authors of the code, which limits the applicability of profilers.
  In this thesis, we explore a new direction to combine performance profiles
and program semantics with a deep learning approach. The key idea is to glean
code summary for semantic information (at a certain level) and integrate it
into a profiler, which can better understand program inefficiencies for
actionable optimization. To be concrete, we combine profiles generated by Async
Profiler (the state-of-the-art Java profiler) with code summarization from a
fine-tuned CodeBERT-based model. We demonstrate the code summaries of any
selected call path in a graphic user interface. Our system can effectively
assist analysis on many Java benchmarks.

</details>


### [3] [A Note on Code Quality Score: LLMs for Maintainable Large Codebases](https://arxiv.org/abs/2508.02732)
*Sherman Wong,Jalaj Bhandari,Leo Zhou Fan Yang,Xylan Xu,Yi Zhuang,Cem Cayiroglu,Payal Bhuptani,Sheela Yadawad,Hung Duong*

Main category: cs.SE

TL;DR: A novel LLM-powered system (CQS) improves code quality at scale by detecting issues and providing actionable code review feedback, achieving reliable precision and user approval in industrial deployment.


<details>
  <summary>Details</summary>
Motivation: Maintaining code quality is challenging in large-scale software systems, especially when many engineers collaborate on the same codebase. Traditional methods are insufficient for timely, high-precision code issue detection and feedback.

Method: The paper introduces the Code Quality Score (CQS) system, which uses two fine-tuned Llama3 models (via SFT and offline reinforcement learning): one to detect code quality issues and the other to provide critiques for LLM-generated code reviews. The system incorporates hand-crafted rules to filter hallucinatory or incorrect responses. The authors also discuss developer feedback curation for LLM fine-tuning.

Result: Offline evaluations show high precision in issue detection. The CQS system has been deployed at industrial scale and consistently achieves a 60% week-over-week user helpfulness rate in production.

Conclusion: The CQS system effectively aids in maintaining code quality in large software projects by combining advanced LLMs, fine-tuning, rule-based filtering, and user feedback. It proves valuable in real-world, large-scale development environments.

Abstract: Maintaining code quality in large-scale software systems presents significant
challenges, particularly in settings where a large numbers of engineers work
concurrently on a codebase. This paper introduces Code Quality Score (CQS)
system to automatically detect issues with a set of code changes and provide
actionable insights. At its core, the CQS system is powered by two Llama3
models, fine-tuned (with SFT and offline RL approaches), to a) detect common
code quality issues related to coding best practices and b) to provide good
``critiques'' for LLM-generated code review respectively. To maintain good user
experience, we layer the system with hand-crafted rules to filter out incorrect
responses/hallucinations. Offline evaluations show that our CQS system is able
to achieve an impressive precision rate for identifying valid issues. This
system has already been rolled out to developers in an industrial scale setting
and has consistently achieved 60\% week over week user helpfulness rate,
demonstrating its effectiveness in a real-world environment. In this paper, we
present details of the CQS system along with some learnings on curating
developer feedback to create training data for LLM fine-tuning.

</details>


### [4] [What's in a Proof? Analyzing Expert Proof-Writing Processes in F* and Verus](https://arxiv.org/abs/2508.02733)
*Rijul Jain,Shraddha Barke,Gabriel Ebner,Md Rakib Hossain Misu,Shan Lu,Sarah Fakhoury*

Main category: cs.SE

TL;DR: The paper analyzes how expert developers interact with proof-oriented programming languages (POPLs) by observing their behavior using fine-grained code telemetry, identifying strategies and practices, and offering design guidance for better AI proof assistants.


<details>
  <summary>Details</summary>
Motivation: POPLs promise formal software verification but have high barriers to entry. Understanding expert user behavior can help create better tools and lower these barriers, especially via smarter proof assistants and engineering.

Method: The authors conducted a user study tracking detailed source code interactions of eight POPL experts using F* and Verus, analyzed behavior and practices, and tested recommendations by implementing an AI proof agent.

Result: The analysis identified three expert strategies and several informal practices not visible in code snapshots but predictive of success. Applying these insights improved the performance of an AI proof agent compared to baseline large language models.

Conclusion: The study finds that expert proof development incorporates strategies like early specification, clear sub-goal decomposition, controlled error handling, and systematic interaction with verifiers. Applying these to an AI proof agent shows performance improvements over standard LLM approaches.

Abstract: Proof-oriented programming languages (POPLs) empower developers to write code
alongside formal correctness proofs, providing formal guarantees that the code
adheres to specified requirements. Despite their powerful capabilities, POPLs
present a steep learning curve and have not yet been adopted by the broader
software community. The lack of understanding about the proof-development
process and how expert proof developers interact with POPLs has hindered the
advancement of effective proof engineering and the development of
proof-synthesis models/tools.
  In this work, we conduct a user study, involving the collection and analysis
of fine-grained source code telemetry from eight experts working with two
languages, F* and Verus. Results reveal interesting trends and patterns about
how experts reason about proofs and key challenges encountered during the proof
development process. We identify three distinct strategies and multiple
informal practices that are not captured final code snapshots, yet are
predictive of task outcomes. We translate these findings into concrete design
guidance for AI proof assistants: bias toward early specification drafting,
explicit sub-goal decomposition, bounded active errors, and disciplined
verifier interaction. We also present a case study of an F* proof agent
grounded in these recommendations, and demonstrate improved performance over
baseline LLMs

</details>


### [5] [Automated Code Repair for C/C++ Static Analysis Alerts](https://arxiv.org/abs/2508.02820)
*David Svoboda,Lori Flynn,William Klieber,Michael Duggan,Nicholas Reimer,Joseph Sible*

Main category: cs.SE

TL;DR: This paper presents an automated program repair (APR) tool that significantly reduces static analysis (SA) alerts in C/C++ code by repairing or dismissing a high percentage of alerts from multiple SA tools. The approach proves effective and minimally impacts code performance, providing practical insights and resources for future research.


<details>
  <summary>Details</summary>
Motivation: Manual triage and repair of SA alerts in C/C++ is labor-intensive, and many alerts are false positives. Automating this process can save significant effort and improve code security and quality.

Method: The authors designed and developed an APR tool targeting three flaw categories detected by multiple SA tools. They conducted empirical testing on alerts from real codebases, publishing both their tool and datasets. Repairs were kept simple and local, supporting trust and developer acceptance.

Result: The tool repaired 8718 out of 9234 alerts in one codebase, and over 80% across two codebases and two SA tools for two flaw categories. Repairs did not significantly affect performance or introduce new issues, with one minor exception.

Conclusion: The APR tool is effective in repairing or dismissing a majority of the SA alerts, influencing updates to coding standards, and does not degrade code performance. It highlights both the capabilities and limitations of automated repair solutions.

Abstract: (Note: This work is a preprint.) Static analysis (SA) tools produce many
diagnostic alerts indicating that source code in C or C++ may be defective and
potentially vulnerable to security exploits. Many of these alerts are false
positives. Identifying the true-positive alerts and repairing the defects in
the associated code are huge efforts that automated program repair (APR) tools
can help with. Our experience showed us that APR can reduce the number of SA
alerts significantly and reduce the manual effort of analysts to review code.
This engineering experience paper details the application of design,
development, and performance testing to an APR tool we built that repairs C/C++
code associated with 3 categories of alerts produced by multiple SA tools. Its
repairs are simple and local. Furthermore, our findings convinced the
maintainers of the CERT Coding Standards to re-assess and update the metrics
used to assess when violations of guidelines are detectable or repairable. We
discuss engineering design choices made to support goals of trustworthiness and
acceptability to developers. Our APR tool repaired 8718 out of 9234 alerts
produced by one SA tool on one codebase. It can repair 3 flaw categories. For 2
flaw categories, 2 SA tools, and 2 codebases, our tool repaired or dismissed as
false positives over 80% of alerts, on average. Tests showed repairs did not
appreciably degrade the performance of the code or cause new alerts to appear
(with the possible exception of sqlite3.c). This paper describes unique
contributions that include a new empirical analysis of SA data, our selection
method for flaw categories to repair, publication of our APR tool, and a
dataset of SA alerts from open-source SA tools run on open-source codebases. It
discusses positive and negative results and lessons learned.

</details>


### [6] [Automated Validation of LLM-based Evaluators for Software Engineering Artifacts](https://arxiv.org/abs/2508.02827)
*Ora Nova Fandina,Eitan Farchi,Shmulik Froimovich,Rami Katan,Alice Podolsky,Orna Raz,Avi Ziv*

Main category: cs.SE

TL;DR: REFINE is an automated framework for rigorously benchmarking LLM code evaluators with fine control over evaluation granularity, now used in IBM for COBOL-related tasks, greatly improving evaluation reliability and supporting key model release choices.


<details>
  <summary>Details</summary>
Motivation: As automation in software engineering increasingly relies on large language models (LLMs) to evaluate code, there is a need for reliable, scalable, and nuanced evaluatorsâ€”existing methods are either costly, subjective, or unable to detect fine-grained quality differences.

Method: The authors propose REFINE, an automated benchmark framework with two modules: (1) Hierarchy Dataset Builder, which auto-generates artifacts of systematically degraded quality, and (2) Evaluator Tester, which measures how well candidate evaluators rank these artifacts compared to expected orderings. The framework allows controllable granularity of evaluation.

Result: REFINE was implemented in IBM's internal workflows for code generation, translation, and summarization tasks in COBOL using industrial data. It successfully identified configurations of LLM evaluators that improved alignment scores from below 0.7 to above 0.9 in certain tasks. These evaluators are now in active use for model release decisions.

Conclusion: REFINE offers a systematic, automated, and nuanced way to benchmark and refine LLM evaluators for software engineering tasks, providing reliable support for production-level decisions and advancing the scalability and objectivity of code evaluation.

Abstract: Automation in software engineering increasingly relies on large language
models (LLMs) to generate, review, and assess code artifacts. However,
establishing LLMs as reliable evaluators remains an open challenge: human
evaluations are costly, subjective and non scalable, while existing automated
methods fail to discern fine grained variations in artifact quality.
  We introduce REFINE (Ranking Evaluators for FIne grained Nuanced Evaluation),
an automated framework for benchmarking LLM based evaluators across software
engineering tasks. REFINE comprises of two modules: Hierarchy Dataset Builder
applies novel generation techniques to automatically synthesize artifacts with
progressively reduced quality, and Evaluator Tester quantifies each candidate
evaluator configuration by measuring how closely its rankings align with
expected ordering.
  A key feature of REFINE is controllability: users can tune the granularity of
degradation to progressively refine evaluator configurations, from coarse
filtering to stress testing on subtle quality gaps.
  While the methodology is general, we focus on coding tasks reflecting the
practical demands in our production setting. REFINE was integrated into IBM's
internal development workflows and applied to code generation, translation, and
summarization for COBOL, an enterprise critical programming language, using
industrial data. It was used to identify LLM as a Judge configurations that
lifted alignment scores from below $0.7$ to above $0.9$ in some coding tasks.
These nuance sensitive evaluators are now actively used by model training teams
to support model release decisions.

</details>


### [7] [Developer Perceptions on Utilising Low-Code Approaches to Build Accessible and Adaptive Applications for Seniors](https://arxiv.org/abs/2508.02968)
*Shavindra Wickramathilaka,John Grundy,Kashumi Madampe,Omar Haggag*

Main category: cs.SE

TL;DR: The paper studies how a low-code tool (AdaptForge) can help developers efficiently build software tailored to seniors' needs by automating accessibility and adaptability, based on interviews with practitioners, and provides recommendations for future tool design.


<details>
  <summary>Details</summary>
Motivation: With the global population aging, there is an increasing need for digital services that support the independence of older adults. Current development practices often lack the resources and time to create software that is both accessible and personalized for seniors, leading to barriers. Additionally, regulatory pressures (like the European Accessibility Act) and personal motivations heighten the need for better development tools.

Method: The paper uses an interview-based empirical study with 18 software practitioners to evaluate AdaptForge, a low-code model-driven engineering tool designed to efficiently create accessible, adaptive applications for seniors.

Result: The study identifies what developers expect from industry-standard accessibility tools and offers empirically grounded recommendations for the design of low-code tools that support accessible and adaptive software development for senior users.

Conclusion: Adopting low-code, model-driven tools such as AdaptForge can help practitioners overcome constraints in time and resources, resulting in more accessible and adaptive applications for seniors, while also aligning with regulatory and personal motivations.

Abstract: The global ageing population presents a growing societal challenge, creating
an urgent need for inclusive technologies that promote autonomy among older
adults. Software practitioners can address this by delivering digital services
that enhance seniors' independence and reduce reliance on routine support from
family members and healthcare infrastructure. However, traditional development
practices, constrained by time and resources, often result in applications with
major accessibility and personalisation barriers. Increasing pressure from
regulatory requirements, such as the European Accessibility Act (EAA), and the
personal empathy many developers feel toward supporting their older loved ones
and their own future selves have created a demand for tools that support the
development of accessible and adaptive software. To address this demand, this
paper presents an interview-based empirical study with 18 software
practitioners, evaluating AdaptForge: a low-code model-driven engineering (MDE)
tool that enables the efficient creation of accessible and adaptive
applications for senior users by mitigating development constraints through
automated code generation. Based on these insights, we identify developer
expectations for adopting such tools as industry-standard solutions and provide
empirically grounded recommendations for designing low-code tools that support
accessible and adaptive software development.

</details>


### [8] [MRG-Bench: Evaluating and Exploring the Requirements of Context for Repository-Level Code Generation](https://arxiv.org/abs/2508.02998)
*Haiyang Li*

Main category: cs.SE

TL;DR: MRG-Bench is a new multi-language, real-world code generation benchmark revealing that current LLM methods struggle with understanding user requirements, highlighting the need for better context-aware and language-specific evaluation and modeling.


<details>
  <summary>Details</summary>
Motivation: Current evaluation datasets for LLM-based code generation are flawed, with limitations such as lack of runnable tests, non-representative code distributions, and focus only on Python, undermining the reliability of evaluation.

Method: The authors introduce MRG-Bench, a new dataset sourced from real-world code repositories, supporting multiple languages (Python, Java, Go), and providing project-level runnable test cases. They conduct experiments on LLMs, long-context models, and RAG-related methods, including error annotation experiments.

Result: Results show current repository-level code generation models have significant performance shortfalls, mainly due to inability to accurately understand user requirements. The effect of contextual information varies by language, suggesting language-specific context design is necessary.

Conclusion: MRG-Bench provides a more realistic benchmark, revealing fundamental deficiencies in LLM-based repository-level code generation, especially around requirement understanding and context utilization, and points to the need for more specialized solutions.

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
code generation. However, current evaluation datasets suffer from issues such
as the lack of runnable test cases, deviation from the distribution of
real-world code, and the ability to evaluate only the Python language. These
limitations undermine the credibility of the evaluation results.
  To address these limitations, we introduce \textbf{MRG-Bench} (Multi-language
Repository-level Code Generation Benchmark), a novel dataset that provides a
more accurate evaluation of LLMs in practical repository-level code generation
tasks. MRG-Bench has three main features: (1) practical data sourced from
real-world code repositories that align to the practical distribution, (2)
multiple programming languages support, including Python, Java, and Go, and (3)
project-level runnable test cases to assess the quality of the generated code.
  Based on MRG-Bench, we conducted extensive experiments including large
language models, long-context models, and RAG-related methods. These evaluation
results demonstrate that \textbf{current repository-level code generation
techniques suffer from significant performance deficiencies}. To further
investigate why models fail, we designed novel experiments to annotate the
underlying causes of generation errors. The results explicitly show that the
majority of methods suffer from "\textbf{difficulty in understanding user
requirements}," failing to comprehend their assigned tasks accurately.
Moreover, the impact of different repository-level contexts on this issue
exhibits significant disparities across different programming languages,
suggesting that, in practice, specialized contextual information needs to be
designed for different languages.

</details>


### [9] [Tool-integrated Reinforcement Learning for Repo Deep Search](https://arxiv.org/abs/2508.03012)
*Zexiong Ma,Chao Peng,Qunhong Zeng,Pengfei Gao,Yanzhen Zou,Bing Xie*

Main category: cs.SE

TL;DR: ToolTrain, a two-stage tool-integrated training framework, boosts LLMs' ability to localize software issues by using repository tools, reaching state-of-the-art results and improving automated bug resolution.


<details>
  <summary>Details</summary>
Motivation: Issue localization in software is hard due to the gap between natural language issue descriptions and code, and requires complex reasoning. Existing LLM-based tools don't adequately bridge this gap.

Method: The authors propose ToolTrain, a two-stage tool-integrated training framework. It combines rejection-sampled supervised fine-tuning and tool-integrated reinforcement learning to train LLMs to use repository retrieval tools effectively for issue localization.

Result: ToolTrain-trained models achieve state-of-the-art results in function-level issue localization, outperforming even advanced models like Claude-3.7. Enhanced localization leads to better end-to-end issue resolution.

Conclusion: Specialized training for issue localization with integrated tool use can significantly improve automated software development performance, validating the effectiveness of the ToolTrain framework.

Abstract: Issue localization, the process of identifying code locations that need
modification to resolve software issues, is a critical yet challenging task in
software development. The semantic gap between natural language issue
descriptions and faulty code requires complex multi-hop reasoning through code
dependencies. Existing LLM-based agents attempt to address this by integrating
repository retrieval tools. However, this transforms issue localization into a
demanding task we call Repo Deep Search, which requires the LLM to effectively
utilize various repository retrieval tools throughout a multi-step reasoning
and navigation process. To tackle this challenge, we present ToolTrain, a
two-stage tool-integrated training framework combining rejection-sampled
supervised fine-tuning and tool-integrated reinforcement learning to enhance
LLMs' ability to use retrieval tools for issue localization. Experimental
results show that ToolTrain-trained models achieve state-of-the-art
performance, with our 32B model even surpassing Claude-3.7 on function-level
localization. The results also show that improved localization performance
translates to better end-to-end issue resolution performance. This further
demonstrates that training for issue localization is a viable and effective
strategy for improving automated software development.

</details>


### [10] [StoneDetector: Conventional and versatile code clone detection for Java](https://arxiv.org/abs/2508.03435)
*Thomas S. Heinze,AndrÃ© SchÃ¤fer,Wolfram Amme*

Main category: cs.SE

TL;DR: StoneDetector is a new tool for finding code clones in Java, effective across both source code and Bytecode, and it performs well compared to other tools.


<details>
  <summary>Details</summary>
Motivation: The motivation is that copy & paste coding frequently leads to duplicated and modified code, causing maintenance problems like bug or vulnerability propagation. Efficiently identifying these code clones helps manage software quality and reliability.

Method: StoneDetector uses a conventional clone detection approach by performing textual comparison of paths derived from dominator trees representing the code. The method is flexible and configurable with different string metrics and hashing techniques.

Result: The paper presents StoneDetector, a platform for detecting code clones in Java source code and Bytecode. StoneDetector works by using textual comparisons of paths formed from dominator trees, allowing detection of exact, near-miss, and harder-to-detect clones. The paper includes a detailed evaluation of the tool against existing methods on benchmark datasets, showing its strong performance and scalability. The platform also supports configuration with various string metrics and hashing algorithms.

Conclusion: The authors conclude that StoneDetector is a versatile and scalable clone detection platform, capable of identifying a variety of clone types in Java code with good performance.

Abstract: Copy & paste is a widespread practice when developing software and, thus,
duplicated and subsequently modified code occurs frequently in software
projects. Since such code clones, i.e., identical or similar fragments of code,
can bloat software projects and cause issues like bug or vulnerability
propagation, their identification is of importance. In this paper, we present
the StoneDetector platform and its underlying method for finding code clones in
Java source and Bytecode. StoneDetector implements a conventional clone
detection approach based upon the textual comparison of paths derived from the
code's representation by dominator trees. In this way, the tool does not only
find exact and syntactically similar near-miss code clones, but also code
clones that are harder to detect due to their larger variety in the syntax. We
demonstrate StoneDetector's versatility as a conventional clone detection
platform and analyze its various available configuration parameters, including
the usage of different string metrics, hashing algorithms, etc. In our
exhaustive evaluation with other conventional clone detectors on several
state-of-the-art benchmarks, we can show StoneDetector's performance and
scalability in finding code clones in both, Java source and Bytecode.

</details>


### [11] [A System Model Generation Benchmark from Natural Language Requirements](https://arxiv.org/abs/2508.03215)
*Dongming Jin,Zhi Jin,Linyu Li,Zheng Fang,Jia Li,Xiaohong Chen*

Main category: cs.SE

TL;DR: SysMBench is a new benchmark for testing LLMs on system model generation. Evaluations from 17 LLMs reveal that their performance is currently quite weak, highlighting significant room for improvement.


<details>
  <summary>Details</summary>
Motivation: Developing formal system models is crucial but challenging due to complex syntax and limited examples. While LLMs are promising for code tasks, their ability to generate system models hasn't been rigorously benchmarked. The paper aims to fill this gap by providing a dedicated evaluation benchmark and metric.

Method: The authors constructed a benchmark (SysMBench) containing 151 human-curated system modeling scenarios, each with requirements, formal system models, and visual diagrams. They designed SysMEval, a semantic evaluation metric, and conducted experiments with 17 LLMs assessed via three traditional metrics and SysMEval under various prompting strategies.

Result: The paper introduces SysMBench, a benchmark dataset of 151 curated scenarios for evaluating large language models (LLMs) on their ability to generate system models in specific model description languages. Each scenario includes a natural language requirements description, a system model, and a visual diagram. A novel semantic-aware metric, SysMEval, is introduced for assessing model quality. Seventeen LLMs are evaluated using both existing metrics and SysMEval, demonstrating that current LLMs perform poorly on this task.

Conclusion: The results show that state-of-the-art LLMs are not yet effective at generating quality system models from natural language requirements, as indicated by low BLEU and semantic evaluation scores. This highlights a gap and motivates future research. The dataset and evaluation framework are made publicly available to support further studies.

Abstract: System models, a critical artifact in software development, provide a formal
abstraction of both the structural and behavioral aspects of software systems,
which can facilitate the early requirements analysis and architecture design.
However, developing system models remains challenging due to the specific
syntax of model description languages and the relative scarcity of public model
examples. While large language models (LLMs) have shown promise in generating
code with programming languages and could potentially aid in system model
development, no benchmarks currently exist for evaluating their ability to
generate system models with specific description languages. We present
SysMBench, which comprises 151 human-curated scenarios spanning a wide range of
popular domains and varying difficulty levels. Each scenario mainly comprises a
natural language requirements description, a system model expressed in a
specific model description language, and a visualized system model diagram. The
requirements description is fed as user input to the LLM, the system model with
description language is used to verify if the generated system model conforms
to the requirements, and the visualized diagram serves to support manual
validation. We introduce SysMEval, a semantic-aware evaluation metric to
evaluate the quality of generated system models. We evaluate 17 popular LLMs on
this task with three traditional metrics and SysMEval, from directly prompting
to three commonly used enhancement strategies. Our in-depth evaluation shows
that LLMs perform poorly on SysMBench, with the highest BLEU of 4% and
SysMEval-F1 of 62%. We release the SysMBench and its evaluation framework to
enable future research on LLM-based system model generation.

</details>


### [12] [ReFuzzer: Feedback-Driven Approach to Enhance Validity of LLM-Generated Test Programs](https://arxiv.org/abs/2508.03603)
*Iti Shree,Karine Even-Mendoz,Tomasz Radzik*

Main category: cs.SE

TL;DR: ReFuzzer refines LLM-generated programs to fix invalid cases, greatly increasing validity and code coverage, and making fuzz testing compilers more effective.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based compiler fuzzers tend to generate test programs that are often syntactically or semantically invalid. These invalid programs reduce the ability of fuzzers to thoroughly test compiler optimizations and backend components.

Method: ReFuzzer is introduced as a framework that refines LLM-generated test programs by systematically detecting and correcting both compilation and runtime violations. It uses a feedback loop with a local LLM to validate and filter out erroneous programs before execution.

Result: ReFuzzer significantly improved the validity of test programs, raising it from 47.0-49.4% to 96.6-97.3%. The average processing time per test was 2.9-3.5 seconds on a dual-GPU machine. Additionally, refuzzing improved code coverage in critical compiler components (e.g., vectorization coverage improved by up to 9.2% depending on the fuzzing type).

Conclusion: ReFuzzer enhances the effectiveness of LLM-based compiler fuzzing by producing a higher percentage of valid test programs and increasing coverage in compiler optimization and IR generation components.

Abstract: Existing LLM-based compiler fuzzers often produce syntactically or
semantically invalid test programs, limiting their effectiveness in exercising
compiler optimizations and backend components. We introduce ReFuzzer, a
framework for refining LLM-generated test programs by systematically detecting
and correcting compilation and runtime violations (e.g. division by zero or
array out-of-bounds accesses). ReFuzzer employs a feedback loop with a local
LLM to validate and filter erroneous programs before execution, improving
fuzzing effectiveness beyond crash detection and enabling the generation of
diverse yet valid test programs.
  We evaluated ReFuzzer's effectiveness across black-, grey- and white-box
fuzzing approaches targeting LLVM/Clang. ReFuzzer improved test programs'
validity from 47.0-49.4% to 96.6-97.3%, with an average processing time of
2.9-3.5 s per test program on a dual-GPU machine. Further, refuzzing
significantly increased code coverage in critical optimization and IR
generation components. For example, vectorization coverage had an absolute
improvement of 9.2%, 2.3%, and 7.1% in black-, grey-, and white-box fuzzing,
enhancing testing effectiveness.

</details>


### [13] [SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization](https://arxiv.org/abs/2508.03258)
*Yueyue Liu,Hongyu Zhang,Yuantian Miao*

Main category: cs.SE

TL;DR: SmartLLMs Scheduler (SLS) uses dynamic, feedback-driven scheduling with adaptive caching to optimize LLM use in software engineering, yielding substantial performance gains and cost savings over static methods.


<details>
  <summary>Details</summary>
Motivation: Despite advances in Large Language Models (LLMs) for software engineering, practical deployment is hampered by high costs, slow response times, and inconsistent performance, especially at scale. Conventional static scheduling approaches require heavy training data and lack adaptability, making them expensive and less flexible.

Method: SLS consists of three main components: (1) An Adaptive Cache Manager to reuse outputs and reduce computation; (2) A Performance-Cost Optimized Scheduler to allocate queries dynamically based on predicted LLM performance and costs; and (3) A Dynamic Update Manager to continuously improve strategies with real-time feedback. The system was evaluated through experiments on log parsing and code generation.

Result: The proposed SmartLLMs Scheduler (SLS) solution outperformed baseline methods, delivering an average performance boost of 198.82% and reducing processing time by 63.28% in software engineering tasks such as log parsing and code generation.

Conclusion: SLS effectively improves performance and reduces costs for LLM deployment in software engineering tasks by leveraging real-time feedback, adaptive caching, and dynamic task allocation, demonstrating strong results in experiments compared to existing approaches.

Abstract: Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable
capabilities in a variety of software engineering tasks. Despite the
advancements, their practical deployment faces challenges, including high
financial costs, long response time, and varying performance, especially when
handling a large number of queries (jobs). Existing optimization strategies for
deploying LLMs for diverse tasks focus on static scheduling, which requires
extensive training data for performance prediction, increasing the
computational costs and limiting the applicability and flexibility. In this
paper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective
scheduling solution. The key idea is to learn LLMs' performance on diverse
tasks and incorporate their real-time feedback to update strategies
periodically. Specifically, SLS incorporates three key components, including an
Adaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic
Update Manager. The Cache Manager stores the outputs of previously processed
queries and employs an adaptive strategy to reduce redundant computations and
minimize response times. For queries not found in the cache, the Scheduler
dynamically allocates them to the most suitable LLM based on the predicted
performance and cost from models that take both query-specific and LLM-specific
features as input. The Update Manager continuously refines the cache and
scheduling strategies based on real-time feedback from the assigned queries to
enhance decision-making and adapt to evolving task characteristics. To evaluate
the effectiveness of SLS, we conduct extensive experiments on two LLM-based
software engineering tasks, including log parsing and code generation. The
results show that SLS significantly outperforms the baseline methods, achieving
an average performance improvement of 198.82% and an average processing time
reduction of 63.28%.

</details>


### [14] [GUI-ReRank: Enhancing GUI Retrieval with Multi-Modal LLM-based Reranking](https://arxiv.org/abs/2508.03298)
*Kristian Kolthoff,Felix Kretzer,Christian Bartelt,Alexander Maedche,Simone Paolo Ponzetto*

Main category: cs.SE

TL;DR: The paper introduces GUI-ReRank, a novel NL-based system for retrieving GUI prototypes that combines fast initial matching with advanced reranking using MLLMs. It outperforms previous methods in accuracy and generalizability, supports easy repository customization, and offers analysis on efficiency and cost.


<details>
  <summary>Details</summary>
Motivation: GUI prototyping is essential for interactive system development, aiding requirements elicitation, stakeholder collaboration, and early design validation. However, creating GUI prototypes is often resource-intensive, requiring significant effort and expertise. While natural language (NL)-based GUI retrieval methods have been developed to ease this burden, existing approaches struggle with limited performance and poor generalizability.

Method: The authors propose GUI-ReRank, a new framework combining rapid embedding-based constrained retrieval for initial candidate selection and powerful Multimodal Large Language Model (MLLM)-based reranking to refine results. The framework also supports customizable repository annotation and embedding, broadening applicability and allowing users to tailor search for their own GUI collections.

Result: Experimental evaluation shows that GUI-ReRank significantly outperforms state-of-the-art (SOTA) learning-to-rank (LTR) models on benchmark NL-based GUI retrieval tasks. It demonstrates higher retrieval accuracy and better generalizability across different GUI datasets. Additionally, a cost and efficiency analysis highlights the computational trade-offs of using MLLMs for reranking.

Conclusion: GUI-ReRank advances NL-based GUI retrieval by integrating efficient initial retrieval with accurate MLLM-based reranking, customizable repository support, and strong performance on diverse datasets. It enables faster and more effective prototype discovery while providing insights on computational cost trade-offs.

Abstract: GUI prototyping is a fundamental component in the development of modern
interactive systems, which are now ubiquitous across diverse application
domains. GUI prototypes play a critical role in requirements elicitation by
enabling stakeholders to visualize, assess, and refine system concepts
collaboratively. Moreover, prototypes serve as effective tools for early
testing, iterative evaluation, and validation of design ideas with both end
users and development teams. Despite these advantages, the process of
constructing GUI prototypes remains resource-intensive and time-consuming,
frequently demanding substantial effort and expertise. Recent research has
sought to alleviate this burden through NL-based GUI retrieval approaches,
which typically rely on embedding-based retrieval or tailored ranking models
for specific GUI repositories. However, these methods often suffer from limited
retrieval performance and struggle to generalize across arbitrary GUI datasets.
In this work, we present GUI-ReRank, a novel framework that integrates rapid
embedding-based constrained retrieval models with highly effective MLLM-based
reranking techniques. GUI-ReRank further introduces a fully customizable GUI
repository annotation and embedding pipeline, enabling users to effortlessly
make their own GUI repositories searchable, which allows for rapid discovery of
relevant GUIs for inspiration or seamless integration into customized LLM-based
RAG workflows. We evaluated our approach on an established NL-based GUI
retrieval benchmark, demonstrating that GUI-ReRank significantly outperforms
SOTA tailored LTR models in both retrieval accuracy and generalizability.
Additionally, we conducted a comprehensive cost and efficiency analysis of
employing MLLMs for reranking, providing valuable insights regarding the
trade-offs between retrieval effectiveness and computational resources. Video:
https://youtu.be/_7x9UCh82ug

</details>


### [15] [Industrial LLM-based Code Optimization under Regulation: A Mixture-of-Agents Approach](https://arxiv.org/abs/2508.03329)
*Mari Ashiga,Vardan Voskanyan,Fateme Dinmohammadi,Jingzhi Gong,Paul Brookes,Matthew Truscott,Rafail Giavrimis,Mike Basios,Leslie Kanthan,Wei Jie*

Main category: cs.SE

TL;DR: The paper introduces a Mixture-of-Agents approach for industrial code optimization that leverages multiple open-source LLMs, providing significant cost and speed benefits in regulated settings, validated with real-world data and compared with existing ensemble and single-model approaches.


<details>
  <summary>Details</summary>
Motivation: Organizations in regulated industries struggle to use commercial LLMs for code optimization due to privacy and compliance concerns, creating a barrier to accessing high-quality automated performance engineering without violating regulations or incurring high costs.

Method: Implemented a Mixture-of-Agents system synthesizing code with several specialized open-source LLMs. Its performance was compared empirically to a vanilla Genetic Algorithm-based ensemble and individual LLMs using 50 real-world code snippets, across seven LLMs, generating over 8,700 variants to assess cost, speed, and optimization quality.

Result: The Mixture-of-Agents (MoA) approach delivers 14.3%-22.2% cost savings and 28.6%-32.2% faster optimization times using open-source models, and both MoA and Genetic Algorithm ensembles outperform individual LLM optimizers. MoA is validated on 50 code snippets with seven LLM combinations and over 8,700 generated variants.

Conclusion: Mixture-of-Agents using open-source LLMs is an effective, regulation-compliant alternative to commercial LLMs for code optimization, delivering better cost-efficiency and speed. The study offers guidelines for choosing between ensemble strategies based on regulatory constraints and model availability.

Abstract: Recent advancements in Large Language Models (LLMs) for code optimization
have enabled industrial platforms to automate software performance engineering
at unprecedented scale and speed. Yet, organizations in regulated industries
face strict constraints on which LLMs they can use - many cannot utilize
commercial models due to data privacy regulations and compliance requirements,
creating a significant challenge for achieving high-quality code optimization
while maintaining cost-effectiveness. We address this by implementing a
Mixture-of-Agents (MoA) approach that directly synthesizes code from multiple
specialized LLMs, comparing it against TurinTech AI's vanilla Genetic Algorithm
(GA)-based ensemble system and individual LLM optimizers using real-world
industrial codebases. Our key contributions include: (1) First MoA application
to industrial code optimization using real-world codebases; (2) Empirical
evidence that MoA excels with open-source models, achieving 14.3% to 22.2% cost
savings and 28.6% to 32.2% faster optimization times for regulated
environments; (3) Deployment guidelines demonstrating GA's advantage with
commercial models while both ensembles outperform individual LLMs; and (4)
Real-world validation across 50 code snippets and seven LLM combinations,
generating over 8,700 variants, addresses gaps in industrial LLM ensemble
evaluation. This provides actionable guidance for organizations balancing
regulatory compliance with optimization performance in production environments.

</details>


### [16] [Key-Augmented Neural Triggers for Knowledge Sharing](https://arxiv.org/abs/2508.03340)
*Alex Wolf,Marco Edoardo Palma,Pooja Rani,Harald C. Gall*

Main category: cs.SE

TL;DR: KANT introduces a novel way to embed and access repository-specific knowledge using synthetic data and 'knowledge anchors,' improving LLM-based code comprehension while lowering latency and supporting privacy-friendly on-premise deployment.


<details>
  <summary>Details</summary>
Motivation: Repository-level code comprehension and knowledge sharing are difficult because relevant information is spread across multiple files, retrieval is inefficient in long contexts, repository-specific data is scarce, and privacy issues hinder industry adoption of proprietary LLMs.

Method: The paper introduces Key-Augmented Neural Triggers (KANT), which embeds knowledge anchors into both training and inference stages. KANT replaces verbose context with these anchors, reducing token load and enabling efficient, on-premise deployment. Specialized synthetic training data is generated directly from code, supporting repository-specific knowledge access.

Result: KANT was evaluated through human qualitative assessments, comparison with SOTA baselines, and generalizability across multiple LLMs. Results indicated the synthetic data matched user needs, KANT was preferred by over 60% of human annotators and 79% of a domain expert's cases, and inference latency dropped by up to 85% for all tested models.

Conclusion: KANT offers a scalable, low-latency, and privacy-friendly solution for repository-level code comprehension by efficiently grounding LLM inference in localized, repository-specific knowledge through synthetic training data and knowledge anchors. It outperforms existing baselines in user preference and speed, making it suitable for industrial use.

Abstract: Repository-level code comprehension and knowledge sharing remain core
challenges in software engineering. Large language models (LLMs) have shown
promise by generating explanations of program structure and logic. However,
these approaches still face limitations: First, relevant knowledge is
distributed across multiple files within a repository, aka semantic
fragmentation. Second, retrieval inefficiency and attention saturation degrade
performance in RAG pipelines, where long, unaligned contexts overwhelm
attention. Third, repository specific training data is scarce and often
outdated. Finally, proprietary LLMs hinder industrial adoption due to privacy
and deployment constraints. To address these issues, we propose Key-Augmented
Neural Triggers (KANT), a novel approach that embeds knowledge anchors into
both training and inference. Unlike prior methods, KANT enables internal access
to repository specific knowledge, reducing fragmentation and grounding
inference in localized context. Moreover, we synthesize specialized data
directly from code. At inference, knowledge anchors replace verbose context,
reducing token overhead and latency while supporting efficient, on premise
deployment. We evaluate KANT via: a qualitative human evaluation of the
synthesized dataset's intent coverage and quality across five dimensions;
compare against SOTA baselines across five qualitative dimensions and inference
speed; and replication across different LLMs to assess generalizability.
Results show that the synthetic training data aligned with information-seeking
needs. KANT achieved over 60% preference from human annotators and a LocalStack
expert (preferring 79% of cases). Also, KANT reduced inference latency by up to
85% across all models. Overall, it is well-suited for scalable, low-latency,
on-premise deployments, providing a strong foundation for code comprehension.

</details>


### [17] [Psychological safety in software workplaces: A systematic literature review](https://arxiv.org/abs/2508.03369)
*Beatriz Santana,LidivÃ¢nio Monte,Bianca Santana de AraÃºjo Silva,Glauco Carneiro,SÃ¡vio Freire,JosÃ© Amancio Macedo Santos,Manoel MendonÃ§a*

Main category: cs.SE

TL;DR: This systematic review shows that psychological safety is vital for software development teams, supporting innovation and performance. Key influencing factors are identified, but important knowledge gaps remain regarding how to best foster PS in different software engineering contexts.


<details>
  <summary>Details</summary>
Motivation: Despite the critical role of psychological safety (PS) in team well-being and performance, particularly in software development, there is limited research exploring PS specifically within software engineering. The complexity and rapid pace of the domain make cultivating PS challenging. The motivation for the paper is to address the absence of a systematic synthesis of knowledge about PS in software engineering.

Method: The authors conducted a systematic literature review, gathering data from four major digital libraries. Both quantitative and qualitative analyses were applied to the extracted information.

Result: The review uncovered increasing scholarly interest in PS in software engineering, with most studies based on Edmondson's framework. The paper identifies factors influencing PS at multiple levelsâ€”such as team autonomy, agile methodologies, and leadership behaviors. These are found to be key antecedents. PS is shown to contribute to innovation, learning, and team performance. Nevertheless, important questions remain about the specific contextual factors, mechanisms, and strategies for effectively fostering PS.

Conclusion: Psychological safety is essential for innovation and effective teamwork in software development, but there are still notable gaps in understanding how it operates in different contexts and how best to strengthen it. The authors highlight the need for further research into practical approaches to enhancing PS across various organizational settings within software engineering.

Abstract: Context: Psychological safety (PS) is an important factor influencing team
well-being and performance, particularly in collaborative and dynamic domains
such as software development. Despite its acknowledged significance, research
on PS within the field of software engineering remains limited. The
socio-technical complexities and fast-paced nature of software development
present challenges to cultivating PS. To the best of our knowledge, no
systematic secondary study has synthesized existing knowledge on PS in the
context of software engineering.
  Objective: This study aims to systematically review and synthesize the
existing body of knowledge on PS in software engineering. Specifically, it
seeks to identify the potential antecedents and consequences associated with
the presence or absence of PS among individuals involved in the software
development process.
  Methods: A systematic literature review was conducted, encompassing studies
retrieved from four digital libraries. The extracted data were subjected to
both quantitative and qualitative analyses.
  Results: The findings indicate a growing academic interest in PS within
software engineering, with the majority of studies grounded in Edmondson's
framework. Factors antecedents of PS were identified at the individual, team,
and organizational levels, including team autonomy, agile methodologies, and
leadership behaviors.
  Conclusion: PS fosters innovation, learning, and team performance within
software development. However, significant gaps persist in understanding the
contextual factors influencing PS, its underlying mechanisms, and effective
strategies for its enhancement. Future research should address these gaps by
investigating the practical applications of PS within diverse organizational
settings in the software engineering domain.

</details>


### [18] [Agentic AI in 6G Software Businesses: A Layered Maturity Model](https://arxiv.org/abs/2508.03393)
*Muhammad Zohaib,Muhammad Azeem Akbar,Sami Hyrynsalmi,Arif Ali Khan*

Main category: cs.SE

TL;DR: The paper maps factors helping or hindering agentic AI adoption in 6G software businesses, summarizing motivators and barriers, and sets the stage for a future maturity model to guide organizations in agent-first transformations.


<details>
  <summary>Details</summary>
Motivation: With the rise of 6G technologies, software businesses are exploring the potential benefits and challenges of agentic AI systems, which can enhance autonomy and decision-making but bring integration and maturity challenges. There is a need to understand the factors influencing their adoption.

Method: The study performed a preliminary thematic mapping based on a multivocal literature review and targeted document scanning, identifying and categorizing motivators and demotivators for the adoption of agentic software in 6G contexts.

Result: The research identified 29 motivators and 27 demotivators, grouping them into five high-level themes for each. This provides an organized perspective on facilitators and barriers to adopting agentic AI in 6G software businesses.

Conclusion: The study's structured thematic mapping informs organizational readiness and acts as a feasibility assessment in the early stages of creating a maturity model. This model aims to guide software organizations in adopting agent-first approaches compatible with 6G requirements.

Abstract: The emergence of agentic AI systems in 6G software businesses presents both
strategic opportunities and significant challenges. While such systems promise
increased autonomy, scalability, and intelligent decision-making across
distributed environments, their adoption raises concerns regarding technical
immaturity, integration complexity, organizational readiness, and
performance-cost trade-offs. In this study, we conducted a preliminary thematic
mapping to identify factors influencing the adoption of agentic software within
the context of 6G. Drawing on a multivocal literature review and targeted
scanning, we identified 29 motivators and 27 demotivators, which were further
categorized into five high-level themes in each group. This thematic mapping
offers a structured overview of the enabling and inhibiting forces shaping
organizational readiness for agentic transformation. Positioned as a
feasibility assessment, the study represents an early phase of a broader
research initiative aimed at developing and validating a layered maturity model
grounded in CMMI model with the software architectural three dimensions
possibly Data, Business Logic, and Presentation. Ultimately, this work seeks to
provide a practical framework to help software-driven organizations assess,
structure, and advance their agent-first capabilities in alignment with the
demands of 6G.

</details>


### [19] [On the Evaluation of Large Language Models in Multilingual Vulnerability Repair](https://arxiv.org/abs/2508.03470)
*Dong wang,Junji Yu,Honglin Shu,Michael Fu,Chakkrit Tantithamthavorn,Yasutaka Kamei,Junjie Chen*

Main category: cs.SE

TL;DR: Instruction-tuned GPT-4o, evaluated across seven programming languages, shows strong performanceâ€”often surpassing previous methodsâ€”in automatic software vulnerability repair, suggesting LLMs are promising for tackling vulnerabilities in diverse programming languages.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning-based software vulnerability repair approaches are typically limited to a single programming language (e.g., C/C++), restricting their applicability in today's multilingual software landscape. Recent large language models (LLMs) have shown potential for language-agnostic tasks but their ability to repair vulnerabilities across various languages remains unclear.

Method: The authors conducted a large-scale empirical study comparing both automated vulnerability repair approaches and state-of-the-art LLMs, notably GPT-4o (instruction-tuned and using few-shot prompting), across seven different programming languages. They systematically evaluated performance metrics, specifically analyzing the effectiveness of these models with unseen languages and varied vulnerability types.

Result: GPT-4o, when instruction-tuned and prompted with few-shot examples, performs competitively with VulMaster, the leading approach. The LLM-based approaches were particularly strong for unique and the most severe vulnerabilities, and demonstrated generalization to previously unseen languages better than traditional methods. Among languages tested, Go achieved the highest effectiveness, while C/C++ lagged behind.

Conclusion: Instruction-tuned LLMs, particularly GPT-4o, are promising for automated, multilingual software vulnerability repair, often outperforming existing state-of-the-art approaches and generalizing well across languages. The paper also discusses reasons for LLMs' failures and highlights the potential of LLMs in this domain.

Abstract: Various Deep Learning-based approaches with pre-trained language models have
been proposed for automatically repairing software vulnerabilities. However,
these approaches are limited to a specific programming language (C/C++). Recent
advances in large language models (LLMs) offer language-agnostic capabilities
and strong semantic understanding, exhibiting potential to overcome
multilingual vulnerability limitations. Although some work has begun to explore
LLMs' repair performance, their effectiveness is unsatisfactory. To address
these limitations, we conducted a large-scale empirical study to investigate
the performance of automated vulnerability repair approaches and
state-of-the-art LLMs across seven programming languages. Results show GPT-4o,
instruction-tuned with few-shot prompting, performs competitively against the
leading approach, VulMaster. Additionally, the LLM-based approach shows
superior performance in repairing unique vulnerabilities and is more likely to
repair the most dangerous vulnerabilities. Instruction-tuned GPT-4o
demonstrates strong generalization on vulnerabilities in previously unseen
language, outperforming existing approaches. Analysis shows Go consistently
achieves the highest effectiveness across all model types, while C/C++ performs
the worst. Based on findings, we discuss the promise of LLM on multilingual
vulnerability repair and the reasons behind LLM's failed cases. This work takes
the first look at repair approaches and LLMs across multiple languages,
highlighting the promising future of adopting LLMs for multilingual
vulnerability repair.

</details>


### [20] [BitsAI-Fix: LLM-Driven Approach for Automated Lint Error Resolution in Practice](https://arxiv.org/abs/2508.03487)
*Yuanpeng Li,Qi Long,Zhiyuan Yao,Jian Xu,Lintao Xie,Xu He,Lu Geng,Xin Han,Yueyan Chen,Wenbo Duan*

Main category: cs.SE

TL;DR: BitsAI-Fix is an LLM-driven automated lint error fixer that scales to industry needs, proves effective at ByteDance, and sets a new standard for managing technical debt in enterprise codebases.


<details>
  <summary>Details</summary>
Motivation: Enterprise codebases are massive and complex, leading to more lint errors than engineers can manually fix. This results in technical debt and reduced development speed.

Method: The paper introduces BitsAI-Fix, a workflow using Large Language Models (LLMs) for automated lint error remediation. It employs tree-sitter for code context, generates LLM-powered patches, and verifies fixes with a lint scan. An innovative progressive reinforcement learning (RL) strategy collects and uses feedback to continuously improve the model, and a new reward mechanism optimizes fixes for correctness and efficiency. A 'code diff matching' approach helps monitor ongoing effectiveness.

Result: In real-world deployment at ByteDance, BitsAI-Fix supported 5,000+ engineers, fixed over 12,000 lint issues, achieved about 85% remediation accuracy, and engaged around 1,000 weekly active users.

Conclusion: The system shows that LLM-based automated code remediation is practical and effective for large enterprises, offering a reference model for similar large-scale, industrial codebases.

Abstract: As enterprise codebases continue to grow in scale and complexity, the volume
of lint errors far exceeds engineers' manual remediation capacity, leading to
continuous accumulation of technical debt and hindered development efficiency.
This paper presents BitsAI-Fix, an automated lint error remediation workflow
based on Large Language Models (LLMs), designed to address this critical
challenge in industrial-scale environments. BitsAI-Fix employs tree-sitter for
context expansion and generates search-and-replace format patches through
specially trained LLMs, followed by lint scan re-verification to output final
remediation results. Additionally, our approach introduces an innovative
progressive reinforcement learning (RL) training strategy that can
automatically acquire verifiable training data during the project cold-start
phase and continuously iterate the model by collecting online samples through
feedback after system deployment. Furthermore, we designed a targeted
rule-based reward mechanism that combines format rewards and correctness
rewards while penalizing redundant modifications. We also propose a "code diff
matching" methodology to continuously track online effectiveness. In production
deployment at ByteDance, our solution has supported over 5,000 engineers,
resolved more than 12,000 static analysis issues, achieved approximately 85%
remediation accuracy, with around 1,000 weekly active adopters. This work
demonstrates the practical feasibility of LLM-based code remediation solutions
in enterprise environments and serves as a reference for automated code fix in
large-scale industrial scenarios.

</details>


### [21] [LaTCoder: Converting Webpage Design to Code with Layout-as-Thought](https://arxiv.org/abs/2508.03560)
*Yi Gui,Zhen Li,Zhongyi Zhang,Guohao Wang,Tianpeng Lv,Gaoyang Jiang,Yi Liu,Dongping Chen,Yao Wan,Hongyu Zhang,Wenbin Jiang,Xuanhua Shi,Hai Jin*

Main category: cs.SE

TL;DR: LaTCoder uses a stepwise, block-based approach to preserve layout fidelity when generating code from webpage designs. It outperforms previous methods, as shown by large improvements in layout metrics and human preference.


<details>
  <summary>Details</summary>
Motivation: Recent advances in multimodal large language models have improved design-to-code tasks, but these models struggle with accurately preserving webpage layouts. This limits their practical utility in real-world UI development, motivating the need for a more layout-aware methodology.

Method: 1. Webpage designs are split into image blocks using an efficient algorithm.
2. Multimodal LLMs are prompted in a Chain-of-Thought approach for each block.
3. Two different assembly strategies (absolute positioning and MLLM-based) are compared, with dynamic selection choosing the best output.
4. Evaluation is done on existing and new complex benchmarks using multiple state-of-the-art MLLMs and both automatic (TreeBLEU, MAE) and human assessments.

Result: LaTCoder, a new approach for converting webpage designs to code, improves layout preservation using a 'Layout-as-Thought' (LaT) strategy. The method involves segmenting the design into image blocks, prompting Large Language Models for each block, and dynamically assembling the outputs. This leads to significantly better results than previous methods, as shown by both automatic and human evaluations.

Conclusion: LaTCoder significantly enhances the layout preservation in design-to-code tasks, outperforming direct model prompting both in automatic scores and human evaluations. Its dynamic, block-based methodology provides a practical and effective workflow for accurate webpage code generation.

Abstract: Converting webpage designs into code (design-to-code) plays a vital role in
User Interface (UI) development for front-end developers, bridging the gap
between visual design and functional implementation. While recent Multimodal
Large Language Models (MLLMs) have shown significant potential in
design-to-code tasks, they often fail to accurately preserve the layout during
code generation. To this end, we draw inspiration from the Chain-of-Thought
(CoT) reasoning in human cognition and propose LaTCoder, a novel approach that
enhances layout preservation in webpage design during code generation with
Layout-as-Thought (LaT). Specifically, we first introduce a simple yet
efficient algorithm to divide the webpage design into image blocks. Next, we
prompt MLLMs using a CoTbased approach to generate code for each block.
Finally, we apply two assembly strategies-absolute positioning and an
MLLM-based method-followed by dynamic selection to determine the optimal
output. We evaluate the effectiveness of LaTCoder using multiple backbone MLLMs
(i.e., DeepSeek-VL2, Gemini, and GPT-4o) on both a public benchmark and a newly
introduced, more challenging benchmark (CC-HARD) that features complex layouts.
The experimental results on automatic metrics demonstrate significant
improvements. Specifically, TreeBLEU scores increased by 66.67% and MAE
decreased by 38% when using DeepSeek-VL2, compared to direct prompting.
Moreover, the human preference evaluation results indicate that annotators
favor the webpages generated by LaTCoder in over 60% of cases, providing strong
evidence of the effectiveness of our method.

</details>


### [22] [Intent Preserving Generation of Diverse and Idiomatic (Code-)Artifacts](https://arxiv.org/abs/2508.03642)
*Oliver Westphal*

Main category: cs.SE

TL;DR: Rather than manually writing complex generators for related artifacts, a compositional approach with reusable abstract building blocks results in easier, more adaptable, and automatic generation of various programming exercise materials.


<details>
  <summary>Details</summary>
Motivation: Automatically generating programming exercises requires automatic program generation, especially for providing sample solutions or defining task requirements. Creating adaptable, varied, and idiomatic program generators is challenging, especially when multiple related artifacts, like behavioral specifications and textual descriptions, must also be generated.

Method: The proposed method involves defining a small set of abstract building blocks and, for each, concrete realizations for different artifacts. The structure of the final artifacts is specified as a composition of these abstract blocks. This abstract specification serves as the common source from which all related artifacts are automatically derived.

Result: The approach enables automatic generation of multiple related artifacts (e.g., sample solutions, test specifications) from a single, abstract specification, increasing adaptability and consistency while reducing manual effort and complexity.

Conclusion: Using compositional abstract building blocks to specify related programming exercise artifacts allows for more scalable, adaptable, and efficient automatic generation, suitable for various contexts.

Abstract: When automatically generating programming exercise tasks one often also needs
to automatically generate programs. At the very least when providing sample
solutions is part of automated feedback. But programs can also be used as part
of the exercise task description to communicate a task's requirements.
  Writing good program generators that produce varied yet idiomatic code while
being easily adaptable for new tasks is challenging. The challenges are
intensified if task generation requires additional artifacts, like a more
general behavior specification for testing or additional textual descriptions.
Manually writing generators for multiple different but strongly related
artifacts gets complicated quickly.
  We present an approach where instead of writing monolithic generators for
multiple connected artifacts one specifies a small set of abstract building
blocks and for each such building block defines sets of concrete realizations
for various kinds of artifacts. Then the intended structure of the resulting
artifacts is specified as a composition of the small abstract building blocks.
This abstract description then serves as the common source from which related
artifacts can be derived automatically. The approach is generic in the kind of
artifacts it can produce and is therefore adaptable to a wide range of
contexts.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [23] [Compositional Quantum Control Flow with Efficient Compilation in Qunity](https://arxiv.org/abs/2508.02857)
*Mikhail Mints,Finn Voichick,Leonidas Lampropoulos,Robert Rand*

Main category: cs.PL

TL;DR: The paper presents an efficient, practical compilation of high-level quantum control flow in Qunity, introducing new abstractions and optimizations that drastically decrease circuit size and make the language usable in practice.


<details>
  <summary>Details</summary>
Motivation: Quantum programming languages largely rely on the quantum circuit model, which makes it hard to provide high-level abstractions, especially for quantum control flow. The Qunity language introduced such a construct but suffered from lack of practical implementation and inefficiency in its compilation approach.

Method: The authors take Qunity as a basis and work on efficient compilation of its high-level quantum control flow constructs. They introduce more abstractions atop Qunity's core language, implement a complete compiler that translates Qunity to OpenQASM 3, and apply optimization techniques at multiple compilation stages, both at low and high levels.

Result: The optimizations and new abstractions significantly reduce the number of qubits and gates required, making the produced quantum circuits much more efficient than those created by the original Qunity compilation procedure.

Conclusion: The work delivers a practical, optimized Qunity compiler, making high-level quantum control flow abstractions feasible and efficient for real quantum hardware.

Abstract: Most existing quantum programming languages are based on the quantum circuit
model of computation, as higher-level abstractions are particularly challenging
to implement - especially ones relating to quantum control flow. The Qunity
language, proposed by Voichick et al., offered such an abstraction in the form
of a quantum control construct, with great care taken to ensure that the
resulting language is still realizable. However, Qunity lacked a working
implementation, and the originally proposed compilation procedure was very
inefficient, with even simple quantum algorithms compiling to unreasonably
large circuits.
  In this work, we focus on the efficient compilation of high-level quantum
control flow constructs, using Qunity as our starting point. We introduce a
wider range of abstractions on top of Qunity's core language that offer
compelling trade-offs compared to its existing control construct. We create a
complete implementation of a Qunity compiler, which converts high-level Qunity
code into the quantum assembly language OpenQASM 3. We develop optimization
techniques for multiple stages of the Qunity compilation procedure, including
both low-level circuit optimizations as well as methods that consider the
high-level structure of a Qunity program, greatly reducing the number of qubits
and gates used by the compiler.

</details>


### [24] [SAGE-HLS: Syntax-Aware AST-Guided LLM for High-Level Synthesis Code Generation](https://arxiv.org/abs/2508.03558)
*M Zafir Sadik Khan,Nowfel Mashnoor,Mohammad Akyash,Kimia Azar,Hadi Kamali*

Main category: cs.PL

TL;DR: This paper presents SAGE-HLS, a specialized LLM for HLS code generation, enabled by a new dataset and fine-tuning strategy. The model achieves excellent synthesizability and promising functional correctness, addressing a major gap in EDA automation.


<details>
  <summary>Details</summary>
Motivation: Despite progress in Large Language Models (LLMs) for code generation, their impact on High-Level Synthesis (HLS) is limited due to the lack of publicly available HLS code datasets. This restricts innovation in automating hardware design via HLS.

Method: The paper introduces SAGE-HLS, a fine-tuned LLM for HLS code generation. It achieves this via: (i) porting synthesizable Verilog to C/C++ to build a 16.7K HLS code dataset, (ii) a specialized fine-tuning strategy using instruction prompting and Abstract Syntax Trees (ASTs), and (iii) a semi-automated evaluation framework (VerilogEval) to assess code functionality.

Result: SAGE-HLS, fine-tuned on the QwenCoder (2.5) 7B model, demonstrates nearly 100% code synthesizability and 75% functional correctness in experiments.

Conclusion: The proposed SAGE-HLS model significantly advances HLS code generation using LLMs, mainly via the creation of a large dataset and a robust fine-tuning and evaluation framework, leading to high synthesizability and functional correctness metrics.

Abstract: In today's rapidly evolving field of electronic design automation (EDA), the
complexity of hardware designs is increasing, necessitating more sophisticated
automation solutions. High-level synthesis (HLS), as a pivotal solution,
automates hardware designs from high-level abstractions (e.g., C/C++). However,
it faces significant challenges, particularly in design space exploration and
optimization. While large language models (LLMs) have shown notable
capabilities in code generation, their application to HLS has been limited due
to the scarcity of (publicly) available HLS code datasets. Hence, research in
this domain has primarily focused on techniques such as prompt engineering and
retrieval-augmented generation (RAG). To overcome this limitation, this paper
introduces SAGE-HLS, the first-of-its-kind fine-tuned LLM specifically for HLS
code generation. Our method includes three key advancements: (i) We implement
Verilog-to-C/C++ porting, converting verified and synthesizable Verilog codes
into corresponding C, creating a dataset of 16.7K HLS codes; (ii) We implement
a fine-tuning strategy, which is based on instruction prompting to code
generation guided by abstract syntax tree (AST); (iii) We develop a
semi-automated evaluation framework using VerilogEval to assess the
functionality of the generated HLS code. Our experiments show that SAGE-HLS,
fined-tuned on the QwenCoder (2.5) 7B model, achieves a near 100% success rate
in code synthesizability and a 75% success rate in functional correctness.

</details>


### [25] [Teaching Introductory Functional Programming Using Haskelite](https://arxiv.org/abs/2508.03640)
*Pedro Vasconcelos*

Main category: cs.PL

TL;DR: The paper discusses using a step-by-step tracing interpreter to teach Haskell in an introductory functional programming course, aiming to help students understand substitution-based computation.


<details>
  <summary>Details</summary>
Motivation: Students often struggle with substitution-based computation in functional programming, especially in the context of recursion, algebraic data types, and higher-order functions. Existing research suggests that step-by-step interpreters can aid understanding.

Method: A step-by-step tracing interpreter for a subset of Haskell was introduced in an introductory course. Feedback was collected from students, and the teaching experience was analyzed.

Result: The interpreter was effective in clarifying misconceptions and improving student understanding of core functional programming concepts. Student feedback was generally positive, and lessons learned will inform further development.

Conclusion: Using a step-by-step interpreter improved studentsâ€™ understanding, helped clarify misconceptions, and provided useful feedback for future improvement of teaching practices and tools.

Abstract: Learning functional programming requires learning a substitution-based
computational model. While substitution should be a familiar concept from
high-school algebra, students often have difficulty applying it to new
settings, such as recursive definitions, algebraic data types and higher-order
functions. Step-by-step interpreters have been shown to help beginners by
clarifying misconceptions and improving understanding.
  This paper reports on the experience of using a step-by-step tracing
interpreter for a subset of Haskell while teaching an introductory functional
programming course at the University of Porto. We describe the use of the
interpreter, present some feedback obtained from students, reflect on the
lessons learned and point directions for further work.

</details>
