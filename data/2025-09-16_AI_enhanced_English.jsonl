{"id": "2509.11418", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.11418", "abs": "https://arxiv.org/abs/2509.11418", "authors": ["Runming Li", "Yue Yao", "Robert Harper"], "title": "Mechanizing Synthetic Tait Computability in Istari", "comment": null, "summary": "Categorical gluing is a powerful technique for proving meta-theorems of type\ntheories such as canonicity and normalization. Synthetic Tait Computability\n(STC) provides an abstract treatment of the complex gluing models by\ninternalizing the gluing category into a modal dependent type theory with a\nphase distinction. This work presents a mechanization of STC in the Istari\nproof assistant. Istari is a Martin-L\\\"{o}f-style extensional type theory with\nequality reflection. Equality reflection eliminates the nuisance of transport\nreasoning typically found in intensional proof assistants. This work develops a\nreusable library for synthetic phase distinction, including modalities,\nextension types, and strict glue types, and applies it to two case studies: (1)\na canonicity model for dependent type theory with dependent products and\nbooleans with large elimination, and (2) a Kripke canonicity model for the\ncost-aware logical framework. Our results demonstrate that the core STC\nconstructions can be formalized essentially verbatim in Istari, preserving the\nelegance of the on-paper arguments while ensuring machine-checked correctness.", "AI": {"tldr": "The paper mechanizes synthetic Tait Computability (STC) in the Istari proof assistant, producing reusable libraries for phase distinction and demonstrating that STC constructions can be straightforwardly formalized in machine-checked proofs, thus improving type-theoretical reasoning and meta-theorem verification.", "motivation": "To formalize meta-theorems such as canonicity and normalization in type theory, categorical gluing is commonly used but traditionally requires complex manual reasoning. Synthetic Tait Computability (STC) abstracts this process, and the authors aim to mechanize STC in the Istari proof assistant to streamline such formalizations and eliminate tedious transport reasoning associated with intensional proof assistants.", "method": "The authors mechanize Synthetic Tait Computability (STC) within Istari\u2014a proof assistant supporting extensional type theory with equality reflection. They create a reusable library supporting synthetic phase distinction, including modalities, extension types, and strict glue types. The method is validated through two case studies: a canonicity model for dependent type theory (with products and booleans), and a Kripke canonicity model for a cost-aware logical framework.", "result": "The study shows that core STC constructions can be formalized in Istari essentially as written in paper arguments. This preserves theoretical elegance and ensures correctness by machine-checking, as demonstrated by the two case studies.", "conclusion": "Mechanizing STC in Istari proof assistant preserves the elegance of traditional arguments, streamlines type-theoretical reasoning, and facilitates precise and reusable formal meta-theory libraries, as evidenced by successful case studies in type theory and cost-aware logical frameworks."}}
{"id": "2509.11901", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2509.11901", "abs": "https://arxiv.org/abs/2509.11901", "authors": ["Kentaro Kobayashi", "Yukiyoshi Kameyama"], "title": "Expressive Power of One-Shot Control Operators and Coroutines", "comment": "Full version of the paper accepted at APLAS 2025. Includes appendices\n  with proofs. 59 pages", "summary": "Control operators, such as exceptions and effect handlers, provide a means of\nrepresenting computational effects in programs abstractly and modularly. While\nmost theoretical studies have focused on multi-shot control operators, one-shot\ncontrol operators -- which restrict the use of captured continuations to at\nmost once -- are gaining attention for their balance between expressiveness and\nefficiency. This study aims to fill the gap. We present a mathematically\nrigorous comparison of the expressive power among one-shot control operators,\nincluding effect handlers, delimited continuations, and even asymmetric\ncoroutines. Following previous studies on multi-shot control operators, we\nadopt Felleisen's macro-expressiveness as our measure of expressiveness. We\nverify the folklore that one-shot effect handlers and one-shot\ndelimited-control operators can be macro-expressed by asymmetric coroutines,\nbut not vice versa. We explain why a previous informal argument fails, and how\nto revise it to make a valid macro-translation.", "AI": {"tldr": "This paper systematically compares one-shot control operators using rigorous definitions. It shows that asymmetric coroutines are more expressive than one-shot effect handlers and delimited continuations, clarifying and correcting previous claims.", "motivation": "One-shot control operators are becoming important due to their balance of expressiveness and efficiency. However, their relative expressiveness compared to each other, and to other abstractions like asymmetric coroutines, is not well understood.", "method": "The authors use mathematically rigorous techniques and Felleisen's concept of macro-expressiveness to formally compare different one-shot control operators. They carefully revisit previous informal arguments and develop valid macro-translations between these abstractions.", "result": "They demonstrate that one-shot effect handlers and one-shot delimited control operators can be macro-expressed using asymmetric coroutines, but the converse is not possible. They also correct and clarify misunderstandings from prior informal arguments about these encodings.", "conclusion": "The paper provides a rigorous foundation for the expressiveness hierarchy among one-shot control abstractions, showing asymmetric coroutines are strictly more expressive under macro-expressiveness than one-shot effect handlers or delimited control operators."}}
{"id": "2509.10819", "categories": ["cs.SE", "cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.10819", "abs": "https://arxiv.org/abs/2509.10819", "authors": ["Christoph Hochrainer", "Valentin W\u00fcstholz", "Maria Christakis"], "title": "Arguzz: Testing zkVMs for Soundness and Completeness Bugs", "comment": null, "summary": "Zero-knowledge virtual machines (zkVMs) are increasingly deployed in\ndecentralized applications and blockchain rollups since they enable verifiable\noff-chain computation. These VMs execute general-purpose programs, frequently\nwritten in Rust, and produce succinct cryptographic proofs. However, zkVMs are\ncomplex, and bugs in their constraint systems or execution logic can cause\ncritical soundness (accepting invalid executions) or completeness (rejecting\nvalid ones) issues.\n  We present Arguzz, the first automated tool for testing zkVMs for soundness\nand completeness bugs. To detect such bugs, Arguzz combines a novel variant of\nmetamorphic testing with fault injection. In particular, it generates\nsemantically equivalent program pairs, merges them into a single Rust program\nwith a known output, and runs it inside a zkVM. By injecting faults into the\nVM, Arguzz mimics malicious or buggy provers to uncover overly weak\nconstraints.\n  We used Arguzz to test six real-world zkVMs (RISC Zero, Nexus, Jolt, SP1,\nOpenVM, and Pico) and found eleven bugs in three of them. One RISC Zero bug\nresulted in a $50,000 bounty, despite prior audits, demonstrating the critical\nneed for systematic testing of zkVMs.", "AI": {"tldr": "Arguzz is a new automated tool combining metamorphic testing and fault injection to systematically detect critical bugs in zero-knowledge virtual machines. It found 11 bugs\u2014including in previously audited systems\u2014highlighting the need for more rigorous zkVM testing.", "motivation": "Zero-knowledge virtual machines (zkVMs) enable secure and verifiable off-chain computation in decentralized applications and blockchains. However, their complexity introduces risks of soundness and completeness bugs, leading to potentially critical security vulnerabilities. Existing auditing practices may not fully uncover these issues.", "method": "The paper presents Arguzz, an automated tool that tests zkVMs for soundness and completeness bugs. Arguzz innovatively combines metamorphic testing\u2014by generating semantically equivalent program pairs\u2014and fault injection, merging them into a single Rust program with known output and running it inside zkVMs. Faults are injected to mimic the actions of malicious or buggy provers, aiming to identify weak constraints.", "result": "Arguzz was applied to six popular real-world zkVMs: RISC Zero, Nexus, Jolt, SP1, OpenVM, and Pico. It discovered eleven bugs in three zkVMs, including one in RISC Zero that resulted in a $50,000 bounty despite prior auditing, showing significant undetected vulnerabilities.", "conclusion": "Arguzz provides a much-needed, systematic and automated approach for uncovering hidden bugs\u2014particularly soundness and completeness issues\u2014in zkVMs, demonstrating the importance of advanced testing methodologies beyond traditional audits."}}
{"id": "2509.11065", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.11065", "abs": "https://arxiv.org/abs/2509.11065", "authors": ["Yuan Si", "Daming Li", "Hanyuan Shi", "Jialu Zhang"], "title": "ViScratch: Using Large Language Models and Gameplay Videos for Automated Feedback in Scratch", "comment": null, "summary": "Block-based programming environments such as Scratch are increasingly popular\nin programming education, in particular for young learners. While the use of\nblocks helps prevent syntax errors, semantic bugs remain common and difficult\nto debug. Existing tools for Scratch debugging rely heavily on predefined rules\nor user manual inputs, and crucially, they ignore the platform's inherently\nvisual nature.\n  We introduce ViScratch, the first multimodal feedback generation system for\nScratch that leverages both the project's block code and its generated gameplay\nvideo to diagnose and repair bugs. ViScratch uses a two-stage pipeline: a\nvision-language model first aligns visual symptoms with code structure to\nidentify a single critical issue, then proposes minimal, abstract syntax tree\nlevel repairs that are verified via execution in the Scratch virtual machine.\n  We evaluate ViScratch on a set of real-world Scratch projects against\nstate-of-the-art LLM-based tools and human testers. Results show that gameplay\nvideo is a crucial debugging signal: ViScratch substantially outperforms prior\ntools in both bug identification and repair quality, even without access to\nproject descriptions or goals. This work demonstrates that video can serve as a\nfirst-class specification in visual programming environments, opening new\ndirections for LLM-based debugging beyond symbolic code alone.", "AI": {"tldr": "ViScratch, a new debugging tool, uses both code and gameplay video to fix bugs in Scratch projects, outperforming existing tools and human testers, and highlighting the value of video in programming education debugging.", "motivation": "Debugging semantic bugs in block-based programming environments like Scratch is challenging and current debugging tools rely heavily on manual input or static rules, while ignoring the visual aspects intrinsic to Scratch.", "method": "ViScratch is introduced as a multimodal system that analyzes both the code and gameplay video of Scratch projects. It uses a vision-language model to align visual symptoms with code structure, identify the main bug, and then proposes minimal repairs verified by re-executing the repaired project.", "result": "ViScratch was tested on real Scratch projects and outperformed state-of-the-art LLM-based tools and human testers in both bug detection and repair quality, demonstrating that gameplay video is a critical debugging signal.", "conclusion": "Integrating video as a specification enhances bug diagnosis and repair in visual programming environments. ViScratch's success suggests new possibilities for multimodal, LLM-based debugging approaches."}}
{"id": "2509.10572", "categories": ["cs.SE", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.10572", "abs": "https://arxiv.org/abs/2509.10572", "authors": ["Ashlesha Akella", "Akshar Kaul", "Krishnasuri Narayanam", "Sameep Mehta"], "title": "Quality Assessment of Tabular Data using Large Language Models and Code Generation", "comment": "EMNLP industry track submitted", "summary": "Reliable data quality is crucial for downstream analysis of tabular datasets,\nyet rule-based validation often struggles with inefficiency, human\nintervention, and high computational costs. We present a three-stage framework\nthat combines statistical inliner detection with LLM-driven rule and code\ngeneration. After filtering data samples through traditional clustering, we\niteratively prompt LLMs to produce semantically valid quality rules and\nsynthesize their executable validators through code-generating LLMs. To\ngenerate reliable quality rules, we aid LLMs with retrieval-augmented\ngeneration (RAG) by leveraging external knowledge sources and domain-specific\nfew-shot examples. Robust guardrails ensure the accuracy and consistency of\nboth rules and code snippets. Extensive evaluations on benchmark datasets\nconfirm the effectiveness of our approach.", "AI": {"tldr": "The paper introduces a novel framework leveraging clustering, LLMs, and RAG to automate and improve data quality validation for tabular datasets, achieving strong results on benchmarks.", "motivation": "Traditional rule-based data validation methods for tabular datasets are inefficient, require significant human effort, and are computationally expensive, motivating the need for a more automated and scalable solution.", "method": "The paper proposes a three-stage framework: (1) statistical outlier detection via clustering, (2) iterative prompting of large language models (LLMs) to generate semantically valid data quality rules, and (3) automatic generation of code validators using code-generating LLMs. Retrieval-augmented generation (RAG) is used to enhance LLM performance with external knowledge and few-shot examples, while robust guardrails maintain accuracy.", "result": "Empirical evaluations on benchmark datasets demonstrate the framework's effectiveness, showing improved data quality validation with reliable rule and code generation.", "conclusion": "Combining statistical methods with LLM-based rule and code creation, complemented by retrieval-augmented generation and guardrails, offers a powerful and reliable solution for automated data quality validation in tabular datasets."}}
{"id": "2509.10649", "categories": ["cs.SE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.10649", "abs": "https://arxiv.org/abs/2509.10649", "authors": ["Johan Cederbladh", "Loek Cleophas", "Eduard Kamburjan", "Lucas Lima", "Rakshit Mittal", "Hans Vangheluwe"], "title": "Reasonable Experiments in Model-Based Systems Engineering", "comment": null, "summary": "With the current trend in Model-Based Systems Engineering towards Digital\nEngineering and early Validation & Verification, experiments are increasingly\nused to estimate system parameters and explore design decisions. Managing such\nexperimental configuration metadata and results is of utmost importance in\naccelerating overall design effort. In particular, we observe it is important\nto 'intelligent-ly' reuse experiment-related data to save time and effort by\nnot performing potentially superfluous, time-consuming, and resource-intensive\nexperiments. In this work, we present a framework for managing experiments on\ndigital and/or physical assets with a focus on case-based reasoning with domain\nknowledge to reuse experimental data efficiently by deciding whether an\nalready-performed experiment (or associated answer) can be reused to answer a\nnew (potentially different) question from the engineer/user without having to\nset up and perform a new experiment. We provide the general architecture for\nsuch an experiment manager and validate our approach using an industrial\nvehicular energy system-design case study.", "AI": {"tldr": "This paper introduces a case-based reasoning framework for managing and reusing experimental data in digital and physical systems engineering. By intelligently deciding when existing data applies to new questions, the framework streamlines design and reduces the need for repeated experiments, as shown in an industrial vehicular system case study.", "motivation": "There is a growing need to efficiently manage and reuse experimental data in the context of digital and physical systems engineering to save time, resources, and avoid redundant experiments.", "method": "The authors propose a framework based on case-based reasoning that leverages domain knowledge to decide whether existing experimental results can answer new engineering queries, thereby avoiding the need for new experiments. They present the architecture of their experiment manager and validate it using an industrial vehicular energy system case study.", "result": "The framework enables efficient reuse of experimental data, preventing unnecessary and resource-consuming experiments by determining the applicability of existing results to new questions.", "conclusion": "Intelligent management and reuse of experiment metadata and results accelerate the overall design process in Model-Based Systems Engineering, validated through a real-world industrial case study."}}
{"id": "2509.10920", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.10920", "abs": "https://arxiv.org/abs/2509.10920", "authors": ["Guan-Yan Yang", "Farn Wang", "You-Zong Gu", "Ya-Wen Teng", "Kuo-Hui Yeh", "Ping-Hsueh Ho", "Wei-Ling Wen"], "title": "TPSQLi: Test Prioritization for SQL Injection Vulnerability Detection in Web Applications", "comment": "20 pages; 8 figures", "summary": "The rapid proliferation of network applications has led to a significant\nincrease in network attacks. According to the OWASP Top 10 Projects report\nreleased in 2021, injection attacks rank among the top three vulnerabilities in\nsoftware projects. This growing threat landscape has increased the complexity\nand workload of software testing, necessitating advanced tools to support agile\ndevelopment cycles. This paper introduces a novel test prioritization method\nfor SQL injection vulnerabilities to enhance testing efficiency. By leveraging\nprevious test outcomes, our method adjusts defense strength vectors for\nsubsequent tests, optimizing the testing workflow and tailoring defense\nmechanisms to specific software needs. This approach aims to improve the\neffectiveness and efficiency of vulnerability detection and mitigation through\na flexible framework that incorporates dynamic adjustments and considers the\ntemporal aspects of vulnerability exposure.", "AI": {"tldr": "The paper presents a new test prioritization method for SQL injection vulnerabilities. By adjusting defense strategies based on test results, the approach boosts efficiency and effectiveness in software security testing.", "motivation": "The rise in network attacks, especially injection attacks (highlighted as a top vulnerability in the OWASP Top 10 2021), has complicated software testing and increased the need for advanced testing tools.", "method": "Proposes a novel test prioritization method for detecting SQL injection vulnerabilities. It adapts defense strength vectors based on previous test outcomes, creating a dynamic and flexible framework.", "result": "The method optimizes testing workflows and customizes defense mechanisms according to specific software requirements, enhancing both the efficiency and effectiveness of vulnerability detection and mitigation.", "conclusion": "The proposed approach successfully improves the efficiency and effectiveness of SQL injection vulnerability testing, with dynamic adjustments and consideration of temporal factors."}}
{"id": "2509.10946", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10946", "abs": "https://arxiv.org/abs/2509.10946", "authors": ["Roberto Morabito", "Guanghan Wu"], "title": "When the Code Autopilot Breaks: Why LLMs Falter in Embedded Machine Learning", "comment": "This paper has been accepted for publication in Computer (IEEE). Upon\n  publication, the copyright will be transferred to IEEE", "summary": "Large Language Models (LLMs) are increasingly used to automate software\ngeneration in embedded machine learning workflows, yet their outputs often fail\nsilently or behave unpredictably. This article presents an empirical\ninvestigation of failure modes in LLM-powered ML pipelines, based on an\nautopilot framework that orchestrates data preprocessing, model conversion, and\non-device inference code generation. We show how prompt format, model behavior,\nand structural assumptions influence both success rates and failure\ncharacteristics, often in ways that standard validation pipelines fail to\ndetect. Our analysis reveals a diverse set of error-prone behaviors, including\nformat-induced misinterpretations and runtime-disruptive code that compiles but\nbreaks downstream. We derive a taxonomy of failure categories and analyze\nerrors across multiple LLMs, highlighting common root causes and systemic\nfragilities. Though grounded in specific devices, our study reveals broader\nchallenges in LLM-based code generation. We conclude by discussing directions\nfor improving reliability and traceability in LLM-powered embedded ML systems.", "AI": {"tldr": "The paper investigates why LLMs often fail or behave unpredictably in automating embedded ML workflows, finds multiple subtle failure modes, and suggests the need for better reliability and traceability in such systems.", "motivation": "Large Language Models (LLMs) are widely used to automate software generation in embedded machine learning workflows, but their outputs often result in silent failures or unpredictable behavior, presenting reliability challenges.", "method": "The study conducts an empirical investigation using an autopilot framework that manages data preprocessing, model conversion, and code generation for on-device inference. It analyzes how factors like prompt format, model behavior, and structural assumptions affect the success and failure of LLM outputs. Errors are taxonomized and analyzed across multiple LLMs to identify root causes and systemic fragilities.", "result": "The analysis uncovers a variety of error-prone behaviors (such as format-induced misinterpretations and code that compiles but fails at runtime), outlines failure categories, and discusses common root causes and systemic weaknesses in current LLM-powered approaches.", "conclusion": "There are significant challenges and systemic weaknesses in current LLM-driven code generation for embedded ML due to subtle and undetected failure modes. Improving reliability and traceability requires new approaches and awareness of these failure characteristics."}}
{"id": "2509.11000", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.11000", "abs": "https://arxiv.org/abs/2509.11000", "authors": ["Omid Gheibi", "Christian K\u00e4stner", "Pooyan Jamshidi"], "title": "Hardness, Structural Knowledge, and Opportunity: An Analytical Framework for Modular Performance Modeling", "comment": null, "summary": "Performance-influence models are beneficial for understanding how\nconfigurations affect system performance, but their creation is challenging due\nto the exponential growth of configuration spaces. While gray-box approaches\nleverage selective \"structural knowledge\" (like the module execution graph of\nthe system) to improve modeling, the relationship between this knowledge, a\nsystem's characteristics (we call them \"structural aspects\"), and potential\nmodel improvements is not well understood. This paper addresses this gap by\nformally investigating how variations in structural aspects (e.g., the number\nof modules and options per module) and the level of structural knowledge impact\nthe creation of \"opportunities\" for improved \"modular performance modeling\". We\nintroduce and quantify the concept of modeling \"hardness\", defined as the\ninherent difficulty of performance modeling. Through controlled experiments\nwith synthetic system models, we establish an \"analytical matrix\" to measure\nthese concepts. Our findings show that modeling hardness is primarily driven by\nthe number of modules and configuration options per module. More importantly,\nwe demonstrate that both higher levels of structural knowledge and increased\nmodeling hardness significantly enhance the opportunity for improvement. The\nimpact of these factors varies by performance metric; for ranking accuracy\n(e.g., in debugging task), structural knowledge is more dominant, while for\nprediction accuracy (e.g., in resource management task), hardness plays a\nstronger role. These results provide actionable insights for system designers,\nguiding them to strategically allocate time and select appropriate modeling\napproaches based on a system's characteristics and a given task's objectives.", "AI": {"tldr": "Building effective performance-influence models is tough, but using more structural system knowledge and understanding the inherent challenges (modeling hardness) can yield substantial improvement. The value of these factors depends on the modeling task\u2014knowing the system's structure is more helpful for ranking tasks, while acknowledging modeling difficulty matters more for prediction tasks. The findings guide designers in making informed modeling choices based on system characteristics and goals.", "motivation": "Performance-influence models help understand how system configurations impact performance, but building such models is hard due to exponentially large configuration spaces. While some approaches use structural knowledge of the system to help with modeling, the precise relationship between this knowledge, specific system characteristics, and model improvement is not well understood.", "method": "The paper conducts a formal investigation using controlled experiments with synthetic system models. It quantifies structural aspects (like number of modules and options per module), introduces 'modeling hardness' as a concept, and constructs an analytical matrix to measure the effects of structural knowledge and system characteristics on modeling opportunities.", "result": "The experiments show that modeling hardness is mainly influenced by the number of modules and configuration options per module. Greater structural knowledge and increased modeling hardness both boost opportunities for improved modular performance modeling. Their relative importance differs: structural knowledge is more crucial for ranking accuracy tasks, while modeling hardness is more important for prediction accuracy tasks.", "conclusion": "The study provides practical guidance for system designers. Depending on the system\u2019s structural aspects and the objective of the modeling task (ranking vs. prediction), designers can strategically choose where to spend their efforts and which modeling strategies to use."}}
{"id": "2509.11132", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11132", "abs": "https://arxiv.org/abs/2509.11132", "authors": ["Xiaoyu Zhang", "Weipeng Jiang", "Juan Zhai", "Shiqing Ma", "Qingshuang Bao", "Chenhao Lin", "Chao Shen", "Tianlin Li", "Yang Liu"], "title": "Rethinking Technology Stack Selection with AI Coding Proficiency", "comment": "23 pages", "summary": "Large language models (LLMs) are now an integral part of software development\nworkflows and are reshaping the whole process. Traditional technology stack\nselection has not caught up. Most of the existing selection methods focus\nsolely on the inherent attributes of the technology, overlooking whether the\nLLM can effectively leverage the chosen technology. For example, when\ngenerating code snippets using popular libraries like Selenium (one of the most\nwidely used test automation tools with over 33k GitHub stars), existing LLMs\nfrequently generate low-quality code snippets (e.g., using deprecated APIs and\nmethods, or containing syntax errors). As such, teams using LLM assistants risk\nchoosing technologies that cannot be used effectively by LLMs, yielding high\ndebugging effort and mounting technical debt. We foresee a practical question\nin the LLM era, is a technology ready for AI-assisted development? In this\npaper, we first propose the concept, AI coding proficiency, the degree to which\nLLMs can utilize a given technology to generate high-quality code snippets. We\nconduct the first comprehensive empirical study examining AI proficiency across\n170 third-party libraries and 61 task scenarios, evaluating six widely used\nLLMs. Our findings reveal that libraries with similar functionalities can\nexhibit up to 84% differences in the quality score of LLM-generated code, while\ndifferent models also exhibit quality gaps among their generation results using\nthe same library. These gaps translate into real engineering costs and can\nsteer developer choices toward a narrow set of libraries with high AI coding\nproficiency, threatening technological diversity in the ecosystem. We call on\nthe community to integrate AI proficiency assessments into technology selection\nframeworks and develop mitigation strategies, preserving competitive balance in\nAI-driven development.", "AI": {"tldr": "Traditional ways of choosing tech for software development don't consider how well LLMs work with those libraries, leading to bad code and extra work. The paper shows big gaps in LLM performance depending on the library, and calls for new selection methods that include 'AI coding proficiency' to avoid narrowing tech choices and losing ecosystem diversity.", "motivation": "The motivation arises from the increasing role of LLMs in software development. Traditional technology stack selection does not account for how well LLMs can use a given technology, leading to risks of low-quality code and increased technical debt.", "method": "They introduce the concept of 'AI coding proficiency' and conduct an empirical study across 170 third-party libraries and 61 scenarios, evaluating six popular LLMs for their code generation capabilities.", "result": "Libraries with similar functions may show up to an 84% difference in LLM-generated code quality. Different LLMs perform unevenly with the same technology, influencing developer choices and potentially threatening technology diversity.", "conclusion": "AI coding proficiency should be integrated into technology selection frameworks. The community should develop strategies to preserve diversity and maintain competitive balance in an AI-driven development landscape."}}
{"id": "2509.11238", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11238", "abs": "https://arxiv.org/abs/2509.11238", "authors": ["Dongming Jin", "Zhi Jin", "Yiran Zhang", "Zheng Fang", "Linyu Li", "Yuanpeng He", "Xiaohong Chen", "Weisong Sun"], "title": "UserTrace: User-Level Requirements Generation and Traceability Recovery from Software Project Repositories", "comment": "21page, 5 figures", "summary": "Software maintainability critically depends on high-quality requirements\ndescriptions and explicit traceability between requirements and code. Although\nautomated code summarization (ACS) and requirements traceability (RT)\ntechniques have been widely studied, existing ACS methods mainly generate\nimplementation-level (i.e., developer-oriented) requirements (IRs) for\nfine-grained units (e.g., methods), while RT techniques often overlook the\nimpact of project evolution. As a result, user-level (i.e., end user-oriented)\nrequirements (URs) and live trace links remain underexplored, despite their\nimportance for supporting user understanding and for validating whether\nAI-generated software aligns with user intent. To address this gap, we propose\nUserTrace, a multi-agent system that automatically generates URs and recovers\nlive trace links (from URs to IRs to code) from software repositories.\nUserTrace coordinates four specialized agents (i.e., Code Reviewer, Searcher,\nWriter, and Verifier) through a three-phase process: structuring repository\ndependencies, deriving IRs for code units, and synthesizing URs with\ndomain-specific context. Our comparative evaluation shows that UserTrace\nproduces URs with higher completeness, correctness, and helpfulness than an\nestablished baseline, and achieves superior precision in trace link recovery\ncompared to five state-of-the-art RT approaches. A user study further\ndemonstrates that UserTrace helps end users validate whether the AI-generated\nrepositories align with their intent.", "AI": {"tldr": "UserTrace is a multi-agent system that automatically generates user-level requirements and maintains live traceability from requirements to code. It outperforms existing tools in completeness and precision and helps users verify that AI-generated software matches their intended needs.", "motivation": "Current automated code summarization and requirements traceability methods focus mostly on developer-oriented requirements and overlook user-level requirements and traceability as projects evolve. This hampers software maintainability and validation regarding user intent.", "method": "The authors propose UserTrace, a multi-agent system consisting of four specialized agents (Code Reviewer, Searcher, Writer, Verifier) that operate in three phases: structuring repository dependencies, deriving implementation-level requirements, and synthesizing user-level requirements with domain-specific context.", "result": "Comparative evaluation reveals that UserTrace generates user-level requirements with better completeness, correctness, and helpfulness than existing baseline methods, and outperforms five state-of-the-art traceability techniques in precision for trace link recovery. A user study further validates its effectiveness in helping end users ensure alignment with their intent.", "conclusion": "UserTrace addresses the gap in user-level requirements and live traceability, supporting better validation of AI-generated software against user intent and improving overall software maintainability."}}
{"id": "2509.11252", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11252", "abs": "https://arxiv.org/abs/2509.11252", "authors": ["Chengze li", "Yitong Zhang", "Jia Li", "Liyi Cai", "Ge Li"], "title": "Beyond Autoregression: An Empirical Study of Diffusion Large Language Models for Code Generation", "comment": null, "summary": "LLMs have become the mainstream approaches to code generation. Existing LLMs\nmainly employ autoregressive generation, i.e. generating code token-by-token\nfrom left to right. However, the underlying autoregressive generation has two\nlimitations in code generation. First, autoregressive LLMs only generate a\ntoken at each step, showing low efficiency in practice. Second, programming is\na non-sequential process involving back-and-forth editing, while autoregressive\nLLMs only employ the left-to-right generation order. These two intrinsic\nlimitations hinder the further development of LLMs in code generation.\nRecently, diffusion LLMs have emerged as a promising alternative. Diffusion\nLLMs address the above limitations with two advances, including multi-token\nprediction (i.e. generating multiple tokens at each step) and flexible\ngeneration order (i.e. flexibly determining which positions to generate\ntokens). However, there is no systematic study exploring diffusion LLMs in code\ngeneration. To bridge the knowledge gap, we present the first empirical study\nof diffusion LLMs for code generation. Our study involves 9 representative\ndiffusion LLMs and conduct experiments on 4 widely used benchmarks. Based on\nthe results, we summarize the following findings. (1) Existing diffusion LLMs\nare competitive with autoregressive LLMs with similar sizes. (2) Diffusion LLMs\nhave a stronger length extrapolation ability than autoregressive LLMs and\nperform better in long code understanding. (3) We explore factors impacting the\neffectiveness and efficiency of diffusion LLMs, and provide practical guidance.\n(4) We discuss several promising further directions to improve diffusion LLMs\non code generation. We open-source all source code, data, and results to\nfacilitate the following research. The code is publicly available at\nhttps://github.com/zhangyitonggg/dllm4code.", "AI": {"tldr": "This paper provides the first comprehensive study of diffusion LLMs for code generation, finding them competitive and sometimes superior to traditional autoregressive LLMs, particularly for long code. Results and open-source resources can inform practical use and future research.", "motivation": "Autoregressive LLMs are currently dominant for code generation but suffer from inefficiency (one token per step) and do not align well with the non-sequential nature of real-world programming. These limitations hinder advancements in code generation.", "method": "The authors conduct the first empirical study of diffusion LLMs for code generation by evaluating 9 representative diffusion LLMs over 4 widely used benchmarks. They compare these models to autoregressive LLMs and analyze factors that impact their effectiveness and efficiency.", "result": "The study finds that diffusion LLMs are competitive with similarly sized autoregressive LLMs, show stronger length extrapolation abilities, perform better on long code tasks, and identifies factors affecting their effectiveness and efficiency. The authors also propose future research directions and provide practical guidance.", "conclusion": "Diffusion LLMs address key limitations of autoregressive LLMs in code generation by generating multiple tokens per step and allowing for flexible order, offering competitive and sometimes superior performance, especially in handling longer code. The findings provide insights and practical guidance for further development."}}
{"id": "2509.11258", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11258", "abs": "https://arxiv.org/abs/2509.11258", "authors": ["Regan Meloche", "Durga Sivakumar", "Amal A. Anda", "Sofana Alfuhaid", "Daniel Amyot", "Luigi Logrippo", "John Mylopoulos"], "title": "A Web-Based Environment for the Specification and Generation of Smart Legal Contracts", "comment": "12 pages, 5 figures, 2 tables, conference", "summary": "Monitoring the compliance of contract performance against legal obligations\nis important in order to detect violations, ideally, as soon as they occur.\nSuch monitoring can nowadays be achieved through the use of smart contracts,\nwhich provide protection against tampering as well as some level of automation\nin handling violations. However, there exists a large gap between natural\nlanguage contracts and smart contract implementations. This paper introduces a\nWeb-based environment that partly fills that gap by supporting the\nuser-assisted refinement of Symboleo specifications corresponding to legal\ncontract templates, followed by the automated generation of monitoring smart\ncontracts deployable on the Hyperledger Fabric platform. This environment,\nillustrated using a sample contract from the transactive energy domain, shows\nmuch potential in accelerating the development of smart contracts in a legal\ncompliance context.", "AI": {"tldr": "The paper introduces a web-based tool to bridge the gap between legal contracts and smart contracts, allowing user-assisted refinement of contract specifications and automatic smart contract generation for Hyperledger Fabric. This accelerates and simplifies the creation of compliance-monitoring smart contracts.", "motivation": "There is a significant gap between natural language contracts (used in legal settings) and the implementation of those contracts as smart contracts, which can automate compliance monitoring and violation detection.", "method": "The paper presents a web-based environment that assists users in refining Symboleo specifications derived from legal contract templates. It then automatically generates monitoring smart contracts that can be deployed on Hyperledger Fabric.", "result": "The proposed environment, demonstrated through a sample contract in the transactive energy domain, shows promise in accelerating smart contract development for legal compliance purposes.", "conclusion": "This environment enables more efficient and reliable translation of legal contract templates into deployable smart contracts, reducing the manual effort and potential for error in creating automated compliance monitoring solutions."}}
{"id": "2509.11312", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11312", "abs": "https://arxiv.org/abs/2509.11312", "authors": ["Wenchao Gu", "Yupan Chen", "Yanlin Wang", "Hongyu Zhang", "Cuiyun Gao", "Michael R. Lyu"], "title": "Weakly Supervised Vulnerability Localization via Multiple Instance Learning", "comment": null, "summary": "Software vulnerability detection has emerged as a significant concern in the\nfield of software security recently, capturing the attention of numerous\nresearchers and developers. Most previous approaches focus on coarse-grained\nvulnerability detection, such as at the function or file level. However, the\ndevelopers would still encounter the challenge of manually inspecting a large\nvolume of code inside the vulnerable function to identify the specific\nvulnerable statements for modification, indicating the importance of\nvulnerability localization. Training the model for vulnerability localization\nusually requires ground-truth labels at the statement-level, and labeling\nvulnerable statements demands expert knowledge, which incurs high costs. Hence,\nthe demand for an approach that eliminates the need for additional labeling at\nthe statement-level is on the rise. To tackle this problem, we propose a novel\napproach called WAVES for WeAkly supervised Vulnerability Localization via\nmultiplE inStance learning, which does not need the additional statement-level\nlabels during the training. WAVES has the capability to determine whether a\nfunction is vulnerable (i.e., vulnerability detection) and pinpoint the\nvulnerable statements (i.e., vulnerability localization). Specifically,\ninspired by the concept of multiple instance learning, WAVES converts the\nground-truth label at the function-level into pseudo labels for individual\nstatements, eliminating the need for additional statement-level labeling. These\npseudo labels are utilized to train the classifiers for the function-level\nrepresentation vectors. Extensive experimentation on three popular benchmark\ndatasets demonstrates that, in comparison to previous baselines, our approach\nachieves comparable performance in vulnerability detection and state-of-the-art\nperformance in statement-level vulnerability localization.", "AI": {"tldr": "WAVES is a weakly supervised method for detecting and localizing software vulnerabilities at the statement level without expensive manual labels, matching existing detection accuracy and outperforming prior methods on localization tasks.", "motivation": "Current software vulnerability detection methods mostly work at coarse granularity (function or file level), requiring manual inspection by developers to locate precise vulnerable statements within code, which is costly and demands expert labeling at statement-level.", "method": "The paper introduces WAVES, a weakly supervised vulnerability localization technique using multiple instance learning. WAVES trains on function-level labels by generating pseudo-labels for statements, removing the need for costly statement-level expert labeling.", "result": "WAVES performs comparably to existing methods in function-level vulnerability detection and achieves state-of-the-art results in localizing vulnerable code statements, validated on three benchmark datasets.", "conclusion": "WAVES significantly reduces labeling costs for vulnerability localization without sacrificing detection accuracy and advances the field to finer-grained, automated vulnerability identification."}}
{"id": "2509.11446", "categories": ["cs.SE", "D.2.0; D.2.1; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.11446", "abs": "https://arxiv.org/abs/2509.11446", "authors": ["Mohammad Amin Zadenoori", "Jacek D\u0105browski", "Waad Alhoshan", "Liping Zhao", "Alessio Ferrari"], "title": "Large Language Models (LLMs) for Requirements Engineering (RE): A Systematic Literature Review", "comment": null, "summary": "Large Language Models (LLMs) are finding applications in numerous domains,\nand Requirements Engineering (RE) is increasingly benefiting from their\ncapabilities to assist with complex, language-intensive tasks. This paper\npresents a systematic literature review of 74 primary studies published between\n2023 and 2024, examining how LLMs are being applied in RE. The study\ncategorizes the literature according to several dimensions, including\npublication trends, RE activities, prompting strategies, and evaluation\nmethods. Our findings indicate notable patterns, among which we observe\nsubstantial differences compared to previous works leveraging standard Natural\nLanguage Processing (NLP) techniques. Most of the studies focus on using LLMs\nfor requirements elicitation and validation, rather than defect detection and\nclassification, which were dominant in the past. Researchers have also\nbroadened their focus and addressed novel tasks, e.g., test generation,\nexploring the integration of RE with other software engineering (SE)\ndisciplines. Although requirements specifications remain the primary focus,\nother artifacts are increasingly considered, including issues from issue\ntracking systems, regulations, and technical manuals. The studies mostly rely\non GPT-based models, and often use Zero-shot or Few-shot prompting. They are\nusually evaluated in controlled environments, with limited use in industry\nsettings and limited integration in complex workflows. Our study outlines\nimportant future directions, such as leveraging the potential to expand the\ninfluence of RE in SE, exploring less-studied tasks, improving prompting\nmethods, and testing in real-world environments. Our contribution also helps\nresearchers and practitioners use LLMs more effectively in RE, by providing a\nlist of identified tools leveraging LLMs for RE, as well as datasets.", "AI": {"tldr": "This paper systematically reviews 74 recent studies on LLMs in Requirements Engineering, revealing shifts in focus and method, highlighting new tasks and integration opportunities, but also underscoring the lack of real-world deployment. It provides practical resources and suggests future research directions.", "motivation": "As LLMs revolutionize various fields, there is a growing need to understand their applications, benefits, and research directions in Requirements Engineering, which involves complex language tasks.", "method": "A systematic literature review of 74 primary studies (2023\u20132024), analyzing trends, activities, prompting strategies, and evaluation methods related to the application of LLMs in Requirements Engineering.", "result": "LLMs are predominantly used for requirements elicitation and validation, with a shift from traditional tasks like defect detection. Research focuses on test generation and integrating RE with other SE disciplines. Most studies use GPT models and zero-/few-shot prompting, with limited real-world or industry application. The paper provides lists of tools and datasets for LLM-based RE.", "conclusion": "The paper concludes that LLMs are making a significant impact on Requirements Engineering, with substantial differences from prior NLP-based approaches. However, most uses are experimental, limited to controlled settings, and call for further research, especially for real-world adoption and broader SE integration."}}
{"id": "2509.11523", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11523", "abs": "https://arxiv.org/abs/2509.11523", "authors": ["Ziliang Wang", "Ge Li", "Jia Li", "Hao Zhu", "Zhi Jin"], "title": "VulAgent: Hypothesis-Validation based Multi-Agent Vulnerability Detection", "comment": null, "summary": "The application of language models to project-level vulnerability detection\nremains challenging, owing to the dual requirement of accurately localizing\nsecurity-sensitive code and correctly correlating and reasoning over complex\nprogram context. We present VulAgent, a multi-agent vulnerability detection\nframework based on hypothesis validation. Our design is inspired by how human\nauditors review code: when noticing a sensitive operation, they form a\nhypothesis about a possible vulnerability, consider potential trigger paths,\nand then verify the hypothesis against the surrounding context. VulAgent\nimplements a semantics-sensitive, multi-view detection pipeline: specialized\nagents, each aligned to a specific analysis perspective (e.g., memory,\nauthorization), collaboratively surface and precisely localize sensitive code\nsites with higher coverage. Building on this, VulAgent adopts a\nhypothesis-validation paradigm: for each vulnerability report, it builds\nhypothesis conditions and a trigger path, steering the LLM to target the\nrelevant program context and defensive checks during verification, which\nreduces false positives. On average across the two datasets, VulAgent improves\noverall accuracy by 6.6%, increases the correct identification rate of\nvulnerable--fixed code pairs by up to 450% (246% on average), and reduces the\nfalse positive rate by about 36% compared with state-of-the-art LLM-based\nbaselines.", "AI": {"tldr": "VulAgent, a multi-agent, hypothesis-based framework, significantly improves project-level vulnerability detection by guiding LLMs to validate suspected vulnerabilities more thoroughly. It outperforms previous methods with higher accuracy, better identification of fixed vulnerabilities, and reduced false positives.", "motivation": "Language models face challenges in project-level vulnerability detection because they must both localize security-sensitive code accurately and analyze complex code contexts. Existing methods struggle to correlate these elements effectively, often leading to high false positive rates and missed vulnerabilities.", "method": "The authors propose VulAgent, a multi-agent framework inspired by human code auditing. It consists of specialized agents focused on different security perspectives (e.g., memory, authorization). VulAgent uses a hypothesis-validation process wherein it formulates vulnerability hypotheses, identifies trigger paths, and directs the language model to verify these against relevant context and defensive checks. This targeted validation reduces errors and improves precision.", "result": "VulAgent demonstrates improved performance on two datasets: it increases overall accuracy by 6.6%, boosts correct recognition of vulnerable-fixed code pairs by up to 450% (246% on average), and lowers the false positive rate by about 36% compared to state-of-the-art LLM-based methods.", "conclusion": "VulAgent\u2019s multi-agent, hypothesis-driven approach provides more precise and context-sensitive project-level vulnerability detection with substantially better accuracy and fewer false positives than previous LLM-based systems."}}
{"id": "2509.11566", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11566", "abs": "https://arxiv.org/abs/2509.11566", "authors": ["Hua Guo", "Yunhong Ji", "Xuan Zhou"], "title": "Sedeve-Kit, a Specification-Driven Development Framework for Building Distributed Systems", "comment": null, "summary": "Developing distributed systems presents significant challenges, primarily due\nto the complexity introduced by non-deterministic concurrency and faults. To\naddress these, we propose a specification-driven development framework. Our\nmethod encompasses three key stages. The first stage defines system\nspecifications and invariants using TLA${^+}$. It allows us to perform model\nchecking on the algorithm's correctness and generate test cases for subsequent\ndevelopment phases. In the second stage, based on the established\nspecifications, we write code to ensure consistency and accuracy in the\nimplementation. Finally, after completing the coding process, we rigorously\ntest the system using the test cases generated in the initial stage. This\nprocess ensures system quality by maintaining a strong connection between the\nabstract design and the concrete implementation through continuous\nverification.", "AI": {"tldr": "This paper introduces a three-stage specification-driven framework for developing distributed systems: formal specification and model checking with TLA+, code generation that's tightly guided by specifications, and comprehensive testing using automatically generated test cases, together ensuring high system quality and reliable implementation.", "motivation": "Distributed systems are difficult to develop due to non-deterministic concurrency and faults, necessitating better approaches to manage complexity and ensure system correctness.", "method": "The authors propose a specification-driven development framework with three stages: (1) defining specifications and invariants in TLA+, model checking the algorithm and generating test cases; (2) implementing the system according to these specifications; (3) testing the system using the generated test cases to verify correctness.", "result": "The proposed methodology strengthens the link between design and implementation, enabling continuous verification and higher assurance of system quality for distributed systems.", "conclusion": "The specification-driven framework ensures the correctness and reliability of distributed systems by combining formal specification, model checking, and rigorous testing."}}
{"id": "2509.11626", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11626", "abs": "https://arxiv.org/abs/2509.11626", "authors": ["Prerna Agarwal", "Himanshu Gupta", "Soujanya Soni", "Rohith Vallam", "Renuka Sindhgatta", "Sameep Mehta"], "title": "Automated Creation and Enrichment Framework for Improved Invocation of Enterprise APIs as Tools", "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) has lead to the\ndevelopment of agents capable of complex reasoning and interaction with\nexternal tools. In enterprise contexts, the effective use of such tools that\nare often enabled by application programming interfaces (APIs), is hindered by\npoor documentation, complex input or output schema, and large number of\noperations. These challenges make tool selection difficult and reduce the\naccuracy of payload formation by up to 25%. We propose ACE, an automated tool\ncreation and enrichment framework that transforms enterprise APIs into\nLLM-compatible tools. ACE, (i) generates enriched tool specifications with\nparameter descriptions and examples to improve selection and invocation\naccuracy, and (ii) incorporates a dynamic shortlisting mechanism that filters\nrelevant tools at runtime, reducing prompt complexity while maintaining\nscalability. We validate our framework on both proprietary and open-source APIs\nand demonstrate its integration with agentic frameworks. To the best of our\nknowledge, ACE is the first end-to-end framework that automates the creation,\nenrichment, and dynamic selection of enterprise API tools for LLM agents.", "AI": {"tldr": "The paper introduces ACE, a framework that makes enterprise APIs easier for LLM agents to use by automating enrichment of tool specs and dynamic shortlisting, addressing key pain points in accuracy and scalability.", "motivation": "Large Language Models (LLMs) are increasingly powerful in enterprise settings but struggle to use external APIs effectively due to poor documentation, complex schemas, and overwhelming numbers of operations. This hinders tool selection and reduces accuracy.", "method": "The paper proposes ACE, an automated framework that transforms complex enterprise APIs into LLM-friendly tools. ACE enriches API specifications with parameter descriptions and examples and introduces a dynamic tool shortlisting mechanism to filter relevant tools during runtime.", "result": "ACE was validated on both proprietary and open-source APIs and successfully integrated with agentic frameworks. The framework improves tool selection and invocation accuracy and reduces prompt complexity while maintaining scalability.", "conclusion": "ACE is an end-to-end automated solution for creating, enriching, and dynamically selecting enterprise API tools, enabling LLM agents to interact more effectively and efficiently with complex APIs."}}
{"id": "2509.11686", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11686", "abs": "https://arxiv.org/abs/2509.11686", "authors": ["Jian Wang", "Xiaofei Xie", "Qiang Hu", "Shangqing Liu", "Yi Li"], "title": "Do Code Semantics Help? A Comprehensive Study on Execution Trace-Based Information for Code Large Language Models", "comment": "EMNLP2025-findings", "summary": "Code Large Language Models (Code LLMs) have opened a new era in programming\nwith their impressive capabilities. However, recent research has revealed\ncritical limitations in their ability to reason about runtime behavior and\nunderstand the actual functionality of programs, which poses significant\nchallenges for their post-training and practical deployment. Specifically, Code\nLLMs encounter two principal issues: (1) a lack of proficiency in reasoning\nabout program execution behavior, as they struggle to interpret what programs\nactually do during runtime, and (2) the inconsistent and fragmented\nrepresentation of semantic information, such as execution traces, across\nexisting methods, which hinders their ability to generalize and reason\neffectively. These challenges underscore the necessity for more systematic\napproaches to enhance the reasoning capabilities of Code LLMs. To address these\nissues, we introduce a generic framework to support integrating semantic\ninformation~(e.g., execution trace) to code task-relevant prompts, and conduct\na comprehensive study to explore the role of semantic information in enhancing\nthe reasoning ability of Code LLMs accordingly. Specifically, we focus on\ninvestigating the usefulness of trace-based semantic information in boosting\nsupervised fine-tuning~(SFT) and post-phase inference of Code LLMs. The\nexperimental results surprisingly disagree with previous works and demonstrate\nthat semantic information has limited usefulness for SFT and test time scaling\nof Code LLM.", "AI": {"tldr": "This paper finds that including semantic information like execution traces into prompts does not meaningfully improve Code LLMs' reasoning during training or inference, challenging prior assumptions.", "motivation": "Code LLMs have shown strong programming capabilities but struggle with understanding program runtime behavior and properly using semantic information like execution traces. There is a need to improve their reasoning abilities for more robust post-training and deployment.", "method": "The authors introduce a general framework for integrating semantic information, such as execution traces, into code task prompts. They systematically study the effect of this semantic information on the reasoning capabilities of Code LLMs, focusing on supervised fine-tuning (SFT) and inference phases.", "result": "Experimental results show that integrating semantic information is less beneficial than previously thought. Semantic information provides limited usefulness for supervised fine-tuning and test-time scaling of Code LLMs.", "conclusion": "Contrary to some previous studies, adding semantic information such as execution traces does not significantly enhance the reasoning abilities of Code LLMs during SFT or inference."}}
{"id": "2509.11691", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11691", "abs": "https://arxiv.org/abs/2509.11691", "authors": ["Lukas Rauh", "Mel-Rick S\u00fcner", "Daniel Schel", "Thomas Bauernhansl"], "title": "AI Asset Management for Manufacturing (AIM4M): Development of a Process Model for Operationalization", "comment": "10 pages, 4 figures, submitted for revision review at International\n  Conference on Industry of the Future and Smart Manufacturing (ISM) 2025", "summary": "The benefits of adopting artificial intelligence (AI) in manufacturing are\nundeniable. However, operationalizing AI beyond the prototype, especially when\ninvolved with cyber-physical production systems (CPPS), remains a significant\nchallenge due to the technical system complexity, a lack of implementation\nstandards and fragmented organizational processes. To this end, this paper\nproposes a new process model for the lifecycle management of AI assets designed\nto address challenges in manufacturing and facilitate effective\noperationalization throughout the entire AI lifecycle. The process model, as a\ntheoretical contribution, builds on machine learning operations (MLOps)\nprinciples and refines three aspects to address the domain-specific\nrequirements from the CPPS context. As a result, the proposed process model\naims to support organizations in practice to systematically develop, deploy and\nmanage AI assets across their full lifecycle while aligning with CPPS-specific\nconstraints and regulatory demands.", "AI": {"tldr": "Organizations struggle to fully implement AI in manufacturing due to system complexity and lack of standards. This paper presents a refined process model based on MLOps principles, specifically designed for CPPS, enabling better lifecycle management and compliance for AI assets.", "motivation": "AI adoption in manufacturing faces substantial obstacles, especially moving beyond prototypes due to technical complexity, lack of standards, and organizational fragmentation.", "method": "The paper proposes a process model for AI lifecycle management, tailored to manufacturing and cyber-physical production system (CPPS) contexts. This model adapts and refines MLOps principles to meet domain-specific requirements.", "result": "The proposed model enhances systematic development, deployment, and management of AI assets throughout their lifecycle, incorporating CPPS-specific constraints and regulatory needs.", "conclusion": "The process model can help organizations effectively operationalize AI in manufacturing by addressing both technical and organizational challenges unique to CPPS environments."}}
{"id": "2509.11708", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11708", "abs": "https://arxiv.org/abs/2509.11708", "authors": ["Zhantong Xue", "Pingchuan Ma", "Zhaoyu Wang", "Shuai Wang"], "title": "From Evaluation to Enhancement: Large Language Models for Zero-Knowledge Proof Code Generation", "comment": null, "summary": "Zero-knowledge proofs (ZKPs) are increasingly deployed in domains such as\nprivacy-preserving authentication, blockchain scalability, and secure finance.\nHowever, authoring ZK programs remains challenging: unlike mainstream\nprogramming, ZK development requires reasoning about finite field arithmetic,\nconstraint systems, and gadgets, making it knowledge-intensive and error-prone.\nWhile large language models (LLMs) have demonstrated strong code generation\ncapabilities in general-purpose languages, their effectiveness for ZK\nprogramming, where correctness hinges on both language mastery and gadget-level\nreasoning, remains unexplored. To address this gap, we propose\n\\textsc{ZK-Eval}, a domain-specific evaluation pipeline that probes LLM\ncapabilities at three levels: language knowledge, gadget competence, and\nend-to-end program generation. Our evaluation of four state-of-the-art LLMs\nreveals that models excel at surface-level syntax but struggle with gadget\nusage and semantic correctness, often yielding incorrect programs. Based on\nthese insights, we introduce \\textsc{ZK-Coder}, an agentic framework that\naugments LLMs with constraint sketching, guided retrieval, and interactive\nrepair. Experiments on Circom and Noir show substantial gains, with success\nrates improving from 17.35\\% to 83.38\\% and from 32.21\\% to 90.05\\%,\nrespectively. With \\textsc{ZK-Eval} and \\textsc{ZK-Coder}, we establish a\nfoundation for systematically measuring and augmenting LLMs in ZK code\ngeneration to lower barriers for practitioners and advance trustworthy\ncomputation.", "AI": {"tldr": "This paper analyzes how well large language models (LLMs) generate zero-knowledge (ZK) programs, finding that while LLMs handle syntax well, they struggle with deeper ZK concepts. By introducing evaluation tools and an augmentation framework, the authors raise program generation success rates dramatically, making ZK development more accessible and robust.", "motivation": "Authoring zero-knowledge (ZK) programs is complex and error-prone, requiring deep domain knowledge of finite field arithmetic, constraint systems, and specialized gadgets. Existing large language models show promise in general coding tasks but have not been evaluated specifically for the nuances of ZK programming.", "method": "The authors develop ZK-Eval, a domain-specific evaluation pipeline that assesses language model capabilities at three levels: language knowledge, gadget competence, and full program generation. They benchmark four state-of-the-art LLMs, then introduce ZK-Coder, an agentic framework that augments LLMs with constraint sketching, guided retrieval, and interactive repair.", "result": "LLMs perform well at basic syntax but frequently fail at gadget use and semantic correctness, often producing incorrect ZK programs. The ZK-Coder framework improves program success rates substantially: from 17.35% to 83.38% for Circom, and from 32.21% to 90.05% for Noir.", "conclusion": "The work provides the first systematic evaluation of LLM capabilities in ZK programming and offers an augmentation framework that dramatically improves outcomes. This lowers development barriers and advances reliable, trustworthy ZK computation."}}
{"id": "2509.11738", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11738", "abs": "https://arxiv.org/abs/2509.11738", "authors": ["Maria K\u00fc\u00fcsvek", "Hina Anwar"], "title": "Toward Greener Background Processes -- Measuring Energy Cost of Autosave Feature", "comment": "Author version. Accepted for publication in the proceedings of the\n  International Conference on Product-Focused Software Process Improvement\n  (PROFES 2025)", "summary": "Background processes in desktop applications are often overlooked in energy\nconsumption studies, yet they represent continuous, automated workloads with\nsignificant cumulative impact. This paper introduces a reusable process for\nevaluating the energy behavior of such features at the level of operational\ndesign. The process works in three phases: 1) decomposing background\nfunctionality into core operations, 2) operational isolation, and 3) controlled\nmeasurements enabling comparative profiling. We instantiate the process in a\ncase study of autosave implementations across three open-source Python-based\ntext editors. Using 900 empirical software-based energy measurements, we\nidentify key design factors affecting energy use, including save frequency,\nbuffering strategy, and auxiliary logic such as change detection. We give four\nactionable recommendations for greener implementations of autosave features in\nPython to support sustainable software practices.", "AI": {"tldr": "Background processes in desktop apps contribute significantly to energy use. This paper presents a three-stage method to analyze their energy behavior, demonstrated with autosave in Python text editors. Key design factors and four actionable recommendations for greener autosave are provided.", "motivation": "Background processes in desktop applications, such as autosave features, are often overlooked in energy consumption studies despite their significant cumulative impact. The motivation is to address this gap and provide a structured approach to evaluating and improving their energy efficiency.", "method": "The paper introduces a three-phase reusable process: 1) decomposing background features into core operations, 2) isolating these operations, and 3) performing controlled energy measurements for comparative profiling. This process is demonstrated via a case study of autosave implementations in three open-source Python text editors, with 900 energy measurement experiments.", "result": "The study identifies crucial design factors affecting energy consumption, notably save frequency, buffering strategies, and auxiliary logic like change detection. These insights result in four actionable recommendations for developing greener autosave features in Python applications.", "conclusion": "Energy consumption by background processes, specifically autosave features, can be systematically evaluated and optimized using the proposed process. Implementing the recommendations can lead to more sustainable software design."}}
{"id": "2509.11748", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.11748", "abs": "https://arxiv.org/abs/2509.11748", "authors": ["Marius Mignard", "Steven Costiou", "Nicolas Anquetil", "Anne Etien"], "title": "Analysing Python Machine Learning Notebooks with Moose", "comment": null, "summary": "Machine Learning (ML) code, particularly within notebooks, often exhibits\nlower quality compared to traditional software. Bad practices arise at three\ndistinct levels: general Python coding conventions, the organizational\nstructure of the notebook itself, and ML-specific aspects such as\nreproducibility and correct API usage. However, existing analysis tools\ntypically focus on only one of these levels and struggle to capture ML-specific\nsemantics, limiting their ability to detect issues. This paper introduces\nVespucci Linter, a static analysis tool with multi-level capabilities, built on\nMoose and designed to address this challenge. Leveraging a metamodeling\napproach that unifies the notebook's structural elements with Python code\nentities, our linter enables a more contextualized analysis to identify issues\nacross all three levels. We implemented 22 linting rules derived from the\nliterature and applied our tool to a corpus of 5,000 notebooks from the Kaggle\nplatform. The results reveal violations at all levels, validating the relevance\nof our multi-level approach and demonstrating Vespucci Linter's potential to\nimprove the quality and reliability of ML development in notebook environments.", "AI": {"tldr": "The paper introduces Vespucci Linter, a new static analysis tool that checks ML notebooks for bad coding practices at multiple levels (Python, notebook structure, and ML-specific issues). Tested on 5,000 Kaggle notebooks, it successfully finds problems that existing tools miss, showing it can improve code quality and reliability for ML projects.", "motivation": "Machine Learning notebooks frequently suffer from poor code quality due to a combination of general coding issues, notebook organizational problems, and ML-specific best practices not being followed. Existing tools fail to address all these levels, missing out on important context and semantics specific to ML.", "method": "The authors propose Vespucci Linter, a static analysis tool that operates at three levels: general Python, notebook structure, and ML-specific practices. It uses a metamodeling strategy to unify the representation of notebook and Python code constructs, and implements 22 rules sourced from relevant literature. The tool is then evaluated on 5,000 Kaggle notebooks.", "result": "The tool detects rule violations across all three levels in the analyzed notebooks, showing that the multi-level, context-aware approach is effective. The detected issues validate the need for multi-level linting in ML notebooks.", "conclusion": "Vespucci Linter can reliably uncover a wide range of code quality problems in ML notebooks, proving the importance and advantage of integrating multiple analysis levels to address unique ML coding requirements. The tool is positioned to help improve both reliability and reproducibility in notebook-based ML development."}}
{"id": "2509.11787", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.11787", "abs": "https://arxiv.org/abs/2509.11787", "authors": ["Pascal Joos", "Islem Bouzenia", "Michael Pradel"], "title": "CodeCureAgent: Automatic Classification and Repair of Static Analysis Warnings", "comment": null, "summary": "Static analysis tools are widely used to detect bugs, vulnerabilities, and\ncode smells. Traditionally, developers must resolve these warnings manually.\nBecause this process is tedious, developers sometimes ignore warnings, leading\nto an accumulation of warnings and a degradation of code quality. This paper\npresents CodeCureAgent, an approach that harnesses LLM-based agents to\nautomatically analyze, classify, and repair static analysis warnings. Unlike\nprevious work, our method does not follow a predetermined algorithm. Instead,\nwe adopt an agentic framework that iteratively invokes tools to gather\nadditional information from the codebase (e.g., via code search) and edit the\ncodebase to resolve the warning. CodeCureAgent detects and suppresses false\npositives, while fixing true positives when identified. We equip CodeCureAgent\nwith a three-step heuristic to approve patches: (1) build the project, (2)\nverify that the warning disappears without introducing new warnings, and (3)\nrun the test suite. We evaluate CodeCureAgent on a dataset of 1,000 SonarQube\nwarnings found in 106 Java projects and covering 291 distinct rules. Our\napproach produces plausible fixes for 96.8% of the warnings, outperforming\nstate-of-the-art baseline approaches by 30.7% and 29.2% in plausible-fix rate,\nrespectively. Manual inspection of 291 cases reveals a correct-fix rate of\n86.3%, showing that CodeCureAgent can reliably repair static analysis warnings.\nThe approach incurs LLM costs of about 2.9 cents (USD) and an end-to-end\nprocessing time of about four minutes per warning. We envision CodeCureAgent\nhelping to clean existing codebases and being integrated into CI/CD pipelines\nto prevent the accumulation of static analysis warnings.", "AI": {"tldr": "CodeCureAgent is an LLM-powered tool that can automatically fix most static analysis warnings in code, outperforming existing solutions in both accuracy and efficiency. It\u2019s cost-effective and suitable for continuous integration workflows, potentially reducing manual workload and improving overall code health.", "motivation": "Static analysis tools are essential for improving code quality by detecting bugs and vulnerabilities, but their warnings require manual resolution, which is tedious and can be ignored by developers, leading to degraded code quality.", "method": "The authors propose CodeCureAgent, an LLM-based agentic system that automatically analyzes, classifies, and repairs static analysis warnings in code. It operates iteratively, employing code searches and edits, and applies a three-step patch approval heuristic: building the project, verifying warning resolution without new issues, and running tests. The agent distinguishes false positives and resolves true positives.", "result": "On 1,000 SonarQube warnings across 106 Java projects (291 rules), CodeCureAgent produced plausible fixes for 96.8% of cases, improving upon previous baselines by over 29%. Manual review found an 86.3% correct-fix rate. The average cost is 2.9 cents and four minutes per warning.", "conclusion": "CodeCureAgent can effectively and efficiently repair static analysis warnings, outperforming prior methods, and is practical for integration into development pipelines to help maintain and improve code quality."}}
