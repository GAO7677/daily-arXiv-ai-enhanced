<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 26]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Reflective Paper-to-Code Reproduction Enabled by Fine-Grained Verification](https://arxiv.org/abs/2508.16671)
*Mingyang Zhou,Quanming Yao,Lun Du,Lanning Wei,Da Zheng*

Main category: cs.SE

TL;DR: RePro is a new framework that uses detailed checklists to guide iterative code generation and correction, resulting in a 13% improvement over existing methods for reproducing machine learning papers from text.


<details>
  <summary>Details</summary>
Motivation: Reproducing machine learning papers is crucial for scientific progress but is difficult due to diverse paper structures, complex methods, and varied configurations. Existing automated methods struggle to accurately reproduce implementation details, especially mathematical formulas and algorithmic logic. Previous work shows feedback and reflection improve agent performance, but current solutions do not effectively leverage these strategies.

Method: The authors propose RePro, a Reflective Paper-to-Code Reproduction framework. RePro automatically extracts a 'fingerprint' from a paper, which consists of detailed and atomic criteria that provide high-quality supervisory signals. The system then generates code based on this fingerprint and iteratively verifies and refines the code using the fingerprint, systematically detecting and correcting discrepancies to ensure alignment with the paper's true implementation.

Result: RePro was evaluated on the PaperBench Code-Dev benchmark. It achieved a 13.0% improvement over baseline methods and was especially effective at revising complex logical and mathematical implementation details.

Conclusion: RePro provides a systematic and effective framework for reproducing machine learning papers by leveraging targeted, reflective iteration and high-quality criteria extracted directly from research papers. This addresses key challenges in paper reproduction and significantly improves the accuracy of automatic code generation from papers.

Abstract: Reproducing machine learning papers is essential for scientific progress but
remains challenging for both humans and automated agents. Existing agent-based
methods often struggle to fully and accurately reproduce implementation details
such as mathematical formulas and algorithmic logic. Previous studies show that
reflection with explicit feedback improves agent performance. However, current
paper reproduction methods fail to effectively adopt this strategy. This gap
mainly arises from the diverse paper patterns, complex method modules, and
varied configurations encountered in research papers. Motivated by how humans
use systematic checklists to efficiently debug complex code, we propose
\textbf{RePro}, a \textbf{Re}flective Paper-to-Code \textbf{Repro}duction
framework that automatically extracts a paper's fingerprint, referring to a
comprehensive set of accurate and atomic criteria serving as high-quality
supervisory signals. The framework first generates code based on the extracted
information, and then leverages the fingerprint within iterative verification
and refinement loop. This approach systematically detects discrepancies and
produces targeted revisions to align generated code with the paper's
implementation details. Extensive experiments on the PaperBench Code-Dev
benchmark have been conducted, RePro achieves 13.0\% performance gap over
baselines, and it correctly revises complex logical and mathematical criteria
in reflecting, on which the effectiveness is obvious.

</details>


### [2] [Cognitive Agents Powered by Large Language Models for Agile Software Project Management](https://arxiv.org/abs/2508.16678)
*Konrad Cinkusz,Jarosław A. Chudziak,Ewa Niewiadomska-Szynkiewicz*

Main category: cs.SE

TL;DR: This paper simulates using LLM-powered agents in Agile project management, finding that these agents measurably improve efficiency, communication, and quality, suggesting a transformative potential for software engineering teams.


<details>
  <summary>Details</summary>
Motivation: To explore how Large Language Model-powered cognitive agents can enhance software project management in Agile environments by automating fundamental roles and improving team collaboration and decision-making.

Method: The study uses the CogniSim ecosystem to deploy and simulate virtual cognitive agents in software project scenarios aligned with the Scaled Agile Framework (SAFe). Iterative simulations measure agents’ performance in tasks such as delegation, inter-agent communication, and project lifecycle management.

Result: Cognitive agents showed advanced task delegation, effective communication, and better project management. They improved metrics such as task completion time, deliverable quality, and communication clarity. The agents demonstrated adaptability and scalability across different and complex project types.

Conclusion: Integrating LLM-powered cognitive agents into Agile project management enhances efficiency, collaboration, and decision-making. This approach refines project task execution and suggests a paradigm shift for modern software development teams.

Abstract: This paper investigates the integration of cognitive agents powered by Large
Language Models (LLMs) within the Scaled Agile Framework (SAFe) to reinforce
software project management. By deploying virtual agents in simulated software
environments, this study explores their potential to fulfill fundamental roles
in IT project development, thereby optimizing project outcomes through
intelligent automation. Particular emphasis is placed on the adaptability of
these agents to Agile methodologies and their transformative impact on
decision-making, problem-solving, and collaboration dynamics. The research
leverages the CogniSim ecosystem, a platform designed to simulate real-world
software engineering challenges, such as aligning technical capabilities with
business objectives, managing interdependencies, and maintaining project
agility. Through iterative simulations, cognitive agents demonstrate advanced
capabilities in task delegation, inter-agent communication, and project
lifecycle management. By employing natural language processing to facilitate
meaningful dialogues, these agents emulate human roles and improve the
efficiency and precision of Agile practices. Key findings from this
investigation highlight the ability of LLM-powered cognitive agents to deliver
measurable improvements in various metrics, including task completion times,
quality of deliverables, and communication coherence. These agents exhibit
scalability and adaptability, ensuring their applicability across diverse and
complex project environments. This study underscores the potential of
integrating LLM-powered agents into Agile project management frameworks as a
means of advancing software engineering practices. This integration not only
refines the execution of project management tasks but also sets the stage for a
paradigm shift in how teams collaborate and address emerging challenges.

</details>


### [3] [Democratizing AI Development: Local LLM Deployment for India's Developer Ecosystem in the Era of Tokenized APIs](https://arxiv.org/abs/2508.16684)
*Vikranth Udandarao,Nipun Misra*

Main category: cs.SE

TL;DR: Deploying LLMs locally, instead of using expensive commercial APIs, greatly boosts hands-on AI learning and experimentation for Indian developers, significantly lowering costs and increasing innovation.


<details>
  <summary>Details</summary>
Motivation: Indian developers often struggle to continuously learn and experiment with commercial LLM APIs due to significant economic and resource barriers.

Method: The study conducted a mixed-methods analysis involving 180 Indian developers, students, and AI enthusiasts, comparing local LLM deployment using Ollama to commercial cloud-based services.

Result: Local LLM deployment reduced costs by 33% and enabled developers to complete over twice as many experimental iterations. It also led to a deeper understanding of advanced AI architectures.

Conclusion: Local deployment of LLMs is crucial for accessible and inclusive AI development in India, dramatically enhancing experimentation, learning, and innovation in resource-limited settings.

Abstract: India's developer community faces significant barriers to sustained
experimentation and learning with commercial Large Language Model (LLM) APIs,
primarily due to economic and infrastructural constraints. This study
empirically evaluates local LLM deployment using Ollama as an alternative to
commercial cloud-based services for developer-focused applications. Through a
mixed-methods analysis involving 180 Indian developers, students, and AI
enthusiasts, we find that local deployment enables substantially greater
hands-on development and experimentation, while reducing costs by 33% compared
to commercial solutions. Developers using local LLMs completed over twice as
many experimental iterations and reported deeper understanding of advanced AI
architectures. Our results highlight local deployment as a critical enabler for
inclusive and accessible AI development, demonstrating how technological
accessibility can enhance learning outcomes and innovation capacity in
resource-constrained environments.

</details>


### [4] [Cybernaut: Towards Reliable Web Automation](https://arxiv.org/abs/2508.16688)
*Ankur Tomar,Hengyue Liang,Indranil Bhattacharya,Natalia Larios,Francesco Carbone*

Main category: cs.SE

TL;DR: Cybernaut is a new AI framework that boosts the reliability and accuracy of web automation in tough enterprise environments, significantly outperforming previous methods and enabling more robust automated workflows.


<details>
  <summary>Details</summary>
Motivation: AI-driven web automation using large language models (LLMs) presents exciting potential in optimizing digital workflows. However, current solutions struggle with real-world deployment, especially in complex, poorly designed internal web applications. Key challenges include consistent execution, accurately identifying HTML elements, achieving human-like accuracy, and a lack of benchmarking data.

Method: The authors introduce 'Cybernaut,' a new framework aimed at enterprise web automation. Its method involves: (1) an SOP generator that translates user demonstrations into automation steps, (2) a specialized HTML DOM recognition system to handle complex interfaces, and (3) a new metric for measuring execution consistency.

Result: Empirical evaluation on internal benchmarks shows Cybernaut improves task execution success rate from 72% to 88.68% over previous methods. It also reliably identifies consistent execution patterns with 84.7% accuracy, enabling better confidence assessment and adaptive guidance.

Conclusion: Cybernaut effectively addresses web automation challenges in enterprise environments, demonstrating significant improvements in consistency and reliability. The work lays groundwork for further advancements in AI-powered automation of complex workflows.

Abstract: The emergence of AI-driven web automation through Large Language Models
(LLMs) offers unprecedented opportunities for optimizing digital workflows.
However, deploying such systems within industry's real-world environments
presents four core challenges: (1) ensuring consistent execution, (2)
accurately identifying critical HTML elements, (3) meeting human-like accuracy
in order to automate operations at scale and (4) the lack of comprehensive
benchmarking data on internal web applications. Existing solutions are
primarily tailored for well-designed, consumer-facing websites (e.g.,
Amazon.com, Apple.com) and fall short in addressing the complexity of
poorly-designed internal web interfaces. To address these limitations, we
present Cybernaut, a novel framework to ensure high execution consistency in
web automation agents designed for robust enterprise use. Our contributions are
threefold: (1) a Standard Operating Procedure (SOP) generator that converts
user demonstrations into reliable automation instructions for linear browsing
tasks, (2) a high-precision HTML DOM element recognition system tailored for
the challenge of complex web interfaces, and (3) a quantitative metric to
assess execution consistency. The empirical evaluation on our internal
benchmark demonstrates that using our framework enables a 23.2% improvement
(from 72% to 88.68%) in task execution success rate over the browser_use.
Cybernaut identifies consistent execution patterns with 84.7% accuracy,
enabling reliable confidence assessment and adaptive guidance during task
execution in real-world systems. These results highlight Cybernaut's
effectiveness in enterprise-scale web automation and lay a foundation for
future advancements in web automation.

</details>


### [5] [A Scalable Framework for the Management of STPA Requirements: a Case Study on eVTOL Operations](https://arxiv.org/abs/2508.16708)
*Shufeng Chen,Halima El Badaoui,Mariat James Elizebeth,Takuya Nakashima,Siddartha Khastgir,Paul Jennings*

Main category: cs.SE

TL;DR: The paper proposes and validates a new framework and toolchain for efficiently prioritising and managing safety requirements produced by STPA, using expert input and Monte-Carlo Simulation to ensure objectivity and traceability, with successful application in the eVTOL aviation sector.


<details>
  <summary>Details</summary>
Motivation: STPA can generate thousands of safety requirements for complex systems, but lacks a structured and practical approach for prioritising and managing these requirements, especially in fast-paced or iterative development environments.

Method: The paper presents a scalable framework for prioritising requirements identified through STPA. It integrates outputs from each STPA step and uses expert evaluation across four factors: implementation time, cost, requirement type, and regulatory coverage. To reduce subjectivity, Monte-Carlo Simulation (MCS) is used to stabilise and calculate requirement rankings. The framework is supported by an automation toolchain that visualises prioritised requirements and enables traceability. Validation was performed via a real-world case study in collaboration with the UK Civil Aviation Authority for eVTOL operations.

Result: The framework was shown to support efficient decision-making and management of high-impact requirements in safety-critical systems. It was validated in a real-world eVTOL case and its findings were incorporated into an official Civil Aviation Authority publication, helping regulators and operators manage operational risks and mitigations.

Conclusion: A practical, scalable framework for managing and prioritising STPA-derived safety requirements was developed and validated, improving requirement traceability, stability, and decision-support for emerging technological systems.

Abstract: System-Theoretic Process Analysis (STPA) is a recommended method for
analysing complex systems, capable of identifying thousands of safety
requirements often missed by traditional techniques such as Failure Mode and
Effects Analysis (FMEA) and Fault Tree Analysis (FTA). However, the absence of
a structured framework for managing and prioritising these requirements
presents challenges, particularly in fast-paced development environments. This
paper introduces a scalable framework for prioritising STPA-derived
requirements. The framework integrates outputs from each STPA step and
incorporates expert evaluations based on four key factors: implementation time,
cost, requirement type, and regulatory coverage. To reduce subjectivity,
Monte-Carlo Simulation (MCS) is employed to calculate and stabilise requirement
rankings. An automation toolchain supports the framework, enabling dynamic
mapping of prioritised requirements in a scaling matrix. This visualisation
aids decision-making and ensures traceability across development phases. The
framework is applicable from early conceptualisation to more advanced stages,
enhancing its utility in iterative system development. The framework was
validated through a real-world case study focused on Electric Vertical Take-off
and Landing (eVTOL) operations, conducted in collaboration with the UK Civil
Aviation Authority. The findings contributed directly to CAP3141, a Civil
Aviation Publication that identifies systemic operational risks and safety
mitigations for regulators, operators, and vertiports. The prioritisation
process supported decision-making by helping stakeholders identify and manage
high-impact requirements efficiently. This work contributes a practical
solution for managing STPA outputs, bridging gaps in requirement prioritisation
and supporting safety-critical development in emerging technologies.

</details>


### [6] [CelloAI: Leveraging Large Language Models for HPC Software Development in High Energy Physics](https://arxiv.org/abs/2508.16713)
*Mohammad Atif,Kriti Chopra,Ozgur Kilic,Tianle Wang,Zhihua Dong,Charles Leggett,Meifeng Lin,Paolo Calafiura,Salman Habib*

Main category: cs.SE

TL;DR: CelloAI is a locally hosted AI assistant that uses large language models and enhanced retrieval strategies to help physicists document and generate code for complex scientific software, addressing HPC integration challenges in HEP. It outperforms prior tools in effectiveness while meeting scientific standards for privacy and transparency.


<details>
  <summary>Details</summary>
Motivation: The motivation is the difficulty of adopting High Performance Computing in High Energy Physics (HEP) due to legacy software complexities, heterogeneous architectures, and poor documentation. There is a need for tools to aid code documentation and generation for efficient and reliable adaptation.

Method: CelloAI is a locally hosted coding assistant based on Large Language Models enhanced with retrieval-augmented generation. It features specialized documentation (Doxygen comments, summaries, chatbot) and code generation (syntax-aware chunking, callgraph integration) components. Evaluation uses real-world data from major HEP experiments and compares embedding models.

Result: CelloAI supports code documentation and generation across large, complex legacy scientific codebases, enhancing code comprehension and reliable code modification. The AI assistant preserves data privacy and offers transparency and safety necessary for scientific computing, outperforming traditional code retrieval approaches in effectiveness.

Conclusion: CelloAI is a practical solution for modernizing and maintaining HEP scientific codes, facilitating documentation and code adaptation within HPC environments, while ensuring privacy and reliability crucial for scientific research.

Abstract: Next-generation High Energy Physics (HEP) experiments will generate
unprecedented data volumes, necessitating High Performance Computing (HPC)
integration alongside traditional high-throughput computing. However, HPC
adoption in HEP is hindered by the challenge of porting legacy software to
heterogeneous architectures and the sparse documentation of these complex
scientific codebases. We present CelloAI, a locally hosted coding assistant
that leverages Large Language Models (LLMs) with retrieval-augmented generation
(RAG) to support HEP code documentation and generation. This local deployment
ensures data privacy, eliminates recurring costs and provides access to large
context windows without external dependencies. CelloAI addresses two primary
use cases, code documentation and code generation, through specialized
components. For code documentation, the assistant provides: (a) Doxygen style
comment generation for all functions and classes by retrieving relevant
information from RAG sources (papers, posters, presentations), (b) file-level
summary generation, and (c) an interactive chatbot for code comprehension
queries. For code generation, CelloAI employs syntax-aware chunking strategies
that preserve syntactic boundaries during embedding, improving retrieval
accuracy in large codebases. The system integrates callgraph knowledge to
maintain dependency awareness during code modifications and provides
AI-generated suggestions for performance optimization and accurate refactoring.
We evaluate CelloAI using real-world HEP applications from ATLAS, CMS, and DUNE
experiments, comparing different embedding models for code retrieval
effectiveness. Our results demonstrate the AI assistant's capability to enhance
code understanding and support reliable code generation while maintaining the
transparency and safety requirements essential for scientific computing
environments.

</details>


### [7] [Who Wins the Race? (R Vs Python) - An Exploratory Study on Energy Consumption of Machine Learning Algorithms](https://arxiv.org/abs/2508.17344)
*Rajrupa Chattaraj,Sridhar Chimalakonda,Vibhu Saujanya Sharma,Vikrant Kaulgud*

Main category: cs.SE

TL;DR: The paper investigates the energy impact of Python vs. R for ML tasks, showing major differences and highlighting the importance of language choice for greener machine learning.


<details>
  <summary>Details</summary>
Motivation: To address the lack of studies on energy consumption associated with ML model training and inference across different programming languages, especially given the environmental concerns of energy usage in ML.

Method: Empirical measurement and comparison of energy consumption and run-time performance in five regression and five classification ML tasks using Python and R.

Result: The study found statistically significant differences in energy costs between Python and R in 95% of cases, with language choice affecting energy efficiency by up to 99.16% for model training and 99.8% for inference tasks.

Conclusion: There are significant differences in energy consumption and performance between Python and R for ML tasks; language choice can greatly impact energy efficiency.

Abstract: The utilization of Machine Learning (ML) in contemporary software systems is
extensive and continually expanding. However, its usage is energy-intensive,
contributing to increased carbon emissions and demanding significant resources.
While numerous studies examine the performance and accuracy of ML, only a
limited few focus on its environmental aspects, particularly energy
consumption. In addition, despite emerging efforts to compare energy
consumption across various programming languages for specific algorithms and
tasks, there remains a gap specifically in comparing these languages for
ML-based tasks. This paper aims to raise awareness of the energy costs
associated with employing different programming languages for ML model training
and inference. Through this empirical study, we measure and compare the energy
consumption along with run-time performance of five regression and five
classification tasks implemented in Python and R, the two most popular
programming languages in this context. Our study results reveal a statistically
significant difference in costs between the two languages in 95% of the cases
examined. Furthermore, our analysis demonstrates that the choice of programming
language can influence energy efficiency significantly, up to 99.16% during
model training and up to 99.8% during inferences, for a given ML task.

</details>


### [8] [EyeMulator: Improving Code Language Models by Mimicking Human Visual Attention](https://arxiv.org/abs/2508.16771)
*Yifan Zhang,Chen Huang,Yueke Zhang,Jiahao Zhang,Toby Jia-Jun Li,Collin McMillan,Kevin Leach,Yu Huang*

Main category: cs.SE

TL;DR: EyeMulator uses eye-tracking data to adjust the way CodeLLMs learn token importance, helping them mimic human attention. This approach yields better results on coding tasks compared to traditional models trained without human attention data.


<details>
  <summary>Details</summary>
Motivation: CodeLLMs use machine attention based solely on token salience in training data, which differs from human developers, who intuitively prioritize tokens based on visual attention. The motivation is to reduce this gap and make CodeLLMs better align with human cognitive patterns.

Method: The authors introduce EyeMulator, a technique that adds special weights to each input token in the loss function during LLM fine-tuning. These weights are derived from eye-tracking data collected from software engineering tasks, reflecting human visual attention patterns.

Result: Models trained with EyeMulator learn to mimic human visual attention and outperform strong CodeLLM baselines on tasks like code translation, completion, and summarization. Ablation studies confirm that the gains are due to the integration of human attention data.

Conclusion: Incorporating human-derived visual attention weights into CodeLLM training produces models that better emulate human judgment and attention during software development tasks, leading to measurable improvements in performance.

Abstract: Code language models (so-called CodeLLMs) are now commonplace in software
development. As a general rule, CodeLLMs are trained by dividing training
examples into input tokens and then learn importance of those tokens in a
process called machine attention. Machine attention is based solely on input
token salience to output token examples during training. Human software
developers are different, as humans intuitively know that some tokens are more
salient than others. While intuition itself is ineffable and a subject of
philosophy, clues about salience are present in human visual attention, since
people tend to look at more salient words more often. In this paper, we present
EyeMulator, a technique for training CodeLLMs to mimic human visual attention
while training for various software development tasks. We add special weights
for each token in each input example to the loss function used during LLM
fine-tuning. We draw these weights from observations of human visual attention
derived from a previously-collected publicly-available dataset of eye-tracking
experiments in software engineering tasks. These new weights ultimately induce
changes in the attention of the subject LLM during training, resulting in a
model that does not need eye-tracking data during inference. Our evaluation
shows that EyeMulator outperforms strong LLM baselines on several tasks such as
code translation, completion and summarization. We further show an ablation
study that demonstrates the improvement is due to subject models learning to
mimic human attention.

</details>


### [9] [DevLicOps: A Framework for Mitigating Licensing Risks in AI-Generated Code](https://arxiv.org/abs/2508.16853)
*Pratyush Nidhi Sharma,Lauren Wright,Anne Herfurth,Munsif Sokiyna,Pratyaksh Nidhi Sharma,Sethu Das,Mikko Siponen*

Main category: cs.SE

TL;DR: As AI coding assistants become common, they bring major legal risks by generating potentially non-compliant code. The paper proposes DevLicOps, a management framework to help organizations handle these risks with governance and incident response strategies, emphasizing the importance of proactive license compliance.


<details>
  <summary>Details</summary>
Motivation: The rapid adoption of AI coding assistants (ACAs) introduces significant legal and compliance risks due to code generation under restrictive licenses, while many developers lack awareness and training in these legal aspects.

Method: The paper introduces DevLicOps, a practical management framework for IT leaders focused on governance, incident response, and balancing tradeoffs regarding ACA-related licensing risks.

Result: The DevLicOps framework supports responsible and risk-aware AI-driven software development by guiding leaders to manage the evolving legal risks of ACA-generated code.

Conclusion: Proactive license compliance, enabled by a structured framework like DevLicOps, is critical as ACA adoption grows and legal standards shift; organizations must address these risks to avoid legal exposure and ensure responsible development.

Abstract: Generative AI coding assistants (ACAs) are widely adopted yet pose serious
legal and compliance risks. ACAs can generate code governed by restrictive
open-source licenses (e.g., GPL), potentially exposing companies to litigation
or forced open-sourcing. Few developers are trained in these risks, and legal
standards vary globally, especially with outsourcing. Our article introduces
DevLicOps, a practical framework that helps IT leaders manage ACA-related
licensing risks through governance, incident response, and informed tradeoffs.
As ACA adoption grows and legal frameworks evolve, proactive license compliance
is essential for responsible, risk-aware software development in the AI era.

</details>


### [10] [TriagerX: Dual Transformers for Bug Triaging Tasks with Content and Interaction Based Rankings](https://arxiv.org/abs/2508.16860)
*Md Afif Al Mamun,Gias Uddin,Lan Xia,Longyu Zhang*

Main category: cs.SE

TL;DR: TriagerX is a dual-transformer-based bug triaging system that considers developer interaction history, outperforming state-of-the-art methods by substantial margins and proving effective in real-world industry deployment.


<details>
  <summary>Details</summary>
Motivation: Pretrained Language Models (PLMs) have shown promise for bug triaging by capturing token semantics better than traditional ML models, but they still focus on less relevant tokens and do not take developer interaction history into account. These limitations hinder their performance and practical adoption for bug triage automation.

Method: The authors propose TriagerX, a dual-transformer architecture. Unlike single-transformer SOTA baselines, TriagerX combines recommendations from the last three layers of two separate transformers for robust content-based candidate developer ranking. It then uses a novel interaction-based ranking that incorporates developers' historical interactions with similar bug fixes.

Result: TriagerX significantly outperforms nine transformer-based SOTA methods on five datasets, with Top-1 and Top-3 developer recommendation accuracy increases of over 10%. Deployment with a large industry partner showed up to 10% improvement for component recommendations and up to 54% for developer recommendations compared to SOTA baselines.

Conclusion: TriagerX effectively addresses the token relevance and interaction history limitations of PLMs, resulting in superior bug triage performance and successful industry deployment.

Abstract: Pretrained Language Models or PLMs are transformer-based architectures that
can be used in bug triaging tasks. PLMs can better capture token semantics than
traditional Machine Learning (ML) models that rely on statistical features
(e.g., TF-IDF, bag of words). However, PLMs may still attend to less relevant
tokens in a bug report, which can impact their effectiveness. In addition, the
model can be sub-optimal with its recommendations when the interaction history
of developers around similar bugs is not taken into account. We designed
TriagerX to address these limitations. First, to assess token semantics more
reliably, we leverage a dual-transformer architecture. Unlike current
state-of-the-art (SOTA) baselines that employ a single transformer
architecture, TriagerX collects recommendations from two transformers with each
offering recommendations via its last three layers. This setup generates a
robust content-based ranking of candidate developers. TriagerX then refines
this ranking by employing a novel interaction-based ranking methodology, which
considers developers' historical interactions with similar fixed bugs. Across
five datasets, TriagerX surpasses all nine transformer-based methods, including
SOTA baselines, often improving Top-1 and Top-3 developer recommendation
accuracy by over 10%. We worked with our large industry partner to successfully
deploy TriagerX in their development environment. The partner required both
developer and component recommendations, with components acting as proxies for
team assignments-particularly useful in cases of developer turnover or team
changes. We trained TriagerX on the partner's dataset for both tasks, and it
outperformed SOTA baselines by up to 10% for component recommendations and 54%
for developer recommendations.

</details>


### [11] [Mind the Gap: A Decade-Scale Empirical Study of Multi-Stakeholder Dynamics in VR Ecosystem](https://arxiv.org/abs/2508.16903)
*Yijun Lu,Hironori Washizaki,Naoyasu Ubayashi,Nobukazu Yoshioka,Chenhao Wu,Masanari Kondo,Yuyin Ma,Jiong Dong,Jianjin Zhao,Dongqi Han*

Main category: cs.SE

TL;DR: By analyzing user reviews and developer discussions together, the study uncovers mismatches between user concerns (notably around inclusivity and safety) and developer priorities, offering practical guidance to bridge these gaps in VR platforms.


<details>
  <summary>Details</summary>
Motivation: Current research often analyzes user or developer feedback separately in VR ecosystems, making it difficult to understand how user concerns are addressed by developers. This paper aims to bridge the gap by examining both perspectives together.

Method: The paper introduces a multi-view empirical framework that uses topic modeling and quantitative impact analysis on a large dataset of 944,320 user reviews and 389,477 developer posts.

Result: The study identified overlaps in concerns between users and developers (like performance and input methods), but also found significant gaps, especially regarding inclusivity and community safety (such as LGBTQ+ representation and child-friendly content), which are often ignored in developer forums despite user emphasis.

Conclusion: Aligning user and developer perspectives reveals actionable gaps in VR ecosystem development, informing targeted improvements for platform governance and the user experience.

Abstract: In the development and evolution of VR ecosystem, platform stakeholders
continuously adapt their products in response to user and technical feedback,
often reflected in subtle shifts in discussion topics or system updates. A
comprehensive understanding of these changes is essential for identifying gaps
between user expectations and developer actions, which can guide more effective
quality assurance and user-centered innovation. While previous studies have
analyzed either user reviews or developer discussions in isolation, such
approaches typically fail to reveal how specific user concerns are (or are not)
addressed by corresponding technical activities. To address this limitation,
our study introduces a multi-view empirical framework that systematically
compares and aligns stakeholder perspectives. By applying topic modeling and
quantitative impact analysis to 944,320 user reviews and 389,477 developer
posts, we identify not only the overlap in concerns (e.g., performance, input
methods), but also clear gaps in areas like inclusivity and community safety
(e.g., LGBTQ+ representation, child-friendly content). Our findings show that
while users repeatedly raise such issues, they are rarely discussed in
developer forums. These insights enable data-driven recommendations for closing
the user-developer gap in VR ecosystems, offering practical implications for
platform governance and the design of next-generation VR systems.

</details>


### [12] [What Developers Ask to ChatGPT in GitHub Pull Requests? an Exploratory Study](https://arxiv.org/abs/2508.17161)
*Julyanara R. Silva,Carlos Eduardo C. Dantas,Marcelo A. Maia*

Main category: cs.SE

TL;DR: This paper manually analyzes developer interactions with ChatGPT in merged GitHub Pull Requests, categorizing 14 types of prompts into four groups. It finds that code generation tasks require more back-and-forth than text reviews or technical questions, thereby offering new insights into how LLMs are used in real-world software development workflows.


<details>
  <summary>Details</summary>
Motivation: There is a lack of understanding about how developers interact with Large Language Models (LLMs), such as ChatGPT, in the context of contributions to software codebases. The study is motivated by the need to explore the nature of these interactions and how they influence software development processes.

Method: The authors conducted a manual evaluation of 155 ChatGPT share links extracted from 139 merged Pull Requests (PRs). They analyzed these links to uncover patterns in interactions between developers, reviewers, and ChatGPT that resulted in changes merged into the main codebase.

Result: The study produced a catalog of 14 types of ChatGPT requests, grouped into four broad categories. Common requests involved code review and generating code snippets for specific tasks, as well as seeking technical explanations or refinements for webpage texts. The study also found that code generation prompts typically required more interactions to reach satisfactory answers, compared to prompts for text review or technical information.

Conclusion: The interaction patterns between developers and ChatGPT are varied, with certain types of requests—especially those related to code generation—being more complex and requiring iterative communication. The research provides a categorization of interaction types, enhancing our understanding of how developers leverage LLMs like ChatGPT in real software development scenarios.

Abstract: The emergence of Large Language Models (LLMs), such as ChatGPT, has
introduced a new set of tools to support software developers in solving pro-
gramming tasks. However, our understanding of the interactions (i.e., prompts)
between developers and ChatGPT that result in contributions to the codebase
remains limited. To explore this limitation, we conducted a manual evaluation
of 155 valid ChatGPT share links extracted from 139 merged Pull Requests (PRs),
revealing the interactions between developers and reviewers with ChatGPT that
led to merges into the main codebase. Our results produced a catalog of 14
types of ChatGPT requests categorized into four main groups. We found a
significant number of requests involving code review and the implementation of
code snippets based on specific tasks. Developers also sought to clarify doubts
by requesting technical explanations or by asking for text refinements for
their web pages. Furthermore, we verified that prompts involving code
generation generally required more interactions to produce the desired answer
compared to prompts requesting text review or technical information.

</details>


### [13] [Agentic AI for Software: thoughts from Software Engineering community](https://arxiv.org/abs/2508.17343)
*Abhik Roychoudhury*

Main category: cs.SE

TL;DR: AI agents are evolving to perform more sophisticated roles in software engineering, beyond code generation, by automating tasks and inferring developer intent. The future of software engineering will rely on trustworthy agentic workflows, including AI-based verification and validation to ensure quality as automation increases.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the growing role of AI agents in software engineering, moving beyond prompt-based code generation to autonomous participation in a wider range of software tasks, and addressing the core challenge of understanding developer intent.

Method: The method is a conceptual and visionary analysis, discussing how AI agents could autonomously perform a variety of software development tasks, leveraging program analysis tools, and the importance of specification inference.

Result: The paper highlights the necessity for progress in intent inference and proposes that trustworthy, agentic AI software workflows should include AI-based verification and validation to manage the increasing volume of AI-generated code.

Conclusion: Successfully integrating AI agents into software engineering requires resolving the challenge of inferring developer intent, alongside developing robust AI-based verification and validation systems for automated code.

Abstract: AI agents have recently shown significant promise in software engineering.
Much public attention has been transfixed on the topic of code generation from
Large Language Models (LLMs) via a prompt. However, software engineering is
much more than programming, and AI agents go far beyond instructions given by a
prompt.
  At the code level, common software tasks include code generation, testing,
and program repair. Design level software tasks may include architecture
exploration, requirements understanding, and requirements enforcement at the
code level. Each of these software tasks involves micro-decisions which can be
taken autonomously by an AI agent, aided by program analysis tools. This
creates the vision of an AI software engineer, where the AI agent can be seen
as a member of a development team.
  Conceptually, the key to successfully developing trustworthy agentic AI-based
software workflows will be to resolve the core difficulty in software
engineering - the deciphering and clarification of developer intent.
Specification inference, or deciphering the intent, thus lies at the heart of
many software tasks, including software maintenance and program repair. A
successful deployment of agentic technology into software engineering would
involve making conceptual progress in such intent inference via agents.
  Trusting the AI agent becomes a key aspect, as software engineering becomes
more automated. Higher automation also leads to higher volume of code being
automatically generated, and then integrated into code-bases. Thus to deal with
this explosion, an emerging direction is AI-based verification and validation
(V & V) of AI generated code. We posit that agentic software workflows in
future will include such AIbased V&V.

</details>


### [14] [Code Difference Guided Fuzzing for FPGA Logic Synthesis Compilers via Bayesian Optimization](https://arxiv.org/abs/2508.17713)
*Zhihao Xu,Shikai Guo,Guilin Zhao,Peiyu Zou,Siwen Wang,Qian Ma,Hui Li,Furui Zhan*

Main category: cs.SE

TL;DR: LSC-Fuzz uses Bayesian optimization to guide mutation and generate diverse HDL test cases, resulting in more effective bug detection in FPGA logic synthesis compilers compared to previous fuzzing strategies.


<details>
  <summary>Details</summary>
Motivation: Bugs in FPGA logic synthesis compilers can cause unexpected behaviors and security risks in safety-critical applications. Existing methods for fuzzing these compilers use simple, unguided mutation strategies, which limits their effectiveness in finding bugs.

Method: The authors propose LSC-Fuzz, a guided mutation testing strategy based on Bayesian optimization. It consists of a test-program generation component, a Bayesian diversity selection component for generating varied HDL code, and an equivalent check component to detect bugs in FPGA logic synthesis compilers.

Result: Over three months, LSC-Fuzz discovered 16 bugs in FPGA logic synthesis compilers, and 12 of these bugs were confirmed by official technical support.

Conclusion: LSC-Fuzz, with its Bayesian-guided mutation and diversity selection, is an effective approach for detecting bugs in FPGA logic synthesis compilers, surpassing previous methods.

Abstract: Field Programmable Gate Arrays (FPGAs) play a crucial role in Electronic
Design Automation (EDA) applications, which have been widely used in
safety-critical environments, including aerospace, chip manufacturing, and
medical devices. A critical step in FPGA development is logic synthesis, which
enables developers to translate their software designs into hardware net lists,
which facilitates the physical implementation of the chip, detailed timing and
power analysis, gate-level simulation, test vector generation, and optimization
and consistency checking. However, bugs or incorrect implementations in FPGA
logic synthesis compilers may lead to unexpected behaviors in target
wapplications, posing security risks. Therefore, it is crucial to eliminate
such bugs in FPGA logic synthesis compilers. The effectiveness of existing
works is still limited by its simple, blind mutation strategy. To address this
challenge, we propose a guided mutation strategy based on Bayesian optimization
called LSC-Fuzz to detect bugs in FPGA logic synthesis compilers. Specifically,
LSC-Fuzz consists of three components: the test-program generation component,
the Bayesian diversity selection component, and the equivalent check component.
By performing test-program generation and Bayesian diversity selection,
LSC-Fuzz generates diverse and complex HDL code, thoroughly testing the FPGA
logic synthesis compilers using equivalent check to detect bugs. Through three
months, LSC-Fuzz has found 16 bugs, 12 of these has been confirmed by official
technical support.

</details>


### [15] [DocFetch - Towards Generating Software Documentation from Multiple Software Artifacts](https://arxiv.org/abs/2508.17719)
*Akhila Sri Manasa Venigalla,Sridhar Chimalakonda*

Main category: cs.SE

TL;DR: DocFetch uses a prompt-based LLM to generate accurate software documentation from diverse artifacts, achieving strong BLEU and ROUGE scores, and helps maintain documentation with less manual effort.


<details>
  <summary>Details</summary>
Motivation: Maintaining software documentation is increasingly difficult due to rapid project development, especially in open-source projects. Existing automated documentation methods focus mainly on source code, missing out on valuable information scattered in other artifacts.

Method: The authors propose DocFetch, a system using a multi-layer prompt-based large language model (LLM) to generate structured documentation from multiple software artifacts. The DocMine dataset was used to consolidate data, and DocFetch's outputs were evaluated with BLEU-4 and ROUGE-L scores against a manually curated ground truth.

Result: DocFetch achieved a highest BLEU-4 score of 43.24% and a ROUGE-L score of 0.39 in generating API-related and file-related documentation information. Other documentation types also reached BLEU-4 scores close to 30%, demonstrating good overall performance.

Conclusion: DocFetch effectively semi-automates the generation of various documentation types by leveraging information from multiple artifacts, easing documentation maintenance and improving project comprehension with minimal effort.

Abstract: Software Documentation plays a major role in the usage and development of a
project. Widespread adoption of open source software projects contributes to
larger and faster development of the projects, making it difficult to maintain
the associated documentation. Existing automated approaches to generate
documentation largely focus on source code. However, information useful for
documentation is observed to be scattered across various artifacts that
co-evolve with the source code. Leveraging this information across multiple
artifacts can reduce the effort involved in maintaining documentation. Hence,
we propose DocFetch, to generate different types of documentation from multiple
software artifacts. We employ a multi-layer prompt based LLM and generate
structured documentation corresponding to different documentation types for the
data consolidated in DocMine dataset. We evaluate the performance of DocFetch
using a manually curated groundtruth dataset by analysing the artifacts in
DocMine. The evaluation yields a highest BLEU-4 score of 43.24% and ROUGE-L
score of 0.39 for generation of api-related and file-related information from
five documentation sources. The generation of other documentation type related
information also reported BLEU-4 scores close to 30% indicating good
performance of the approach. Thus,DocFetch can be employed to
semi-automatically generate documentation, and helps in comprehending the
projects with minimal effort in maintaining the documentation.

</details>


### [16] [RepoTransAgent: Multi-Agent LLM Framework for Repository-Aware Code Translation](https://arxiv.org/abs/2508.17720)
*Ziqi Guan,Xin Yin,Zhiyuan Peng,Chao Ni*

Main category: cs.SE

TL;DR: RepoTransAgent introduces a multi-agent LLM framework that systematically improves repository-aware code translation by handling context, prompting, and error refinement. It significantly beats existing methods on Java-C# tasks, showing strong generalizability and robustness.


<details>
  <summary>Details</summary>
Motivation: Current code translation tools using large language models (LLMs) struggle with repository-level translation due to lack of contextual understanding, rigid prompt construction, and poor error correction. These issues limit the effectiveness of translating complex, real-world code.

Method: RepoTransAgent is proposed, a multi-agent LLM framework that divides the code translation process into three specialized subtasks: (1) context retrieval with retrieval-augmented generation, (2) dynamic and adaptive prompt construction, and (3) iterative code refinement with reflection-based error correction. Each subtask is handled by a dedicated agent.

Result: RepoTransAgent was evaluated on hundreds of Java-C# translation pairs from six open-source projects and showed significant improvement over existing methods, achieving up to 55.34% compile rate and 45.84% pass rate.

Conclusion: RepoTransAgent is effective, robust, and generalizable for repository-aware code translation, outperforming current state-of-the-art methods on real-world data.

Abstract: Repository-aware code translation is critical for modernizing legacy systems,
enhancing maintainability, and enabling interoperability across diverse
programming languages. While recent advances in large language models (LLMs)
have improved code translation quality, existing approaches face significant
challenges in practical scenarios: insufficient contextual understanding,
inflexible prompt designs, and inadequate error correction mechanisms. These
limitations severely hinder accurate and efficient translation of complex,
real-world code repositories. To address these challenges, we propose
RepoTransAgent, a novel multi-agent LLM framework for repository-aware code
translation. RepoTransAgent systematically decomposes the translation process
into specialized subtasks-context retrieval, dynamic prompt construction, and
iterative code refinement-each handled by dedicated agents. Our approach
leverages retrieval-augmented generation (RAG) for contextual information
gathering, employs adaptive prompts tailored to varying repository scenarios,
and introduces a reflection-based mechanism for systematic error correction. We
evaluate RepoTransAgent on hundreds of Java-C# translation pairs from six
popular open-source projects. Experimental results demonstrate that
RepoTransAgent significantly outperforms state-of-the-art baselines in both
compile and pass rates. Specifically, RepoTransAgent achieves up to 55.34%
compile rate and 45.84% pass rate. Comprehensive analysis confirms the
robustness and generalizability of RepoTransAgent across different LLMs,
establishing its effectiveness for real-world repository-aware code
translation.

</details>


### [17] [Logging Requirement for Continuous Auditing of Responsible Machine Learning-based Applications](https://arxiv.org/abs/2508.17851)
*Patrick Loic Foalem,Leuson Da Silva,Foutse Khomh,Heng Li,Ettore Merlo*

Main category: cs.SE

TL;DR: The paper reviews logging as a tool for auditing machine learning systems, finds major gaps, and recommends integrating responsible AI metrics into logging practices to enhance transparency, accountability, and compliance.


<details>
  <summary>Details</summary>
Motivation: Machine learning is being used more widely in decision-making systems, but there are persistent concerns about ethical and legal compliance due to lack of transparency, fairness, and accountability. Traditional software often uses logging for auditing, which could be applied to ML systems.

Method: The study explores the use of logging practices as a way to systematically audit ML systems for compliance and accountability. It analyzes current methods, identifying deficiencies and opportunities, and emphasizes the integration of responsible AI metrics into logging.

Result: The paper finds significant shortcomings in current logging practices and highlights the need for better tools and standards. Enhanced logging that includes responsible AI metrics would support transparency, accountability, and help meet regulatory and societal demands.

Conclusion: The work provides actionable recommendations for practitioners and tool developers on improving logging in ML systems to achieve better auditability and responsible AI, thereby increasing accountability and trustworthiness.

Abstract: Machine learning (ML) is increasingly applied across industries to automate
decision-making, but concerns about ethical and legal compliance remain due to
limited transparency, fairness, and accountability. Monitoring through logging
a long-standing practice in traditional software offers a potential means for
auditing ML applications, as logs provide traceable records of system behavior
useful for debugging, performance analysis, and continuous auditing.
systematically auditing models for compliance or accountability. The findings
underscore the need for enhanced logging practices and tooling that
systematically integrate responsible AI metrics. Such practices would support
the development of auditable, transparent, and ethically responsible ML
systems, aligning with growing regulatory requirements and societal
expectations. By highlighting specific deficiencies and opportunities, this
work provides actionable guidance for both practitioners and tool developers
seeking to strengthen the accountability and trustworthiness of ML
applications.

</details>


### [18] [modelSolver: A Symbolic Model-Driven Solver for Power Network Simulation and Monitoring](https://arxiv.org/abs/2508.17882)
*Izudin Dzafic,Rabih A. Jabr*

Main category: cs.SE

TL;DR: modelSolver introduces a programming-free framework for power system analysis, allowing easy and intuitive model creation and modification with mathematical expressions, making advanced analysis accessible to non-programmer domain experts.


<details>
  <summary>Details</summary>
Motivation: Developing advanced power system analysis tools currently demands significant programming skills, making customization and model definition challenging for domain experts who lack coding experience.

Method: The paper introduces 'modelSolver,' a framework based on symbolic mathematical modeling, allowing users to define models through mathematical expressions rather than traditional programming constructs. It supports power flow and state estimation, flexible model specification, and automatic data conversion for compatibility with MATPOWER.

Result: modelSolver enables intuitive custom model creation for power systems, supports advanced functions like voltage regulators and state estimation, and removes programming barriers for users. It is accessible for students and professionals and simplifies the workflow of power system analysis.

Conclusion: modelSolver democratizes power system modeling by allowing domain experts to create and modify advanced power system analysis models without needing programming skills, thereby making these analyses more accessible and efficient.

Abstract: The development of advanced software tools for power system analysis requires
extensive programming expertise. Even when using open-source tools, programming
skills are essential to modify built-in models. This can be particularly
challenging for domain experts who lack coding proficiency. This paper
introduces modelSolver, a software solution with a new framework centered
around symbolic mathematical modeling. The proposed paradigm facilitates
defining models through intuitive mathematical expressions, thus eliminating
the need for traditional programming constructs such as arrays, loops, and
sparse matrix computations. The modelSolver focuses on power flow and state
estimation using an open-box approach, which allows users to specify custom
models using either real or complex variables. Unlike existing tools that rely
on hard-coded models, modelSolver enables the representation of a wide range of
advanced functionalities, including power flow with voltage regulators and load
tap changers, continuation power flow, and Gauss-Newton state estimation with
equality constraints. Compatibility with MATPOWER is ensured via a converter
that automates importing data files. The framework prioritizes model-driven
development and empowers domain experts to focus on power system modeling
without programming barriers. It aims to simplify power system computations,
making them more accessible to students, scientists, and practitioners.

</details>


### [19] [A Defect Classification Framework for AI-Based Software Systems (AI-ODC)](https://arxiv.org/abs/2508.17900)
*Mohammed O. Alannsary*

Main category: cs.SE

TL;DR: AI defect analysis models don't fit unique AI challenges. This paper introduces a modified ODC approach (AIODC), focusing on Data, Learning, and Thinking defects. Applied to ML bugs, Learning phase defects are most common/severe; Thinking defects harm trust/accuracy. AIODC helps identify high-risk issues and improve AI quality assurance.


<details>
  <summary>Details</summary>
Motivation: Current defect analysis models are inadequate for AI systems because they do not account for their unique characteristics. With AI systems proliferating, it is essential to have models that can effectively classify and analyze defects specific to AI.

Method: The paper proposes a framework based on Orthogonal Defect Classification (ODC), modified for AI systems by introducing new classification dimensions (Data, Learning, Thinking), an added attribute, a new severity level, and replacing impact areas with those relevant to AI. It applies this framework to a public Machine Learning bug dataset and analyzes the results statistically.

Result: Defects in the Learning phase were the most common and most associated with high-severity ratings. Defects in the Thinking phase had a strong impact on trustworthiness and accuracy. The framework (AIODC) could effectively pinpoint high-risk defect categories.

Conclusion: Modifying ODC for AI systems is feasible and effective. The AIODC framework provides valuable insights into defect categorization and can guide targeted quality assurance in AI system development.

Abstract: Artificial Intelligence has gained a lot of attention recently, it has been
utilized in several fields ranging from daily life activities, such as
responding to emails and scheduling appointments, to manufacturing and
automating work activities. Artificial Intelligence systems are mainly
implemented as software solutions, and it is essential to discover and remove
software defects to assure its quality using defect analysis which is one of
the major activities that contribute to software quality. Despite the
proliferation of AI-based systems, current defect analysis models fail to
capture their unique attributes. This paper proposes a framework inspired by
the Orthogonal Defect Classification (ODC) paradigm and enables defect analysis
of Artificial Intelligence systems while recognizing its special attributes and
characteristics. This study demonstrated the feasibility of modifying ODC for
AI systems to classify its defects. The ODC was adjusted to accommodate the
Data, Learning, and Thinking aspects of AI systems which are newly introduced
classification dimensions. This adjustment involved the introduction of an
additional attribute to the ODC attributes, the incorporation of a new severity
level, and the substitution of impact areas with characteristics pertinent to
AI systems. The framework was showcased by applying it to a publicly available
Machine Learning bug dataset, with results analyzed through one-way and two-way
analysis. The case study indicated that defects occurring during the Learning
phase were the most prevalent and were significantly linked to high-severity
classifications. In contrast, defects identified in the Thinking phase had a
disproportionate effect on trustworthiness and accuracy. These findings
illustrate AIODC's capability to identify high-risk defect categories and
inform focused quality assurance measures.

</details>


### [20] [Evaluating Citizen Satisfaction with Saudi Arabia's E-Government Services: A Standards-Based, Theory-Informed Approach](https://arxiv.org/abs/2508.17912)
*Mohammed O. Alannsary*

Main category: cs.SE

TL;DR: Saudi citizens report high satisfaction with e-government usability and trust, but service clarity and emotional engagement need improvement. The findings can guide policymakers in refining platforms, and contribute to theory by integrating technical standards with adoption models.


<details>
  <summary>Details</summary>
Motivation: With increasing reliance on digital government platforms for public service delivery, it is critical to understand how citizens assess these services to improve usability, trust, and inclusiveness.

Method: The study utilizes a quality-in-use framework based on ISO/IEC 25010 and ISO/IEC 25022 standards, analyzed using the Unified Theory of Acceptance and Use of Technology (UTAUT). Data were collected via a structured questionnaire administered to 500 citizens, yielding 276 valid responses.

Result: The study found high levels of citizen satisfaction in terms of usability and trust. Despite Saudi Arabia's strong global e-government rankings, issues remain in service clarity and system responsiveness. Emotional engagement was low, suggesting users view the platforms as practical tools rather than engaging experiences.

Conclusion: Citizen satisfaction with Saudi e-government services is high, particularly for usability and trust. However, service clarity and responsiveness still need improvement, and digital platforms are seen as functional rather than emotionally engaging. The research provides important insights for policymakers and advances theoretical integration of standards-based and behavioral adoption models.

Abstract: As digital government platforms become central to public service delivery,
understanding citizen assessment is crucial for enhancing usability, trust, and
inclusivity. This study investigates citizen satisfaction with the e-government
services in Saudi Arabia through a quality-in-use framework based on ISO/IEC
25010 and ISO/IEC 25022 standards, interpreted through the lens of the Unified
Theory of Acceptance and Use of Technology (UTAUT). A structured questionnaire
was administered to 500 citizens, yielding 276 valid responses. Satisfaction
was evaluated across four dimensions: overall satisfaction, feature
satisfaction, trust, and emotional engagement (pleasure). The findings
demonstrate consistently high levels of satisfaction regarding usability and
trust, aligning with Saudi Arabia's top-tier global ranking in e-government
development. However, the results also highlight persistent challenges related
to service clarity and system responsiveness. Emotional engagement was limited,
indicating that users perceive these services primarily as functional tools
rather than as engaging digital experiences. The study offers valuable insights
for policymakers and contributes to the theoretical integration of
standards-based and behavioral adoption models in the context of citizenship.

</details>


### [21] [DesCartes Builder: A Tool to Develop Machine-Learning Based Digital Twins](https://arxiv.org/abs/2508.17988)
*Eduardo de Conto,Blaise Genest,Arvind Easwaran,Nicholas Ng,Shweta Menon*

Main category: cs.SE

TL;DR: Digital twin engineering in civil engineering needs structured ML pipelines. DesCartes Builder is an open-source tool that visually and modularly streamlines ML-based DT design. Its usefulness is demonstrated for predicting plastic strain in structural systems.


<details>
  <summary>Details</summary>
Motivation: Digital twins are widely used in civil engineering but lack structured, systematic engineering practices when leveraging machine learning. Typical ML approaches, which train a single model for a task, do not align with the multi-model, multi-domain requirements of DTs.

Method: The authors introduce DesCartes Builder, an open-source tool that employs a flexible visual data flow paradigm along with a library of parameterizable core operations and tailored ML algorithms for building and managing ML-based digital twin pipelines. The tool enables specification, composition, and reuse of models for real-time DT prototypes and instances.

Result: DesCartes Builder's effectiveness and usability are demonstrated via a civil engineering use case, specifically in creating a real-time DT prototype that accurately predicts plastic strain in structures.

Conclusion: A systematic, visual, and modular ML engineering approach is necessary for the robust development of digital twins. DesCartes Builder offers a practical solution, improving structure, reusability, and real-time efficiency in DT creation.

Abstract: Digital twins (DTs) are increasingly utilized to monitor, manage, and
optimize complex systems across various domains, including civil engineering. A
core requirement for an effective DT is to act as a fast, accurate, and
maintainable surrogate of its physical counterpart, the physical twin (PT). To
this end, machine learning (ML) is frequently employed to (i) construct
real-time DT prototypes using efficient reduced-order models (ROMs) derived
from high-fidelity simulations of the PT's nominal behavior, and (ii)
specialize these prototypes into DT instances by leveraging historical sensor
data from the target PT. Despite the broad applicability of ML, its use in DT
engineering remains largely ad hoc. Indeed, while conventional ML pipelines
often train a single model for a specific task, DTs typically require multiple,
task- and domain-dependent models. Thus, a more structured approach is required
to design DTs.
  In this paper, we introduce DesCartes Builder, an open-source tool to enable
the systematic engineering of ML-based pipelines for real-time DT prototypes
and DT instances. The tool leverages an open and flexible visual data flow
paradigm to facilitate the specification, composition, and reuse of ML models.
It also integrates a library of parameterizable core operations and ML
algorithms tailored for DT design. We demonstrate the effectiveness and
usability of DesCartes Builder through a civil engineering use case involving
the design of a real-time DT prototype to predict the plastic strain of a
structure.

</details>


### [22] [Previously on... Automating Code Review](https://arxiv.org/abs/2508.18003)
*Robert Heumüller,Frank Ortmeier*

Main category: cs.SE

TL;DR: A systematic review of ML/DL-based Modern Code Review automation shows widespread inconsistencies in tasks, metrics, and datasets. The authors call for deeper standardization, clearer evaluation, and provide guidance for future research.


<details>
  <summary>Details</summary>
Motivation: Modern Code Review (MCR), essential in software engineering, requires significant time and resources. There is growing interest in automating these tasks via ML/DL, but inconsistencies exist in how tasks are defined, evaluated, and benchmarked. Thus, the field needs comprehensive analysis and recommendations.

Method: A systematic survey of 691 publications, narrowing to 24 relevant studies on MCR automation published between May 2015 and April 2024. Each study was evaluated based on tasks, models, metrics, baselines, results, validity issues, and availability of artifacts.

Result: The analysis found significant variability and lack of standardization in task definitions and metrics (with 48 task-metric combinations, 22 unique to a paper). Dataset reuse is limited. Methodological challenges, like temporal bias, are under-addressed. Recommendations for improved future research are provided.

Conclusion: This study clarifies MCR automation research, highlights pitfalls, and calls for better standardization and rigorous evaluation, supporting future advances in the field.

Abstract: Modern Code Review (MCR) is a standard practice in software engineering, yet
it demands substantial time and resource investments. Recent research has
increasingly explored automating core review tasks using machine learning (ML)
and deep learning (DL). As a result, there is substantial variability in task
definitions, datasets, and evaluation procedures. This study provides the first
comprehensive analysis of MCR automation research, aiming to characterize the
field's evolution, formalize learning tasks, highlight methodological
challenges, and offer actionable recommendations to guide future research.
Focusing on the primary code review tasks, we systematically surveyed 691
publications and identified 24 relevant studies published between May 2015 and
April 2024. Each study was analyzed in terms of tasks, models, metrics,
baselines, results, validity concerns, and artifact availability. In
particular, our analysis reveals significant potential for standardization,
including 48 task metric combinations, 22 of which were unique to their
original paper, and limited dataset reuse. We highlight challenges and derive
concrete recommendations for examples such as the temporal bias threat, which
are rarely addressed so far. Our work contributes to a clearer overview of the
field, supports the framing of new research, helps to avoid pitfalls, and
promotes greater standardization in evaluation practices.

</details>


### [23] [A Large-Scale Study on Developer Engagement and Expertise in Configurable Software System Projects](https://arxiv.org/abs/2508.18070)
*Karolina M. Milano,Wesley K. G. Assunção,Bruno B. P. Cafeo*

Main category: cs.SE

TL;DR: The paper finds that a small group of developers handle most of the variable code in configurable software systems. Current expertise metrics do not accurately identify who these specialists are, pointing to a need for better metrics to improve workload distribution and task assignment.


<details>
  <summary>Details</summary>
Motivation: Variability in configurable software systems (CSSs) is essential, but its implementation using pre-processor directives complicates maintenance and increases error risk. There is limited understanding of how variable code is distributed among developers and whether current expertise metrics can identify proficiency in handling this type of code.

Method: The authors mined repositories of 25 CSS projects, analyzing 450,255 commits from 9,678 developers. They evaluated developers' involvement with variable vs. mandatory code, measured workload concentration, and assessed the accuracy of conventional expertise metrics in this context.

Result: The results indicate that 59% of developers never changed variable code, while 17% handled 83% of it, highlighting a concentration of expertise. Conventional expertise metrics demonstrated only around 55% precision and 50% recall in identifying developers engaged with variable code.

Conclusion: Variable code maintenance is concentrated among a small subset of developers, and existing expertise metrics are insufficient for distinguishing proficiency in variable code. More refined metrics are needed to better support task assignment and distribute workload more equitably in CSS projects.

Abstract: Modern systems operate in multiple contexts making variability a fundamental
aspect of Configurable Software Systems (CSSs). Variability, implemented via
pre-processor directives (e.g., #ifdef blocks) interleaved with other code and
spread across files, complicates maintenance and increases error risk. Despite
its importance, little is known about how variable code is distributed among
developers or whether conventional expertise metrics adequately capture
variable code proficiency. This study investigates developers' engagement with
variable versus mandatory code, the concentration of variable code workload,
and the effectiveness of expertise metrics in CSS projects. We mined
repositories of 25 CSS projects, analyzing 450,255 commits from 9,678
developers. Results show that 59% of developers never modified variable code,
while about 17% were responsible for developing and maintaining 83% of it. This
indicates a high concentration of variable code expertise among a few
developers, suggesting that task assignments should prioritize these
specialists. Moreover, conventional expertise metrics performed
poorly--achieving only around 55% precision and 50% recall in identifying
developers engaged with variable code. Our findings highlight an unbalanced
distribution of variable code responsibilities and underscore the need to
refine expertise metrics to better support task assignments in CSS projects,
thereby promoting a more equitable workload distribution.

</details>


### [24] [Debian in the Research Software Ecosystem: A Bibliometric Analysis](https://arxiv.org/abs/2508.18073)
*Joenio Marques da Costa,Christina von Flach*

Main category: cs.SE

TL;DR: The paper presents a bibliometric analysis of academic publications mentioning Debian, mapping key research trends, influential works, demographics, and areas needing further attention in Debian-related science.


<details>
  <summary>Details</summary>
Motivation: Debian has played a significant role in scientific and academic projects, but there has not been a comprehensive study mapping this involvement through academic publications. The authors seek to systematically chart and analyze the academic landscape around Debian.

Method: The study conducts a bibliometric analysis based on publications found by searching for 'Debian' in titles, abstracts, or keywords in the Scopus database. The analysis calculates metrics such as co-citation, co-authorship, and word co-occurrence, underpinned by predefined research questions and inclusion/exclusion criteria.

Result: A categorized collection of articles from various scientific fields referring to Debian was compiled. The study presents a mapped overview of the relevant academic literature and demographic/bibliometric trends (e.g., influential articles, key researchers and countries, and leading conferences), with data made publicly available.

Conclusion: This bibliometric and demographic analysis reveals the intellectual structure of Debian-related academic research, provides insights into publication trends, and highlights opportunities and underexplored areas for future research.

Abstract: Context: The Debian system has historically participated in academic works
and scientific projects, with well-known examples including NeuroDebian, Debian
Med, Debsources, Debian Science, and Debian GIS, where the scientific relevance
of Debian and its contribution to the Research Software ecosystem are evident.
  Objective: The objective of this study is to investigate the Debian system
through academic publications, with the aim of classifying articles, mapping
research, identifying trends, and finding opportunities.
  Method: The study is based on a bibliometric analysis starting with an
initial search for the term "Debian" in the titles, abstracts, or keywords of
academic publications, using the Scopus database. This analysis calculates
metrics of co-citation, co-authorship, and word co-occurrence, and is guided by
a set of research questions and criteria for inclusion and exclusion to conduct
the bibliometric analysis.
  Results: The study includes a set of articles published across various fields
of knowledge, providing a map of the academic publication space about Debian.
The study's data will be available in a public repository, reporting
demographic and bibliometric trends, including the most cited articles, active
countries, researchers, and popular conferences.
  Conclusion: Results includes a bibliometric and demographic analysis
identified in publications about Debian, shedding light on the intellectual
structure of academic research. The results of the analyses can help
researchers gain an overview of existing trends in publications about Debian
and identify areas that require more attention from the scientific community.

</details>


### [25] [LLM-Guided Genetic Improvement: Envisioning Semantic Aware Automated Software Evolution](https://arxiv.org/abs/2508.18089)
*Karine Even-Mendoza,Alexander Brownlee,Alina Geiger,Carol Hanna,Justyna Petke,Federica Sarro,Dominik Sobania*

Main category: cs.SE

TL;DR: The paper introduces PatchCat, which uses automated clustering to organize and assess LLM-generated software edits in Genetic Improvement. PatchCat improves interpretability and efficiency, identifies patch types, detects ineffective edits early, and saves computational resources. This marks an important step toward goal-driven, semantic-aware software improvement using local LLMs.


<details>
  <summary>Details</summary>
Motivation: Traditional Genetic Improvement (GI) of software is effective for optimizing performance properties but works primarily at the syntactic level. GI lacks the semantic-aware editing capabilities of Large Language Models (LLMs), which, however, are not goal-directed or easily steered for specific improvements.

Method: The paper augments GI by incorporating automated clustering of software patches proposed by LLMs. This approach, called PatchCat, categorizes LLM-suggested patches using clustering techniques and assesses their effectiveness.

Result: PatchCat was able to identify 18 distinct types of LLM-generated software patches and categorize newly suggested patches with high accuracy. It can detect 'NoOp' edits in advance, allowing for the possibility of skipping unnecessary test suite executions, thereby saving resources. It also works with small, local LLMs.

Conclusion: The integration of semantic clustering of LLM edits into GI is a promising step toward more interpretable, efficient, and resource-friendly software improvement. PatchCat demonstrates initial success and opens up opportunities for deeper, principled understanding and direction of LLM-driven GI processes.

Abstract: Genetic Improvement (GI) of software automatically creates alternative
software versions that are improved according to certain properties of
interests (e.g., running-time). Search-based GI excels at navigating large
program spaces, but operates primarily at the syntactic level. In contrast,
Large Language Models (LLMs) offer semantic-aware edits, yet lack goal-directed
feedback and control (which is instead a strength of GI). As such, we propose
the investigation of a new research line on AI-powered GI aimed at
incorporating semantic aware search. We take a first step at it by augmenting
GI with the use of automated clustering of LLM edits. We provide initial
empirical evidence that our proposal, dubbed PatchCat, allows us to
automatically and effectively categorize LLM-suggested patches. PatchCat
identified 18 different types of software patches and categorized newly
suggested patches with high accuracy. It also enabled detecting NoOp edits in
advance and, prospectively, to skip test suite execution to save resources in
many cases. These results, coupled with the fact that PatchCat works with
small, local LLMs, are a promising step toward interpretable, efficient, and
green GI. We outline a rich agenda of future work and call for the community to
join our vision of building a principled understanding of LLM-driven mutations,
guiding the GI search process with semantic signals.

</details>


### [26] [A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code](https://arxiv.org/abs/2508.18106)
*Keke Lian,Bin Wang,Lei Zhang,Libo Chen,Junjie Wang,Ziming Zhao,Yujiu Yang,Haotong Duan,Haoran Zhao,Shuang Liao,Mingda Guo,Jiazheng Quan,Yilu Zhong,Chenhao He,Zichuan Chen,Jie Wu,Haoling Li,Zhaoxuan Li,Jiongchi Yu,Hui Li,Dong Zhang*

Main category: cs.SE

TL;DR: A.S.E is a new, realistic benchmark for evaluating LLM-generated code security at the repository level. It reveals strong performance by Claude-3.7-Sonnet and top security scores by Qwen3-235B-A22B-Instruct, with concise decoding strategies outperforming complex reasoning for patching security flaws.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for evaluating the security of code generated by large language models (LLMs) are inadequate. They focus only on isolated code snippets, use unstable evaluation methods that lack reproducibility, and do not consider how the quality of input context affects the security of the output code.

Method: The authors propose A.S.E (AI Code Generation Security Evaluation), a benchmark that uses real-world repositories with known CVEs. It preserves complete repository context (including build systems and dependencies) and employs a reproducible, containerized framework with expert-defined rules for stable and auditable assessment of security, build quality, and generation stability.

Result: Testing leading LLMs on A.S.E led to three major findings: (1) Claude-3.7-Sonnet provided the best overall performance; (2) The security differences between proprietary and open-source models were small, with Qwen3-235B-A22B-Instruct achieving the highest security score; (3) Simple, "fast-thinking" decoding strategies outperformed more complex reasoning methods in security patching.

Conclusion: A.S.E offers a more realistic and robust benchmark for assessing the security of LLM-generated code at the repository level, revealing gaps and differences among current LLMs and highlighting the strengths of concise decoding strategies.

Abstract: The increasing adoption of large language models (LLMs) in software
engineering necessitates rigorous security evaluation of their generated code.
However, existing benchmarks are inadequate, as they focus on isolated code
snippets, employ unstable evaluation methods that lack reproducibility, and
fail to connect the quality of input context with the security of the output.
To address these gaps, we introduce A.S.E (AI Code Generation Security
Evaluation), a benchmark for repository-level secure code generation. A.S.E
constructs tasks from real-world repositories with documented CVEs, preserving
full repository context like build systems and cross-file dependencies. Its
reproducible, containerized evaluation framework uses expert-defined rules to
provide stable, auditable assessments of security, build quality, and
generation stability. Our evaluation of leading LLMs on A.S.E reveals three key
findings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The
security gap between proprietary and open-source models is narrow;
Qwen3-235B-A22B-Instruct attains the top security score. (3) Concise,
``fast-thinking'' decoding strategies consistently outperform complex,
``slow-thinking'' reasoning for security patching.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [27] [SafeTree: Expressive Tree Policies for Microservices](https://arxiv.org/abs/2508.16746)
*Karuna Grewal,P. Brighten Godfrey,Justin Hsu*

Main category: cs.PL

TL;DR: The paper introduces a new policy language and enforcement mechanism for microservices, allowing fine-grained, service tree-aware control of inter-service communication. Their solution, built on Istio, requires no code changes and adds only minimal latency, enhancing security beyond current deployment tools.


<details>
  <summary>Details</summary>
Motivation: Current microservice deployment tools only allow for simple, single-hop communication policies, which can be overly permissive and do not take into account the complex hierarchical structure of microservice calls. This lack restricts security and control over inter-service communications.

Method: The authors design a novel policy language that can express service tree structures and develop a dynamic enforcement mechanism based on visibly pushdown automata. They implement a runtime monitor on top of Istio, a popular service mesh, enabling programmable, distributed online enforcement of policies without requiring changes to the microservices themselves.

Result: The implemented monitor is able to enforce sophisticated safety properties reflecting service tree communication patterns, while incurring only minimal latency overhead, measured in milliseconds.

Conclusion: The approach offers an expressive, practical, and minimally invasive solution for fine-grained control over microservice communication, improving security and flexibility without significant performance penalties.

Abstract: A microservice-based application is composed of multiple self-contained
components called microservices, and controlling inter-service communication is
important for enforcing safety properties. Presently, inter-service
communication is configured using microservice deployment tools. However, such
tools only support a limited class of single-hop policies, which can be overly
permissive because they ignore the rich service tree structure of microservice
calls. Policies that can express the service tree structure can offer
development and security teams more fine-grained control over communication
patterns.
  To this end, we design an expressive policy language to specify service tree
structures, and we develop a visibly pushdown automata-based dynamic
enforcement mechanism to enforce service tree policies. Our technique is
non-invasive: it does not require any changes to service implementations, and
does not require access to microservice code. To realize our method, we build a
runtime monitor on top of a service mesh, an emerging network infrastructure
layer that can control inter-service communication during deployment. In
particular, we employ the programmable network traffic filtering capabilities
of Istio, a popular service mesh implementation, to implement an online and
distributed monitor. Our experiments show that our monitor can enforce rich
safety properties while adding minimal latency overhead on the order of
milliseconds.

</details>


### [28] [Syntactic Completions with Material Obligations](https://arxiv.org/abs/2508.16848)
*David Moon,Andrew Blinn,Thomas J. Porter,Cyrus Omar*

Main category: cs.PL

TL;DR: The paper proposes tylr, a novel code editor and parser that helps users deal with syntax errors by representing them as 'obligations,' improving error correction and offering a user-friendly editing experience, with positive results shown in user studies.


<details>
  <summary>Details</summary>
Motivation: Code editors often struggle to provide helpful services—like navigation and code understanding—when the user's code contains syntax errors. Existing error recovery strategies either lose significant amounts of code or result in too many repair options, making them impractical for effective developer support.

Method: The authors introduce 'tylr,' a parser and editor generator using a new tile-based parsing theory. This approach generalizes the notion of 'holes' (places in code that need fixing) to 'obligations' that can represent a greater variety of syntax gaps. The parsing strategy extends operator-precedence parsing with 'grammar walks' and a grammar zipper-based molding system. tylr visually presents these obligations to the user within the editor.

Result: The paper presents tylr through examples, formally describes the underlying mechanisms, and evaluates it via a human subjects study. The results show that visualizing syntactic obligations can be both usable and useful for developers, highlighting positive user feedback along with areas for further research.

Conclusion: tylr provides a new approach to real-time syntax error recovery and code editing by representing syntax issues as obligations directly in the editor UI, blending features of text and structure editors. The study indicates that this method can assist developers effectively when dealing with malformed code.

Abstract: Code editors provide essential services that help developers understand,
navigate, and modify programs. However, these services often fail in the
presence of syntax errors. Existing syntax error recovery techniques, like
panic mode and multi-option repairs, are either too coarse, e.g. in deleting
large swathes of code, or lead to a proliferation of possible completions. This
paper introduces $\texttt{tylr}$, a parser and editor generator that completes
arbitrarily malformed code by inserting obligations, which generalize holes to
cover missing operands, operators, mixfix keywords, and sort transitions.
$\texttt{tylr}$ is backed by a novel theory of tile-based parsing, which
extends operator-precedence parsing in two ways. First, traditional token
precedence comparisons are replaced by a notion of grammar walks, which form
the basis for generating obligations. Second, a distinct "molding" system based
on grammar zippers expand grammar expressivity by allowing the system to
disambiguate between possible parses and completions based on an obligation
minimization criterion. In addition to serving as a novel approach to error
correction, $\texttt{tylr}$'s design enables the development of an editor that
visually materializes obligations to the human user, serving as a novel hybrid
between a text editor and a structure editor. We introduce $\texttt{tylr}$ by
example, then formalize its key ideas. Finally, we conduct a human subjects
study to evaluate the extent to which an editor like $\texttt{tylr}$ that
materializes syntactic obligations might be usable and useful, finding both
points of positivity and interesting new avenues for future work.

</details>
