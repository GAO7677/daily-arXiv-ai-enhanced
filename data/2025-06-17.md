<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 22]
- [cs.PL](#cs.PL) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [The CAISAR Platform: Extending the Reach of Machine Learning Specification and Verification](https://arxiv.org/abs/2506.12084)
*Michele Alberti,François Bobot,Julien Girard-Satabin,Alban Grastien,Aymeric Varasse,Zakaria Chihani*

Main category: cs.SE

TL;DR: CAISAR is a new open-source platform that allows the formal specification and verification of complex properties in machine learning models (including neural networks, SVMs, and boosted trees) by automatically translating specifications into queries for existing verification tools, thus overcoming current limitations in expressiveness and tool fragmentation.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the fragmentation in the field of formal specification and verification of machine learning programs, where a variety of tools have emerged that are mostly limited to verifying simple local robustness properties. Existing tools are difficult to compare and cannot handle more complex properties, especially those involving multiple neural networks or various model types.

Method: The authors present CAISAR, an open-source platform that introduces a new specification language capable of modeling complex properties in machine learning systems, such as those involving neural networks, support vector machines, and boosted trees. The platform automatically translates these specifications into queries compatible with existing state-of-the-art verification tools using automated graph editing.

Result: The paper demonstrates, through concrete use-cases, that CAISAR can effectively model and specify complex properties, and successfully leverage existing provers by automatically translating its specifications, thus enabling the use of unmodified, off-the-shelf verification tools.

Conclusion: CAISAR streamlines the specification and verification process for complex machine learning systems, bridging the gap caused by fragmented and narrowly focused tools. It enables automatic, tool-agnostic verification of more sophisticated properties across various machine learning models.

Abstract: The formal specification and verification of machine learning programs saw
remarkable progress in less than a decade, leading to a profusion of tools.
However, diversity may lead to fragmentation, resulting in tools that are
difficult to compare, except for very specific benchmarks. Furthermore, this
progress is heavily geared towards the specification and verification of a
certain class of property, that is, local robustness properties. But while
provers are becoming more and more efficient at solving local robustness
properties, even slightly more complex properties, involving multiple neural
networks for example, cannot be expressed in the input languages of winners of
the International Competition of Verification of Neural Networks VNN-Comp. In
this tool paper, we present CAISAR, an open-source platform dedicated to
machine learning specification and verification. We present its specification
language, suitable for modelling complex properties on neural networks, support
vector machines and boosted trees. We show on concrete use-cases how
specifications written in this language are automatically translated to queries
to state-of-the-art provers, notably by using automated graph editing
techniques, making it possible to use their off-the-shelf versions. The
artifact to reproduce the paper claims is available at the following DOI:
https://doi.org/10.5281/zenodo.15209510

</details>


### [2] [Quantum-Inspired Differentiable Integral Neural Networks (QIDINNs): A Feynman-Based Architecture for Continuous Learning Over Streaming Data](https://arxiv.org/abs/2506.12111)
*Oscar Boullosa Dapena*

Main category: cs.SE

TL;DR: The paper proposes QIDINNs—a new neural architecture using integral-based learning inspired by quantum mechanics—to achieve stable, efficient, and interpretable real-time learning on streaming data, showing promising results and paving the way for quantum-classical hybrid models.


<details>
  <summary>Details</summary>
Motivation: Real-time continuous learning on streaming, temporally-unbounded data is limited by the computational cost and stability challenges of traditional gradient-based models like BPTT.

Method: The paper introduces Quantum-Inspired Differentiable Integral Neural Networks (QIDINNs), which use differentiation under the integral sign (a Feynman technique) to express neural updates as integrals over historical data, enabling smoother and more stable updates. This approach is inspired by Feynman's path integral, making it compatible with quantum gradient estimation and allowing hybrid classical-quantum computation.

Result: The QIDINN model is evaluated on both synthetic and real-world streaming tasks, demonstrating improved stability and effectiveness over previous methods. Potential for quantum extensions and scalable implementations is discussed.

Conclusion: QIDINNs provide a computationally efficient, stable, and physically interpretable framework for continuous learning on streaming data, with promising avenues for quantum integration and broader scalability.

Abstract: Real-time continuous learning over streaming data remains a central challenge
in deep learning and AI systems. Traditional gradient-based models such as
backpropagation through time (BPTT) face computational and stability
limitations when dealing with temporally unbounded data. In this paper, we
introduce a novel architecture, Quantum-Inspired Differentiable Integral Neural
Networks (QIDINNs), which leverages the Feynman technique of differentiation
under the integral sign to formulate neural updates as integrals over
historical data. This reformulation allows for smoother, more stable learning
dynamics that are both physically interpretable and computationally tractable.
Inspired by Feynman's path integral formalism and compatible with quantum
gradient estimation frameworks, QIDINNs open a path toward hybrid
classical-quantum neural computation. We demonstrate our model's effectiveness
on synthetic and real-world streaming tasks, and we propose directions for
quantum extensions and scalable implementations.

</details>


### [3] [Can LLMs Generate High-Quality Test Cases for Algorithm Problems? TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure](https://arxiv.org/abs/2506.12278)
*Zheyuan Yang,Zexi Kuang,Xue Xia,Yilun Zhao*

Main category: cs.SE

TL;DR: TestCase-Eval is a new benchmark for evaluating LLMs' ability to generate test cases for algorithm problems. It assesses both coverage of diverse failures and the capacity to expose specific faults, analyzing 19 LLMs and uncovering valuable insights into their real-world testing capabilities.


<details>
  <summary>Details</summary>
Motivation: Evaluating LLMs on code generation typically focuses on generating correct code, but systematic test-case generation is an under-explored capability with real-world importance. Current benchmarks lack comprehensive and structured evaluation for test-case generation in competitive programming.

Method: Developed TestCase-Eval, a benchmark containing 500 algorithm problems and 100,000 human-written solutions from Codeforces. Introduces two tasks: Fault Coverage (coverage of diverse inputs and failure types) and Fault Exposure (ability to generate inputs revealing specific bugs). 19 leading LLMs were comprehensively evaluated using this benchmark.

Result: The benchmark highlighted differences in the strengths and weaknesses of leading LLMs for generating effective test cases. The systematic evaluation revealed variability in LLMs' ability to cover diverse failure modes and expose specific faults in coding problems.

Conclusion: TestCase-Eval offers a rigorous, large-scale benchmark for the evaluation of LLMs in test-case generation, revealing nuanced insights into their capabilities and limitations in supporting robust software engineering tasks.

Abstract: We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs
in test-case generation. TestCase-Eval includes 500 algorithm problems and
100,000 human-crafted solutions from the Codeforces platform. It focuses on two
pivotal tasks: (1) Fault Coverage, which measures how well LLM-generated test
sets probe diverse input scenarios and cover a wide range of potential failure
modes. (2) Fault Exposure, which evaluates whether LLMs can craft a tailored
test input that reveals a specific incorrect code implementation. We provide a
comprehensive assessment of 19 state-of-the-art open-source and proprietary
LLMs on TestCase-Eval, offering insights into their strengths and limitations
in generating effective test cases for algorithm problems.

</details>


### [4] [The Foundation Cracks: A Comprehensive Study on Bugs and Testing Practices in LLM Libraries](https://arxiv.org/abs/2506.12320)
*Weipeng Jiang,Xiaoyu Zhang,Xiaofei Xie,Jiongchi Yu,Yuhan Zhi,Shiqing Ma,Chao Shen*

Main category: cs.SE

TL;DR: LLM libraries face many interface-related bugs and poor bug detection due to weak testing practices. This study classifies bug/root cause types and test strategies, recommending improvements for better AI library reliability.


<details>
  <summary>Details</summary>
Motivation: Large Language Model (LLM) libraries are crucial for today's AI systems, but frequent bugs and quality issues threaten their reliability. There is a lack of comprehensive understanding about the nature of these bugs and the effectiveness of testing in such libraries.

Method: The authors conducted a comprehensive empirical investigation, analyzing 313 bug-fixing commits from two leading LLM libraries (HuggingFace Transformers and vLLM). They manually classified bug symptoms and root causes, and evaluated 7,748 test functions to categorize test oracle strategies and assess the effectiveness of current testing practices.

Result: API misuse is the most frequent root cause of bugs (32.17%-48.19%), marking a shift from traditional algorithm-focused defects to more interface-oriented issues. Predefined expected outputs are the most common test oracle. Many bugs remain undetected due to inadequate test cases, lack of proper test drivers, and weak oracles.

Conclusion: The study exposes a strong prevalence of interface/API-related bugs and inadequate testing effectiveness in LLM libraries. The findings suggest specific avenues for improving quality assurance, such as strengthening test cases, drivers, and oracles.

Abstract: Large Language Model (LLM) libraries have emerged as the foundational
infrastructure powering today's AI revolution, serving as the backbone for LLM
deployment, inference optimization, fine-tuning, and production serving across
diverse applications. Despite their critical role in the LLM ecosystem, these
libraries face frequent quality issues and bugs that threaten the reliability
of AI systems built upon them. To address this knowledge gap, we present the
first comprehensive empirical investigation into bug characteristics and
testing practices in modern LLM libraries. We examine 313 bug-fixing commits
extracted across two widely-adopted LLM libraries: HuggingFace Transformers and
vLLM.Through rigorous manual analysis, we establish comprehensive taxonomies
categorizing bug symptoms into 5 types and root causes into 14 distinct
categories.Our primary discovery shows that API misuse has emerged as the
predominant root cause (32.17%-48.19%), representing a notable transition from
algorithm-focused defects in conventional deep learning frameworks toward
interface-oriented problems. Additionally, we examine 7,748 test functions to
identify 7 distinct test oracle categories employed in current testing
approaches, with predefined expected outputs (such as specific tensors and text
strings) being the most common strategy. Our assessment of existing testing
effectiveness demonstrates that the majority of bugs escape detection due to
inadequate test cases (41.73%), lack of test drivers (32.37%), and weak test
oracles (25.90%). Drawing from these findings, we offer some recommendations
for enhancing LLM library quality assurance.

</details>


### [5] [How Developers Use AI Agents: When They Work, When They Don't, and Why](https://arxiv.org/abs/2506.12347)
*Aayush Kumar,Yasharth Bajpai,Sumit Gulwani,Gustavo Soares,Emerson Murphy-Hill*

Main category: cs.SE

TL;DR: Developer collaboration with SWE agents is more successful when iterative and interactive, but trust and debugging challenges remain; design improvements are needed for better real-world performance.


<details>
  <summary>Details</summary>
Motivation: Software Engineering Agents (SWE agents) can perform development tasks autonomously but struggle with complex, ambiguous real-world problems. Understanding how developers interact and collaborate with such agents is important for improving their usefulness.

Method: The study involved observing 19 developers as they used an in-IDE SWE agent to try to solve 33 open issues in repositories to which the developers had previously contributed.

Result: Participants resolved about half of the issues. Success was higher among those who took incremental, interactive, and iterative approaches rather than a single-shot attempt. Challenges included trust, debugging, and testing collaborations.

Conclusion: Active, iterative collaboration with SWE agents leads to greater developer success, but significant communication and trust issues must be addressed to enhance agent effectiveness and collaboration.

Abstract: Software Engineering Agents (SWE agents) can autonomously perform development
tasks on benchmarks like SWE Bench, but still face challenges when tackling
complex and ambiguous real-world tasks. Consequently, SWE agents are often
designed to allow interactivity with developers, enabling collaborative
problem-solving. To understand how developers collaborate with SWE agents and
the communication challenges that arise in such interactions, we observed 19
developers using an in-IDE agent to resolve 33 open issues in repositories to
which they had previously contributed. Participants successfully resolved about
half of these issues, with participants solving issues incrementally having
greater success than those using a one-shot approach. Participants who actively
collaborated with the agent and iterated on its outputs were also more
successful, though they faced challenges in trusting the agent's responses and
collaborating on debugging and testing. These results have implications for
successful developer-agent collaborations, and for the design of more effective
SWE agents.

</details>


### [6] [A Mapping Study About Training in Industry Context in Software Engineering](https://arxiv.org/abs/2506.12590)
*Breno Alves de Andrade,Rodrigo Siqueira,Lidiane Gomes,Antonio Oliveira,Danilo Monteiro Ribeiro*

Main category: cs.SE

TL;DR: The paper systematically maps research on corporate training in software engineering, finding a focus on instructional strategies and identifying gaps in needs analysis and simulation-based training. It calls for more rigorous and diverse research to improve training effectiveness in the industry.


<details>
  <summary>Details</summary>
Motivation: There is a lack of systematized understanding of how corporate training initiatives in software engineering are designed, implemented, and evaluated. This paper aims to address this gap and provide a structured overview of the current state of research in this area.

Method: A systematic mapping study was conducted by selecting and analyzing 26 primary studies related to corporate training in software engineering. The studies were categorized according to Eduardo Salas' framework, covering four areas: Training Needs Analysis, Antecedent Training Conditions, Training Methods and Instructional Strategies, and Post-Training Conditions.

Result: The study found that most research focuses on Training Methods and Instructional Strategies, with significant gaps identified in Job/Task Analysis and Simulation-based Training and Games. The majority of studies reviewed were experience reports and many lacked methodological rigor and longitudinal assessment.

Conclusion: This work provides a structured overview of corporate training research in software engineering, identifying underexplored areas and suggesting future research directions. It highlights challenges and current trends, offering insights for academics and practitioners to design more effective training programs.

Abstract: Context: Corporate training plays a strategic role in the continuous
development of professionals in the software engineering industry. However,
there is a lack of systematized understanding of how training initiatives are
designed, implemented, and evaluated within this domain.
  Objective: This study aims to map the current state of research on corporate
training in software engineering in industry settings, using Eduardo Salas'
training framework as an analytical lens.
  Method: A systematic mapping study was conducted involving the selection and
analysis of 26 primary studies published in the field. Each study was
categorized according to Salas' four key areas: Training Needs Analysis,
Antecedent Training Conditions, Training Methods and Instructional Strategies,
and Post-Training Conditions.
  Results: The findings show a predominance of studies focusing on Training
Methods and Instructional Strategies. Significant gaps were identified in other
areas, particularly regarding Job/Task Analysis and Simulation-based Training
and Games. Most studies were experience reports, lacking methodological rigor
and longitudinal assessment.
  Conclusions: The study offers a structured overview of how corporate training
is approached in software engineering, revealing underexplored areas and
proposing directions for future research. It contributes to both academic and
practical communities by highlighting challenges, methodological trends, and
opportunities for designing more effective training programs in industry.

</details>


### [7] [Real-Time Agile Software Management for Edge and Fog Computing Based Smart City Infrastructure](https://arxiv.org/abs/2506.12616)
*Debasish Jana,Pinakpani Pal,Pawan Kumar*

Main category: cs.SE

TL;DR: This paper introduces the ROOF framework to decentralize data processing in smart cities via fog and edge computing. It demonstrates improved latency, energy savings, redundancy reduction, and strong security through various technologies, validated by international case studies. Key challenges and the role of AI in future urban analytics are also explored.


<details>
  <summary>Details</summary>
Motivation: The demand for smart cities is rapidly increasing, necessitating new solutions for scalable, secure, and energy-efficient real-time data processing due to the predicted surge of IoT devices (over 40 billion by 2030). Existing cloud-based systems are unable to sufficiently address bandwidth, latency, and energy constraints.

Method: The paper implements the ROOF (Real-time Onsite Operations Facilitation) framework, which decentralizes computing by introducing intermediary fog and edge networks for local data processing. Key techniques include fog caching, ultra-low-power wireless transmission, AI-driven resource allocation, TLS encryption, blockchain authentication, and edge-level access control. Case studies from Bhubaneswar, Barcelona, and Copenhagen are used for validation.

Result: ROOF reduces latency, avoids redundancy, saves energy, and improves resource efficiency in smart city scenarios. Enhanced security is achieved with blockchain authentication and edge-level controls. The case studies affirm the framework's benefits in real-world traffic and environmental monitoring applications.

Conclusion: ROOF effectively addresses existing limitations of cloud-based IoT management architectures in smart cities. The framework offers scalable and secure real-time processing with reduced latency and energy consumption. Future challenges and the growing importance of AI-driven analytics in urban infrastructure are discussed.

Abstract: The evolution of smart cities demands scalable, secure, and energy-efficient
architectures for real-time data processing. With the number of IoT devices
expected to exceed 40 billion by 2030, traditional cloud-based systems are
increasingly constrained by bandwidth, latency, and energy limitations. This
paper leverages the ROOF (Real-time Onsite Operations Facilitation) framework
with decentralized computing at intermediary fog and peripheral edge network
layers to reduce latency by processing data near its point of origin. ROOF
features fog caching to avoid redundancy, ultra-low-power wireless transmission
for energy savings, and AI-driven resource allocation for efficiency. Security
is enhanced through TLS encryption, blockchain-based authentication, and
edge-level access control. Case studies from Bhubaneswar, Barcelona and
Copenhagen validate the use of ROOF in traffic systems and environmental
monitoring. The paper concludes by outlining key challenges and prospects of
AI-driven analytics in smart urban infrastructure.

</details>


### [8] [Social Media Reactions to Open Source Promotions: AI-Powered GitHub Projects on Hacker News](https://arxiv.org/abs/2506.12643)
*Prachnachai Meakpaiboonwattana,Warittha Tarntong,Thai Mekratanavorakul,Chaiyong Ragkhitwetsagul,Pattaraporn Sangaroonsilp,Raula Kula,Morakot Choetkiertikul,Kenichi Matsumoto,Thanwadee Sunetnanta*

Main category: cs.SE

TL;DR: Sharing AI-related open-source projects on Hacker News can significantly boost community engagement and development activity on GitHub, making the platform valuable for project promotion and growth.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the increasing influence of social media, especially in the context of the AI and open-source software (OSS) ecosystem. The authors aim to understand how platforms like Hacker News can impact the visibility and developmental growth of AI-based OSS projects.

Method: The authors analyzed 2,195 stories on Hacker News and their comments over two years. They tracked the activity on 1,814 associated GitHub repositories—specifically monitoring metrics like forks, stars, and contributors—after the projects were shared on Hacker News to assess the impact.

Result: At least 19% of AI developers promoted their GitHub projects on Hacker News, frequently receiving positive engagement. The data showed a significant increase in developer activity, as seen in the rise of forks, stars, and contributors on these repositories after being shared on the platform.

Conclusion: Hacker News is an effective platform for promoting AI OSS projects, leading to increased visibility, engagement, and accelerated software development activity.

Abstract: Social media platforms have become more influential than traditional news
sources, shaping public discourse and accelerating the spread of information.
With the rapid advancement of artificial intelligence (AI), open-source
software (OSS) projects can leverage these platforms to gain visibility and
attract contributors. In this study, we investigate the relationship between
Hacker News, a social news site focused on computer science and
entrepreneurship, and the extent to which it influences developer activity on
the promoted GitHub AI projects.
  We analyzed 2,195 Hacker News (HN) stories and their corresponding comments
over a two-year period. Our findings reveal that at least 19\% of AI developers
promoted their GitHub projects on Hacker News, often receiving positive
engagement from the community. By tracking activity on the associated 1,814
GitHub repositories after they were shared on Hacker News, we observed a
significant increase in forks, stars, and contributors. These results suggest
that Hacker News serves as a viable platform for AI-powered OSS projects, with
the potential to gain attention, foster community engagement, and accelerate
software development.

</details>


### [9] [Towards Lean Research Inception: Assessing Practical Relevance of Formulated Research Problems](https://arxiv.org/abs/2506.12669)
*Anrafel Fernandes Pereira,Marcos Kalinowski,Maria Teresa Baldassarre,Jürgen Börstler,Nauman bin Ali,Daniel Mendez*

Main category: cs.SE

TL;DR: The paper presents and evaluates the Lean Research Inception (LRI) framework for assessing practical relevance in SE research. It identifies 'valuable', 'feasible', and 'applicable' as key criteria, validated through expert feedback. The approach appears promising but needs further evaluation.


<details>
  <summary>Details</summary>
Motivation: Many Software Engineering (SE) research contributions lack practical relevance due to oversimplified industrial perspectives, weak connections to industry, and poorly-defined research problems. There is a need for clear criteria that align research with industrial needs.

Method: The authors introduce the Lean Research Inception (LRI) framework and evaluate it during a workshop with SE researchers experienced in industry-academia collaboration. They retroactively applied LRI to an existing paper, facilitating discussion on three criteria (valuable, feasible, applicable) using a semantic differential scale and gathering participant feedback.

Result: The study found strong agreement among participants regarding the importance of the three criteria: valuable (83.3%), feasible (76.2%), and applicable (73.8%) in making research relevant to industry needs. Feedback suggested refinement of the criteria, especially in clarifying terminology and expanding the concept of 'valuable' to include business value, ROI, and originality.

Conclusion: While further evaluation is required, the LRI framework and its three assessment criteria (valuable, feasible, applicable) show promise in helping assess the practical relevance of SE research problems, contributing confidence to their utility within the research and practitioner community.

Abstract: [Context] The lack of practical relevance in many Software Engineering (SE)
research contributions is often rooted in oversimplified views of industrial
practice, weak industry connections, and poorly defined research problems.
Clear criteria for evaluating SE research problems can help align their value,
feasibility, and applicability with industrial needs. [Goal] In this paper, we
introduce the Lean Research Inception (LRI) framework, designed to support the
formulation and assessment of practically relevant research problems in SE. We
describe its initial evaluation strategy conducted in a workshop with a network
of SE researchers experienced in industry-academia collaboration and report the
evaluation of its three assessment criteria (valuable, feasible, and
applicable) regarding their importance in assessing practical relevance.
[Method] We applied LRI retroactively to a published research paper, engaging
workshop participants in discussing and assessing the research problem by
applying the proposed criteria using a semantic differential scale.
Participants provided feedback on the criteria's importance and completeness,
drawn from their own experiences in industry-academia collaboration. [Results]
The findings reveal an overall agreement on the importance of the three
criteria - valuable (83.3%), feasible (76.2%), and applicable (73.8%) - for
aligning research problems with industrial needs. Qualitative feedback
suggested adjustments in terminology with a clearer distinction between
feasible and applicable, and refinements for valuable by more clearly
considering business value, ROI, and originality. [Conclusion] While LRI
constitutes ongoing research and requires further evaluation, our results
strengthen our confidence that the three criteria applied using the semantic
differential scale can already help the community assess the practical
relevance of SE research problems.

</details>


### [10] [Get on the Train or be Left on the Station: Using LLMs for Software Engineering Research](https://arxiv.org/abs/2506.12691)
*Bianca Trinkenreich,Fabio Calefato,Geir Hanssen,Kelly Blincoe,Marcos Kalinowski,Mauro Pezzè,Paolo Tell,Margaret-Anne Storey*

Main category: cs.SE

TL;DR: LLMs are set to transform SE research, offering significant benefits and risks. The community must ensure human oversight and develop guidelines to balance innovation with scientific rigor and ethical responsibility.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are rapidly becoming central to software engineering (SE) research, with the potential to revolutionize practices. This paper is motivated by the need for the SE research community to proactively address the challenges and opportunities brought by LLMs, emphasizing the importance of maintaining human agency and scientific rigor as these technologies are integrated into research.

Method: The paper employs McLuhan's Tetrad of Media Laws as a theoretical framework. Through this lens and supported by discussions at the 2nd Copenhagen Symposium on Human-Centered AI in SE, the authors analyze the multifaceted impact of LLMs on SE research, considering aspects like enhancement, obsolescence, retrieval, and reversal.

Result: The analysis shows that LLMs can boost research through faster ideation and automation, but may also obsolete some traditional practices and introduce new risks if over-relied upon. Key findings include identifying both innovative opportunities and potential hazards tied to LLM adoption in SE research.

Conclusion: SE researchers should not passively accept LLMs but actively shape their adoption, ensuring human oversight, interpretability, and ethical responsibility. Proactive community efforts are needed to realize the benefits of LLMs while establishing guidelines and frameworks to mitigate associated risks, thus maintaining rigor and impact in AI-augmented research.

Abstract: The adoption of Large Language Models (LLMs) is not only transforming
software engineering (SE) practice but is also poised to fundamentally disrupt
how research is conducted in the field. While perspectives on this
transformation range from viewing LLMs as mere productivity tools to
considering them revolutionary forces, we argue that the SE research community
must proactively engage with and shape the integration of LLMs into research
practices, emphasizing human agency in this transformation. As LLMs rapidly
become integral to SE research - both as tools that support investigations and
as subjects of study - a human-centric perspective is essential. Ensuring human
oversight and interpretability is necessary for upholding scientific rigor,
fostering ethical responsibility, and driving advancements in the field.
Drawing from discussions at the 2nd Copenhagen Symposium on Human-Centered AI
in SE, this position paper employs McLuhan's Tetrad of Media Laws to analyze
the impact of LLMs on SE research. Through this theoretical lens, we examine
how LLMs enhance research capabilities through accelerated ideation and
automated processes, make some traditional research practices obsolete,
retrieve valuable aspects of historical research approaches, and risk reversal
effects when taken to extremes. Our analysis reveals opportunities for
innovation and potential pitfalls that require careful consideration. We
conclude with a call to action for the SE research community to proactively
harness the benefits of LLMs while developing frameworks and guidelines to
mitigate their risks, to ensure continued rigor and impact of research in an
AI-augmented future.

</details>


### [11] [Humanity's Last Code Exam: Can Advanced LLMs Conquer Human's Hardest Code Competition?](https://arxiv.org/abs/2506.12713)
*Xiangyang Li,Xiaopeng Li,Kuicai Dong,Quanhu Zhang,Rongju Ruan,Xinyi Dai,Xiaoshuang Liu,Shengchun Xu,Yasheng Wang,Ruiming Tang*

Main category: cs.SE

TL;DR: The paper introduces HLCE, a new, much harder code generation benchmark. Even top LLMs perform poorly, highlighting significant weaknesses and the need for further progress in advanced LLM coding and reasoning.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for LLM code generation, such as APPs and LiveCodeBench, are of medium difficulty and no longer challenge state-of-the-art models. There is a need for more difficult and representative benchmarks to properly assess advanced LLM reasoning and code generation capabilities.

Method: The authors introduce Humanity's Last Code Exam (HLCE), a new benchmark consisting of 235 highly challenging programming problems from the ICPC World Finals and International Olympiad in Informatics (2010-2024). They design an integrated sandbox for reproducible evaluation, assess top LLMs on HLCE, and propose a 'self-recognition' task to measure LLMs' ability to predict their own code generation performance.

Result: Even the best LLMs, such as o4-mini(high) and Gemini-2.5 Pro, achieve pass@1 rates of only 15.9% and 11.4% respectively. LLMs' self-recognition ability is not well correlated with actual code generation performance. Test-time scaling law analysis shows there is substantial room for improvement on difficult programming tasks.

Conclusion: HLCE poses a significantly higher challenge for LLM code generation than existing benchmarks, revealing that advanced LLMs still struggle with complex tasks. This benchmark is likely to become an important standard for evaluating progress in LLM reasoning and human-AI programming collaboration.

Abstract: Code generation is a core capability of large language models (LLMs), yet
mainstream benchmarks (e.g., APPs and LiveCodeBench) contain questions with
medium-level difficulty and pose no challenge to advanced LLMs. To better
reflected the advanced reasoning and code generation ability, We introduce
Humanity's Last Code Exam (HLCE), comprising 235 most challenging problems from
the International Collegiate Programming Contest (ICPC World Finals) and the
International Olympiad in Informatics (IOI) spanning 2010 - 2024. As part of
HLCE, we design a harmonized online-offline sandbox that guarantees fully
reproducible evaluation. Through our comprehensive evaluation, we observe that
even the strongest reasoning LLMs: o4-mini(high) and Gemini-2.5 Pro, achieve
pass@1 rates of only 15.9% and 11.4%, respectively. Meanwhile, we propose a
novel "self-recognition" task to measure LLMs' awareness of their own
capabilities. Results indicate that LLMs' self-recognition abilities are not
proportionally correlated with their code generation performance. Finally, our
empirical validation of test-time scaling laws reveals that current advanced
LLMs have substantial room for improvement on complex programming tasks. We
expect HLCE to become a milestone challenge for code generation and to catalyze
advances in high-performance reasoning and human-AI collaborative programming.
Our code and dataset are also public
available(https://github.com/Humanity-s-Last-Code-Exam/HLCE).

</details>


### [12] [MCTS-Refined CoT: High-Quality Fine-Tuning Data for LLM-Based Repository Issue Resolution](https://arxiv.org/abs/2506.12728)
*Yibo Wang,Zhihao Peng,Ying Wang,Zhao Wei,Hai Yu,Zhiliang Zhu*

Main category: cs.SE

TL;DR: Existing open-source LLMs struggle with software issue resolution due to poor CoT training data. This paper proposes MCTS-REFINE, which rigorously validates and refines reasoning steps using enhanced rejection sampling, producing higher-quality training data and resulting in substantial performance gains above previous methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations in current methods for generating high-quality Chain-of-Thought (CoT) data used for training LLMs in automated software engineering, especially for issue resolution tasks. Existing approaches suffer from weak rejection sampling and inadequate step validation, leading to poor learning outcomes and flawed reasoning chains in open-source models, which underperform compared to proprietary solutions due to these data quality issues.

Method: The paper introduces MCTS-REFINE, an improved Monte Carlo Tree Search algorithm enhanced with a reflection mechanism for correcting reasoning errors via rigorous rejection sampling and refinement. The method decomposes issue resolution into three subtasks with ground-truth criteria and applies strict intermediate output validation, requiring alignment with developer-generated patches at each step.

Result: LLMs fine-tuned with the CoT data generated through MCTS-REFINE show notable performance gains on issue resolution benchmarks (SWE-bench Lite and SWE-bench Verified), with Qwen2.5-72B-Instruct surpassing prior state-of-the-art results at the same parameter scale, achieving 28.3% and 35.0% resolution rates, compared to the baselines' 24.7% and 32.8%.

Conclusion: MCTS-REFINE significantly enhances the quality of CoT data for LLM fine-tuning by robustly validating intermediate reasoning, enabling open-source models to achieve superior performance in software issue resolution, reducing the gap with proprietary solutions.

Abstract: LLMs demonstrate strong performance in auto-mated software engineering,
particularly for code generation and issue resolution. While proprietary models
like GPT-4o achieve high benchmarks scores on SWE-bench, their API dependence,
cost, and privacy concerns limit adoption. Open-source alternatives offer
transparency but underperform in complex tasks, especially sub-100B parameter
models. Although quality Chain-of-Thought (CoT) data can enhance reasoning,
current methods face two critical flaws: (1) weak rejection sampling reduces
data quality, and (2) inadequate step validation causes error accumulation.
These limitations lead to flawed reasoning chains that impair LLMs'ability to
learn reliable issue resolution. The paper proposes MCTS-REFINE, an enhanced
Monte Carlo Tree Search (MCTS)-based algorithm that dynamically validates and
optimizes intermediate reasoning steps through a rigorous rejection sampling
strategy, generating high-quality CoT data to improve LLM performance in issue
resolution tasks. Key innovations include: (1) augmenting MCTS with a
reflection mechanism that corrects errors via rejection sampling and
refinement, (2) decomposing issue resolution into three subtasks-File
Localization, Fault Localization, and Patch Generation-each with clear
ground-truth criteria, and (3) enforcing a strict sampling protocol where
intermediate outputs must exactly match verified developer patches, ensuring
correctness across reasoning paths. Experiments on SWE-bench Lite and SWE-bench
Verified demonstrate that LLMs fine-tuned with our CoT dataset achieve
substantial improvements over baselines.Notably, Qwen2.5-72B- Instruct achieves
28.3%(Lite) and 35.0%(Verified) resolution rates, surpassing SOTA baseline
SWE-Fixer-Qwen-72B with the same parameter scale, which only reached
24.7%(Lite) and 32.8%(Verified).

</details>


### [13] [IDOL: Improved Different Optimization Levels Testing for Solidity Compilers](https://arxiv.org/abs/2506.12760)
*Lantian Li,Yejian Liang,Zhongxing Yu*

Main category: cs.SE

TL;DR: The paper proposes IDOL, a method for testing Solidity compilers by generating contract variants through reverse optimizations. This enhances the detection of bugs in compiler optimizations, as shown by the discovery of three new bugs.


<details>
  <summary>Details</summary>
Motivation: Smart contracts are immutable once deployed, so any vulnerabilities or flaws can result in major financial or legal risks. The compiler's reliability directly impacts contract quality and security. Thus, there's a pressing need for robust compiler testing.

Method: The study introduces the Improved Different Optimization Levels (IDOL) approach, which generates semantically equivalent variants of a smart contract by reversing optimization transformations. By comparing optimized and unoptimized contract forms, IDOL aims to more thoroughly exercise and test the Solidity compiler's optimization logic.

Result: Preliminary evaluations using IDOL have uncovered three confirmed optimization-related bugs in the Solidity compiler.

Conclusion: IDOL enables more effective testing of smart contract compilers by maximizing the scenarios in which optimization logic can be triggered, helping discover security or correctness issues.

Abstract: As blockchain technology continues to evolve and mature, smart contracts have
become a key driving force behind the digitization and automation of
transactions. Smart contracts greatly simplify and refine the traditional
business transaction processes, and thus have had a profound impact on various
industries such as finance and supply chain management. However, because smart
contracts cannot be modified once deployed, any vulnerabilities or design flaws
within the contract cannot be easily fixed, potentially leading to significant
financial losses or even legal issues. The compiler, as a critical component in
the development process, directly affects the quality and security of smart
contracts. This paper innovatively proposes a method, known as the Improved
Different Optimization Levels (IDOL), for testing the Solidity compiler. The
key idea behind IDOL is to perform reverse optimization transformations (i.e.,
change optimized form into unoptimized form) to generate semantically
equivalent variants of the smart contracts under test, aiming to maximize the
opportunities to trigger the optimization logic of compilers. We conducted a
preliminary evaluation of IDOL and three confirmed compiler optimization bugs
have been uncovered at the time of writing.

</details>


### [14] [Towards Operation Proof Obligation Generation for VDM](https://arxiv.org/abs/2506.12858)
*Nick Battle,Peter Gorm Larsen*

Main category: cs.SE

TL;DR: This paper reviews progress and ongoing work to enhance proof obligation generation for explicit operation bodies in VDM tools, identifying both improvements and remaining gaps.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to improve the generation of proof obligations (which ensure model consistency) in formal modeling tools, particularly for VDM tools where support for explicit operation bodies has been insufficient.

Method: The authors describe and analyze the progress made in developing improved support for proof obligation generation related to explicit operation bodies, including a discussion of tool capabilities and incomplete areas.

Result: The work presents the capabilities achieved so far in generating proof obligations for explicit operation bodies within VDM tools, as well as areas where work is still outstanding.

Conclusion: Improved support for proof obligation generation in VDM tools, particularly for explicit operation bodies, is ongoing. Progress has been made, but some challenges and incomplete areas remain.

Abstract: All formalisms have the ability to ensure that their models are internally
consistent. Potential inconsistencies are generally highlighted by assertions
called proof obligations, and the generation of these obligations is an
important role of the tools that support the method. This capability has been
available for VDM tools for many years. However, support for obligation
generation for explicit operation bodies has always been limited. This work
describes the current state of work to address this, showing the capabilities
so far and highlighting the work remaining.

</details>


### [15] [Designing Deep Learning Frameworks for LLMs:Challenges, Expectations, and Opportunities](https://arxiv.org/abs/2506.13114)
*Yanzhou Mu,Rong Wang,Juan Zhai,Chunrong Fang,Xiang Chen,Jiacong Wu,An Guo,Jiawei Shen,Bingzhuo Li,Zhenyu Chen*

Main category: cs.SE

TL;DR: The paper systematically analyzes challenges faced by deep learning frameworks in supporting large language models (LLMs) via issue reports and interviews, builds a detailed taxonomy of problems, and offers recommendations to improve LLM development by addressing identified technical shortcomings.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) are increasingly used in industry, and their reliance on deep learning (DL) frameworks brings unique challenges due to their scale, complexity, and requirements. There is limited understanding of what specific challenges DL frameworks face when supporting LLM development.

Method: The authors performed a large-scale analysis of issue reports from three widely-used DL frameworks (MindSpore, PyTorch, and TensorFlow) and eight LLM toolkits (such as Megatron). This analysis was augmented by interviews with 11 LLM users and 8 developers of DL frameworks. They then constructed a comprehensive taxonomy of issues, requirements, and questions, and assessed their importance and priority with practitioner input.

Result: The study developed a detailed taxonomy of LLM-related problems, including four question themes (with 9 sub-themes), four requirement themes (with 15 sub-themes), and ten bug themes (with 45 sub-themes). Based on this, they identified key technical challenges and misalignments between user needs and developer priorities. Five major findings were discovered and five actionable recommendations were proposed to enhance DL framework reliability, usability, and testability.

Conclusion: Current DL frameworks have critical limitations in supporting LLMs. Addressing these issues with targeted improvements in reliability, usability, and testability is essential for the advancement of LLM applications. The study’s taxonomy and recommendations provide concrete guidance to framework developers and the broader community.

Abstract: Large language models (LLMs) drive significant advancements in real industry
applications. LLMs rely on DL frameworks for efficient model construction,
distributed execution, and optimized deployment. Their large parameter scale
and long execution cycles place extreme demands on DL frameworks in terms of
scalability, stability, and efficiency. Therefore, poor usability, limited
functionality, and subtle bugs in DL frameworks may hinder development
efficiency and cause severe failures or resource waste. However, a fundamental
question remains underinvestigated, i.e., What challenges do DL frameworks face
in supporting LLMs? To seek an answer, we investigate these challenges through
a large-scale analysis of issue reports from three major DL frameworks
(MindSpore, PyTorch, TensorFlow) and eight associated LLM toolkits (e.g.,
Megatron). We construct a taxonomy of LLM-centric bugs, requirements, and user
questions and enrich it through interviews with 11 LLM users and eight DL
framework developers, uncovering key technical challenges and misalignments
between user needs and developer priorities. Our contributions are threefold:
(1) we develop a comprehensive taxonomy comprising four question themes (nine
sub-themes), four requirement themes (15 sub-themes), and ten bug themes (45
sub-themes); (2) we assess the perceived importance and priority of these
challenges based on practitioner insights; and (3) we identify five key
findings across the LLM development and propose five actionable recommendations
to improve the reliability, usability, and testability of DL frameworks. Our
results highlight critical limitations in current DL frameworks and offer
concrete guidance for advancing their support for the next generation of LLM
construction and applications.

</details>


### [16] [Querying Large Automotive Software Models: Agentic vs. Direct LLM Approaches](https://arxiv.org/abs/2506.13171)
*Lukasz Mazur,Nenad Petrovic,James Pontes Miranda,Ansgar Radermacher,Robert Rasche,Alois Knoll*

Main category: cs.SE

TL;DR: Comparing direct prompting and agentic LLM approaches for querying large software models, the paper shows agentic methods are as accurate but far more efficient, making them the most viable solution for large-scale, privacy-sensitive industrial applications.


<details>
  <summary>Details</summary>
Motivation: Large software models are hard to interact with and analyze using traditional methods. The emergence of large language models (LLMs) offers an opportunity to overcome these challenges by enabling natural language interactions with software artifacts.

Method: The paper investigates two LLM-based methods for querying software models: (1) direct prompting (putting the entire software model into the context) and (2) an agentic approach that uses LLM-driven agents along with file-access tools. Both methods are evaluated with an Ecore metamodel for timing analysis and optimization in automotive/embedded domains.

Result: The agentic approach achieves similar accuracy to direct prompting but with significantly greater efficiency regarding token usage. This makes agentic methods more suitable for large models, especially in the automotive industry, where model size makes direct prompting impractical. The experiments demonstrate this using small LLMs suitable for local, private deployment.

Conclusion: Agentic LLM-based approaches present a more efficient and practical way to interact with large software models compared to direct prompting, especially when privacy and resource-efficiency are required. This is particularly relevant in industries like automotive, where these constraints are significant.

Abstract: Large language models (LLMs) offer new opportunities for interacting with
complex software artifacts, such as software models, through natural language.
They present especially promising benefits for large software models that are
difficult to grasp in their entirety, making traditional interaction and
analysis approaches challenging. This paper investigates two approaches for
leveraging LLMs to answer questions over software models: direct prompting,
where the whole software model is provided in the context, and an agentic
approach combining LLM-based agents with general-purpose file access tools. We
evaluate these approaches using an Ecore metamodel designed for timing analysis
and software optimization in automotive and embedded domains. Our findings show
that while the agentic approach achieves accuracy comparable to direct
prompting, it is significantly more efficient in terms of token usage. This
efficiency makes the agentic approach particularly suitable for the automotive
industry, where the large size of software models makes direct prompting
infeasible, establishing LLM agents as not just a practical alternative but the
only viable solution. Notably, the evaluation was conducted using small LLMs,
which are more feasible to be executed locally - an essential advantage for
meeting strict requirements around privacy, intellectual property protection,
and regulatory compliance. Future work will investigate software models in
diverse formats, explore more complex agent architectures, and extend agentic
workflows to support not only querying but also modification of software
models.

</details>


### [17] [From Empirical Evaluation to Context-Aware Enhancement: Repairing Regression Errors with LLMs](https://arxiv.org/abs/2506.13182)
*Anh Ho,Thanh Le-Cong,Bach Le,Christine Rizkallah*

Main category: cs.SE

TL;DR: The study shows that while traditional APR tools can't fix real-world Java regression bugs, LLM-based approaches can, and using bug-inducing code changes as extra context makes them nearly twice as effective.


<details>
  <summary>Details</summary>
Motivation: Despite significant advances in Automated Program Repair (APR), especially with large language models (LLMs), little is known about their effectiveness in fixing regression bugs. There is a need to empirically evaluate modern APR techniques on real-world regression bugs to address this gap.

Method: The authors introduce RegMiner4APR, a curated benchmark of 99 regression bugs from 32 mainstream Java GitHub repositories. They analyze the benchmark and empirically evaluate both traditional and LLM-based APR tools on these bugs. They further experiment with enriching LLM-based APR tools using bug-inducing code change context information.

Result: Conventional APR tools failed to repair any regression bugs. LLM-based APR approaches showed promising results, and augmenting them with bug-inducing change context boosted their repair success by 1.8x.

Conclusion: LLM-based APR approaches demonstrate significant promise for fixing regression bugs, especially when provided with bug-inducing change context. Traditional APR tools are ineffective in this setting. Future research should further leverage context information to enhance APR capabilities.

Abstract: [...] Since then, various APR approaches, especially those leveraging the
power of large language models (LLMs), have been rapidly developed to fix
general software bugs. Unfortunately, the effectiveness of these advanced
techniques in the context of regression bugs remains largely unexplored. This
gap motivates the need for an empirical study evaluating the effectiveness of
modern APR techniques in fixing real-world regression bugs.
  In this work, we conduct an empirical study of APR techniques on Java
regression bugs. To facilitate our study, we introduce RegMiner4APR, a
high-quality benchmark of Java regression bugs integrated into a framework
designed to facilitate APR research. The current benchmark includes 99
regression bugs collected from 32 widely used real-world Java GitHub
repositories. We begin by conducting an in-depth analysis of the benchmark,
demonstrating its diversity and quality. Building on this foundation, we
empirically evaluate the capabilities of APR to regression bugs by assessing
both traditional APR tools and advanced LLM-based APR approaches. Our
experimental results show that classical APR tools fail to repair any bugs,
while LLM-based APR approaches exhibit promising potential. Motivated by these
results, we investigate impact of incorporating bug-inducing change information
into LLM-based APR approaches for fixing regression bugs. Our results highlight
that this context-aware enhancement significantly improves the performance of
LLM-based APR, yielding 1.8x more successful repairs compared to using
LLM-based APR without such context.

</details>


### [18] [Empirical Evaluation of Large Language Models in Automated Program Repair](https://arxiv.org/abs/2506.13186)
*Jiajun Sun,Fengjie Li,Xinzhu Qi,Hongyu Zhang,Jiajun Jiang*

Main category: cs.SE

TL;DR: Modern large language models for automated program repair do not always benefit from larger size or generality; specialized models, prompt strategies, and early generations matter more. The findings guide future LLM-based repair system designs.


<details>
  <summary>Details</summary>
Motivation: Automated program repair (APR) is essential due to rising software bugs, but existing research primarily focuses on earlier, smaller language models and Java benchmarks, leaving the capabilities of modern, large-scale LLMs underexplored.

Method: A comprehensive empirical study evaluating four open-source large language models (CodeLlama, LLaMA, StarCoder, DeepSeek-Coder) with varying parameters, architectures, and intended purposes. The evaluation spans two bug scenarios, three programming languages, and four prompting strategies, analyzing over 600,000 generated patches across six benchmarks.

Result: Key findings are: (1) specialized models like CodeLlama can outperform larger, general-purpose models; (2) increased model size does not guarantee better repair performance; (3) correct patches frequently appear early during generation; (4) prompt choice notably impacts outcomes.

Conclusion: The study’s results deliver practical insights for building more effective and efficient LLM-based automated program repair systems, highlighting model specialization, prompt engineering, and the importance of not assuming linear improvement with larger models.

Abstract: The increasing prevalence of software bugs has made automated program repair
(APR) a key research focus. Large language models (LLMs) offer new
opportunities for APR, but existing studies mostly rely on smaller,
earlier-generation models and Java benchmarks. The repair capabilities of
modern, large-scale LLMs across diverse languages and scenarios remain
underexplored. To address this, we conduct a comprehensive empirical study of
four open-source LLMs, CodeLlama, LLaMA, StarCoder, and DeepSeek-Coder,
spanning 7B to 33B parameters, diverse architectures, and purposes. We evaluate
them across two bug scenarios (enterprise-grades and algorithmic), three
languages (Java, C/C++, Python), and four prompting strategies, analyzing over
600K generated patches on six benchmarks. Key findings include: (1) model
specialization (e.g., CodeLlama) can outperform larger general-purpose models
(e.g., LLaMA); (2) repair performance does not scale linearly with model size;
(3) correct patches often appear early in generation; and (4) prompts
significantly affect results. These insights offer practical guidance for
designing effective and efficient LLM-based APR systems.

</details>


### [19] [Isolating Noisy Labelled Test Cases in Human-in-the-Loop Oracle Learning](https://arxiv.org/abs/2506.13273)
*Charaka Geethal Kapugama*

Main category: cs.SE

TL;DR: ISONOISE is a technique that effectively detects incorrectly labeled test cases in human-in-the-loop oracle learning, improving reliability with over 67% accuracy and little relabeling needed.


<details>
  <summary>Details</summary>
Motivation: Incorrectly labeled test cases introduced during the human-in-the-loop oracle learning can degrade the performance and reliability of machine learning models. Detecting and correcting such mislabeled data is crucial for improving the training process.

Method: The paper proposes ISONOISE, a technique for programs with numeric inputs. ISONOISE first isolates suspected mislabeled test cases by measuring their disagreement with the rest of the test suite. It then trains an intermediate test oracle on slightly disagreeing cases and presents the most suspect cases for relabeling. Mislabeled cases trigger updates to the intermediate oracle, and this loop repeats until no further mislabeled cases are found.

Result: ISONOISE was evaluated in the LEARN2FIX human-in-the-loop oracle learning setting and showed over 67% accuracy in identifying mislabeled test cases, while needing only a small number of relabeling queries.

Conclusion: ISONOISE enhances the reliability of human-in-the-loop oracle learning by efficiently identifying and enabling correction of mislabeled test cases with high accuracy and low relabeling effort.

Abstract: Incorrectly labelled test cases can adversely affect the training process of
human-in-the-loop oracle learning tech-niques. This paper introduces ISONOISE,
a technique designed to identify such mislabelled test cases introduced during
human-in-the-loop oracle learning. This technique can be applied to programs
taking numeric inputs. Given a compromised automatic test oracle and its
training test suite, ISONOISE first isolates thetest cases suspected of being
mislabelled. This task is performed based on the level of disagreement of a
test case with respect to the others. An intermediate automatic test oracle is
trained based on the slightly disagreeing test cases. Based on the predictions
of this intermediate oracle, the test cases suspected of being mislabelled are
systematically presented for relabelling. When mislabelled test cases are
found, the intermediate test oracle is updated. This process repeats until no
mislabelled test case is found in relabelling. ISONOISE was evaluated within
the human-in-the-loop oracle learning method used in LEARN2FIX. Experimental
results demonstrate that ISONOISE can identify mislabelled test cases
introduced by the human in LEARN2FIX with over 67% accuracy, while requiring
only a small number of relabelling queries. These findings highlight the
potential of ISONOISE to enhance the reliability of human-in-the-loop oracle
learning.

</details>


### [20] [Adopting Use Case Descriptions for Requirements Specification: an Industrial Case Study](https://arxiv.org/abs/2506.13303)
*Julian Frattini,Anja Frattini*

Main category: cs.SE

TL;DR: This paper studied how use case descriptions are actually used in industry versus textbook recommendations, analyzing over a thousand requirements in a large company. It found that real-world practices differ notably from guidelines, and only a few factors (like solution-orientation) truly affect use case quality and software development. This points to a need for research to focus on what really matters in practice.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address gaps in the literature regarding the practical use of use case (UC) descriptions for specifying functional requirements. Specifically, it seeks to understand the real-world adoption of quality recommendations, whether these recommendations align with actual UC quality, and which factors influence UC quality in industry settings.

Method: The authors conducted an empirical study within a large, globally distributed case company. They analyzed 1188 business requirements comprising 1192 UCs collected from 2020 to 2024. 273 template-style UC descriptions were manually assessed for quality based on established guidelines. The study used descriptive statistics to examine adoption patterns and inferential statistics to explore factors affecting UC quality and its impact on software development.

Result: The findings show a notable deviation between the actual use of UC descriptions in the company and the recommendations found in literature. Among the various factors investigated, only a few—such as solution-orientation—were found to significantly impact UC quality or subsequent software development processes.

Conclusion: The study concludes that real-world adoption of UC description guidelines differs from textbook advice, and most recommended factors do not significantly affect UC quality. This insight suggests a need to realign future UC quality research with practical realities and to focus on factors that genuinely affect quality in practice.

Abstract: Context: Use case (UC) descriptions are a prominent format for specifying
functional requirements. Existing literature abounds with recommendations on
how to write high-quality UC descriptions but lacks insights into (1) their
real-world adoption, (2) whether these recommendations correspond to actual
quality, and (3) which factors influence the quality of UCs. Objectives: We aim
to contribute empirical evidence about the adoption of UC descriptions in a
large, globally distributed case company. Methods: We surveyed 1188 business
requirements of a case company that were elicited from 2020-01-01 until
2024-12-31 and contained 1192 UCs in various forms. Among these, we manually
evaluated the 273 template-style UC descriptions against established quality
guidelines. We generated descriptive statistics of the format's adoption over
the surveyed time frame. Furthermore, we used inferential statistics to
determine (a) how properties of the requirements engineering process affected
the UC quality and (b) how UC quality affects subsequent software development
activities. Results and Conclusions: Our descriptive results show how the
adoption of UC descriptions in practice deviates from textbook recommendations.
However, our inferential results suggest that only a few phenomena like
solution-orientation show an actual impact in practice. These results can steer
UC quality research into a more relevant direction.

</details>


### [21] [Model Context Protocol (MCP) at First Glance: Studying the Security and Maintainability of MCP Servers](https://arxiv.org/abs/2506.13538)
*Mohammed Mehedi Hasan,Hao Li,Emad Fallahzadeh,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: The paper examines 1,899 open-source MCP servers and finds several security and maintainability issues unique to MCP's architecture, highlighting the necessity for new MCP-specific vulnerability detection, while also showing the ongoing importance of traditional software analysis techniques.


<details>
  <summary>Details</summary>
Motivation: With the rise of Foundation Models (FMs) like GPT-4 across various industries, these models' real-world impact is limited by their reliance on text-based interfaces. The recent introduction of the Model Context Protocol (MCP) aims to standardize tool integration for FMs, but its AI-driven, non-deterministic design may present new risks that have not been systematically studied.

Method: The authors conducted the first large-scale empirical analysis of MCP. They evaluated 1,899 open-source MCP servers using a hybrid analysis pipeline: a combination of a general-purpose static analysis tool and a specialized MCP-specific scanner. State-of-the-art health metrics were employed to assess the security, health, and maintainability of the servers.

Result: The study found that while MCP servers generally display strong health metrics, there are unique risks: eight distinct types of vulnerabilities were identified, only three of which are common to traditional software systems. 7.2% of the servers have general vulnerabilities, 5.5% have MCP-specific tool poisoning, 66% suffer from code smells, and 14.4% contain at least ten previously-reported bug patterns.

Conclusion: The non-deterministic nature of MCP introduces new and unique vulnerabilities not adequately addressed by current tools. There is a critical need for vulnerability detection techniques specific to MCP, although traditional analysis and refactoring continue to be valuable.

Abstract: Although Foundation Models (FMs), such as GPT-4, are increasingly used in
domains like finance and software engineering, reliance on textual interfaces
limits these models' real-world interaction. To address this, FM providers
introduced tool calling-triggering a proliferation of frameworks with distinct
tool interfaces. In late 2024, Anthropic introduced the Model Context Protocol
(MCP) to standardize this tool ecosystem, which has become the de facto
standard with over eight million weekly SDK downloads. Despite its adoption,
MCP's AI-driven, non-deterministic control flow introduces new risks to
sustainability, security, and maintainability, warranting closer examination.
  Towards this end, we present the first large-scale empirical study of MCP.
Using state-of-the-art health metrics and a hybrid analysis pipeline, combining
a general-purpose static analysis tool with an MCP-specific scanner, we
evaluate 1,899 open-source MCP servers to assess their health, security, and
maintainability. Despite MCP servers demonstrating strong health metrics, we
identify eight distinct vulnerabilities-only three overlapping with traditional
software vulnerabilities. Additionally, 7.2% of servers contain general
vulnerabilities and 5.5% exhibit MCP-specific tool poisoning. Regarding
maintainability, while 66% exhibit code smells, 14.4% contain ten bug patterns
overlapping prior research. These findings highlight the need for MCP-specific
vulnerability detection techniques while reaffirming the value of traditional
analysis and refactoring practices.

</details>


### [22] [DesignCoder: Hierarchy-Aware and Self-Correcting UI Code Generation with Large Language Models](https://arxiv.org/abs/2506.13663)
*Yunnong Chen,Shixian Ding,YingYing Zhang,Wenkai Chen,Jinzhou Du,Lingyun Sun,Liuqing Chen*

Main category: cs.SE

TL;DR: DesignCoder introduces a hierarchical and self-correcting framework for automated front-end code generation, achieving substantial improvements in both visual and structural code quality over current methods, as verified by benchmarks and developer evaluations.


<details>
  <summary>Details</summary>
Motivation: Multimodal large language models (MLLMs) have enabled automated code generation for front-end interfaces, streamlining development. However, these models often struggle to ensure visual consistency and functional completeness, and lack mechanisms to assess fidelity and correctness of the rendered components.

Method: The authors propose DesignCoder, a hierarchical-aware and self-correcting code generation framework. Key innovations include UI Grouping Chains for nested UI understanding, a hierarchical divide-and-conquer strategy for code generation, and a self-correction mechanism to identify and fix errors in the generated code.

Result: DesignCoder, evaluated on a diverse UI mockup dataset from both open-source and industry sources, outperforms state-of-the-art baselines in React Native. The method achieves significant performance increases in visual similarity metrics (MSE, CLIP, SSIM) and code structure similarity measures (TreeBLEU, Container Match, Tree Edit Distance). A user study with professional developers further confirms that the generated code is highly usable, readable, and maintainable.

Conclusion: DesignCoder offers an efficient and practical solution for agile front-end development by improving code quality, maintainability, and alignment with industry best practices. It enables teams to focus on core functionalities rather than interface code quality concerns.

Abstract: Multimodal large language models (MLLMs) have streamlined front-end interface
development by automating code generation. However, these models also introduce
challenges in ensuring code quality. Existing approaches struggle to maintain
both visual consistency and functional completeness in the generated
components. Moreover, they lack mechanisms to assess the fidelity and
correctness of the rendered pages. To address these issues, we propose
DesignCoder, a novel hierarchical-aware and self-correcting automated code
generation framework. Specifically, we introduce UI Grouping Chains, which
enhance MLLMs' capability to understand and predict complex nested UI
hierarchies. Subsequently, DesignCoder employs a hierarchical
divide-and-conquer approach to generate front-end code. Finally, we incorporate
a self-correction mechanism to improve the model's ability to identify and
rectify errors in the generated code. Extensive evaluations on a dataset of UI
mockups collected from both open-source communities and industry projects
demonstrate that DesignCoder outperforms state-of-the-art baselines in React
Native, a widely adopted UI framework. Our method achieves a 37.63%, 9.52%,
12.82% performance increase in visual similarity metrics (MSE, CLIP, SSIM) and
significantly improves code structure similarity in terms of TreeBLEU,
Container Match, and Tree Edit Distance by 30.19%, 29.31%, 24.67%. Furthermore,
we conducted a user study with professional developers to assess the quality
and practicality of the generated code. Results indicate that DesignCoder
aligns with industry best practices, demonstrating high usability, readability,
and maintainability. Our approach provides an efficient and practical solution
for agile front-end development, enabling development teams to focus more on
core functionality and product innovation.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [23] [A Fast, Reliable, and Secure Programming Language for LLM Agents with Code Actions](https://arxiv.org/abs/2506.12202)
*Stephen Mell,Botong Zhang,David Mell,Shuo Li,Ramya Ramalingam,Nathan Yu,Steve Zdancewic,Osbert Bastani*

Main category: cs.PL

TL;DR: The paper introduces Quasar, a new language for code actions in LLM agents, which provides improved performance, security, and reliability compared to Python, as shown in VQA experiments.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs often use Python for tool-calling code actions, but Python has limitations in performance, security, and reliability for this purpose.

Method: The authors propose a new programming language, Quasar, specifically designed for LLM-driven code actions. Key features of Quasar include automated parallelization, uncertainty quantification, and enhanced security mechanisms. The approach allows LLMs to initially write in a subset of Python, which is then transpiled to Quasar. The method is evaluated by comparing Quasar-based code actions with Python-based actions in a VQA (Visual Question Answering) agent called ViperGPT on the GQA dataset.

Result: Using Quasar enables LLM agents to maintain high task performance, reduces execution time by up to 42%, decreases the frequency of user approval interactions by up to 52% (enhancing security), and increases reliability through uncertainty quantification to meet desired confidence thresholds.

Conclusion: Quasar is a promising language for LLM-driven code actions, offering meaningful improvements in performance, security, and reliability over Python, and is validated on real-world agent tasks.

Abstract: Modern large language models (LLMs) are often deployed as agents, calling
external tools adaptively to solve tasks. Rather than directly calling tools,
it can be more effective for LLMs to write code to perform the tool calls,
enabling them to automatically generate complex control flow such as
conditionals and loops. Such code actions are typically provided as Python
code, since LLMs are quite proficient at it; however, Python may not be the
ideal language due to limited built-in support for performance, security, and
reliability. We propose a novel programming language for code actions, called
Quasar, which has several benefits: (1) automated parallelization to improve
performance, (2) uncertainty quantification to improve reliability and mitigate
hallucinations, and (3) security features enabling the user to validate
actions. LLMs can write code in a subset of Python, which is automatically
transpiled to Quasar. We evaluate our approach on the ViperGPT visual question
answering agent, applied to the GQA dataset, demonstrating that LLMs with
Quasar actions instead of Python actions retain strong performance, while
reducing execution time when possible by 42%, improving security by reducing
user approval interactions when possible by 52%, and improving reliability by
applying conformal prediction to achieve a desired target coverage level.

</details>


### [24] [Freer Arrows and Why You Need Them in Haskell](https://arxiv.org/abs/2506.12212)
*Grant VanDomelen,Gan Shen,Lindsey Kuper,Yao Li*

Main category: cs.PL

TL;DR: Freer arrows fix a known limitation of freer monads (poor static analysis) and are shown—via Haskell case studies—to be practical and expressive, making them useful for domains needing static verification.


<details>
  <summary>Details</summary>
Motivation: Freer monads are widely used for their expressiveness but are difficult to analyze statically, which limits certain compiler optimizations and error checking.

Method: The paper investigates alternative structures called freer arrows. Several variants of freer arrows are proposed, and their applicability is demonstrated through a case study focused on choreographic programming in Haskell.

Result: Freer arrows prove to be more amenable to static analysis than freer monads. The case study shows that they can be effectively used in Haskell for choreographic programming.

Conclusion: Freer arrows present a viable alternative to freer monads when static analysis is required. The proposed variants expand their usability, and their effectiveness is validated in a real-world programming scenario.

Abstract: Freer monads are a useful structure commonly used in various domains due to
their expressiveness. However, a known issue with freer monads is that they are
not amenable to static analysis. This paper explores freer arrows, a relatively
expressive structure that is amenable to static analysis. We propose several
variants of freer arrows. We conduct a case study on choreographic programming
to demonstrate the usefulness of freer arrows in Haskell.

</details>


### [25] [StacKAT: Infinite State Network Verification](https://arxiv.org/abs/2506.13383)
*Jules Jacobs,Nate Foster,Tobias Kappé,Dexter Kozen,Lily Saada,Alexandra Silva,Jana Wagemaker*

Main category: cs.PL

TL;DR: StacKAT is a new network verification language that introduces stack operations for richer expressiveness, enabling analysis of complex behaviors like parsing and source routing. It supports formal equivalence checking and has a complete axiomatization, addressing key gaps in prior solutions.


<details>
  <summary>Details</summary>
Motivation: Existing network verification languages like NetKAT cannot express a wide range of network behaviors such as parsing, source routing, and telemetry, due to limitations like lack of stack operations and more complex state handling.

Method: The authors develop StacKAT, a network verification language that extends expressiveness by incorporating loops, finite state variables, nondeterminism, and a stack with push/pop operations. They interpret variables and the stack as analogous to parsed headers and packet contents, enabling more complex modeling. They also propose a decision procedure based on finite automata to check program equivalence and provide a formal axiomatization of the language's equivalence, establishing completeness.

Result: StacKAT is able to express complex network behaviors that are out of reach for previous languages like NetKAT. The proposed decision procedure supports equivalence checking and provides counterexamples for non-equivalence. The axiomatization is shown to be complete.

Conclusion: StacKAT extends the capabilities of network verification languages to cover behaviors like parsing, source routing, and telemetry, which previous languages struggled to represent. Its decision procedure and axiomatization enable rigorous verification and analysis of network programs.

Abstract: We develop StacKAT, a network verification language featuring loops, finite
state variables, nondeterminism, and - most importantly - access to a stack
with accompanying push and pop operations. By viewing the variables and stack
as the (parsed) headers and (to-be-parsed) contents of a network packet,
StacKAT can express a wide range of network behaviors including parsing, source
routing, and telemetry. These behaviors are difficult or impossible to model
using existing languages like NetKAT. We develop a decision procedure for
StacKAT program equivalence, based on finite automata. This decision procedure
provides the theoretical basis for verifying network-wide properties and is
able to provide counterexamples for inequivalent programs. Finally, we provide
an axiomatization of StacKAT equivalence and establish its completeness.

</details>
