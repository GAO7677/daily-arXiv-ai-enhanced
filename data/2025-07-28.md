<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Exploring the Landscape of Fairness Interventions in Software Engineering](https://arxiv.org/abs/2507.18726)
*Sadia Afrin Mim*

Main category: cs.SE

TL;DR: The paper surveys efforts to improve fairness in AI by summarizing existing studies and solutions aimed at reducing bias and other risks in real-world applications.


<details>
  <summary>Details</summary>
Motivation: AI is increasingly used in critical sectors like healthcare and finance, but brings challenges due to potential risks such as biased data, making fairness a major concern.

Method: The paper conducts a survey, compiling and summarizing previous research and different interventions developed to address fairness issues in AI systems.

Result: It provides an overview of the various studies and approaches that tackle fairness problems in AI, outlining the landscape of current solutions.

Conclusion: This survey paper offers a comprehensive summary of the methods adopted to mitigate fairness-related risks in AI applications.

Abstract: Current developments in AI made it broadly significant for reducing human
labor and expenses across several essential domains, including healthcare and
finance. However, the application of AI in the actual world poses multiple
risks and disadvantages due to potential risk factors in data (e.g., biased
dataset). Practitioners developed a number of fairness interventions for
addressing these kinds of problems. The paper acts as a survey, summarizing the
various studies and approaches that have been developed to address fairness
issues

</details>


### [2] [Agentic Program Repair from Test Failures at Scale: A Neuro-symbolic approach with static analysis and test execution feedback](https://arxiv.org/abs/2507.18755)
*Chandra Maddila,Adam Tait,Claire Chang,Daniel Cheng,Nauman Ahmad,Vijayaraghavan Murali,Marshall Roch,Arnaud Avondet,Aaron Meltzer,Victor Montalvao,Michael Hopko,Chris Waterson,Parth Thakkar,Renuka Fernandez,Kristian Kristensen,Sivan Barzily,Sherry Chen,Rui Abreu,Nachiappan Nagappan,Payam Shodjai,Killian Murphy,James Everingham,Aparna Ramani,Peter C. Rigby*

Main category: cs.SE

TL;DR: This paper presents an agentic LLM-based system for automated program repair in large organizations, showing competitive model performance and practical fix acceptance rates using a combination of neural and symbolic feedback mechanisms.


<details>
  <summary>Details</summary>
Motivation: Large organizations with vast codebases face challenges in efficiently repairing code at scale based on test failures. The advent of large language models (LLMs) has opened up new opportunities for automating sophisticated program repair, motivating this study to develop and assess an agentic system for automatic code fixing using LLMs.

Method: The authors developed an 'Engineering Agent' using Llama as the foundational LLM and the ReAct framework for agentic reasoning and action. The system operates by starting from test failures triaged by a rule-based bot, then employing 15 different agentic actions (from file reading to patch generation). Feedback mechanisms include static analysis and test execution results, allowing iterative solution refinement. Patch quality is validated using a combination of an LLM-as-a-Judge and human review.

Result: Benchmark tests demonstrate that a specialized 70B model can compete with a larger vanilla Llama-405B model. The ReAct harness benefits when augmented with symbolic information from static analyzers and test traces. The agent achieved a solve rate of 42.3% with an average of 11.8 feedback loops per task. In production, 80% of fixes were reviewed, and 31.5% of those were accepted (overall, 25.5% of all generated fixes landed). Engineer feedback ranged from positive (quick approvals, surprises) to mixed (partially correct solutions used as starting points).

Conclusion: Automated program repair using LLM-driven Engineering Agents is feasible and can efficiently address code issues at scale in large organizations, yielding a significant proportion of accepted code fixes and positive developer engagement. Combining neural reasoning with symbolic tools further enhances repair effectiveness.

Abstract: Aim: With the advent of LLMs, sophisticated agentic program repair has become
viable at large organizations with large codebases. In this work, we develop an
Engineering Agent that fixes the source code based on test failures at scale
across diverse software offerings internally.
  Method: Using Llama as the base, we employ the ReAct harness to develop an
agent. We start with a test failure that was triaged by a rule-based test
failure bot. We then set up an agentic harness and allow the agent to reason
and run a set of 15 actions from reading a file to generating a patch. We
provide feedback to the agent through static analysis and test failures so it
can refine its solution. We leverage an LLM-as-a-Judge to ensure that the patch
conforms to the standards followed by a human review to land fixes.
  Benchmark Findings: We curated offline benchmarks for our patch generator,
the Engineering Agent loop, and the LLM-as-a-Judge. In offline evaluations we
found that a specialized 70B model is highly competitive with the much larger
but vanilla Llama-405B. In an ablation study, we found that the ReAct harness
(neural model) benefited from the symbolic information from static analysis
tools and test execution traces. A model that strikes a balance between the
solve rate and error rate vs the cost and latency has a benchmark solve rate of
42.3% using an average 11.8 feedback iterations.
  Production Findings: In a three month period, 80% of the generated fixes were
reviewed, of which 31.5% were landed (25.5% of the total number of generated
fixes).
  Feedback from Engineers: We used open coding to extract qualitative themes
from engineers' feedback. We saw positive feedback in the form of quick
approvals, gratitude, and surprise. We also found mixed feedback when the
Engineering Agent's solution was partially correct and it served as a good
starting point.

</details>


### [3] [MemoCoder: Automated Function Synthesis using LLM-Supported Agents](https://arxiv.org/abs/2507.18812)
*Yiping Jia,Zhen Ming Jiang,Shayan Noei,Ying Zou*

Main category: cs.SE

TL;DR: MemoCoder is a collaborative, multi-agent framework that helps LLMs improve code generation and debugging by learning iteratively from past fixes and deploying supervisory agents. Experiments show it outperforms existing approaches on major benchmarks.


<details>
  <summary>Details</summary>
Motivation: Large Language Models help with code generation but struggle with iterative debugging and adapting to diverse or complex problem structures. Existing methods like fine-tuning or self-repair are either too expensive or do not allow for accumulating and leveraging past fixes.

Method: The paper introduces MemoCoder, a multi-agent framework. It features a Fixing Knowledge Set for storing and retrieving successful code repairs and a central Mentor Agent that supervises the repair process by identifying patterns and refining strategies. MemoCoder is evaluated on different code generation benchmarks (MBPP, HumanEval, LiveCodeBench).

Result: MemoCoder outperforms baseline methods including zero-shot prompting and Self-Repair across multiple benchmarks, showing significant improvement in Pass@10 and Pass@50 metrics.

Conclusion: MemoCoder enables more effective and persistent learning from code repairs, leading to better code generation performance by leveraging iterative refinement and accumulated knowledge.

Abstract: With the widespread adoption of Large Language Models (LLMs) such as GitHub
Copilot and ChatGPT, developers increasingly rely on AI-assisted tools to
support code generation. While LLMs can generate syntactically correct
solutions for well-structured programming tasks, they often struggle with
challenges that require iterative debugging, error handling, or adaptation to
diverse problem structures. Existing approaches such as fine-tuning or
self-repair strategies either require costly retraining or lack mechanisms to
accumulate and reuse knowledge from previous attempts.
  To address these limitations, we propose MemoCoder, a multi-agent framework
that enables collaborative problem solving and persistent learning from past
fixes. At the core of MemoCoder is a Fixing Knowledge Set, which stores
successful repairs and supports retrieval for future tasks. A central Mentor
Agent supervises the repair process by identifying recurring error patterns and
refining high-level fixing strategies, providing a novel supervisory role that
guides the self-repair loop. We evaluate MemoCoder across three public
benchmarks -- MBPP, HumanEval, and LiveCodeBench -- spanning a range of problem
complexities. Experimental results show that MemoCoder consistently outperforms
both zero-shot prompting and a Self-Repair strategy, with improvements ranging
from 3.1% to 12.1% in Pass@10 and from 1.4% to 14.5% in Pass@50, demonstrating
its effectiveness in iterative refinement and knowledge-guided code generation.

</details>


### [4] [Exploring the Jupyter Ecosystem: An Empirical Study of Bugs and Vulnerabilities](https://arxiv.org/abs/2507.18833)
*Wenyuan Jiang,Diany Pressato,Harsh Darji,Thibaud Lutellier*

Main category: cs.SE

TL;DR: Jupyter notebooks pose unique challenges compared to traditional software, leading to more bugs—mainly configuration problems—and higher security risks. These findings suggest a need for tailored software engineering tools and improved practices specific to notebooks.


<details>
  <summary>Details</summary>
Motivation: Jupyter notebooks are widely used by data scientists, but their unique features (such as the integration of scripts, markdown, and images) make them tougher to analyze compared to traditional software, leading to gaps in existing software engineering tools and studies.

Method: The study collected a large dataset of Jupyter notebooks from two major platforms and conducted quantitative analyses on their characteristics. Qualitative research using grounded theory was performed to create a bug taxonomy. Additionally, security-related commits and vulnerability reports were analyzed to assess deployment risks.

Result: Configuration issues are the most common bugs in notebooks, followed by incorrect API usage. Popular deployment frameworks for notebooks also exhibit notable vulnerabilities.

Conclusion: Jupyter notebooks lack the support typical of traditional software, resulting in more complex code, frequent misconfigurations, and poorer maintenance practices.

Abstract: Background. Jupyter notebooks are one of the main tools used by data
scientists. Notebooks include features (configuration scripts, markdown,
images, etc.) that make them challenging to analyze compared to traditional
software. As a result, existing software engineering models, tools, and studies
do not capture the uniqueness of Notebook's behavior. Aims. This paper aims to
provide a large-scale empirical study of bugs and vulnerabilities in the
Notebook ecosystem. Method. We collected and analyzed a large dataset of
Notebooks from two major platforms. Our methodology involved quantitative
analyses of notebook characteristics (such as complexity metrics, contributor
activity, and documentation) to identify factors correlated with bugs.
Additionally, we conducted a qualitative study using grounded theory to
categorize notebook bugs, resulting in a comprehensive bug taxonomy. Finally,
we analyzed security-related commits and vulnerability reports to assess risks
associated with Notebook deployment frameworks. Results. Our findings highlight
that configuration issues are among the most common bugs in notebook documents,
followed by incorrect API usage. Finally, we explore common vulnerabilities
associated with popular deployment frameworks to better understand risks
associated with Notebook development. Conclusions. This work highlights that
notebooks are less well-supported than traditional software, resulting in more
complex code, misconfiguration, and poor maintenance.

</details>
