<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 8]
- [cs.PL](#cs.PL) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Comparative Evaluation of Large Language Models for Test-Skeleton Generation](https://arxiv.org/abs/2509.04644)
*Subhang Boorlagadda,Nitya Naga Sai Atluri,Muhammet Mustafa Olmez,Edward F. Gehringer*

Main category: cs.SE

TL;DR: Evaluated four LLMs for generating Ruby test skeletons; DeepSeek delivered the most maintainable results, while GPT-4 was complete but inconsistent. Prompt design is crucial for quality output.


<details>
  <summary>Details</summary>
Motivation: Manual creation of test skeletons for TDD is time-consuming and prone to errors, especially for students and large development teams. Automating this process using LLMs could improve efficiency and consistency.

Method: The study evaluates four LLMs (GPT-4, DeepSeek-Chat, Llama4-Maverick, Gemma2-9B) on their ability to generate RSpec test skeletons for a Ruby class. Outputs were assessed via static analysis and blind expert review on criteria such as correctness, clarity, maintainability, and best practice alignment.

Result: DeepSeek-Chat generated the most maintainable and well-structured skeletons. GPT-4 produced outputs that were more complete but less consistent with conventions. Differences in code structure interpretation and prompt/context effects were observed across models.

Conclusion: Key factors influencing LLM output quality for test skeleton generation are prompt design and contextual input. While some models (DeepSeek) excel in maintainability, others (GPT-4) may require further refinement for consistent best practice adherence.

Abstract: This paper explores the use of Large Language Models (LLMs) to automate the
generation of test skeletons -- structural templates that outline unit test
coverage without implementing full test logic. Test skeletons are especially
important in test-driven development (TDD), where they provide an early
framework for systematic verification. Traditionally authored manually, their
creation can be time-consuming and error-prone, particularly in educational or
large-scale development settings. We evaluate four LLMs -- GPT-4,
DeepSeek-Chat, Llama4-Maverick, and Gemma2-9B -- on their ability to generate
RSpec skeletons for a real-world Ruby class developed in a university software
engineering course. Each model's output is assessed using static analysis and a
blind expert review to measure structural correctness, clarity,
maintainability, and conformance to testing best practices. The study reveals
key differences in how models interpret code structure and testing conventions,
offering insights into the practical challenges of using LLMs for automated
test scaffolding. Our results show that DeepSeek generated the most
maintainable and well-structured skeletons, while GPT-4 produced more complete
but conventionally inconsistent output. The study reveals prompt design and
contextual input as key quality factors.

</details>


### [2] [Real-Time Performance Benchmarking of TinyML Models in Embedded Systems (PICO: Performance of Inference, CPU, and Operations)](https://arxiv.org/abs/2509.04721)
*Abhishek Dey,Saurabh Srivastava,Gaurav Singh,Robert G. Pettit*

Main category: cs.SE

TL;DR: The paper introduces a new benchmarking framework for TinyML on embedded devices, showing how two popular hardware platforms compare in key performance metrics. The results guide practical TinyML deployment decisions.


<details>
  <summary>Details</summary>
Motivation: There is a growing need to evaluate TinyML models on resource-constrained embedded systems, but existing benchmarks may lack modularity and platform independence. This paper aims to provide a benchmark framework specifically for real-time TinyML performance on embedded devices.

Method: The authors propose and implement PICO-TINYML-BENCHMARK, a modular, platform-agnostic benchmarking framework. They evaluate three TinyML models—Gesture Classification, Keyword Spotting, and MobileNet V2—across two platforms (BeagleBone AI64 and Raspberry Pi 4), using real-world datasets and metrics such as inference latency, CPU usage, memory efficiency, and prediction stability.

Result: Benchmarking results show that the BeagleBone AI64 platform performs best for consistent inference latency in AI tasks, while the Raspberry Pi 4 provides superior resource efficiency and cost effectiveness. The collected metrics reveal important trade-offs between platforms.

Conclusion: The provided benchmarking framework effectively highlights the strengths and weaknesses of different embedded platforms for TinyML deployment. It helps bridge the gap between theory and practical application by giving actionable recommendations for optimizing TinyML on real-world embedded systems.

Abstract: This paper presents PICO-TINYML-BENCHMARK, a modular and platform-agnostic
framework for benchmarking the real-time performance of TinyML models on
resource-constrained embedded systems. Evaluating key metrics such as inference
latency, CPU utilization, memory efficiency, and prediction stability, the
framework provides insights into computational trade-offs and platform-specific
optimizations. We benchmark three representative TinyML models -- Gesture
Classification, Keyword Spotting, and MobileNet V2 -- on two widely adopted
platforms, BeagleBone AI64 and Raspberry Pi 4, using real-world datasets.
Results reveal critical trade-offs: the BeagleBone AI64 demonstrates consistent
inference latency for AI-specific tasks, while the Raspberry Pi 4 excels in
resource efficiency and cost-effectiveness. These findings offer actionable
guidance for optimizing TinyML deployments, bridging the gap between
theoretical advancements and practical applications in embedded systems.

</details>


### [3] [NovaQ: Improving Quantum Program Testing through Diversity-Guided Test Case Generation](https://arxiv.org/abs/2509.04763)
*Tiancheng Jin,Shangzhou Xia,Jianjun Zhao*

Main category: cs.SE

TL;DR: NovaQ is a novel quantum program testing framework that enhances bug detection and test diversity by generating and prioritizing novel quantum circuit inputs, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: As quantum computing progresses, reliable quantum programs are more critical, but testing them thoroughly is a challenge due to their unique behaviors and state spaces.

Method: NovaQ is proposed as a testing framework combining a distribution-based test case generator (mutates quantum circuit parameters for diverse states) and a novelty-driven evaluator (measures behavioral novelty via magnitude, phase, and entanglement metrics). It prioritizes test inputs that explore rarely tested behavioral regions.

Result: NovaQ achieves higher diversity in test cases and detects more bugs compared to existing baseline testing approaches, as demonstrated on various quantum programs.

Conclusion: NovaQ provides a more effective method for uncovering bugs and increasing behavioral coverage in quantum programs by focusing on diversity and novelty in test input selection.

Abstract: Quantum programs are designed to run on quantum computers, leveraging quantum
circuits to solve problems that are intractable for classical machines. As
quantum computing advances, ensuring the reliability of quantum programs has
become increasingly important. This paper introduces NovaQ, a diversity-guided
testing framework for quantum programs. NovaQ combines a distribution-based
test case generator with a novelty-driven evaluation module. The generator
produces diverse quantum state inputs by mutating circuit parameters, while the
evaluator quantifies behavioral novelty based on internal circuit state
metrics, including magnitude, phase, and entanglement. By selecting inputs that
map to infrequently covered regions in the metric space, NovaQ effectively
explores under-tested program behaviors. We evaluate NovaQ on quantum programs
of varying sizes and complexities. Experimental results show that NovaQ
consistently achieves higher test input diversity and detects more bugs than
existing baseline approaches.

</details>


### [4] [Code Review Without Borders: Evaluating Synthetic vs. Real Data for Review Recommendation](https://arxiv.org/abs/2509.04810)
*Yogev Cohen,Dudi Ohayon,Romy Somkin,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.SE

TL;DR: This paper shows that LLMs can generate synthetic code review data to train classifiers for new programming languages, helping automate code review when there isn’t enough labelled data.


<details>
  <summary>Details</summary>
Motivation: There is a lack of labelled data in emerging programming languages, which creates a bottleneck for automating code review decisions, despite the availability of large volumes of unlabelled code.

Method: The paper leverages Large Language Models (LLMs) to translate code changes from well-resourced languages into underrepresented or emerging languages, generating synthetic training data for supervised classifier training. Supervised classifiers are trained on this data and compared with models trained on real labelled data.

Result: LLM-generated synthetic data can effectively bootstrap review recommendation systems and narrow the performance gap between low-resource and well-resourced settings.

Conclusion: Automated code review systems can be extended to new languages using LLM-generated synthetic training data, providing a scalable solution even when annotated data is minimal or unavailable.

Abstract: Automating the decision of whether a code change requires manual review is
vital for maintaining software quality in modern development workflows.
However, the emergence of new programming languages and frameworks creates a
critical bottleneck: while large volumes of unlabelled code are readily
available, there is an insufficient amount of labelled data to train supervised
models for review classification. We address this challenge by leveraging Large
Language Models (LLMs) to translate code changes from well-resourced languages
into equivalent changes in underrepresented or emerging languages, generating
synthetic training data where labelled examples are scarce. We assume that
although LLMs have learned the syntax and semantics of new languages from
available unlabelled code, they have yet to fully grasp which code changes are
considered significant or review-worthy within these emerging ecosystems. To
overcome this, we use LLMs to generate synthetic change examples and train
supervised classifiers on them. We systematically compare the performance of
these classifiers against models trained on real labelled data. Our experiments
across multiple GitHub repositories and language pairs demonstrate that
LLM-generated synthetic data can effectively bootstrap review recommendation
systems, narrowing the performance gap even in low-resource settings. This
approach provides a scalable pathway to extend automated code review
capabilities to rapidly evolving technology stacks, even in the absence of
annotated data.

</details>


### [5] [Integrating Large Language Models in Software Engineering Education: A Pilot Study through GitHub Repositories Mining](https://arxiv.org/abs/2509.04877)
*Maryam Khan,Muhammad Azeem Akbar,Jussi Kasurinen*

Main category: cs.SE

TL;DR: This paper empirically validates key motivators and demotivators of using LLMs in SE education by analyzing 400 GitHub repositories. It confirms the importance of engagement, motivation, and programming support as motivators, and highlights concerns like plagiarism and security as major demotivators. Some concerns seen in literature did not appear in practice. The findings help shape a future framework for responsible LLM use in education.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the growing adoption of Large Language Models (LLMs) like ChatGPT in software engineering education. With the increasing use, there is a need to systematically investigate the responsible and effective integration of LLMs into educational curricula, understanding both the opportunities and the challenges they present.

Method: The authors conducted a pilot repository mining study, analyzing 400 GitHub project repositories. They focused on README files and issue discussions to find evidence of motivators and demotivators previously identified in their literature review.

Result: The study identified strong motivators for LLM adoption, including engagement and motivation, process understanding, and programming assistance (with numerous mentions for each). Prominent demotivators were also found, particularly plagiarism/IP concerns, as well as security and privacy issues. However, some demotivators, such as challenges in learning outcome evaluation or curriculum redesign, were not observed in the repositories.

Conclusion: This empirical study provides early validation for the motivators and demotivators taxonomy reflecting real-world usage themes. It exposes gaps in research and sets the groundwork for a comprehensive framework to responsibly guide LLM integration into software engineering education curricula.

Abstract: Context: Large Language Models (LLMs) such as ChatGPT are increasingly
adopted in software engineering (SE) education, offering both opportunities and
challenges. Their adoption requires systematic investigation to ensure
responsible integration into curricula. Objective: This doctoral research aims
to develop a validated framework for integrating LLMs into SE education through
a multi-phase process, including taxonomies development, empirical
investigation, and case studies. This paper presents the first empirical step.
Method: We conducted a pilot repository mining study of 400 GitHub projects,
analyzing README files and issues discussions to identify the presence of
motivator and demotivator previously synthesized in our literature review [ 8]
study. Results: Motivators such as engagement and motivation (227 hits),
software engineering process understanding (133 hits), and programming
assistance and debugging support (97 hits) were strongly represented.
Demotivators, including plagiarism and IP concerns (385 hits), security,
privacy and data integrity (87 hits), and over-reliance on AI in learning (39
hits), also appeared prominently. In contrast, demotivators such as challenges
in evaluating learning outcomes and difficulty in curriculum redesign recorded
no hits across the repositories. Conclusion: The study provides early empirical
validation of motivators/demotivators taxonomies with respect to their themes,
highlights research practice gaps, and lays the foundation for developing a
comprehensive framework to guide the responsible adoption of LLMs in SE
education.

</details>


### [6] [FuzzRDUCC: Fuzzing with Reconstructed Def-Use Chain Coverage](https://arxiv.org/abs/2509.04967)
*Kai Feng,Jeremy Singer,Angelos K Marnerides*

Main category: cs.SE

TL;DR: FuzzRDUCC is a new binary-only fuzzing framework that uses symbolic execution for dataflow analysis, exposing vulnerabilities missed by control flow-guided fuzzers, and demonstrates effectiveness by finding unique crashes in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Binary-only fuzzing is limited in code coverage and vulnerability detection because it mainly uses control flow edge coverage, missing bugs related to dataflow.

Method: FuzzRDUCC integrates dataflow analysis with fuzzing by employing symbolic execution to extract definition-use chains from binary executables. It uses a heuristic algorithm to select relevant def-use chains, maintaining efficiency and fuzzing thoroughness.

Result: FuzzRDUCC identifies crucial dataflow paths and uncovers security vulnerabilities that traditional fuzzers miss. Evaluation on binutils benchmark showed it found unique crashes that existing fuzzers did not.

Conclusion: FuzzRDUCC advances vulnerability detection for binaries by integrating efficient dataflow tracking into the fuzzing process, making it a strong candidate for future security testing tools.

Abstract: Binary-only fuzzing often struggles with achieving thorough code coverage and
uncovering hidden vulnerabilities due to limited insight into a program's
internal dataflows. Traditional grey-box fuzzers guide test case generation
primarily using control flow edge coverage, which can overlook bugs not easily
exposed through control flow analysis alone. We argue that integrating dataflow
analysis into the fuzzing process can enhance its effectiveness by revealing
how data propagates through the program, thereby enabling the exploration of
execution paths that control flow-based methods might miss. In this context, we
introduce FuzzRDUCC, a novel fuzzing framework that employs symbolic execution
to reconstruct definition-use (def-use) chains directly from binary
executables. FuzzRDUCC identifies crucial dataflow paths and exposes security
vulnerabilities without incurring excessive computational overhead, due to a
novel heuristic algorithm that selects relevant def-use chains without
affecting the thoroughness of the fuzzing process. We evaluate FuzzRDUCC using
the binutils benchmark and demonstrate that it can identify unique crashes not
found by state-of-the-art fuzzers. Hence, establishing FuzzRDUCC as a feasible
solution for next generation vulnerability detection and discovery mechanisms.

</details>


### [7] [GenAI-based test case generation and execution in SDV platform](https://arxiv.org/abs/2509.05112)
*Denesa Zyberaj,Lukasz Mazur,Nenad Petrovic,Pankhuri Verma,Pascal Hirmer,Dirk Slama,Xiangwei Cheng,Alois Knoll*

Main category: cs.SE

TL;DR: GenAI (LLMs and VLMs) are used to automate the creation of vehicle software test cases from requirements and diagrams, reducing manual labor, but some human oversight is still needed due to platform and AI limitations.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper lies in reducing manual effort and increasing efficiency in generating structured test cases for software-defined vehicles by harnessing the capabilities of GenAI, particularly Large Language Models and Vision-Language Models.

Method: The paper presents a methodology that leverages GenAI technologies to translate natural language requirements and system diagrams into Gherkin test cases, integrates Vehicle Signal Specification modeling, and executes these test cases within a vendor-neutral digital.auto playground.

Result: The proposed approach is evaluated using a Child Presence Detection System use case, resulting in significant reductions in manual test specification efforts and enabling rapid execution of the generated tests.

Conclusion: While the GenAI-driven solution substantially automates the test case generation and execution process, some manual intervention remains necessary due to current limitations of the pipeline and the constraints of the digital.auto platform.

Abstract: This paper introduces a GenAI-driven approach for automated test case
generation, leveraging Large Language Models and Vision-Language Models to
translate natural language requirements and system diagrams into structured
Gherkin test cases. The methodology integrates Vehicle Signal Specification
modeling to standardize vehicle signal definitions, improve compatibility
across automotive subsystems, and streamline integration with third-party
testing tools. Generated test cases are executed within the digital.auto
playground, an open and vendor-neutral environment designed to facilitate rapid
validation of software-defined vehicle functionalities. We evaluate our
approach using the Child Presence Detection System use case, demonstrating
substantial reductions in manual test specification effort and rapid execution
of generated tests. Despite significant automation, the generation of test
cases and test scripts still requires manual intervention due to current
limitations in the GenAI pipeline and constraints of the digital.auto platform.

</details>


### [8] [AI Agents for Web Testing: A Case Study in the Wild](https://arxiv.org/abs/2509.05197)
*Naimeng Ye,Xiao Yu,Ruize Xu,Tianyi Peng,Zhou Yu*

Main category: cs.SE

TL;DR: WebProber, an LLM and AI agent-based web testing tool, simulates human interactions to detect usability issues missed by traditional methods, showing the potential for smarter, user-centered automated testing frameworks.


<details>
  <summary>Details</summary>
Motivation: Traditional web testing misses complex user behaviors and usability issues, as it mainly focuses on code coverage and load testing. With the rise of large language models (LLMs) and AI agents, there is an opportunity to improve web testing by simulating human interactions and identifying more nuanced usability problems.

Method: The authors introduce WebProber, an AI agent-based web testing framework that uses LLM-powered agents to autonomously explore websites, simulate user interactions, detect bugs and usability issues, and generate human-readable reports. The prototype is evaluated via a case study on 120 academic personal websites.

Result: WebProber was able to uncover 29 usability issues across 120 tested websites. Many of these issues were not detected by traditional web testing tools, suggesting the superiority of the agent-based approach in identifying nuanced usability problems.

Conclusion: The study demonstrates that agent-based, LLM-powered web testing can identify usability issues that traditional methods often miss. This approach offers a promising direction for future, user-centered web testing frameworks that can better ensure high-quality user experiences.

Abstract: Automated web testing plays a critical role in ensuring high-quality user
experiences and delivering business value. Traditional approaches primarily
focus on code coverage and load testing, but often fall short of capturing
complex user behaviors, leaving many usability issues undetected. The emergence
of large language models (LLM) and AI agents opens new possibilities for web
testing by enabling human-like interaction with websites and a general
awareness of common usability problems. In this work, we present WebProber, a
prototype AI agent-based web testing framework. Given a URL, WebProber
autonomously explores the website, simulating real user interactions,
identifying bugs and usability issues, and producing a human-readable report.
We evaluate WebProber through a case study of 120 academic personal websites,
where it uncovered 29 usability issues--many of which were missed by
traditional tools. Our findings highlight agent-based testing as a promising
direction while outlining directions for developing next-generation,
user-centered testing frameworks.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [9] [A Large-Scale Study of Floating-Point Usage in Statically Typed Languages](https://arxiv.org/abs/2509.04936)
*Andrea Gilot,Tobias Wrigstad,Eva Darulova*

Main category: cs.PL

TL;DR: This paper presents a comprehensive empirical study of floating-point usage in public GitHub repositories, confirming its widespread use and highlighting differences between benchmarks and real-world code to guide future research tools.


<details>
  <summary>Details</summary>
Motivation: Reasoning about floating-point arithmetic is difficult, and current techniques are not fully relevant to real-world code because the characteristics of actual floating-point code are not well understood.

Method: The paper performs a large-scale empirical study of floating-point arithmetic usage by mining public GitHub repositories written in statically typed languages. It uses random sampling, intrinsic filtering to avoid bias, keyword searches, and code parsing for identifying floating-point usage and language constructs.

Result: The study confirms that floating-point arithmetic is widely used in real-world code. It finds that while some aspects of benchmark code used in research are representative of real-world usage, there are notable differences in other aspects.

Conclusion: The results and released dataset can guide the development and evaluation of future static and dynamic analysis techniques for floating-point arithmetic, helping them better address users' real-world needs.

Abstract: Reasoning about floating-point arithmetic is notoriously hard. While static
and dynamic analysis techniques or program repair have made significant
progress, more work is still needed to make them relevant to real-world code.
On the critical path to that goal is understanding what real-world
floating-point code looks like. To close that knowledge gap, this paper
presents the first large-scale empirical study of floating-point arithmetic
usage in statically typed languages across public GitHub repositories. We
follow state-of the art mining practices including random sampling and
filtering based on only intrinsic properties to avoid bias, and identify
floating-point usage by searching for keywords in the source code, and
programming language constructs (e.g., loops) by parsing the code. Our
evaluation supports the claim often made in papers that floating-point
arithmetic is widely used. Comparing statistics such as size and usage of
certain constructs and functions, we find that benchmarks used in literature to
evaluate automated reasoning techniques for floating-point arithmetic are in
certain aspects representative of 'real-world' code, but not in all. We aim for
our study and dataset to help future techniques for floating-point arithmetic
to be designed and evaluated to match actual users' expectations.

</details>


### [10] [AI-Assisted Modeling: DSL-Driven AI Interactions](https://arxiv.org/abs/2509.05160)
*Steven Smyth,Daniel Busch,Moez Ben Haj Hmida,Edward A. Lee,Bernhard Steffen*

Main category: cs.PL

TL;DR: The paper presents a VS Code extension that enhances AI programming by enabling immediate, visual, and formal validation of models through domain-specific graphical representations, supporting trustworthiness and efficiency in code development.


<details>
  <summary>Details</summary>
Motivation: AI-assisted programming enhances software development, but transparency and trustworthy validation are lacking. Integrating formal verification and making AI outputs more understandable are key challenges addressed.

Method: The authors propose the integration of domain-specific modeling techniques with AI programming tools, delivering instantaneous, graphical visualizations of the AI-generated code. This includes interactive support for programming, natural language, voice, and step-wise refinement with immediate feedback. They implemented a Visual Studio Code extension for Lingua Franca as a prototype.

Result: The resulting prototype enables the creation, visualization, and verification of formal models within a development environment, demonstrating instantaneous visual feedback and domain-specific adaptation. It facilitates both intuitive modeling and rigorous validation.

Conclusion: Combining AI code generation with transparent, domain-specific visualizations and formal verification methods significantly improves both the efficiency and reliability of software development and validation workflows.

Abstract: AI-assisted programming greatly increases software development performance.
We enhance this potential by integrating transparency through domain-specific
modeling techniques and providing instantaneous, graphical visualizations that
accurately represent the semantics of AI-generated code. This approach
facilitates visual inspection and formal verification, such as model checking.
  Formal models can be developed using programming, natural language prompts,
voice commands, and stage-wise refinement, with immediate feedback after each
transformation step. This support can be tailored to specific domains or
intended purposes, improving both code generation and subsequent validation
processes.
  To demonstrate the effectiveness of this approach, we have developed a
prototype as a Visual Studio Code extension for the Lingua Franca language.
This prototype showcases the potential for novel domain-specific modeling
practices, offering an advancement in how models are created, visualized, and
verified.

</details>


### [11] [Non-Termination Proving: 100 Million LoC and Beyond](https://arxiv.org/abs/2509.05293)
*Julien Vanegue,Jules Villard,Peter O'Hearn,Azalea Raad*

Main category: cs.PL

TL;DR: Pulse Infinite is a scalable proof-based tool that detects non-termination in massive software projects, finding many new issues and setting a new benchmark for real-world applicability.


<details>
  <summary>Details</summary>
Motivation: Detecting non-termination (divergence) bugs in large-scale software is challenging, and previous techniques only worked on small codebases, limiting their practical use for industry-scale software.

Method: The authors introduce Pulse Infinite, a compositional and under-approximate proof-based tool designed to identify non-termination in large codebases. The compositional approach supports scalability, and under-approximation ensures soundness in proving divergence.

Result: Pulse Infinite was applied to over a hundred million lines of code in various programming languages (C, C++, Hack), finding over 30 previously unknown non-termination issues in real-world open-source and proprietary software.

Conclusion: Pulse Infinite establishes a new state of the art for detecting divergence in real-world, large-scale codebases, overcoming the scalability limitations of prior work.

Abstract: We report on our tool, Pulse Infinite, that uses proof techniques to show
non-termination (divergence) in large programs. Pulse Infinite works
compositionally and under-approximately: the former supports scale, and the
latter ensures soundness for proving divergence. Prior work focused on small
benchmarks in the tens or hundreds of lines of code (LoC), and scale limits
their practicality: a single company may have tens of millions, or even
hundreds of millions of LoC or more. We report on applying Pulse Infinite to
over a hundred million lines of open-source and proprietary software written in
C, C++, and Hack, identifying over 30 previously unknown issues, establishing a
new state of the art for detecting divergence in real-world codebases.

</details>
