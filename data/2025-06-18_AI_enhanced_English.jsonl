{"id": "2506.14485", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2506.14485", "abs": "https://arxiv.org/abs/2506.14485", "authors": ["Sascha Rechenberger", "Thom Fr\u00fchwirth"], "title": "Optimized Execution of FreeCHR", "comment": "This is a preprint of a paper submitted to the 39th Workshop on (Constraint and Functional) Logic Programming (WLP 2025)", "summary": "Constraint Handling Rules (CHR) is a rule-based programming language that rewrites collections of constraints. It is typically embedded into a general-purpose language. There exists a plethora of implementations for numerous host languages. However, the existing implementations often re-invent the method of embedding, which impedes maintenance and weakens assertions of correctness. To formalize and thereby unify the embedding of a ground subset of CHR into arbitrary host languages, we introduced the framework FreeCHR and proved it to be a valid representation of classical CHR. For the sake of simplicity, abstract implementations of our framework did not yet include a concrete matching algorithm nor optimizations. In this paper, we introduce an improved execution algorithm for FreeCHR. We also provide an evaluation of the algorithm via benchmarks which suggest the effectiveness of our implemented optimizations.", "AI": {"tldr": "This paper presents an improved execution algorithm and optimizations for FreeCHR, providing a more efficient and unified method for embedding CHR in various host languages, as supported by benchmark results.", "motivation": "Existing implementations of Constraint Handling Rules (CHR) in various host languages often reinvent the embedding process, leading to difficulties in maintenance and correctness. There is a need for a formal, unified approach to embedding CHR that avoids these drawbacks.", "method": "The authors introduce an improved execution algorithm for FreeCHR, a framework for embedding a ground subset of CHR into arbitrary host languages. They validate the algorithm through benchmark evaluation, focusing on the effectiveness of their optimizations.", "result": "The benchmarks suggest that the implemented optimizations in the improved execution algorithm for FreeCHR are effective.", "conclusion": "The improved execution algorithm enhances the FreeCHR framework, making the embedding of CHR into host languages more unified, efficient, and maintainable, with demonstrated performance gains through benchmarking."}}
{"id": "2506.13800", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13800", "abs": "https://arxiv.org/abs/2506.13800", "authors": ["Abul Ehtesham", "Aditi Singh", "Saket Kumar"], "title": "Enhancing Clinical Decision Support and EHR Insights through LLMs and the Model Context Protocol: An Open-Source MCP-FHIR Framework", "comment": null, "summary": "Enhancing clinical decision support (CDS), reducing documentation burdens, and improving patient health literacy remain persistent challenges in digital health. This paper presents an open-source, agent-based framework that integrates Large Language Models (LLMs) with HL7 FHIR data via the Model Context Protocol (MCP) for dynamic extraction and reasoning over electronic health records (EHRs). Built on the established MCP-FHIR implementation, the framework enables declarative access to diverse FHIR resources through JSON-based configurations, supporting real-time summarization, interpretation, and personalized communication across multiple user personas, including clinicians, caregivers, and patients. To ensure privacy and reproducibility, the framework is evaluated using synthetic EHR data from the SMART Health IT sandbox (https://r4.smarthealthit.org/), which conforms to the FHIR R4 standard. Unlike traditional approaches that rely on hardcoded retrieval and static workflows, the proposed method delivers scalable, explainable, and interoperable AI-powered EHR applications. The agentic architecture further supports multiple FHIR formats, laying a robust foundation for advancing personalized digital health solutions.", "AI": {"tldr": "This paper presents an open-source framework linking large language models to medical EHR data via a new protocol, enabling scalable, personalized, explainable, and privacy-preserving digital health solutions for various user types.", "motivation": "Clinical decision support is often limited by EHR data complexity, high clinician documentation burden, and the need to communicate effectively with different users like clinicians, caregivers, and patients. There is also a need for privacy, scalability, and explainability in deploying AI for healthcare.", "method": "The paper introduces an open-source, agent-based framework that connects Large Language Models (LLMs) with HL7 FHIR EHR data via the Model Context Protocol (MCP). It allows declarative access to FHIR resources through JSON, facilitating dynamic extraction, summarization, and personalized communication, evaluated with synthetic EHR data conforming to FHIR R4 standards.", "result": "The framework demonstrates the ability to deliver dynamic, real-time summarization and interpretation of EHR data for multiple user roles. It supports scalable, explainable, and interoperable AI applications using synthetic data, and accommodates multiple FHIR formats through its agentic architecture.", "conclusion": "By integrating LLMs with structured EHR data using a new protocol, the framework advances personalized digital health, enhances CDS, reduces documentation burden, supports health literacy, and ensures privacy and scalability in digital health solutions."}}
{"id": "2506.13804", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13804", "abs": "https://arxiv.org/abs/2506.13804", "authors": ["Edward McDaid", "Sarah McDaid"], "title": "Instruction and Solution Probabilities as Heuristics for Inductive Programming", "comment": "10 pages, 10 figures", "summary": "Instruction subsets (ISs) are heuristics that can shrink the size of the inductive programming (IP) search space by tens of orders of magnitude. Here, we extend the IS approach by introducing instruction and solution probabilities as additional heuristics. Instruction probability reflects the expectation of an instruction occurring in a solution, based on the frequency of instruction occurrence in a large code sample. The solution probability for a partial or complete program is simply the product of all constituent instruction probabilities, including duplicates. We treat the minimum solution probabilities observed in code sample program units of different sizes as solution probability thresholds. These thresholds are used to prune the search space as partial solutions are constructed, thereby eliminating any branches containing unlikely combinations of instructions. The new approach has been evaluated using a large sample of human code. We tested two formulations of instruction probability: one based on instruction occurrence across the entire code sample and another that measured the distribution separately for each IS. Our results show that both variants produce substantial further reductions in the IP search space size of up to tens of orders of magnitude, depending on solution size. In combination with IS, reductions of over 100 orders of magnitude can be achieved. We also carried out cross-validation testing to show that the heuristics should work effectively with unseen code. The approach is described and the results and some ideas for future work are discussed.", "AI": {"tldr": "By leveraging instruction and solution probabilities alongside instruction subsets, this approach drastically reduces inductive programming search spaces\u2014sometimes by over 100 orders of magnitude\u2014resulting in much more efficient program synthesis.", "motivation": "Inductive programming involves searching large solution spaces, which can be computationally expensive. Existing approaches use instruction subsets (ISs) to reduce the search space, but further efficiency gains are desired.", "method": "The paper extends the IS approach by introducing new heuristics: instruction probability and solution probability. Instruction probability is based on the frequency of instruction occurrences in a large code sample. Solution probability is the product of instruction probabilities in a partial or complete program. Minimum observed solution probabilities are used as thresholds to prune unlikely branches. Two probability formulations are evaluated, and cross-validation testing is used.", "result": "Both probability formulations significantly reduce the IP search space size\u2014up to tens of orders of magnitude, depending on solution size. When combined with IS, overall search space reductions exceed 100 orders of magnitude. Cross-validation indicates the heuristics generalize to unseen code.", "conclusion": "Incorporating instruction and solution probabilities as heuristics with IS dramatically improves inductive programming efficiency by greatly shrinking the search space. These heuristics can generalize well and suggest promising directions for further research."}}
{"id": "2506.13815", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.13815", "abs": "https://arxiv.org/abs/2506.13815", "authors": ["Shrinivass Arunachalam Balasubramanian"], "title": "Signal-First Architectures: Rethinking Front-End Reactivity", "comment": "18 pages, 4 figures", "summary": "Modern front-end frameworks face escalating reactivity management challenges, including performance degradation from complex observable chains and unpredictable re-renders. This paper introduces Signal-First Architecture--a novel paradigm where granular, dependency-tracked signals are the atomic unit of reactivity. Unlike traditional RxJS or NgRx patterns, Signal-First enforces reactive flows from explicit signal declarations, with derived values via computed() and side effects scoped to effect(). This model ensures deterministic behavior by eliminating implicit subscriptions and optimizing reactive graph evaluation.\n  We present a comparative analysis of three Angular reactivity models: RxJS service-based, NgRx global stores, and pure Signal-First implementations. Through controlled benchmarking, including Chrome DevTools performance tracing, memory heap snapshots, and Lighthouse audits, this study quantifies Signal-First advantages.", "AI": {"tldr": "The paper proposes Signal-First Architecture for Angular, showing through benchmarks that it outperforms RxJS and NgRx in clarity, predictability, and performance by using explicit signals and deterministic reactive flows.", "motivation": "Modern front-end frameworks struggle with reactivity management, such as performance issues from complex observable chains and unpredictable component re-renders. The paper aims to address these problems by introducing a new reactivity paradigm.", "method": "The authors introduce the Signal-First Architecture, using explicit signal declarations, and compare it to RxJS service-based and NgRx global store reactivity models in Angular. The comparison employs benchmarking tests\u2014Chrome DevTools tracing, memory snapshots, and Lighthouse audits.", "result": "Signal-First offers measurable improvements over current paradigms. It eliminates implicit subscriptions, provides deterministic reactivity, and optimizes performance and memory usage based on benchmarking results.", "conclusion": "Signal-First Architecture provides a clearer, more predictable, and performant model for reactivity in front-end applications compared to traditional RxJS and NgRx approaches."}}
{"id": "2506.13820", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.13820", "abs": "https://arxiv.org/abs/2506.13820", "authors": ["Shraddha Surana", "Ashwin Srinivasan", "Michael Bain"], "title": "Structured Program Synthesis using LLMs: Results and Insights from the IPARC Challenge", "comment": null, "summary": "The IPARC Challenge, inspired by ARC, provides controlled program synthesis tasks over synthetic images to evaluate automatic program construction, focusing on sequence, selection, and iteration. This set of 600 tasks has resisted automated solutions. This paper presents a structured inductive programming approach with LLMs that successfully solves tasks across all IPARC categories. The controlled nature of IPARC reveals insights into LLM-based code generation, including the importance of prior structuring, LLMs' ability to aid structuring (requiring human refinement), the need to freeze correct code, the efficiency of code reuse, and how LLM-generated code can spark human creativity. These findings suggest valuable mechanisms for human-LLM collaboration in tackling complex program synthesis.", "AI": {"tldr": "Structured inductive programming using LLMs, along with human guidance, can solve previously unsolved IPARC program synthesis tasks, revealing best practices for human-LLM collaboration and effective code generation.", "motivation": "The motivation is to evaluate and improve the automatic program construction capabilities of LLMs using a controlled set of program synthesis tasks (IPARC Challenge), which have proven resistant to automation.", "method": "The paper uses a structured inductive programming approach supported by large language models (LLMs) to solve complex program synthesis tasks across various categories, emphasizing sequence, selection, and iteration.", "result": "LLMs, with some human refinement and structured guidance, can successfully solve all categories of the IPARC Challenge. The process highlights the importance of structured approaches, the need to freeze correct code, the benefits of code reuse, and how human-LLM collaboration can foster creativity.", "conclusion": "LLM-based approaches, particularly when paired with structured methods and human involvement, offer promising mechanisms for collaborative, complex program synthesis, as demonstrated on the IPARC Challenge."}}
