{"id": "2506.10043", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10043", "abs": "https://arxiv.org/abs/2506.10043", "authors": ["Yongqian Sun", "Yu Luo", "Xidao Wen", "Yuan Yuan", "Xiaohui Nie", "Shenglin Zhang", "Tong Liu", "Xi Luo"], "title": "TrioXpert: An automated incident management framework for microservice system", "comment": null, "summary": "Automated incident management plays a pivotal role in large-scale\nmicroservice systems. However, many existing methods rely solely on\nsingle-modal data (e.g., metrics, logs, and traces) and struggle to\nsimultaneously address multiple downstream tasks, including anomaly detection\n(AD), failure triage (FT), and root cause localization (RCL). Moreover, the\nlack of clear reasoning evidence in current techniques often leads to\ninsufficient interpretability. To address these limitations, we propose\nTrioXpert, an end-to-end incident management framework capable of fully\nleveraging multimodal data. TrioXpert designs three independent data processing\npipelines based on the inherent characteristics of different modalities,\ncomprehensively characterizing the operational status of microservice systems\nfrom both numerical and textual dimensions. It employs a collaborative\nreasoning mechanism using large language models (LLMs) to simultaneously handle\nmultiple tasks while providing clear reasoning evidence to ensure strong\ninterpretability. We conducted extensive evaluations on two popular\nmicroservice system datasets, and the experimental results demonstrate that\nTrioXpert achieves outstanding performance in AD (improving by 4.7% to 57.7%),\nFT (improving by 2.1% to 40.6%), and RCL (improving by 1.6% to 163.1%) tasks.", "AI": {"tldr": "TrioXpert uses multimodal data and LLMs for more effective and interpretable incident management in microservices, significantly improving results in anomaly detection, failure triage, and root cause localization.", "motivation": "Current automated incident management approaches for microservice systems mainly use single-modal data (like logs, metrics, or traces) and struggle with multitasking and interpretability. There is a need for solutions that can leverage multimodal data and provide clear reasoning.", "method": "The proposed framework, TrioXpert, incorporates three independent data processing pipelines, each designed for a specific data modality. It uses large language models (LLMs) for collaborative reasoning, enabling simultaneous handling of anomaly detection, failure triage, and root cause localization tasks, alongside generating reasoning evidence for better interpretability.", "result": "Experiments on two well-known microservice datasets show that TrioXpert delivers significant improvements: 4.7%\u201357.7% in anomaly detection, 2.1%\u201340.6% in failure triage, and 1.6%\u2013163.1% in root cause localization.", "conclusion": "TrioXpert successfully addresses the limitations of current methods by leveraging multimodal data and providing interpretability, achieving outstanding and interpretable performance across multiple incident management tasks in microservice systems."}}
{"id": "2506.10049", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10049", "abs": "https://arxiv.org/abs/2506.10049", "authors": ["Francesco Vinci", "Gyunam Park", "Wil van der Aalst", "Massimiliano de Leoni"], "title": "Online Discovery of Simulation Models for Evolving Business Processes (Extended Version)", "comment": null, "summary": "Business Process Simulation (BPS) refers to techniques designed to replicate\nthe dynamic behavior of a business process. Many approaches have been proposed\nto automatically discover simulation models from historical event logs,\nreducing the cost and time to manually design them. However, in dynamic\nbusiness environments, organizations continuously refine their processes to\nenhance efficiency, reduce costs, and improve customer satisfaction. Existing\ntechniques to process simulation discovery lack adaptability to real-time\noperational changes. In this paper, we propose a streaming process simulation\ndiscovery technique that integrates Incremental Process Discovery with Online\nMachine Learning methods. This technique prioritizes recent data while\npreserving historical information, ensuring adaptation to evolving process\ndynamics. Experiments conducted on four different event logs demonstrate the\nimportance in simulation of giving more weight to recent data while retaining\nhistorical knowledge. Our technique not only produces more stable simulations\nbut also exhibits robustness in handling concept drift, as highlighted in one\nof the use cases.", "AI": {"tldr": "This paper introduces a novel approach that integrates online machine learning with incremental process discovery, enabling real-time, adaptive business process simulations that effectively balance recent changes and historical knowledge.", "motivation": "Existing business process simulation discovery techniques are not adaptable to real-time operational changes, limiting their usefulness in dynamic business environments where processes are frequently refined.", "method": "The paper proposes a streaming process simulation discovery technique that combines Incremental Process Discovery with Online Machine Learning, giving more weight to recent data while preserving historical knowledge.", "result": "Experiments on four event logs show that the proposed method generates more stable simulations and is robust to concept drift, by efficiently adapting to evolving process behaviors.", "conclusion": "The proposed technique advances process simulation by adapting to real-time changes, balancing recent and historical data, and providing robust and stable simulations even under concept drift."}}
{"id": "2506.10051", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10051", "abs": "https://arxiv.org/abs/2506.10051", "authors": ["Md Istiak Hossain Shihab", "Christopher Hundhausen", "Ahsun Tariq", "Summit Haque", "Yunhan Qiao", "Brian Mulanda"], "title": "The Effects of GitHub Copilot on Computing Students' Programming Effectiveness, Efficiency, and Processes in Brownfield Programming Tasks", "comment": "14 pages, 5 figures", "summary": "When graduates of computing degree programs enter the software industry, they\nwill most likely join teams working on legacy code bases developed by people\nother than themselves. In these so-called brownfield software development\nsettings, generative artificial intelligence (GenAI) coding assistants like\nGitHub Copilot are rapidly transforming software development practices, yet the\nimpact of GenAI on student programmers performing brownfield development tasks\nremains underexplored. This paper investigates how GitHub Copilot influences\nundergraduate students' programming performance, behaviors, and understanding\nwhen completing brownfield programming tasks in which they add new code to an\nunfamiliar code base. We conducted a controlled experiment in which 10\nundergraduate computer science students completed highly similar brownfield\ndevelopment tasks with and without Copilot in a legacy web application. Using a\nmixed-methods approach combining performance analysis, behavioral analysis, and\nexit interviews, we found that students completed tasks 35% faster (p < 0.05)\nand made 50% more solution progress p (< 0.05) when using Copilot. Moreover,\nour analysis revealed that, when using Copilot, students spent 11% less time\nmanually writing code (p < 0.05), and 12% less time conducting web searches (p\n< 0.05), providing evidence of a fundamental shift in how they engaged in\nprogramming. In exit interviews, students reported concerns about not\nunderstanding how or why Copilot suggestions work. This research suggests the\nneed for computing educators to develop new pedagogical approaches that\nleverage GenAI assistants' benefits while fostering reflection on how and why\nGenAI suggestions address brownfield programming tasks. Complete study results\nand analysis are presented at https://ghcopilot-icer.github.io/.", "AI": {"tldr": "GitHub Copilot helps students work faster and more efficiently on unfamiliar code, but may hinder their understanding. Teaching methods need to adapt to balance AI benefits with understanding code logic.", "motivation": "Computing students entering the workforce often work with existing code (brownfield development), but the effects of using AI coding assistants like GitHub Copilot in this context are not well understood.", "method": "A controlled experiment with 10 undergraduate computer science students who performed similar brownfield programming tasks both with and without GitHub Copilot, using a mixed-methods approach: performance and behavioral analysis plus exit interviews.", "result": "With Copilot, students completed tasks 35% faster and achieved 50% more solution progress. They spent 11% less time writing code and 12% less time searching the web. However, students expressed concerns about not fully understanding Copilot's code suggestions.", "conclusion": "GenAI coding assistants like Copilot can greatly enhance student productivity and efficiency in brownfield programming tasks, but raise challenges for student understanding. Educators should adapt pedagogy to leverage AI benefits while encouraging deep understanding of AI-generated code."}}
{"id": "2506.10056", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2506.10056", "abs": "https://arxiv.org/abs/2506.10056", "authors": ["Gabriel Orlanski", "Nicholas Roberts", "Aws Albarghouthi", "Frederic Sala"], "title": "Reward Models Enable Scalable Code Verification by Trading Accuracy for Throughput", "comment": "29 pages, 6 figures, code released here:\n  https://github.com/SprocketLab/orm-code-verifier", "summary": "The standard paradigm for solving coding tasks via large language models\n(LLMs) is to generate-then-rank programs, where the latter step uses a verifier\nin the ranking process. The growing consensus is that a comprehensive verifier\n(e.g., a full test suite) should be prioritized over an outcome reward model\n(ORM) whenever possible, with little consideration given to the trade-offs\ninvolved. We aim to challenge this assumption by systematically exploring the\ntradeoff between speed and accuracy. We find that ORMs play a crucial role in\nscaling verification through trading accuracy for speed, even when a\ncomprehensive verifier is available. Their value becomes especially apparent\nwhen used in a generate-prune-then-rank approach, where a faster but less\naccurate verifier removes incorrect solutions prior to ranking -- leading to a\nsystem that is 11.65x faster while only being 8.33% less accurate than the full\ntest suite. We analyze the generate-prune-then-rank approach and show that it\nworks by filtering out incorrect but highly ranked solutions. These findings\nenable the design of scalable and accurate program ranking systems.", "AI": {"tldr": "Using a fast, less accurate verifier for early pruning in program ranking can make the verification process over 11 times faster with only a small drop in accuracy, highlighting the practicality of outcome reward models for scalable and efficient coding task solutions with LLMs.", "motivation": "The prevailing approach for coding tasks with LLMs emphasizes comprehensive verifiers, with little attention paid to speed-accuracy trade-offs. The paper seeks to challenge the assumption that full verifiers should always be prioritized over outcome reward models (ORMs).", "method": "The authors systematically explore the trade-off between speed and accuracy in coding solutions by comparing ORM-based verification and full test suite verification. They particularly analyze a generate-prune-then-rank strategy, where a fast but less accurate verifier (ORM) is used to prune incorrect solutions before ranking.", "result": "The generate-prune-then-rank system, which uses a fast ORM for pruning before a comprehensive verifier for ranking, achieves a speedup of 11.65x while incurring only an 8.33% accuracy loss compared to always using the full test suite.", "conclusion": "Outcome reward models offer significant value in scaling verification, making program ranking much more efficient with minimal accuracy loss, especially when combined with comprehensive verifiers in a staged workflow."}}
{"id": "2506.10021", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10021", "abs": "https://arxiv.org/abs/2506.10021", "authors": ["Jordi de la Torre"], "title": "From Tool Calling to Symbolic Thinking: LLMs in a Persistent Lisp Metaprogramming Loop", "comment": null, "summary": "We propose a novel architecture for integrating large language models (LLMs)\nwith a persistent, interactive Lisp environment. This setup enables LLMs to\ndefine, invoke, and evolve their own tools through programmatic interaction\nwith a live REPL. By embedding Lisp expressions within generation and\nintercepting them via a middleware layer, the system allows for stateful\nexternal memory, reflective programming, and dynamic tool creation. We present\na design framework and architectural principles to guide future implementations\nof interactive AI systems that integrate symbolic programming with neural\nlanguage generation.", "AI": {"tldr": "A new system bridges LLMs with a live Lisp environment, allowing models to program, create tools, and remember state through symbolic interaction, setting the stage for more interactive and capable AI.", "motivation": "Current large language models (LLMs) are limited in their ability to remember state and dynamically extend their capabilities. Integrating them with a persistent programming environment could overcome these limitations.", "method": "The authors embed a large language model within an interactive Lisp environment. By embedding Lisp expressions in LLM outputs and intercepting them with middleware, the system enables programmatic dialogue, stateful memory, and dynamic tool creation via a live REPL.", "result": "The system successfully allows LLMs to define, invoke, and evolve custom tools, providing stateful interaction and programmability. The authors also establish a set of architectural principles and a design framework for combining symbolic programming (Lisp) and neural language generation.", "conclusion": "This architecture offers a promising approach for enhancing LLMs with external memory, dynamic tool use, and reflective programming by integrating them with a symbolic environment. It lays important groundwork for future AI systems combining symbolic and neural computation."}}
{"id": "2506.10204", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10204", "abs": "https://arxiv.org/abs/2506.10204", "authors": ["Andrei Paleyes", "Radzim Sendyka", "Diana Robinson", "Christian Cabrera", "Neil D. Lawrence"], "title": "Prompt Variability Effects On LLM Code Generation", "comment": null, "summary": "Code generation is one of the most active areas of application of Large\nLanguage Models (LLMs). While LLMs lower barriers to writing code and\naccelerate development process, the overall quality of generated programs\ndepends on the quality of given prompts. Specifically, functionality and\nquality of generated code can be sensitive to user's background and familiarity\nwith software development. It is therefore important to quantify LLM's\nsensitivity to variations in the input. To this end we propose a synthetic\nevaluation pipeline for code generation with LLMs, as well as a systematic\npersona-based evaluation approach to expose qualitative differences of LLM\nresponses dependent on prospective user background. Both proposed methods are\ncompletely independent from specific programming tasks and LLMs, and thus are\nwidely applicable. We provide experimental evidence illustrating utility of our\nmethods and share our code for the benefit of the community.", "AI": {"tldr": "The paper introduces new, broadly applicable methods for systematically evaluating how LLM-generated code quality changes based on user background and prompt variations, providing both experimental evidence and public code releases.", "motivation": "The motivation of this paper is to address how the quality and functionality of code generated by Large Language Models (LLMs) depend on the quality of the prompts, particularly influenced by the user's programming knowledge and background. The authors aim to better understand and quantify LLMs' sensitivity to these variations.", "method": "The paper proposes two main methods: (1) a synthetic evaluation pipeline for code generation with LLMs; and (2) a systematic, persona-based evaluation approach that examines how LLM responses qualitatively differ based on variations in prospective user backgrounds. These methods are designed to be independent of both specific programming tasks and particular LLM models, making them broadly applicable.", "result": "Experimental results illustrate the practical value of both the synthetic evaluation and persona-based approaches in uncovering sensitivity and qualitative differences in LLM-generated code. The authors also make their code available to the community.", "conclusion": "The study concludes that the presented evaluation frameworks are effective at systematically quantifying and exposing how LLM code generation varies depending on prompt and user background. These widely applicable methods can help improve evaluation standards and understanding of LLM behavior in code generation tasks."}}
{"id": "2506.10026", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2506.10026", "abs": "https://arxiv.org/abs/2506.10026", "authors": ["Tesla Zhang", "Sonya Simkin", "Rui Li", "Yue Yao", "Stephanie Balzer"], "title": "A Language-Agnostic Logical Relation for Message-Passing Protocols", "comment": "19 pages, 8 figures", "summary": "Today's computing landscape has been gradually shifting to applications\ntargeting distributed and *heterogeneous* systems, such as cloud computing and\nInternet of Things (IoT) applications. These applications are predominantly\n*concurrent*, employ *message-passing*, and interface with *foreign objects*,\nranging from externally implemented code to actual physical devices such as\nsensors. Verifying that the resulting systems adhere to the intended protocol\nof interaction is challenging -- the usual assumption of a common\nimplementation language, let alone a type system, no longer applies, ruling out\nany verification method based on them. This paper develops a framework for\ncertifying *protocol compliance* of heterogeneous message-passing systems. It\ncontributes the first mechanization of a *language-agnostic logical relation*,\nasserting that its inhabitants comply with the protocol specified. This\ndefinition relies entirely on a labelled transition-based semantics,\naccommodating arbitrary inhabitants, typed and untyped alike, including foreign\nobjects. As a case study, the paper considers two scenarios: (1) *per-instance\nverification* of a specific application or hardware device, and (2)\n*once-and-for-all verification* of well-typed applications for a given type\nsystem. The logical relation and both scenarios are mechanized in the Coq\ntheorem prover.", "AI": {"tldr": "This paper introduces and mechanizes a language-agnostic verification framework for certifying protocol compliance in heterogeneous, message-passing systems, overcoming the limitations of traditional language or type system-based approaches.", "motivation": "Modern distributed and heterogeneous systems, such as those used in cloud computing and IoT, often involve concurrent, message-passing applications interacting with diverse and sometimes foreign components. Traditional verification, which relies on shared language or type system assumptions, is not applicable in these environments, motivating the need for new verification frameworks.", "method": "The paper introduces a new framework based on a labelled transition-based semantics to define a language-agnostic logical relation for protocol compliance. This framework is flexible enough to include both typed and untyped components, as well as foreign objects. The authors mechanize their definition and verification scenarios in the Coq theorem prover.", "result": "The framework supports certifying protocol compliance in two different ways: (1) verifying compliance for specific application or hardware device instances, and (2) universally verifying all well-typed applications under a given type system. Both approaches are demonstrated and mechanized in Coq.", "conclusion": "The work provides the first mechanized language-agnostic logical relation for protocol verification in heterogeneous message-passing systems, enabling protocol compliance certification even when usual language or type system assumptions do not hold."}}
{"id": "2506.10280", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.10280", "abs": "https://arxiv.org/abs/2506.10280", "authors": ["Samiha Shimmi", "Hamed Okhravi", "Mona Rahimi"], "title": "AI-Based Software Vulnerability Detection: A Systematic Literature Review", "comment": null, "summary": "Software vulnerabilities in source code pose serious cybersecurity risks,\nprompting a shift from traditional detection methods (e.g., static analysis,\nrule-based matching) to AI-driven approaches. This study presents a systematic\nreview of software vulnerability detection (SVD) research from 2018 to 2023,\noffering a comprehensive taxonomy of techniques, feature representations, and\nembedding methods. Our analysis reveals that 91% of studies use AI-based\nmethods, with graph-based models being the most prevalent. We identify key\nlimitations, including dataset quality, reproducibility, and interpretability,\nand highlight emerging opportunities in underexplored techniques such as\nfederated learning and quantum neural networks, providing a roadmap for future\nresearch.", "AI": {"tldr": "This paper systematically reviews recent AI-based software vulnerability detection methods, finds graph-based models most common, notes limitations like dataset quality, and points to future opportunities in emerging AI technologies.", "motivation": "Traditional software vulnerability detection methods are limited, prompting a need to understand and classify recent AI-driven approaches for improved cybersecurity.", "method": "A systematic review of SVD literature from 2018-2023, including taxonomy creation and analysis of methods and trends in the field.", "result": "91% of studies reviewed use AI-based methods, with graph-based models most common. Key limitations and opportunities for less-explored techniques are identified.", "conclusion": "The study concludes that AI-based methods dominate software vulnerability detection research, especially graph-based models. However, limitations like dataset quality and interpretability persist, and there are promising future directions in emerging technologies."}}
{"id": "2506.10781", "categories": ["cs.PL", "68N15, 68U35", "D.3.0; K.3.2"], "pdf": "https://arxiv.org/pdf/2506.10781", "abs": "https://arxiv.org/abs/2506.10781", "authors": ["Zhiyao Zhong", "Cyrus Omar"], "title": "Hazel Deriver: A Live Editor for Constructing Rule-Based Derivations", "comment": "5 pages, 2 figures, includes a preliminary user study; intended for\n  computer science education and PL/HCI conference audiences", "summary": "Students in programming languages and formal logic courses often struggle\nwith constructing rule-based derivation trees due to the complexity of applying\ninference rules, the lack of immediate feedback, and the manual effort required\nfor handwritten proofs. We present Hazel Deriver, a live, web-based editor\ndesigned to scaffold derivation construction through multiple layers of\nsupport. Built on the Hazel live programming environment, it provides a\nstructured, interactive experience that encourages iterative exploration and\nreal-time feedback. A preliminary user study with former students suggests that\nHazel Deriver reduces the perceived difficulty of derivation tasks while\nimproving conceptual understanding and engagement. We discuss the design of its\nlayered scaffolding features and raise questions about balancing system\nguidance with learner autonomy.", "AI": {"tldr": "Hazel Deriver is a web-based tool that helps students construct rule-based derivation trees more easily and effectively, providing interactive support and real-time feedback. Initial studies show it lowers difficulty and boosts understanding, but questions remain about the optimal amount of system guidance.", "motivation": "Students in programming languages and formal logic often find it challenging to build rule-based derivation trees due to complex inference rules, insufficient feedback, and the tedious nature of handwritten proofs.", "method": "The authors introduce Hazel Deriver, a web-based, live editor that scaffolds user derivation construction through interactive and structured support. Built on the Hazel environment, it offers real-time feedback and encourages exploration. They conducted a preliminary user study with students to evaluate its effectiveness.", "result": "The user study indicates that Hazel Deriver reduces the perceived difficulty of derivation tasks and enhances students' conceptual understanding and engagement.", "conclusion": "Hazel Deriver effectively assists students in constructing derivation trees by offering multiple layers of support and feedback. The paper also highlights the importance of balancing automated guidance with user independence."}}
{"id": "2506.10322", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10322", "abs": "https://arxiv.org/abs/2506.10322", "authors": ["Xueying Du", "Kai Yu", "Chong Wang", "Yi Zou", "Wentai Deng", "Zuoyu Ou", "Xin Peng", "Lingming Zhang", "Yiling Lou"], "title": "Minimizing False Positives in Static Bug Detection via LLM-Enhanced Path Feasibility Analysis", "comment": null, "summary": "Static bug analyzers play a crucial role in ensuring software quality.\nHowever, existing analyzers for bug detection in large codebases often suffer\nfrom high false positive rates. This is primarily due to the limited\ncapabilities of analyzers in path feasibility validation with multiple\nconditional branches and complex data dependencies. While current LLM-based\napproaches attempt to address this issue, their effectiveness remains limited\ndue to insufficient constraint cascade analysis and scalability challenges in\nlarge projects. To address this challenge, we propose an iterative path\nfeasibility analysis framework LLM4PFA. By leveraging LLM agent based targeted\nconstraint reasoning, and key context-aware analysis driven by agent planning,\nLLM4PFA effectively enhances complex inter-procedural path feasibility analysis\nfor minimizing false positives in static bug detection. Evaluation results show\nthat LLM4PFA precisely filters out 72% to 96% false positives reported during\nstatic bug detection, significantly outperforming all the baselines by 41.1% -\n105.7% improvements; meanwhile LLM4PFA only misses 3 real bugs of 45 true\npositives.", "AI": {"tldr": "LLM4PFA is a new LLM-based static bug analysis framework that dramatically reduces false positives (up to 96%) in large codebases compared to previous methods, with minimal loss in true bug detection.", "motivation": "Existing static bug analyzers struggle with high false positive rates, especially in large codebases with complex control and data dependencies. Current solutions, including LLM-based approaches, are limited by poor constraint analysis and scalability issues.", "method": "The paper proposes LLM4PFA, an iterative path feasibility analysis framework that leverages LLM agents for targeted constraint reasoning and context-aware analysis. The agent uses planned, context-driven analysis to better assess path feasibility, aiming to reduce false positives in static bug detection.", "result": "LLM4PFA effectively filters out 72% to 96% of false positives in static bug detection, significantly outperforming baseline methods with 41.1% to 105.7% improvements. The system only misses 3 real bugs out of 45 true positives.", "conclusion": "LLM4PFA provides a significant advancement in reducing false positives in static bug detection through sophisticated, scalable LLM-driven feasibility analysis, confirming its effectiveness and precision through strong evaluation results."}}
{"id": "2506.10913", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2506.10913", "abs": "https://arxiv.org/abs/2506.10913", "authors": ["Ashley Samuelson", "Andrew K. Hirsch", "Ethan Cecchetti"], "title": "Choreographic Quick Changes: First-Class Location (Set) Polymorphism", "comment": "In submission to OOPSLA 2025", "summary": "Choreographic programming is a promising new paradigm for programming\nconcurrent systems where a developer writes a single centralized program that\ncompiles to individual programs for each node. Existing choreographic\nlanguages, however, lack critical features integral to modern systems, like the\nability of one node to dynamically compute who should perform a computation and\nsend that decision to others. This work addresses this gap with $\\lambda_{QC}$,\nthe first typed choreographic language with \\emph{first class process names}\nand polymorphism over both types and (sets of) locations. $\\lambda_{QC}$ also\nimproves expressive power over previous work by supporting algebraic and\nrecursive data types as well as multiply-located values. We formalize and\nmechanically verify our results in Rocq, including the standard choreographic\nguarantee of deadlock freedom.", "AI": {"tldr": "\u03bb_{QC} is a new choreographic language for concurrent systems that allows dynamic assignment of computations, first class process names, and advanced type features, all within a mechanically verified framework ensuring deadlock freedom.", "motivation": "Existing choreographic programming languages, though useful for concurrent systems, lack support for dynamic assignment of computation (letting a node compute and inform others who should perform which tasks) as well as features important for modern applications like process name handling and polymorphism over locations.", "method": "The paper introduces a new choreographic language, called \u03bb_{QC}, which adds first class process names and polymorphism over types and sets of locations. The language supports algebraic and recursive data types and values associated with multiple locations. The work is formalized and mechanically verified in the Rocq system.", "result": "\u03bb_{QC} provides greater expressive power than previous choreographic languages. Its features allow a node to dynamically compute and communicate computational assignments. The authors prove standard guarantees, including deadlock freedom, within a verified framework.", "conclusion": "The introduction of \u03bb_{QC} significantly enhances choreographic programming for concurrent systems by supporting dynamic process assignment and extending the language with modern features, all with formally verified guarantees."}}
{"id": "2506.10330", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10330", "abs": "https://arxiv.org/abs/2506.10330", "authors": ["Seyed Moein Abtahi", "Akramul Azim"], "title": "Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements", "comment": "Accepted at FORGE 2025", "summary": "This study examined code issue detection and revision automation by\nintegrating Large Language Models (LLMs) such as OpenAI's GPT-3.5 Turbo and\nGPT-4o into software development workflows. A static code analysis framework\ndetects issues such as bugs, vulnerabilities, and code smells within a\nlarge-scale software project. Detailed information on each issue was extracted\nand organized to facilitate automated code revision using LLMs. An iterative\nprompt engineering process is applied to ensure that prompts are structured to\nproduce accurate and organized outputs aligned with the project requirements.\nRetrieval-augmented generation (RAG) is implemented to enhance the relevance\nand precision of the revisions, enabling LLM to access and integrate real-time\nexternal knowledge. The issue of LLM hallucinations - where the model generates\nplausible but incorrect outputs - is addressed by a custom-built \"Code\nComparison App,\" which identifies and corrects erroneous changes before\napplying them to the codebase. Subsequent scans using the static code analysis\nframework revealed a significant reduction in code issues, demonstrating the\neffectiveness of combining LLMs, static analysis, and RAG to improve code\nquality, streamline the software development process, and reduce time and\nresource expenditure.", "AI": {"tldr": "This paper demonstrates that integrating LLMs, static analysis, and RAG enhances automated code issue detection and correction, significantly improving code quality and efficiency while minimizing errors and resource use.", "motivation": "The motivation is to enhance code quality and streamline software development by automating code issue detection and revision, reducing manual labor and errors.", "method": "The method integrates Large Language Models (LLMs), notably GPT-3.5 Turbo and GPT-4o, into static code analysis workflows. The process extracts and structures code issues, employs iterative prompt engineering for LLM-based revisions, uses retrieval-augmented generation (RAG) for better accuracy, and relies on a custom 'Code Comparison App' to detect and correct LLM hallucinations before applying changes.", "result": "Static code analysis after applying LLM-based revisions showed a significant reduction in code issues, indicating improved code quality and development efficiency.", "conclusion": "Combining LLMs, static analysis, and RAG effectively automates the detection and correction of code issues, reduces hallucinations, and leads to higher code quality and resource savings in software projects."}}
{"id": "2506.10365", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10365", "abs": "https://arxiv.org/abs/2506.10365", "authors": ["Shuyang Hou", "Zhangxiao Shen", "Huayi Wu", "Haoyue Jiao", "Ziqi Liu", "Lutong Xie", "Chang Liu", "Jianyuan Liang", "Yaxian Qing", "Xiaopu Zhang", "Dehua Peng", "Zhipeng Gui", "Xuefeng Guan"], "title": "AutoGEEval++: A Multi-Level and Multi-Geospatial-Modality Automated Evaluation Framework for Large Language Models in Geospatial Code Generation on Google Earth Engine", "comment": null, "summary": "Geospatial code generation is becoming a key frontier in integrating\nartificial intelligence with geo-scientific analysis, yet standardised\nautomated evaluation tools for this task remain absent. This study presents\nAutoGEEval++, an enhanced framework building on AutoGEEval, and the first\nautomated assessment system for large language models (LLMs) generating\ngeospatial code on Google Earth Engine (GEE). It supports diverse data\nmodalities and varying task complexities. Built on the GEE Python API,\nAutoGEEval++ features a benchmark dataset-AutoGEEval++-Bench-with 6,365 test\ncases across 26 data types and three task categories: unit, combo, and theme\ntests. It includes a submission programme and a judge module to realise an\nend-to-end automated evaluation pipeline from code generation to\nexecution-based validation. The framework adopts multi-dimensional\nmetrics-accuracy, resource usage, run-time efficiency, and error\ntypes-balancing hallucination control and efficiency, and enabling boundary\ntesting and error pattern analysis. Using AutoGEEval++, we evaluate 24\nstate-of-the-art LLMs (as of June 2025), including general-purpose,\nreasoning-enhanced, code-centric, and geoscience-specific models. Results\nreveal clear performance, stability, and error differences across task types,\nmodel designs, and deployment settings, confirming AutoGEEval++'s practical\nvalue and scalability in vertical-domain code generation. This work establishes\nthe first standardised evaluation protocol and foundational benchmark for\nGEE-based LLM code generation, providing a unified basis for performance\ncomparison and a methodological framework for systematic, domain-specific code\nevaluation.", "AI": {"tldr": "AutoGEEval++ is a new automated framework and benchmark for evaluating LLM-generated geospatial code on Google Earth Engine, enabling standardized, multi-dimensional, and scalable assessment of 24 top LLMs, and setting the groundwork for future research in this domain.", "motivation": "While geospatial code generation is increasingly important for integrating AI with geo-scientific tasks, there is a lack of standardized and automated evaluation tools to assess large language models (LLMs) in this domain.", "method": "The paper introduces AutoGEEval++, an improved framework over AutoGEEval, which systematically evaluates LLMs that generate code for Google Earth Engine (GEE). AutoGEEval++ uses a Python API-based pipeline that includes a benchmark dataset with 6,365 test cases across 26 data types and three levels of task complexity. It offers automated submission, judging, and execution-based validation as well as multi-dimensional evaluation metrics (accuracy, resource usage, run-time, error types).", "result": "AutoGEEval++ is used to evaluate 24 cutting-edge LLMs from various categories (general, reasoning, code-focused, geoscience-specific). Testing revealed distinct differences in model performance and error rates based on task type and model design, demonstrating the practical effectiveness and scalability of the framework.", "conclusion": "AutoGEEval++ establishes the first standardized and systematic benchmark and evaluation protocol for LLM-generated GEE code, providing a robust foundation and unified methodology for fair performance comparisons and domain-specific code evaluation."}}
{"id": "2506.10803", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2506.10803", "abs": "https://arxiv.org/abs/2506.10803", "authors": ["Ryan Gibb", "Patrick Ferris", "David Allsopp", "Michael Winston Dales", "Mark Elvers", "Thomas Gazagnaire", "Sadiq Jaffer", "Thomas Leonard", "Jon Ludlam", "Anil Madhavapeddy"], "title": "Solving Package Management via Hypergraph Dependency Resolution", "comment": "Submitted to SPLASH 2025", "summary": "Package managers are everywhere, with seemingly every language and operating\nsystem implementing their own solution. The lack of interoperability between\nthese systems means that multi-lingual projects are unable to express precise\ndependencies across language ecosystems, and external system and hardware\ndependencies are typically implicit and unversioned. We define HyperRes, a\nformal system for describing versioned dependency resolution using a hypergraph\nthat is expressive enough to model many ecosystems and solve dependency\nconstraints across them. We define translations from dozens of existing package\nmanagers to HyperRes and comprehensively demonstrate that dependency resolution\ncan work across ecosystems that are currently distinct. This does not require\nusers to shift their choice of package managers; instead, HyperRes allows for\nthe translation of packaging metadata between ecosystems, and for solving to be\nprecisely specialised to a particular deployment environment.", "AI": {"tldr": "HyperRes is a new formal system that models and resolves dependencies across different package management ecosystems, enabling cross-language and cross-platform compatibility without changing existing workflows.", "motivation": "Package managers for different languages and operating systems lack interoperability, making it difficult for multi-lingual projects to express and manage dependencies across ecosystems. Additionally, system and hardware dependencies are often implicit and not versioned.", "method": "The authors introduce HyperRes, a formal system based on a hypergraph model, to describe and resolve versioned dependencies. They also develop translations from many existing package managers to HyperRes, allowing cross-ecosystem dependency resolution.", "result": "The approach is demonstrated across dozens of package management ecosystems. The authors show that it is possible to resolve dependencies across currently distinct systems and ecosystems without requiring users to abandon their preferred package managers.", "conclusion": "HyperRes enables cross-ecosystem dependency resolution by translating packaging metadata between various systems. It allows for precise, environment-specific resolution without necessitating a change in how users currently manage their packages."}}
{"id": "2506.10376", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.10376", "abs": "https://arxiv.org/abs/2506.10376", "authors": ["Fan Wu", "Cuiyun Gao", "Shuqing Li", "Xin-Cheng Wen", "Qing Liao"], "title": "MLLM-Based UI2Code Automation Guided by UI Layout Information", "comment": "Accepted by the 34th International Symposium on Software Testing and\n  Analysis (ISSTA 2025)", "summary": "Converting user interfaces into code (UI2Code) is a crucial step in website\ndevelopment, which is time-consuming and labor-intensive. The automation of\nUI2Code is essential to streamline this task, beneficial for improving the\ndevelopment efficiency. There exist deep learning-based methods for the task;\nhowever, they heavily rely on a large amount of labeled training data and\nstruggle with generalizing to real-world, unseen web page designs. The advent\nof Multimodal Large Language Models (MLLMs) presents potential for alleviating\nthe issue, but they are difficult to comprehend the complex layouts in UIs and\ngenerate the accurate code with layout preserved. To address these issues, we\npropose LayoutCoder, a novel MLLM-based framework generating UI code from\nreal-world webpage images, which includes three key modules: (1) Element\nRelation Construction, which aims at capturing UI layout by identifying and\ngrouping components with similar structures; (2) UI Layout Parsing, which aims\nat generating UI layout trees for guiding the subsequent code generation\nprocess; and (3) Layout-Guided Code Fusion, which aims at producing the\naccurate code with layout preserved. For evaluation, we build a new benchmark\ndataset which involves 350 real-world websites named Snap2Code, divided into\nseen and unseen parts for mitigating the data leakage issue, besides the\npopular dataset Design2Code. Extensive evaluation shows the superior\nperformance of LayoutCoder over the state-of-the-art approaches. Compared with\nthe best-performing baseline, LayoutCoder improves 10.14% in the BLEU score and\n3.95% in the CLIP score on average across all datasets.", "AI": {"tldr": "LayoutCoder is a new framework using multimodal large language models to automatically generate accurate website code from UI images, substantially improving over existing methods in both standard and new benchmarks.", "motivation": "Automating the process of converting user interfaces (UIs) into code is vital for increasing website development efficiency, but existing deep learning methods require large labeled datasets and struggle to generalize to unseen web designs. Multimodal Large Language Models (MLLMs) could help, but face challenges in comprehending complex layouts and generating accurate, layout-preserving code.", "method": "The paper proposes LayoutCoder, a novel framework based on MLLMs for UI code generation from real-world webpage images. LayoutCoder includes: (1) Element Relation Construction (identifies and groups structurally similar components to capture the UI layout); (2) UI Layout Parsing (generates UI layout trees to guide code generation); (3) Layout-Guided Code Fusion (produces accurate code with layout preserved). The authors also built Snap2Code, a new benchmark dataset of 350 real websites, divided into seen and unseen to prevent data leakage, alongside using Design2Code for evaluation.", "result": "LayoutCoder outperforms state-of-the-art methods, achieving a 10.14% improvement in BLEU score and a 3.95% gain in CLIP score on average across all datasets compared to the best-performing baseline.", "conclusion": "LayoutCoder, with its novel design focused on layout understanding and code generation from website images, effectively improves the accuracy and generalizability of automated UI2Code conversion, outperforming previous methods on benchmark datasets."}}
{"id": "2506.10397", "categories": ["cs.SE", "cs.CY", "cs.DC"], "pdf": "https://arxiv.org/pdf/2506.10397", "abs": "https://arxiv.org/abs/2506.10397", "authors": ["Mir Mohammad Yousuf", "Shabir Ahmad Sofi"], "title": "Bug Classification in Quantum Software: A Rule-Based Framework and Its Evaluation", "comment": "25 pages, 5 figures", "summary": "Accurate classification of software bugs is essential for improving software\nquality. This paper presents a rule-based automated framework for classifying\nissues in quantum software repositories by bug type, category, severity, and\nimpacted quality attributes, with additional focus on quantum-specific bug\ntypes. The framework applies keyword and heuristic-based techniques tailored to\nquantum computing. To assess its reliability, we manually classified a\nstratified sample of 4,984 issues from a dataset of 12,910 issues across 36\nQiskit repositories. Automated classifications were compared with ground truth\nusing accuracy, precision, recall, and F1-score. The framework achieved up to\n85.21% accuracy, with F1-scores ranging from 0.7075 (severity) to 0.8393\n(quality attribute). Statistical validation via paired t-tests and Cohen's\nKappa showed substantial to almost perfect agreement for bug type (k = 0.696),\ncategory (k = 0.826), quality attribute (k = 0.818), and quantum-specific bug\ntype (k = 0.712). Severity classification showed slight agreement (k = 0.162),\nsuggesting room for improvement. Large-scale analysis revealed that classical\nbugs dominate (67.2%), with quantum-specific bugs at 27.3%. Frequent bug\ncategories included compatibility, functional, and quantum-specific defects,\nwhile usability, maintainability, and interoperability were the most impacted\nquality attributes. Most issues (93.7%) were low severity; only 4.3% were\ncritical. A detailed review of 1,550 quantum-specific bugs showed that over\nhalf involved quantum circuit-level problems, followed by gate errors and\nhardware-related issues.", "AI": {"tldr": "A rule-based framework effectively classifies quantum software bugs by type, severity, and impacted qualities, achieving strong accuracy except for severity. Most bugs are classical and low-severity, but quantum-specific defects often involve circuit-level issues.", "motivation": "There is a need for accurate and automated classification of software bugs specifically in quantum software repositories, due to the increasing complexity and specialized nature of quantum computing. Better classification helps improve overall software quality and maintenance.", "method": "The paper introduces a rule-based, automated classification framework using keyword and heuristic-based techniques tailored for quantum computing bugs. The framework classifies issues by type, category, severity, and impacted quality attributes, with a special focus on quantum-specific bugs. It was evaluated by manually classifying a stratified sample of issues and comparing automated results to this ground truth using several performance metrics and statistical tests.", "result": "The automated framework achieved up to 85.21% accuracy with F1-scores ranging from 0.7075 (severity) to 0.8393 (quality attribute). Cohen's Kappa showed substantial agreement for bug type, category, and quality attribute, but only slight agreement for severity classification. Classical bugs were more prevalent than quantum-specific bugs, and most reported issues were of low severity. Quantum circuit-level problems were the most common in quantum-specific bugs.", "conclusion": "The proposed automated framework is reliable for most classification tasks in quantum software repositories, except for severity classification, which requires further improvement. The framework revealed significant trends in bug distribution and defect types, highlighting the dominance of classical bugs and the prevalence of circuit-level problems in quantum-related defects."}}
{"id": "2506.10426", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10426", "abs": "https://arxiv.org/abs/2506.10426", "authors": ["Xiao Yu", "Haoxuan Chen", "Feifei Niu", "Xing Hu", "Jacky Wai Keung", "Xin Xia"], "title": "Towards Understanding Bugs in Distributed Training and Inference Frameworks for Large Language Models", "comment": null, "summary": "With the rapid development of large language models (LLMs), distributed\ntraining and inference frameworks like DeepSpeed have become essential for\nscaling model training and inference across multiple GPUs or nodes. However,\nthe increasing complexity of these frameworks brings non-trivial software bugs,\nwhich may degrade training performance, cause unexpected failures, and result\nin significant resource waste. Understanding framework bugs' characteristics is\nfundamental for quality assurance, allowing the design of more effective\ndebugging and repair methods. Thus, our paper conducts the first large-scale\nempirical analysis of 308 fixed bugs across three popular distributed\ntraining/inference frameworks: DeepSpeed, Megatron-LM, and Colossal-AI. We\nexamine bug symptoms, root causes, bug identification and fixing efforts, and\ncommon low-effort fixing strategies. Additionally, the distributed nature of\nthese frameworks introduces unique bug root causes, such as allocation strategy\nerror and distributed communication error. Diagnosing and fixing complex bugs\nremains challenging due to factors like the disconnect between symptoms and\nroot causes, high bug reproduction costs, and low-level or cross-component\ninteractions. Interestingly, we observe that 48% of bug fixes require minimal\ncode changes (<=10 LOC) and follow simple strategies such as conditional logic\noptimization, parameter handling enhancement, or version compatibility\nhandling, indicating potential for automation. Based on these insights, we\noffer several implications for improving the reliability of both distributed\ntraining and inference frameworks and their dependent LLM projects, while also\nidentifying opportunities to leverage LLM-based tools for automated debugging\nand repair.", "AI": {"tldr": "This paper empirically analyzes 308 bugs in popular distributed LLM training/inference frameworks, finding that half of fixes are simple, but diagnosing bugs often remains hard. The results suggest opportunities for automated bug fixing and improved framework reliability using these insights.", "motivation": "Distributed training and inference frameworks are crucial for scaling large language model development, but their increasing software complexity introduces bugs that can degrade performance and waste resources. Understanding these bugs is key to improving software reliability and enabling better debugging tools.", "method": "The authors conduct a large-scale empirical analysis of 308 fixed bugs in three major distributed framework\u2014DeepSpeed, Megatron-LM, and Colossal-AI. They examine bug symptoms, root causes, efforts required for identification and fixing, and common strategies applied in bug resolution.", "result": "The study finds that distributed frameworks exhibit unique bug causes like allocation strategy and communication errors. Many fixes (48%) are simple (\u226410 lines of code), relying on straightforward strategies. However, diagnosing and resolving bugs is often difficult due to the disconnect between symptoms and causes, high effort needed for bug reproduction, and complicated interactions across system components.", "conclusion": "The paper highlights opportunities to automate bug fixing in distributed frameworks, thanks to the simplicity of nearly half of observed fixes. Insights from this study suggest directions for building better debugging tools\u2014including those powered by LLMs\u2014and recommend improvements for the reliability of both distributed frameworks and dependent LLM projects."}}
{"id": "2506.10484", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10484", "abs": "https://arxiv.org/abs/2506.10484", "authors": ["Fangwen Mu", "Junjie Wang", "Lin Shi", "Song Wang", "Shoubin Li", "Qing Wang"], "title": "EXPEREPAIR: Dual-Memory Enhanced LLM-based Repository-Level Program Repair", "comment": null, "summary": "Automatically repairing software issues remains a fundamental challenge at\nthe intersection of software engineering and AI. Although recent advancements\nin Large Language Models (LLMs) have demonstrated potential for\nrepository-level repair tasks, current methodologies exhibit two notable\nlimitations: (1) they often address issues in isolation, neglecting to\nincorporate insights from previously resolved issues, and (2) they rely on\nstatic and rigid prompting strategies, which constrain their ability to\ngeneralize across diverse and evolving issue scenarios. Inspired by the dual\nmemory systems of human cognition, where episodic and semantic memories work\nsynergistically to support human reasoning and decision-making, we propose\nExpeRepair, a novel LLM-based approach that continuously learns from historical\nrepair experiences through dual-channel knowledge accumulation. ExpeRepair\norganizes historical repair experiences into two complementary memories: an\nepisodic memory that stores concrete repair demonstrations, and a semantic\nmemory that encodes abstract reflective insights. At inference time, ExpeRepair\nactivates both memory systems by retrieving relevant demonstrations from\nepisodic memory and recalling high-level repair insights from semantic memory.\nIt further enhances adaptability through dynamic prompt composition,\nsynergistically integrating both memory types to replace static prompts with\ncontext-aware, experience-driven prompts. Experiments on the SWE-bench Lite\nbenchmark demonstrate that ExpeRepair achieves a pass@1 score of 49.3% with\nClaude 3.7 Sonnet, outperforming all state-of-the-art open-source methods.", "AI": {"tldr": "ExpeRepair leverages both concrete and abstract past repair knowledge with context-driven prompting, significantly improving LLM-based code repair over existing approaches.", "motivation": "Automatic software repair is challenging, and current LLM-based methods have two key limitations: they don't learn from previous repairs and use static prompts that restrict adaptability.", "method": "The paper introduces ExpeRepair, an LLM-based approach inspired by human memory systems. It uses episodic memory to store concrete past repair instances and semantic memory to store abstract insights. At inference, it retrieves both concrete and high-level memories and composes dynamic, context-driven prompts.", "result": "ExpeRepair, evaluated on the SWE-bench Lite benchmark with Claude 3.7 Sonnet, achieves a pass@1 score of 49.3%, outperforming all current open-source methods.", "conclusion": "Incorporating dual-memory systems and dynamic prompt construction enables LLMs to better generalize and adapt for software repair, leading to state-of-the-art performance."}}
{"id": "2506.10501", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10501", "abs": "https://arxiv.org/abs/2506.10501", "authors": ["Surya Jasper", "Minh Luu", "Evan Pan", "Aakash Tyagi", "Michael Quinn", "Jiang Hu", "David Kebo Houngninou"], "title": "BugGen: A Self-Correcting Multi-Agent LLM Pipeline for Realistic RTL Bug Synthesis", "comment": null, "summary": "Hardware complexity continues to strain verification resources, motivating\nthe adoption of machine learning (ML) methods to improve debug efficiency.\nHowever, ML-assisted debugging critically depends on diverse and scalable bug\ndatasets, which existing manual or automated bug insertion methods fail to\nreliably produce. We introduce BugGen, a first of its kind, fully autonomous,\nmulti-agent pipeline leveraging Large Language Models (LLMs) to systematically\ngenerate, insert, and validate realistic functional bugs in RTL. BugGen\npartitions modules, selects mutation targets via a closed-loop agentic\narchitecture, and employs iterative refinement and rollback mechanisms to\nensure syntactic correctness and functional detectability. Evaluated across\nfive OpenTitan IP blocks, BugGen produced 500 unique bugs with 94% functional\naccuracy and achieved a throughput of 17.7 validated bugs per hour-over five\ntimes faster than typical manual expert insertion. Additionally, BugGen\nidentified 104 previously undetected bugs in OpenTitan regressions,\nhighlighting its utility in exposing verification coverage gaps. Compared\nagainst Certitude, BugGen demonstrated over twice the syntactic accuracy,\ndeeper exposure of testbench blind spots, and more functionally meaningful and\ncomplex bug scenarios. Furthermore, when these BugGen-generated datasets were\nemployed to train ML-based failure triage models, we achieved high\nclassification accuracy (88.1%-93.2%) across different IP blocks, confirming\nthe practical utility and realism of generated bugs. BugGen thus provides a\nscalable solution for generating high-quality bug datasets, significantly\nenhancing verification efficiency and ML-assisted debugging.", "AI": {"tldr": "BugGen is an autonomous LLM-driven system that quickly and accurately generates realistic hardware bugs, enhancing both coverage and efficiency in hardware verification and ML-based debugging.", "motivation": "Due to increasing hardware complexity, verification resources are under pressure, and machine learning methods for debugging require diverse, scalable bug datasets. Existing ways to create such datasets (manual or automated bug insertion) are inadequate.", "method": "The authors introduce BugGen, an autonomous, multi-agent pipeline using Large Language Models (LLMs) to automatically generate, insert, and validate realistic functional bugs in RTL (Register Transfer Level). BugGen partitions hardware modules, selects mutation targets, and uses iterative refinement and rollback processes to ensure correctness and detectability of inserted bugs.", "result": "BugGen was tested on five OpenTitan IP blocks and produced 500 unique bugs with 94% functional accuracy, at a throughput over five times faster than manual expert insertion. It uncovered 104 previously undetected bugs and outperformed Certitude in syntactic accuracy, testbench blind spot exposure, and production of complex, meaningful bug scenarios. BugGen-generated datasets also helped ML-based debugging models achieve high classification accuracy (88.1%-93.2%).", "conclusion": "BugGen offers a scalable and effective tool for generating realistic hardware bug datasets, improving both verification workflows and the effectiveness of ML-based debugging."}}
{"id": "2506.10525", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10525", "abs": "https://arxiv.org/abs/2506.10525", "authors": ["Junhang Cheng", "Fang Liu", "Chengru Wu", "Li Zhang"], "title": "AdaptiveLLM: A Framework for Selecting Optimal Cost-Efficient LLM for Code-Generation Based on CoT Length", "comment": "Accepted by Internetware 2025", "summary": "While Large Language Models (LLMs) have significantly advanced code\ngeneration efficiency, they face inherent challenges in balancing performance\nand inference costs across diverse programming tasks. Dynamically selecting the\noptimal LLM based on task difficulty and resource constraints offers a\npromising approach to achieve an optimal balance between efficiency and\nperformance. However, existing model selection methods are resource-intensive\nand often neglect cost efficiency. Moreover, these approaches rely on\nhuman-annotated difficulty labels that are frequently inaccessible in\nreal-world settings and may not align with the LLM's own assessment of task\ndifficulty. In this paper, we introduce AdaptiveLLM, a framework that\ndynamically selects optimal LLMs for a given coding task by automatically\nassessing task difficulty. Our framework first estimates task difficulty using\nChain-of-Thought lengths generated by reasoning model, clusters these into\nthree difficulty levels via k-means, and fine-tunes CodeBERT to embed\ndifficulty-aware features. A trained XGBoost classifier then selects the best\nmodel for each problem, optimizing the performance-cost trade-off. Experimental\nresults show that AdaptiveLLM achieves a 7.86% improvement in pass@1 score\nwhile reducing resource consumption by 88.9% compared to baseline method\nComplexityNet. When compared to a single model, AdaptiveLLM demonstrates an\napproximately 15% accuracy improvement, while maintaining the same level of\ncost consumption. Apart from that, the difficulty assessment using CoT provides\nmore reliable selection criteria than human evaluation. Our replication package\nis available at https://github.com/cjhCoder7/AdaptiveLLM.", "AI": {"tldr": "AdaptiveLLM improves code generation by automatically assessing task difficulty and selecting the most suitable LLM, resulting in higher accuracy and much lower resource use compared to baselines and single-model approaches, without needing human-annotated difficulty labels.", "motivation": "Large Language Models (LLMs) are widely used for code generation, but there is a need to balance between computational efficiency (cost) and performance (accuracy) across coding tasks with varying difficulty. Existing methods for selecting the best model are costly and depend on human-created difficulty labels, which are often unavailable and may not reflect the LLMs' real capabilities.", "method": "The authors propose AdaptiveLLM, a system that dynamically assesses coding task difficulty using Chain-of-Thought (CoT) lengths, clusters these into three difficulty levels using k-means, embeds difficulty-aware features via fine-tuned CodeBERT, and employs an XGBoost classifier to choose the optimal LLM for each problem.", "result": "AdaptiveLLM achieves a 7.86% increase in pass@1 score while reducing resource use by 88.9% compared to the ComplexityNet baseline. Versus a single model, it improves accuracy by about 15% with similar cost. The automatic difficulty assessment using CoT is more reliable than human labels.", "conclusion": "AdaptiveLLM enables more efficient and accurate code generation by automatically estimating task difficulty and selecting the best LLM, outperforming baseline and single-model methods both in accuracy and resource use. The framework avoids dependence on human-annotated data and offers better cost-performance trade-offs."}}
{"id": "2506.10624", "categories": ["cs.SE", "cs.AR"], "pdf": "https://arxiv.org/pdf/2506.10624", "abs": "https://arxiv.org/abs/2506.10624", "authors": ["Lukas J\u00fcnger", "Jan Henrik Weinstock", "Tim Kraus"], "title": "Scalable Software Testing in Fast Virtual Platforms: Leveraging SystemC, QEMU and Containerization", "comment": "Published in DVCon China 2025", "summary": "The ever-increasing complexity of HW/SW systems presents a persistent\nchallenge, particularly in safety-critical domains like automotive, where\nextensive testing is imperative. However, the availability of hardware often\nlags behind, hindering early-stage software development. To address this,\nVirtual Platforms (VPs) based on the SystemC TLM-2.0 standard have emerged as a\npivotal solution, enabling pre-silicon execution and testing of unmodified\ntarget software. In this study, we propose an approach leveraging\ncontainerization to encapsulate VPs in order to reduce environment dependencies\nand enable cloud deployment for fast, parallelized test execution, as well as\nopen-source VP technologies such as QEMU and VCML to obviate the need for seat\nlicenses. To demonstrate the efficacy of our approach, we present an Artificial\nIntelligence (AI) accelerator VP case study. Through our research, we offer a\nrobust solution to address the challenges posed by the complexity of HW/SW\nsystems, with practical implications for accelerating HW/SW co-development.", "AI": {"tldr": "Using containerized, open-source virtual platforms for cloud-based, parallel software testing addresses hardware-software co-development challenges, enabling faster and more scalable verification in safety-critical domains.", "motivation": "The increasing complexity of hardware/software (HW/SW) systems, particularly in safety-critical domains like automotive, creates challenges for early software development due to hardware availability lagging behind. Extensive testing is needed, but hardware delays hinder the process.", "method": "The paper proposes using containerization to encapsulate Virtual Platforms (VPs) based on SystemC TLM-2.0. This reduces environment dependencies and enables cloud-based, parallelized test execution. Open-source VP technologies (QEMU and VCML) are used to avoid licensing constraints. The approach is demonstrated via a case study involving an Artificial Intelligence (AI) accelerator VP.", "result": "The proposed approach enables pre-silicon execution and testing of unmodified target software efficiently. By containerizing VPs and leveraging the cloud, the methodology allows fast, parallel testing without expensive licenses. The case study validates the feasibility and effectiveness of this approach.", "conclusion": "Containerized, open-source SystemC TLM-2.0-based VPs solve early-stage HW/SW system testing challenges by enabling scalable, environment-independent, cloud-based testing. This approach supports accelerated HW/SW co-development, especially in domains requiring extensive verification."}}
{"id": "2506.10654", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10654", "abs": "https://arxiv.org/abs/2506.10654", "authors": ["Abir Bouraffa", "Carolin Brandt", "Andy Zaidmann", "Walid Maalej"], "title": "Not One to Rule Them All: Mining Meaningful Code Review Orders From GitHub", "comment": null, "summary": "Developers use tools such as GitHub pull requests to review code, discuss\nproposed changes, and request modifications. While changed files are commonly\npresented in alphabetical order, this does not necessarily coincide with the\nreviewer's preferred navigation sequence. This study investigates the different\nnavigation orders developers follow while commenting on changes submitted in\npull requests. We mined code review comments from 23,241 pull requests in 100\npopular Java and Python repositories on GitHub to analyze the order in which\nthe reviewers commented on the submitted changes. Our analysis shows that for\n44.6% of pull requests, the reviewers comment in a non-alphabetical order.\nAmong these pull requests, we identified traces of alternative meaningful\norders: 20.6% (2,134) followed a largest-diff-first order, 17.6% (1,827) were\ncommented in the order of the files' similarity to the pull request's title and\ndescription, and 29% (1,188) of pull requests containing changes to both\nproduction and test files adhered to a test-first order. We also observed that\nthe proportion of reviewed files to total submitted files was significantly\nhigher in non-alphabetically ordered reviews, which also received slightly\nfewer approvals from reviewers, on average. Our findings highlight the need for\nadditional support during code reviews, particularly for larger pull requests,\nwhere reviewers are more likely to adopt complex strategies rather than\nfollowing a single predefined order.", "AI": {"tldr": "Many reviewers do not follow the alphabetical order of files when reviewing pull requests. Instead, they use strategies such as reviewing the largest changes first or prioritizing test files. These strategies are more common in larger pull requests and suggest code review tools should support more flexible navigation options.", "motivation": "Current code review tools like GitHub present changed files in alphabetical order, though this may not align with how reviewers prefer to navigate changes. Understanding preferred navigation orders can help improve review tools and processes.", "method": "The researchers mined code review comments from 23,241 pull requests across 100 popular Java and Python GitHub repositories. They analyzed the sequence of file reviews and comments to identify common navigation strategies.", "result": "44.6% of pull requests were reviewed in a non-alphabetical order. Within this group, significant proportions used largest-diff-first, similarity-to-title/description, or test-first strategies. Non-alphabetical orders were associated with a higher proportion of reviewed files, but slightly fewer overall approvals.", "conclusion": "The study concludes that developers often adopt complex, meaningful navigation strategies rather than the default alphabetical order, especially for larger pull requests. This suggests a need for better tool support accommodating diverse review workflows."}}
{"id": "2506.10704", "categories": ["cs.SE", "cs.AI", "D.2.1; D.2.4; D.2.10; F.4.1; F.4.3"], "pdf": "https://arxiv.org/pdf/2506.10704", "abs": "https://arxiv.org/abs/2506.10704", "authors": ["Arshad Beg", "Diarmuid O'Donoghue", "Rosemary Monahan"], "title": "Formalising Software Requirements using Large Language Models", "comment": "Accepted and presented as a poster in ADAPT Annual Conference\n  (AACS2025) on 15th of May, 2025", "summary": "This paper is a brief introduction to our recently initiated project named\nVERIFAI: Traceability and verification of natural language requirements. The\nproject addresses the challenges in the traceability and verification of formal\nspecifications through providing support for the automatic generation of the\nformal specifications and the traceability of the requirements from the initial\nsoftware design stage through the systems implementation and verification.\nApproaches explored in this project include Natural Language Processing, use of\nontologies to describe the software system domain, reuse of existing software\nartefacts from similar systems (i.e. through similarity based reuse) and large\nlanguage models to identify and declare the specifications as well as use of\nartificial intelligence to guide the process.", "AI": {"tldr": "VERIFAI is an early-stage project aiming to automate the traceability and verification of natural language software requirements using AI, NLP, ontologies, and LLMs to generate formal specs and track requirements throughout software development.", "motivation": "Traceability and verification of requirements in software systems are difficult because requirements are typically written in natural language, and translating these into formal specifications is complex and error-prone. There is also often a lack of tools supporting automated traceability throughout the system lifecycle.", "method": "The project investigates a combination of Natural Language Processing, ontologies to model domains, similarity-based reuse of existing artefacts, and large language models (LLMs) to automate the identification and formalization of requirements. AI technologies guide the traceability and verification processes.", "result": "The project is in its early stages; initial findings suggest that leveraging NLP, ontologies, LLMs, and AI can potentially automate and improve the traceability and verification of requirements, though concrete results are yet to be detailed.", "conclusion": "Automatic generation of formal specifications and improved traceability are feasible with advances in AI, LLMs, and ontologies. The project lays out foundational methodologies to be developed further for practical application in software engineering."}}
{"id": "2506.10770", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10770", "abs": "https://arxiv.org/abs/2506.10770", "authors": ["Joran Leest", "Claudia Raibulet", "Patricia Lago", "Ilias Gerostathopoulos"], "title": "From Tea Leaves to System Maps: Context-awareness in Monitoring Operational Machine Learning Models", "comment": null, "summary": "Machine learning (ML) models in production do not fail due to statistical\nanomalies in their input data; they fail due to contextual misalignment -- when\ntheir environment deviates from training assumptions, leading to unreliable\npredictions. Effective ML monitoring requires rich contextual information to\nmove beyond detecting statistical shifts toward meaningful alerts and\nsystematic root-cause analysis. Yet, surprisingly, despite extensive research\nin ML monitoring and related disciplines (drift detection, data validation,\nout-of-distribution detection), there is no shared understanding of how to use\ncontextual information -- striking, given that monitoring involves\ninterpretation of information in context. In response, this paper presents a\nsystematic review to characterize and structure the various types of contextual\ninformation in this domain. Our analysis examines 94 primary studies across\ndata mining, databases, software engineering, and ML. We introduce the\nContextual System--Aspect--Representation (C-SAR) framework, a conceptual model\nthat synthesizes our findings. We also identify 20 recurring and potentially\nreusable patterns of specific system, aspect, and representation combinations,\nand map them to the monitoring activities they support. This study provides a\nnew perspective on ML monitoring: from interpreting \"tea leaves\" of\nobservational statistics into constructing and managing \"system maps\" that\nenable systematic and reliable ML monitoring practices.", "AI": {"tldr": "This paper reviews and organizes how contextual information should be used in ML monitoring, proposing the C-SAR framework and identifying practical patterns for better system reliability, moving beyond just spotting statistical anomalies.", "motivation": "Current ML models in production often fail not just due to statistical anomalies but because of contextual misalignments\u2014changes in the environment not anticipated during training. Existing monitoring practices lack a unified understanding or effective use of contextual information, though context is crucial for interpreting performance and diagnosing issues.", "method": "The paper conducts a systematic review of 94 studies from diverse fields like data mining, databases, software engineering, and ML. It characterizes types of contextual information for ML monitoring and introduces the Contextual System--Aspect--Representation (C-SAR) framework to synthesize findings. Additionally, it identifies 20 reusable patterns linking system, aspect, and representation, mapping them to monitoring activities.", "result": "The study introduces C-SAR, a conceptual model to structure contextual information in ML monitoring. It also articulates 20 recurring patterns that can be used across monitoring activities. This work reframes ML monitoring from just detecting statistical anomalies to systematically incorporating context for more reliable and actionable monitoring.", "conclusion": "The paper establishes a new theoretical framework and set of patterns for using contextual information in ML monitoring. This advances practice from surface-level anomaly detection to deep, systematic root-cause analysis and reliable system management. It emphasizes that successful monitoring hinges on context-aware approaches rather than pure statistical observation."}}
{"id": "2506.10785", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10785", "abs": "https://arxiv.org/abs/2506.10785", "authors": ["Vinaik Chhetri", "Krishna Upadhyay", "A. B. Siddique", "Umar Farooq"], "title": "What Users Value and Critique: Large-Scale Analysis of User Feedback on AI-Powered Mobile Apps", "comment": "12 pages, 6 figures, 5 tables", "summary": "Artificial Intelligence (AI)-powered features have rapidly proliferated\nacross mobile apps in various domains, including productivity, education,\nentertainment, and creativity. However, how users perceive, evaluate, and\ncritique these AI features remains largely unexplored, primarily due to the\noverwhelming volume of user feedback. In this work, we present the first\ncomprehensive, large-scale study of user feedback on AI-powered mobile apps,\nleveraging a curated dataset of 292 AI-driven apps across 14 categories with\n894K AI-specific reviews from Google Play. We develop and validate a\nmulti-stage analysis pipeline that begins with a human-labeled benchmark and\nsystematically evaluates large language models (LLMs) and prompting strategies.\nEach stage, including review classification, aspect-sentiment extraction, and\nclustering, is validated for accuracy and consistency. Our pipeline enables\nscalable, high-precision analysis of user feedback, extracting over one million\naspect-sentiment pairs clustered into 18 positive and 15 negative user topics.\nOur analysis reveals that users consistently focus on a narrow set of themes:\npositive comments emphasize productivity, reliability, and personalized\nassistance, while negative feedback highlights technical failures (e.g.,\nscanning and recognition), pricing concerns, and limitations in language\nsupport. Our pipeline surfaces both satisfaction with one feature and\nfrustration with another within the same review. These fine-grained,\nco-occurring sentiments are often missed by traditional approaches that treat\npositive and negative feedback in isolation or rely on coarse-grained analysis.\nTo this end, our approach provides a more faithful reflection of the real-world\nuser experiences with AI-powered apps. Category-aware analysis further uncovers\nboth universal drivers of satisfaction and domain-specific frustrations.", "AI": {"tldr": "This paper presents a validated, scalable pipeline using LLMs to extract fine-grained user sentiments from nearly 900K reviews of 292 AI-powered apps. It reveals that users' satisfaction and frustrations hinge on a few key themes, and demonstrates that nuanced feedback\u2014often missed in prior work\u2014can be systematically surfaced to guide app improvement.", "motivation": "The authors observe a lack of understanding about how users truly perceive and critique AI features in mobile apps due to the massive amount of user feedback, which is difficult to analyze manually or with traditional approaches.", "method": "They curated a large dataset of 894,000 AI-specific user reviews from 292 AI-enabled apps across 14 categories. They then designed and validated a multi-stage analysis pipeline (including human benchmarking, use of LLMs, and prompting strategies) to classify, extract, and cluster aspect-sentiment pairs for high-precision insights.", "result": "Their pipeline extracted over a million aspect-sentiment pairs, sorted into 18 positive and 15 negative user topics. Users consistently focus on productivity, reliability, and personalization positively, whereas they complain most about technical failures, pricing, and language support. Their approach detects nuanced, co-occurring sentiments within single reviews, which traditional methods often miss. Category-wise analysis uncovered both shared and unique user experience drivers.", "conclusion": "The study introduces a scalable, high-precision pipeline for mining large-scale user feedback on AI-powered apps, offering richer and more nuanced insights than previous, more superficial methods. This approach yields a more accurate picture of real user experiences and can help developers improve AI features in mobile apps."}}
{"id": "2506.10833", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10833", "abs": "https://arxiv.org/abs/2506.10833", "authors": ["Fabian C. Pe\u00f1a", "Steffen Herbold"], "title": "Evaluating Large Language Models on Non-Code Software Engineering Tasks", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode understanding and generation; however, their effectiveness on non-code\nSoftware Engineering (SE) tasks remains underexplored. We present the first\ncomprehensive benchmark, which we name `Software Engineering Language\nUnderstanding' (SELU), for evaluating LLMs on 17 non-code tasks, spanning from\nidentifying whether a requirement is functional or non-functional to estimating\nthe effort and complexity of backlog items. SELU covers classification,\nregression, Named Entity Recognition (NER), and Masked Language Modeling (MLM)\ntargets, with data drawn from diverse sources such as code repositories, issue\ntracking systems, and developer forums. We fine-tune 22 open-source LLMs,\nprompt two proprietary alternatives, and train two baselines. Performance is\nmeasured using metrics such as F1-macro, SMAPE, F1-micro, and accuracy, and\ncompared via the Bayesian signed-rank test. Our results show that\nmoderate-scale decoder-only models consistently form a top-tier, exhibiting\nhigh mean performance and low across-task variance, while domain adaptation via\ncode-focused pre-training might yield only modest improvements. These insights\nguide model selection for non-code SE workflows and highlight directions for\nexpanding SELU to generative and design-oriented scenarios.", "AI": {"tldr": "This paper introduces SELU, a benchmark for testing how well language models perform on non-code software engineering tasks. It finds that moderately-sized decoder-only models work best, and that focusing on code during pre-training offers limited advantages for these tasks.", "motivation": "Large Language Models (LLMs) have shown strong capabilities in code tasks, but their utility for non-code Software Engineering (SE) tasks has not been thoroughly evaluated. There is a need for a comprehensive benchmark to analyze and compare LLM performance in such tasks.", "method": "The authors introduce the 'Software Engineering Language Understanding' (SELU) benchmark covering 17 non-code SE tasks, including classification, regression, NER, and MLM. They draw data from various SE-related sources. They evaluate 22 fine-tuned open-source LLMs, 2 proprietary models (prompted), and 2 baseline models, using diverse performance metrics and statistical tests to compare results.", "result": "Moderate-scale decoder-only models achieve the highest and most consistent performance across the tasks, while domain adaptation through code-focused pre-training only provides limited additional benefit.", "conclusion": "The SELU benchmark facilitates robust evaluation and comparison of LLMs on non-code SE tasks, guiding model selection for such workflows. The research also underscores the need to expand the benchmark toward generative and design-oriented tasks."}}
{"id": "2506.10869", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.10869", "abs": "https://arxiv.org/abs/2506.10869", "authors": ["Quinn Thibeault", "Giulia Pedrielli"], "title": "MultiCoSim: A Python-based Multi-Fidelity Co-Simulation Framework", "comment": null, "summary": "Simulation is a foundational tool for the analysis and testing of\ncyber-physical systems (CPS), underpinning activities such as algorithm\ndevelopment, runtime monitoring, and system verification. As CPS grow in\ncomplexity and scale, particularly in safety-critical and learning-enabled\nsettings, accurate analysis and synthesis increasingly rely on the rapid use of\nsimulation experiments. Because CPS inherently integrate hardware, software,\nand physical processes, simulation platforms must support co-simulation of\nheterogeneous components at varying levels of fidelity. Despite recent advances\nin high-fidelity modeling of hardware, firmware, and physics, co-simulation in\ndiverse environments remains challenging. These limitations hinder the\ndevelopment of reusable benchmarks and impede the use of simulation for\nautomated and comparative evaluation.\n  Existing simulation tools often rely on rigid configurations, lack automation\nsupport, and present obstacles to portability and modularity. Many are\nconfigured through static text files or impose constraints on how simulation\ncomponents are represented and connected, making it difficult to flexibly\ncompose systems or integrate components across platforms.\n  To address these challenges, we introduce MultiCoSim, a Python-based\nsimulation framework that enables users to define, compose, and configure\nsimulation components programmatically. MultiCoSim supports distributed,\ncomponent-based co-simulation and allows seamless substitution and\nreconfiguration of components. We demonstrate the flexibility of MultiCoSim\nthrough case studies that include co-simulations involving custom\nautomaton-based controllers, as well as integration with off-the-shelf\nplatforms like the PX4 autopilot for aerial robotics. These examples highlight\nMultiCoSim's capability to streamline CPS simulation pipelines for research and\ndevelopment.", "AI": {"tldr": "Existing CPS simulation tools are rigid and hard to adapt. MultiCoSim, a Python-based framework, offers programmatic, modular, and flexible co-simulation, supporting both custom and off-the-shelf components, which helps streamline CPS research and development.", "motivation": "As cyber-physical systems (CPS) become more complex, especially in safety-critical and learning-enabled contexts, there is a growing need for simulation tools that can handle heterogeneous components and variable fidelity effectively. Existing simulation frameworks are often rigid, lack automation, and do not support easy integration or modularity, hindering research and development.", "method": "The authors propose MultiCoSim, a Python-based simulation framework. MultiCoSim allows users to programmatically define, compose, and configure simulation components. It supports distributed, component-based co-simulation and enables easy substitution and reconfiguration of components. Case studies\u2014including custom automaton-based controllers and integration with PX4 autopilot\u2014are used to demonstrate the framework's flexibility.", "result": "MultiCoSim streamlines the process of setting up and managing co-simulations involving diverse components. The provided case studies show successful integration and improved flexibility, usability, and modularity compared to conventional tools. MultiCoSim facilitates comparative and automated evaluation of CPS.", "conclusion": "MultiCoSim addresses key limitations in existing CPS simulation platforms by enabling flexible, modular, and automated co-simulation of heterogeneous components, thereby accelerating research and development in the field."}}
{"id": "2506.10954", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10954", "abs": "https://arxiv.org/abs/2506.10954", "authors": ["Lianghong Guo", "Yanlin Wang", "Caihua Li", "Pengyu Yang", "Jiachi Chen", "Wei Tao", "Yingtian Zou", "Duyu Tang", "Zibin Zheng"], "title": "SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks", "comment": null, "summary": "Constructing large-scale datasets for the GitHub issue resolution task is\ncrucial for both training and evaluating the software engineering capabilities\nof Large Language Models (LLMs). However, the traditional process for creating\nsuch benchmarks is notoriously challenging and labor-intensive, particularly in\nthe stages of setting up evaluation environments, grading test outcomes, and\nvalidating task instances. In this paper, we propose SWE-Factory, an automated\npipeline designed to address these challenges. To tackle these issues, our\npipeline integrates three core automated components. First, we introduce\nSWE-Builder, a multi-agent system that automates evaluation environment\nconstruction, which employs four specialized agents that work in a\ncollaborative, iterative loop and leverages an environment memory pool to\nenhance efficiency. Second, we introduce a standardized, exit-code-based\ngrading method that eliminates the need for manually writing custom parsers.\nFinally, we automate the fail2pass validation process using these reliable exit\ncode signals. Experiments on 671 issues across four programming languages show\nthat our pipeline can effectively construct valid task instances; for example,\nwith GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at $0.045 per\ninstance, while with Gemini-2.5-flash, it achieves comparable performance at\nthe lowest cost of $0.024 per instance. We also demonstrate that our\nexit-code-based grading achieves 100% accuracy compared to manual inspection,\nand our automated fail2pass validation reaches a precision of 0.92 and a recall\nof 1.00. We hope our automated pipeline will accelerate the collection of\nlarge-scale, high-quality GitHub issue resolution datasets for both training\nand evaluation. Our code and datasets are released at\nhttps://github.com/DeepSoftwareAnalytics/swe-factory.", "AI": {"tldr": "SWE-Factory automates the challenging process of building and validating GitHub issue resolution datasets, dramatically reducing manual effort, lowering costs, and achieving highly reliable results for software engineering LLM research.", "motivation": "Creating large-scale datasets for GitHub issue resolution is vital for training and evaluating Large Language Models (LLMs) in software engineering. However, building these benchmarks traditionally requires significant manual effort due to the complexities in evaluation, grading, and validation.", "method": "The authors propose SWE-Factory, an automated pipeline composed of three core components: (1) SWE-Builder\u2014a multi-agent system for automating environment construction using collaborative agents and an environment memory pool; (2) a standardized, exit-code-based grading system that replaces manual custom parsers; and (3) automated fail2pass validation leveraging exit code signals.", "result": "The pipeline was tested on 671 issues across four programming languages. SWE-Builder (using GPT-4.1-mini) generated 269 valid instances at $0.045 each, and Gemini-2.5-flash performed similarly for $0.024 per instance. The exit-code grading system matched manual inspection with 100% accuracy, and fail2pass validation achieved a precision of 0.92 and a recall of 1.00.", "conclusion": "SWE-Factory effectively automates the creation, grading, and validation of issue resolution tasks, making it feasible to scale up the construction of high-quality datasets for evaluating and training LLMs in software engineering. The tools and datasets are publicly available."}}
