<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 21]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A Taxonomy of Prompt Defects in LLM Systems](https://arxiv.org/abs/2509.14404)
*Haoye Tian,Chong Wang,BoYang Yang,Lyuye Zhang,Yang Liu*

Main category: cs.SE

TL;DR: This paper provides the first systematic taxonomy of prompt defects in LLM applications, analyzes their root causes, impacts, and mitigation strategies, and calls for more rigorous engineering methods to ensure reliable use of LLMs.


<details>
  <summary>Details</summary>
Motivation: Prompt design for LLMs is still empirical and error-prone. Small mistakes in prompts can lead to significant problems like unreliability and inefficiency, yet no systematic framework has existed to understand or address these failures. The paper aims to bring structure, analysis, and engineering rigor to this crucial aspect of LLM software development.

Method: The paper systematically surveys prompt defects using a taxonomy organized along six dimensions, refines these into specific subtypes with examples and root cause analysis, and distills practical mitigation strategies for each subtype. The methodology leverages software engineering principles and real-world development scenarios.

Result: The authors deliver the first comprehensive taxonomy of prompt defects, demonstrate how these manifest in real workflows, analyze root causes and impacts, and present associated mitigation strategies. They summarize findings in a consolidated framework linking each defect type to its effects and remedies.

Conclusion: The paper concludes by highlighting the need for rigorous, engineering-oriented methodologies to ensure the dependability of LLM-driven systems, summarizing a master taxonomy that connects prompt defects, their impacts, and mitigation strategies, and calling attention to open research challenges.

Abstract: Large Language Models (LLMs) have become key components of modern software,
with prompts acting as their de-facto programming interface. However, prompt
design remains largely empirical and small mistakes can cascade into
unreliable, insecure, or inefficient behavior. This paper presents the first
systematic survey and taxonomy of prompt defects, recurring ways that prompts
fail to elicit their intended behavior from LLMs. We organize defects along six
dimensions: (1) Specification and Intent, (2) Input and Content, (3) Structure
and Formatting, (4) Context and Memory, (5) Performance and Efficiency, and (6)
Maintainability and Engineering. Each dimension is refined into fine-grained
subtypes, illustrated with concrete examples and root cause analysis. Grounded
in software engineering principles, we show how these defects surface in real
development workflows and examine their downstream effects. For every subtype,
we distill mitigation strategies that span emerging prompt engineering
patterns, automated guardrails, testing harnesses, and evaluation frameworks.
We then summarize these strategies in a master taxonomy that links defect,
impact, and remedy. We conclude with open research challenges and a call for
rigorous engineering-oriented methodologies to ensure that LLM-driven systems
are dependable by design.

</details>


### [2] [Automating Modelica Module Generation Using Large Language Models: A Case Study on Building Control Description Language](https://arxiv.org/abs/2509.14623)
*Hanlong Wan,Xing Lu,Yan Chen,Karthik Devaprasad,Laura Hinkle*

Main category: cs.SE

TL;DR: This study shows that large language models (LLMs) can automate parts of Modelica control module development, saving 40-60% in development time. Claude Sonnet 4 performed best with engineered prompts, while GPT 4o struggled. Human-in-the-loop evaluation is crucial, as LLMs currently cannot fully validate code behavior. More advanced grounding and pre-simulation validation are needed for broader LLM adoption in this field.


<details>
  <summary>Details</summary>
Motivation: Developing control modules for dynamic energy systems in Modelica is labor intensive and requires specialized expertise, which motivates automation to speed up workflow and make development accessible.

Method: The authors designed a workflow combining standardized prompts, library-aware grounding, automated compilation (OpenModelica), and human-in-the-loop evaluation to generate Control Description Language modules with large language models. They compared zero-shot and engineered prompts across GPT 4o and Claude Sonnet 4, tested retrieval augmented generation vs. hard rule search, and assessed outputs via human and AI evaluation.

Result: Claude Sonnet 4 provided up to full success for basic logic blocks with engineered prompts, and an 83% success rate for control modules. Failed outputs needed moderate human repair. GPT 4o failed in zero-shot mode. Automated retrieval often mismatched modules, while deterministic rule search performed better. LLM-generated modules reduced development time from 10-20 to 4-6 hours (40-60% savings). Human evaluators outperformed AI evaluators.

Conclusion: LLMs can significantly reduce development time for control modules in Modelica, but current solutions have limitations in behavioral validation and accuracy. Improved grounding and validation methods are needed for robust, reliable automation.

Abstract: Dynamic energy systems and controls require advanced modeling frameworks to
design and test supervisory and fault tolerant strategies. Modelica is a widely
used equation based language, but developing control modules is labor intensive
and requires specialized expertise. This paper examines the use of large
language models (LLMs) to automate the generation of Control Description
Language modules in the Building Modelica Library as a case study. We developed
a structured workflow that combines standardized prompt scaffolds, library
aware grounding, automated compilation with OpenModelica, and human in the loop
evaluation. Experiments were carried out on four basic logic tasks (And, Or,
Not, and Switch) and five control modules (chiller enable/disable, bypass valve
control, cooling tower fan speed, plant requests, and relief damper control).
The results showed that GPT 4o failed to produce executable Modelica code in
zero shot mode, while Claude Sonnet 4 achieved up to full success for basic
logic blocks with carefully engineered prompts. For control modules, success
rates reached 83 percent, and failed outputs required medium level human repair
(estimated one to eight hours). Retrieval augmented generation often produced
mismatches in module selection (for example, And retrieved as Or), while a
deterministic hard rule search strategy avoided these errors. Human evaluation
also outperformed AI evaluation, since current LLMs cannot assess simulation
results or validate behavioral correctness. Despite these limitations, the LLM
assisted workflow reduced the average development time from 10 to 20 hours down
to 4 to 6 hours per module, corresponding to 40 to 60 percent time savings.
These results highlight both the potential and current limitations of LLM
assisted Modelica generation, and point to future research in pre simulation
validation, stronger grounding, and closed loop evaluation.

</details>


### [3] [Evolution of Kernels: Automated RISC-V Kernel Optimization with Large Language Models](https://arxiv.org/abs/2509.14265)
*Siyuan Chen,Zhichao Lu,Qingfu Zhang*

Main category: cs.SE

TL;DR: This paper introduces a novel LLM-based framework, EoK, for automated kernel optimization in domains lacking technical references, like RISC-V. By mining and reusing proven optimization strategies from existing libraries, EoK outperforms experts and previous methods, showing strong potential for automated design in emerging hardware platforms.


<details>
  <summary>Details</summary>
Motivation: Automated kernel design faces significant challenges in domains like RISC-V, which lack comprehensive technical documentation and mature codebases, unlike established areas such as CUDA. There is a critical need for approaches that can overcome software ecosystem barriers in these emerging hardware platforms.

Method: The authors introduce Evolution of Kernels (EoK), a novel framework that leverages large language models (LLMs) combined with evolutionary program search. EoK mitigates reference scarcity by mining reusable optimization ideas from existing kernel library histories. These insights guide LLM explorations, which are further tailored for RISC-V using Retrieval-Augmented Generation (RAG) and context-specific techniques.

Result: EoK achieves a median speedup of 1.27x across 80 RISC-V kernel design tasks, outperforming human experts on all tasks and improving upon previous LLM-based methods by 20%.

Conclusion: Incorporating human experience and historically effective optimization techniques into LLM-driven frameworks enables automated kernel design for reference-scarce domains like RISC-V, demonstrating substantial improvements over human and prior automated approaches.

Abstract: Automated kernel design is critical for overcoming software ecosystem
barriers in emerging hardware platforms like RISC-V. While large language
models (LLMs) have shown promise for automated kernel optimization,
demonstrating success in CUDA domains with comprehensive technical documents
and mature codebases, their effectiveness remains unproven for reference-scarce
domains like RISC-V. We present Evolution of Kernels (EoK), a novel LLM-based
evolutionary program search framework that automates kernel design for domains
with limited reference material. EoK mitigates reference scarcity by mining and
formalizing reusable optimization ideas (general design principles + actionable
thoughts) from established kernel libraries' development histories; it then
guides parallel LLM explorations using these ideas, enriched via
Retrieval-Augmented Generation (RAG) with RISC-V-specific context, prioritizing
historically effective techniques. Empirically, EoK achieves a median 1.27x
speedup, surpassing human experts on all 80 evaluated kernel design tasks and
improving upon prior LLM-based automated kernel design methods by 20%. These
results underscore the viability of incorporating human experience into
emerging domains and highlight the immense potential of LLM-based automated
kernel optimization.

</details>


### [4] [SALT4Decompile: Inferring Source-level Abstract Logic Tree for LLM-Based Binary Decompilation](https://arxiv.org/abs/2509.14646)
*Yongpan Wang,Xin Xu,Xiaojie Zhu,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: SALTM introduces a novel logic-abstracting approach to guide LLMs in decompiling binaries, achieving superior semantic recovery and usability compared to existing tools and methods.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based decompilation approaches treat assembly code as a linear sequence and overlook complex control flows and isolated data segments, limiting their ability to recover accurate high-level source code semantics.

Method: The proposed method, SALTM, abstracts stable logical features from binary code and constructs a Source-level Abstract Logic Tree (SALT) from assembly code. It then fine-tunes an LLM using the generated SALT for decompilation and further refines the output with error correction and symbol recovery.

Result: SALTM outperforms general-purpose LLMs, commercial decompilers, and other decompilation methods on benchmarks such as Decompile-Eval, showing a 70.4% TCP rate with a 10.6% improvement. It remains robust against four common obfuscation techniques and provides better assistance to human analysts in comprehending binary functions.

Conclusion: SALTM effectively bridges the semantic gap between binary and source code by abstracting logical structures, resulting in more accurate and readable decompiled code that surpasses state-of-the-art competitors and aids human analysis.

Abstract: Decompilation is widely used in reverse engineering to recover high-level
language code from binary executables. While recent approaches leveraging Large
Language Models (LLMs) have shown promising progress, they typically treat
assembly code as a linear sequence of instructions, overlooking arbitrary jump
patterns and isolated data segments inherent to binary files. This limitation
significantly hinders their ability to correctly infer source code semantics
from assembly code. To address this limitation, we propose \saltm, a novel
binary decompilation method that abstracts stable logical features shared
between binary and source code. The core idea of \saltm is to abstract selected
binary-level operations, such as specific jumps, into a high-level logic
framework that better guides LLMs in semantic recovery. Given a binary
function, \saltm constructs a Source-level Abstract Logic Tree (\salt) from
assembly code to approximate the logic structure of high-level language. It
then fine-tunes an LLM using the reconstructed \salt to generate decompiled
code. Finally, the output is refined through error correction and symbol
recovery to improve readability and correctness. We compare \saltm to three
categories of baselines (general-purpose LLMs, commercial decompilers, and
decompilation methods) using three well-known datasets (Decompile-Eval, MBPP,
Exebench). Our experimental results demonstrate that \saltm is highly effective
in recovering the logic of the source code, significantly outperforming
state-of-the-art methods (e.g., 70.4\% TCP rate on Decompile-Eval with a 10.6\%
improvement). The results further validate its robustness against four commonly
used obfuscation techniques. Additionally, analyses of real-world software and
a user study confirm that our decompiled output offers superior assistance to
human analysts in comprehending binary functions.

</details>


### [5] [Automated and Context-Aware Code Documentation Leveraging Advanced LLMs](https://arxiv.org/abs/2509.14273)
*Swapnil Sharma Sarker,Tanzina Taher Ifty*

Main category: cs.SE

TL;DR: The paper addresses the lack of effective automated solutions and datasets for template-based (Javadoc) code documentation. By releasing a new, context-rich dataset and evaluating several open-source LLMs, the authors find LLaMA-3.1 to be the most reliable for this task, suggesting it as a practical solution for automated documentation generation.


<details>
  <summary>Details</summary>
Motivation: Manual code documentation is tedious and time-consuming, and current automated approaches have mostly focused on code summarization rather than structured, template-based documentation like Javadocs. There is also a lack of suitable datasets that include modern language features and context for this task.

Method: The authors created a novel, context-aware dataset specifically for Javadoc generation, incorporating structural and semantic information from up-to-date Java codebases. They evaluated five open-source large language models (LLMs)—LLaMA-3.1, Gemma-2, Phi-3, Mistral, and Qwen-2.5—using zero-shot, few-shot, and fine-tuned methods to compare their performances.

Result: Among the tested models, LLaMA-3.1 consistently delivered strong results across various setups, proving to be reliable for automated Javadoc generation.

Conclusion: LLaMA-3.1 is a robust and practical alternative to proprietary systems for automated, context-aware, template-based Javadoc generation, particularly when powered with a tailored, modern dataset. The study also fills the dataset gap in this research area.

Abstract: Code documentation is essential to improve software maintainability and
comprehension. The tedious nature of manual code documentation has led to much
research on automated documentation generation. Existing automated approaches
primarily focused on code summarization, leaving a gap in template-based
documentation generation (e.g., Javadoc), particularly with publicly available
Large Language Models (LLMs). Furthermore, progress in this area has been
hindered by the lack of a Javadoc-specific dataset that incorporates modern
language features, provides broad framework/library coverage, and includes
necessary contextual information. This study aims to address these gaps by
developing a tailored dataset and assessing the capabilities of publicly
available LLMs for context-aware, template-based Javadoc generation. In this
work, we present a novel, context-aware dataset for Javadoc generation that
includes critical structural and semantic information from modern Java
codebases. We evaluate five open-source LLMs (including LLaMA-3.1, Gemma-2,
Phi-3, Mistral, Qwen-2.5) using zero-shot, few-shot, and fine-tuned setups and
provide a comparative analysis of their performance. Our results demonstrate
that LLaMA 3.1 performs consistently well and is a reliable candidate for
practical, automated Javadoc generation, offering a viable alternative to
proprietary systems.

</details>


### [6] [Code Less to Code More: Streamlining Language Server Protocol and Type System Development for Language Families](https://arxiv.org/abs/2509.15150)
*Federico Bruzzone,Walter Cazzola,Luca Favalli*

Main category: cs.SE

TL;DR: This paper introduces Typelang for modular type system implementation, automates language server and editor plugin generation, and achieves massive reductions in developer effort for editing support across languages and editors.


<details>
  <summary>Details</summary>
Motivation: Supporting multiple languages across multiple editors is complex, involving redundant implementations and lack of modularity and reusability in current solutions. There is a need to simplify and automate the generation of language servers and plugins for editors, leveraging type systems and modular design.

Method: The paper introduces Typelang (a family of DSLs for type systems), a modular language server generation process, a variant-oriented programming paradigm, a cross-artifact coordination layer, and an LSP plugin generator. Typelang is implemented in Neverlang to generate language servers and plugins.

Result: Empirical evaluation shows a 93.48% reduction in characters for type system implementation and complete automation of LSP plugin creation across three editors, drastically reducing the effort required to provide editing support for language families.

Conclusion: The proposed approach significantly simplifies and automates the process of editing support for language families, enabling modularity, reusability, and reduced development effort through the use of Typelang and automated plugin generation.

Abstract: Developing editing support for $L$ languages in $E$ editors is complex and
time-consuming. Some languages do not provide dedicated editors, while others
offer a single native editor. The $\textit{language server protocol}$ (LSP)
reduces the language-editor combinations $L \times E$ to $L + E$, where a
single language server communicates with editors via LSP plugins. However,
overlapping implementations of linguistic components remain an issue. Existing
language workbenches struggle with modularity, reusability, and leveraging type
systems for language server generation. In this work, we propose: (i) Typelang,
a family of domain-specific languages for modular, composable, and reusable
type system implementation, (ii) a modular language server generation process,
producing servers for languages built in a modular workbench, (iii) the
variant-oriented programming paradigm and a cross-artifact coordination layer
to manage interdependent software variants, and (iv) an LSP plugin generator,
reducing $E$ to $1$ by automating plugin creation for multiple editors. To
simplify editing support for language families, each language artifact
integrates its own Typelang variant, used to generate language servers. This
reduces combinations to $T \times 1$, where $T = L$ represents the number of
type systems. Further reuse of language artifacts across languages lowers this
to $N \times 1$, where $N << T$, representing unique type systems. We implement
Typelang in Neverlang, generating language servers for each artifact and LSP
plugins for three editors. Empirical evaluation shows a 93.48% reduction in
characters needed for type system implementation and 100% automation of LSP
plugin generation, significantly lowering effort for editing support in
language families, especially when artifacts are reused.

</details>


### [7] [Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization](https://arxiv.org/abs/2509.14279)
*Robert Tjarko Lange,Qi Sun,Aaditya Prasad,Maxence Faldor,Yujin Tang,David Ha*

Main category: cs.SE

TL;DR: The paper introduces robust-kbench, a diverse new benchmark for evaluating CUDA kernels, and an agentic LLM-based framework for automating their translation, optimization, and verification from PyTorch code. This approach produces faster and more accurate CUDA kernels than torch implementations, with robust verification and optimization strategies that address shortcomings in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing LLM approaches in software engineering focus on high-level tasks and lack optimizations for low-level CUDA kernel implementations. Additionally, current CUDA kernel generation benchmarks have weaknesses such as loopholes and inadequate diversity, which prevent proper evaluation of LLM generalization and performance.

Method: The paper introduces robust-kbench, a new benchmark designed for diverse and rigorous evaluation of CUDA kernel performance and correctness. It also presents an agentic framework that automates CUDA kernel generation, verification, and optimization using frontier LLMs. The pipeline first translates PyTorch code to CUDA kernels, then applies an evolutionary meta-generation procedure with LLM-based verifiers to optimize runtime and ensure correctness.

Result: Their approach generates CUDA kernels that outperform standard torch implementations in practical scenarios including forward and backward passes. The method is able to fuse operations, deploy various runtime optimization strategies, and the verifier workflow efficiently identifies incorrect kernels, improving hardware verification.

Conclusion: The proposed robust-kbench and agentic framework enable rigorous and automated evaluation and optimization of CUDA kernels via LLMs, demonstrating significant improvements over existing torch implementations and addressing loopholes in previous benchmarks.

Abstract: Recent advances in large language models (LLMs) demonstrate their
effectiveness in scaling test-time compute for software engineering tasks.
However, these approaches often focus on high-level solutions, with limited
attention to optimizing low-level CUDA kernel implementations. Additionally,
existing kernel generation benchmarks suffer from exploitable loopholes and
insufficient diversity in testing conditions, hindering true generalization
assessment. To address these limitations, we introduce robust-kbench, a new
benchmark for rigorous evaluation of kernel performance and correctness across
varied scenarios. Furthermore, we present a comprehensive agentic framework
that automates CUDA kernel discovery, verification, and optimization. This
pipeline enables frontier LLMs to translate torch code to CUDA kernels and
iteratively improve their runtime within our robust evaluation setting. Our
sequential workflow first translates PyTorch code into equivalent CUDA kernels.
It then optimizes their runtime using a novel evolutionary meta-generation
procedure tailored to the CUDA ecosystem, guided by LLM-based verifiers for
correctness and efficient filtering. Evaluated on robust-kbench, our approach
produces CUDA kernels outperforming torch implementations for practical
applications, including forward and backward passes. It can fuse operations and
deploy various runtime optimization strategies. The verifier workflow
accurately classifies incorrect kernels, enhancing hardware verification
efficiency.

</details>


### [8] [SCoGen: Scenario-Centric Graph-Based Synthesis of Real-World Code Problems](https://arxiv.org/abs/2509.14281)
*Xifeng Yao,Dongyu Lang,Wu Zhang,Xintong Guo,Huarui Xie,Yinhao Ni,Ping Liu,Guang Shen,Yi Bai,Dandan Tu,Changzheng Zhang*

Main category: cs.SE

TL;DR: The paper introduces a novel framework to automatically generate realistic coding problems using knowledge mined from real-world datasets. This approach supports better training and evaluation of code large language models by producing more relevant, diverse, and complex problems than existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the bottleneck in advancing code large language models, which is due to a lack of real-world coding problems for training and evaluation.

Method: The paper proposes a framework that synthesizes realistic code problems. It extracts domain knowledge, domain skills, and coding skills from real-world datasets (like Stack Overflow and Kaggle), constructs a scenario-centric graph interconnecting these elements, and applies a sampling strategy to generate diverse and complex problems reflecting real-world scenarios.

Result: Experimental results show that the proposed framework produces code problems that enable better performance of language models compared to state-of-the-art open-source models, across various benchmarks.

Conclusion: The framework effectively fills the gap of real-world code problems, facilitating further advancement of code large language models and outperforming existing open-source solutions in generating useful training and evaluation data.

Abstract: Significant advancements have been made in the capabilities of code large
language models, leading to their rapid adoption and application across a wide
range of domains. However, their further advancements are often constrained by
the scarcity of real-world coding problems. To bridge this gap, we propose a
novel framework for synthesizing code problems that emulate authentic
real-world scenarios. This framework systematically integrates domain
knowledge, domain skills, and coding skills, all of which are meticulously
extracted from real-world programming-related datasets, including Stack
Overflow and Kaggle. The extracted elements serve as the foundational building
blocks for constructing code problems. To align the generated problems with
practical applications, application scenarios are also mined from the
aforementioned datasets. These scenarios are then utilized to construct a
scenario-centric graph that interconnects domain knowledge, domain skills, and
coding skills. Based on this structured representation, a sampling strategy on
the graph is designed, which effectively controls the generation of a code
problem with complexity and diversity, reflects real-world challenges.
Experimental results demonstrate that the proposed method consistently achieves
superior performance over state-of-the-art open-source large language models of
varying sizes and functionalities, including both coders and general-purpose
models, across a diverse set of real-world benchmarks.

</details>


### [9] [Monitoring Machine Learning Systems: A Multivocal Literature Review](https://arxiv.org/abs/2509.14294)
*Hira Naveed,Scott Barnett,Chetan Arora,John Grundy,Hourieh Khalajzadeh,Omar Haggag*

Main category: cs.SE

TL;DR: This paper systematically reviews 136 sources on machine learning monitoring in production, highlighting current practices, tools, and gaps. It compares formal and practical literature, offering recommendations and future research directions to enhance ML system reliability.


<details>
  <summary>Details</summary>
Motivation: Maintaining reliable machine learning (ML) systems in dynamic production environments is challenging due to frequent runtime issues such as data drift and operational changes that degrade model performance. Early detection through monitoring is essential to prevent negative outcomes and maintain trust.

Method: A multivocal literature review (MLR) was conducted, following Garousi's established guidelines. The review covered 136 papers and analyzed them across four areas: motivations and contexts, techniques and tools used, contributions and benefits, and limitations of current practices.

Result: The study categorizes and summarizes the ML monitoring landscape, covering both academic and 'gray' literature. It reveals commonalities and gaps in current ML monitoring practices, provides recommendations, and identifies limitations for future improvement.

Conclusion: The review offers a comprehensive mapping of ML monitoring approaches and points out important differences and overlaps between formal and gray literature. It serves as a significant resource for both researchers and practitioners by aiding in choosing monitoring solutions and identifying areas that require further research.

Abstract: Context: Dynamic production environments make it challenging to maintain
reliable machine learning (ML) systems. Runtime issues, such as changes in data
patterns or operating contexts, that degrade model performance are a common
occurrence in production settings. Monitoring enables early detection and
mitigation of these runtime issues, helping maintain users' trust and prevent
unwanted consequences for organizations. Aim: This study aims to provide a
comprehensive overview of the ML monitoring literature. Method: We conducted a
multivocal literature review (MLR) following the well established guidelines by
Garousi to investigate various aspects of ML monitoring approaches in 136
papers. Results: We analyzed selected studies based on four key areas: (1) the
motivations, goals, and context; (2) the monitored aspects, specific
techniques, metrics, and tools; (3) the contributions and benefits; and (4) the
current limitations. We also discuss several insights found in the studies,
their implications, and recommendations for future research and practice.
Conclusion: Our MLR identifies and summarizes ML monitoring practices and gaps,
emphasizing similarities and disconnects between formal and gray literature.
Our study is valuable for both academics and practitioners, as it helps select
appropriate solutions, highlights limitations in current approaches, and
provides future directions for research and tool development.

</details>


### [10] [On the Illusion of Success: An Empirical Study of Build Reruns and Silent Failures in Industrial CI](https://arxiv.org/abs/2509.14347)
*Henri Aïdasso,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: The paper empirically examines silent failures in CI—when builds falsely appear successful—and identifies their prevalence, causes, and categories. It highlights the need for detection and mitigation strategies to ensure CI trustworthiness and software quality.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address issues in Continuous Integration (CI) where build outcomes can be unreliable due to non-deterministic or silent failures. While previous studies looked at visible job failures, silent failures—successful builds that actually fail partly or wholly—remain underexplored despite their potential to let bugs slip into production.

Method: The authors conducted the first empirical study of silent failures by analyzing reruns of successful CI jobs. They examined 142,387 jobs from 81 industrial projects, using mixed-effects statistical models on 32 variables to identify correlates. They also performed qualitative analysis on 92 public issues to categorize types of silent failures.

Result: Key findings include that 11% of 'successful' jobs are rerun, with over a third happening after more than 24 hours. Factors related to increased reruns include testing/static analysis tasks, scripting languages (like Shell), and developer's past rerun behaviors. The study identified 11 categories of silent failures, with the most common being artifact operation errors, caching problems, and ignored exit codes.

Conclusion: Silent failures in CI are more common than previously recognized and have specific patterns and contributors. Identifying and understanding these enables better CI practices, improved reliability, and heightened team awareness.

Abstract: Reliability of build outcomes is a cornerstone of effective Continuous
Integration (CI). Yet in practice, developers often struggle with
non-deterministic issues in the code or CI infrastructure, which undermine
trust in build results. When faced with such unexpected outcomes, developers
often repeatedly rerun jobs hoping for true success, but this practice is known
to increase CI costs and reduce productivity. While recent studies have focused
on intermittent job failures, no prior work has investigated silent failures,
where build jobs are marked as successful but fail to complete all or part of
their tasks. Such silent failures often go unnoticed, creating an illusion of
success with detrimental consequences such as bugs escaping into production.
This paper presents the first empirical study of silent failures through the
practice of rerunning successful jobs. An analysis of 142,387 jobs across 81
industrial projects shows that 11% of successful jobs are rerun, with 35% of
these reruns occurring after more than 24 hours. Using mixed-effects models on
32 independent variables (AUC of 85%), we identified key factors associated
with reruns of successful jobs, notably testing and static analysis tasks,
scripting languages like Shell, and developers prior rerun tendencies. A
further analysis of 92 public issues revealed 11 categories of silent failures
aligning with these factors, the most frequent being artifact operation errors,
caching errors, and ignored exit codes. Overall, our findings provide valuable
insights into the circumstances and causes of silent failures to raise
awareness among teams, and present solutions to improve CI reliability.

</details>


### [11] [CodeLSI: Leveraging Foundation Models for Automated Code Generation with Low-Rank Optimization and Domain-Specific Instruction Tuning](https://arxiv.org/abs/2509.14373)
*Huy Le,Phong Nguyen,Hao Do,Tuan Nguyen,Thien Pham,Anh Nguyen-Duc,Tho Quan*

Main category: cs.SE

TL;DR: CodeLSI is a new framework for automated code generation that combines low-rank optimization and domain-specific tuning. It produces more accurate, relevant code at lower cost, operating securely on internal infrastructure, and outperforms baseline models for company-specific tasks.


<details>
  <summary>Details</summary>
Motivation: Automated code generation via Foundation Models (FMs) is promising but faces issues with domain specificity, cost, and security, particularly regarding third-party APIs. There is a need for a solution that enables secure, efficient, and targeted code generation within company infrastructures.

Method: The paper proposes CodeLSI, a framework that leverages low-rank optimization to cut down on computational resources and employs domain-specific instruction tuning to tailor code generation to organizational requirements. The methodology was validated by implementing CodeLSI on real-world JavaScript tasks using internal project datasets.

Result: CodeLSI generated high-quality, context-aware code that exceeded baseline models in relevance, accuracy, and domain appropriateness. Low-rank optimization allowed training and fine-tuning with less computational cost, making it feasible on company infrastructure.

Conclusion: Combining low-rank optimization with domain-specific tuning improves the practicality and performance of FMs in automated code generation. CodeLSI is a secure, cost-effective alternative to commercial API-based solutions, fostering faster and more targeted innovation.

Abstract: Context: Automated code generation using Foundation Models (FMs) offers
promising solutions for enhancing software development efficiency. However,
challenges remain in ensuring domain specificity, cost-effectiveness, and
security - especially when relying on third-party APIs. This paper introduces
CodeLSI, a framework that combines low-rank optimization and domain-specific
instruction tuning to address these challenges.
  Objectives: The aim of this study is to develop and evaluate CodeLSI, a novel
approach for generating high-quality code tailored to specific domains, using
FMs fine-tuned on company infrastructure without dependence on external APIs.
  Methods: CodeLSI applies low-rank adaptation techniques to reduce the
computational cost of model pre-training and fine-tuning. Domain-specific
instruction tuning is employed to align code generation with organizational
needs. We implemented and tested the framework on real-world JavaScript coding
tasks using datasets drawn from internal software projects.
  Results: Experimental evaluations show that CodeLSI produces high-quality,
context aware code. It outperforms baseline models in terms of relevance,
accuracy, and domain fit. The use of low-rank optimization significantly
reduced resource requirements, enabling scalable training on company-owned
infrastructure.
  Conclusion: CodeLSI demonstrates that combining low-rank optimization with
domain specific tuning can enhance the practicality and performance of FMs for
automated code generation. This approach provides a secure, cost-efficient
alternative to commercial API based solutions and supports faster, more
targeted innovation in software development.

</details>


### [12] [An LLM-based multi-agent framework for agile effort estimation](https://arxiv.org/abs/2509.14483)
*Thanh-Long Bui,Hoa Khanh Dam,Rashina Hoda*

Main category: cs.SE

TL;DR: This paper introduces an LLM-based multi-agent framework for collaborative, explainable, and accurate agile software effort estimation, which outperforms existing methods and is well-received by practitioners.


<details>
  <summary>Details</summary>
Motivation: Current agile software effort estimation relies heavily on subjective human judgment, leading to inaccuracies and inconsistencies. Existing machine learning solutions improve accuracy, but lack transparency, explainability, and the ability to collaborate interactively with human team members.

Method: The paper proposes a novel multi-agent framework based on Large Language Models (LLMs). This framework allows agents to generate effort estimates, and also enables them to coordinate, communicate, and discuss these estimates with human developers and with each other to reach consensus.

Result: Experiments on real-world datasets show that this LLM-based approach outperforms current state-of-the-art techniques across all evaluation metrics in most cases. Additionally, a human study revealed that software development practitioners had a very positive collaborative experience with the agents for agile effort estimation.

Conclusion: Leveraging LLMs in a multi-agent system provides accurate, explainable, and collaborative effort estimation in agile software development, offering clear improvements over traditional approaches and black-box ML models. Practitioners found the framework effective and beneficial in real-world scenarios.

Abstract: Effort estimation is a crucial activity in agile software development, where
teams collaboratively review, discuss, and estimate the effort required to
complete user stories in a product backlog. Current practices in agile effort
estimation heavily rely on subjective assessments, leading to inaccuracies and
inconsistencies in the estimates. While recent machine learning-based methods
show promising accuracy, they cannot explain or justify their estimates and
lack the capability to interact with human team members. Our paper fills this
significant gap by leveraging the powerful capabilities of Large Language
Models (LLMs). We propose a novel LLM-based multi-agent framework for agile
estimation that not only can produce estimates, but also can coordinate,
communicate and discuss with human developers and other agents to reach a
consensus. Evaluation results on a real-life dataset show that our approach
outperforms state-of-the-art techniques across all evaluation metrics in the
majority of the cases. Our human study with software development practitioners
also demonstrates an overwhelmingly positive experience in collaborating with
our agents in agile effort estimation.

</details>


### [13] [Evaluating the Effectiveness of Coverage-Guided Fuzzing for Testing Deep Learning Library APIs](https://arxiv.org/abs/2509.14626)
*Feiran Qin,M. M. Abid Naziri,Hengyu Ai,Saikat Dutta,Marcelo d'Amorim*

Main category: cs.SE

TL;DR: The paper introduces FlashFuzz, a system that uses large language models to automatically create test harnesses for deep learning libraries, enabling effective coverage-guided fuzzing. FlashFuzz achieves significantly better coverage, validity, and speed than previous tools, and discovers numerous previously unknown bugs in PyTorch and TensorFlow, validating the effectiveness of CGF in this challenging domain.


<details>
  <summary>Details</summary>
Motivation: Deep learning libraries like PyTorch are foundational to AI applications, but finding bugs in them is difficult. Previous fuzzing methods lack coverage guidance, which limits their ability to uncover bugs and achieve high code coverage. The authors are motivated to determine if coverage-guided fuzzing (CGF) can be practically and effectively applied to these libraries, which is challenging due to the need for specialized API-level test harnesses.

Method: The authors introduce FlashFuzz, a system that utilizes Large Language Models (LLMs) to automatically synthesize API-level harnesses. FlashFuzz combines templates, helper functions, and API documentation to transform low-level fuzzer inputs into valid API calls, employing a feedback-driven approach to iteratively refine these harnesses. The technique is applied to a large set of APIs from PyTorch and TensorFlow.

Result: FlashFuzz automatically created harnesses for 1,151 PyTorch and 662 TensorFlow APIs. It outperformed existing state-of-the-art fuzzers (ACETest, PathFinder, TitanFuzz) with up to 101.13-212.88% higher code coverage, 1.0x-5.4x better validity rate, and 1x-1182x faster input generation speeds. FlashFuzz discovered 42 new bugs in PyTorch and TensorFlow, 8 of which have already been fixed.

Conclusion: Coverage-guided fuzzing, facilitated by automated harness synthesis with LLMs, is a highly effective approach for testing deep learning libraries. FlashFuzz sets a new performance baseline for DL library fuzzing and demonstrates practical scalability, efficiency, and higher bug-finding capability compared to previous methods.

Abstract: Deep Learning (DL) libraries such as PyTorch provide the core components to
build major AI-enabled applications. Finding bugs in these libraries is
important and challenging. Prior approaches have tackled this by performing
either API-level fuzzing or model-level fuzzing, but they do not use coverage
guidance, which limits their effectiveness and efficiency. This raises an
intriguing question: can coverage guided fuzzing (CGF), in particular
frameworks like LibFuzzer, be effectively applied to DL libraries, and does it
offer meaningful improvements in code coverage, bug detection, and scalability
compared to prior methods?
  We present the first in-depth study to answer this question. A key challenge
in applying CGF to DL libraries is the need to create a test harness for each
API that can transform byte-level fuzzer inputs into valid API inputs. To
address this, we propose FlashFuzz, a technique that leverages Large Language
Models (LLMs) to automatically synthesize API-level harnesses by combining
templates, helper functions, and API documentation. FlashFuzz uses a feedback
driven strategy to iteratively synthesize and repair harnesses. With this
approach, FlashFuzz synthesizes harnesses for 1,151 PyTorch and 662 TensorFlow
APIs. Compared to state-of-the-art fuzzing methods (ACETest, PathFinder, and
TitanFuzz), FlashFuzz achieves up to 101.13 to 212.88 percent higher coverage
and 1.0x to 5.4x higher validity rate, while also delivering 1x to 1182x
speedups in input generation. FlashFuzz has discovered 42 previously unknown
bugs in PyTorch and TensorFlow, 8 of which are already fixed. Our study
confirms that CGF can be effectively applied to DL libraries and provides a
strong baseline for future testing approaches.

</details>


### [14] [Wireless Communication Performance Testing: From Laboratory Environment to Research Vessel](https://arxiv.org/abs/2509.14740)
*Andrei-Raoul Morariu,Andreas Strandberg,Bogdan Iancu,Jerker Bjorkqvist*

Main category: cs.SE

TL;DR: Obstacles and placement aboard vehicles like boats can notably hinder wireless signal transmission. Measurements in lab and field settings confirm that environmental conditions must be considered for reliable shared spectrum communication.


<details>
  <summary>Details</summary>
Motivation: The paper aims to better understand how environmental factors, including obstacles and dynamic placement, affect signal transmission in shared spectrum environments, particularly in practical settings like onboard an electric research boat.

Method: The study uses empirical measurements of signal transmission in both laboratory and outdoor environments. It examines how objects that obstruct the line of sight between transmitter and receiver attenuate the signal. It also analyzes the effect of distance and placement on an electric research boat.

Result: The study found that objects in the line of sight significantly attenuate signal strength. Distance and location aboard the boat further impact signal transmission efficiency, indicating that environmental factors do influence wireless communication performance in obstructed and dynamic settings.

Conclusion: Environmental factors, such as obstacles and physical arrangement, play a crucial role in wireless signal transmission within shared spectrum environments. Effective wireless communication requires consideration of these factors, especially in dynamic contexts like a moving research boat.

Abstract: This study investigates signal transmission within a shared spectrum,
focusing on measurements conducted both in laboratory and outdoor environments.
The objective was to demonstrate how laboratory objects obstructing the line of
sight can attenuate the signal between a transmitter (Tx) and a receiver (Rx).
Additionally, we examined the impact of distance and placement in various
locations aboard an electric research boat on signal transmission efficiency.
These findings contribute to understanding whether the environmental factors
influence wireless communication in dynamic and obstructed environments.

</details>


### [15] [On the Use of Agentic Coding Manifests: An Empirical Study of Claude Code](https://arxiv.org/abs/2509.14744)
*Worawalan Chatlatanagulchai,Kundjanasith Thonglek,Brittany Reid,Yutaro Kashiwa,Pattara Leelaprute,Arnon Rungsawang,Bundit Manaskasemsak,Hajimu Iida*

Main category: cs.SE

TL;DR: The paper analyzes 253 agent manifest files and finds they are generally simple in structure and focused on operational and technical information, highlighting a need for improved documentation.


<details>
  <summary>Details</summary>
Motivation: There is a lack of comprehensive and accessible documentation for creating agent manifests (such as Claude.md), which are crucial for agentic coding tools to function effectively. This gap presents a significant challenge for developers using these tools.

Method: The authors analyzed 253 Claude.md files from 242 repositories to identify structural patterns and common themes in how these agent manifests are constructed.

Result: The study found that agent manifests generally have shallow hierarchies, usually featuring one main heading and several subsections. The content is mainly composed of operational commands, technical implementation notes, and high-level architecture details.

Conclusion: Agent manifests for agentic coding tools tend to follow simple structures and cover a set of recurring content types, which points to opportunities for standardization and better documentation practices.

Abstract: Agentic coding tools receive goals written in natural language as input,
break them down into specific tasks, and write/execute the actual code with
minimal human intervention. Key to this process are agent manifests,
configuration files (such as Claude.md) that provide agents with essential
project context, identity, and operational rules. However, the lack of
comprehensive and accessible documentation for creating these manifests
presents a significant challenge for developers. We analyzed 253 Claude.md
files from 242 repositories to identify structural patterns and common content.
Our findings show that manifests typically have shallow hierarchies with one
main heading and several subsections, with content dominated by operational
commands, technical implementation notes, and high-level architecture.

</details>


### [16] [On the Use of Agentic Coding: An Empirical Study of Pull Requests on GitHub](https://arxiv.org/abs/2509.14745)
*Miku Watanabe,Hao Li,Yutaro Kashiwa,Brittany Reid,Hajimu Iida,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: Most agent-generated pull requests are accepted and merged in open-source projects, but many still need human editing to fix bugs or meet project norms. Autonomous code generation shows promise, but human involvement is important for best results.


<details>
  <summary>Details</summary>
Motivation: With the growing adoption of large language models (LLMs) in software development, especially for generating code and creating pull requests with minimal human input, there is limited understanding of their practical acceptance in real-world open-source projects.

Method: An empirical study was conducted by analyzing 567 GitHub pull requests generated by Claude Code (an autonomous agentic coding tool) across 157 diverse open-source projects. The acceptance, modification, and utility of these PRs were systematically evaluated.

Result: 83.8% of agent-generated pull requests were accepted and merged, and 54.9% of those did not require further modification. The remaining 45.1% benefited from human revisions, particularly for bug fixes, documentation improvements, and compliance with project-specific standards.

Conclusion: Agent-assisted pull requests are widely accepted in real open-source projects, but human oversight and edits remain crucial for higher quality, especially for tasks beyond simple refactoring and documentation.

Abstract: Large language models (LLMs) are increasingly being integrated into software
development processes. The ability to generate code and submit pull requests
with minimal human intervention, through the use of autonomous AI agents, is
poised to become a standard practice. However, little is known about the
practical usefulness of these pull requests and the extent to which their
contributions are accepted in real-world projects. In this paper, we
empirically study 567 GitHub pull requests (PRs) generated using Claude Code,
an agentic coding tool, across 157 diverse open-source projects. Our analysis
reveals that developers tend to rely on agents for tasks such as refactoring,
documentation, and testing. The results indicate that 83.8% of these
agent-assisted PRs are eventually accepted and merged by project maintainers,
with 54.9% of the merged PRs are integrated without further modification. The
remaining 45.1% require additional changes benefit from human revisions,
especially for bug fixes, documentation, and adherence to project-specific
standards. These findings suggest that while agent-assisted PRs are largely
acceptable, they still benefit from human oversight and refinement.

</details>


### [17] [RulER: Automated Rule-Based Semantic Error Localization and Repair for Code Translation](https://arxiv.org/abs/2509.14829)
*Shuo Jin,Songqiang Chen,Xiaoyuan Xie,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: RulER is a rule-based debugging tool for automated code translation, addressing the lack of robust error correction in existing methods. By extracting and applying translation rules from correct LLM outputs, RulER greatly improves error localization and repair success compared to current benchmarks, offering a more effective way to utilize LLM coding knowledge.


<details>
  <summary>Details</summary>
Motivation: Automated code translation often produces erroneous outputs due to imperfect models, and current debugging approaches lack robust references for code alignment and repair template design, reducing their effectiveness.

Method: The authors introduce RulER, a rule-based debugging method that automatically derives code translation rules from correct LLM-generated translations. RulER dynamically combines existing rules to align and repair more code statements, using these rules as reliable references for error localization and repair.

Result: RulER was evaluated on Java-to-C++ and Python-to-C++ translations from four code translation models, significantly outperforming BatFix and TransMap with 20% higher error localization and 272% higher repair success rates. It also surpasses direct LLM prompting for patch generation.

Conclusion: RulER leverages translation rules extracted from LLM outputs to enhance debugging of code translations, providing a reliable and reusable reference for effective error localization and repair. It demonstrates substantial improvements over existing methods and highlights a new approach to extracting coding knowledge from LLMs.

Abstract: Automated code translation aims to convert programs between different
programming languages while maintaining their functionality. Due to the
imperfections of code translation models, the generated translations may
contain errors that compromise their reliability. Existing automated debugging
methods for code translation rely on code alignments and repair patch templates
to locate and fix erroneous translations. However, existing methods lack
reliable references to construct code alignments and design repair patch
templates, which significantly impacts their localization accuracy and repair
effectiveness. To address these limitations, we reintroduce code translation
rules and propose a rule-based debugging method for code translation, called
RulER. RulER automatically derives code translation rules from correct
translations generated by LLMs, enabling the efficient collection of diverse
translation rules. In addition, RulER dynamically combines the existing rules
on expandable nodes like expressions and tokens to further adaptively align
more statements. These rules capture clear and detailed structural
correspondences between source and target programming languages. Therefore,
they can serve as reliable and reusable references for code alignment and
repair template design, enabling RulER to locate and fix translation errors
effectively. Our evaluation of RulER on Java-to-C++ and Python-to-C++
translations produced by four code translation models demonstrates that RulER
outperforms state-of-the-art methods, BatFix and TransMap. Our experimental
results show that RulER outperformed the best baseline by 20% and 272% in terms
of error localization rates and repair success rates, respectively. RulER
exhibits superior repair performance compared to directly prompting LLMs for
patch generation, demonstrating a promising methodology for extracting and
leveraging coding knowledge from LLMs.

</details>


### [18] [CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects](https://arxiv.org/abs/2509.14856)
*Hanyang Guo,Xunjin Zheng,Zihan Liao,Hang Yu,Peng DI,Ziyin Zhang,Hong-Ning Dai*

Main category: cs.SE

TL;DR: Current benchmarks for automated code review are inadequate due to lack of real-world complexity and context. CodeFuse-CR-Bench fills this gap, enabling realistic and comprehensive evaluation of LLMs. Tests reveal no LLM excels universally; Gemini 2.5 Pro performs best overall. Diverse robustness among LLMs highlights the need for richer evaluation methods and informs the development of smarter code review assistants.


<details>
  <summary>Details</summary>
Motivation: Existing automated code review benchmarks use simplified, context-poor data and evaluate only isolated sub-tasks. This doesn't reflect the complex, context-rich nature of real-world code review, thus limiting LLM progress.

Method: The authors introduced CodeFuse-CR-Bench, a new comprehensiveness-aware benchmark for evaluating repository-level automated code review. The benchmark includes 601 curated instances from 70 Python projects, spanning nine PR problem domains and providing rich, multi-faceted context per instance. They proposed a novel evaluation framework that blends rule-based and model-based review quality assessments.

Result: A large-scale evaluation of advanced LLMs using this benchmark shows no single LLM is best at all aspects of code review. Gemini 2.5 Pro achieved the highest overall performance, and different LLMs vary in robustness to redundant context.

Conclusion: Holistic, multi-dimensional evaluation is essential for advancing intelligent and practical code review tools. The benchmark and findings offer actionable insights for future development of LLM-based code review systems.

Abstract: Automated code review (CR) is a key application for Large Language Models
(LLMs), but progress is hampered by a "reality gap": existing benchmarks
evaluate models on isolated sub-tasks using simplified, context-poor data. This
fails to reflect the holistic context-rich nature of real-world CR. To bridge
this gap, we introduce CodeFuse-CR-Bench, the first comprehensiveness-aware
benchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601
high-quality instances from 70 Python projects covering nine Pull-Request (PR)
problem domains, where each instance provides rich, multi-faceted context
including the associated issue, PR details, and repository state, enabling
end-to-end evaluation. Beyond superficial metrics, we also propose a novel
evaluation framework that combines rule-based checks for location and syntax
with model-based judgments of review quality. We present the first large-scale
assessment of state-of-the-art LLMs on this comprehensive CR task. Our results
establish crucial baselines and reveal that (1) no single LLM dominates all
aspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive
performance; and (3) different LLMs exhibit varying robustness to redundant
context. These findings highlight the necessity of holistic, multi-dimensional
evaluation and provide actionable insights for advancing truly intelligent yet
practical CR assistants.

</details>


### [19] [CARGO: A Framework for Confidence-Aware Routing of Large Language Models](https://arxiv.org/abs/2509.14899)
*Amine Barrak,Yosr Fourati,Michael Olchawa,Emna Ksontini,Khalil Zoghlami*

Main category: cs.SE

TL;DR: CARGO is a lightweight, confidence-guided routing framework for choosing the best LLM for a given prompt. It achieves high accuracy and expert-level routing with minimal overhead, making it suitable for real-world, multi-model LLM use.


<details>
  <summary>Details</summary>
Motivation: With the increase in the number and variety of large language models (LLMs), it has become important to efficiently route user prompts to the best-suited model, balancing the trade-offs between performance and cost. Existing routing systems often require significant supervision or are not flexible enough for domain-specific needs.

Method: The authors propose CARGO (Category-Aware Routing with Gap-based Optimization), a lightweight, confidence-aware two-stage framework. It uses an embedding-based regressor trained on pairwise LLM output comparisons to predict which model will perform best for a given prompt. If the regressor's prediction is uncertain, it invokes a binary classifier. CARGO also provides category-specific regressors trained on task groups such as mathematics, coding, reasoning, summarization, and creative writing.

Result: CARGO was evaluated on four leading LLMs: GPT-4o, Claude 3.5 Sonnet, DeepSeek V3, and Perplexity Sonar. It achieved a top-1 routing accuracy of 76.4% and win rates between 72% and 89% against individual expert models.

Conclusion: The study demonstrates that CARGO's confidence-guided, lightweight routing system achieves expert-level performance in model selection for user prompts with minimal computational overhead. This provides a practical and efficient solution for deploying multiple LLMs in real-world applications.

Abstract: As large language models (LLMs) proliferate in scale, specialization, and
latency profiles, the challenge of routing user prompts to the most appropriate
model has become increasingly critical for balancing performance and cost. We
introduce CARGO (Category-Aware Routing with Gap-based Optimization), a
lightweight, confidence-aware framework for dynamic LLM selection. CARGO
employs a single embedding-based regressor trained on LLM-judged pairwise
comparisons to predict model performance, with an optional binary classifier
invoked when predictions are uncertain. This two-stage design enables precise,
cost-aware routing without the need for human-annotated supervision. To capture
domain-specific behavior, CARGO also supports category-specific regressors
trained across five task groups: mathematics, coding, reasoning, summarization,
and creative writing. Evaluated on four competitive LLMs (GPT-4o, Claude 3.5
Sonnet, DeepSeek V3, and Perplexity Sonar), CARGO achieves a top-1 routing
accuracy of 76.4% and win rates ranging from 72% to 89% against individual
experts. These results demonstrate that confidence-guided, lightweight routing
can achieve expert-level performance with minimal overhead, offering a
practical solution for real-world, multi-model LLM deployments.

</details>


### [20] ["Let it be Chaos in the Plumbing!" Usage and Efficacy of Chaos Engineering in DevOps Pipelines](https://arxiv.org/abs/2509.14931)
*Stefano Fossati,Damian Andrew Tamburri,Massimiliano Di Penta,Marco Tonnarelli*

Main category: cs.SE

TL;DR: Chaos Engineering remains core to system resilience, but industry practice is evolving towards more automation, risk-aware processes, and controlled experiments. This paper summarizes recent trends and provides a new framework for future CE use.


<details>
  <summary>Details</summary>
Motivation: Chaos Engineering (CE) was created to proactively improve the resilience of distributed systems by simulating failures, especially beneficial within the DevOps context. The paper aims to understand how industry practitioners have recently adopted and adapted CE to address evolving production challenges.

Method: The authors conducted a systematic gray literature review, analyzing 50 sources published between 2019 and early 2024. They developed a classification framework extending the foundational concepts of CE into ten defined categories.

Result: The review found that while CE's foundational principles are still key, practitioners now focus more on controlled experimentation, automation, and risk mitigation. This reflects a shift to accommodate agile and rapidly changing DevOps environments.

Conclusion: The study advances understanding of both the theory and practice of CE, offering a new conceptual framework and practical insights for applying CE to enhance system robustness. It provides clear guidance for future research and industrial implementations of CE in dynamic production settings.

Abstract: Chaos Engineering (CE) has emerged as a proactive method to improve the
resilience of modern distributed systems, particularly within DevOps
environments. Originally pioneered by Netflix, CE simulates real-world failures
to expose weaknesses before they impact production. In this paper, we present a
systematic gray literature review that investigates how industry practitioners
have adopted and adapted CE principles over recent years. Analyzing 50 sources
published between 2019 and early 2024, we developed a comprehensive
classification framework that extends the foundational CE principles into ten
distinct concepts. Our study reveals that while the core tenets of CE remain
influential, practitioners increasingly emphasize controlled experimentation,
automation, and risk mitigation strategies to align with the demands of agile
and continuously evolving DevOps pipelines. Our results enhance the
understanding of how CE is intended and implemented in practice, and offer
guidance for future research and industrial applications aimed at improving
system robustness in dynamic production environments.

</details>


### [21] [Orion: Fuzzing Workflow Automation](https://arxiv.org/abs/2509.15195)
*Max Bazalii,Marius Fleischer*

Main category: cs.SE

TL;DR: Orion automates fuzz testing using LLMs and traditional tools, greatly reducing human effort and finding new vulnerabilities in real software.


<details>
  <summary>Details</summary>
Motivation: Fuzz testing processes are still highly manual despite automation in input generation and monitoring; prior solutions only address individual components, not the whole workflow. Orion is motivated by the need to automate the entire fuzzing process to scale vulnerability discovery beyond human-centric limits.

Method: Orion integrates LLM-based reasoning for semantic tasks (like code understanding) with deterministic tools for tasks needing high precision (such as verification and refinement), automating the main manual fuzzing bottlenecks. Its effectiveness is measured both by reduction in human effort and by discovery of novel vulnerabilities.

Result: This paper introduces Orion, a framework that automates much of the manual work involved in fuzzing, such as code analysis, harness configuration, and result triage, by integrating large language models (LLMs) with traditional analysis tools. Orion employs LLMs for semantic understanding and guidance, and deterministic tools for precise or iterative tasks. The results show that Orion reduces the required human effort by 46-204 times, depending on the workflow stage, and uncovers two new vulnerabilities in the clib library.

Conclusion: Orion enables highly automated fuzzing campaigns, making them practical even for large or complex projects, as it minimizes human involvement and is effective in discovering real-world software vulnerabilities.

Abstract: Fuzz testing is one of the most effective techniques for finding software
vulnerabilities. While modern fuzzers can generate inputs and monitor
executions automatically, the overall workflow, from analyzing a codebase, to
configuring harnesses, to triaging results, still requires substantial manual
effort. Prior attempts focused on single stages such as harness synthesis or
input minimization, leaving researchers to manually connect the pieces into a
complete fuzzing campaign.
  We introduce Orion, a framework that automates the the manual bottlenecks of
fuzzing by integrating LLM reasoning with traditional tools, allowing campaigns
to scale to settings where human effort alone was impractical. Orion uses LLMs
for code reasoning and semantic guidance, while relying on deterministic tools
for verification, iterative refinement, and tasks that require precision.
Across our benchmark suite, Orion reduces human effort by 46-204x depending on
the workflow stage, and we demonstrate its effectiveness through the discovery
of two previously unknown vulnerabilities in the widely used open-source clib
library.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [22] [DeliverC: Teaching Pointers through GenAI-Powered Game-Based Learning](https://arxiv.org/abs/2509.14496)
*Wyatt Petula,Anushcka Joshi,Peggy Tu,Amrutha Somasundar,Suman Saha*

Main category: cs.PL

TL;DR: DeliverC, a GenAI-powered game for learning C pointers, boosted student confidence and understanding, though feedback sometimes lacked clarity. The study shows promise for combining GenAI and games in programming education, but future refinement of feedback quality is needed.


<details>
  <summary>Details</summary>
Motivation: Teaching complex programming concepts like C pointers is challenging, and existing game-based tools often lack adaptive real-time support.

Method: Developed DeliverC, a GenAI-powered game using GPT-4-mini to give personalized hints and dynamically create pointer-related challenges. Conducted a pilot study with 25 students using gameplay data and a 15-item survey to assess motivation, self-efficacy, metacognition, and feedback quality.

Result: Most students gained confidence and reflection, and error rates dropped as levels became more scaffolded. Engagement dropped with increased difficulty, and feedback sometimes lacked clarity.

Conclusion: DeliverC shows promise in enhancing student engagement and learning in systems programming, but AI-generated feedback needs improvement for clarity. Game-based, GenAI-enhanced tools can support personalized and interactive practice in tough programming areas.

Abstract: While game-based learning is widely used in programming education, few tools
offer adaptive, real-time support for complex topics, such as C pointers. We
present DeliverC, a GenAI-enhanced game that integrates GPT-4-mini to provide
personalized hints and generate pointer-related challenges on the fly. In a
pilot study involving 25 undergraduate students, we investigated the impact of
the system on learning through gameplay data and a 15-item survey that covered
constructs such as motivation, self-efficacy, metacognition, and feedback
quality. Results show that most students felt more confident and reflective
after using the tool, and error rates decreased as students progressed through
scaffolded levels. However, participation decreased with task difficulty, and
some students reported receiving unclear or vague feedback. These findings
suggest that DeliverC can enhance engagement and understanding in systems
programming, although refinement in AI-generated feedback is still needed. Our
study highlights the potential of combining GenAI with game-based learning to
support personalized and interactive practice in traditionally challenging
programming domains.

</details>


### [23] [Refinement-Types Driven Development: A study](https://arxiv.org/abs/2509.15005)
*Facundo Domínguez,Arnaud Spiwack*

Main category: cs.PL

TL;DR: SMT solvers aren't just for formal verification—they can enhance everyday programming when combined with refinement types in languages like Haskell. The paper shows this through a case study and a practical implementation, suggesting a more enjoyable programming future with such tools.


<details>
  <summary>Details</summary>
Motivation: SMT solvers are typically used for formal verification, but the authors believe their power can be beneficial for everyday programming tasks if integrated with the compiler's type system.

Method: The paper integrates SMT solvers into static checks via refinement types in Liquid Haskell and demonstrates this approach with a case study on handling binder scopes in compilers. They also implement a prototype theory of finite maps for Liquid Haskell's solver.

Result: The use of SMT solvers, through refinement types, significantly augments traditional type checkers, making programming both simpler and more robust. The theory of finite maps prototype supports enhanced reasoning abilities in Liquid Haskell.

Conclusion: SMT solvers, when used with refinement types, can make everyday programming tasks safer and easier, and future programming could benefit greatly from such integration. The presented finite map theory prototype demonstrates how these capabilities can be realized in practice.

Abstract: This paper advocates for the broader application of SMT solvers in everyday
programming, challenging the conventional wisdom that these tools are solely
for formal methods and verification. We claim that SMT solvers, when seamlessly
integrated into a compiler's static checks, significantly enhance the
capabilities of ordinary type checkers in program composition. Specifically, we
argue that refinement types, as embodied by Liquid Haskell, enable the use of
SMT solvers in mundane programming tasks. Through a case study on handling
binder scopes in compilers, we envision a future where ordinary programming is
made simpler and more enjoyable with the aid of refinement types and SMT
solvers. As a secondary contribution, we present a prototype implementation of
a theory of finite maps for Liquid Haskell's solver, developed to support our
case study.

</details>
