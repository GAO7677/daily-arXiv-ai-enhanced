<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 13]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Pattern-Based File and Data Access with Python Glob: A Comprehensive Guide for Computational Research](https://arxiv.org/abs/2509.08843)
*Sidney Shapiro*

Main category: cs.SE

TL;DR: This paper showcases the Python glob module as a crucial, under-recognized tool for scalable, reproducible file access and management in research workflows, offering practical examples with popular analytical libraries and advocating glob as the default reference for pattern-based file matching in Python.


<details>
  <summary>Details</summary>
Motivation: Pattern-based file access is essential yet under-documented in computational research, and there is a lack of concise references for leveraging Python's glob for file pattern matching in data-driven workflows.

Method: The paper describes use cases and provides concrete Python code examples using glob in combination with libraries such as pandas, scikit-learn, and matplotlib to demonstrate practical application in various analytical domains.

Result: The paper shows that glob enables efficient, scalable file traversal and integration with data analytics pipelines, supporting large-scale data ingestion, organizational analysis, and AI dataset construction.

Conclusion: Python's glob module is established as a methodological building block for reproducible research and data engineering. The paper serves as a concise bridge between foundational concepts and applied practices, aiming to make glob the go-to citation for file pattern matching in Python.

Abstract: Pattern-based file access is a fundamental but often under-documented aspect
of computational research. The Python glob module provides a simple yet
powerful way to search, filter, and ingest files using wildcard patterns,
enabling scalable workflows across disciplines. This paper introduces glob as a
versatile tool for data science, business analytics, and artificial
intelligence applications. We demonstrate use cases including large-scale data
ingestion, organizational data analysis, AI dataset construction, and
reproducible research practices. Through concrete Python examples with widely
used libraries such as pandas,scikit-learn, and matplotlib, we show how glob
facilitates efficient file traversal and integration with analytical pipelines.
By situating glob within the broader context of reproducible research and data
engineering, we highlight its role as a methodological building block. Our goal
is to provide researchers and practitioners with a concise reference that
bridges foundational concepts and applied practice, making glob a default
citation for file pattern matching in Python-based research workflows.

</details>


### [2] [A Systematic Mapping Study on Chatbots in Programming Education](https://arxiv.org/abs/2509.08857)
*Marcelino Garcia,Renato Garcia,Arthur Parizotto,Andre Mendes,Pedro Valle,Ricardo Vilela,Renato Balancieri,Williamson Silva*

Main category: cs.SE

TL;DR: This paper systematically reviews research on educational chatbots for programming. Python is the main programming language, and most chatbots teach foundational concepts. The study maps current trends, gaps, and offers guidance for new tool development.


<details>
  <summary>Details</summary>
Motivation: Educational chatbots are increasingly used to support teaching programming, especially for beginners. The motivation is to systematically map and review how these agents are developed and used in programming education.

Method: Systematic Mapping Study (SMS): The study reviewed 3,216 publications and selected 54 relevant papers. These were analyzed according to five subquestions: chatbot types, programming languages, educational content, interaction models, and application contexts.

Result: The study finds most chatbots are designed for teaching Python, with a focus on basic programming concepts. There is a range of pedagogical strategies and technological architectures employed. The review also points out trends and gaps in current literature.

Conclusion: The findings highlight the predominance of Python-focused educational chatbots, diverse teaching approaches, and varied technical solutions. The study provides recommendations and insights for future development of programming education tools.

Abstract: Educational chatbots have gained prominence as support tools for teaching
programming, particularly in introductory learning contexts. This paper
presents a Systematic Mapping Study (SMS) that investigated how such agents
have been developed and applied in programming education. From an initial set
of 3,216 publications, 54 studies were selected and analyzed based on five
research subquestions, addressing chatbot types, programming languages used,
educational content covered, interaction models, and application contexts. The
results reveal a predominance of chatbots designed for Python instruction,
focusing on fundamental programming concepts, and employing a wide variety of
pedagogical approaches and technological architectures. In addition to
identifying trends and gaps in the literature, this study provides insights to
inform the development of new educational tools for programming instruction.

</details>


### [3] [GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial Analysis-Function Calling vs Code Generation](https://arxiv.org/abs/2509.08863)
*Qianqian Luo,Liuchang Xu,Qingming Lin,Sensen Wu,Ruichen Mao,Chao Wang,Hailin Feng,Bo Huang,Zhenhong Du*

Main category: cs.SE

TL;DR: This paper introduces GeoJSON Agents, a multi-agent LLM framework for automating GIS tasks, leveraging Function Calling and Code Generation methods, and demonstrates substantial performance gains over general-purpose models in processing GeoJSON data.


<details>
  <summary>Details</summary>
Motivation: LLMs lack specialized GIS expertise despite their rapid advancement. The authors aim to address these shortcomings by leveraging LLMs within a dedicated GIS multi-agent framework, ultimately increasing the accuracy and scalability of spatial data analysis.

Method: The authors designed a multi-agent architecture with three key components: task parsing, agent collaboration, and result integration. They benchmarked two approaches (Function Calling and Code Generation) using 70 diverse GIS tasks and evaluated their performance with GPT-4o as the model backbone.

Result: Function Calling-based GeoJSON Agents achieved 85.71% accuracy, while Code Generation-based agents reached 97.14%, both far surpassing the accuracy of standard LLMs (48.57%). The Code Generation method offered more flexibility, whereas Function Calling provided greater stability.

Conclusion: GeoJSON Agents significantly increase the automation and effectiveness of GIS tasks. The Code Generation approach delivers higher accuracy and flexibility, while the Function Calling method ensures more stable execution. Both methods outperform general LLMs, marking an important step towards advanced GeoAI systems.

Abstract: LLMs have made substantial progress in task automation and natural language
understanding.However,without expertise in GIS,they continue to encounter
limitations.To address these issues, we propose GeoJSON Agents-a multi-agent
LLM architecture.This framework transforms natural language tasks into
structured GeoJSON operation commands and processes spatial data using two
widely adopted LLM enhancement techniques:Function Calling and Code
Generation.The architecture consists of three components-task parsing,agent
collaboration,and result integration-aimed at enhancing both the performance
and scalability of GIS automation.The Planner agent interprets natural language
tasks into structured GeoJSON commands.Then,specialized Worker agents
collaborate according to assigned roles to perform spatial data processing and
analysis,either by invoking predefined function APIs or by dynamically
generating and executing Python-based spatial analysis code.Finally,the system
integrates the outputs from multiple execution rounds into
reusable,standards-compliant GeoJSON files.To systematically evaluate the
performance of the two approaches,we constructed a benchmark dataset of 70
tasks with varying complexity and conducted experiments using OpenAI's GPT-4o
as the core model.Results indicate that the Function Calling-based GeoJSON
Agent achieved an accuracy of 85.71%,while the Code Generation-based agent
reached 97.14%,both significantly outperforming the best-performing
general-purpose model (48.57%).Further analysis reveals that the Code
Generation provides greater flexibility,whereas the Function Calling approach
offers more stable execution.This study is the first to introduce an LLM
multi-agent framework for GeoJSON data and to compare the strengths and
limitations of two mainstream LLM enhancement methods,offering new perspectives
for improving GeoAI system performance.

</details>


### [4] [TraceRAG: A LLM-Based Framework for Explainable Android Malware Detection and Behavior Analysis](https://arxiv.org/abs/2509.08865)
*Guangyu Zhang,Xixuan Wang,Shiyu Sun,Peiyan Xiao,Kun Sun,Yanhai Xiong*

Main category: cs.SE

TL;DR: TraceRAG uses large language models to summarize Android code, retrieve relevant behavior based on natural language questions, and generate easy-to-understand reports, achieving high accuracy in malware detection and behavior identification.


<details>
  <summary>Details</summary>
Motivation: Malicious Android applications often use advanced evasion techniques, hiding harmful behaviors within legitimate code, making them hard to detect with traditional methods. There is a critical need for robust frameworks that can not only recover these hidden behaviors but also provide explainable results.

Method: TraceRAG, a retrieval-augmented generation (RAG) framework, is introduced. It generates method-level code summaries, indexes them in a vector database, retrieves relevant snippets using natural language queries, and provides multi-turn, human-readable reports that explain detected malicious behaviors along with associated code.

Result: The framework achieved a 96% malware detection accuracy and 83.81% behavior identification accuracy, validated with updated VirusTotal scans and manual checks. Expert evaluations indicate that the generated reports are practically useful.

Conclusion: TraceRAG successfully bridges the gap between code analysis and natural language explanations, making malware detection more transparent and effective for analysts. It significantly outperforms traditional methods in both detection and explainability.

Abstract: Sophisticated evasion tactics in malicious Android applications, combined
with their intricate behavioral semantics, enable attackers to conceal
malicious logic within legitimate functions, underscoring the critical need for
robust and in-depth analysis frameworks. However, traditional analysis
techniques often fail to recover deeply hidden behaviors or provide
human-readable justifications for their decisions. Inspired by advances in
large language models (LLMs), we introduce TraceRAG, a retrieval-augmented
generation (RAG) framework that bridges natural language queries and Java code
to deliver explainable malware detection and analysis. First, TraceRAG
generates summaries of method-level code snippets, which are indexed in a
vector database. At query time, behavior-focused questions retrieve the most
semantically relevant snippets for deeper inspection. Finally, based on the
multi-turn analysis results, TraceRAG produces human-readable reports that
present the identified malicious behaviors and their corresponding code
implementations. Experimental results demonstrate that our method achieves 96\%
malware detection accuracy and 83.81\% behavior identification accuracy based
on updated VirusTotal (VT) scans and manual verification. Furthermore, expert
evaluation confirms the practical utility of the reports generated by TraceRAG.

</details>


### [5] [Benchmarking Energy Efficiency of Large Language Models Using vLLM](https://arxiv.org/abs/2509.08867)
*K. Pronk,Q. Zhao*

Main category: cs.SE

TL;DR: The paper presents a new benchmark for measuring energy efficiency of LLMs under realistic conditions, highlighting key factors affecting energy use and helping developers make sustainable choices.


<details>
  <summary>Details</summary>
Motivation: The increasing deployment of Large Language Models (LLMs) is leading to significant energy consumption, impacting climate change. Developers need more accurate data on energy efficiency to make sustainable choices, but current benchmarks do not adequately represent real-world LLM usage.

Method: The paper introduces the LLM Efficiency Benchmark, which simulates realistic production scenarios using vLLM, a high-throughput, production-ready backend. It examines the effects of model size, architecture, and request concurrency on inference energy efficiency.

Result: The study shows that energy efficiency benchmarks can be designed to better align with practical conditions. Factors such as model size, architecture, and concurrent requests significantly influence energy consumption during inference.

Conclusion: The LLM Efficiency Benchmark provides more applicable insights into energy use in real-world deployments, assisting developers to build more sustainable AI solutions.

Abstract: The prevalence of Large Language Models (LLMs) is having an growing impact on
the climate due to the substantial energy required for their deployment and
use. To create awareness for developers who are implementing LLMs in their
products, there is a strong need to collect more information about the energy
efficiency of LLMs. While existing research has evaluated the energy efficiency
of various models, these benchmarks often fall short of representing realistic
production scenarios. In this paper, we introduce the LLM Efficiency Benchmark,
designed to simulate real-world usage conditions. Our benchmark utilizes vLLM,
a high-throughput, production-ready LLM serving backend that optimizes model
performance and efficiency. We examine how factors such as model size,
architecture, and concurrent request volume affect inference energy efficiency.
Our findings demonstrate that it is possible to create energy efficiency
benchmarks that better reflect practical deployment conditions, providing
valuable insights for developers aiming to build more sustainable AI systems.

</details>


### [6] [CLARA: A Developer's Companion for Code Comprehension and Analysis](https://arxiv.org/abs/2509.09072)
*Ahmed Adnan,Mushfiqur Rahman,Saad Sakib Noor,Kazi Sakib*

Main category: cs.SE

TL;DR: CLARA is a context-aware browser extension that uses advanced inference models to help users comprehend, refactor, and evaluate the quality of code in open-source projects without extensive setup, demonstrating strong usability and accuracy in both evaluations and a user study.


<details>
  <summary>Details</summary>
Motivation: Developers and researchers frequently need to comprehend and analyze open-source project codebases, but current tools require prior setup, lack context-awareness, and demand substantial manual effort.

Method: CLARA, a browser extension, leverages a state-of-the-art inference model to provide assistance in understanding code files and fragments, code refactoring, and code quality detection. Its effectiveness was assessed via qualitative evaluation using existing datasets and methodologies, as well as a comprehensive user study with 10 developers and researchers.

Result: The study found that CLARA is useful, accurate, and practical for code comprehension and analysis tasks.

Conclusion: CLARA significantly improves the practicality and efficiency of code comprehension and analysis for developers and researchers.

Abstract: Code comprehension and analysis of open-source project codebases is a task
frequently performed by developers and researchers. However, existing tools
that practitioners use for assistance with such tasks often require prior
project setup, lack context-awareness, and involve significant manual effort.
To address this, we present CLARA, a browser extension that utilizes a
state-of-the-art inference model to assist developers and researchers in: (i)
comprehending code files and code fragments, (ii) code refactoring, and (iii)
code quality attribute detection. We qualitatively evaluated CLARA's inference
model using existing datasets and methodology, and performed a comprehensive
user study with 10 developers and academic researchers to assess its usability
and usefulness. The results show that CLARA is useful, accurate, and practical
in code comprehension and analysis tasks. CLARA is an open-source tool
available at https://github.com/SaadNoor555/CLARA_tool_demo. A video showing
the full capabilities of CLARA can be found at
https://youtu.be/VDKVXvIH41Q?si=qBFsmS_Y4m_9x3YH.

</details>


### [7] [Probing Pre-trained Language Models on Code Changes: Insights from ReDef, a High-Confidence Just-in-Time Defect Prediction Dataset](https://arxiv.org/abs/2509.09192)
*Doha Nam,Taehyoun Kim,Duksan Ryu,Jongmoon Baik*

Main category: cs.SE

TL;DR: A new, reliable benchmark dataset (ReDef) enables better evaluation of code change defect prediction, showing that pre-trained code models mostly depend on shallow input cues rather than truly understanding code modifications.


<details>
  <summary>Details</summary>
Motivation: The motivation comes from the need to improve the reliability and precision of Just-in-Time software defect prediction (JIT-SDP), which suffers from noisy and imprecise datasets for identifying bug-inducing code changes.

Method: The authors introduce 'ReDef,' a new dataset of function-level code modifications identified as defective or clean in C/C++ projects. Defective cases are based on revert commits, clean ones are history-validated, and ambiguous cases are filtered via a GPT-assisted triage process. They use this dataset to systematically evaluate pre-trained language models (PLMs) on code change reasoning using various encoding strategies and counterfactual perturbations.

Result: Compact diff-style encoding of code changes leads to consistently better performance across PLMs compared to whole-function encoding. However, the PLMs' seemingly robust performance against counterfactual tests reveals reliance on superficial input cues, not true semantic understanding of code modifications.

Conclusion: The ReDef dataset provides a higher-quality benchmark for JIT-SDP, and while encoding choice matters for PLM performance on code modification reasoning, current models still lack true comprehension of code edits and depend on shallow cues.

Abstract: Just-in-Time software defect prediction (JIT-SDP) plays a critical role in
prioritizing risky code changes during code review and continuous integration.
However, existing datasets often suffer from noisy labels and low precision in
identifying bug-inducing commits. To address this, we present ReDef
(Revert-based Defect dataset), a high-confidence benchmark of function-level
modifications curated from 22 large-scale C/C++ projects. Defective cases are
anchored by revert commits, while clean cases are validated through post-hoc
history checks. Ambiguous instances are conservatively filtered out via a
GPT-assisted triage process involving multiple votes and audits. This pipeline
yields 3,164 defective and 10,268 clean modifications, offering substantially
more reliable labels than prior existing resources. Beyond dataset
construction, we provide the first systematic evaluation of how pre-trained
language models (PLMs) reason about code modifications -- specifically, which
input encodings most effectively expose change information, and whether models
genuinely capture edit semantics. We fine-tune CodeBERT, CodeT5+, and UniXcoder
under five encoding strategies, and further probe their sensitivity through
counterfactual perturbations that swap added/deleted blocks, invert diff
polarity, or inject spurious markers. Our results show that compact diff-style
encodings consistently outperform whole-function formats across all PLMs, with
statistical tests confirming large, model-independent effects. However, under
counterfactual tests, performance degrades little or not at all -- revealing
that what appears to be robustness in fact reflects reliance on superficial
cues rather than true semantic understanding. These findings indicate that,
unlike in snapshot-based tasks, current PLMs remain limited in their ability to
genuinely comprehend code modifications.

</details>


### [8] [On Integrating Large Language Models and Scenario-Based Programming for Improving Software Reliability](https://arxiv.org/abs/2509.09194)
*Ayelet Berzack,Guy Katz*

Main category: cs.SE

TL;DR: LLMs are powerful for code generation but can be confidently wrong. By integrating LLMs with scenario-based programming, developers can guide, verify, and correct AI outputs. The proposed structured approach reduced errors and allowed partial formal verification in a Connect4 agent case study, showing its practical value.


<details>
  <summary>Details</summary>
Motivation: LLMs, despite their capabilities, can produce erroneous code with convincing confidence. This poses a risk in software development. The authors are motivated to harness LLM productivity while mitigating error risk, seeking a more reliable integration with established software engineering techniques.

Method: They propose a structured methodology that combines LLMs with traditional software engineering, particularly focusing on the Scenario-Based Programming (SBP) paradigm—an event-driven approach allowing human expertise to guide, inspect, and verify LLM output.

Result: Using their methodology, they designed and implemented a Connect4 game agent. This combined approach resulted in a highly competent agent, capable of defeating strong existing agents, and allowed for formal verification of correctness in some cases. They also observed favorable ease-of-use with their method.

Conclusion: Combining LLMs with SBP leverages human expertise to guide AI-driven code creation, improves reliability, enables formal verification, and offers a practical, user-friendly workflow for software development.

Abstract: Large Language Models (LLMs) are fast becoming indispensable tools for
software developers, assisting or even partnering with them in crafting complex
programs. The advantages are evident -- LLMs can significantly reduce
development time, generate well-organized and comprehensible code, and
occasionally suggest innovative ideas that developers might not conceive on
their own. However, despite their strengths, LLMs will often introduce
significant errors and present incorrect code with persuasive confidence,
potentially misleading developers into accepting flawed solutions.
  In order to bring LLMs into the software development cycle in a more reliable
manner, we propose a methodology for combining them with ``traditional''
software engineering techniques in a structured way, with the goal of
streamlining the development process, reducing errors, and enabling users to
verify crucial program properties with increased confidence. Specifically, we
focus on the Scenario-Based Programming (SBP) paradigm -- an event-driven,
scenario-based approach for software engineering -- to allow human developers
to pour their expert knowledge into the LLM, as well as to inspect and verify
its outputs.
  To evaluate our methodology, we conducted a significant case study, and used
it to design and implement the Connect4 game. By combining LLMs and SBP we were
able to create a highly-capable agent, which could defeat various strong
existing agents. Further, in some cases, we were able to formally verify the
correctness of our agent. Finally, our experience reveals interesting insights
regarding the ease-of-use of our proposed approach. The full code of our
case-study will be made publicly available with the final version of this
paper.

</details>


### [9] [Altered Histories in Version Control System Repositories: Evidence from the Trenches](https://arxiv.org/abs/2509.09294)
*Solal Rapaport,Laurent Pautet,Samuel Tardieu,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: Large-scale study finds that rewriting history in public Git repositories is common, often introducing security and governance risks. The authors present the GitHistorian tool to help spot and describe such alterations, supporting better management of repository integrity.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the risks introduced when developers rewrite history in public version control system (VCS) branches, which can compromise repository integrity, reproducibility, and security, especially in the context of the software supply chain.

Method: The authors conducted a large-scale analysis of 111 million repositories archived by Software Heritage, searching for and categorizing Git history alterations. Two case studies were performed to illustrate the real-world impact of such alterations. Additionally, the authors developed GitHistorian, a tool to detect and describe history changes in Git repositories.

Result: The investigation found history alterations in 1.22 million repositories, amounting to 8.7 million rewritten histories. The study revealed that these alterations often involve retroactively changing licenses or removing secrets such as private keys, both considered bad practices. The GitHistorian tool was introduced to help detect such events.

Conclusion: History rewriting in public Git repositories is a widespread phenomenon with significant implications for security and project governance. The authors provide empirical insights and a practical tool (GitHistorian) to address these risks and assist developers in identifying problematic history changes.

Abstract: Version Control Systems (VCS) like Git allow developers to locally rewrite
recorded history, e.g., to reorder and suppress commits or specific data in
them. These alterations have legitimate use cases, but become problematic when
performed on public branches that have downstream users: they break push/pull
workflows, challenge the integrity and reproducibility of repositories, and
create opportunities for supply chain attackers to sneak into them nefarious
changes. We conduct the first large-scale investigation of Git history
alterations in public code repositories. We analyze 111 M (millions)
repositories archived by Software Heritage, which preserves VCS histories even
across alterations. We find history alterations in 1.22 M repositories, for a
total of 8.7 M rewritten histories. We categorize changes by where they happen
(which repositories, which branches) and what is changed in them (files or
commit metadata). Conducting two targeted case studies we show that altered
histories recurrently change licenses retroactively, or are used to remove
''secrets'' (e.g., private keys) committed by mistake. As these behaviors
correspond to bad practices-in terms of project governance or security
management, respectively-that software recipients might want to avoid, we
introduce GitHistorian, an automated tool, that developers can use to spot and
describe history alterations in public Git repositories.

</details>


### [10] [Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on Open & Industry Data](https://arxiv.org/abs/2509.09313)
*Moritz Mock,Thomas Forrer,Barbara Russo*

Main category: cs.SE

TL;DR: Academic deep learning vulnerability detectors often don't fit industrial needs. The paper evaluates CodeBERT for vulnerability detection across domains, develops an integrated tool (AI-DO), and finds that domain-specific training is strong locally but lacks cross-domain power, while open-source fine-tuning with undersampling helps generalizability.


<details>
  <summary>Details</summary>
Motivation: Many academic deep learning solutions for vulnerability detection are not accessible or easily applicable in industrial settings, presenting issues of trust, compatibility, expertise gaps, and workflow integration.

Method: The authors evaluate CodeBERT's performance in detecting vulnerable functions across both industrial and open-source software. They analyze cross-domain generalization, class imbalance strategies, and develop AI-DO, a CI/CD-integrated recommender system using fine-tuned CodeBERT. The tool's usefulness is assessed via a professional IT survey.

Result: CodeBERT models trained on industrial data perform well on data from the same domain but do not generalize to open-source code. Fine-tuning on open-source data with undersampling improves vulnerability detection.

Conclusion: Industrial data-trained models are effective within their own domain but not across domains. Fine-tuning on diverse data and managing class imbalance are crucial for improved vulnerability detection.

Abstract: Deep learning solutions for vulnerability detection proposed in academic
research are not always accessible to developers, and their applicability in
industrial settings is rarely addressed. Transferring such technologies from
academia to industry presents challenges related to trustworthiness, legacy
systems, limited digital literacy, and the gap between academic and industrial
expertise. For deep learning in particular, performance and integration into
existing workflows are additional concerns. In this work, we first evaluate the
performance of CodeBERT for detecting vulnerable functions in industrial and
open-source software. We analyse its cross-domain generalisation when
fine-tuned on open-source data and tested on industrial data, and vice versa,
also exploring strategies for handling class imbalance. Based on these results,
we develop AI-DO(Automating vulnerability detection Integration for Developers'
Operations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated
recommender system that uses fine-tuned CodeBERT to detect and localise
vulnerabilities during code review without disrupting workflows. Finally, we
assess the tool's perceived usefulness through a survey with the company's IT
professionals. Our results show that models trained on industrial data detect
vulnerabilities accurately within the same domain but lose performance on
open-source code, while a deep learner fine-tuned on open data, with
appropriate undersampling techniques, improves the detection of
vulnerabilities.

</details>


### [11] [ORCA: Unveiling Obscure Containers In The Wild](https://arxiv.org/abs/2509.09322)
*Jacopo Bufalino,Agathe Blaise,Stefano Secci*

Main category: cs.SE

TL;DR: Outdated or modified containers hinder standard SCA tools, exposing security risks. ORCA, a new tool, better analyzes obscure containers, improving coverage by a median 40% over leading alternatives.


<details>
  <summary>Details</summary>
Motivation: Modern software relies heavily on open-source components in containers, but unintentionally obscure container images undermine the reliability of Software Composition Analysis (SCA), increasing security risks.

Method: The authors analyzed 600 popular container images from well-known registries, evaluated SCA tools, and developed ORCA—a new tool designed to be resilient to container obscuration.

Result: ORCA significantly improves the detection of contents in obscure containers, achieving a median 40% better file coverage than popular tools like Docker Scout and Syft.

Conclusion: Existing SCA tools often fail to analyze obscure containers found in trusted registries. The newly proposed ORCA tool addresses these limitations, offering enhanced analysis and file coverage.

Abstract: Modern software development increasingly depends on open-source libraries and
third-party components, which are often encapsulated into containerized
environments. While improving the development and deployment of applications,
this approach introduces security risks, particularly when outdated or
vulnerable components are inadvertently included in production environments.
Software Composition Analysis (SCA) is a critical process that helps identify
and manage packages and dependencies inside a container. However, unintentional
modifications to the container filesystem can lead to incomplete container
images, which compromise the reliability of SCA tools. In this paper, we
examine the limitations of both cloud-based and open-source SCA tools when
faced with such obscure images. An analysis of 600 popular containers revealed
that obscure containers exist in well-known registries and trusted images and
that many tools fail to analyze such containers. To mitigate these issues, we
propose an obscuration-resilient methodology for container analysis and
introduce ORCA (Obscuration-Resilient Container Analyzer), its open-source
implementation. We reported our findings to all vendors using their appropriate
channels. Our results demonstrate that ORCA effectively detects the content of
obscure containers and achieves a median 40% improvement in file coverage
compared to Docker Scout and Syft.

</details>


### [12] [LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering](https://arxiv.org/abs/2509.09614)
*Jielin Qiu,Zuxin Liu,Zhiwei Liu,Rithesh Murthy,Jianguo Zhang,Haolin Chen,Shiyu Wang,Ming Zhu,Liangwei Yang,Juntao Tan,Zhepeng Cen,Cheng Qian,Shelby Heinecke,Weiran Yao,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.SE

TL;DR: LoCoBench is a new benchmark to rigorously test long-context LLMs on realistic, complex software development scenarios across many languages and tasks. Current models fall short on these challenging tasks, showing that long-context code understanding remains an unsolved problem. LoCoBench is available for further research.


<details>
  <summary>Details</summary>
Motivation: Recent advances in language models (LLMs) with extremely long context windows allow for processing millions of tokens, which creates opportunities for improved code understanding and software development tools. However, current benchmarks focus mostly on short-context tasks or single-function completion and do not adequately test these new capabilities. There is a critical need for benchmarks that assess LLMs' performance in realistic, large-scale codebase scenarios.

Method: The paper introduces LoCoBench, a new benchmark designed to evaluate long-context LLMs on realistic software development tasks. LoCoBench consists of 8,000 scenarios covering 10 programming languages, with varying context lengths from 10K to 1M tokens. It covers 8 categories of tasks crucial for software engineering, such as architectural understanding, bug investigation, and integration testing. The benchmark uses a 5-phase generation pipeline to create diverse scenarios and defines a comprehensive evaluation framework with 17 distinct metrics, including the new LoCoBench Score (LCBS).

Result: State-of-the-art long-context LLMs were evaluated using LoCoBench, and the results revealed significant performance gaps, especially when reasoning about complex, large-scale codebases across long contexts. The current generation of long-context LLMs struggle to maintain high performance in these scenarios, highlighting areas that demand further research and development.

Conclusion: Long-context LLMs have considerable potential for software engineering, but their capabilities on large-scale, realistic codebases are still lacking. LoCoBench establishes a challenging and comprehensive standard for evaluating these abilities, exposing gaps in current models and encouraging advancement in long-context code understanding. The benchmark and associated tools are publicly available for the research community.

Abstract: The emergence of long-context language models with context windows extending
to millions of tokens has created new opportunities for sophisticated code
understanding and software development evaluation. We propose LoCoBench, a
comprehensive benchmark specifically designed to evaluate long-context LLMs in
realistic, complex software development scenarios. Unlike existing code
evaluation benchmarks that focus on single-function completion or short-context
tasks, LoCoBench addresses the critical evaluation gap for long-context
capabilities that require understanding entire codebases, reasoning across
multiple files, and maintaining architectural consistency across large-scale
software systems. Our benchmark provides 8,000 evaluation scenarios
systematically generated across 10 programming languages, with context lengths
spanning 10K to 1M tokens, a 100x variation that enables precise assessment of
long-context performance degradation in realistic software development
settings. LoCoBench introduces 8 task categories that capture essential
long-context capabilities: architectural understanding, cross-file refactoring,
multi-session development, bug investigation, feature implementation, code
comprehension, integration testing, and security analysis. Through a 5-phase
pipeline, we create diverse, high-quality scenarios that challenge LLMs to
reason about complex codebases at unprecedented scale. We introduce a
comprehensive evaluation framework with 17 metrics across 4 dimensions,
including 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our
evaluation of state-of-the-art long-context models reveals substantial
performance gaps, demonstrating that long-context understanding in complex
software development represents a significant unsolved challenge that demands
more attention. LoCoBench is released at:
https://github.com/SalesforceAIResearch/LoCoBench.

</details>


### [13] [I Know Who Clones Your Code: Interpretable Smart Contract Similarity Detection](https://arxiv.org/abs/2509.09630)
*Zhenguang Liu,Lixun Ma,Zhongzheng Mu,Chengkun Wei,Xiaojun Xu,Yingying Jiao,Kui Ren*

Main category: cs.SE

TL;DR: SmartDetector is a novel, interpretable smart contract similarity detection tool that breaks down code for fine-grained comparison using AST decomposition and classifier optimization, outperforming prior methods with a 95.88% F1-score.


<details>
  <summary>Details</summary>
Motivation: The reuse of open-source code in smart contracts increases programming efficiency but also spreads bugs widely. Existing methods for detecting code similarity in smart contracts are limited, facing challenges with detailed semantic comparison and interpretability.

Method: The paper introduces SmartDetector, which breaks down the abstract syntax tree (AST) of smart contract functions into smaller statement trees for fine-grained semantic comparison. A classifier compares corresponding statement trees between functions to compute similarity. Hyperparameter optimization is addressed using a mathematically derived cosine-wise diffusion process.

Result: SmartDetector was evaluated on three large real-world datasets and was shown to outperform existing state-of-the-art methods, with an average improvement of 14.01% in F1-score. The overall average F1-score achieved was 95.88%.

Conclusion: SmartDetector provides an explainable and effective method for detecting similarity between smart contract functions at a fine-grained level, overcoming limitations of previous approaches and enhancing bug detection capabilities in smart contract code.

Abstract: Widespread reuse of open-source code in smart contract development boosts
programming efficiency but significantly amplifies bug propagation across
contracts, while dedicated methods for detecting similar smart contract
functions remain very limited. Conventional abstract-syntax-tree (AST) based
methods for smart contract similarity detection face challenges in handling
intricate tree structures, which impedes detailed semantic comparison of code.
Recent deep-learning based approaches tend to overlook code syntax and
detection interpretability, resulting in suboptimal performance.
  To fill this research gap, we introduce SmartDetector, a novel approach for
computing similarity between smart contract functions, explainable at the
fine-grained statement level. Technically, SmartDetector decomposes the AST of
a smart contract function into a series of smaller statement trees, each
reflecting a structural element of the source code. Then, SmartDetector uses a
classifier to compute the similarity score of two functions by comparing each
pair of their statement trees. To address the infinite hyperparameter space of
the classifier, we mathematically derive a cosine-wise diffusion process to
efficiently search optimal hyperparameters. Extensive experiments conducted on
three large real-world datasets demonstrate that SmartDetector outperforms
current state-of-the-art methods by an average improvement of 14.01% in
F1-score, achieving an overall average F1-score of 95.88%.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [14] [Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs](https://arxiv.org/abs/2509.09019)
*Mohit Tekriwal,John Sarracino*

Main category: cs.PL

TL;DR: The paper presents a formal proof approach for verifying floating-point compiler optimizations, focusing on FMA in LLVM using Rocq. Initial results validate the method's feasibility and outline directions for analyzing more complex optimizations, improving both safety and performance in scientific computing.


<details>
  <summary>Details</summary>
Motivation: Scientific computing demands aggressive compiler optimizations for better performance and resource usage, but requires these optimizations to be correct, especially for floating-point computations, which are highly sensitive to errors.

Method: The authors leverage the Verified LLVM framework in the Rocq theorem prover to formally prove the correctness of Fused-Multiply-Add (FMA) optimization at the LLVM IR level for the arithmetic expression a * b + c.

Result: They successfully demonstrate a preliminary correctness proof for FMA optimization using Rocq and suggest future extensions to cover more program features and further floating-point optimizations.

Conclusion: Formal verification of floating-point optimizations, such as FMA within LLVM, is feasible using theorem provers. The work lays the groundwork for extending such proofs to broader optimizations, enhancing both correctness and performance assurance.

Abstract: Scientific computing programs often undergo aggressive compiler optimization
to achieve high performance and efficient resource utilization. While
performance is critical, we also need to ensure that these optimizations are
correct. In this paper, we focus on a specific class of optimizations,
floating-point optimizations, notably due to fast math, at the LLVM IR level.
We present a preliminary work, which leverages the Verified LLVM framework in
the Rocq theorem prover, to prove the correctness of Fused-Multiply-Add (FMA)
optimization for a basic block implementing the arithmetic expression $a * b +
c$ . We then propose ways to extend this preliminary results by adding more
program features and fast math floating-point optimizations.

</details>


### [15] [Dependent-Type-Preserving Memory Allocation](https://arxiv.org/abs/2509.09059)
*Paulette Koronkevich,William J. Bowman*

Main category: cs.PL

TL;DR: This work creates a typed intermediate language and compiler techniques to ensure that the safety guarantees from dependently typed languages (like Coq or Agda) are preserved throughout compilation and linking, preventing violations such as unsafe memory access when integrating with external code.


<details>
  <summary>Details</summary>
Motivation: Dependently typed languages allow programmers to specify and verify detailed program properties, but these guarantees can be violated after compilation, particularly when linking with external programs which are not verified. The motivation is to prevent these specification violations even after compilation and linking.

Method: The paper introduces a typed intermediate language that supports dependent memory allocation, and describes a dependent-type-preserving compiler pass for memory allocation. These methods aim to preserve type information and enforce type safety throughout the compilation and linking process.

Result: The ongoing work has led to the development of a typed intermediate language and a compiler pass that preserves dependent types for memory allocation, facilitating type checking during linking to prevent errors such as linking with uninitialized memory.

Conclusion: Type-preserving compilation and careful design of intermediate languages can extend the safety guarantees of dependently typed programming languages beyond type checking and compilation stages, helping ensure program properties remain intact even after linking with external code.

Abstract: Dependently typed programming languages such as Coq, Agda, Idris, and F*,
allow programmers to write detailed specifications of their programs and prove
their programs meet these specifications. However, these specifications can be
violated during compilation since they are erased after type checking. External
programs linked with the compiled program can violate the specifications of the
original program and change the behavior of the compiled program -- even when
compiled with a verified compiler. For example, since Coq does not allow
explicitly allocating memory, a programmer might link their Coq program with a
C program that can allocate memory. Even if the Coq program is compiled with a
verified compiler, the external C program can still violate the memory-safe
specification of the Coq program by providing an uninitialized pointer to
memory. This error could be ruled out by type checking in a language expressive
enough to indicate whether memory is initialized versus uninitialized. Linking
with a program with an uninitialized pointer could be considered ill-typed, and
our linking process could prevent linking with ill-typed programs. To
facilitate type checking during linking, we can use type-preserving
compilation, which preserves the types through the compilation process. In this
ongoing work, we develop a typed intermediate language that supports dependent
memory allocation, as well as a dependent-type-preserving compiler pass for
memory allocation.

</details>
