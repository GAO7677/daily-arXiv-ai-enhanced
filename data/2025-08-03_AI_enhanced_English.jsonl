{"id": "2507.23087", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23087", "abs": "https://arxiv.org/abs/2507.23087", "authors": ["Fabian Stiehle", "Hans Weytjens", "Ingo Weber"], "title": "On LLM-Assisted Generation of Smart Contracts from Business Processes", "comment": "Accepted at the Workshop on Distributed Ledger Technologies in\n  Business Process Management, At the International Conference for Business\n  Process Management (BPM), 2025", "summary": "Large language models (LLMs) have changed the reality of how software is\nproduced. Within the wider software engineering community, among many other\npurposes, they are explored for code generation use cases from different types\nof input. In this work, we present an exploratory study to investigate the use\nof LLMs for generating smart contract code from business process descriptions,\nan idea that has emerged in recent literature to overcome the limitations of\ntraditional rule-based code generation approaches. However, current LLM-based\nwork evaluates generated code on small samples, relying on manual inspection,\nor testing whether code compiles but ignoring correct execution. With this\nwork, we introduce an automated evaluation framework and provide empirical data\nfrom larger data sets of process models. We test LLMs of different types and\nsizes in their capabilities of achieving important properties of process\nexecution, including enforcing process flow, resource allocation, and\ndata-based conditions. Our results show that LLM performance falls short of the\nperfect reliability required for smart contract development. We suggest future\nwork to explore responsible LLM integrations in existing tools for code\ngeneration to ensure more reliable output. Our benchmarking framework can serve\nas a foundation for developing and evaluating such integrations.", "AI": {"tldr": "The paper finds that while LLMs are promising for generating smart contract code from business process descriptions, they are not yet reliable enough for practical use. The study introduces an automated framework for more thorough evaluation and suggests that future efforts should focus on safer integration of LLMs in code generation tools.", "motivation": "Current rule-based code generation approaches have limitations in generating smart contract code from business process descriptions. The community is interested in using large language models (LLMs) for this task, but current evaluations are insufficient, as they mostly rely on small samples and surface-level checks.", "method": "The paper introduces an automated evaluation framework to empirically assess LLM-generated smart contract code. The authors tested LLMs of different types and sizes on large data sets of process models, evaluating the extent to which the codes fulfill essential execution properties such as process flow, resource allocation, and data-based conditions.", "result": "LLMs do not yet provide the perfect reliability needed for smart contract development. The performance of LLMs in generating reliable smart contract code falls short when tested on comprehensive properties and larger datasets.", "conclusion": "LLMs are not yet fully reliable for smart contract code generation from business process descriptions. The proposed automated evaluation framework offers a basis for further research and development, calling for responsible integration of LLMs into code generation tools to improve dependability."}}
{"id": "2507.23118", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23118", "abs": "https://arxiv.org/abs/2507.23118", "authors": ["Mattia Di Profio", "Mingjun Zhong", "Yaji Sripada", "Marcel Jaspars"], "title": "FlowETL: An Autonomous Example-Driven Pipeline for Data Engineering", "comment": null, "summary": "The Extract, Transform, Load (ETL) workflow is fundamental for populating and\nmaintaining data warehouses and other data stores accessed by analysts for\ndownstream tasks. A major shortcoming of modern ETL solutions is the extensive\nneed for a human-in-the-loop, required to design and implement\ncontext-specific, and often non-generalisable transformations. While related\nwork in the field of ETL automation shows promising progress, there is a lack\nof solutions capable of automatically designing and applying these\ntransformations. We present FlowETL, a novel example-based autonomous ETL\npipeline architecture designed to automatically standardise and prepare input\ndatasets according to a concise, user-defined target dataset. FlowETL is an\necosystem of components which interact together to achieve the desired outcome.\nA Planning Engine uses a paired input-output datasets sample to construct a\ntransformation plan, which is then applied by an ETL worker to the source\ndataset. Monitoring and logging provide observability throughout the entire\npipeline. The results show promising generalisation capabilities across 14\ndatasets of various domains, file structures, and file sizes.", "AI": {"tldr": "FlowETL is a novel autonomous ETL system that uses example data pairs to automatically generate and apply transformations, reducing manual work and generalising well across different datasets.", "motivation": "Current ETL solutions require significant manual intervention for context-specific and non-generalisable data transformations. Existing automation approaches have not addressed the full automation of transformation design and application.", "method": "FlowETL is introduced as an example-based, autonomous ETL pipeline. It takes a paired input-output dataset as examples. A Planning Engine constructs a transformation plan, which is executed by an ETL worker. The architecture provides monitoring and logging for transparency.", "result": "FlowETL demonstrates promising generalisation abilities across 14 datasets with diverse domains, structures, and sizes.", "conclusion": "FlowETL significantly reduces the human-in-the-loop requirement by automating the design and application of ETL transformations through an example-based approach, showing effectiveness on various datasets."}}
{"id": "2507.23120", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23120", "abs": "https://arxiv.org/abs/2507.23120", "authors": ["Jordi Cabot"], "title": "Vibe Modeling: Challenges and Opportunities", "comment": null, "summary": "There is a pressing need for better development methods and tools to keep up\nwith the growing demand and increasing complexity of new software systems. New\ntypes of user interfaces, the need for intelligent components, sustainability\nconcerns, ... bring new challenges that we need to handle. In the last years,\nmodel-driven engineering (MDE) has been key to improving the quality and\nproductivity of software development, but models themselves are becoming\nincreasingly complex to specify and manage. At the same time, we are witnessing\nthe growing popularity of vibe coding approaches that rely on Large Language\nModels (LLMs) to transform natural language descriptions into running code at\nthe expenses of code vulnerabilities, scalability issues and maintainability\nconcerns. In this paper, we introduce the concept of \\textit{vibe modeling} as\na novel approach to integrate the best of both worlds (AI and MDE) to speed up\nthe development of reliable complex systems. We outline the key concepts of\nvibe modeling and highlight the opportunities and open challenges it presents\nfor the future of modeling.", "AI": {"tldr": "This paper proposes 'vibe modeling' as a new hybrid method combining AI-powered code generation and model-driven engineering to tackle modern software complexity, detailing its benefits and highlighting research directions.", "motivation": "There is a growing demand for software systems that are increasingly complex, featuring new types of user interfaces, intelligent components, and sustainability requirements, which current development methods struggle to adequately address.", "method": "The paper introduces 'vibe modeling,' which aims to integrate Large Language Model (LLM) powered 'vibe coding' with established Model-Driven Engineering (MDE) approaches, combining the strengths of both for software development.", "result": "The paper presents the key concepts of vibe modeling, discusses its potential benefits, and outlines open challenges and opportunities for further research in this hybrid approach.", "conclusion": "Vibe modeling offers a promising path to address complexity, quality, and productivity challenges in software development by marrying AI-driven coding with MDE principles, though it presents new research challenges."}}
{"id": "2507.23168", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23168", "abs": "https://arxiv.org/abs/2507.23168", "authors": ["Elmira Onagh", "Maleknaz Nayebi"], "title": "Extension Decisions in Open Source Software Ecosystem", "comment": "Paper published in JSS journal", "summary": "GitHub Marketplace is expanding by approximately 41% annually, with new\ntools; however, many additions replicate existing functionality. We study this\nphenomenon in the platform's largest segment, Continuous Integration (CI), by\nlinking 6,983 CI Actions to 3,869 providers and mining their version histories.\nOur graph model timestamps every functionality's debut, tracks its adoption,\nand clusters redundant tools. We find that approximately 65% of new CI Actions\nreplicate existing capabilities, typically within six months, and that a small\nset of first-mover Actions accounts for most subsequent forks and extensions.\nThese insights enable developers to choose the optimal moment to launch, target\nunmet functionality, and help maintainers eliminate redundant tools. We publish\nthe complete graph and dataset to encourage longitudinal research on innovation\nand competition in software ecosystems, and to provide practitioners with a\ndata-driven roadmap for identifying emerging trends and guiding product\nstrategy.", "AI": {"tldr": "A detailed analysis of nearly 7,000 CI Actions on GitHub Marketplace reveals that 65% are redundant copies of existing tools. Most new, redundant tools appear within six months of the original, clustered around a small core of popular first-movers. The data and methods aim to help developers innovate more strategically and cut down on duplicated efforts.", "motivation": "The GitHub Marketplace is rapidly expanding, but many new tools seem to duplicate existing functionalities. The paper seeks to investigate the prevalence, timing, and impact of such functional redundancy, focusing especially on the popular Continuous Integration (CI) segment.", "method": "The study analyzes 6,983 CI Actions and maps them to 3,869 providers. By mining their version histories and employing a graph model, the research timestamps when specific functionalities first appeared, tracks their adoption, and identifies clusters of redundant tools.", "result": "The study finds that about 65% of new CI Actions replicate pre-existing functionalities, often launching within six months of an original debut. Furthermore, most forks and extensions are tied to a small core of first-mover Actions.", "conclusion": "The findings can help developers strategically time their launches and focus on unmet needs, while assisting maintainers in identifying and reducing unnecessary redundancy. The team releases a comprehensive dataset and graph to facilitate future research and to guide practitioners in trend identification and strategy."}}
{"id": "2507.23151", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.23151", "abs": "https://arxiv.org/abs/2507.23151", "authors": ["Louis Rustenholz", "Pedro Lopez-Garcia", "Manuel V. Hermenegildo"], "title": "Abstractions of Sequences, Functions and Operators", "comment": "Under consideration for publication in STTT", "summary": "We present theoretical and practical results on the order theory of lattices\nof functions, focusing on Galois connections that abstract (sets of) functions\n- a topic known as higher-order abstract interpretation.\n  We are motivated by the challenge of inferring closed-form bounds on\nfunctions which are defined recursively, i.e. as the fixed point of an operator\nor, equivalently, as the solution to a functional equation. This has multiple\napplications in program analysis (e.g. cost analysis, loop acceleration,\ndeclarative language analysis) and in hybrid systems governed by differential\nequations.\n  Our main contribution is a new family of constraint-based abstract domains\nfor abstracting numerical functions, B-bound domains, which abstract a function\nf by a conjunction of bounds from a preselected set of boundary functions. They\nallow inferring highly non-linear numerical invariants, which classical\nnumerical abstract domains struggle with. We uncover a convexity property in\nthe constraint space that simplifies, and, in some cases, fully automates,\ntransfer function design.\n  We also introduce domain abstraction, a functor that lifts arbitrary mappings\nin value space to Galois connections in function space. This supports\nabstraction from symbolic to numerical functions (i.e. size abstraction), and\nenables dimensionality reduction of equations.\n  We base our constructions of transfer functions on a simple operator\nlanguage, starting with sequences, and extending to more general functions,\nincluding multivariate, piecewise, and non-discrete domains.", "AI": {"tldr": "This paper introduces new theoretical tools (B-bound domains and domain abstraction) for inferring complex bounds on recursively defined functions, improving automation and precision in program and systems analysis.", "motivation": "The motivation comes from the challenge of inferring closed-form bounds on recursively defined functions (i.e., fixed points of operators or solutions to functional equations), relevant for applications in program analysis (such as cost analysis, loop acceleration, and declarative language analysis) and hybrid systems governed by differential equations.", "method": "The authors introduce a new family of constraint-based abstract domains, called B-bound domains, to abstract numerical functions. They also present the concept of domain abstraction, a functor that lifts arbitrary mappings in value space to Galois connections in function space. Their constructions are based on a simple operator language, starting with sequences and extending to multivariate, piecewise, and non-discrete functions.", "result": "The B-bound domains enable the inference of highly non-linear numerical invariants, overcoming limitations of classical numerical abstract domains. The authors uncover a convexity property in the constraint space that simplifies or even fully automates the design of transfer functions. Their approach supports abstraction from symbolic to numerical functions, allows for dimensionality reduction, and generalizes to a variety of functional spaces.", "conclusion": "The paper advances higher-order abstract interpretation by proposing new abstract domains (B-bound domains) and domain abstraction techniques. These enable efficient and automated analysis of complex recursive functions, supporting a wide range of applications in program analysis and systems governed by differential equations."}}
{"id": "2507.23178", "categories": ["cs.SE", "cs.AI", "I.2.5"], "pdf": "https://arxiv.org/pdf/2507.23178", "abs": "https://arxiv.org/abs/2507.23178", "authors": ["Siyuan Liu", "Zhice Yang", "Huangxun Chen"], "title": "AutoBridge: Automating Smart Device Integration with Centralized Platform", "comment": "14 pages, 12 figures, under review", "summary": "Multimodal IoT systems coordinate diverse IoT devices to deliver\nhuman-centered services. The ability to incorporate new IoT devices under the\nmanagement of a centralized platform is an essential requirement. However, it\nrequires significant human expertise and effort to program the complex IoT\nintegration code that enables the platform to understand and control the device\nfunctions. Therefore, we propose AutoBridge to automate IoT integration code\ngeneration. Specifically, AutoBridge adopts a divide-and-conquer strategy: it\nfirst generates device control logic by progressively retrieving\ndevice-specific knowledge, then synthesizes platformcompliant integration code\nusing platform-specific knowledge. To ensure correctness, AutoBridge features a\nmulti-stage debugging pipeline, including an automated debugger for virtual IoT\ndevice testing and an interactive hardware-in-the-loop debugger that requires\nonly binary user feedback (yes and no) for real-device verification. We\nevaluate AutoBridge on a benchmark of 34 IoT devices across two open-source IoT\nplatforms. The results demonstrate that AutoBridge can achieves an average\nsuccess rate of 93.87% and an average function coverage of 94.87%, without any\nhuman involvement. With minimal binary yes and no feedback from users, the code\nis then revised to reach 100% function coverage. A user study with 15\nparticipants further shows that AutoBridge outperforms expert programmers by\n50% to 80% in code accuracy, even when the programmers are allowed to use\ncommercial code LLMs.", "AI": {"tldr": "AutoBridge automates the creation and debugging of IoT integration code, achieving high accuracy and coverage with minimal human input, and outperforms both manual programming and commercial code generation tools.", "motivation": "Integrating new IoT devices into centralized platforms is currently labor-intensive, demanding significant programming expertise and manual effort. This limits scalability and efficiency, highlighting a need for automated solutions.", "method": "The paper proposes AutoBridge, a system that automates IoT integration code generation using a divide-and-conquer strategy. First, it generates device control logic by retrieving device-specific knowledge, then synthesizes platform-compliant code using platform knowledge. It includes a multi-stage debugging pipeline with both automated and interactive (binary feedback) debugging for validation.", "result": "AutoBridge was evaluated on 34 IoT devices with two open-source platforms, achieving a 93.87% average success rate and 94.87% average function coverage without human input. With minimal user feedback, it reached 100% coverage. In a user study with 15 participants, AutoBridge produced code that was 50%-80% more accurate than that produced by expert programmers, even when they used commercial code LLMs.", "conclusion": "AutoBridge significantly simplifies and automates IoT integration, outperforming human experts and existing tools in both accuracy and efficiency. Its combination of automated generation and streamlined debugging greatly reduces human involvement, making large-scale, human-centered IoT system deployment much more practical."}}
{"id": "2507.23205", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23205", "abs": "https://arxiv.org/abs/2507.23205", "authors": ["Hebi Li", "Forrest Sheng Bao", "Qi Xiao", "Jin Tian"], "title": "Kernel-FFI: Transparent Foreign Function Interfaces for Interactive Notebooks", "comment": null, "summary": "Foreign Function Interfaces (FFIs) are essential for enabling\ninteroperability between programming languages, yet existing FFI solutions are\nill-suited for the dynamic, interactive workflows prevalent in modern notebook\nenvironments such as Jupyter. Current approaches require extensive manual\nconfiguration, introduce significant boilerplate, and often lack support for\nrecursive calls and object-oriented programming (OOP) constructs-features\ncritical for productive, multi-language development.\n  We present Kernel-FFI, a transparent, language-agnostic framework that\nenables seamless cross-language function calls and object manipulation within\ninteractive notebooks. Kernel-FFI employs source-level transformation to\nautomatically rewrite cross-language invocations, eliminating the need for\nmanual bindings or boilerplate. Kernel-FFI provides robust support for OOP by\nenabling foreign object referencing and automatic resource management across\nlanguage boundaries. Furthermore, to address the blocking nature of Jupyter\nkernels and support recursive and asynchronous foreign calls, we introduce a\nnovel side-channel communication mechanism. Our tool will be open-sourced and\navailable at https://codepod.io/docs/kernel-ffi", "AI": {"tldr": "Kernel-FFI automates and simplifies cross-language calls and object management in Jupyter notebooks, removing manual setup and supporting complex features like recursion and OOP. It greatly improves developer productivity and will be open-sourced.", "motivation": "Existing Foreign Function Interface (FFI) solutions are inadequate for modern, dynamic workflows in notebook environments like Jupyter. They require manual setup, too much boilerplate, and often do not support essential features such as recursion and object-oriented constructs.", "method": "The authors introduce Kernel-FFI, a framework that uses source-level transformation to automatically rewrite cross-language function calls and object usage. This removes the need for manual bindings or boilerplate code. The system also incorporates a new side-channel communication method to handle asynchronous and recursive cross-language calls, and supports object-oriented programming across language boundaries with automatic resource management.", "result": "Kernel-FFI enables transparent, language-agnostic interoperability for function calls and object manipulation in interactive notebooks. It successfully supports recursion, asynchronous calls, and OOP constructs, providing a seamless developer experience.", "conclusion": "Kernel-FFI fills a crucial gap in supporting dynamic, multi-language programming in environments such as Jupyter notebooks. It automates the integration process, reduces manual overhead, and supports greater flexibility and productivity for developers. The tool will be made open-source for broader community use."}}
{"id": "2507.23269", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.23269", "abs": "https://arxiv.org/abs/2507.23269", "authors": ["Peter Fettke", "Fabiana Fournier", "Lior Limonad", "Andreas Metzger", "Stefanie Rinderle-Ma", "Barbara Weber"], "title": "XABPs: Towards eXplainable Autonomous Business Processes", "comment": null, "summary": "Autonomous business processes (ABPs), i.e., self-executing workflows\nleveraging AI/ML, have the potential to improve operational efficiency, reduce\nerrors, lower costs, improve response times, and free human workers for more\nstrategic and creative work. However, ABPs may raise specific concerns\nincluding decreased stakeholder trust, difficulties in debugging, hindered\naccountability, risk of bias, and issues with regulatory compliance. We argue\nfor eXplainable ABPs (XABPs) to address these concerns by enabling systems to\narticulate their rationale. The paper outlines a systematic approach to XABPs,\ncharacterizing their forms, structuring explainability, and identifying key BPM\nresearch challenges towards XABPs.", "AI": {"tldr": "ABPs can greatly benefit organizations but have significant challenges regarding trust and accountability. This paper introduces the concept of explainable ABPs (XABPs), provides a structure for them, and identifies important research directions to make them more transparent and trustworthy.", "motivation": "Autonomous business processes (ABPs), powered by AI/ML, offer significant operational benefits but introduce concerns around trust, debugging, accountability, bias, and regulatory compliance.", "method": "The paper proposes a systematic approach to eXplainable Autonomous Business Processes (XABPs), outlining ways to structure explainability and formulating major business process management (BPM) research challenges in this area.", "result": "A framework for XABPs is characterized, including forms and structure for explainability, with key BPM research challenges identified to further enable explainable ABPs.", "conclusion": "Explainability is essential for trustworthy and effective deployment of ABPs. The proposed approach provides groundwork for research on making ABPs more transparent and reliable."}}
{"id": "2507.23348", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23348", "abs": "https://arxiv.org/abs/2507.23348", "authors": ["Han Li", "Yuling Shi", "Shaoxin Lin", "Xiaodong Gu", "Heng Lian", "Xin Wang", "Yantao Jia", "Tao Huang", "Qianxiang Wang"], "title": "SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution", "comment": "Our code and data are available at\n  https://github.com/YerbaPage/SWE-Debate", "summary": "Issue resolution has made remarkable progress thanks to the advanced\nreasoning capabilities of large language models (LLMs). Recently, agent-based\nframeworks such as SWE-agent have further advanced this progress by enabling\nautonomous, tool-using agents to tackle complex software engineering tasks.\nWhile existing agent-based issue resolution approaches are primarily based on\nagents' independent explorations, they often get stuck in local solutions and\nfail to identify issue patterns that span across different parts of the\ncodebase. To address this limitation, we propose SWE-Debate, a competitive\nmulti-agent debate framework that encourages diverse reasoning paths and\nachieves more consolidated issue localization. SWE-Debate first creates\nmultiple fault propagation traces as localization proposals by traversing a\ncode dependency graph. Then, it organizes a three-round debate among\nspecialized agents, each embodying distinct reasoning perspectives along the\nfault propagation trace. This structured competition enables agents to\ncollaboratively converge on a consolidated fix plan. Finally, this consolidated\nfix plan is integrated into an MCTS-based code modification agent for patch\ngeneration. Experiments on the SWE-bench benchmark show that SWE-Debate\nachieves new state-of-the-art results in open-source agent frameworks and\noutperforms baselines by a large margin.", "AI": {"tldr": "SWE-Debate is a novel multi-agent debate framework for software issue resolution that enables agents to collaborate and reason diversely, leading to superior results over previous methods.", "motivation": "Existing agent-based frameworks for issue resolution, despite leveraging advanced large language models, are limited by independent agent explorations that lead to local solutions and overlook issue patterns across the codebase.", "method": "The authors introduce SWE-Debate, a competitive multi-agent debate framework. It generates multiple fault propagation traces via code dependency traversal, organizes a structured three-round debate among specialized agents with diverse reasoning perspectives, consolidates their findings into a fix plan, and uses an MCTS-based agent for code patch generation.", "result": "SWE-Debate achieves state-of-the-art results on the SWE-bench benchmark and significantly outperforms existing open-source agent-based baselines for software issue resolution.", "conclusion": "Structured debate among agents with different reasoning approaches enables more accurate issue localization and better software repair outcomes compared to independent agent strategies."}}
{"id": "2507.23356", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23356", "abs": "https://arxiv.org/abs/2507.23356", "authors": ["Shmulik Froimovich", "Raviv Gal", "Wesam Ibraheem", "Avi Ziv"], "title": "Quality Evaluation of COBOL to Java Code Transformation", "comment": "Submitted to ASE 2025", "summary": "We present an automated evaluation system for assessing COBOL-to-Java code\ntranslation within IBM's watsonx Code Assistant for Z (WCA4Z). The system\naddresses key challenges in evaluating LLM-based translators, including model\nopacity and the complexity of translation quality assessment. Our approach\ncombines analytic checkers with LLM-as-a-judge (LaaJ) techniques to deliver\nscalable, multi-faceted evaluations. The system supports continuous integration\nworkflows, enables large-scale benchmarking, and reduces reliance on manual\nreview. We describe the system architecture, evaluation strategies, and\nreporting mechanisms that provide actionable insights for developers and\nproject managers, facilitating the evolution of high-quality, modernized\ncodebases.", "AI": {"tldr": "The paper introduces an automated, hybrid evaluation system for LLM-based COBOL-to-Java code translation that combines analytic and LLM judgment techniques, improves scalability and automation, and supports high-quality legacy code modernization.", "motivation": "The paper addresses the difficulty of evaluating code translated by large language model (LLM)-based systems, especially the lack of transparency and the challenge of accurately measuring translation quality in COBOL-to-Java migration. There is a need to automate this process to support large scale modernizations and reduce manual effort.", "method": "The authors designed an automated evaluation system that leverages both analytic checkers and LLM-as-a-judge (LaaJ) methods. This hybrid approach allows the system to provide comprehensive, scalable, and multifaceted assessments of translation quality.", "result": "The evaluation system integrates seamlessly with continuous integration workflows, supports large-scale benchmarking, and minimizes dependence on manual code review. It offers architecture, strategies, and reporting that help developers and managers improve the quality of migrated codebases.", "conclusion": "The presented system enhances the ability to assess LLM-based COBOL-to-Java code translations, making the evaluation process more scalable, actionable, and less reliant on manual intervention. This enables more effective modernization of legacy systems."}}
{"id": "2507.23361", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23361", "abs": "https://arxiv.org/abs/2507.23361", "authors": ["Silin Chen", "Shaoxin Lin", "Xiaodong Gu", "Yuling Shi", "Heng Lian", "Longfei Yun", "Dong Chen", "Weiguo Sun", "Lin Cao", "Qianxiang Wang"], "title": "SWE-Exp: Experience-Driven Software Issue Resolution", "comment": "Our code and data are available at\n  https://github.com/YerbaPage/SWE-Exp", "summary": "Recent advances in large language model (LLM) agents have shown remarkable\nprogress in software issue resolution, leveraging advanced techniques such as\nmulti-agent collaboration and Monte Carlo Tree Search (MCTS). However, current\nagents act as memoryless explorers - treating each problem separately without\nretaining or reusing knowledge from previous repair experiences. This leads to\nredundant exploration of failed trajectories and missed chances to adapt\nsuccessful issue resolution methods to similar problems. To address this\nproblem, we introduce SWE-Exp, an experience - enhanced approach that distills\nconcise and actionable experience from prior agent trajectories, enabling\ncontinuous learning across issues. Our method introduces a multi-faceted\nexperience bank that captures both successful and failed repair attempts.\nSpecifically, it extracts reusable issue resolution knowledge at different\nlevels - from high-level problem comprehension to specific code changes.\nExperiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%\nPass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach\nestablishes a new paradigm in which automated software engineering agents\nsystematically accumulate and leverage repair expertise, fundamentally shifting\nfrom trial-and-error exploration to strategic, experience-driven issue\nresolution.", "AI": {"tldr": "The paper proposes SWE-Exp, an approach that enables LLM agents to remember and reuse past repair experiences for software issue resolution, leading to higher success rates by avoiding redundant mistakes and applying learned solutions.", "motivation": "Current LLM agents for software issue resolution don't reuse knowledge from previous experiences. As a result, they redundantly repeat failed exploration and miss the chance to apply successful strategies to similar problems.", "method": "The authors introduce SWE-Exp, an experience-enhanced approach that builds an experience bank from prior agent trajectories. This bank captures both successful and failed repair attempts at multiple levels, from problem understanding to specific code changes, enabling continuous learning and knowledge reuse.", "result": "SWE-Exp achieves a state-of-the-art resolution rate (41.6% Pass@1) on the SWE-bench-Verified benchmark when used with open-source agent frameworks.", "conclusion": "SWE-Exp allows software engineering agents to systematically accumulate and leverage repair expertise, moving from a pure trial-and-error approach to a strategic, experience-driven resolution process."}}
{"id": "2507.23370", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23370", "abs": "https://arxiv.org/abs/2507.23370", "authors": ["Trae Research Team", "Pengfei Gao", "Zhao Tian", "Xiangxin Meng", "Xinchen Wang", "Ruida Hu", "Yuanan Xiao", "Yizhou Liu", "Zhao Zhang", "Junjie Chen", "Cuiyun Gao", "Yun Lin", "Yingfei Xiong", "Chao Peng", "Xia Liu"], "title": "Trae Agent: An LLM-based Agent for Software Engineering with Test-time Scaling", "comment": "Pengfei Gao and Zhao Tian contributed equally to this technical\n  report", "summary": "Software issue resolution is a critical challenge in software engineering and\nhas garnered increasing attention in recent years. With the rapid advancement\nof large language models (LLMs), substantial progress has been made in\naddressing real-world software engineering tasks. Recent studies have\nintroduced ensemble reasoning techniques to enhance the performance of\nLLM-based issue resolution. However, existing prompting-based methods still\nface limitations in effectively exploring large ensemble spaces and lack the\ncapacity for repository-level understanding, both of which constrain their\noverall effectiveness. In this paper, we propose Trae Agent, the first\nagent-based ensemble reasoning approach for repository-level issue resolution.\nTrae Agent formulates our goal as an optimal solution search problem and\naddresses two key challenges, i.e., large ensemble spaces and repository-level\nunderstanding, through modular agents for generation, pruning, and selection.\nWe conduct extensive experiments using three leading LLMs on the widely-adopted\nSWE-bench benchmark, comparing Trae Agent against four state-of-the-art\nensemble reasoning techniques. Experimental results demonstrate that Trae Agent\nconsistently achieves superior performance, with an average improvement of\n10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first\nplace on the SWE-bench Verified leaderboard, with a notable Pass@1 score of\n75.20%. We are pleased to release Trae Agent as an open-source project to\nsupport the research community, with all resources available at\nhttps://github.com/bytedance/trae-agent.", "AI": {"tldr": "Trae Agent is a novel agent-based ensemble approach for software issue resolution, leveraging modular agents to improve repository-level understanding and solution quality. It significantly outperforms previous methods on the SWE-bench benchmark, is open source, and sets a new state-of-the-art in the field.", "motivation": "With the advancement of large language models (LLMs), software issue resolution has seen improved automated solutions, yet current prompting-based ensemble methods struggle with large solution spaces and lack deep, repository-level understanding. This constrains their effectiveness on real-world software repositories.", "method": "The authors introduce Trae Agent, an agent-based ensemble reasoning framework. Trae Agent models issue resolution as a search for optimal fixes using specialized modular agents for generation, pruning, and selection. This modularity enables better handling of extensive ensemble spaces and repository-wide understanding. Experiments were run on the SWE-bench benchmark using three leading LLMs, with comparisons to four ensemble reasoning baselines.", "result": "Trae Agent surpassed all four state-of-the-art ensemble reasoning baselines, with an average Pass@1 improvement of 10.22%. It achieved a leading score of 75.20% Pass@1 on the SWE-bench Verified leaderboard, demonstrating robust effectiveness.", "conclusion": "The introduction of Trae Agent represents a major advancement in agent-based ensemble reasoning for software issue resolution, particularly at the repository level. Its modular design overcomes key limitations of prior prompting-based methods, delivering state-of-the-art results. The open-source release supports further research in this domain."}}
{"id": "2507.23425", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23425", "abs": "https://arxiv.org/abs/2507.23425", "authors": ["Daphn\u00e9 Larrivain", "Shinhyung Yang", "Wilhelm Hasselbring"], "title": "Dynamic and Static Analysis of Python Software with Kieker Including Reconstructed Architectures", "comment": "9 pages, 9 figures", "summary": "The Kieker observability framework is a tool that provides users with the\nmeans to design a custom observability pipeline for their application.\nOriginally tailored for Java, supporting Python with Kieker is worthwhile.\nPython's popularity has exploded over the years, thus making structural\ninsights of Python applications highly valuable. Our Python analysis pipeline\ncombines static and dynamic analysis in order to build a complete picture of a\ngiven system.", "AI": {"tldr": "This paper extends the Kieker observability framework to support Python by developing a new analysis pipeline that uses both static and dynamic analyses, thus providing deeper insights into Python applications.", "motivation": "Kieker was originally designed for Java applications. However, due to the increasing popularity of Python, there is growing value in providing structural insight and observability tools for Python applications.", "method": "The paper presents a Python analysis pipeline that integrates both static and dynamic analysis techniques to achieve comprehensive application assessment.", "result": "The developed pipeline enables users to gain a thorough understanding of Python systems, enhancing observability and analysis capabilities in the Kieker framework for Python applications.", "conclusion": "Supporting Python in the Kieker observability framework is beneficial, and the combination of static and dynamic analysis offers robust insights into Python systems for users."}}
{"id": "2507.23640", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23640", "abs": "https://arxiv.org/abs/2507.23640", "authors": ["Samah Kansab", "Mohammed Sayagh", "Francis Bordeleau", "Ali Tizghadam"], "title": "An Empirical Study on the Amount of Changes Required for Merge Request Acceptance", "comment": null, "summary": "Code review (CR) is essential to software development, helping ensure that\nnew code is properly integrated. However, the CR process often involves\nsignificant effort, including code adjustments, responses to reviewers, and\ncontinued implementation. While past studies have examined CR delays and\niteration counts, few have investigated the effort based on the volume of code\nchanges required, especially in the context of GitLab Merge Requests (MRs),\nwhich remains underexplored. In this paper, we define and measure CR effort as\nthe amount of code modified after submission, using a dataset of over 23,600\nMRs from four GitLab projects. We find that up to 71% of MRs require\nadjustments after submission, and 28% of these involve changes to more than 200\nlines of code. Surprisingly, this effort is not correlated with review time or\nthe number of participants. To better understand and predict CR effort, we\ntrain an interpretable machine learning model using metrics across multiple\ndimensions: text features, code complexity, developer experience, review\nhistory, and branching. Our model achieves strong performance (AUC 0.84-0.88)\nand reveals that complexity, experience, and text features are key predictors.\nHistorical project characteristics also influence current review effort. Our\nfindings highlight the feasibility of using machine learning to explain and\nanticipate the effort needed to integrate code changes during review.", "AI": {"tldr": "This paper analyzes the effort required in GitLab Merge Requests by measuring post-submission code changes and finds that most MRs demand further modification, often at a significant scale. Machine learning models, using project and developer metrics, predict review effort with high accuracy, highlighting potential for process improvement.", "motivation": "Code review (CR) is a critical but effort-intensive phase in software development, especially in collaborative environments like GitLab Merge Requests. While previous research has focused on delays and the number of review iterations, little is known about the actual code modification effort required post-submission, a gap this study seeks to address.", "method": "The study quantitatively defines and measures CR effort as the amount of code changed after MR submission. It analyzes a dataset of over 23,600 GitLab MRs, assessing the extent of post-submission code changes. Additionally, it applies interpretable machine learning, utilizing features such as text, code complexity, developer experience, review history, and branching, to predict CR effort.", "result": "The study found that up to 71% of MRs require changes after initial submission, with 28% of those requiring over 200 lines of modifications. Notably, the effort expended isn't correlated with review duration or the number of reviewers. The machine learning model used in the study achieved an AUC between 0.84 and 0.88, identifying complexity, developer experience, and text features as key predictors. Past project characteristics also impact current review effort.", "conclusion": "Machine learning models can effectively explain and predict code review effort in GitLab MRs. Key drivers include code complexity, developer experience, and characteristics reflected in text and project history. This opens opportunities for more data-driven management of code review integration."}}
