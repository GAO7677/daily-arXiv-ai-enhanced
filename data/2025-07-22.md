<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 36]
- [cs.PL](#cs.PL) [Total: 8]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Impact of Code Context and Prompting Strategies on Automated Unit Test Generation with Modern General-Purpose Large Language Models](https://arxiv.org/abs/2507.14256)
*Jakub Walczak,Piotr Tomalak,Artur Laskowski*

Main category: cs.SE

TL;DR: Including docstrings and using the chain-of-thought prompting strategy significantly enhances the quality of LLM-generated unit tests, with Gemini 2.5 Pro achieving superior results in branch coverage and mutation score.


<details>
  <summary>Details</summary>
Motivation: Testing is critical for software reliability, especially unit testing, which is highly repetitive and labor-intensive. Automatically generating unit tests with LLMs could improve productivity but requires understanding how context and prompting affect test quality.

Method: The paper systematically investigates the impact of code context (e.g., including docstrings and full implementation) and different prompting strategies (such as chain-of-thought) on the quality and adequacy of unit tests generated by various LLMs from different model families. The evaluation uses metrics like branch coverage, mutation score, and compilation success rate.

Result: Including docstrings improves test adequacy, while supplying the full code yields smaller additional gains. The chain-of-thought prompting strategy delivers the best results, achieving up to 96.3% branch coverage, 57% mutation score, and near-perfect compilation rate. Among all tested models, Gemini 2.5 Pro (M5) outperforms others in mutation score and branch coverage, and remains at the top in compilation rate.

Conclusion: Prompting strategy, especially chain-of-thought, and code context (notably docstrings) significantly influence the effectiveness of LLM-generated unit tests. Gemini 2.5 Pro achieves the best overall results among evaluated models. The findings provide practical guidance for leveraging generative AI in software testing.

Abstract: Generative AI is gaining increasing attention in software engineering, where
testing remains an indispensable reliability mechanism. According to the widely
adopted testing pyramid, unit tests constitute the majority of test cases and
are often schematic, requiring minimal domain expertise. Automatically
generating such tests under the supervision of software engineers can
significantly enhance productivity during the development phase of the software
lifecycle.
  This paper investigates the impact of code context and prompting strategies
on the quality and adequacy of unit tests generated by various large language
models (LLMs) across several families. The results show that including
docstrings notably improves code adequacy, while further extending context to
the full implementation yields definitely smaller gains. Notably, the
chain-of-thought prompting strategy -- applied even to 'reasoning' models --
achieves the best results, with up to 96.3\% branch coverage, a 57\% average
mutation score, and near-perfect compilation success rate. Among the evaluated
models, M5 (Gemini 2.5 Pro) demonstrated superior performance in both mutation
score and branch coverage being still in top in terms of compilation success
rate.
  All the code and resulting test suites are publicly available at
https://github.com/peetery/LLM-analysis.

</details>


### [2] [Leveraging LLMs for Formal Software Requirements -- Challenges and Prospects](https://arxiv.org/abs/2507.14330)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: Formal verification relies on translating ambiguous natural language requirements into precise specifications, a challenging task. This paper reviews current literature and discusses automated approaches—NLP, ontologies, LLMs—to improve this process, outlining key challenges and future research directions.


<details>
  <summary>Details</summary>
Motivation: There is a significant challenge in formal verification due to the difficulty of generating formal, unambiguous requirement specifications from natural language, a key bottleneck especially for safety-critical software systems.

Method: The paper synthesizes existing literature and examines automated and semi-automated techniques—including NLP, ontology-based modeling, artefact reuse, and LLMs—for converting informal requirements into formal specifications.

Result: The paper identifies recurring challenges and outlines prospective research directions for generating verifiable specifications from informal requirements, but does not present experimental or evaluative results.

Conclusion: There is a need for further research on leveraging automated tools and advanced language models to bridge the gap between informal natural language requirements and formal verifiable specifications.

Abstract: Software correctness is ensured mathematically through formal verification,
which involves the resources of generating formal requirement specifications
and having an implementation that must be verified. Tools such as
model-checkers and theorem provers ensure software correctness by verifying the
implementation against the specification. Formal methods deployment is
regularly enforced in the development of safety-critical systems e.g.
aerospace, medical devices and autonomous systems. Generating these
specifications from informal and ambiguous natural language requirements
remains the key challenge. Our project, VERIFAI^{1}, aims to investigate
automated and semi-automated approaches to bridge this gap, using techniques
from Natural Language Processing (NLP), ontology-based domain modelling,
artefact reuse, and large language models (LLMs). This position paper presents
a preliminary synthesis of relevant literature to identify recurring challenges
and prospective research directions in the generation of verifiable
specifications from informal requirements.

</details>


### [3] [Developing Shared Vocabulary System For Collaborative Software Engineering](https://arxiv.org/abs/2507.14396)
*Carey Lai Zheng Hui,Johnson Britto Jessia Esther Leena,Kumuthini Subramanian,Zhao Chenyu,Shubham Rajeshkumar Jariwala*

Main category: cs.SE

TL;DR: Creating and adopting shared vocabularies in software engineering teams, while initially effortful, results in clearer documentation and better collaboration. This strengthens communication and helps reduce misunderstandings and defects.


<details>
  <summary>Details</summary>
Motivation: Communication gaps in software engineering persistently cause misunderstandings, inefficiencies, and defects. This paper is motivated by the need to understand the technical reasons behind these communication breakdowns and to explore solutions for enhancing mutual understanding among collaborators.

Method: The study used a Design Science Research (DSR) framework consisting of three iterative phases: (1) identifying problems through thematic analysis of communication data and semi-structured interviews, (2) developing a methodology for creating shared vocabularies using Grounded Theory principles, and (3) empirically validating the approach through controlled experiments.

Result: Empirical validation found that, although adoption introduced some initial overhead, implementing shared vocabulary systems led to significant improvements in information density, documentation clarity, and collaboration efficiency within software engineering teams.

Conclusion: Establishing structured, shared vocabulary systems can greatly enhance communication and collaboration in software development, overcoming many issues caused by ambiguous or inconsistent language. However, initial implementation may involve a learning curve.

Abstract: Effective communication is a critical factor in successful software
engineering collaboration. However, communication gaps remain a persistent
challenge, often leading to misunderstandings, inefficiencies, and defects.
This research investigates the technical factors contributing to such
misunderstandings and explores the measurable benefits of establishing shared
vocabulary systems within software documentation and codebases. Using a Design
Science Research (DSR) framework, the study was structured into three iterative
phases: problem identification, method development, and empirical validation.
The problem identification phase involved thematic analysis of communication
data and semi-structured interviews, revealing key factors such as ambiguous
messaging, misalignment in documentation, inconsistent code review feedback,
and API integration miscommunication. Grounded Theory principles were employed
to design a structured methodology for collaborative vocabulary development.
Empirical validation through controlled experiments demonstrated that while
initial adoption introduced overhead, the shared vocabulary system
significantly improved information density, documentation clarity, and
collaboration efficiency over time. Findings offer actionable insights for
improving communication practices in software engineering, while also
identifying limitations and directions for future research.

</details>


### [4] [On the Effect of Token Merging on Pre-trained Models for Code](https://arxiv.org/abs/2507.14423)
*Mootez Saad,Hao Li,Tushar Sharma,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: The paper investigates ways to merge subtoken representations in code models, showing that this can save computation and sometimes improve results, especially for code translation, with only minor losses in other tasks like vulnerability detection.


<details>
  <summary>Details</summary>
Motivation: Tokenization in code language models produces longer sequences than traditional compiler token streams, which leads to higher computational costs and can impact model efficiency.

Method: The paper proposes and evaluates two strategies for merging the hidden representations of subtokens forming the same semantic unit: one using average pooling and another using a learning-based method. These strategies are tested with six code-specific language models across three tasks.

Result: The proposed methods reduce computational operations between 1% and 19%. For vulnerability detection, there is a modest decrease in F1 score (up to 1.82 points), while for code translation, there is an improvement of 2.47 CodeBLEU points.

Conclusion: Merging subtoken representations can improve computational efficiency with minimal or even positive impact on downstream task performance, depending on the application. This represents a step toward more efficient language models for code.

Abstract: Tokenization is a fundamental component of language models for code. It
involves breaking down the input into units that are later passed to the
language model stack to learn high-dimensional representations used in various
contexts, from classification to generation. However, the output of these
tokenizers is often longer than that traditionally used in compilers and
interpreters. This could result in undesirable effects, such as increased
computational overhead. In this work, we investigate the effect of merging the
hidden representations of subtokens that belong to the same semantic unit, such
as subtokens that form a single identifier. We propose two strategies: one
based on averaging the representations and another that leverages a
learning-based approach. Both methods can be seamlessly integrated with
existing language models for code. We conduct experiments using six language
models for code: CodeBERT, GraphCodeBERT, UniXCoder, CdoeT5, CodeT5+ (220M),
and CodeT5+ (770M), across three software engineering tasks: vulnerability
detection, code classification, and code translation. Results show that these
strategies can reduce the number of floating-point operations by $1\%$ to
$19\%$. Regarding downstream performance, the most significant degradation was
observed in the vulnerability detection task, where the F1 score decreased by
$1.82$ points compared to the baseline. In contrast, for code translation, we
observed an improvement of $2.47$ points in CodeBLEU. This work contributes to
the broader effort of improving language models for code across multiple
dimensions, including both computational efficiency and downstream performance.

</details>


### [5] [Architectural Degradation: Definition, Motivations, Measurement and Remediation Approaches](https://arxiv.org/abs/2507.14547)
*Noman Ahmad,Ruoyu Su,Matteo Esposito,Andrea Janes,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: Architectural degradation harms software sustainability, but literature is fragmented on definitions and solutions. This review synthesizes 108 studies, highlighting the evolution of the concept, detection maturity, but lack of integrated and proactive remediation. More holistic approaches are urgently needed.


<details>
  <summary>Details</summary>
Motivation: Architectural degradation threatens software quality, maintainability, and adaptability, but existing studies present fragmented definitions, metrics, and remediation approaches, making cohesive understanding and effective management difficult.

Method: The authors conducted a multivocal literature review of 108 sources from both academic and gray literature, systematically extracting and analyzing definitions, causes, metrics, measurement techniques, tools, and remediation strategies associated with architectural degradation. They also developed a taxonomy to explore the evolution of concepts and identify research gaps.

Result: The review found evolving definitions of degradation, now including code violations, design drift, and structural decay, with causes spanning architectural, code, and process debt. The study cataloged 54 metrics and 31 measurement techniques, mostly focused on detection (e.g., smells, cohesion/coupling). However, tools rarely support ongoing remediation, and integration between metrics, tools, and repair processes is lacking.

Conclusion: Architectural degradation is both a technical and organizational issue. While detection methods are mature, continuous remediation is underexplored. There is a critical need for cohesive, holistic, and proactive strategies that integrate detection metrics, tools, and remediation logic to ensure sustainable software architecture.

Abstract: Architectural degradation, also known as erosion, decay, or aging, impacts
system quality, maintainability, and adaptability. Although widely
acknowledged, current literature shows fragmented definitions, metrics, and
remediation strategies. Our study aims to unify understanding of architectural
degradation by identifying its definitions, causes, metrics, tools, and
remediation approaches across academic and gray literature. We conducted a
multivocal literature review of 108 studies extracting definitions, causes,
metrics, measurement approaches, tools, and remediation strategies. We
developed a taxonomy encompassing architectural, code, and process debt to
explore definition evolution, methodological trends, and research gaps.
Architectural degradation has shifted from a low-level issue to a
socio-technical concern. Definitions now address code violations, design drift,
and structural decay. Causes fall under architectural (e.g., poor
documentation), code (e.g., hasty fixes), and process debt (e.g., knowledge
loss). We identified 54 metrics and 31 measurement techniques, focused on
smells, cohesion/coupling, and evolution. Yet, most tools detect issues but
rarely support ongoing or preventive remediation. Degradation is both technical
and organizational. While detection is well-studied, continuous remediation
remains lacking. Our study reveals missed integration between metrics, tools,
and repair logic, urging holistic, proactive strategies for sustainable
architecture.

</details>


### [6] [Emerging Trends in Software Architecture from the Practitioners Perspective: A Five Year Review](https://arxiv.org/abs/2507.14554)
*Ruoyu Su,Noman ahmad,Matteo Esposito,Andrea Janes,Davide Taibi,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: By analyzing over 5,000 talks from major industry conferences, this paper shows that a few core technologies (Kubernetes, Serverless) dominate software architecture practices, primarily in deployment and operation. The study highlights evolving industry priorities and the need for research to address architectural qualities more holistically.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand how software architecture practices are shifting due to emerging technologies (cloud, microservices, containers) and to gain insights into what drives technology adoption in modern industry settings.

Method: The authors analyzed 5,677 talks from eight major industry conferences over five years. They used large language models and expert validation to extract information on technologies, their purposes, and usage contexts. Relationships among technologies and their placement within DevOps pipelines were also examined.

Result: The analysis identified Kubernetes, Cloud Native, Serverless, and Containers as dominant technologies. Practitioner focus is mainly on deployment, communication, AI, and observability, with five main technology communities identified. Most technologies are widely used across DevOps stages, especially in hybrid deployments, but there is less emphasis on early stages like planning and coding.

Conclusion: A small set of core technologies dominates current software architecture practice, especially in later DevOps stages. Practitioners emphasize deployment and operational concerns, highlighting technology purpose and context, while calling for broader research-driven perspectives on architectural quality and evolution.

Abstract: Software architecture plays a central role in the design, development, and
maintenance of software systems. With the rise of cloud computing,
microservices, and containers, architectural practices have diversified.
Understanding these shifts is vital. This study analyzes software architecture
trends across eight leading industry conferences over five years. We
investigate the evolution of software architecture by analyzing talks from top
practitioner conferences, focusing on the motivations and contexts driving
technology adoption. We analyzed 5,677 talks from eight major industry
conferences, using large language models and expert validation to extract
technologies, their purposes, and usage contexts. We also explored how
technologies interrelate and fit within DevOps and deployment pipelines. Among
450 technologies, Kubernetes, Cloud Native, Serverless, and Containers dominate
by frequency and centrality. Practitioners present technology mainly related to
deployment, communication, AI, and observability. We identify five technology
communities covering automation, coordination, cloud AI, monitoring, and
cloud-edge. Most technologies span multiple DevOps stages and support hybrid
deployment. Our study reveals that a few core technologies, like Kubernetes and
Serverless, dominate the contemporary software architecture practice. These are
mainly applied in later DevOps stages, with limited focus on early phases like
planning and coding. We also show how practitioners frame technologies by
purpose and context, reflecting evolving industry priorities. Finally, we
observe how only research can provide a more holistic lens on architectural
design, quality, and evolution.

</details>


### [7] [Harnessing LLMs for Document-Guided Fuzzing of OpenCV Library](https://arxiv.org/abs/2507.14558)
*Bin Duan,Tarek Mahmud,Meiru Che,Yan Yan,Naipeng Dong,Dan Dongseong Kim,Guowei Yang*

Main category: cs.SE

TL;DR: VISTAFUZZ uses LLMs to intelligently fuzz OpenCV's API based on its documentation, found 17 new bugs during tests, and proved effective for automated, document-guided software testing.


<details>
  <summary>Details</summary>
Motivation: Ensuring the reliability of the widely used OpenCV computer vision library is crucial, as bugs in its implementation can negatively impact a large range of downstream applications.

Method: The paper proposes VISTAFUZZ, a novel document-guided fuzzing technique that leverages large language models (LLMs) to parse API documentation, extract standardized information and constraints, and then generates systematic test inputs for OpenCV APIs based on these.

Result: VISTAFUZZ was evaluated on 330 OpenCV APIs. It successfully detected 17 new bugs, 10 of which were confirmed, and 5 were subsequently fixed.

Conclusion: VISTAFUZZ demonstrates the effectiveness of combining LLMs with fuzzing techniques to improve the reliability of critical computer vision libraries like OpenCV, leading to the discovery and resolution of previously unknown issues.

Abstract: The combination of computer vision and artificial intelligence is
fundamentally transforming a broad spectrum of industries by enabling machines
to interpret and act upon visual data with high levels of accuracy. As the
biggest and by far the most popular open-source computer vision library, OpenCV
library provides an extensive suite of programming functions supporting
real-time computer vision. Bugs in the OpenCV library can affect the downstream
computer vision applications, and it is critical to ensure the reliability of
the OpenCV library. This paper introduces VISTAFUZZ, a novel technique for
harnessing large language models (LLMs) for document-guided fuzzing of the
OpenCV library. VISTAFUZZ utilizes LLMs to parse API documentation and obtain
standardized API information. Based on this standardized information, VISTAFUZZ
extracts constraints on individual input parameters and dependencies between
these. Using these constraints and dependencies, VISTAFUZZ then generates new
input values to systematically test each target API. We evaluate the
effectiveness of VISTAFUZZ in testing 330 APIs in the OpenCV library, and the
results show that VISTAFUZZ detected 17 new bugs, where 10 bugs have been
confirmed, and 5 of these have been fixed.

</details>


### [8] [A first look at License Variants in the PyPI Ecosystem](https://arxiv.org/abs/2507.14594)
*Weiwei Xu,Hengzhi Ye,Kai Gao,Minghui Zhou*

Main category: cs.SE

TL;DR: This paper reveals that license variants are common and create downstream compliance issues. The authors introduce accurate, efficient tools (LV-Parser, LV-Compat) that outperform existing methods in identifying license incompatibilities in software packages.


<details>
  <summary>Details</summary>
Motivation: Open-source license variants make compliance analysis difficult due to modified and custom licenses, but their impact and prevalence are not well understood and current tools fail to handle them effectively.

Method: The authors conducted a comprehensive empirical study of license variants in the PyPI ecosystem. They proposed LV-Parser, which uses diff-based techniques and large language models for license analysis, and LV-Compat, an automated pipeline for detecting license incompatibilities within dependency networks.

Result: Textual license variations are common, but only 2% are substantial modifications. Despite this, variants create significant compliance risks—10.7% of downstream dependencies are license-incompatible. LV-Parser achieved 0.936 accuracy and reduced computational cost by 30%. LV-Compat identified 5.2 times more incompatible packages than existing tools, with 0.98 precision.

Conclusion: License variants impose real compliance issues in open-source ecosystems. The authors provide the first significant empirical study on these variants and introduce new tools that enhance both the detection and efficiency of license compatibility analysis.

Abstract: Open-source licenses establish the legal foundation for software reuse, yet
license variants, including both modified standard licenses and custom-created
alternatives, introduce significant compliance complexities. Despite their
prevalence and potential impact, these variants are poorly understood in modern
software systems, and existing tools do not account for their existence,
leading to significant challenges in both effectiveness and efficiency of
license analysis. To fill this knowledge gap, we conduct a comprehensive
empirical study of license variants in the PyPI ecosystem. Our findings show
that textual variations in licenses are common, yet only 2% involve substantive
modifications. However, these license variants lead to significant compliance
issues, with 10.7% of their downstream dependencies found to be
license-incompatible.
  Inspired by our findings, we introduce LV-Parser, a novel approach for
efficient license variant analysis leveraging diff-based techniques and large
language models, along with LV-Compat, an automated pipeline for detecting
license incompatibilities in software dependency networks. Our evaluation
demonstrates that LV-Parser achieves an accuracy of 0.936 while reducing
computational costs by 30%, and LV-Compat identifies 5.2 times more
incompatible packages than existing methods with a precision of 0.98.
  This work not only provides the first empirical study into license variants
in software packaging ecosystem but also equips developers and organizations
with practical tools for navigating the complex landscape of open-source
licensing.

</details>


### [9] [An Efficient Algorithm for Generating Minimal Unique-Cause MC/DC Test cases for Singular Boolean Expressions](https://arxiv.org/abs/2507.14687)
*Robin Lee,Youngho Nam*

Main category: cs.SE

TL;DR: The paper introduces 'Robin’s Rule,' an optimal algorithm for generating minimal test sets to achieve the strictest MC/DC coverage for SBEs in critical systems, validated to be both more efficient and just as rigorous as current commercial tools.


<details>
  <summary>Details</summary>
Motivation: Modified Condition/Decision Coverage (MC/DC) is essential for critical systems' safety, but generating efficient test sets for its strictest form (Unique-Cause MC/DC) has been largely unexplored. The problem is pressing because nearly all conditional decisions in large-scale avionics (99.7%) are Singular Boolean Expressions (SBEs), which are ideally suited for this coverage.

Method: The authors propose 'Robin's Rule,' a deterministic algorithm to generate a minimal test set—N + 1 cases for N conditions—that achieves 100% Unique-Cause MC/DC for SBEs, without constructing a full truth table. They validate their approach using benchmarks formed from TCAS-II specifications and industry-standard verification tools.

Result: The proposed method reliably achieves full (100%) Unique-Cause MC/DC using the smallest theoretically possible test set, and outperforms a commercial tool in efficiency.

Conclusion: 'Robin’s Rule' provides a provably optimal and practical solution for efficiently achieving safety-critical verification, specifically for SBEs dominant in real-world avionics, ensuring both rigor and resource savings.

Abstract: Modified Condition/Decision Coverage (MC/DC) is a mandatory structural
coverage criterion for ensuring the reliability and safety of critical systems.
While its strictest form, Unique-Cause MC/DC, offers the highest assurance,
research on its efficient test generation has been lacking. This gap is
particularly significant, as an analysis of large-scale avionics systems shows
that 99.7% of all conditional decisions are, in fact, Singular Boolean
Expressions (SBEs) the ideal structure for applying Unique-Cause MC/DC. This
paper proposes 'Robin's Rule', a deterministic algorithm that directly
constructs a minimal test set of N + 1 cases to guarantee 100% Unique-Cause
MC/DC for SBEs with N conditions, without generating a full truth table. To
validate our approach, we constructed a benchmark by reformulating the TCAS-II
specifications into SBEs and verified the results using an industry-standard,
certified commercial tool. The results confirm that our method consistently
achieves 100% coverage with the theoretical minimum number of tests and is more
efficient than the commercial tool. This work provides a practical and provably
optimal solution for verifying safety-critical systems, ensuring both rigor and
efficiency.

</details>


### [10] [HistoryFinder: Advancing Method-Level Source Code History Generation with Accurate Oracles and Enhanced Algorithm](https://arxiv.org/abs/2507.14716)
*Shahidul Islam,Ashik Aowal,Md Sharif Uddin,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: The paper introduces HistoryFinder, a new, validated method history generation tool that offers superior accuracy and competitive runtime compared to existing tools. Comprehensive evaluation shows it is the best overall choice for reconstructing method histories, and it's made readily accessible for practical use.


<details>
  <summary>Details</summary>
Motivation: Accurately and efficiently reconstructing a method's change history is essential for various software engineering tasks, but current evaluation methods are limited by inaccurate ground truth oracles. This hinders the effective assessment and improvement of existing tools.

Method: The authors constructed two new oracles—the corrected CodeShovel oracle and a newly developed HistoryFinder oracle—using automated analysis and expert-guided manual validation. They introduced HistoryFinder, a new tool for method history generation, and performed extensive evaluations across 400 methods from 40 open-source repositories, comparing it with existing tools based on precision, recall, F1 score, and runtime.

Result: HistoryFinder consistently outperformed existing research-based tools (CodeShovel, CodeTracker, IntelliJ, and Git-based baselines) in terms of precision, recall, and F1 score, while also achieving the lowest mean and median execution times among research-based tools. Although Git-based tools had faster runtimes, they suffered from much lower precision and recall.

Conclusion: HistoryFinder is the best overall tool for method history reconstruction when both accuracy and efficiency are required, outperforming current research and Git-based tools. The tool is made easily accessible via web, CLI, and Java library interfaces to encourage adoption.

Abstract: Reconstructing a method's change history efficiently and accurately is
critical for many software engineering tasks, including maintenance,
refactoring, and comprehension. Despite the availability of method history
generation tools such as CodeShovel and CodeTracker, existing evaluations of
their effectiveness are limited by inaccuracies in the ground truth oracles
used. In this study, we systematically construct two new oracles -- the
corrected CodeShovel oracle and a newly developed HistoryFinder oracle -- by
combining automated analysis with expert-guided manual validation. We also
introduce HistoryFinder, a new method history generation tool designed to
improve not only the accuracy and completeness of method change histories but
also to offer competitive runtime performance. Through extensive evaluation
across 400 methods from 40 open-source repositories, we show that HistoryFinder
consistently outperforms CodeShovel, CodeTracker, IntelliJ, and Git-based
baselines in terms of precision, recall, and F1 score. Moreover, HistoryFinder
achieves competitive runtime performance, offering the lowest mean and median
execution times among all the research-based tools.
  While Git-based tools exhibit the fastest runtimes, this efficiency comes at
the cost of significantly lower precision and recall -- leaving HistoryFinder
as the best overall choice when both accuracy and efficiency are important. To
facilitate adoption, we provide a web interface, CLI, and Java library for
flexible usage.

</details>


### [11] [Investigating the Role of LLMs Hyperparameter Tuning and Prompt Engineering to Support Domain Modeling](https://arxiv.org/abs/2507.14735)
*Vladyslav Bulhakov,Giordano d'Aloisio,Claudio Di Sipio,Antinisca Di Marco,Davide Di Ruscio*

Main category: cs.SE

TL;DR: Instead of relying on costly and risky model fine-tuning, smart tuning of hyperparameters plus tailored prompt design can meaningfully boost large language model performance for specialized domain modeling tasks.


<details>
  <summary>Details</summary>
Motivation: General-purpose large language models (LLMs) struggle with creating accurate domain models, and fine-tuning them is resource-intensive and risky. There is a need for less resource-heavy ways to boost domain modeling performance in LLMs.

Method: The paper uses search-based approaches for hyperparameter tuning and applies prompt engineering to the Llama 3.1 model, initially focusing on a medical data model. The optimized approach is then tested across ten diverse application domains to measure generalizability.

Result: Hyperparameter tuning and prompt engineering significantly improved domain model generation for the targeted medical data model and offered notable improvements for most other tested domains, though results were not universally optimal across all domains.

Conclusion: Combining hyperparameter tuning and prompt engineering can enhance the effectiveness of large language models in generating domain models from text, providing a practical alternative to resource-intensive fine-tuning.

Abstract: The introduction of large language models (LLMs) has enhanced automation in
software engineering tasks, including in Model Driven Engineering (MDE).
However, using general-purpose LLMs for domain modeling has its limitations.
One approach is to adopt fine-tuned models, but this requires significant
computational resources and can lead to issues like catastrophic forgetting.
  This paper explores how hyperparameter tuning and prompt engineering can
improve the accuracy of the Llama 3.1 model for generating domain models from
textual descriptions. We use search-based methods to tune hyperparameters for a
specific medical data model, resulting in a notable quality improvement over
the baseline LLM. We then test the optimized hyperparameters across ten diverse
application domains.
  While the solutions were not universally applicable, we demonstrate that
combining hyperparameter tuning with prompt engineering can enhance results
across nearly all examined domain models.

</details>


### [12] [Toward Inclusive AI-Driven Development: Exploring Gender Differences in Code Generation Tool Interactions](https://arxiv.org/abs/2507.14770)
*Manaal Basha,Ivan Beschastnikh,Gema Rodriguez-Perez,Cleidson R. B. de Souza*

Main category: cs.SE

TL;DR: This study proposes a controlled experiment to examine gender differences in how developers use code generation tools. The goal is to uncover disparities in task performance and cognitive load, ultimately guiding the development of more inclusive and fair developer tools.


<details>
  <summary>Details</summary>
Motivation: Code Generation Tools (CGTs) are transforming programming workflows, but there is a lack of understanding regarding their effectiveness and fairness across varying user demographics, particularly gender. Previous studies suggest that gender can impact technology use and cognitive load, motivating this study to investigate these differences in the context of CGTs.

Method: The study uses a mixed-subjects counterbalanced design with 54 participants (equal gender distribution). Each participant will complete two programming tasks (medium to hard difficulty) using CGT assistance and, separately, internet access only. Tasks and conditions are counterbalanced. Data collected will include cognitive load surveys, screen recordings, and performance metrics (completion time, code correctness, CGT interactions). Statistical methods will be used to analyze differences between groups.

Result: There are no results yet, as the paper describes a proposed study. The expected outcome is to identify gender-based differences in CGT usage, task performance, and cognitive load.

Conclusion: The study aims to inform the design of more fair, accountable, transparent, and ethical CGTs. Anticipated contributions include better understanding of gender disparities and pathways to more inclusive and equitable AI tools for software development.

Abstract: Context: The increasing reliance on Code Generation Tools (CGTs), such as
Windsurf and GitHub Copilot, are revamping programming workflows and raising
critical questions about fairness and inclusivity. While CGTs offer potential
productivity enhancements, their effectiveness across diverse user groups have
not been sufficiently investigated. Objectives: We hypothesize that developers'
interactions with CGTs vary based on gender, influencing task outcomes and
cognitive load, as prior research suggests that gender differences can affect
technology use and cognitive processing. Methods: The study will employ a
mixed-subjects design with 54 participants, evenly divided by gender for a
counterbalanced design. Participants will complete two programming tasks
(medium to hard difficulty) with only CGT assistance and then with only
internet access. Task orders and conditions will be counterbalanced to mitigate
order effects. Data collection will include cognitive load surveys, screen
recordings, and task performance metrics such as completion time, code
correctness, and CGT interaction behaviors. Statistical analyses will be
conducted to identify statistically significant differences in CGT usage.
Expected Contributions: Our work can uncover gender differences in CGT
interaction and performance among developers. Our findings can inform future
CGT designs and help address usability and potential disparities in interaction
patterns across diverse user groups. Conclusion: While results are not yet
available, our proposal lays the groundwork for advancing fairness,
accountability, transparency, and ethics (FATE) in CGT design. The outcomes are
anticipated to contribute to inclusive AI practices and equitable tool
development for all users.

</details>


### [13] [On the Effectiveness of Large Language Models in Writing Alloy Formulas](https://arxiv.org/abs/2502.15441)
*Yang Hong,Shan Jiang,Yulei Fu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: The paper demonstrates that large language models can reliably generate and complete Alloy declarative specifications from natural language, making specification writing easier and potentially leading to more robust and dependable software systems.


<details>
  <summary>Details</summary>
Motivation: Declarative specifications are crucial for developing safe and reliable software systems, but writing them accurately is a significant challenge. The paper is motivated by the need to simplify and improve the correctness of writing such specifications, specifically in the Alloy language.

Method: The authors design a controlled experiment using two major large language models (LLMs), ChatGPT and DeepSeek. They evaluate the LLMs on three tasks: (1) generating complete Alloy formulas from natural language, (2) producing alternative but equivalent Alloy formulas, and (3) completing partial Alloy formula sketches based on English descriptions, across 11 established subject specifications.

Result: The results indicate that both ChatGPT and DeepSeek perform well across all evaluated tasks. They effectively generate accurate and multiple unique Alloy formulas from both natural language and Alloy, and successfully complete formula sketches without needing test cases.

Conclusion: LLMs like ChatGPT and DeepSeek markedly enhance the process of writing declarative specifications in Alloy. Their ability to synthesize and complete formulas from natural language helps make specification writing more accessible, potentially broadening the use and correctness of specifications in safe software development.

Abstract: Declarative specifications have a vital role to play in developing safe and
dependable software systems. Writing specifications correctly, however, remains
particularly challenging. This paper presents a controlled experiment on using
large language models (LLMs) to write declarative formulas in the well-known
language Alloy. Our use of LLMs is three-fold. One, we employ LLMs to write
complete Alloy formulas from given natural language descriptions (in English).
Two, we employ LLMs to create alternative but equivalent formulas in Alloy with
respect to given Alloy formulas. Three, we employ LLMs to complete sketches of
Alloy formulas and populate the holes in the sketches by synthesizing Alloy
expressions and operators so that the completed formulas accurately represent
the desired properties (that are given in natural language). We conduct the
experimental evaluation using 11 well-studied subject specifications and employ
two popular LLMs, namely ChatGPT and DeepSeek. The experimental results show
that the LLMs generally perform well in synthesizing complete Alloy formulas
from input properties given in natural language or in Alloy, and are able to
enumerate multiple unique solutions. Moreover, the LLMs are also successful at
completing given sketches of Alloy formulas with respect to natural language
descriptions of desired properties (without requiring test cases). We believe
LLMs offer a very exciting advance in our ability to write specifications, and
can help make specifications take a pivotal role in software development and
enhance our ability to build robust software.

</details>


### [14] [VeriOpt: PPA-Aware High-Quality Verilog Generation via Multi-Role LLMs](https://arxiv.org/abs/2507.14776)
*Kimia Tasnia,Alexander Garcia,Tasnuva Farheen,Sazadur Rahman*

Main category: cs.SE

TL;DR: VeriOpt enhances LLM-generated hardware code by optimizing for power, performance, and area, utilizing role-based prompting and PPA-aware feedback, achieving significant efficiency gains and high functional correctness in industrial benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based approaches for hardware design focus on functional correctness of Verilog code but overlook key Power, Performance, and Area (PPA) requirements crucial for industrial applications.

Method: The authors introduce VeriOpt, which uses role-based prompting (with roles like Planner, Programmer, Reviewer, Evaluator) paired with direct integration of PPA constraints and multi-modal feedback (such as synthesis reports and timing diagrams) in the LLM prompting process.

Result: VeriOpt yielded up to 88% reduction in power, 76% reduction in area, and 73% better timing closure compared to standard LLM RTL generation, while retaining 86% functionality success rate, as validated by industry EDA tools.

Conclusion: VeriOpt effectively bridges the gap between functionally correct and PPA-optimized Verilog generated by LLMs, enabling more practical adoption in industrial hardware design workflows.

Abstract: The rapid adoption of large language models(LLMs) in hardware design has
primarily focused on generating functionally correct Verilog code, overlooking
critical Power Performance-Area(PPA) metrics essential for industrial-grade
designs. To bridge this gap, we propose VeriOpt, a novel framework that
leverages role-based prompting and PPA-aware optimization to enable LLMs to
produce high-quality, synthesizable Verilog. VeriOpt structures LLM
interactions into specialized roles (e.g., Planner, Programmer, Reviewer,
Evaluator) to emulate human design workflows, while integrating PPA constraints
directly into the prompting pipeline. By combining multi-modal feedback (e.g.,
synthesis reports, timing diagrams) with PPA aware prompting, VeriOpt achieves
PPA-efficient code generation without sacrificing functional correctness.
Experimental results demonstrate up to 88% reduction in power, 76% reduction in
area and 73% improvement in timing closure compared to baseline LLM-generated
RTL, validated using industry standard EDA tools. At the same time achieves 86%
success rate in functionality evaluation. Our work advances the
state-of-the-art AI-driven hardware design by addressing the critical gap
between correctness and quality, paving the way for reliable LLM adoption in
production workflows.

</details>


### [15] [Enhancing Repository-Level Code Generation with Call Chain-Aware Multi-View Context](https://arxiv.org/abs/2507.14791)
*Yang Liu,Li Zhang,Fang Liu,Zhuohang Wang,Donglin Wei,Zhishuo Yang,Kechi Zhang,Jia Li,Lin Shi*

Main category: cs.SE

TL;DR: RepoScope, a new static analysis-based method, greatly improves repository-level code generation by providing LLMs with richer, structurally-informed context. It beats existing methods in benchmark tests, requires no retraining, and boosts generation accuracy by over 36%.


<details>
  <summary>Details</summary>
Motivation: Existing repository-level code generation methods struggle to find relevant contextual information that captures the repository’s rich semantics and often have a narrow contextual perspective. They also fail to consider structural relationships in code, which hurts large language models’ (LLMs) ability to interpret context for code generation.

Method: RepoScope is introduced as a new approach. It builds a Repository Structural Semantic Graph (RSSG) and retrieves a comprehensive, four-view context that combines both structural and similarity-based contexts. RepoScope uses a novel call chain prediction method that leverages repository structural semantics to enhance callee identification, and presents a structure-preserving serialization algorithm for constructing LLM prompts. Importantly, it relies only on static analysis and requires no extra training or multiple LLM queries.

Result: RepoScope achieves up to a 36.35% relative improvement in pass@1 scores on CoderEval and DevEval benchmarks over prior state-of-the-art methods. Additional experiments show RepoScope improves code generation across various tasks and integrates well with existing solutions.

Conclusion: RepoScope provides a more contextually aware and structurally rich method for repository-level code generation, significantly outperforming current methods in both accuracy and adaptability, without requiring extra model training or multiple LLM queries.

Abstract: Repository-level code generation aims to generate code within the context of
a specified repository. Existing approaches typically employ
retrieval-augmented generation (RAG) techniques to provide LLMs with relevant
contextual information extracted from the repository. However, these approaches
often struggle with effectively identifying truly relevant contexts that
capture the rich semantics of the repository, and their contextual perspectives
remains narrow. Moreover, most approaches fail to account for the structural
relationships in the retrieved code during prompt construction, hindering the
LLM's ability to accurately interpret the context. To address these issues, we
propose RepoScope, which leverages call chain-aware multi-view context for
repository-level code generation. RepoScope constructs a Repository Structural
Semantic Graph (RSSG) and retrieves a comprehensive four-view context,
integrating both structural and similarity-based contexts. We propose a novel
call chain prediction method that utilizes the repository's structural
semantics to improve the identification of callees in the target function.
Additionally, we present a structure-preserving serialization algorithm for
prompt construction, ensuring the coherence of the context for the LLM.
Notably, RepoScope relies solely on static analysis, eliminating the need for
additional training or multiple LLM queries, thus ensuring both efficiency and
generalizability. Evaluation on widely-used repository-level code generation
benchmarks (CoderEval and DevEval) demonstrates that RepoScope outperforms
state-of-the-art methods, achieving up to a 36.35% relative improvement in
pass@1 scores. Further experiments emphasize RepoScope's potential to improve
code generation across different tasks and its ability to integrate effectively
with existing approaches.

</details>


### [16] [Think Like an Engineer: A Neuro-Symbolic Collaboration Agent for Generative Software Requirements Elicitation and Self-Review](https://arxiv.org/abs/2507.14969)
*Sai Zhang,Zhenchang Xing,Jieshan Chen,Dehai Zhao,Zizhong Zhu,Xiaowang Zhang,Zhiyong Feng,Xiaohong Li*

Main category: cs.SE

TL;DR: RequireCEG helps non-programmers clearly define software requirements by translating ambiguous user stories into structured, causally-expressive scenarios, improving coverage and variety over previous methods.


<details>
  <summary>Details</summary>
Motivation: End-User Software Engineering (EUSE) seeks to let non-professional users control software development with natural language, but user requirements are often ambiguous, making generative software development difficult. Structured languages help but do not fully capture causal logic between requirements' preconditions and actions.

Method: The authors propose RequireCEG, a requirement elicitation and review agent using a neuro-symbolic approach with causal-effect graphs (CEGs). It analyzes user narratives with feature trees to define scope and requirements, constructs CEGs to capture causality, and uses these CEGs to refine and validate Gherkin scenarios for consistency.

Result: RequireCEG was evaluated on the new RGPair benchmark, achieving an 87% coverage rate and increasing requirements diversity by 51.88%.

Conclusion: RequireCEG offers a novel, effective neuro-symbolic method for end-user-driven requirements elicitation and review, addressing ambiguity and causal logic in user-generated requirements. This approach significantly improves coverage and diversity in generated software requirements.

Abstract: The vision of End-User Software Engineering (EUSE) is to empower
non-professional users with full control over the software development
lifecycle. It aims to enable users to drive generative software development
using only natural language requirements. However, since end-users often lack
knowledge of software engineering, their requirement descriptions are
frequently ambiguous, raising significant challenges to generative software
development. Although existing approaches utilize structured languages like
Gherkin to clarify user narratives, they still struggle to express the causal
logic between preconditions and behavior actions. This paper introduces
RequireCEG, a requirement elicitation and self-review agent that embeds
causal-effect graphs (CEGs) in a neuro-symbolic collaboration architecture.
RequireCEG first uses a feature tree to analyze user narratives hierarchically,
clearly defining the scope of software components and their system behavior
requirements. Next, it constructs the self-healing CEGs based on the elicited
requirements, capturing the causal relationships between atomic preconditions
and behavioral actions. Finally, the constructed CEGs are used to review and
optimize Gherkin scenarios, ensuring consistency between the generated Gherkin
requirements and the system behavior requirements elicited from user
narratives. To evaluate our method, we created the RGPair benchmark dataset and
conducted extensive experiments. It achieves an 87% coverage rate and raises
diversity by 51.88%.

</details>


### [17] [The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering](https://arxiv.org/abs/2507.15003)
*Hao Li,Haoxiang Zhang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: This paper introduces AIDev, the first large-scale, real-world dataset of AI coding agent activity. It enables empirical research into how autonomous agents contribute to software engineering, revealing both their speed advantage and trust gaps compared to humans. AIDev is intended as a living resource to drive future studies and improvements in human-AI collaboration.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the emergence of AI teammates—autonomous, goal-driven agents working alongside human software developers. While these agents, such as coding assistants, are transforming the software engineering landscape, most prior research has focused on theorizing their impact rather than providing practical, empirical resources.

Method: The authors introduce AIDev, a large-scale dataset that captures real-world activity of autonomous coding agents. They collected and structured data from over 456,000 pull requests involving five major AI coding agents across 61,000 repositories and 47,000 developers, providing rich metadata on code changes, authorship, review processes, and integration outcomes.

Result: AIDev reveals key findings: AI coding agents often submit code much faster than humans but have lower pull request acceptance rates, highlighting trust and utility gaps. The dataset uncovers that while submission frequency rises, the structural complexity of the agents’ contributions is lower. The open dataset enables new empirical research into AI-native software development, benchmarking, collaboration, and governance.

Conclusion: AIDev represents a foundational, extensible resource for studying AI teammates in real-world software engineering. It grounds discussions of AI-driven software development (SE 3.0) in empirical reality and is expected to spur research into human-AI collaboration, benchmarking, and optimization. The dataset is publicly available to facilitate ongoing innovation.

Abstract: The future of software engineering--SE 3.0--is unfolding with the rise of AI
teammates: autonomous, goal-driven systems collaborating with human developers.
Among these, autonomous coding agents are especially transformative, now
actively initiating, reviewing, and evolving code at scale. This paper
introduces AIDev, the first large-scale dataset capturing how such agents
operate in the wild. Spanning over 456,000 pull requests by five leading
agents--OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code--across
61,000 repositories and 47,000 developers, AIDev provides an unprecedented
empirical foundation for studying autonomous teammates in software development.
  Unlike prior work that has largely theorized the rise of AI-native software
engineering, AIDev offers structured, open data to support research in
benchmarking, agent readiness, optimization, collaboration modeling, and AI
governance. The dataset includes rich metadata on PRs, authorship, review
timelines, code changes, and integration outcomes--enabling exploration beyond
synthetic benchmarks like SWE-bench. For instance, although agents often
outperform humans in speed, their PRs are accepted less frequently, revealing a
trust and utility gap. Furthermore, while agents accelerate code
submission--one developer submitted as many PRs in three days as they had in
three years--these are structurally simpler (via code complexity metrics).
  We envision AIDev as a living resource: extensible, analyzable, and ready for
the SE and AI communities. Grounding SE 3.0 in real-world evidence, AIDev
enables a new generation of research into AI-native workflows and supports
building the next wave of symbiotic human-AI collaboration. The dataset is
publicly available at https://github.com/SAILResearch/AI_Teammates_in_SE3.
  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering
Agent

</details>


### [18] [Survey of GenAI for Automotive Software Development: From Requirements to Executable Code](https://arxiv.org/abs/2507.15025)
*Nenad Petrovic,Vahid Zolfaghari,Andre Schamschurko,Sven Kirchner,Fengjunjie Pan,Chengdng Wu,Nils Purschke,Aleksei Velsh,Krzysztof Lebioda,Yinglei Song,Yi Zhang,Lukasz Mazur,Alois Knoll*

Main category: cs.SE

TL;DR: This paper reviews the role of generative AI (GenAI)—including LLMs, RAG, and VLMs—in automotive software development, focusing on requirements, compliance, and coding. It proposes a generalized workflow for GenAI integration and reports on industry adoption levels based on a survey.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve efficiency and reduce human intervention and effort in automotive software development, which is currently hampered by complex, time-consuming, and expensive processes due to heavy requirements and stringent regulations.

Method: The paper conducts a literature review exploring the use of generative AI (GenAI) technologies—specifically Large Language Models (LLMs), Retrieval Augmented Generation (RAG), and Vision Language Models (VLMs)—for multiple steps in automotive software development. It analyzes prompting techniques for code generation and gathers practical insights via a survey of industry partners.

Result: The study maps out existing GenAI technologies relevant to automotive software development, highlights their applications in requirements handling, compliance, and code generation, and presents a generalized workflow for GenAI-aided development. It also reports on the actual GenAI tools being adopted by industry professionals, as identified through a survey.

Conclusion: GenAI has significant potential to transform automotive software development across various key stages by automating tasks, streamlining compliance, and aiding code generation. The paper provides both a conceptual overview and some initial practical findings about GenAI adoption in the automotive sector.

Abstract: Adoption of state-of-art Generative Artificial Intelligence (GenAI) aims to
revolutionize many industrial areas by reducing the amount of human
intervention needed and effort for handling complex underlying processes.
Automotive software development is considered to be a significant area for
GenAI adoption, taking into account lengthy and expensive procedures, resulting
from the amount of requirements and strict standardization. In this paper, we
explore the adoption of GenAI for various steps of automotive software
development, mainly focusing on requirements handling, compliance aspects and
code generation. Three GenAI-related technologies are covered within the
state-of-art: Large Language Models (LLMs), Retrieval Augmented Generation
(RAG), Vision Language Models (VLMs), as well as overview of adopted prompting
techniques in case of code generation. Additionally, we also derive a
generalized GenAI-aided automotive software development workflow based on our
findings from this literature review. Finally, we include a summary of a survey
outcome, which was conducted among our automotive industry partners regarding
the type of GenAI tools used for their daily work activities.

</details>


### [19] [Can LLMs Generate User Stories and Assess Their Quality?](https://arxiv.org/abs/2507.15157)
*Giovanni Quattrocchi,Liliana Pasquale,Paola Spoletini,Luciano Baresi*

Main category: cs.SE

TL;DR: This paper finds that LLMs can generate and evaluate agile user stories at a quality level comparable to humans, though with less creativity and slightly lower adherence to acceptance criteria. LLMs can also reliably automate semantic quality assessment, potentially saving significant manual effort.


<details>
  <summary>Details</summary>
Motivation: Eliciting and specifying high-quality software requirements is difficult, especially in agile frameworks. While automated tools handle syntactic assessment, semantic evaluation remains largely manual and time-consuming. The authors aim to explore how large language models (LLMs) can help automate these activities, particularly for user stories (US).

Method: The authors used 10 state-of-the-art large language models (LLMs) to automatically generate user stories by simulating customer interviews. They compared the quality of LLM-generated US with those written by humans (domain experts and students) and examined LLMs' ability to assess semantic quality when given clear criteria.

Result: LLMs can produce user stories that match human quality in terms of coverage and style, though they lag in diversity and creativity. LLM-generated stories less frequently meet acceptance quality criteria, and this gap is consistent across different LLM sizes. LLMs are reliable in assessing US semantic quality when clear evaluation guidelines are provided, potentially reducing human effort in large-scale evaluations.

Conclusion: LLMs show promise for automating both the generation and semantic assessment of user stories, offering comparable results to humans in quality but with reduced creativity and acceptance fulfillment. Given specific evaluation criteria, LLMs can greatly assist in scaling up quality assessments.

Abstract: Requirements elicitation is still one of the most challenging activities of
the requirements engineering process due to the difficulty requirements
analysts face in understanding and translating complex needs into concrete
requirements. In addition, specifying high-quality requirements is crucial, as
it can directly impact the quality of the software to be developed. Although
automated tools allow for assessing the syntactic quality of requirements,
evaluating semantic metrics (e.g., language clarity, internal consistency)
remains a manual and time-consuming activity. This paper explores how LLMs can
help automate requirements elicitation within agile frameworks, where
requirements are defined as user stories (US). We used 10 state-of-the-art LLMs
to investigate their ability to generate US automatically by emulating customer
interviews. We evaluated the quality of US generated by LLMs, comparing it with
the quality of US generated by humans (domain experts and students). We also
explored whether and how LLMs can be used to automatically evaluate the
semantic quality of US. Our results indicate that LLMs can generate US similar
to humans in terms of coverage and stylistic quality, but exhibit lower
diversity and creativity. Although LLM-generated US are generally comparable in
quality to those created by humans, they tend to meet the acceptance quality
criteria less frequently, regardless of the scale of the LLM model. Finally,
LLMs can reliably assess the semantic quality of US when provided with clear
evaluation criteria and have the potential to reduce human effort in
large-scale assessments.

</details>


### [20] [Deep Learning Framework Testing via Heuristic Guidance Based on Multiple Model Measurements](https://arxiv.org/abs/2507.15181)
*Yinglong Zou,Juan Zhai,Chunrong Fang,Yanzhou Mu,Jiawei Liu,Zhenyu Chen*

Main category: cs.SE

TL;DR: Existing testing methods for deep learning frameworks miss important aspects of model evaluation. DLMMM is proposed to address these issues by simultaneously considering and fusing multiple quantitative factors, including bug detection, operator combination diversity, and execution time, thus achieving better and more efficient testing outcomes.


<details>
  <summary>Details</summary>
Motivation: Existing methods for testing deep learning frameworks rely on heuristic indicators to measure bug detection effectiveness but have significant limitations: lack of quantitative measurement of operator combinations, ignoring model execution time, and overlooking correlations between different model measurements.

Method: The authors propose DLMMM, a new deep learning framework testing method. DLMMM quantitatively measures three aspects—bug detection performance, operator combination variety, and model execution time—then fuses these measurements by considering their mutual correlations to balance the trade-offs. It also introduces multi-level heuristic guidance during the model generation process.

Result: DLMMM enables more comprehensive and effective testing by utilizing multiple fused measurements, addressing trade-offs among different aspects of model testing, and generating better test input models for deep learning frameworks.

Conclusion: DLMMM overcomes the three core limitations of prior testing methods by integrating multiple quantitative measurements and their correlations, thus enhancing testing efficiency and the probability of uncovering framework bugs.

Abstract: Deep learning frameworks serve as the foundation for developing and deploying
deep learning applications. To enhance the quality of deep learning frameworks,
researchers have proposed numerous testing methods using deep learning models
as test inputs. However, existing methods predominantly measure model bug
detection effectiveness as heuristic indicators, presenting three critical
limitations: Firstly, existing methods fail to quantitatively measure model's
operator combination variety, potentially missing critical operator
combinations that could trigger framework bugs. Secondly, existing methods
neglect measuring model execution time, resulting in the omission of numerous
models potential for detecting more framework bugs within limited testing time.
Thirdly, existing methods overlook correlation between different model
measurements, relying simply on single-indicator heuristic guidance without
considering their trade-offs. To overcome these limitations, we propose DLMMM,
the first deep learning framework testing method to include multiple model
measurements into heuristic guidance and fuse these measurements to achieve
their trade-off. DLMMM firstly quantitatively measures model's bug detection
performance, operator combination variety, and model execution time. After
that, DLMMM fuses the above measurements based on their correlation to achieve
their trade-off. To further enhance testing effectiveness, DLMMM designs
multi-level heuristic guidance for test input model generation.

</details>


### [21] [Cultural Impact on Requirements Engineering Activities: Bangladeshi Practitioners' View](https://arxiv.org/abs/2507.15188)
*Chowdhury Shahriar Muzammel,Maria Spichkova,James Harland*

Main category: cs.SE

TL;DR: The study examines how Bangladeshi culture affects Requirements Engineering processes and highlights the importance of cultural awareness for effective and inclusive software development.


<details>
  <summary>Details</summary>
Motivation: Requirements Engineering (RE) involves significant interactions among stakeholders, and cultural differences can affect these interactions. As software projects become more global, understanding Cultural Influences (CIs) is necessary to avoid misunderstandings and support diversity, especially since some regions like Bangladesh have not been thoroughly studied in this context.

Method: The paper investigates the adoption of RE processes within the cultural context of Bangladesh by examining how local socio-cultural characteristics influence RE activities.

Result: The study identifies specific cultural influences within Bangladesh that impact the implementation and effectiveness of RE activities in software development projects.

Conclusion: Understanding and addressing cultural influences in RE can help practitioners in Bangladesh (and similar contexts) to reduce misunderstandings and support more inclusive software engineering practices.

Abstract: Requirements Engineering (RE) is one of the most interaction-intensive phases
of software development. This means that RE activities might be especially
impacted by stakeholders' national culture. Software development projects
increasingly have a very diverse range of stakeholders. To future-proof RE
activities, we need to help RE practitioners avoid misunderstandings and
conflicts that might arise from not understanding potential Cultural Influences
(CIs). Moreover, an awareness of CIs supports diversity and inclusion in the IT
profession. Bangladesh has a growing IT sector with some unique socio-cultural
characteristics, and has been largely overlooked in this research field. In
this study, we aim to investigate how the RE process is adopted in the context
of Bangladeshi culture and what cultural influences impact overall RE
activities.

</details>


### [22] [Towards Using Personas in Requirements Engineering: What Has Been Changed Recently?](https://arxiv.org/abs/2507.15197)
*Chowdhury Shahriar Muzammel,Maria Spichkova,James Harland*

Main category: cs.SE

TL;DR: The paper maps recent studies (2023-2025) on personas in requirements engineering, highlighting that AI (especially generative) is now commonly used for creating and validating personas, with templates and validation gaining more attention.


<details>
  <summary>Details</summary>
Motivation: Personas are increasingly used in requirements engineering (RE) to capture user needs and expectations. With the rise of Generative AI, there may be recent shifts in how personas are created and validated. The authors want to map current trends and approaches, especially those influenced by new AI methods.

Method: The authors conducted a systematic mapping study (SMS) covering literature from April 2023 to April 2025. They reviewed 22 relevant publications, analyzing themes such as persona representation, construction, validation, and the range of RE activities involving personas.

Result: The study found a growing application of AI-based approaches for both persona construction and validation. Template-based personas have become more popular, and there is a noticeable increase in studies that address persona validation.

Conclusion: Recent work in RE shows an increasing integration of AI, particularly generative models, in the creation and validation of personas. There's a trend toward standardized, template-based personas and greater attention to their validation in the field.

Abstract: In requirements engineering (RE), personas are now being used to represent
user expectations and needs. This systematic mapping study (SMS) aims to
explore the most recent studies and to cover recent changes in trends,
especially related to the recent evolution of Generative AI approaches. Our SMS
covers the period between April 2023 and April 2025. We identified 22 relevant
publications and analysed persona representation, construction, validation, as
well as RE activities covered by personas. We identified that a number of
studies applied AI-based solutions for persona construction and validation. We
observed that template-based personas are becoming more popular nowadays. We
also observed an increase in the proportion of studies covering validation
aspects.

</details>


### [23] [SimdBench: Benchmarking Large Language Models for SIMD-Intrinsic Code Generation](https://arxiv.org/abs/2507.15224)
*Yibo He,Shuoran Zhao,Jiaming Huang,Yingjie Fu,Hao Yu,Cunjian Huang,Tao Xie*

Main category: cs.SE

TL;DR: A new benchmark (SimdBench) for generating SIMD code with LLMs reveals that current models are much less effective at this task than for scalar code, pointing to the need for improvements and further research.


<details>
  <summary>Details</summary>
Motivation: SIMD intrinsic programming is essential for high performance in critical software, but coding it is challenging, and support for SIMD code generation with Large Language Models (LLMs) is unstudied. Existing code-generation benchmarks do not address SIMD, making performance and correctness in this domain unknown.

Method: The authors introduce SimdBench, a new benchmark specifically for SIMD-intrinsic code generation. SimdBench features 136 tasks using five major SIMD instruction sets (SSE, AVX, Neon, SVE, RVV). They systematically evaluate 18 LLMs using this benchmark, assessing both code correctness and performance.

Result: Their experiments show that all LLMs have a significant drop in successful code generation (pass@k metrics) when producing SIMD-intrinsic code compared to scalar code. The paper presents new findings and identifies potential research directions for improving LLMs’ performance in this area.

Conclusion: LLMs currently struggle more with SIMD-intrinsic code generation than scalar code generation. SimdBench highlights these challenges and will help the research community advance LLM capabilities in high-performance code generation.

Abstract: SIMD (Single Instruction Multiple Data) instructions and their compiler
intrinsics are widely supported by modern processors to accelerate
performance-critical tasks. SIMD intrinsic programming, a trade-off between
coding productivity and high performance, is widely used in the development of
mainstream performance-critical libraries and daily computing tasks. Large
Language Models (LLMs), which have demonstrated strong and comprehensive
capabilities in code generation, show promise in assisting programmers with the
challenges of SIMD intrinsic programming. However, existing code-generation
benchmarks focus on only scalar code, and it is unclear how LLMs perform in
generating vectorized code using SIMD intrinsics. To fill this gap, we propose
SimdBench, the first code benchmark specifically designed for SIMD-intrinsic
code generation, comprising 136 carefully crafted tasks and targeting five
representative SIMD intrinsics: SSE (x86 Streaming SIMD Extension), AVX (x86
Advanced Vector Extension), Neon (ARM Advanced SIMD Extension), SVE (ARM
Scalable Vector Extension), and RVV (RISC-V Vector Extension). We conduct a
systematic evaluation (measuring both correctness and performance) of 18
representative LLMs on SimdBench, resulting in a series of novel and insightful
findings. Our evaluation results demonstrate that LLMs exhibit a universal
decrease in pass@k during SIMD-intrinsic code generation compared to
scalar-code generation. Our in-depth analysis highlights promising directions
for the further advancement of LLMs in the challenging domain of SIMD-intrinsic
code generation. SimdBench is fully open source at
https://anonymous.4open.science/r/SimdBench-1B3F/ to benefit the broader
research community.

</details>


### [24] [Code Clone Detection via an AlphaFold-Inspired Framework](https://arxiv.org/abs/2507.15226)
*Changguo Jia,Yi Zhan,Tianqi Zhao,Hengzhi Ye,Minghui Zhou*

Main category: cs.SE

TL;DR: AlphaCC, inspired by AlphaFold, introduces a novel, multi-language semantic code clone detector using protein sequence modeling strategies, and achieves superior accuracy and efficiency across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Existing code clone detection methods either inadequately capture code semantics or rely heavily on language-specific analyzers. There is a need for a more general and semantically robust approach.

Method: Inspired by AlphaFold, the paper proposes AlphaCC, which models code fragments as token sequences and uses a sequence-to-structure-like approach for semantic inference. It first converts code into token sequences and constructs multiple sequence alignments (MSA), applies a modified attention-based encoder (adapted from AlphaFold) to generate contextual semantic representations, and finally computes similarity scores using a late interaction strategy for binary classification of code clones.

Result: AlphaCC demonstrates strong semantic understanding, outperforming all baselines on two semantic clone detection datasets and showing broad applicability across programming languages. It also achieves competitive efficiency suitable for large-scale deployment.

Conclusion: AlphaCC leverages protein sequence modeling concepts to improve code clone detection's semantic understanding and language generality, confirming its effectiveness and practicality through extensive evaluation.

Abstract: Code clone detection, which aims to identify functionally equivalent code
fragments, plays a critical role in software maintenance and vulnerability
analysis. Substantial methods have been proposed to detect code clones, but
they fall short in capturing code semantics or relying on language-specific
analyzers. Inspired by the remarkable success of AlphaFold in predicting
three-dimensional protein structures from protein sequences, in this paper, we
leverage AlphaFold for code clone detection based on the insight that protein
sequences and token sequences share a common linear sequential structure. In
particular, we propose AlphaCC, which represents code fragments as token
sequences to ensure multi-language applicability and adapts AlphaFold's
sequence-to-structure modeling capability to infer code semantics. The pipeline
of AlphaCC goes through three steps. First, AlphaCC transforms each input code
fragment into a token sequence and, motivated by AlphaFold's use of multiple
sequence alignment (MSA) to enhance contextual understanding, constructs an MSA
from lexically similar token sequences. Second, AlphaCC adopts a modified
attention-based encoder based on AlphaFold to model dependencies within and
across token sequences. Finally, unlike AlphaFold's protein structure
prediction task, AlphaCC computes similarity scores between token sequences
through a late interaction strategy and performs binary classification to
determine code clone pairs. Comprehensive evaluations on three language-diverse
datasets demonstrate AlphaCC's applicability across multiple programming
languages. On two semantic clone detection datasets, it consistently
outperforms all baselines, showing strong semantic understanding. Moreover,
AlphaCC maintains competitive efficiency, enabling practical usage in
large-scale clone detection tasks.

</details>


### [25] [FaultLine: Automated Proof-of-Vulnerability Generation Using LLM Agents](https://arxiv.org/abs/2507.15241)
*Vikram Nitin,Baishakhi Ray,Roshanak Zilouchian Moghaddam*

Main category: cs.SE

TL;DR: FaultLine, an LLM-powered workflow, outperforms prior methods in automatically generating vulnerability exploit tests across programming languages, but the task is still challenging; their resources are open for community use.


<details>
  <summary>Details</summary>
Motivation: Software vulnerability reports often lack proof-of-vulnerability (PoV) tests, which are important for validating fixes, preventing regressions, and helping developers understand exploits. Manual generation of these tests is difficult due to the complexity of reasoning about control and data flow in programs.

Method: The authors introduce FaultLine, an LLM agent workflow that leverages structured, step-by-step reasoning inspired by static and dynamic program analysis to automatically generate PoV test cases. FaultLine identifies input flow from an API to the vulnerable sink, determines path constraints, and generates tests in a feedback-driven loop, without employing language-specific analysis, enabling multi-language use.

Result: FaultLine was evaluated on a challenging dataset of 100 vulnerabilities across Java, C, and C++ projects. It successfully generated PoV tests for 16 projects, compared to 9 by the state-of-the-art CodeAct 2.1, representing a 77% relative improvement.

Conclusion: FaultLine demonstrates that hierarchical reasoning enhances LLM agent performance in PoV test generation across multiple languages. However, the general problem remains difficult. The project's code and dataset are publicly released to foster further research.

Abstract: Despite the critical threat posed by software security vulnerabilities,
reports are often incomplete, lacking the proof-of-vulnerability (PoV) tests
needed to validate fixes and prevent regressions. These tests are crucial not
only for ensuring patches work, but also for helping developers understand how
vulnerabilities can be exploited. Generating PoV tests is a challenging
problem, requiring reasoning about the flow of control and data through deeply
nested levels of a program.
  We present FaultLine, an LLM agent workflow that uses a set of carefully
designed reasoning steps, inspired by aspects of traditional static and dynamic
program analysis, to automatically generate PoV test cases. Given a software
project with an accompanying vulnerability report, FaultLine 1) traces the flow
of an input from an externally accessible API ("source") to the "sink"
corresponding to the vulnerability, 2) reasons about the conditions that an
input must satisfy in order to traverse the branch conditions encountered along
the flow, and 3) uses this reasoning to generate a PoV test case in a
feedback-driven loop. FaultLine does not use language-specific static or
dynamic analysis components, which enables it to be used across programming
languages.
  To evaluate FaultLine, we collate a challenging multi-lingual dataset of 100
known vulnerabilities in Java, C and C++ projects. On this dataset, FaultLine
is able to generate PoV tests for 16 projects, compared to just 9 for CodeAct
2.1, a popular state-of-the-art open-source agentic framework. Thus, FaultLine
represents a 77% relative improvement over the state of the art. Our findings
suggest that hierarchical reasoning can enhance the performance of LLM agents
on PoV test generation, but the problem in general remains challenging. We make
our code and dataset publicly available in the hope that it will spur further
research in this area.

</details>


### [26] [Input Reduction Enhanced LLM-based Program Repair](https://arxiv.org/abs/2507.15251)
*Boyang Yang,Luyao Ren,Xin Yin,Jiadong Ren,Haoye Tian,Shunfu Jin*

Main category: cs.SE

TL;DR: The paper introduces ReduceFix, an approach that prompts LLMs to generate smaller, failure-preserving test inputs for automated program repair. This significantly improves repair rates and overcomes the limitations of long-input prompts, as demonstrated on a new benchmark dataset.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) struggle to effectively repair code when given long test inputs, as key information may be lost due to the "lost-in-the-middle" issue, negatively impacting Automated Program Repair (APR) performance.

Method: The authors propose ReduceFix, an LLM-based APR approach that automatically reduces test inputs by generating a "reducer" via the LLM itself, ensuring that reduced inputs still induce the same failure. The reduced inputs are then used to guide the patch generation process.

Result: ReduceFix was evaluated using LFTBench, a new benchmark of 200 real bugs with large failure-inducing inputs. ReduceFix shrank inputs by 89.1% on average, improved pass@10 repair rates by up to 53.8% compared to using the original test inputs, and by 17.6% versus omitting tests. Enhanced ChatRepair with this reduction achieved a 21.3% higher fix rate. Ablation studies confirmed the importance of input length and compressed failure information.

Conclusion: Automatically reducing failure-inducing test inputs is an effective way to improve the scalability and performance of LLM-based APR approaches.

Abstract: Large Language Models (LLMs) have shown great potential in Automated Program
Repair (APR). Test inputs, being crucial for reasoning the root cause of
failures, are always included in the prompt for LLM-based APR. Unfortunately,
LLMs struggle to retain key information in long prompts. When the test inputs
are extensive in the prompt, this may trigger the "lost-in-the-middle" issue,
compromising repair performance. To address this, we propose ReduceFix, an
LLM-based APR approach with a built-in component that automatically reduces
test inputs while retaining their failure-inducing behavior. ReduceFix prompts
an LLM to generate a reducer that minimizes failure-inducing test inputs
without human effort, and then feeds the reduced failure-inducing inputs to
guide patch generation.
  For targeted evaluation, we constructed LFTBench, the first long-input APR
benchmark with 200 real bugs from 20 programming tasks, each paired with a
failure-inducing input whose median size is 1 MB. On this benchmark, ReduceFix
shrinks inputs by 89.1% on average and improves overall pass@10 by up to 53.8%
relative to a prompt that includes the original test, and by 17.6% compared
with omitting the test entirely. Adding the same reduction step to ChatRepair
increases its fix rate by 21.3% without other changes. Ablation studies further
highlight the impact of input length and compressed failure information on
repair success. These results underscore that automatically reducing failing
inputs is a practical and powerful complement to LLM-based APR, significantly
improving its scalability and effectiveness.

</details>


### [27] [Butterfly Effects in Toolchains: A Comprehensive Analysis of Failed Parameter Filling in LLM Tool-Agent Systems](https://arxiv.org/abs/2507.15296)
*Qian Xiong,Yuekai Huang,Ziyou Jiang,Zhiyuan Chang,Yujia Zheng,Tianhao Li,Mingyang Li*

Main category: cs.SE

TL;DR: The paper categorizes various parameter failures in tool agent LLMs, analyzes the causes, and proposes solutions to boost reliability—including standardizing outputs and improving feedback mechanisms.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the desire to enhance the reliability and effectiveness of tool agents powered by Large Language Models (LLMs), particularly in addressing the issue of parameter failure that limits the paradigm's capabilities.

Method: The authors construct a taxonomy of parameter failures by analyzing the invocation chain of a mainstream tool agent. They apply 15 input perturbation methods to three different input sources to investigate the link between input types and parameter failure categories.

Result: Experimental results reveal that parameter name hallucination failure is mostly due to inherent LLM constraints, whereas other failure types are mainly caused by issues with the input sources.

Conclusion: To mitigate parameter failures, the paper recommends standardizing tool return formats, enhancing error feedback mechanisms, and ensuring parameter consistency to improve tool-agent reliability.

Abstract: The emergence of the tool agent paradigm has broadened the capability
boundaries of the Large Language Model (LLM), enabling it to complete more
complex tasks. However, the effectiveness of this paradigm is limited due to
the issue of parameter failure during its execution. To explore this phenomenon
and propose corresponding suggestions, we first construct a parameter failure
taxonomy in this paper. We derive five failure categories from the invocation
chain of a mainstream tool agent. Then, we explore the correlation between
three different input sources and failure categories by applying 15 input
perturbation methods to the input. Experimental results show that parameter
name hallucination failure primarily stems from inherent LLM limitations, while
issues with input sources mainly cause other failure patterns. To improve the
reliability and effectiveness of tool-agent interactions, we propose
corresponding improvement suggestions, including standardizing tool return
formats, improving error feedback mechanisms, and ensuring parameter
consistency.

</details>


### [28] [StackTrans: From Large Language Model to Large Pushdown Automata Model](https://arxiv.org/abs/2507.15343)
*Kechi Zhang,Ge Li,Jia Li,Huangzhao Zhang,Yihong Dong,Jia Li,Jingjing Xu,Zhi Jin*

Main category: cs.SE

TL;DR: StackTrans is a stack-augmented Transformer that overcomes standard models' limitations with formal language structures. It outperforms larger LLMs in reasoning tasks, demonstrating enhanced efficiency and capability.


<details>
  <summary>Details</summary>
Motivation: The Transformer architecture, despite powering advancements in large language models, struggles to effectively represent formal languages characterized by the Chomsky hierarchy (e.g., regular expressions, deterministic context-free grammars). This bottleneck constrains the reasoning and syntactic capabilities of LLMs.

Method: The authors introduce StackTrans, a Transformer variant that incorporates differentiable hidden state stacks between layers, inspired by pushdown automata. These stack operations (push, pop) are fully differentiable, allowing StackTrans to learn stack manipulation in an end-to-end fashion. Importantly, this method is designed to be compatible with existing efficient attention mechanisms such as flash-attention.

Result: StackTrans was evaluated on benchmarks for formal languages and large-scale natural language tasks. It consistently outperformed standard Transformers and other baseline architectures. StackTrans models were scaled up to 7B parameters, and the StackTrans-360M model, trained from scratch, surpassed open-source LLMs with much larger parameter counts on relevant tasks.

Conclusion: Incorporating stack-based memory into Transformers enables them to effectively capture hierarchical language structures, which standard Transformers struggle with. StackTrans improves reasoning abilities and efficiency, opening new possibilities for language model design.

Abstract: The Transformer architecture has emerged as a landmark advancement within the
broad field of artificial intelligence, effectively catalyzing the advent of
large language models (LLMs). However, despite its remarkable capabilities and
the substantial progress it has facilitated, the Transformer architecture still
has some limitations. One such intrinsic limitation is its inability to
effectively capture the Chomsky hierarchy, such as regular expressions or
deterministic context-free grammars. Drawing inspiration from pushdown
automata, which efficiently resolve deterministic context-free grammars using
stacks, we propose StackTrans to address the aforementioned issue within LLMs.
Unlike previous approaches that modify the attention computation, StackTrans
explicitly incorporates hidden state stacks between Transformer layers. This
design maintains compatibility with existing frameworks like flash-attention.
Specifically, our design features stack operations -- such as pushing and
popping hidden states -- that are differentiable and can be learned in an
end-to-end manner. Our comprehensive evaluation spans benchmarks for both
Chomsky hierarchies and large-scale natural languages. Across these diverse
tasks, StackTrans consistently outperforms standard Transformer models and
other baselines. We have successfully scaled StackTrans up from 360M to 7B
parameters. In particular, our from-scratch pretrained model StackTrans-360M
outperforms several larger open-source LLMs with 2-3x more parameters,
showcasing its superior efficiency and reasoning capability.

</details>


### [29] [Applying the Chinese Wall Reverse Engineering Technique to Large Language Model Code Editing](https://arxiv.org/abs/2507.15599)
*Manatsawin Hanmongkolchai*

Main category: cs.SE

TL;DR: Using a strong code model to guide a weaker, ethical model with instructions boosts task performance, but is constrained by the lack of public domain data for training.


<details>
  <summary>Details</summary>
Motivation: Large language models for code are powerful but raise copyright concerns due to undisclosed training datasets; ethically aligned models with curated data are less competitive, creating a need for utility-improving methods.

Method: They propose the 'Chinese Wall' technique, where a high-quality model produces detailed instructions for a weaker, ethically aligned model, enabling the latter to perform complex programming tasks.

Result: This technique improves Comma v0.1 1T's benchmark performance by over 66% and Starcoder2 Instruct by about 20% compared to when those models run alone.

Conclusion: The 'Chinese Wall' method can significantly boost the utility of ethically aligned code LLMs, but its practical impact is currently limited by the scarcity of copyright-clear training data.

Abstract: Large language models for code (Code LLM) are increasingly utilized in
programming environments. Despite their utility, the training datasets for top
LLM remain undisclosed, raising concerns about potential copyright violations.
Some models, such as Pleias and Comma put emphasis on data curation and
licenses, however, with limited training data these models are not competitive
and only serve as proof of concepts. To improve the utility of these models, we
propose an application of the "Chinese Wall" technique, inspired by the reverse
engineering technique of the same name -- a high quality model is used to
generate detailed instructions for a weaker model. By doing so, a weaker but
ethically aligned model may be used to perform complicated tasks that,
otherwise, can only be completed by more powerful models. In our evaluation,
we've found that this technique improves Comma v0.1 1T's performance in
CanItEdit benchmark by over 66%, and Starcoder2 Instruct by roughly 20%
compared to when running the same model on the benchmark alone. The practical
application of this technique today, however, may be limited due to the lack of
models trained on public domain content without copyright restrictions.

</details>


### [30] [Hot Topics and Common Challenges: an Empirical Study of React Discussions on Stack Overflow](https://arxiv.org/abs/2507.15624)
*Yusuf Sulistyo Nugroho,Ganno Tribuana Kurniaji,Syful Islam,Mohammed Humayun Kabir,Vanesya Aura Ardity,Md. Kamal Uddin*

Main category: cs.SE

TL;DR: This paper analyzes Stack Overflow posts to uncover the main problems React users face, finding that algorithmic errors are the most frequent, particularly among mid-reputation users. The authors suggest the community should offer more support around algorithmic issues to help new adopters.


<details>
  <summary>Details</summary>
Motivation: While React is popular for building user interfaces in single-page applications, the specific user challenges associated with React are not well-documented. The paper aims to fill this gap by analyzing React-related questions on Stack Overflow.

Method: The study uses exploratory data analysis to examine Stack Overflow questions tagged with React. It analyzes frequently mentioned keywords, classifies the types of errors discussed, and examines the distribution of these errors based on user reputation.

Result: Algorithmic errors are the most commonly discussed issue across all user groups. Mid-reputation users contribute the majority of these error reports (55.77%). The analysis also identifies the eight most common keywords related to React questions: code, link, vir, href, connect, azure, windows, and website.

Conclusion: Error classification reveals algorithmic errors as the most prevalent challenge for React users, with mid-reputation users particularly affected. The study suggests that more resources and guidance on algorithmic problem-solving would benefit the community. The findings provide insight for future research and resource allocation to better support React developers.

Abstract: React is a JavaScript library used to build user interfaces for single-page
applications. Although recent studies have shown the popularity and advantages
of React in web development, the specific challenges users face remain unknown.
Thus, this study aims to analyse the React-related questions shared on Stack
Overflow. The study utilizes an exploratory data analysis to investigate the
most frequently discussed keywords, error classification, and user
reputation-based errors, which is the novelty of this work. The results show
the top eight most frequently used keywords on React-related questions, namely,
code, link, vir, href, connect, azure, windows, and website. The error
classification of questions from the sample shows that algorithmic error is the
most frequent issue faced by all groups of users, where mid-reputation users
contribute the most, accounting for 55.77%. This suggests the need for the
community to provide guidance materials in solving algorithm-related problems.
We expect that the results of this study will provide valuable insight into
future research to support the React community during the early stages of
implementation, facilitating their ability to effectively overcome challenges
to adoption.

</details>


### [31] [SustainDiffusion: Optimising the Social and Environmental Sustainability of Stable Diffusion Models](https://arxiv.org/abs/2507.15663)
*Giordano d'Aloisio,Tosin Fadahunsi,Jay Choy,Rebecca Moussa,Federica Sarro*

Main category: cs.SE

TL;DR: SustainDiffusion makes text-to-image models less biased and more energy-efficient without changing the model's architecture, achieving this by optimizing prompts and hyperparameters.


<details>
  <summary>Details</summary>
Motivation: Text-to-image generation models like Stable Diffusion are widely used, but there are growing concerns about their social (e.g., biases) and environmental (e.g., energy use) impacts. The motivation is to mitigate these impacts without sacrificing the quality of generated images.

Method: The paper introduces SustainDiffusion, a search-based method that optimizes hyperparameters and prompt structures for Stable Diffusion. The aim is to reduce gender and ethnic bias in generated images and decrease energy consumption, all while preserving image quality. The method does not require fine-tuning or changing the model's architecture.

Result: SustainDiffusion significantly reduces gender bias by 68%, ethnic bias by 59%, and energy consumption by 48% when compared to six baseline methods using 56 test prompts. The improvements hold across multiple runs and different prompts.

Conclusion: It is possible to enhance the social and environmental sustainability of text-to-image models like Stable Diffusion through careful optimization, without modifying the model itself.

Abstract: Background: Text-to-image generation models are widely used across numerous
domains. Among these models, Stable Diffusion (SD) - an open-source
text-to-image generation model - has become the most popular, producing over 12
billion images annually. However, the widespread use of these models raises
concerns regarding their social and environmental sustainability.
  Aims: To reduce the harm that SD models may have on society and the
environment, we introduce SustainDiffusion, a search-based approach designed to
enhance the social and environmental sustainability of SD models.
  Method: SustainDiffusion searches the optimal combination of hyperparameters
and prompt structures that can reduce gender and ethnic bias in generated
images while also lowering the energy consumption required for image
generation. Importantly, SustainDiffusion maintains image quality comparable to
that of the original SD model.
  Results: We conduct a comprehensive empirical evaluation of SustainDiffusion,
testing it against six different baselines using 56 different prompts. Our
results demonstrate that SustainDiffusion can reduce gender bias in SD3 by 68%,
ethnic bias by 59%, and energy consumption (calculated as the sum of CPU and
GPU energy) by 48%. Additionally, the outcomes produced by SustainDiffusion are
consistent across multiple runs and can be generalised to various prompts.
  Conclusions: With SustainDiffusion, we demonstrate how enhancing the social
and environmental sustainability of text-to-image generation models is possible
without fine-tuning or changing the model's architecture.

</details>


### [32] [Modeling CubeSat Storage Battery Discharge: Equivalent Circuit Versus Machine Learning Approaches](https://arxiv.org/abs/2507.15666)
*Igor Turkin,Lina Volobuieva,Andriy Chukhray,Oleksandr Liubimov*

Main category: cs.SE

TL;DR: The paper compares analytical (equivalent circuit) and machine learning approaches for modeling CubeSat battery discharge. While the analytical model is transparent and interpretable, the machine learning model offers superior accuracy and adaptability, making it preferable for practical satellite operations.


<details>
  <summary>Details</summary>
Motivation: The motivation for this study is to enhance the prediction of battery discharge in CubeSat satellites, which is crucial for ensuring the fault tolerance and operational reliability of satellite equipment in orbit. The research seeks to inform the choice between analytical and machine learning modeling approaches.

Method: The study analyzes CubeSat orbital power system data, including battery and solar panel voltage, current, and temperature, using two modeling approaches: (1) analytical modeling with equivalent circuit models based on physical laws, and (2) machine learning models trained on empirical data to develop predictive capabilities.

Result: The comparative analysis shows that the equivalent circuit (analytical) model is transparent and interpretable, allowing for better understanding of parameter relationships, but is less flexible to changing conditions. The machine learning model, on the other hand, provides higher accuracy and adapts better to complex or non-standard behaviors seen in real-world satellite data.

Conclusion: Machine learning models are more suitable for accurate and adaptive prediction of CubeSat battery discharge compared to analytical equivalent circuit models, especially under complex or non-standard operational conditions.

Abstract: The subject of the article is the study and comparison of two approaches to
modelling the battery discharge of a CubeSat satellite: analytical using
equivalent circuit and machine learning. The article aims to make a reasoned
choice of the approach to modelling the battery discharge of a CubeSat
satellite. Modelling the battery discharge of a satellite will enable the
prediction of the consequences of disconnecting the autonomous power system and
ensure the fault tolerance of equipment in orbit. Therefore, the selected study
is relevant and promising. This study focuses on the analysis of CubeSat
satellite data, based explicitly on orbital data samples of the power system,
which include data available at the time of the article publication. The
dataset contains data on the voltage, current, and temperature of the battery
and solar panels attached to the five sides of the satellite. In this context,
two approaches are considered: analytical modelling based on physical laws and
machine learning, which uses empirical data to create a predictive model.
Results: A comparative analysis of the modeling results reveals that the
equivalent circuit approach has the advantage of transparency, as it identifies
possible parameters that facilitate understanding of the relationships.
However, the model is less flexible to environmental changes or non-standard
satellite behavior. The machine learning model demonstrated more accurate
results, as it can account for complex dependencies and adapt to actual
conditions, even when they deviate from theoretical assumptions.

</details>


### [33] [BugScope: Learn to Find Bugs Like Human](https://arxiv.org/abs/2507.15671)
*Jinyao Guo,Chengpeng Wang,Dominic Deluca,Jinjie Liu,Zhuo Zhang,Xiangyu Zhang*

Main category: cs.SE

TL;DR: BugScope is a novel LLM-based system that learns from examples to detect software bugs more effectively than traditional and current LLM-based tools, as demonstrated by strong results on both benchmark datasets and real-world open-source projects.


<details>
  <summary>Details</summary>
Motivation: Software bug detection is challenging due to the variety and complexity of real-world defects. Traditional static analysis tools have limited coverage and adaptability, while LLM-based methods still struggle with complex bugs and limited analysis context.

Method: The paper presents BugScope, a multi-agent system driven by large language models. BugScope learns new bug patterns from examples, uses program slicing to synthesize detection contexts, and constructs tailored prompts to guide LLMs for accurate reasoning in bug detection tasks.

Result: BugScope was evaluated on a dataset of 40 real-world bugs from 21 open-source projects, achieving 87.04% precision and 90.00% recall, outperforming industrial tools by 0.44 F1 score. It also found 141 new bugs in large projects (e.g., Linux kernel); 78 were fixed and 7 confirmed by developers.

Conclusion: BugScope significantly improves bug detection capabilities by leveraging example-based learning and context-sensitive LLM prompting, achieving superior performance and real-world impact compared to existing tools.

Abstract: Detecting software bugs remains a fundamental challenge due to the extensive
diversity of real-world defects. Traditional static analysis tools often rely
on symbolic workflows, which restrict their coverage and hinder adaptability to
customized bugs with diverse anti-patterns. While recent advances incorporate
large language models (LLMs) to enhance bug detection, these methods continue
to struggle with sophisticated bugs and typically operate within limited
analysis contexts. To address these challenges, we propose BugScope, an
LLM-driven multi-agent system that emulates how human auditors learn new bug
patterns from representative examples and apply that knowledge during code
auditing. Given a set of examples illustrating both buggy and non-buggy
behaviors, BugScope synthesizes a retrieval strategy to extract relevant
detection contexts via program slicing and then constructs a tailored detection
prompt to guide accurate reasoning by the LLM. Our evaluation on a curated
dataset of 40 real-world bugs drawn from 21 widely-used open-source projects
demonstrates that BugScope achieves 87.04% precision and 90.00% recall,
surpassing state-of-the-art industrial tools by 0.44 in F1 score. Further
testing on large-scale open-source systems, including the Linux kernel,
uncovered 141 previously unknown bugs, of which 78 have been fixed and 7
confirmed by developers, highlighting BugScope's substantial practical impact.

</details>


### [34] [Do AI models help produce verified bug fixes?](https://arxiv.org/abs/2507.15822)
*Li Huang,Ilgiz Mustafin,Marco Piccioni,Alessandro Schena,Reto Weber,Bertrand Meyer*

Main category: cs.SE

TL;DR: A controlled study using formal verification shows that while LLMs offer distinct advantages in automatic program repair, they also have unexpected limitations. The paper presents a reusable experimental method, analyzes usage patterns, and gives validated recommendations for effective use of LLMs in debugging.


<details>
  <summary>Details</summary>
Motivation: The motivation is to investigate whether Large Language Models (LLMs) can significantly improve Automatic Program Repair (APR), and to determine how programmers actually utilize LLMs when debugging, ensuring that proposed code corrections are truly effective.

Method: The study used a program-proving environment that formally verifies the correctness of code fixes. Two groups of programmers were randomly assigned: one with access to LLMs for assistance, and one without. Both groups validated their debugging solutions using proof tools. The experiment followed the Goal-Query-Metric (GQM) methodology to structure research questions, specific queries, and measurable metrics. Programmer behaviors were analyzed through full-session recordings.

Result: Results indicate surprising findings regarding the practical role and effectiveness of LLMs in debugging and APR, contrasting with commonly held expectations. The study also produced: a reusable experimental methodology, a detailed fine-grain analysis of programmer behavior, a taxonomy of seven distinct usage patterns of LLMs during debugging, and actionable advice for effective use of LLMs in debugging tasks.

Conclusion: This work takes a first, rigorous step toward defining the practical and optimal role of AI/LLMs in APR, identifying both strengths and unexpected limitations, and provides methodological and actionable insights for the research and practitioner community.

Abstract: Among areas of software engineering where AI techniques -- particularly,
Large Language Models -- seem poised to yield dramatic improvements, an
attractive candidate is Automatic Program Repair (APR), the production of
satisfactory corrections to software bugs. Does this expectation materialize in
practice? How do we find out, making sure that proposed corrections actually
work? If programmers have access to LLMs, how do they actually use them to
complement their own skills?
  To answer these questions, we took advantage of the availability of a
program-proving environment, which formally determines the correctness of
proposed fixes, to conduct a study of program debugging with two randomly
assigned groups of programmers, one with access to LLMs and the other without,
both validating their answers through the proof tools. The methodology relied
on a division into general research questions (Goals in the Goal-Query-Metric
approach), specific elements admitting specific answers (Queries), and
measurements supporting these answers (Metrics). While applied so far to a
limited sample size, the results are a first step towards delineating a proper
role for AI and LLMs in providing guaranteed-correct fixes to program bugs.
  These results caused surprise as compared to what one might expect from the
use of AI for debugging and APR. The contributions also include: a detailed
methodology for experiments in the use of LLMs for debugging, which other
projects can reuse; a fine-grain analysis of programmer behavior, made possible
by the use of full-session recording; a definition of patterns of use of LLMs,
with 7 distinct categories; and validated advice for getting the best of LLMs
for debugging and Automatic Program Repair.

</details>


### [35] [Investigating the Use of LLMs for Evidence Briefings Generation in Software Engineering](https://arxiv.org/abs/2507.15828)
*Mauro Marcelino,Marcos Alves,Bianca Trinkenreich,Bruno Cartaxo,Sérgio Soares,Simone D. J. Barbosa,Marcos Kalinowski*

Main category: cs.SE

TL;DR: This paper proposes an experiment to test whether evidence briefings generated by a RAG-based LLM are as accurate, understandable, and useful as those made by humans. Results and conclusions will be shared after the experimentation phase.


<details>
  <summary>Details</summary>
Motivation: Evidence briefings are helpful tools for communicating research findings to software engineers, but their manual creation poses a bottleneck for widespread use. To overcome this, automating the production process may increase adoption in the software engineering community.

Method: The authors developed a Retrieval-Augmented Generation (RAG) based LLM tool to generate evidence briefings. They then used this tool to automatically generate versions of two evidence briefings previously created by humans. The study is a controlled experiment that evaluates LLM-generated versus human-made briefings on three metrics: content fidelity, ease of understanding, and perceived usefulness, as rated by researchers and practitioners.

Result: Results have not yet been reported, as the paper describes a registered report protocol and the experiments are pending.

Conclusion: Final conclusions will depend on the outcome of the experimental trials, which are not yet completed at this stage.

Abstract: [Context] An evidence briefing is a concise and objective transfer medium
that can present the main findings of a study to software engineers in the
industry. Although practitioners and researchers have deemed Evidence Briefings
useful, their production requires manual labor, which may be a significant
challenge to their broad adoption. [Goal] The goal of this registered report is
to describe an experimental protocol for evaluating LLM-generated evidence
briefings for secondary studies in terms of content fidelity, ease of
understanding, and usefulness, as perceived by researchers and practitioners,
compared to human-made briefings. [Method] We developed an RAG-based LLM tool
to generate evidence briefings. We used the tool to automatically generate two
evidence briefings that had been manually generated in previous research
efforts. We designed a controlled experiment to evaluate how the LLM-generated
briefings compare to the human-made ones regarding perceived content fidelity,
ease of understanding, and usefulness. [Results] To be reported after the
experimental trials. [Conclusion] Depending on the experiment results.

</details>


### [36] [Observing Fine-Grained Changes in Jupyter Notebooks During Development Time](https://arxiv.org/abs/2507.15831)
*Sergey Titov,Konstantin Grotov,Cristina Sarasua,Yaroslav Golubev,Dhivyabharathi Ramasamy,Alberto Bacchelli,Abraham Bernstein,Timofey Bryksin*

Main category: cs.SE

TL;DR: The paper introduces tools and datasets for analyzing how Jupyter notebooks are developed in data science, revealing that much notebook activity involves small fixes and iterative changes. This positions notebooks as key debugging environments, and the results pave the way for further studies in this area.


<details>
  <summary>Details</summary>
Motivation: Although software engineering has benefited from fine-grained log analysis, similar approaches have not been applied to computational notebooks—which are increasingly used in data science. There is a gap in understanding how these notebooks are developed and changed over time.

Method: The authors introduced a toolset to collect granular logs of code changes in Jupyter notebooks. They collected data from more than 100 hours of notebook work, involving 20 developers performing data analysis and machine learning tasks. The resulting dataset included code changes and cell executions, which were systematically analyzed to classify the types of changes made during development.

Result: Analysis revealed that many changes between notebook cell executions were minor corrections and code refinements. This indicates that computational notebooks serve not only exploratory and development purposes but are also actively used for debugging. Additional insights and suggestions for future research are also presented based on the data.

Conclusion: This study provides the first fine-grained log analysis of computational notebooks in data science, offering a new dataset and toolset for understanding how developers work with these tools. It highlights the frequent, iterative nature of code changes and positions notebooks as valuable debugging tools, opening avenues for future research.

Abstract: In software engineering, numerous studies have focused on the analysis of
fine-grained logs, leading to significant innovations in areas such as
refactoring, security, and code completion. However, no similar studies have
been conducted for computational notebooks in the context of data science.
  To help bridge this research gap, we make three scientific contributions: we
(1) introduce a toolset for collecting code changes in Jupyter notebooks during
development time; (2) use it to collect more than 100 hours of work related to
a data analysis task and a machine learning task (carried out by 20 developers
with different levels of expertise), resulting in a dataset containing 2,655
cells and 9,207 cell executions; and (3) use this dataset to investigate the
dynamic nature of the notebook development process and the changes that take
place in the notebooks.
  In our analysis of the collected data, we classified the changes made to the
cells between executions and found that a significant number of these changes
were relatively small fixes and code iteration modifications. This suggests
that notebooks are used not only as a development and exploration tool but also
as a debugging tool. We report a number of other insights and propose potential
future research directions on the novel data.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [37] [NPUEval: Optimizing NPU Kernels with LLMs and Open Source Compilers](https://arxiv.org/abs/2507.14403)
*Sarunas Kalade,Graham Schelle*

Main category: cs.PL

TL;DR: The paper introduces NPUEval, a new open-source benchmark for evaluating LLM-generated NPU kernels, showing current models struggle with the task, which highlights opportunities and challenges for future research in AI code generation for specialized hardware.


<details>
  <summary>Details</summary>
Motivation: Efficiently running AI workloads on neural processing units (NPUs) in power-sensitive devices, like AI PCs, requires highly optimized kernel libraries. However, NPU programming is new and fragmented, lacking the rich ecosystem and resources found in GPU programming, making it especially challenging for large language models (LLMs) to assist with code generation.

Method: The paper introduces NPUEval, a benchmark that comprises 102 common machine learning operators. The authors evaluate LLM-generated NPU kernel code on real hardware for functional correctness and vectorization efficiency. They leverage open-source compiler tools for the AMD NPU and test various state-of-the-art LLMs, including both proprietary and open-weight models.

Result: Some advanced LLMs, such as DeepSeek R1, are able to achieve over 50% vectorization out-of-the-box on select kernels. However, the average efficiency score across all benchmarks is about 10%, even when feedback and optimized examples are provided, indicating the difficulty of the task for current LLMs.

Conclusion: NPUEval is presented as a crucial open-source benchmark for evaluating and advancing LLM-assisted code generation and NPU kernel optimization. Current LLMs show only modest performance, underscoring the challenges and the need for further research.

Abstract: Neural processing units (NPUs) are gaining prominence in power-sensitive
devices like client devices, with AI PCs being defined by their inclusion of
these specialized processors. Running AI workloads efficiently on these devices
requires libraries of optimized kernels. Creating efficient kernels demands
expertise in domain-specific C++ with vector intrinsics and in-depth knowledge
of the target architecture. Unlike GPU programming, which has had years to
mature, NPU programming is new, with smaller and more fragmented developer
communities across hardware platforms. This fragmentation poses a challenge
when utilizing LLMs to assist in writing NPU kernels, as domain-specific
optimized code examples are underrepresented in LLM pre-training data.
  In this paper we introduce NPUEval -- a benchmark for writing and evaluating
NPU kernels, consisting of 102 common operators for machine learning workloads.
We evaluate LLM generated code on actual hardware based on both functional
correctness and vectorization efficiency using open source compiler tools
targeting the AMD NPU. We evaluate a range of state-of-the-art LLMs with a mix
of proprietary and open-weight models. Latest reasoning models like DeepSeek
R1, show promising results achieving out-of-the-box 50%+ vectorization on
select kernels. However, the average score across the entire dataset remains
roughly 10% even with compiler feedback and vectorized kernel examples --
showing that this is a challenging dataset even for frontier models. The
dataset and evaluation code will be released with a permissive open source
license, providing an essential benchmark for advancing research in code
generation and NPU kernel optimization.

</details>


### [38] [Timetide: A programming model for logically synchronous distributed systems](https://arxiv.org/abs/2507.14471)
*Logan Kenwright,Partha Roop,Nathan Allen,Călin Caşcaval,Avinash Malik*

Main category: cs.PL

TL;DR: Deterministic distributed programming is hard due to physical clock synchronization, but Timetide introduces a multiclock, logically synchronous model that is distributable and formally verifiable without expensive hardware clocks.


<details>
  <summary>Details</summary>
Motivation: Deterministic programming in distributed systems is challenging due to reliance on expensive and non-scalable physical clock synchronization in existing time-triggered languages. Traditional synchronous languages typically target centralized applications.

Method: The authors develop a novel multiclock semantics for synchronous programming, leading to the creation of their new programming model, Timetide. This model builds on logical synchrony instead of physical clock synchrony, and addresses aspects of distributed computation such as network communication delays and formal verification.

Result: Timetide enables the deterministic distribution of synchronous programs without requiring physical clock synchronization or clock gating, and supports formal verification. It is also scalable and suitable for distributed applications.

Conclusion: Timetide represents a significant advance as the first multiclock synchronous language that achieves both distributability and formal verification capabilities without the typical requirement for expensive physical clock synchronization. The approach makes deterministic distributed programming more practical and scalable.

Abstract: Massive strides in deterministic models have been made using synchronous
languages. They are mainly focused on centralised applications, as the
traditional approach is to compile away the concurrency. Time triggered
languages such as Giotto and Lingua Franca are suitable for distribution albeit
that they rely on expensive physical clock synchronisation, which is both
expensive and may suffer from scalability. Hence, deterministic programming of
distributed systems remains challenging. We address the challenges of
deterministic distribution by developing a novel multiclock semantics of
synchronous programs. The developed semantics is amenable to seamless
distribution. Moreover, our programming model, Timetide, alleviates the need
for physical clock synchronisation by building on the recently proposed logical
synchrony model for distributed systems. We discuss the important aspects of
distributing computation, such as network communication delays, and explore the
formal verification of Timetide programs. To the best of our knowledge,
Timetide is the first multiclock synchronous language that is both amenable to
distribution and formal verification without the need for physical clock
synchronisation or clock gating.

</details>


### [39] [Hear Your Code Fail, Voice-Assisted Debugging for Python](https://arxiv.org/abs/2507.15007)
*Sayed Mahbub Hasan Amiri,Md. Mainul Islam,Mohammad Shakhawat Hossen,Sayed Majhab Hasan Amiri,Mohammad Shawkat Ali Mamun,Sk. Humaun Kabir,Naznin Akter*

Main category: cs.PL

TL;DR: This paper presents a Python plugin that turns runtime errors into both spoken and visual feedback, significantly lowering cognitive load and speeding up debugging—especially helping the visually impaired and beginners. It works quickly and efficiently across major platforms, with plans for even smarter, multilingual features.


<details>
  <summary>Details</summary>
Motivation: Debugging silent runtime errors in Python often relies on visual stack traces, which are challenging for individuals with visual impairments and can create high cognitive load for all users. There is a need to make debugging more accessible, efficient, and multimodal to improve cognitive efficiency and support diverse users, especially those who multitask or are learning programming.

Method: The research implements a voice-assisted debugging plugin by combining a global exception hook in Python with pyttsx3 for text-to-speech, and a Tkinter-based graphical user interface. The system vocalizes errors and offers visual tracebacks with interactive documentation. It is evaluated empirically with 50 participants for cognitive load, identification speed, and system performance metrics (voice latency, CPU overhead).

Result: Empirical results show a 37% reduction in cognitive load and 78% faster error identification compared to traditional stack-trace debugging. The plugin operates with sub-1.2 second voice latency and less than 18% CPU overhead. Pilot educational studies indicate 45% faster debugging skill acquisition for novices. The tool is validated for compatibility across major operating systems and Python 3.7+.

Conclusion: The system fundamentally improves error diagnostics in Python by introducing efficient, accessible multimodal feedback. It benefits visually impaired users and multitaskers, with notable educational advantages for beginners. Future enhancements will add GPT-based repair suggestions and multilingual auditory output. The work sets a new standard for cognitive efficiency in debugging and bridges important accessibility gaps.

Abstract: This research introduces an innovative voice-assisted debugging plugin for
Python that transforms silent runtime errors into actionable audible
diagnostics. By implementing a global exception hook architecture with pyttsx3
text-to-speech conversion and Tkinter-based GUI visualization, the solution
delivers multimodal error feedback through parallel auditory and visual
channels. Empirical evaluation demonstrates 37% reduced cognitive load (p<0.01,
n=50) compared to traditional stack-trace debugging, while enabling 78% faster
error identification through vocalized exception classification and
contextualization. The system achieves sub-1.2 second voice latency with under
18% CPU overhead during exception handling, vocalizing error types and
consequences while displaying interactive tracebacks with documentation deep
links. Criteria validate compatibility across Python 3.7+ environments on
Windows, macOS, and Linux platforms. Needing only two lines of integration
code, the plugin significantly boosts availability for aesthetically impaired
designers and supports multitasking workflows through hands-free error medical
diagnosis. Educational applications show particular promise, with pilot studies
indicating 45% faster debugging skill acquisition among novice programmers.
Future development will incorporate GPT-based repair suggestions and real-time
multilingual translation to further advance auditory debugging paradigms. The
solution represents a fundamental shift toward human-centric error diagnostics,
bridging critical gaps in programming accessibility while establishing new
standards for cognitive efficiency in software development workflows.

</details>


### [40] [Invariant Generation for Floating-Point Programs via Constraint Solving](https://arxiv.org/abs/2507.15017)
*Xuran Cai,Liqian Chen,Hongfei Fu*

Main category: cs.PL

TL;DR: This paper presents a new constraint-based framework and algorithms for generating precise invariants in floating-point programs, achieving better accuracy and speed than existing methods.


<details>
  <summary>Details</summary>
Motivation: Floating-point arithmetic introduces small rounding errors, which can accumulate and cause significant problems in numeric-intensive computations. Ensuring correctness in floating-point programs requires careful analysis of these errors, particularly when generating program invariants.

Method: The paper develops a theoretical framework that combines the first-order differential characterization technique from FPTaylor with constraint solving methods to efficiently generate tight invariants for floating-point programs. Two polynomial invariant generation algorithms are introduced: one requiring an initial coarse invariant (but broadly applicable) and one independent of such input (but limited to polynomial programs). Conditional branches are also handled within this framework.

Result: The proposed algorithms outperform state-of-the-art methods in both precision and time efficiency for invariant generation on a range of benchmarks.

Conclusion: Combining differential characterization and constraint solving enables more efficient and precise invariant generation for floating-point programs, providing a practical tool to address floating-point errors and improving upon current techniques.

Abstract: In numeric-intensive computations, it is well known that the execution of
floating-point programs is imprecise as floating point arithmetics (e.g.,
addition, subtraction, multiplication, division, etc.) incurs rounding errors.
Albeit the rounding error is small for every single floating-point operation,
the aggregation of such error in multiple operations may be dramatic and cause
catastrophic program failures. Therefore, to ensure the correctness of
floating-point programs, the effect of floating point error needs to be
carefully taken into account. In this work, we consider the invariant
generation for floating point programs, whose aim is to generate tight
invariants under the perturbation of floating point errors. Our main
contribution is a theoretical framework on how to apply constraint solving
methods to address the invariant generation problem. In our framework, we
propose a novel combination between the first-order differential
characterization by FPTaylor (TOPLAS 2018) and constraint solving methods,
aiming to reduce the computational burden of constraint solving. Moreover, we
devise two polynomial invariant generation algorithms to instantiate the
framework. The first algorithm is applicable to a wide range of floating-point
operations but requires an initial (coarse) invariant as external input, while
the second does not require an initial invariant but is limited to polynomial
programs. Furthermore, we show how conditional branches, a difficult issue in
floating-point analysis, can be handled in our framework. Experimental results
show that our algorithms outperform SOTA approaches in both the time efficiency
and the precision of the generated invariants over a variety of benchmarks.

</details>


### [41] [A Few Fit Most: Improving Performance Portability of SGEMM on GPUs using Multi-Versioning](https://arxiv.org/abs/2507.15277)
*Robert Hochgraf,Sreepathi Pai*

Main category: cs.PL

TL;DR: Manual optimization for GPUs is tedious, and autotuning isn't flexible to changes. This paper introduces a framework that automatically generates multiple code versions for better performance portability. It beats standard methods in speed and generalizes well to new hardware, without repeatedly retuning.


<details>
  <summary>Details</summary>
Motivation: Hand-optimizing linear algebra kernels for various GPU devices is complex and labor-intensive. While autotuning is commonly used for automatic performance optimization, it tends to overfit and requires re-tuning whenever the environment changes (e.g., device, input).

Method: The authors propose using multi-versioning, which generates several variants of the same code, to achieve performance portability. They introduce a framework called 'portability tuning' that automatically generates multi-versioned code, eliminating the need for retuning.

Result: The framework was evaluated on execution times for GEMM kernels from the CLBlast linear algebra library. The portability tuning technique outperformed CLBlast's default kernels, often approaching within 10% of theoretical maximum performance. The generated programs generalized effectively to unseen devices, performing comparably to autotuning without device-specific retuning.

Conclusion: Portability tuning, through automatic code multi-versioning, enables high and portable performance for linear algebra kernels across diverse GPU devices and execution environments, without repeated expensive autotuning.

Abstract: Hand-optimizing linear algebra kernels for different GPU devices and
applications is complex and labor-intensive. Instead, many developers use
automatic performance tuning (autotuning) to achieve high performance on a
variety of devices. However, autotuning "overfits", and must be redone if any
part of the environment changes, such as if the device or input characteristics
change.
  In most non-trivial cases, a single compute kernel cannot maintain
near-optimal performance across all environments. Changing the kernel to
specialize it to the current execution environment is possible, but on GPUs,
runtime tuning and compilation can be expensive.
  In this work, we use multi-versioning -- producing several variants of the
same code -- as a way to generate performance portable code. We describe a
framework called portability tuning that can automatically generate
multi-versioned code whose performance is portable, requiring no retuning.
  We evaluate our framework on a dataset of execution times for GEMM kernels
from the CLBlast linear algebra library. We find our portability tuning
techniques outperform CLBlast's default kernels -- often approaching within 10%
of the theoretical maximum performance -- despite CLBlast using autotuning
techniques. Further, we find that our generated programs generalize well to new
and unseen devices, matching the performance of autotuning without ever
portability tuning for those devices.

</details>


### [42] [Bayesian Separation Logic](https://arxiv.org/abs/2507.15530)
*Shing Hin Ho,Nicolas Wu,Azalea Raad*

Main category: cs.PL

TL;DR: The paper introduces a new probabilistic separation logic, BaSL, that extends reasoning to capture Bayesian updating in Bayesian probabilistic programming languages. BaSL is mathematically rigorous and enables formal reasoning about key properties and constructs in such languages, filling a major gap in current semantics and logic tools.


<details>
  <summary>Details</summary>
Motivation: Bayesian probabilistic programming languages (BPPLs) allow modeling uncertainty via code, but existing logics cannot reason about Bayesian updating—the core feature of BPPLs. There is a need for a formal logic that can reason about expected values, independence, and Bayesian updating within these languages.

Method: The authors introduce Bayesian separation logic (BaSL), which extends probabilistic separation logic to handle Bayesian updating. BaSL is built on Kripke resource monoids instantiated with σ-finite measure spaces over the Hilbert cube, and it is semantically compatible with denotational semantics of BPPLs using s-finite kernels. The approach leverages the Rokhlin-Simmons disintegration theorem from measure theory to prove an internal version of Bayes' theorem.

Result: BaSL can model key Bayesian probabilistic programming concepts like Bayesian updating, unnormalised and conditional distributions, soft and conjugate priors, and more. The semantics maintain modularity. Using BaSL, the paper proves properties such as expected values, variable correlations in network models, and computes posterior distributions for various statistical models.

Conclusion: BaSL bridges the gap between existing probabilistic logics and Bayesian updating, providing the foundation to reason formally about BPPLs while supporting modular proofs and crucial probabilistic programming constructs.

Abstract: Bayesian probabilistic programming languages (BPPLs) let users denote
statistical models as code while the interpreter infers the posterior
distribution. The semantics of BPPLs are usually mathematically complex and
unable to reason about desirable properties such as expected values and
independence of random variables. To reason about these properties in a
non-Bayesian setting, probabilistic separation logics such as PSL and Lilac
interpret separating conjunction as probabilistic independence of random
variables. However, no existing separation logic can handle Bayesian updating,
which is the key distinguishing feature of BPPLs.
  To close this gap, we introduce Bayesian separation logic (BaSL), a
probabilistic separation logic that gives semantics to BPPL. We prove an
internal version of Bayes' theorem using a result in measure theory known as
the Rokhlin-Simmons disintegration theorem. Consequently, BaSL can model
probabilistic programming concepts such as Bayesian updating, unnormalised
distribution, conditional distribution, soft constraint, conjugate prior and
improper prior while maintaining modularity via the frame rule. The model of
BaSL is based on a novel instantiation of Kripke resource monoid via
$\sigma$-finite measure spaces over the Hilbert cube, and the semantics of
Hoare triple is compatible with an existing denotational semantics of BPPL
based on the category of $s$-finite kernels. Using BaSL, we then prove
properties of statistical models such as the expected value of Bayesian coin
flip, correlation of random variables in the collider Bayesian network, and the
posterior distributions of the burglar alarm model, a parameter estimation
algorithm, and the Gaussian mixture model.

</details>


### [43] [Formal Analysis of Networked PLC Controllers Interacting with Physical Environments](https://arxiv.org/abs/2507.15596)
*Jaeseo Lee,Kyungmin Bae*

Main category: cs.PL

TL;DR: The paper presents a new verification framework for industrial PLC systems that models both network communication and continuous dynamics, using partial order reduction to keep verification efficient and thorough.


<details>
  <summary>Details</summary>
Motivation: PLCs are critical in industrial automation and their applications are increasingly complex. Current verification focuses only on isolated PLC programs, neglecting important factors like physical environment interactions and network communication, which can cause errors in real-world systems.

Method: The paper introduces a unified formal framework that combines discrete PLC semantics, networked communication, and continuous physical system behaviors. It uses partial order reduction to control state explosion during verification.

Result: The proposed framework allows precise analysis of PLC-driven systems, accounting for both continuous dynamics and networked interactions, with improved scalability due to partial order reduction.

Conclusion: The unified framework overcomes the limitations of previous approaches by enabling comprehensive formal verification of industrial systems, considering both communication and continuous behaviors.

Abstract: Programmable Logic Controllers (PLCs) are widely used in industrial
automation to control physical systems. As PLC applications become increasingly
complex, ensuring their correctness is crucial. Existing formal verification
techniques focus on individual PLC programs in isolation, often neglecting
interactions with physical environments and network communication between
controllers. This limitation poses significant challenges in analyzing
real-world industrial systems, where continuous dynamics and communication
delays play a critical role. In this paper, we present a unified formal
framework that integrates discrete PLC semantics, networked communication, and
continuous physical behaviors. To mitigate state explosion, we apply partial
order reduction, significantly reducing the number of explored states while
maintaining correctness. Our framework enables precise analysis of PLC-driven
systems with continuous dynamics and networked communication.

</details>


### [44] [Closure Conversion, Flat Environments, and the Complexity of Abstract Machines](https://arxiv.org/abs/2507.15843)
*Beniamino Accattoli,Dan Ghica,Giulio Guerrieri,Cláudio Belo Lourenço,Claudio Sacerdoti Coen*

Main category: cs.PL

TL;DR: This paper compares closure conversion and abstract machine approaches in compiling functional languages, presenting new techniques for correctness proofs and environment handling, and demonstrates that closure conversion alters execution dynamics but keeps total complexity unchanged.


<details>
  <summary>Details</summary>
Motivation: Closure conversion and abstract machines handle closures and environments in related yet distinct ways. Understanding their relationship, especially in simple settings, can improve correctness proofs and efficiency in functional language compilation.

Method: The study uses a simple lambda-calculus with tuples as the source language, defines abstract machines for both source and closure-converted target languages (with flat closures/environments), introduces a new correctness proof technique, designs a new environment handling method, and adapts existing complexity analyses to compare the time complexity before and after closure conversion.

Result: The paper provides: (1) a new proof technique for closure conversion correctness; (2) a new way to handle environments in abstract machines leveraging closure invariants; (3) an adapted time complexity analysis showing closure conversion trades increased code size for decreased dynamic execution costs, but overall complexity remains unchanged.

Conclusion: Closure conversion and abstract machines can be more closely related than traditionally thought. New techniques allow for simpler correctness proofs, more efficient environment handling, and show that closure conversion, while changing certain computational aspects, preserves overall complexity.

Abstract: Closure conversion is a program transformation at work in compilers for
functional languages to turn inner functions into global ones, by building
closures pairing the transformed functions with the environment of their free
variables. Abstract machines rely on similar and yet different concepts of
closures and environments.
  In this paper, we study the relationship between the two approaches. We adopt
a very simple {\lambda}-calculus with tuples as source language and study
abstract machines for both the source language and the target of closure
conversion. Moreover, we focus on the simple case of flat
closures/environments, that is, with no sharing of environments. We provide
three contributions.
  Firstly, a new simple proof technique for the correctness of closure
conversion, inspired by abstract machines.
  Secondly, we show how the closure invariants of the target language allow us
to design a new way of handling environments in abstract machines, not
suffering the shortcomings of other styles.
  Thirdly, we study the machines from the point of view of time complexity,
adapting analyses by Accattoli and co-authors. We show that closure conversion
decreases various dynamic costs while increasing the size of the initial code.
Despite these changes, the overall complexity of the machines before and after
closure conversion turns out to be the same.

</details>
