<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 28]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Flow Sensitivity without Control Flow Graph: An Efficient Andersen-Style Flow-Sensitive Pointer Analysis](https://arxiv.org/abs/2508.01974)
*Jiahao Zhang,Xiao Cheng,Yuxiang Lei*

Main category: cs.SE

TL;DR: CG-FSPTA is a new, flow-sensitive pointer analysis method that replaces control flow graphs with an optimized constraint graph approach, yielding over 7x faster analyses and substantial memory savings without sacrificing precision, making it suitable for large software systems.


<details>
  <summary>Details</summary>
Motivation: Flow-sensitive pointer analysis provides high precision for modeling pointer behaviors in programs, which is crucial for various tasks like alias analysis and compiler optimization. However, existing techniques based on control flow graphs suffer from high computational costs due to their complexity, hampering scalability and efficiency.

Method: The authors propose CG-FSPTA, a novel approach that utilizes a flow-sensitive constraint graph (FSConsG) instead of traditional control flow graphs. This method adopts set-constraint graphs from flow-insensitive pointer analysis but augments them to retain flow sensitivity, leveraging graph optimizations and dynamic solving techniques to improve efficiency.

Result: Experimental evaluation on benchmark programs shows that CG-FSPTA reduces memory usage by an average of 33.05% and achieves a 7.27x speedup over state-of-the-art methods, while maintaining the same precision in analysis results.

Conclusion: CG-FSPTA offers a scalable, efficient, and precise alternative to traditional flow-sensitive pointer analysis, making it practical for analyzing large-scale software systems and paving the way for future improvements in program analysis.

Abstract: Flow-sensitive pointer analysis constitutes an essential component of precise
program analysis for accurately modeling pointer behaviors by incorporating
control flows. Flow-sensitive pointer analysis is extensively used in alias
analysis, taint analysis, program understanding, compiler optimization, etc.
Existing flow-sensitive pointer analysis approaches, which are conducted based
on control flow graphs, have significantly advanced the precision of pointer
analysis via sophisticated techniques to leverage control flow information.
However, they inevitably suffer from computational inefficiencies when
resolving points-to information due to the inherent complex structures of
control flow graphs. We present CG-FSPTA, a Flow-Sensitive Constraint Graph
(FSConsG) based flow-sensitive pointer analysis to overcome the inefficiency of
control-flow-graph-based analysis. CG-FSPTA uses a flow-sensitive variant to
leverage the structural advantages of set-constraint graphs (which are commonly
used in flow-insensitive pointer analysis) while keeping the flow sensitivity
of variable definitions and uses, allowing the incorporation of sophisticated
graph optimization and dynamic solving techniques. In this way, CG-FSPTA
achieves significant efficiency improvements while keeping the precision of
flow-sensitive analysis. Experimental evaluations on benchmark programs
demonstrate that CG-FSPTA, significantly reduces both memory usage and
execution time while maintaining precision. In particular, by solving in the
FSConsG, CG-FSPTA achieves an average memory reduction of 33.05\% and
accelerates flow-sensitive pointer analysis by 7.27x compared to the
state-of-art method. These experimental results underscore the efficacy of
CG-FSPTA as a scalable solution to analyze large-scale software systems,
establishing a robust foundation for future advancements in efficient program
analysis frameworks.

</details>


### [2] [TestWeaver: Execution-aware, Feedback-driven Regression Testing Generation with Large Language Models](https://arxiv.org/abs/2508.01255)
*Cuong Chi Le,Cuong Duc Van,Tung Duy Vu,Thai Minh Pham Vu,Hoang Nhat Phan,Huy Nhat Phan,Tien N. Nguyen*

Main category: cs.SE

TL;DR: TestWeaver uses program analysis and contextual annotations to boost code coverage and regression test effectiveness in LLM-based test generation, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Recent advances in large language models (LLMs) show promise in automating test generation for regression testing. However, such approaches often stagnate in improving code coverage due to limited reasoning about program execution, resulting in the so-called coverage plateau.

Method: The paper introduces TestWeaver, an LLM-based approach that integrates lightweight program analysis to guide test generation. TestWeaver innovates through: (1) providing the LLM with the backward slice from the target line instead of the full context to reduce hallucinations; (2) including close test cases that share control-flow similarities to enhance execution context; and (3) injecting execution in-line annotations to encode variable states as comments along executed paths.

Result: Empirical results show that TestWeaver improves coverage-guided test generation, accelerates code coverage growth, and produces more effective regression test cases compared to existing LLM-based methods.

Conclusion: TestWeaver enhances LLM-driven regression test generation by guiding the models with more contextual and targeted program information, thereby overcoming the coverage plateau and generating better tests.

Abstract: Regression testing ensures that code changes do not unintentionally break
existing functionality. While recent advances in large language models (LLMs)
have shown promise in automating test generation for regression testing, they
often suffer from limited reasoning about program execution, resulting in
stagnated coverage growth - a phenomenon known as the coverage plateau. In this
paper, we present TestWeaver, a novel LLM-based approach that integrates
lightweight program analysis to guide test generation more effectively.
TestWeaver introduces three key innovations: (1) it reduces hallucinations and
improves focus by supplying the LLM with the backward slice from the target
line instead of full program context; (2) it identifies and incorporates close
test cases - those that share control-flow similarities with the path to the
target line - to provide execution context within the LLM's context window; and
(3) it enhances LLM's reasoning with execution in-line annotations that encode
variable states as comments along executed paths. By equipping LLMs with these
targeted and contextualized inputs, TestWeaver improves coverage-guided test
generation and mitigates redundant explorations. Empirical results demonstrate
that TestWeaver accelerates code coverage growth and generates more effective
regression test cases than existing LLM-based approaches.

</details>


### [3] [Screencast-Based Analysis of User-Perceived GUI Responsiveness](https://arxiv.org/abs/2508.01337)
*Wei Liu,Linqiang Guo,Yi Wen Heng,Chenglin Li,Tse-Hsun,Chen,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: This paper presents a computer vision-based tool that analyzes screencast videos from automated mobile app testing to measure GUI responsiveness, achieving high accuracy and scalability in industrial settings where traditional methods fall short.


<details>
  <summary>Details</summary>
Motivation: Ensuring GUI responsiveness is crucial for mobile app user satisfaction, but it's difficult to detect and quantify user-perceived delays at scale, especially in industrial test environments with many devices and OS variations. Existing methods often fail to accurately capture real user experience or to scale effectively.

Method: The paper introduces \tool, a lightweight, black-box approach that analyzes mobile screencast videos taken during automated GUI testing. By using computer vision, \tool identifies user interactions and detects visual delays directly from the video. It computes two metrics—response time and finish time—by analyzing frame-level changes after user actions.

Result: \tool was evaluated on a benchmark dataset with 2,458 interactions from 64 popular Android apps. It achieved high accuracy: 0.96 precision and 0.93 recall in detecting interactions, and measured response and finish times within 50 ms and 100 ms errors for over 89% of cases. \tool is currently deployed in industrial testing, where it analyzes thousands of screencasts daily and uncovers GUI responsiveness issues missed by traditional methods.

Conclusion: A lightweight, scalable video analysis tool can accurately measure user-perceived GUI delays in mobile apps, improving detection of performance issues at scale and supporting better debugging in industrial pipelines.

Abstract: GUI responsiveness is critical for a positive user experience in mobile
applications. Even brief delays in visual feedback can frustrate users and lead
to negative reviews. However, detecting and quantifying such user-perceived
delays remains challenging, especially in industrial testing pipelines that
evaluate thousands of apps daily across diverse devices and OS versions.
Existing techniques based on static analysis or system metrics, while useful,
may not accurately capture user-perceived issues or scale effectively.
  In this experience paper, we present \tool, a lightweight and black-box
technique that measures GUI responsiveness directly from mobile screencasts --
video recordings captured during automated GUI testing. \tool detects user
interactions and visual delays, helping developers identify GUI performance
issues that affect the user experience. It uses computer vision to detect user
interactions and analyzes frame-level visual changes to compute two key
metrics: response time (from user action to first visual feedback) and finish
time (until visual feedback stabilizes). We evaluate \tool on a manually
annotated benchmark of 2,458 interactions from 64 popular Android apps. \tool
achieves 0.96 precision and 0.93 recall in detecting interactions, and measures
response and finish times within 50\,ms and 100\,ms error, respectively, for
over 89\% of interactions. The tool has been deployed in an industrial testing
pipeline and analyzes thousands of screencasts daily, uncovering responsiveness
issues missed by traditional tools and improving performance debugging
efficiency.

</details>


### [4] [HyClone: Bridging LLM Understanding and Dynamic Execution for Semantic Code Clone Detection](https://arxiv.org/abs/2508.01357)
*Yunhao Liang,Ruixuan Ying,Takuya Taniguchi,Guwen Lyu,Zhe Cui*

Main category: cs.SE

TL;DR: The paper introduces a two-stage method combining LLM semantic screening and execution validation, significantly improving semantic clone detection in code. This approach outperforms direct LLM methods in accuracy and shows promise for broader applications.


<details>
  <summary>Details</summary>
Motivation: Traditional code clone detection struggles particularly with 'semantic clones' (Type 4), where similar functionality is implemented with different code structures. Recent progress in large language models improved code understanding, but their sensitivity to syntax means they still perform suboptimally for these cases.

Method: The paper proposes a two-stage framework: (1) An LLM-based screening stage to filter out obvious non-clone code pairs based on semantic analysis; (2) An execution-based validation stage, where LLM-generated test inputs enable cross-execution validation to check for functional equivalence in the remaining code pairs.

Result: Experimental evaluation shows the proposed framework significantly improves precision, recall, and F1-score over directly using LLMs for clone detection.

Conclusion: The proposed two-stage approach—combining LLM semantics with execution-based validation—effectively detects semantic code clones, outperforming prior LLM-only methods. Future work will look into cross-language detection and scalability.

Abstract: Code clone detection is a critical task in software engineering, aimed at
identifying duplicated or similar code fragments within or across software
systems. Traditional methods often fail to capture functional equivalence,
particularly for semantic clones (Type 4), where code fragments implement
identical functionality despite differing syntactic structures. Recent advances
in large language models (LLMs) have shown promise in understanding code
semantics. However, directly applying LLMs to code clone detection yields
suboptimal results due to their sensitivity to syntactic differences. To
address these challenges, we propose a novel two-stage framework that combines
LLM-based screening with execution-based validation for detecting semantic
clones in Python programs. In the first stage, an LLM evaluates code pairs to
filter out obvious non-clones based on semantic analysis. For pairs not
identified as clones, the second stage employs an execution-based validation
approach, utilizing LLM-generated test inputs to assess functional equivalence
through cross-execution validation. Our experimental evaluation demonstrates
significant improvements in precision, recall, and F1-score compared to direct
LLM-based detection, highlighting the framework's effectiveness in identifying
semantic clones. Future work includes exploring cross-language clone detection
and optimizing the framework for large-scale applications.

</details>


### [5] [An Empirical Validation of Open Source Repository Stability Metrics](https://arxiv.org/abs/2508.01358)
*Elijah Kayode Adejumo,Brittany Johnson*

Main category: cs.SE

TL;DR: The paper empirically validates a control theory-based Composite Stability Index for open-source repository health using 100 GitHub projects. Weekly sampling and median-based statistics improve accuracy, confirming the framework's real-world utility and offering actionable metrics for project monitoring.


<details>
  <summary>Details</summary>
Motivation: Open source software has become integral to global software supply chains, making it vital to assess and promote its stability and sustainability. The motivation is to empirically validate metrics—specifically the Composite Stability Index (CSI)—previously proposed on theoretical grounds to evaluate the stability of open-source repositories.

Method: The study conducts the first empirical validation of the Composite Stability Index (CSI) using data from 100 highly ranked GitHub repositories, analyzing metrics like commit frequency patterns, issue resolution rates, and pull request merge rates. The approach compares weekly versus daily commit frequency sampling and evaluates the use of median over mean for better statistical accuracy in measuring resolution and review times.

Result: The study found that weekly commit frequency sampling is a more feasible and reliable measure of repository stability than daily sampling. Using the median instead of the mean when evaluating issues and pull request resolution and review times improves the stability index's accuracy. The researchers also derived data-driven half-width parameters to better align stability scores with actual project behavior.

Conclusion: This empirical validation supports the use of control theory approaches, particularly the CSI, for assessing open-source project stability. The findings enhance the practical applicability of these metrics, offering improved, evidence-based guidelines for real-world open-source project monitoring.

Abstract: Over the past few decades, open source software has been continuously
integrated into software supply chains worldwide, drastically increasing
reliance and dependence. Because of the role this software plays, it is
important to understand ways to measure and promote its stability and potential
for sustainability. Recent work proposed the use of control theory to
understand repository stability and evaluate repositories' ability to return to
equilibrium after a disturbance such as the introduction of a new feature
request, a spike in bug reports, or even the influx or departure of
contributors. This approach leverages commit frequency patterns, issue
resolution rate, pull request merge rate, and community activity engagement to
provide a Composite Stability Index (CSI). While this framework has theoretical
foundations, there is no empirical validation of the CSI in practice. In this
paper, we present the first empirical validation of the proposed CSI by
experimenting with 100 highly ranked GitHub repositories. Our results suggest
that (1) sampling weekly commit frequency pattern instead of daily is a more
feasible measure of commit frequency stability across repositories and (2)
improved statistical inferences (swapping mean with median), particularly with
ascertaining resolution and review times in issues and pull request, improves
the overall issue and pull request stability index. Drawing on our empirical
dataset, we also derive data-driven half-width parameters that better align
stability scores with real project behavior. These findings both confirm the
viability of a control-theoretic lens on open-source health and provide
concrete, evidence-backed applications for real-world project monitoring tools.

</details>


### [6] [From Technical Excellence to Practical Adoption: Lessons Learned Building an ML-Enhanced Trace Analysis Tool](https://arxiv.org/abs/2508.01430)
*Kaveh Shahedi,Matthew Khouzam,Heng Li,Maxime Lamothe,Foutse Khomh*

Main category: cs.SE

TL;DR: Industrial adoption of advanced trace analysis tools is hindered more by issues of usability and trust than by technical limitations. Tools like TMLL, which prioritize user control, embedded expertise, and transparency, are more likely to be adopted than technically superior but less usable alternatives. Designing for adoption, not just capability, is essential for successful deployment of automated software engineering tools.


<details>
  <summary>Details</summary>
Motivation: Advanced trace analysis tools for understanding complex software behavior exist, but their adoption in industry remains low. The authors wanted to understand and address the barriers to adoption, as improving technical capability alone was not translating into practical industry use.

Method: The authors partnered with Ericsson Montréal for a year to develop TMLL (Trace-Server Machine Learning Library) and studied real-world adoption challenges. They used expert collaboration, integration into existing workflows, user surveys (40 professionals), and gathered qualitative feedback through collaborations and peer review.

Result: The research identified the 'Excellence Paradox': higher technical sophistication can hinder adoption if it reduces usability, transparency, or user trust. Survey results showed that users value trust and transparency more than technical complexity, preferring semi-automated tools that allow user control. TMLL, which emphasizes embedded expertise, transparent explanations, and incremental adoption, was validated by positive feedback and successful integration at Ericsson and the Eclipse Foundation.

Conclusion: Sustainable adoption of automated trace analysis tools requires focusing on usability, transparency, and trust—instead of just technical excellence. Adoption-focused design with embedded expertise and user control is crucial for real-world use.

Abstract: System tracing has become essential for understanding complex software
behavior in modern systems, yet sophisticated trace analysis tools face
significant adoption gaps in industrial settings. Through a year-long
collaboration with Ericsson Montr\'eal, developing TMLL (Trace-Server Machine
Learning Library, now in the Eclipse Foundation), we investigated barriers to
trace analysis adoption. Contrary to assumptions about complexity or automation
needs, practitioners struggled with translating expert knowledge into
actionable insights, integrating analysis into their workflows, and trusting
automated results they could not validate. We identified what we called the
Excellence Paradox: technical excellence can actively impede adoption when
conflicting with usability, transparency, and practitioner trust. TMLL
addresses this through adoption-focused design that embeds expert knowledge in
interfaces, provides transparent explanations, and enables incremental
adoption. Validation through Ericsson's experts' feedback, Eclipse Foundation's
integration, and a survey of 40 industry and academic professionals revealed
consistent patterns: survey results showed that 77.5% prioritize quality and
trust in results over technical sophistication, while 67.5% prefer
semi-automated analysis with user control, findings supported by qualitative
feedback from industrial collaboration and external peer review. Results
validate three core principles: cognitive compatibility, embedded expertise,
and transparency-based trust. This challenges conventional capability-focused
tool development, demonstrating that sustainable adoption requires
reorientation toward adoption-focused design with actionable implications for
automated software engineering tools.

</details>


### [7] [Tuning LLM-based Code Optimization via Meta-Prompting: An Industrial Perspective](https://arxiv.org/abs/2508.01443)
*Jingzhi Gong,Rafail Giavrimis,Paul Brookes,Vardan Voskanyan,Fan Wu,Mari Ashiga,Matthew Truscott,Mike Basios,Leslie Kanthan,Jie Xu,Zheng Wang*

Main category: cs.SE

TL;DR: Meta-Prompted Code Optimization (MPCO) automates high-quality, LLM-adaptive code optimization prompts, overcoming prompt engineering bottlenecks in multi-LLM setups. MPCO boosts performance up to 19%, generalizes across top LLMs, and streamlines practical deployment in industry.


<details>
  <summary>Details</summary>
Motivation: Deploying multiple large language models (LLMs) for automatic code optimization is popular, but a major challenge is that prompts optimized for one LLM often do not generalize, requiring costly, model-specific prompt engineering. This limits scalable use in industry.

Method: The authors propose Meta-Prompted Code Optimization (MPCO), a framework that automatically generates task- and model-specific prompts by integrating project metadata, task requirements, and the particular context of each LLM. It uses meta-prompting for dynamic prompt generation and is deployed on an industrial platform (ARTEMIS) for validation and scalability.

Result: Experiments across five real codebases (366 hours of runtime) showed MPCO provided up to 19.06% performance improvement and the highest statistical ranking versus baselines. 96% of the top optimizations came from meaningful code edits. Ablation and sensitivity analyses highlight that full context integration is critical, and all major LLMs can serve as effective meta-prompters.

Conclusion: MPCO effectively overcomes cross-model prompt engineering bottlenecks, delivering significant, validated code optimization benefits and practical deployment guidance for multi-LLM industrial environments.

Abstract: There is a growing interest in leveraging large language models (LLMs) for
automated code optimization. However, industrial platforms deploying multiple
LLMs face a critical challenge: prompts optimized for one LLM often fail with
others, requiring expensive model-specific prompt engineering. This cross-model
prompt engineering bottleneck severely limits the practical deployment of
multi-LLM optimization systems in production environments. To address this, we
introduce Meta-Prompted Code Optimization (MPCO), a framework that
automatically generates high-quality, task-specific prompts across diverse LLMs
while maintaining industrial efficiency requirements. MPCO leverages
meta-prompting to dynamically synthesize context-aware optimization prompts by
integrating project metadata, task requirements, and LLM-specific contexts, and
it seamlessly deploys on the ARTEMIS industrial platform for automated
validation and scaling.
  Our comprehensive evaluation on five real-world codebases with 366 hours of
runtime benchmarking demonstrates MPCO's effectiveness: it achieves overall
performance improvements up to 19.06% with the best statistical rank across all
systems compared to baseline methods. Analysis shows that 96% of the
top-performing optimizations stem from meaningful edits. Through systematic
ablation studies and meta-prompter sensitivity analysis, we identify that
comprehensive context integration is essential for effective meta-prompting,
and that all three major LLMs can serve effectively as meta-prompters,
providing actionable insights for industrial practitioners.

</details>


### [8] [Directed Grammar-Based Test Generation](https://arxiv.org/abs/2508.01472)
*Lukas Kirschner,Ezekiel Soremekun*

Main category: cs.SE

TL;DR: FdLoop is a test generation approach that uses feedback and probabilistic grammars to create inputs tailored to specific testing goals (like code coverage or causing failures). It outperforms existing methods on diverse software and input types, proving more effective and flexible for complex software testing.


<details>
  <summary>Details</summary>
Motivation: Existing grammar-based test generators struggle to produce inputs tailored to specific testing goals, limiting their effectiveness in uncovering certain behaviors or issues in software. This research aims to address this limitation by developing a method that can generate goal-specific test inputs automatically and efficiently.

Method: The authors propose FdLoop, an automated test generation approach that uses test feedback and a probabilistic grammar. FdLoop learns from existing inputs by selecting, evolving, and adjusting the input distribution toward goal-specific inputs for each testing goal. It is concretely demonstrated for four goals: unique code coverage, generating complex inputs, causing program exceptions, and inducing long execution time.

Result: FdLoop was evaluated on three input formats (JSON, CSS, JavaScript) and 20 open-source programs. It outperformed five baseline test generators (random, probabilistic, inverse-probabilistic, EvoGFuzz, DynaMosa) in 86% of cases, sometimes doubling the effectiveness of the best baseline in inducing erroneous behaviors. The individual components of FdLoop contributed positively to its outcomes, and the method is effective for both single and multiple testing goals across parameter settings.

Conclusion: FdLoop is a superior, scalable approach for goal-specific test input generation, outperforming existing grammar-based and evolutionary test generators in both effectiveness and adaptability.

Abstract: To effectively test complex software, it is important to generate
goal-specific inputs, i.e., inputs that achieve a specific testing goal.
However, most state-of-the-art test generators are not designed to target
specific goals. Notably, grammar-based test generators, which (randomly)
produce syntactically valid inputs via an input specification (i.e., grammar)
have a low probability of achieving an arbitrary testing goal. This work
addresses this challenge by proposing an automated test generation approach
(called FdLoop) which iteratively learns relevant input properties from
existing inputs to drive the generation of goal-specific inputs. Given a
testing goal, FdLoop iteratively selects, evolves and learn the input
distribution of goal-specific test inputs via test feedback and a probabilistic
grammar. We concretize FdLoop for four testing goals, namely unique code
coverage, input-to-code complexity, program failures (exceptions) and long
execution time. We evaluate FdLoop using three (3) well-known input formats
(JSON, CSS and JavaScript) and 20 open-source software. In most (86%) settings,
FdLoop outperforms all five tested baselines namely the baseline grammar-based
test generators (random, probabilistic and inverse-probabilistic methods),
EvoGFuzz and DynaMosa. FdLoop is (up to) twice (2X) as effective as the best
baseline (EvoGFuzz) in inducing erroneous behaviors. In addition, we show that
the main components of FdLoop (i.e., input mutator, grammar mutator and test
feedbacks) contribute positively to its effectiveness. Finally, our evaluation
demonstrates that FdLoop effectively achieves single testing goals (revealing
erroneous behaviors, generating complex inputs, or inducing long execution
time) and scales to multiple testing goals across varying parameter settings.

</details>


### [9] [GitHub Marketplace: Driving Automation and Fostering Innovation in Software Development](https://arxiv.org/abs/2508.01489)
*SK. Golam Saroar,Waseefa Ahmed,Elmira Onagh,Maleknaz Nayebi*

Main category: cs.SE

TL;DR: The paper systematically analyzes GitHub Marketplace's automation tools, compares industry trends with academic research, and identifies areas for academic contributions to real-world OSS automation practices.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the increasing adoption of automation in open-source software (OSS) development and the need to understand the trends, characteristics, and dynamics of GitHub Marketplace, which provides automation tools for OSS projects. There is also a noted disconnect between academic research and industry practices in software automation.

Method: The study provides a systematic analysis of the GitHub Marketplace, comparing the trends and tools in the industry with developments described in academic literature.

Result: The analysis identifies trends in the use of automation tools in OSS, highlights differences between academic advancements and industry practices, and pinpoints opportunities for academia to contribute more directly to industry innovation.

Conclusion: This work bridges the gap between academic research and industry practice in software automation by characterizing the state of the GitHub Marketplace and suggesting how academia can drive practical improvements.

Abstract: GitHub, a central hub for collaborative software development, has
revolutionized the open-source software (OSS) ecosystem through its GitHub
Marketplace, a platform launched in 2017 to host automation tools aimed at
enhancing the efficiency and scalability of software projects. As the adoption
of automation in OSS production grows, understanding the trends,
characteristics, and underlying dynamics of this marketplace has become vital.
Furthermore, despite the rich repository of academic research on software
automation, a disconnect persists between academia and industry practices. This
study seeks to bridge this gap by providing a systematic analysis of the GitHub
Marketplace, comparing trends observed in industry tools with advancements
reported in academic literature, and identifying areas where academia can
contribute to practical innovation.

</details>


### [10] [OpenLambdaVerse: A Dataset and Analysis of Open-Source Serverless Applications](https://arxiv.org/abs/2508.01492)
*Angel C. Chavez-Moreno,Cristina L. Abad*

Main category: cs.SE

TL;DR: The paper presents OpenLambdaVerse, a new dataset of real-world GitHub projects using the Serverless Framework with AWS Lambda. Through extensive analysis, the work sheds light on current practices, architectures, and challenges in serverless computing, offering an essential resource for both practitioners and researchers.


<details>
  <summary>Details</summary>
Motivation: Serverless computing, especially using Function-as-a-Service (FaaS) like AWS Lambda via the Serverless Framework, is rapidly evolving. However, there is a need for updated, comprehensive insights into real-world usage, architecture, and practices surrounding these tools.

Method: The paper introduces OpenLambdaVerse, a curated dataset built from current GitHub repositories utilizing the Serverless Framework with AWS Lambda functions. The dataset is created by applying a methodology similar to the Wonderless dataset, with added filtering to ensure relevance and currency. The authors then analyze and characterize these repositories regarding application size, complexity, programming languages, function triggers, project maturity, and security practices.

Result: The analysis of OpenLambdaVerse provides a detailed understanding of the modern serverless application landscape, including prevalent languages, architectures, trigger types, and the state of security and project maturity. The dataset and findings offer actionable insights into how serverless technologies are currently used in production and research.

Conclusion: OpenLambdaVerse fills a significant knowledge gap by offering an up-to-date, detailed dataset and analysis of real-world serverless applications using AWS Lambda and the Serverless Framework, facilitating further research and best practice development in the field.

Abstract: Function-as-a-Service (FaaS) is at the core of serverless computing, enabling
developers to easily deploy applications without managing computing resources.
With an Infrastructure-as-Code (IaC) approach, frameworks like the Serverless
Framework use YAML configurations to define and deploy APIs, tasks, workflows,
and event-driven applications on cloud providers, promoting zero-friction
development. As with any rapidly evolving ecosystem, there is a need for
updated insights into how these tools are used in real-world projects. Building
on the methodology established by the Wonderless dataset for serverless
computing (and applying multiple new filtering steps), OpenLambdaVerse
addresses this gap by creating a dataset of current GitHub repositories that
use the Serverless Framework in applications that contain one or more AWS
Lambda functions. We then analyze and characterize this dataset to get an
understanding of the state-of-the-art in serverless architectures based on this
stack. Through this analysis we gain important insights on the size and
complexity of current applications, which languages and runtimes they employ,
how are the functions triggered, the maturity of the projects, and their
security practices (or lack of). OpenLambdaVerse thus offers a valuable,
up-to-date resource for both practitioners and researchers that seek to better
understand evolving serverless workloads.

</details>


### [11] [Exploring Direct Instruction and Summary-Mediated Prompting in LLM-Assisted Code Modification](https://arxiv.org/abs/2508.01523)
*Ningzhi Tang,Emory Smith,Yu Huang,Collin McMillan,Toby Jia-Jun Li*

Main category: cs.SE

TL;DR: This paper compares two prompting methods for LLM-driven code modification, finding each has strengths and weaknesses shaped by developer needs and context, and calls for better-designed prompt interactions and code summaries to support practical use.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) are increasingly used for code generation, but their application in code modification is less explored. Constructing effective prompts for code modification has unique challenges compared to code generation, and prior work has been limited to narrow domains.

Method: The study explores two LLM prompting strategies for code modification: Direct Instruction Prompting (developers describe changes explicitly in language) and Summary-Mediated Prompting (developers modify generated code summaries). 15 developers completed modification tasks using both strategies across several scenarios. The study observed developer workflows and analyzed the advantages and disadvantages of each prompting strategy.

Result: Developers used an iterative workflow involving code understanding, edit localization, and output validation. Direct Instruction Prompting was more flexible and easier to specify, while Summary-Mediated Prompting improved code comprehension and prompt scaffolding. The choice of strategy depended on context, such as urgency and familiarity. Usability challenges remain, including the need for granular, traceable summaries and consistency in summary generation.

Conclusion: LLM-assisted code modification requires more user-friendly prompt interactions. Both prompting strategies have trade-offs, and the optimal strategy depends on developer goals and context. Improving summary granularity, summary-code traceability, and consistency is necessary for better usability.

Abstract: This paper presents a study of using large language models (LLMs) in
modifying existing code. While LLMs for generating code have been widely
studied, their role in code modification remains less understood. Although
"prompting" serves as the primary interface for developers to communicate
intents to LLMs, constructing effective prompts for code modification
introduces challenges different from generation. Prior work suggests that
natural language summaries may help scaffold this process, yet such approaches
have been validated primarily in narrow domains like SQL rewriting. This study
investigates two prompting strategies for LLM-assisted code modification:
Direct Instruction Prompting, where developers describe changes explicitly in
free-form language, and Summary-Mediated Prompting, where changes are made by
editing the generated summaries of the code. We conducted an exploratory study
with 15 developers who completed modification tasks using both techniques
across multiple scenarios. Our findings suggest that developers followed an
iterative workflow: understanding the code, localizing the edit, and validating
outputs through execution or semantic reasoning. Each prompting strategy
presented trade-offs: direct instruction prompting was more flexible and easier
to specify, while summary-mediated prompting supported comprehension, prompt
scaffolding, and control. Developers' choice of strategy was shaped by task
goals and context, including urgency, maintainability, learning intent, and
code familiarity. These findings highlight the need for more usable prompt
interactions, including adjustable summary granularity, reliable summary-code
traceability, and consistency in generated summaries.

</details>


### [12] [RepoForge: Training a SOTA Fast-thinking SWE Agent with an End-to-End Data Curation Pipeline Synergizing SFT and RL at Scale](https://arxiv.org/abs/2508.01550)
*Zhilong Chen,Chengzong Zhao,Boyuan Chen,Dayi Lin,Yihao Chen,Arthur Leung,Gopi Krishnan Rajbahadur,Gustavo A. Oliva,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: RepoForge is an autonomous pipeline that drastically improves the efficiency and performance of software engineering LLM training, achieving state-of-the-art results with smaller models and much lower costs for storage, evaluation, and labeling.


<details>
  <summary>Details</summary>
Motivation: Training large language models (LLMs) for software engineering is hampered by expensive infrastructure, inefficient processes, limited data, and high costs for quality control and labeling. The paper seeks to overcome these bottlenecks to improve LLM training for software engineering tasks.

Method: The authors propose RepoForge, an autonomous end-to-end pipeline for generating, evaluating, and training software engineering agents at scale. RepoForge integrates storage-efficient sandboxing, a Ray-powered evaluation harness for distributed processing, automated data generation from real GitHub commits, SPICE-based automated data labeling, and an RL training scaffold.

Result: RepoForge-8B-Agent, trained with the pipeline, achieves 17.4% on the SWE-Bench-Verified benchmark, setting a new state-of-the-art for ≤8B non-thinking LLMs. The approach also generates 7,304 executable environments without manual intervention, achieves 14x storage reduction, improves evaluation speed by over 70%, and reduces labeling costs by 19,000x.

Conclusion: RepoForge addresses and mitigates major bottlenecks in training SWE LLMs—specifically storage, speed, data quality, and cost—leading to significant advances in efficiency and model performance. The work demonstrates that even models with ≤8B parameters can reach state-of-the-art results when trained with efficient, autonomous pipelines.

Abstract: Training software engineering (SWE) LLMs is bottlenecked by expensive
infrastructure, inefficient evaluation pipelines, scarce training data, and
costly quality control. We present RepoForge, an autonomous, end-to-end
pipeline that generates, evaluates, and trains SWE agents at scale. Our key
contributions include: (1) RepoForge-8B-Agent, achieving 17.4\% on
SWE-Bench-Verified~\citep{swebench_verified2024}, establishing new
state-of-the-art for $\leq$8B non-thinking LLMs; (2) 7,304 executable
environments auto-generated from real GitHub commits with zero manual
intervention; (3) 14$\times$ storage reduction (1.4GB $\rightarrow$ 102MB per
instance) via intelligent dependency management and image pruning; (4) $>$70\%
faster evaluation using a Ray-powered~\citep{ray2018} distributed RepoForge
harness; (5) 19,000$\times$ cheaper labeling through our automated
SPICE~\citep{spice2024} difficulty assessment technique. By unifying
storage-efficient sandboxing, Ray-powered evaluation harness, automated data
generation, SPICE-based labeling, and bubble-free RL scaffold, we demonstrate
that even $\leq$8B models can reach new state-of-the-art performance on
demanding benchmarks like SWE-Bench-Verified. Our approach addresses critical
bottlenecks in SWE agent training: high storage costs of container-based
evaluation, inefficient sequential reward pipelines, limited availability of
high-quality training data, expensive manual labeling, and multi-turn RL
pipeline bottlenecks.

</details>


### [13] [PCREQ: Automated Inference of Compatible Requirements for Python Third-party Library Upgrades](https://arxiv.org/abs/2508.02023)
*Huashan Lei,Guanping Xiao,Yepang Liu,Zheng Zheng*

Main category: cs.SE

TL;DR: PCREQ is a novel tool that automatically finds compatible Python library versions by checking both version and code compatibility. It significantly outperforms existing methods in accuracy and speed, reducing manual work in maintaining Python dependencies.


<details>
  <summary>Details</summary>
Motivation: Upgrading Python third-party libraries (TPLs) often causes compatibility issues, including both version and code compatibility problems, which can result in system failures. Existing tools mostly focus on detecting dependency conflicts but neglect code-level incompatibilities and lack solutions to fully automate finding compatible library versions.

Method: The proposed approach, PCREQ, automatically infers compatible requirements by combining version and code compatibility analysis. It includes six modules for knowledge acquisition, version compatibility assessment, API extraction, code compatibility assessment, version adjustment, and filling missing TPLs. PCREQ analyzes dependencies, checks for both types of compatibility, and iteratively produces a requirements.txt file and repair report.

Result: PCREQ was evaluated using REQBench, a benchmark with 2,095 upgrade test cases, including difficult ones unsolvable by pip. PCREQ achieved a 94.03% success rate in inferring compatible requirements, significantly outperforming other tools and LLM-based methods. Each case was processed on average in about 61 seconds.

Conclusion: PCREQ automates and improves the inference of compatible library versions for Python projects, efficiently addressing both version and code compatibility issues, and substantially reduces manual troubleshooting in dependency upgrades.

Abstract: Python third-party libraries (TPLs) are essential in modern software
development, but upgrades often cause compatibility issues, leading to system
failures. These issues fall into two categories: version compatibility issues
(VCIs) and code compatibility issues (CCIs). Existing tools mainly detect
dependency conflicts but overlook code-level incompatibilities, with no
solution fully automating the inference of compatible versions for both VCIs
and CCIs. To fill this gap, we propose PCREQ, the first approach to
automatically infer compatible requirements by combining version and code
compatibility analysis. PCREQ integrates six modules: knowledge acquisition,
version compatibility assessment, invoked APIs and modules extraction, code
compatibility assessment, version change, and missing TPL completion. PCREQ
collects candidate versions, checks for conflicts, identifies API usage,
evaluates code compatibility, and iteratively adjusts versions to generate a
compatible requirements.txt with a detailed repair report. To evaluate PCREQ,
we construct REQBench, a large-scale benchmark with 2,095 upgrade test cases
(including 406 unsolvable by pip). Results show PCREQ achieves a 94.03%
inference success rate, outperforming PyEGo (37.02%), ReadPyE (37.16%), and
LLM-based approaches (GPT-4o, DeepSeek V3/R1) by 18-20%. PCREQ processes each
case from REQBench in 60.79s on average, demonstrating practical efficiency.
PCREQ significantly reduces manual effort in troubleshooting upgrades,
advancing Python dependency maintenance automation.

</details>


### [14] [BiFuzz: A Two-Stage Fuzzing Tool for Open-World Video Games](https://arxiv.org/abs/2508.02144)
*Yusaku Kato,Norihiro Yoshida,Erina Makihara,Katsuro Inoue*

Main category: cs.SE

TL;DR: BiFuzz is a specialized automated testing tool for open-world video games that mutates both strategies and actions, helping detect movement-related failures more effectively than traditional fuzzers.


<details>
  <summary>Details</summary>
Motivation: Open-world video games have vast and complex environments, making automated testing difficult. Traditional fuzzing techniques may not effectively explore such broad search spaces to uncover failures.

Method: The authors proposed BiFuzz, a two-stage fuzzer that mutates both the overall gameplay strategy and specific test cases, like movement paths, in a step-by-step fashion. This approach is tailored for testing open-world video games.

Result: BiFuzz was able to dynamically mutate strategies and movement patterns, enabling it to detect 'stucking' failures in open-world games. The effectiveness was shown in their study and resources are publicly available.

Conclusion: BiFuzz improves automated playtesting of open-world video games by effectively detecting specific types of failures such as 'stucking' through advanced input mutations.

Abstract: Open-world video games present a broader search space than other games,
posing challenges for test automation. Fuzzing, which generates new inputs by
mutating an initial input, is commonly used to uncover failures. In this study,
we proposed BiFuzz, a two-stage fuzzer designed for automated testing of
open-world video games, and investigated its effectiveness. The results
revealed that BiFuzz mutated the overall strategy of gameplay and test cases,
including actual movement paths, step by step. Consequently, BiFuzz can detect
`stucking' failures. The tool and its video are at
https://github.com/Yusaku-Kato/BiFuzz.

</details>


### [15] [An MLIR-based Compilation Framework for Control Flow Management on CGRAs](https://arxiv.org/abs/2508.02167)
*Yuxuan Wang,Cristian Tirelli,Giovanni Ansaloni,Laura Pozzi,David Atienza*

Main category: cs.SE

TL;DR: This paper introduces a CGRA compilation framework that, unlike previous work, manages control flow via modular compilation passes instead of specialized hardware. This approach enables broader application support and achieves significant speedups (up to 2.1X) over current methods, relying on advanced compiler optimizations for performance.


<details>
  <summary>Details</summary>
Motivation: Although CGRAs offer high flexibility and efficiency for accelerating compute-intensive workloads, their widespread use is limited by the complexity of compiling for architectures that span both spatial and temporal dimensions. Current CGRA compilers mainly support data flow with little attention to control flow, restricting application domains and relying on specialized hardware for control divergences.

Method: The authors propose a modular compilation framework consisting of various transformation and optimization passes. Their method directly addresses control flow management at the compilation level for applications with arbitrary control flow, targeting abstract CGRA meshes in a hardware-agnostic fashion. The approach also introduces a novel mapping methodology as a compilation back-end, actively handling hardware resource constraints and ensuring feasible mappings.

Result: The proposed framework broadens application support beyond simple data flow to include those with complex control flows, while not relying on specialized hardware. The authors report up to 2.1X speedups over state-of-the-art solutions strictly via compiler optimizations.

Conclusion: Control flow on CGRAs can be efficiently managed at the compilation level, lifting hardware-specific burdens and enabling high-performance, flexible acceleration of varied workloads. Their compilation framework demonstrates significant performance improvement using solely software-level techniques.

Abstract: Coarse Grained Reconfigurable Arrays (CGRAs) present both high flexibility
and efficiency, making them well-suited for the acceleration of intensive
workloads. Nevertheless, a key barrier towards their widespread adoption is
posed by CGRA compilation, which must cope with a multi-dimensional space
spanning both the spatial and the temporal domains. Indeed, state-of-the-art
compilers are limited in scope as they mostly deal with the data flow of
applications, while having little or no support for control flow. Hence, they
mostly target the mapping of single loops and/or delegate the management of
control flow divergences to ad-hoc hardware units.
  Conversely, in this paper we show that control flow can be effectively
managed and optimized at the compilation level, allowing for a broad set of
applications to be targeted while being hardware-agnostic and achieving high
performance. We embody our methodology in a modular compilation framework
consisting of transformation and optimization passes, enabling support for
applications with arbitrary control flows running on abstract CGRA meshes. We
also introduce a novel mapping methodology that acts as a compilation back-end,
addressing the limitations in available CGRA hardware resources and
guaranteeing a feasible solution in the compilation process. Our framework
achieves up to 2.1X speedups over state-of-the-art approaches, purely through
compilation optimizations.

</details>


### [16] [Highly Interactive Testing for Uninterrupted Development Flow](https://arxiv.org/abs/2508.02176)
*Andrew Tropin*

Main category: cs.SE

TL;DR: By integrating tests directly into highly interactive development environments, this paper's library enables instant feedback and easy access to debugging tools, reducing delays and disruptions in developer workflow.


<details>
  <summary>Details</summary>
Motivation: Traditional testing methods disrupt developer workflow in highly interactive development environments (HIDEs), as tests are executed separately from the HIDE tools, causing delays and context-switching issues.

Method: The authors present a library that provides a runtime representation of tests. This allows tests to be tightly integrated with HIDE tooling, enabling developers to interact with tests and access debugging tools directly within the development environment upon a test failure.

Result: The proposed approach enables immediate access to HIDE tools when a test fails and achieves subsecond test re-execution times, significantly improving developer focus.

Conclusion: Integrating tests into the HIDE through runtime representation maintains uninterrupted development flow and quick feedback, overcoming limitations of traditional, separate test execution.

Abstract: Highly interactive development environments (HIDEs) enable uninterrupted
development flow through continuous program evolution and rapid hypothesis
checking. However, traditional testing approaches -- typically executed
separately via CLI -- isolate tests from HIDE tooling (interactive debuggers,
value and stack inspectors, etc.) and introduce disruptive delays due to coarse
execution granularity and lack of runtime context. This disconnect breaks
development flow by exceeding critical attention thresholds. In this paper we
present a library that provides runtime representation for tests, allowing
tight integration with HIDEs, and enabling immediate access to HIDE tooling in
the context of test failure. We then describe development workflows enhanced
with testing and demonstrate how they achieve subsecond test reexecution times
crucial for maintaining developer focus.

</details>


### [17] [A Methodological Framework for LLM-Based Mining of Software Repositories](https://arxiv.org/abs/2508.02233)
*Vincenzo De Martino,Joel Castaño,Fabio Palomba,Xavier Franch,Silverio Martínez-Fernández*

Main category: cs.SE

TL;DR: This paper analyzes how large language models are methodologically integrated into mining software repositories research. Through a mixed-method study, it identifies key approaches, threats, and mitigation strategies, culminating in the PRIMES 2.0 framework to guide rigorous, reproducible LLM-based MSR research.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are being increasingly applied to software engineering research, especially for automating repository mining. However, there is little understanding of how LLMs are methodologically integrated within the Mining Software Repositories (MSR) field, as existing studies mainly discuss specific capabilities and benchmarks without a holistic view.

Method: The authors performed a mixed-method study, including a rapid literature review and a questionnaire survey focused on the use of LLMs in Mining Software Repositories (LLM4MSR). They systematically identified methodological approaches, potential threats to empirical rigor, and mitigation strategies.

Result: The study identified 15 methodological approaches, nine main empirical threats, and 25 mitigation strategies. Based on these insights, they proposed PRIMES 2.0, an empirical framework with six stages and 23 methodological substeps, each tied to identified threats and actionable mitigation strategies.

Conclusion: The resulting PRIMES 2.0 framework provides structured methodological guidance, helping researchers ensure transparency and reproducibility in LLM-based MSR studies. The work lays a more robust foundation for future research in integrating LLMs methodologically in software repository mining.

Abstract: Large Language Models (LLMs) are increasingly used in software engineering
research, offering new opportunities for automating repository mining tasks.
However, despite their growing popularity, the methodological integration of
LLMs into Mining Software Repositories (MSR) remains poorly understood.
Existing studies tend to focus on specific capabilities or performance
benchmarks, providing limited insight into how researchers utilize LLMs across
the full research pipeline. To address this gap, we conduct a mixed-method
study that combines a rapid review and questionnaire survey in the field of
LLM4MSR. We investigate (1) the approaches and (2) the threats that affect the
empirical rigor of researchers involved in this field. Our findings reveal 15
methodological approaches, nine main threats, and 25 mitigation strategies.
Building on these findings, we present PRIMES 2.0, a refined empirical
framework organized into six stages, comprising 23 methodological substeps,
each mapped to specific threats and corresponding mitigation strategies,
providing prescriptive and adaptive support throughout the lifecycle of
LLM-based MSR studies. Our work contributes to establishing a more transparent
and reproducible foundation for LLM-based MSR research.

</details>


### [18] [Dialogue Systems Engineering: A Survey and Future Directions](https://arxiv.org/abs/2508.02279)
*Mikio Nakano,Hironori Takeuchi,Sadahiro Yoshikawa,Yoichi Matsuyama,Kazunori Komatani*

Main category: cs.SE

TL;DR: This paper introduces and surveys 'Dialogue Systems Engineering,' mapping it to established software engineering frameworks, and identifies future challenges and directions needed to support the evolving field of dialogue systems.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the growing demand for robust development, operation, and maintenance frameworks specific to dialogue systems, especially as advanced large language models enable their widespread application in society and business.

Method: The authors enumerate and align key knowledge areas of Dialogue Systems Engineering with those from the established Software Engineering Body of Knowledge (SWEBOK) Version 4.0, then systematically survey and analyze these areas to identify gaps and suggest future directions.

Result: The study provides a comprehensive survey of dialogue systems engineering knowledge areas and highlights unexplored topics and challenges unique to this emerging subfield.

Conclusion: Dialogue systems present unique challenges that require an evolution of existing software engineering practices. Tailored engineering frameworks for dialogue systems are necessary to ensure their effective, reliable, and sustainable lifecycle management.

Abstract: This paper proposes to refer to the field of software engineering related to
the life cycle of dialogue systems as Dialogue Systems Engineering, and surveys
this field while also discussing its future directions. With the advancement of
large language models, the core technologies underlying dialogue systems have
significantly progressed. As a result, dialogue system technology is now
expected to be applied to solving various societal issues and in business
contexts. To achieve this, it is important to build, operate, and continuously
improve dialogue systems correctly and efficiently. Accordingly, in addition to
applying existing software engineering knowledge, it is becoming increasingly
important to evolve software engineering tailored specifically to dialogue
systems. In this paper, we enumerate the knowledge areas of dialogue systems
engineering based on those of software engineering, as defined in the Software
Engineering Body of Knowledge (SWEBOK) Version 4.0, and survey each area. Based
on this survey, we identify unexplored topics in each area and discuss the
future direction of dialogue systems engineering.

</details>


### [19] [Interoperable verification and dissemination of software assets in repositories using COAR Notify](https://arxiv.org/abs/2508.02335)
*Matteo Cancellieri,Martin Docekal,David Pride,Morane Gruenpeter,David Douard,Petr Knoth*

Main category: cs.SE

TL;DR: This paper presents the SoFAIR project, which applies machine learning and interoperable protocols to improve the discoverability, citation, and reuse of research software mentioned in academic papers by linking repositories, authors, and archival services.


<details>
  <summary>Details</summary>
Motivation: Open research software is often difficult to discover, cite, and reuse because it is not clearly referenced in academic papers.

Method: The SoFAIR project uses machine learning to automatically extract software mentions from research papers and integrates various repository systems and protocols—including HAL, Software Heritage, and COAR Notify Protocol—to archive, cite, and communicate software-related information.

Result: The implementation of this system enables automated, interoperable communication between repositories and authors, improving the archiving, citation, and access to research software.

Conclusion: The SoFAIR workflow and its use of the COAR Notify Protocol can greatly improve the visibility, credibility, and FAIRness of research software by making it more discoverable and properly cited as a bibliographic record.

Abstract: The discoverability, attribution, and reusability of open research software
are often hindered by its obscurity within academic manuscripts. To address
this, the SoFAIR project (2024-2025) introduces a comprehensive workflow
leveraging machine learning tools for extracting software mentions from
research papers. The project integrates repository systems, authors, and
services like HAL and Software Heritage to ensure proper archiving, citation,
and accessibility of research software in alignment with FAIR principles. To
enable interoperable communication across the various systems we present an
integration of the COAR Notify Protocol, which facilitates automated,
interoperable communication among repositories and authors to validate and
disseminate software mentions. This paper outlines the SoFAIR workflow and the
implementation of the COAR Notify Protocol, emphasising its potential to
enhance the visibility and credibility of research software as first-class
bibliographic records.

</details>


### [20] [Vision Language Model-based Testing of Industrial Autonomous Mobile Robots](https://arxiv.org/abs/2508.02338)
*Jiahui Wu,Chengjie Lu,Aitor Arrieta,Shaukat Ali,Thomas Peyrucain*

Main category: cs.SE

TL;DR: The paper introduces a simulated testing method for autonomous mobile robots using vision-language models to safely and efficiently generate diverse, challenging human-robot interaction scenarios that help reveal potential safety failures and unpredictable robot behaviors.


<details>
  <summary>Details</summary>
Motivation: Autonomous Mobile Robots (AMRs) operate in environments shared with humans whose behavior is unpredictable. Testing all possible human-robot interactions is challenging, costly, and can be hazardous in real life. Thus, there is a need for effective, safe, and comprehensive testing methods for AMRs' safety and functionality in response to diverse human behaviors.

Method: The authors propose a Vision Language Model (VLM)-based testing approach called RVSG. This approach employs VLM to generate a variety of human behaviors that intentionally violate the functional and safety requirements of AMRs. Testing is conducted in a simulator using the latest robot from PAL Robotics, evaluating the ability to generate requirement-violating scenarios across different requirements and navigation routes.

Result: Compared to the baseline, the RVSG approach can effectively generate scenarios that violate requirements, increasing the variability in robot behavior and helping to uncover previously unknown or uncertain behaviors in AMRs.

Conclusion: The VLM-based RVSG testing approach is effective for generating diverse, challenging scenarios for AMR testing, improving the identification of requirement-violating and uncertain behaviors without the risks and limitations of real-world testing.

Abstract: Autonomous Mobile Robots (AMRs) are deployed in diverse environments (e.g.,
warehouses, retail spaces, and offices), where they work alongside humans.
Given that human behavior can be unpredictable and that AMRs may not have been
trained to handle all possible unknown and uncertain behaviors, it is important
to test AMRs under a wide range of human interactions to ensure their safe
behavior. Moreover, testing in real environments with actual AMRs and humans is
often costly, impractical, and potentially hazardous (e.g., it could result in
human injury). To this end, we propose a Vision Language Model (VLM)-based
testing approach (RVSG) for industrial AMRs developed by PAL Robotics in Spain.
Based on the functional and safety requirements, RVSG uses the VLM to generate
diverse human behaviors that violate these requirements. We evaluated RVSG with
several requirements and navigation routes in a simulator using the latest AMR
from PAL Robotics. Our results show that, compared with the baseline, RVSG can
effectively generate requirement-violating scenarios. Moreover, RVSG-generated
scenarios increase variability in robot behavior, thereby helping reveal their
uncertain behaviors.

</details>


### [21] [JC-Finder: Detecting Java Clone-based Third-Party Library by Class-level Tree Analysis](https://arxiv.org/abs/2508.02397)
*Lida Zhao,Chaofan Li,Yueming Wu,Lyuye Zhang,Jiahui Wu,Chengwei Liu,Sen Chen,Yutao Hu,Zhengzi Xu,Yi Liu,Jingquan Ge,Jun Sun,Yang Liu*

Main category: cs.SE

TL;DR: JC-Finder is a new tool for Java that efficiently and accurately detects reused third-party libraries introduced by code clones, outperforming existing tools and revealing many libraries not declared in package managers.


<details>
  <summary>Details</summary>
Motivation: Reusing third-party libraries (TPL) is common in software development, but chaotic management and unauthorized use, especially through code clones, cause maintenance and ethical issues. Existing SCA tools either focus on package management or do not cater to Java's prevalent clone-based reuse.

Method: The authors developed JC-Finder, a clone-based SCA tool designed specifically for Java. It operates by capturing features at the class level, preserving inter-function relationships, and filtering out trivial or duplicated code elements. The tool was evaluated using 9,965 Maven libraries as reference data and tested on over 1,000 GitHub projects.

Result: JC-Finder achieved an F1-score of 0.818, outperforming a function-level tool by 0.427 and running approximately 9 times faster. When scaled to 7,947 GitHub projects, it identified TPL reuse via code clones in 789 projects and detected 2,142 TPLs, including 26.20% more TPLs not declared in package managers.

Conclusion: JC-Finder fills a critical gap for Java clone-based SCA, enabling more accurate and efficient detection of third-party library reuse through code cloning, surpassing existing tools in accuracy, speed, and comprehensiveness.

Abstract: While reusing third-party libraries (TPL) facilitates software development,
its chaotic management has brought great threats to software maintenance and
the unauthorized use of source code also raises ethical problems such as
misconduct on copyrighted code. To identify TPL reuse in projects, Software
Composition Analysis (SCA) is employed, and two categories of SCA techniques
are used based on how TPLs are introduced: clone-based SCA and
package-manager-based SCA (PM-based SCA). Although introducing TPLs by clones
is prevalent in Java, no clone-based SCA tools are specially designed for Java.
Also, directly applying clone-based SCA techniques from other tools is
problematic. To fill this gap, we introduce JC-Finder, a novel clone-based SCA
tool that aims to accurately and comprehensively identify instances of TPL
reuse introduced by source code clones in Java projects. JC-Finder achieves
both accuracy and efficiency in identifying TPL reuse from code cloning by
capturing features at the class level, maintaining inter-function
relationships, and excluding trivial or duplicated elements. To evaluate the
efficiency of JC-Finder, we applied it to 9,965 most popular Maven libraries as
reference data and tested the TPL reuse of 1,000 GitHub projects. The result
shows that JC-Finder achieved an F1-score of 0.818, outperforming the other
function-level tool by 0.427. The average time taken for resolving TPL reuse is
14.2 seconds, which is approximately 9 times faster than the other tool. We
further applied JC-Finder to 7,947 GitHub projects, revealing TPL reuse by code
clones in 789 projects (about 9.89% of all projects) and identifying a total of
2,142 TPLs. JC-Finder successfully detects 26.20% more TPLs that are not
explicitly declared in package managers.

</details>


### [22] [Quantum Machine Learning-based Test Oracle for Autonomous Mobile Robots](https://arxiv.org/abs/2508.02407)
*Xinyi Wang,Qinghua Xu,Paolo Arcaini,Shaukat Ali,Thomas Peyrucain*

Main category: cs.SE

TL;DR: The paper introduces QuReBot, a quantum-classical hybrid test oracle for robot regression testing, which reduces prediction error by 15% over classical neural networks, addressing the challenge of testing autonomous robots in uncertain environments.


<details>
  <summary>Details</summary>
Motivation: Regression testing is necessary for robot software that frequently updates, but it is difficult to define correct robot behavior (test oracles) due to unpredictable environments. Machine learning-based oracles could solve this problem, but further innovation is needed for reliable prediction.

Method: The authors developed a test oracle using quantum machine learning (QML), specifically a hybrid framework called QuReBot. This framework combines quantum reservoir computing (QRC) and a simple neural network with a residual connection, aiming to predict robot behavior more accurately and efficiently.

Result: The QRC approach alone failed to converge and yielded high prediction errors. However, the QuReBot hybrid framework successfully converged and reduced prediction error by 15% compared to a classical neural network baseline. The framework's performance was further analyzed under different settings.

Conclusion: QuReBot, the hybrid quantum-classical framework, outperforms both standalone quantum and classical neural approaches for predicting robot behavior, showing promise for effective regression testing in robot software. Practical recommendations for framework configurations are also provided.

Abstract: Robots are increasingly becoming part of our daily lives, interacting with
both the environment and humans to perform their tasks. The software of such
robots often undergoes upgrades, for example, to add new functionalities, fix
bugs, or delete obsolete functionalities. As a result, regression testing of
robot software becomes necessary. However, determining the expected correct
behavior of robots (i.e., a test oracle) is challenging due to the potentially
unknown environments in which the robots must operate. To address this
challenge, machine learning (ML)-based test oracles present a viable solution.
This paper reports on the development of a test oracle to support regression
testing of autonomous mobile robots built by PAL Robotics (Spain), using
quantum machine learning (QML), which enables faster training and the
construction of more precise test oracles. Specifically, we propose a hybrid
framework, QuReBot, that combines both quantum reservoir computing (QRC) and a
simple neural network, inspired by residual connection, to predict the expected
behavior of a robot. Results show that QRC alone fails to converge in our case,
yielding high prediction error. In contrast, QuReBot converges and achieves 15%
reduction of prediction error compared to the classical neural network
baseline. Finally, we further examine QuReBot under different configurations
and offer practical guidance on optimal settings to support future robot
software testing.

</details>


### [23] [TreeRanker: Fast and Model-agnostic Ranking System for Code Suggestions in IDEs](https://arxiv.org/abs/2508.02455)
*Daniele Cipollone,Egor Bogomolov,Arie van Deursen,Maliheh Izadi*

Main category: cs.SE

TL;DR: A novel, efficient method uses language models to better rank code completion suggestions in IDEs, improving relevance without changing existing systems.


<details>
  <summary>Details</summary>
Motivation: Token-level code completion is important for developer productivity in IDEs, but current ranking methods are limited either by hand-crafted heuristics or basic machine learning models that lack deep contextual understanding and generalization.

Method: The authors propose a lightweight, model-agnostic scoring method for ranking static code completions. Their technique involves organizing valid completions into a prefix tree and using a single greedy decoding pass of language models to score tokens across all completions, eliminating the need for beam search or extensive model adaptation.

Result: The new approach achieves precise, token-aware ranking of code completion suggestions efficiently and without requiring changes to existing language models or IDE architecture.

Conclusion: This work demonstrates a practical, fast, and architecture-compatible way to significantly improve code completion ranking in modern IDEs by integrating language model scoring, thus enhancing developer experience.

Abstract: Token-level code completion is one of the most critical features in modern
Integrated Development Environments (IDEs). It assists developers by suggesting
relevant identifiers and APIs during coding. While completions are typically
derived from static analysis, their usefulness depends heavily on how they are
ranked, as correct predictions buried deep in the list are rarely seen by
users. Most current systems rely on hand-crafted heuristics or lightweight
machine learning models trained on user logs, which can be further improved to
capture context information and generalize across projects and coding styles.
In this work, we propose a new scoring approach to ranking static completions
using language models in a lightweight and model-agnostic way. Our method
organizes all valid completions into a prefix tree and performs a single greedy
decoding pass to collect token-level scores across the tree. This enables a
precise token-aware ranking without needing beam search, prompt engineering, or
model adaptations. The approach is fast, architecture-agnostic, and compatible
with already deployed models for code completion. These findings highlight a
practical and effective pathway for integrating language models into already
existing tools within IDEs, and ultimately providing smarter and more
responsive developer assistance.

</details>


### [24] [An Efficient and Adaptive Next Edit Suggestion Framework with Zero Human Instructions in IDEs](https://arxiv.org/abs/2508.02473)
*Xinfang Chen,Siyang Xiao,Xianying Zhu,Junhong Xie,Ming Liang,Dajun Chen,Wei Jiang,Yong Li,Peng Di*

Main category: cs.SE

TL;DR: NES is a new LLM-driven code editing system that effectively predicts and suggests code edits without explicit instructions, achieving better accuracy and lower latency than existing tools, and is already used at scale in industry.


<details>
  <summary>Details</summary>
Motivation: Existing AI-powered code editing tools require explicit natural language instructions and suffer from high latency, which makes them less effective for seamless developer workflows. There's a need for more efficient and intuitive code editing assistance.

Method: The authors propose NES (Next Edit Suggestion), an LLM-based code editing framework that does not require explicit instructions. NES uses a dual-model architecture and is trained on two novel datasets (SFT and DAPO), focusing on understanding developers’ intent from historical editing patterns and optimizing for low latency.

Result: NES achieves high accuracy in predicting code edit locations (75.6% and 81.6% on two tasks) and strong scores for intent-aligned edits (91.36% ES and 27.7% EMR), outperforming state-of-the-art code LLMs. Its datasets also improve open-source model performance.

Conclusion: NES offers instruction-free, low-latency code editing suggestions that integrate smoothly with developer workflows and scale to large organizations. Its practical deployment and superior performance metrics indicate it is a viable, industry-ready solution for code editing.

Abstract: Code editing, including modifying, refactoring, and maintaining existing
code, is the most frequent task in software development and has garnered
significant attention from AI-powered tools. However, existing solutions that
translate explicit natural language instructions into code edits face critical
limitations, such as heavy reliance on human instruction input and high
latency, which hinder their effective integration into a developer's workflow.
We observe that developers' habitual behaviors and coding objectives are often
reflected in their historical editing patterns, making this data key to
addressing existing limitations. To leverage these insights, we propose NES
(Next Edit Suggestion), an LLM-driven code editing framework that delivers an
instruction-free and low-latency experience. Built on a dual-model architecture
and trained with our high-quality SFT and DAPO datasets, NES enhances
productivity by understanding developer intent while optimizing inference to
minimize latency. NES is a scalable, industry-ready solution with a continuous
Tab key interaction workflow, seamlessly adopted by a FinTech company with over
20,000 developers. Evaluations on real-world datasets show NES achieves 75.6%
and 81.6% accuracy in two tasks of predicting next edit locations, alongside
91.36% ES and 27.7% EMR for intent-aligned edits, outperforming SOTA models.
Our open-sourced SFT and DAPO datasets have been demonstrated to enhance the
performance of open-source CodeLLMs. The demonstration of NES is available at
https://youtu.be/yGoyYOe6fbY.

</details>


### [25] [Commit Stability as a Signal for Risk in Open-Source Projects](https://arxiv.org/abs/2508.02487)
*Elijah Kayode Adejumo,Brittany Johnson,Mariam Guizani*

Main category: cs.SE

TL;DR: Most open source projects lack stable development patterns, which are linked to resilience and governance quality. Frequent commits alone don't guarantee stability; more comprehensive metrics are needed to assess OSS project risk and health.


<details>
  <summary>Details</summary>
Motivation: Open source software (OSS) is now critical to global technical infrastructure and economic value. As its organizational importance grows, understanding how these projects evolve—especially their resilience to disruptions—is essential, yet understudied.

Method: The study builds on the Composite Stability Index (CSI) framework, empirically validating commit frequency patterns across 100 top-ranked OSS repositories. The analysis examines stability at daily, weekly, and monthly levels, and investigates correlations with factors like programming language, application domain, and governance models.

Result: Findings show that only 2% of repositories are stable daily, 29% weekly, and 50% monthly; the rest are unstable. Languages and blockchain projects tend to foster higher stability. Only two repositories showed stability across all granularities, linked to mature governance and consistent development processes. High commit volume does not equate to higher stability. Additional metrics like issue resolution and PR merges offer more depth.

Conclusion: Stable commit patterns suggest organizational maturity and resilience but are relatively rare among popular OSS projects. Broader, multi-faceted metrics are needed for a complete and reliable measure of resilience and risk in OSS projects.

Abstract: Open source software (OSS) generates trillions of dollars in economic value
and has become essential to technical infrastructures worldwide. As
organizations increasingly depend on OSS, understanding project evolution is
critical. While existing metrics provide insights into project health, one
dimension remains understudied: project resilience -- the ability to return to
normal operations after disturbances such as contributor departures, security
vulnerabilities, and bug report spikes. We hypothesize that stable commit
patterns reflect underlying project characteristics such as mature governance,
sustained contributors, and robust development processes that enable
resilience. Building on the Composite Stability Index (CSI) framework, we
empirically validate commit frequency patterns across 100 highly ranked
repositories. Our findings reveal that only 2\% of repositories exhibit daily
stability, 29\% achieve weekly stability, and 50\% demonstrate monthly
stability, while half remain unstable across all temporal levels. Programming
languages and blockchain applications were the most stable. We identified two
exemplary repositories that achieved stability at all three granularities,
whose governance models, CI cadence, and release policies could serve as
reference frameworks. We observed that large yearly commit throughput does not
necessarily correlate with stability. Beyond commits, stability can be enriched
with issue-resolution times, PR merge rates, and community-engagement metrics
to broaden resilience assessment and sharpen stability-based risk evaluation.

</details>


### [26] [Bridging Language Gaps in Open-Source Documentation with Large-Language-Model Translation](https://arxiv.org/abs/2508.02497)
*Elijah Kayode Adejumo,Brittany Johnson,Mariam Guizani*

Main category: cs.SE

TL;DR: LLMs like ChatGPT 4 and Claude can translate open source documentation with reasonable accuracy, but often struggle to preserve layouts, code, and links. The paper introduces TRIFID, a tool to automatically check translation fidelity, offering a foundation for improved multilingual documentation in open source projects.


<details>
  <summary>Details</summary>
Motivation: Most open source repositories lack essential documentation in languages other than English, limiting accessibility for non-English speakers. The study is motivated by the need to make technical documentation more globally accessible, and explores whether large language models (LLMs) can effectively translate such documentation, which contains a mix of language, code, and formatting.

Method: The authors analyzed community translation activities within open source repositories and compared English-to-German translations of 50 README files using OpenAI's ChatGPT 4 and Anthropic's Claude. They then evaluated fidelity challenges and introduced TRIFID, a translation fidelity scoring framework that assesses preservation of code, links, and formatting.

Result: Translation activities are rare and mostly present in larger repositories. LLMs can generate overall accurate translations, but struggle with structural fidelity, such as preserving hyperlinks and maintaining consistent formatting.

Conclusion: LLMs show promise for assisting in technical documentation translation, but face challenges in structural and formatting fidelity. The newly introduced TRIFID framework provides an effective way to automatically assess translation fidelity, laying the groundwork for further automated, LLM-driven support in open source documentation internationalization.

Abstract: While open source communities attract diverse contributors globally, few
repositories provide essential documentation in languages other than English.
Large language models (LLMs) have demonstrated remarkable capabilities in
software engineering tasks and translations across domains. However, little is
known about LLM capabilities in translating open-source technical
documentation, which mixes natural language, code, URLs, and markdown
formatting. To understand the need and potential for LLMs in technical
documentation translation, we evaluated community translation activity and
English-to-German translations of 50 README files using OpenAI's ChatGPT 4 and
Anthropic's Claude. We found scarce translation activity, mostly in larger
repositories and community-driven in nature. LLM performance comparison
suggests they can provide accurate translations. However, analysis revealed
fidelity challenges: both models struggled to preserve structural components
(e.g., hyperlinks) and exhibited formatting inconsistencies. These findings
highlight both promise and challenges of LLM-assisted documentation
internationalization. As a first step toward translation-aware continuous
integration pipelines, we introduce TRIFID, an early-stage translation fidelity
scoring framework that automatically checks how well translations preserve
code, links, and formatting. Our efforts provide a foundation for automated
LLM-driven support for creating and maintaining open source documentation.

</details>


### [27] [Automatic Identification of Machine Learning-Specific Code Smells](https://arxiv.org/abs/2508.02541)
*Peter Hamfelt,Ricardo Britto,Lincoln Rocha,Camilo Almendra*

Main category: cs.SE

TL;DR: This paper introduces MLpylint, a static analysis tool designed specifically to detect machine learning code smells. Through literature reviews, expert consultations, and validation on open-source projects, the tool proves effective, with plans for further integration into development workflows.


<details>
  <summary>Details</summary>
Motivation: Current research on code smells in machine learning (ML) applications is lacking in tools and studies focused on ML-specific code smells. As ML grows more vital to various industries, identifying and addressing these smells is increasingly important.

Method: The paper uses the Design Science Methodology. It starts with a literature review to identify ML-specific code smells, followed by further literature review and expert consultation to design the static code analysis tool (MLpylint). The tool is then evaluated using data from 160 open-source ML projects from GitHub and validated through a survey of 15 ML professionals.

Result: The evaluation and expert validation demonstrate that MLpylint is effective and useful in identifying ML-specific code smells.

Conclusion: The developed MLpylint tool is effective for the identification of ML-specific code smells, and future work will focus on integrating this tool into real-world development workflows to enhance developer productivity and innovation.

Abstract: Machine learning (ML) has rapidly grown in popularity, becoming vital to many
industries. Currently, the research on code smells in ML applications lacks
tools and studies that address the identification and validity of ML-specific
code smells. This work investigates suitable methods and tools to design and
develop a static code analysis tool (MLpylint) based on code smell criteria.
This research employed the Design Science Methodology. In the problem
identification phase, a literature review was conducted to identify ML-specific
code smells. In solution design, a secondary literature review and
consultations with experts were performed to select methods and tools for
implementing the tool. We evaluated the tool on data from 160 open-source ML
applications sourced from GitHub. We also conducted a static validation through
an expert survey involving 15 ML professionals. The results indicate the
effectiveness and usefulness of the MLpylint. We aim to extend our current
approach by investigating ways to introduce MLpylint seamlessly into
development workflows, fostering a more productive and innovative developer
environment.

</details>


### [28] [Meta-RAG on Large Codebases Using Code Summarization](https://arxiv.org/abs/2508.02611)
*Vali Tawosia,Salwa Alamir,Xiaomo Liu,Manuela Veloso*

Main category: cs.SE

TL;DR: The paper introduces Meta-RAG, a multi-agent LLM system that effectively pinpoints bugs in large codebases by condensing code into summarized natural language forms and using LLMs for bug localization, achieving state-of-the-art accuracy on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Software development requires not only code implementation but also efficient code maintenance, including bug localization in large codebases, which is challenging due to code complexity and scale. Current automated solutions using LLMs have limitations in handling these tasks effectively.

Method: The authors propose a multi-agent system for bug localization in large codebases. The core of their approach is Meta-RAG (a novel Retrieval Augmented Generation technique), which summarizes codebases using information retrieval and LLMs to produce a compact natural language representation. An LLM agent then analyzes these summaries to identify crucial code segments related to bugs.

Result: Using the SWE-bench Lite dataset, Meta-RAG achieved 84.67% accuracy in file-level bug localization and 53.0% in function-level localization, surpassing previous state-of-the-art methods.

Conclusion: Meta-RAG demonstrates highly effective state-of-the-art bug localization in large codebases, providing a practical tool for code maintenance and paving the way for further LLM-based automation in software development beyond code generation.

Abstract: Large Language Model (LLM) systems have been at the forefront of applied
Artificial Intelligence (AI) research in a multitude of domains. One such
domain is software development, where researchers have pushed the automation of
a number of code tasks through LLM agents. Software development is a complex
ecosystem, that stretches far beyond code implementation and well into the
realm of code maintenance. In this paper, we propose a multi-agent system to
localize bugs in large pre-existing codebases using information retrieval and
LLMs. Our system introduces a novel Retrieval Augmented Generation (RAG)
approach, Meta-RAG, where we utilize summaries to condense codebases by an
average of 79.8\%, into a compact, structured, natural language representation.
We then use an LLM agent to determine which parts of the codebase are critical
for bug resolution, i.e. bug localization. We demonstrate the usefulness of
Meta-RAG through evaluation with the SWE-bench Lite dataset. Meta-RAG scores
84.67 % and 53.0 % for file-level and function-level correct localization
rates, respectively, achieving state-of-the-art performance.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [29] [Efficient compilation and execution of synchronous programs via type-state programming](https://arxiv.org/abs/2508.01199)
*Avinash Malik*

Main category: cs.PL

TL;DR: This paper presents a new linear time compilation method for synchronous programs that generates faster executables without increasing binary size, outperforming current compilers in execution speed.


<details>
  <summary>Details</summary>
Motivation: Compiling imperative synchronous programs into efficient, small, and fast executables is challenging due to the state space explosion problem when composing multiple FSMs.

Method: The paper introduces graph-based rewrite rules for kernel constructs, applied in a linear time algorithm to generate FSMs, and encodes these FSMs into type-state programs using C++ template meta-programming.

Result: The linear time compilation technique produces binaries comparable in size to existing methods but achieves execution times that are on average 31-60% faster than state-of-the-art compilers.

Conclusion: The proposed compilation approach provides a more efficient executable generation for synchronous programs by addressing state space explosion and improving runtime performance.

Abstract: Synchronous programs are used extensively in implementation of safety
critical embedded software. Imperative synchronous programming languages model
multiple Finite State Machines (FSMs) executing in lockstep at logical clock
ticks. The synchronous view of time along with the FSM based design enables
easier formal verification. The synchronous composition of multiple FSMs,
during compilation, results in the well known state space explosion problem.
Hence, efficiently compiling imperative synchronous programs into small and
fast executables is challenging. This paper introduces a novel linear time
compilation technique for automata based compilation of synchronous programs.
Graph based rewrite rules for kernel programming constructs are introduced. A
linear time algorithm applies these rules to produce a FSM. The FSM is then
encoded into a type-state program using template meta-programming in C++.
Experimental results show that the compilation time and generated binary size
is comparable, while the execution times are on average 31-60% faster than
current state-of-the-art compilers.

</details>


### [30] [Proceedings 14th International Workshop on Trends in Functional Programming in Education](https://arxiv.org/abs/2508.02305)
*Rose Bohrer*

Main category: cs.PL

TL;DR: TFPIE is a workshop designed to unite people interested in functional programming in education, providing an open forum for sharing new and tested ideas, with publication review done after the event.


<details>
  <summary>Details</summary>
Motivation: There is a growing interest in how functional programming can be effectively used in education to improve teaching and learning outcomes.

Method: Organize a one-day workshop (TFPIE) that provides a platform for researchers, teachers, and professionals to discuss and share ideas on functional programming in education. The workshop encourages open discussion and uses a post-event review process for publications.

Result: The workshop brings together participants from various backgrounds to share novel and classroom-tested ideas on the use of functional programming in education, promoting collaboration and discussion.

Conclusion: TFPIE serves as an important event for the community involved in functional programming education, fostering innovation, discussion, and dissemination of best practices.

Abstract: The goal of TFPIE is to gather researchers, teachers and professionals that
use, or are interested in the use of, functional programming in education.
TFPIE aims to be a venue where novel ideas, classroom-tested ideas and
work-in-progress on the use of functional programming in education are
discussed. The one-day workshop will foster a spirit of open discussion by
having a review process for publication after the workshop.

</details>
