{"id": "2508.11034", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.11034", "abs": "https://arxiv.org/abs/2508.11034", "authors": ["Antonio Collante", "Samuel Abedu", "SayedHassan Khatoonabadi", "Ahmad Abdellatif", "Ebube Alor", "Emad Shihab"], "title": "The Impact of Large Language Models (LLMs) on Code Review Process", "comment": null, "summary": "Large language models (LLMs) have recently gained prominence in the field of\nsoftware development, significantly boosting productivity and simplifying\nteamwork. Although prior studies have examined task-specific applications, the\nphase-specific effects of LLM assistance on the efficiency of code review\nprocesses remain underexplored. This research investigates the effect of GPT on\nGitHub pull request (PR) workflows, with a focus on reducing resolution time,\noptimizing phase-specific performance, and assisting developers. We curated a\ndataset of 25,473 PRs from 9,254 GitHub projects and identified GPT-assisted\nPRs using a semi-automated heuristic approach that combines keyword-based\ndetection, regular expression filtering, and manual verification until\nachieving 95% labeling accuracy. We then applied statistical modeling,\nincluding multiple linear regression and Mann-Whitney U test, to evaluate\ndifferences between GPT-assisted and non-assisted PRs, both at the overall\nresolution level and across distinct review phases. Our research has revealed\nthat early adoption of GPT can substantially boost the effectiveness of the PR\nprocess, leading to considerable time savings at various stages. Our findings\nsuggest that GPT-assisted PRs reduced median resolution time by more than 60%\n(9 hours compared to 23 hours for non-assisted PRs). We discovered that\nutilizing GPT can reduce the review time by 33% and the waiting time before\nacceptance by 87%. Analyzing a sample dataset of 300 GPT-assisted PRs, we\ndiscovered that developers predominantly use GPT for code optimization (60%),\nbug fixing (26%), and documentation updates (12%). This research sheds light on\nthe impact of the GPT model on the code review process, offering actionable\ninsights for software teams seeking to enhance workflows and promote seamless\ncollaboration.", "AI": {"tldr": "Using GPT in GitHub PR workflows dramatically speeds up code review and acceptance\u2014resolution times drop by more than 60%, especially by optimizing code, fixing bugs, and updating documentation. This highlights clear productivity gains and practical workflow improvements for software teams.", "motivation": "While large language models are increasingly used in software development, their specific effects during different phases of code review, especially in GitHub pull request workflows, are not well understood. The study seeks to fill this gap by quantifying how LLM assistance (specifically GPT) affects efficiency and outcomes in PR processes.", "method": "The authors curated a large dataset of over 25,000 GitHub pull requests from more than 9,000 projects. They identified GPT-assisted PRs using a heuristic that combines keyword-based detection, regular expression filtering, and manual verification to ensure high labeling accuracy. They then used statistical modeling techniques, including multiple linear regression and the Mann-Whitney U test, to analyze differences between GPT-assisted and non-assisted PRs, both in terms of total resolution time and particular review phases.", "result": "GPT-assisted pull requests saw a median resolution time reduction of over 60% (9 hours vs. 23 hours). Review times dropped by 33%, and waiting times before acceptance decreased by 87%. Usage analysis of 300 GPT-assisted PRs showed GPT was primarily used for code optimization (60%), bug fixing (26%), and documentation (12%).", "conclusion": "Incorporating GPT into PR workflows can greatly improve efficiency and productivity, mainly by reducing resolution and waiting times. The study provides key insights for teams aiming to leverage LLMs to streamline code review and foster better collaboration."}}
{"id": "2508.11110", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11110", "abs": "https://arxiv.org/abs/2508.11110", "authors": ["Mukul Singh", "Gust Verbruggen", "Vu Le", "Sumit Gulwani"], "title": "Diffusion is a code repair operator and generator", "comment": "12 pages", "summary": "Code diffusion models generate code by iteratively removing noise from the\nlatent representation of a code snippet. During later steps of the diffusion\nprocess, when the code snippet has almost converged, differences between\ndiscrete representations of these snippets look like last-mile repairs applied\nto broken or incomplete code. We evaluate the extent to which this resemblance\ncan be exploited to leverage pre-trained code diffusion models for the problem\nof last-mile repair by considering two applications with significant potential.\nFirst, we can leverage the diffusion model for last-mile repair by adding noise\nto a broken code snippet and resuming the diffusion process. Second, we can\nleverage the diffusion model to generate arbitrary amount of training data for\nlast-mile repair tasks (that are computationally more efficient) by sampling an\nintermediate program (input) and the final program (output) from the diffusion\nprocess. We perform experiments on 3 domains (Python, Excel and PowerShell) to\nevaluate applications, as well as analyze properties.", "AI": {"tldr": "The paper shows code diffusion models can fix broken code snippets ('last-mile repair') and efficiently produce training data for such tasks, validated on Python, Excel, and PowerShell experiments.", "motivation": "The motivation is to improve 'last-mile' code repair\u2014the final modifications needed to fix broken or incomplete code\u2014by exploiting the capabilities of code diffusion models.", "method": "The paper explores using pre-trained code diffusion models for last-mile code repair in two ways: (1) by adding noise to broken code and then allowing the diffusion model to repair it via its denoising process, and (2) by generating large amounts of synthetic training data for repair tasks using the diffusion process.", "result": "Experiments are performed on three domains: Python, Excel, and PowerShell. The results demonstrate the potential of both leveraging the diffusion process for repairing code and for creating training data for such tasks.", "conclusion": "Pre-trained code diffusion models can be successfully adapted to last-mile code repair and data generation tasks, offering promising results across multiple programming domains."}}
{"id": "2508.11126", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.11126", "abs": "https://arxiv.org/abs/2508.11126", "authors": ["Huanting Wang", "Jingzhi Gong", "Huawei Zhang", "Zheng Wang"], "title": "AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities", "comment": null, "summary": "AI agentic programming is an emerging paradigm in which large language models\n(LLMs) autonomously plan, execute, and interact with external tools like\ncompilers, debuggers, and version control systems to iteratively perform\ncomplex software development tasks. Unlike conventional code generation tools,\nagentic systems are capable of decomposing high-level goals, coordinating\nmulti-step processes, and adapting their behavior based on intermediate\nfeedback. These capabilities are transforming the software development\npractice. As this emerging field evolves rapidly, there is a need to define its\nscope, consolidate its technical foundations, and identify open research\nchallenges. This survey provides a comprehensive and timely review of AI\nagentic programming. We introduce a taxonomy of agent behaviors and system\narchitectures, and examine core techniques including planning, memory and\ncontext management, tool integration, and execution monitoring. We also analyze\nexisting benchmarks and evaluation methodologies used to assess coding agent\nperformance. Our study identifies several key challenges, including limitations\nin handling long context, a lack of persistent memory across tasks, and\nconcerns around safety, alignment with user intent, and collaboration with\nhuman developers. We discuss emerging opportunities to improve the reliability,\nadaptability, and transparency of agentic systems. By synthesizing recent\nadvances and outlining future directions, this survey aims to provide a\nfoundation for research and development in building the next generation of\nintelligent and trustworthy AI coding agents.", "AI": {"tldr": "This comprehensive survey reviews AI agentic programming, providing a taxonomy, technical overview, and analysis of evaluation methods. It highlights current limitations and research challenges while suggesting directions for building more capable and reliable AI coding agents.", "motivation": "AI agentic programming, where LLMs autonomously handle complex software tasks, is a rapidly developing field. Its capabilities are poised to significantly impact software development, but its scope, technical foundations, and research challenges need to be clarified and organized as the field evolves.", "method": "The paper conducts a comprehensive survey, presenting a taxonomy of agent behaviors and system architectures. It reviews core techniques (planning, memory/context management, tool integration, execution monitoring), benchmarks, and evaluation methodologies. The study synthesizes recent research advances and discusses challenges and future opportunities.", "result": "The survey identifies key challenges for AI agentic programming, such as issues with handling long context, limited persistent memory, safety and alignment concerns, and difficulties collaborating with humans. It also highlights existing benchmarks and evaluation practices, and points out emerging opportunities to enhance reliability, adaptability, and transparency.", "conclusion": "This survey consolidates the state of AI agentic programming by reviewing techniques, challenges, and evaluation methods. It creates a foundational reference to guide future research and development in building more advanced and trustworthy AI coding agents."}}
{"id": "2508.11147", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2508.11147", "abs": "https://arxiv.org/abs/2508.11147", "authors": ["Zhengquan Li", "Zhenhao Li", "Zishuo Ding"], "title": "From Feedback to Failure: Automated Android Performance Issue Reproduction", "comment": "10page, 8 figures", "summary": "Mobile application performance is a vital factor for user experience. Yet,\nperformance issues are notoriously difficult to detect within development\nenvironments, where their manifestations are often less conspicuous and\ndiagnosis proves more challenging. To address this limitation, we propose\nRevPerf, an advanced performance issue reproduction tool that leverages app\nreviews from Google Play to acquire pertinent information. RevPerf employs\nrelevant reviews and prompt engineering to enrich the original review with\nperformance issue details. An execution agent is then employed to generate and\nexecute commands to reproduce the issue. After executing all necessary steps,\nthe system incorporates multifaceted detection methods to identify performance\nissues by monitoring Android logs, GUI changes, and system resource utilization\nduring the reproduction process. Experimental results demonstrate that our\nproposed framework achieves a 70\\% success rate in reproducing performance\nissues on the dataset we constructed and manually validated.", "AI": {"tldr": "RevPerf uses Google Play app reviews and prompt engineering to automatically reproduce and detect mobile performance issues, achieving a 70% success rate.", "motivation": "Mobile app performance greatly impacts user experience, but performance issues are hard to detect and diagnose in development environments. Traditional approaches do not leverage real user feedback or automated reproduction of such problems.", "method": "The authors propose RevPerf, a tool that extracts relevant information from Google Play app reviews using prompt engineering, enriches those reviews with performance issue details, and uses an execution agent to generate and execute commands to reproduce reported issues. Detection involves monitoring logs, GUI changes, and resource usage.", "result": "RevPerf was experimentally evaluated on a constructed dataset and achieved a 70% success rate in reproducing performance issues that were manually validated.", "conclusion": "RevPerf efficiently utilizes user reviews to automate the reproduction and detection of mobile app performance issues, significantly increasing the ability to diagnose real-world problems that affect user experience."}}
{"id": "2508.11297", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.11297", "abs": "https://arxiv.org/abs/2508.11297", "authors": ["Casper Bach"], "title": "Generic Reduction-Based Interpreters (Extended Version)", "comment": null, "summary": "Reduction-based interpreters are traditionally defined in terms of a one-step\nreduction function which systematically decomposes a term into a potential\nredex and context, contracts the redex, and recomposes it to construct the new\nterm to be further reduced. While implementing such interpreters follows a\nsystematic recipe, they often require interpreter engineers to write a\nsubstantial amount of code -- much of it boilerplate. In this paper, we apply\nwell-known techniques from generic programming to reduce boilerplate code in\nreduction-based interpreters.", "AI": {"tldr": "Generic programming techniques can greatly reduce the boilerplate code required to implement reduction-based interpreters, making their development easier and more efficient.", "motivation": "Reduction-based interpreters are systematic to implement but demand significant boilerplate code, making development inefficient.", "method": "The paper applies generic programming techniques to the design and implementation of reduction-based interpreters, aiming to minimize repetitive boilerplate.", "result": "By leveraging generic programming, the authors demonstrate a method to significantly reduce boilerplate code in such interpreters.", "conclusion": "Implementing reduction-based interpreters becomes more efficient and concise with the proposed generic programming strategy, streamlining the engineering process."}}
{"id": "2508.11179", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.11179", "abs": "https://arxiv.org/abs/2508.11179", "authors": ["Pei Liu", "Terry Zhuo", "Jiawei Deng", "Zhenchang Xing", "Qinghua Lu", "Xiaoning Du", "Hongyu Zhan"], "title": "PTMPicker: Facilitating Efficient Pretrained Model Selection for Application Developers", "comment": null, "summary": "The rapid emergence of pretrained models (PTMs) has attracted significant\nattention from both Deep Learning (DL) researchers and downstream application\ndevelopers. However, selecting appropriate PTMs remains challenging because\nexisting methods typically rely on keyword-based searches in which the keywords\nare often derived directly from function descriptions. This often fails to\nfully capture user intent and makes it difficult to identify suitable models\nwhen developers also consider factors such as bias mitigation, hardware\nrequirements, or license compliance. To address the limitations of\nkeyword-based model search, we propose PTMPicker to accurately identify\nsuitable PTMs. We first define a structured template composed of common and\nessential attributes for PTMs and then PTMPicker represents both candidate\nmodels and user-intended features (i.e., model search requests) in this unified\nformat. To determine whether candidate models satisfy user requirements, it\ncomputes embedding similarities for function-related attributes and uses\nwell-crafted prompts to evaluate special constraints such as license compliance\nand hardware requirements. We scraped a total of 543,949 pretrained models from\nHugging Face to prepare valid candidates for selection. PTMPicker then\nrepresented them in the predefined structured format by extracting their\nassociated descriptions. Guided by the extracted metadata, we synthesized a\ntotal of 15,207 model search requests with carefully designed prompts, as no\nsuch search requests are readily available. Experiments on the curated PTM\ndataset and the synthesized model search requests show that PTMPicker can help\nusers effectively identify models,with 85% of the sampled requests successfully\nlocating appropriate PTMs within the top-10 ranked candidates.", "AI": {"tldr": "PTMPicker is a new system for finding pretrained models that goes beyond keyword search by using a structured format and embedding similarities. It better matches user needs including technical and legal constraints, achieving high success rates in experiments.", "motivation": "Selecting appropriate pretrained models (PTMs) is challenging due to limitations of keyword-based search that often miss user intent and requirements like bias, hardware, or license constraints.", "method": "The authors propose PTMPicker, which introduces a structured template of essential PTM attributes. It represents both models and user requests in this format, matches function-related features via embedding similarity, and addresses special constraints (e.g., license, hardware) using tailored prompts. The system processes over 500k models and synthesizes over 15k search requests for evaluation.", "result": "PTMPicker was evaluated on a large dataset of scraped models and synthesized search requests. It successfully identified suitable models for 85% of sampled requests within the top-10 search results.", "conclusion": "PTMPicker significantly improves the relevance and effectiveness of PTM search over traditional keyword-based approaches, especially when special requirements matter."}}
{"id": "2508.11443", "categories": ["cs.PL", "cs.DS"], "pdf": "https://arxiv.org/pdf/2508.11443", "abs": "https://arxiv.org/abs/2508.11443", "authors": ["William Henrich Due", "Martin Elsman", "Troels Henriksen"], "title": "Towards Efficient Hash Maps in Functional Array Languages", "comment": null, "summary": "We present a systematic derivation of a data-parallel implementation of\ntwo-level, static and collision-free hash maps, by giving a functional\nformulation of the Fredman et al. construction, and then flattening it. We\ndiscuss the challenges of providing a flexible, polymorphic, and abstract\ninterface to hash maps in a functional array language, with particular\nattention paid to the problem of dynamically sized keys, which we address by\nassociating each hash map with an arbitrary context. The algorithm is\nimplemented in Futhark, and the achieved GPU execution performance is compared\non simple benchmark problems. We find that our hash maps outperform\nconventional tree/search-based approaches. Furthermore, our implementation is\ncompared against the state-of-the-art cuCollections library, which is\nsignificantly faster for hash map construction, and to a lesser degree for\nlookups. We explain to which extent the performance difference is due to\nlow-level code generation limitation in the Futhark compiler, and to which\nextent it can be attributed to the data-parallel programming vocabulary not\nproviding the constructs necessary to express the equivalent of the algorithms\nused by cuCollections. We end by reflecting to which extent the functional\narray language programming model could, or should, be extended to address these\nweaknesses.", "AI": {"tldr": "This paper develops an efficient, collision-free data-parallel hash map algorithm for GPUs, implemented in Futhark. While outperforming traditional approaches, it still trails behind cuCollections due to compiler and programming model limitations. The authors suggest language model extensions to improve performance.", "motivation": "The motivation is to create efficient, collision-free, static hash maps suitable for parallel execution, addressing challenges in providing flexible and polymorphic interfaces in functional array languages, and solving issues related to dynamically sized keys.", "method": "The authors provide a functional formulation of the Fredman et al. hash map construction, systematically derive a data-parallel implementation, and flatten the algorithm for execution in the Futhark language. They address the challenge of dynamically sized keys using associated contexts. GPU performance benchmarks are used for evaluation and comparison.", "result": "Their implementation in Futhark is superior to tree/search-based methods but lags behind cuCollections in construction speed and, to a lesser extent, lookups. The performance gap arises from Futhark compiler limitations and the absence of specific constructs in data-parallel programming.", "conclusion": "The paper concludes that their data-parallel hash map implementation in Futhark outperforms conventional tree/search-based approaches but falls short compared to the cuCollections library, especially in hash map construction. The limitations stem from the Futhark compiler's code generation and the intrinsic constraints of data-parallel programming. They propose that extending the functional array language model may address these weaknesses."}}
{"id": "2508.11222", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.11222", "abs": "https://arxiv.org/abs/2508.11222", "authors": ["Haonan Zhang", "Dongxia Wang", "Yi Liu", "Kexin Chen", "Jiashui Wang", "Xinlei Ying", "Long Liu", "Wenhai Wang"], "title": "ORFuzz: Fuzzing the \"Other Side\" of LLM Safety -- Testing Over-Refusal", "comment": null, "summary": "Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously\nrejecting benign queries due to overly conservative safety measures - a\ncritical functional flaw that undermines their reliability and usability.\nCurrent methods for testing this behavior are demonstrably inadequate,\nsuffering from flawed benchmarks and limited test generation capabilities, as\nhighlighted by our empirical user study. To the best of our knowledge, this\npaper introduces the first evolutionary testing framework, ORFuzz, for the\nsystematic detection and analysis of LLM over-refusals. ORFuzz uniquely\nintegrates three core components: (1) safety category-aware seed selection for\ncomprehensive test coverage, (2) adaptive mutator optimization using reasoning\nLLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge\nmodel validated to accurately reflect user perception of toxicity and refusal.\nOur extensive evaluations demonstrate that ORFuzz generates diverse, validated\nover-refusal instances at a rate (6.98% average) more than double that of\nleading baselines, effectively uncovering vulnerabilities. Furthermore,\nORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly\ntransferable test cases that achieves a superior 63.56% average over-refusal\nrate across 10 diverse LLMs, significantly outperforming existing datasets.\nORFuzz and ORFuzzSet provide a robust automated testing framework and a\nvaluable community resource, paving the way for developing more reliable and\ntrustworthy LLM-based software systems.", "AI": {"tldr": "Over-refusal is a major problem in LLMs, where benign queries are wrongly rejected. Existing approaches to identify this problem are inadequate. The paper presents ORFuzz, a novel, automated framework with enhanced test generation and human-aligned evaluation to systematically detect over-refusal. ORFuzz produces more diverse and effective tests than prior methods, and its benchmark dataset sets a new standard for evaluating LLM reliability.", "motivation": "Large Language Models (LLMs) often excessively refuse benign user requests due to overly stringent safety measures, which reduces their reliability and usability. Existing evaluation methods and benchmarks are not sufficient to adequately detect and analyze this issue.", "method": "The paper introduces ORFuzz, an evolutionary testing framework designed to systematically uncover and analyze over-refusal behavior in LLMs. ORFuzz comprises three main components: (1) safety category-aware seed selection for diverse test coverage, (2) adaptive mutator optimization using reasoning LLMs to generate challenging test cases, and (3) OR-Judge, a human-aligned evaluation model for perceiving toxicity and refusal accuracy.", "result": "Experimental results show that ORFuzz generates validated over-refusal instances at a rate of 6.98% on average, more than double that of previous methods. ORFuzz also produced a new benchmark dataset, ORFuzzSet, containing 1,855 transferable test cases with a 63.56% average over-refusal rate across 10 LLMs, substantially outperforming existing datasets.", "conclusion": "ORFuzz and the ORFuzzSet benchmark offer an effective automated approach to detecting and diagnosing over-refusal in LLMs, contributing critical tools and resources to the community for improving the reliability and trustworthiness of LLM-based applications."}}
{"id": "2508.11257", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11257", "abs": "https://arxiv.org/abs/2508.11257", "authors": ["Marc Pavel", "Nenad Petrovic", "Lukasz Mazur", "Vahid Zolfaghari", "Fengjunjie Pan", "Alois Knoll"], "title": "Hallucination in LLM-Based Code Generation: An Automotive Case Study", "comment": null, "summary": "Large Language Models (LLMs) have shown significant potential in automating\ncode generation tasks offering new opportunities across software engineering\ndomains. However, their practical application remains limited due to\nhallucinations - outputs that appear plausible but are factually incorrect,\nunverifiable or nonsensical. This paper investigates hallucination phenomena in\nthe context of code generation with a specific focus on the automotive domain.\nA case study is presented that evaluates multiple code LLMs for three different\nprompting complexities ranging from a minimal one-liner prompt to a prompt with\nCovesa Vehicle Signal Specifications (VSS) as additional context and finally to\na prompt with an additional code skeleton. The evaluation reveals a high\nfrequency of syntax violations, invalid reference errors and API knowledge\nconflicts in state-of-the-art models GPT-4.1, Codex and GPT-4o. Among the\nevaluated models, only GPT-4.1 and GPT-4o were able to produce a correct\nsolution when given the most context-rich prompt. Simpler prompting strategies\nfailed to yield a working result, even after multiple refinement iterations.\nThese findings highlight the need for effective mitigation techniques to ensure\nthe safe and reliable use of LLM generated code, especially in safety-critical\ndomains such as automotive software systems.", "AI": {"tldr": "The study finds that state-of-the-art LLMs frequently hallucinate in automotive code generation, with syntax and API errors common. Only context-rich prompts reliably produce correct code, indicating safer model use requires better mitigation techniques and explicit input.", "motivation": "LLMs are promising for automated code generation, especially in software engineering. Their adoption is limited by hallucinations\u2014plausible yet incorrect or nonsensical outputs\u2014which is particularly concerning in safety-critical domains like automotive software.", "method": "A case study evaluates several code-focused LLMs (GPT-4.1, Codex, GPT-4o) using three levels of prompt complexity: a simple one-liner, prompt with Covesa VSS context, and prompt with additional code skeleton.", "result": "High hallucination rates were observed as syntax violations, invalid references, and API conflicts. Only GPT-4.1 and GPT-4o, with most detailed prompts, generated correct solutions. Simpler prompts failed even after refinements.", "conclusion": "Context-rich prompting is essential for reliable code generation with current LLMs, especially for critical domains. Existing models still produce frequent errors unless very explicit information is provided, highlighting the urgent need for mitigation strategies."}}
{"id": "2508.11305", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.11305", "abs": "https://arxiv.org/abs/2508.11305", "authors": ["Xin Wang", "Zhenhao Li", "Zishuo Ding"], "title": "Defects4Log: Benchmarking LLMs for Logging Code Defect Detection and Reasoning", "comment": null, "summary": "Logging code is written by developers to capture system runtime behavior and\nplays a vital role in debugging, performance analysis, and system monitoring.\nHowever, defects in logging code can undermine the usefulness of logs and lead\nto misinterpretations. Although prior work has identified several logging\ndefect patterns and provided valuable insights into logging practices, these\nstudies often focus on a narrow range of defect patterns derived from limited\nsources (e.g., commit histories) and lack a systematic and comprehensive\nanalysis. Moreover, large language models (LLMs) have demonstrated promising\ngeneralization and reasoning capabilities across a variety of code-related\ntasks, yet their potential for detecting logging code defects remains largely\nunexplored.\n  In this paper, we derive a comprehensive taxonomy of logging code defects,\nwhich encompasses seven logging code defect patterns with 14 detailed\nscenarios. We further construct a benchmark dataset, \\dataset, consisting of\n164 developer-verified real-world logging defects. Then we propose an automated\nframework that leverages various prompting strategies and contextual\ninformation to evaluate LLMs' capability in detecting and reasoning logging\ncode defects. Experimental results reveal that LLMs generally struggle to\naccurately detect and reason logging code defects based on the source code\nonly. However, incorporating proper knowledge (e.g., detailed scenarios of\ndefect patterns) can lead to 10.9\\% improvement in detection accuracy. Overall,\nour findings provide actionable guidance for practitioners to avoid common\ndefect patterns and establish a foundation for improving LLM-based reasoning in\nlogging code defect detection.", "AI": {"tldr": "This paper presents a thorough taxonomy and benchmark for logging code defects, showing that LLMs are limited when working with only source code but are noticeably improved with scenario-based guidance, raising accuracy by 10.9%. The work informs practitioners and future research on enhancing LLMs for logging code quality.", "motivation": "Defects in logging code can negatively impact debugging and system monitoring, but prior studies lack comprehensive analyses and do not fully explore LLMs' potential in defect detection.", "method": "The authors create a comprehensive taxonomy of logging code defects (seven patterns, 14 scenarios) and construct a developer-verified benchmark dataset. They then propose an automated framework that uses various prompting and contextual strategies to evaluate large language models (LLMs) in detecting and reasoning about logging defects.", "result": "LLMs struggle with accurate detection and reasoning on source code alone, but providing them with targeted knowledge about defect scenarios boosts detection accuracy by 10.9%.", "conclusion": "Systematic knowledge incorporation can improve LLM performance in logging code defect detection. The study offers guidance for practitioners to avoid defects and sets a foundation for future improvements in LLM-powered reasoning for logging defects."}}
{"id": "2508.11468", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.11468", "abs": "https://arxiv.org/abs/2508.11468", "authors": ["Zhihao Gong", "Zeyu Sun", "Dong Huang", "Qingyuan Liang", "Jie M. Zhang", "Dan Hao"], "title": "TRACY: Benchmarking Execution Efficiency of LLM-Based Code Translation", "comment": null, "summary": "Automatic code translation is a fundamental task in modern software\ndevelopment. While the advent of Large Language Models (LLMs) has significantly\nimproved the correctness of code translation, the critical dimension of\nexecution efficiency remains overlooked. To address this gap, we introduce\nTRACY, the first comprehensive benchmark designed to evaluate the execution\nefficiency of LLM-translated code. TRACY is constructed through an LLM-driven\ntwo-stage pipeline: an initial stage generates a suite of stress tests to\namplify performance differences, followed by an efficiency-oriented task\npruning stage that isolates the efficiency-distinguishing tasks. The resulting\nbenchmark comprises 1,011 code translation tasks across C++, Java, and Python,\neach accompanied by an average of 22.1 verified reference translations and 10\ncomputationally demanding tests. Our extensive evaluation of 26 representative\nLLMs reveals that even top-tier LLMs struggle to consistently produce efficient\ncode translations. For instance, Claude-4-think, the leading model for\ncorrectness, ranks eighth overall when time efficiency is taken into account,\nsurpassed by several smaller open-source models. We further pinpoint that\nalgorithmic flaws and improper resource handling are the most detrimental,\ncausing a median time slowdown of 5.6$\\times$ and memory increase of\n12.0$\\times$, respectively. Our work underscores the necessity of jointly\noptimizing for correctness and efficiency in future LLM-based code translation.", "AI": {"tldr": "TRACY is a new benchmark for measuring efficiency in LLM-generated code translation, revealing major models struggle with performance despite high correctness. Efficiency flaws like bad algorithms and resource handling slow down code drastically. Future LLMs need joint optimization for accuracy and speed.", "motivation": "Existing large language models (LLMs) have greatly improved the correctness of automatic code translation, but ignore execution efficiency, which is crucial in real-world software development.", "method": "The authors introduce TRACY, a new benchmark specifically designed to evaluate the execution efficiency of LLM-generated code translations. TRACY uses a two-stage LLM-driven pipeline: it first generates stress tests to highlight performance differences, then prunes tasks to focus on those that best distinguish efficiency.", "result": "TRACY consists of 1,011 code translation tasks across three languages, with thorough reference translations and demanding efficiency tests. Evaluation of 26 LLMs shows that top models for correctness, like Claude-4-think, perform poorly in efficiency, and smaller open-source models can outperform them. The study identifies algorithmic flaws and poor resource management as causes of severe efficiency losses.", "conclusion": "The study reveals a significant gap in execution efficiency among LLM-translated code and stresses the importance of optimizing both correctness and efficiency. TRACY provides the first comprehensive benchmark to drive future improvements."}}
{"id": "2508.11571", "categories": ["cs.SE", "cs.DM"], "pdf": "https://arxiv.org/pdf/2508.11571", "abs": "https://arxiv.org/abs/2508.11571", "authors": ["Alexander Bakhtin"], "title": "Temporal Network Analysis of Microservice Architectural Degradation", "comment": null, "summary": "Microservice architecture can be modeled as a network of microservices making\ncalls to each other, commonly known as the service dependency graph. Network\nScience can provide methods to study such networks. In particular, temporal\nnetwork analysis is a branch of Network Science that analyzes networks evolving\nwith time. In microservice systems, temporal networks can arise if we examine\nthe architecture of the system across releases or monitor a deployed system\nusing tracing.\n  In this research summary paper, I discuss the challenges in obtaining\ntemporal networks from microservice systems and analyzing them with the\ntemporal network methods. In particular, the most complete temporal network\nthat we could obtain contains 7 time instances and 42 microservices, which\nlimits the potential analysis that could be applied.", "AI": {"tldr": "This paper explores using temporal network analysis for studying microservice architectures but finds limitations in data collection and analysis due to small scale and few time points.", "motivation": "Microservice systems are dynamic, changing across releases or during operation. The traditional static analysis may not capture their evolving dependencies. The paper aims to leverage temporal network analysis to better understand these systems.", "method": "The paper discusses obtaining temporal networks by analyzing service dependency graphs over time, either through system releases or runtime tracing. It then explores application of temporal network science methods to these networks.", "result": "The most comprehensive temporal network constructed had only 7 time points and 42 microservices, which constrained the depth of achievable analysis.", "conclusion": "There are significant challenges both in collecting sufficient temporal data from microservice systems and in applying meaningful temporal network analysis due to limited scale and temporal resolution."}}
