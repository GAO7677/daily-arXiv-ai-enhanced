{"id": "2510.01379", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01379", "abs": "https://arxiv.org/abs/2510.01379", "authors": ["Huashan Chen", "Zhenyu Qi", "Haotang Li", "Hong Chen", "Jinfu Chen", "Kebin Peng", "In Kee Kim", "Kyu Hyung Lee", "Sen He"], "title": "Beyond Single LLMs: Enhanced Code Generation via Multi-Stage Performance-Guided LLM Orchestration", "comment": null, "summary": "While Large Language Models (LLMs) have become the predominant paradigm for\nautomated code generation, current single-model approaches fundamentally ignore\nthe heterogeneous computational strengths that different models exhibit across\nprogramming languages, algorithmic domains, and development stages. This paper\nchallenges the single-model convention by introducing a multi-stage,\nperformance-guided orchestration framework that dynamically routes coding tasks\nto the most suitable LLMs within a structured generate-fix-refine workflow. Our\napproach is grounded in a comprehensive empirical study of 17 state-of-the-art\nLLMs across five programming languages (Python, Java, C++, Go, and Rust) using\nHumanEval-X benchmark. The study, which evaluates both functional correctness\nand runtime performance metrics (execution time, mean/max memory utilization,\nand CPU efficiency), reveals pronounced performance heterogeneity by language,\ndevelopment stage, and problem category. Guided by these empirical insights, we\npresent PerfOrch, an LLM agent that orchestrates top-performing LLMs for each\ntask context through stage-wise validation and rollback mechanisms. Without\nrequiring model fine-tuning, PerfOrch achieves substantial improvements over\nstrong single-model baselines: average correctness rates of 96.22% and 91.37%\non HumanEval-X and EffiBench-X respectively, surpassing GPT-4o's 78.66% and\n49.11%. Beyond correctness gains, the framework delivers consistent performance\noptimizations, improving execution time for 58.76% of problems with median\nspeedups ranging from 17.67% to 27.66% across languages on two benchmarks. The\nframework's plug-and-play architecture ensures practical scalability, allowing\nnew LLMs to be profiled and integrated seamlessly, thereby offering a paradigm\nfor production-grade automated software engineering that adapts to the rapidly\nevolving generative AI landscape.", "AI": {"tldr": "This paper proposes PerfOrch, a framework that orchestrates multiple LLMs for automated code generation, significantly outperforming single-model approaches in correctness and efficiency across several programming languages.", "motivation": "Recent approaches to automated code generation rely heavily on a single large language model, neglecting the fact that different LLMs may perform better or worse depending on language, domain, or stage of development. This paper seeks to address this limitation by leveraging the strengths of a diverse set of LLMs.", "method": "The paper introduces a multi-stage, performance-guided orchestration framework that dynamically assigns coding tasks to the LLM best suited for each step within a generate-fix-refine workflow. The method is grounded in an empirical evaluation of 17 leading LLMs across five programming languages and uses benchmarks that assess both functional correctness and runtime performance.", "result": "The proposed PerfOrch agent achieves average correctness rates of 96.22% and 91.37% on two benchmarks, outperforming GPT-4o and other single-model baselines by a large margin. The framework also optimizes execution time in nearly 59% of cases, with median speedups of 17.67%-27.66% across languages, and supports seamless integration of new models.", "conclusion": "PerfOrch presents a scalable, production-ready orchestration framework that maximizes code generation quality and efficiency by leveraging performance heterogeneity across LLMs, representing an adaptive paradigm for automated software engineering in an evolving AI ecosystem."}}
{"id": "2510.01514", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01514", "abs": "https://arxiv.org/abs/2510.01514", "authors": ["J. Alexander Curtis", "Sharadha Kasiviswanathan", "Nasir Eisty"], "title": "Deciphering WONTFIX: A Mixed-Method Study on Why GitHub Issues Get Rejected", "comment": null, "summary": "Context: The ``wontfix'' label is a widely used yet narrowly understood tool\nin GitHub repositories, indicating that an issue will not be pursued further.\nDespite its prevalence, the impact of this label on project management and\ncommunity dynamics within open-source software development is not clearly\ndefined. Objective: This study examines the prevalence and reasons behind\nissues being labeled as wontfix across various open-source repositories on\nGitHub. Method: Employing a mixed-method approach, we analyze both quantitative\ndata to assess the prevalence of the wontfix label and qualitative data to\nexplore the reasoning that it was used. Data were collected from 3,132 of\nGitHub's most-popular repositories. Later, we employ open coding and thematic\nanalysis to categorize the reasons behind wontfix labels, providing a\nstructured understanding of the issue management landscape. Results: Our\nfindings show that about 30% of projects on GitHub apply the wontfix label to\nsome issues. These issues most often occur on user-submitted issues for bug\nreports and feature requests. The study identified eight common themes behind\nlabeling issues as wontfix, ranging from user-specific control factors to\nmaintainer-specific decisions. Conclusions: The wontfix label is a critical\ntool for managing resources and guiding contributor efforts in GitHub projects.\nHowever, it can also discourage community involvement and obscure the\ntransparency of project management. Understanding these reasons aids project\nmanagers in making informed decisions and fostering efficient collaboration\nwithin open-source communities.", "AI": {"tldr": "This paper analyzes the use of the 'wontfix' label in over 3,000 GitHub repositories, revealing that 30% of projects use it, mostly on user-submitted issues. By categorizing the reasons for applying the label, the study highlights both its utility in resource management and contributor guidance, as well as its potential downsides for community engagement and transparency.", "motivation": "The paper is motivated by the widespread use of the 'wontfix' label in GitHub repositories, alongside the lack of clarity about its impact on project management and community dynamics within open-source development.", "method": "The study uses a mixed-method approach: quantitative analysis to measure how often the 'wontfix' label is used, and qualitative analysis\u2014including open coding and thematic analysis\u2014to understand why the label is applied. Data is drawn from 3,132 popular GitHub repositories.", "result": "About 30% of GitHub projects use the 'wontfix' label for some issues, mainly on user-submitted bug reports and feature requests. Eight common themes were identified in the reasons for applying the label, which span user control factors and maintainer decisions.", "conclusion": "The 'wontfix' label plays a crucial role in managing resources and directing contributor efforts in GitHub projects, but it may also discourage community participation and reduce transparency. Awareness of the reasons behind using this label helps project managers make better decisions and enhance collaboration in open-source communities."}}
{"id": "2510.01635", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01635", "abs": "https://arxiv.org/abs/2510.01635", "authors": ["Yifei Chen", "Sarra Habchi", "Lili Wei"], "title": "MIMIC: Integrating Diverse Personality Traits for Better Game Testing Using Large Language Model", "comment": "13 pages, 7 figures, 6 tables. This paper is accepted by the 40th\n  IEEE/ACM International Conference on Automated Software Engineering, ASE 2025", "summary": "Modern video games pose significant challenges for traditional automated\ntesting algorithms, yet intensive testing is crucial to ensure game quality. To\naddress these challenges, researchers designed gaming agents using\nReinforcement Learning, Imitation Learning, or Large Language Models. However,\nthese agents often neglect the diverse strategies employed by human players due\nto their different personalities, resulting in repetitive solutions in similar\nsituations. Without mimicking varied gaming strategies, these agents struggle\nto trigger diverse in-game interactions or uncover edge cases.\n  In this paper, we present MIMIC, a novel framework that integrates diverse\npersonality traits into gaming agents, enabling them to adopt different gaming\nstrategies for similar situations. By mimicking different playstyles, MIMIC can\nachieve higher test coverage and richer in-game interactions across different\ngames. It also outperforms state-of-the-art agents in Minecraft by achieving a\nhigher task completion rate and providing more diverse solutions. These results\nhighlight MIMIC's significant potential for effective game testing.", "AI": {"tldr": "MIMIC is a new AI framework that gives automated gaming agents different personality traits to better mimic human-like playstyles, resulting in improved and more diverse automated game testing outcomes, especially demonstrated in Minecraft.", "motivation": "Traditional automated testing algorithms struggle with the complex and varied environments of modern video games, and current AI agents (using Reinforcement Learning, Imitation Learning, or Large Language Models) tend to miss the diversity of human playstyles, resulting in repetitive behaviors and poor test coverage.", "method": "The paper introduces MIMIC, a framework that incorporates a range of personality traits into gaming agents, allowing them to simulate varied human gaming strategies in similar situations.", "result": "MIMIC enables agents to achieve greater test coverage and more diverse in-game interactions compared to existing approaches. In Minecraft, it surpassed current state-of-the-art agents in both task completion rate and the diversity of solutions.", "conclusion": "MIMIC displays strong potential for improving the effectiveness of automated game testing by better mimicking human diversity, leading to more thorough and realistic test coverage."}}
{"id": "2510.01740", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01740", "abs": "https://arxiv.org/abs/2510.01740", "authors": ["Kypros Iacovou", "Georgia M. Kapitsaki", "Evangelia Vanezi"], "title": "FOSS-chain: using blockchain for Open Source Software license compliance", "comment": null, "summary": "Open Source Software (OSS) is widely used and carries licenses that indicate\nthe terms under which the software is provided for use, also specifying\nmodification and distribution rules. Ensuring that users are respecting OSS\nlicense terms when creating derivative works is a complex process. Compliance\nissues arising from incompatibilities among licenses may lead to legal\ndisputes. At the same time, the blockchain technology with immutable entries\noffers a mechanism to provide transparency when it comes to licensing and\nensure software changes are recorded. In this work, we are introducing an\nintegration of blockchain and license management when creating derivative\nworks, in order to tackle the issue of OSS license compatibility. We have\ndesigned, implemented and performed a preliminary evaluation of FOSS-chain, a\nweb platform that uses blockchain and automates the license compliance process,\ncovering 14 OSS licenses. We have evaluated the initial prototype version of\nthe FOSS-chain platform via a small scale user study. Our preliminary results\nare promising, demonstrating the potential of the platform for adaptation on\nrealistic software systems.", "AI": {"tldr": "This paper introduces FOSS-chain, a blockchain-powered platform that automates open source license compliance for derivative works, covering 14 OSS licenses. Preliminary user study results indicate it can improve transparency and compatibility management in software systems.", "motivation": "The motivation is driven by the complexity of ensuring compliance with open source software (OSS) licenses, particularly when creating derivative works, and the potential legal issues from license incompatibilities.", "method": "The authors designed, implemented, and performed a preliminary evaluation of FOSS-chain, a web platform that leverages blockchain technology to automate license compliance, covering 14 OSS licenses. The platform was evaluated through a small-scale user study.", "result": "The preliminary results from the user study are promising, showing that FOSS-chain has potential for adaptation in real-world software systems.", "conclusion": "Integrating blockchain with OSS license management can help automate compliance, offer transparency, and mitigate legal risks associated with license incompatibility. FOSS-chain shows promising results as a solution."}}
{"id": "2510.01754", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01754", "abs": "https://arxiv.org/abs/2510.01754", "authors": ["Hina Anwar"], "title": "ARENA: A tool for measuring and analysing the energy efficiency of Android apps", "comment": null, "summary": "To build energy-efficient apps, there is a need to estimate and analyze their\nenergy consumption in typical usage scenarios. The energy consumption of\nAndroid apps could be estimated via software-based and hardware-based\napproaches. Software-based approaches, while easier to implement, are not as\naccurate as hardware-based approaches. The process of measuring the energy\nconsumption of an Android app via a hardware-based approach typically involves\n1) setting up a measurement environment, 2) executing the app under test on a\nmobile device, 3) recording current/voltage data via a hardware device to\nmeasure energy consumption, and 4) cleaning and aggregating data for analyses,\nreports, and visualizations. Specialized scripts are written for selected\nhardware and software components to ensure reliable energy measurements. The\nenergy measurement process is repeated many times and aggregated to remove\nnoise. These steps make the hardware-based energy measurement process\ntime-consuming and not easy to adapt or reproduce. There is a lack of\nopen-source tools available for developers and researchers to take reliable\nenergy measurements via hardware devices. In this paper, we present and\ndemonstrate ARENA, a support tool that enables developers and researchers to\nconnect to a physical measurement device without leaving the comfort of their\nIDE. Developers could use ARENA during development to compare energy\nconsumption between different apps or versions of the same app. ARENA\ncalculates energy consumption on an Android smartphone by executing a test\nscenario on the app under development. Further, ARENA helps aggregate,\nstatistically analyze, report, and visualize the data, allowing developers and\nresearchers to dig into the data directly or visually. We implemented ARENA as\nan IntelliJ and Android Studio plugin.", "AI": {"tldr": "Measuring Android app energy consumption with hardware is accurate but cumbersome. ARENA, a plugin for popular IDEs, simplifies this process by connecting directly to measurement devices, providing integrated data analysis and visualization.", "motivation": "Accurately measuring the energy consumption of Android apps is important for building energy-efficient software, but current hardware-based measurement methods are time-consuming, difficult to adapt or reproduce, and lack adequate open-source support.", "method": "The authors present ARENA, a tool integrated as a plugin for IntelliJ and Android Studio, which connects developers and researchers to physical energy measurement devices from within the IDE. ARENA guides users through executing test scenarios, measuring energy, then aggregating, analyzing, reporting, and visualizing the results.", "result": "ARENA streamlines the process of hardware-based energy measurement for Android apps, making it accessible directly from popular development environments and simplifying data analysis, reporting, and visualization.", "conclusion": "The paper concludes that ARENA addresses the need for an accessible, reproducible, and reliable hardware-based energy measurement tool, significantly reducing the barriers to energy analysis in Android app development."}}
{"id": "2510.01825", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01825", "abs": "https://arxiv.org/abs/2510.01825", "authors": ["Zhenyu Yang", "Yue Pan", "Zhen Yang", "Zhongxing Yu"], "title": "Towards Speeding up Program Repair with Non-Autoregressive Model", "comment": "30 pages, 8 figures, 7 tables. arXiv admin note: substantial text\n  overlap with arXiv:2406.16526", "summary": "Enlightened by the success of machine learning techniques in various\napplication areas, recent years have witnessed a surge of research efforts on\nautomatic program repair (APR) using machine learning techniques. Previous\nmachine learning-based APR techniques essentially modified bugs in the\nautoregressive (AR) manner, which predicts future values based on past values.\nDue to the manner of token-by-token generation, the AR-based APR technique has\na huge time delay. In particular, the delay of the APR model with a large\nnumber of parameters is more serious. To address the issue, we aim to apply the\nnon-autoregressive (NAR) method to the APR task, which can output target code\nin a parallel manner to avoid huge repair delays. However, the naive use of the\nNAR manner for the APR task suffers from the issue of compromised patch\nquality. To effectively adapt the NAR manner for the APR task, we in this paper\npropose NARRepair, the first customized NAR code generation model for the APR\ntask. The NARRepair model features three major novelties, including 1) the\nrepair action predictor for alleviating the over-correction issue, 2) the\ninter-token dependency extractor for alleviating the issue of lacking\ninter-token dependency information, and 3) the two-stage decoder for\nalleviating the issue of lacking contextual information. We evaluated NARRepair\non three widely used datasets in the APR community, and the results show that\n1) compared to other APR techniques, the NARRepair model has the best\nperformance within the limited repair time, and 2) compared to AR-based APR\ntechniques, the repair speed of NARRepair has been increased by 1.4-6.4 times\nin the GPU environment. Overall, the results show that NARRepair has achieved\nstate-of-the-art comprehensive performance in terms of repair speed and\naccuracy.", "AI": {"tldr": "NARRepair is a new non-autoregressive machine learning model for automatic program repair that fixes bugs much faster and with higher accuracy than previous approaches, solving major issues found in autoregressive and naive NAR methods.", "motivation": "Current machine learning-based automatic program repair (APR) uses autoregressive (AR) models, which generate bug fixes token-by-token and suffer significant time delays, especially with large models. Non-autoregressive (NAR) methods could accelerate the process but tend to compromise patch quality.", "method": "The authors propose NARRepair, a customized non-autoregressive (NAR) code generation model for APR. NARRepair incorporates (1) a repair action predictor to reduce over-correction, (2) an inter-token dependency extractor to restore dependency information between code tokens, and (3) a two-stage decoder to enhance contextual awareness for patch generation.", "result": "Empirical evaluation on three standard APR datasets shows NARRepair outperforms existing APR approaches in both repair speed and accuracy. NARRepair is 1.4-6.4 times faster than autoregressive techniques in GPU settings and achieves the best performance within constrained repair times.", "conclusion": "NARRepair successfully addresses the speed and quality limitations of previous machine learning-based APR approaches, providing state-of-the-art results for both fast and high-quality program repair."}}
{"id": "2510.01960", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01960", "abs": "https://arxiv.org/abs/2510.01960", "authors": ["Victor Lira", "Paulo Borba", "Rodrigo Bonif\u00e1cio", "Galileu Santos e Matheus barbosa"], "title": "RefFilter: Improving Semantic Conflict Detection via Refactoring-Aware Static Analysis", "comment": null, "summary": "Detecting semantic interference remains a challenge in collaborative software\ndevelopment. Recent lightweight static analysis techniques improve efficiency\nover SDG-based methods, but they still suffer from a high rate of false\npositives. A key cause of these false positives is the presence of\nbehavior-preserving code refactorings, which current techniques cannot\neffectively distinguish from changes that impact behavior and can interfere\nwith others. To handle this problem we present RefFilter, a refactoring-aware\ntool for semantic interference detection. It builds on existing static\ntechniques by incorporating automated refactoring detection to improve\nprecision. RefFilter discards behavior-preserving refactorings from reports,\nreducing false positives while preserving detection coverage. To evaluate\neffectiveness and scalability, use two datasets: a labeled dataset with 99\nscenarios and ground truth, and a novel dataset of 1,087 diverse merge\nscenarios that we have built. Experimental results show that RefFilter reduces\nfalse positives by nearly 32% on the labeled dataset. While this reduction\ncomes with a non significant increase in false negatives, the overall gain in\nprecision significantly outweighs the minor trade-off in recall. These findings\ndemonstrate that refactoring-aware interference detection is a practical and\neffective strategy for improving merge support in modern development workflows.", "AI": {"tldr": "RefFilter is a tool that enhances semantic interference detection in code merges by automatically recognizing and excluding behavior-preserving refactorings, significantly reducing false positives. Evaluations demonstrate notable precision improvements with minimal cost to recall, supporting better merge workflows in software development.", "motivation": "Collaborative software development faces challenges in detecting semantic interference caused by code changes that may impact others' work. Existing lightweight static analysis techniques, although more efficient than SDG-based methods, have a high rate of false positives, mainly due to inability to distinguish behavior-preserving code refactorings from real behavioral changes.", "method": "The authors propose 'RefFilter', a tool that integrates automated refactoring detection into static interference analysis. RefFilter filters out behavior-preserving refactorings from reports, aiming to minimize false positives without sacrificing significant detection coverage. Their methodology involves evaluating RefFilter on two datasets: a labeled dataset with ground truth, and a large set of real-world merge scenarios.", "result": "RefFilter achieves a nearly 32% reduction in false positives in the labeled dataset, with only a minor, non-significant increase in false negatives. This improvement in precision substantially outweighs the small trade-off in recall.", "conclusion": "Refactoring-aware semantic interference detection, as implemented in RefFilter, can be a practical and effective solution for supporting code merging in collaborative software development. The approach improves efficiency and accuracy without significant loss of coverage."}}
{"id": "2510.01994", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01994", "abs": "https://arxiv.org/abs/2510.01994", "authors": ["Chen Yang", "Lin Yang", "Ziqi Wang", "Dong Wang", "Jianyi Zhou", "Junjie Chen"], "title": "Clarifying Semantics of In-Context Examples for Unit Test Generation", "comment": "accepted in the research track of ASE 2025", "summary": "Recent advances in large language models (LLMs) have enabled promising\nperformance in unit test generation through in-context learning (ICL). However,\nthe quality of in-context examples significantly influences the effectiveness\nof generated tests-poorly structured or semantically unclear test examples\noften lead to suboptimal outputs. In this paper, we propose CLAST, a novel\ntechnique that systematically refines unit tests to improve their semantic\nclarity, thereby enhancing their utility as in-context examples. The approach\ndecomposes complex tests into logically clearer ones and improves semantic\nclarity through a combination of program analysis and LLM-based rewriting. We\nevaluated CLAST on four open-source and three industrial projects. The results\ndemonstrate that CLAST largely outperforms UTgen, the state-of-the-art\nrefinement technique, in both preserving test effectiveness and enhancing\nsemantic clarity. Specifically, CLAST fully retains the original effectiveness\nof unit tests, while UTgen reduces compilation success rate (CSR), pass rate\n(PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%,\n35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user\nstudy preferred the semantic clarity of CLAST-refined tests. Notably,\nincorporating CLAST-refined tests as examples effectively improves ICL-based\nunit test generation approaches such as RAGGen and TELPA, resulting in an\naverage increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for\ngenerated tests, compared to incorporating UTgen-refined tests. The insights\nfrom the follow-up user study not only reinforce CLAST's potential impact in\nsoftware testing practice but also illuminate avenues for future research.", "AI": {"tldr": "CLAST is a new method for refining unit tests to improve clarity and effectiveness as in-context examples for LLMs. It substantially outperforms prior approaches on technical metrics and user preference, and greatly boosts test generation quality.", "motivation": "Current large language model (LLM) based unit test generation via in-context learning (ICL) is highly sensitive to the quality and clarity of the provided test examples. Poorly structured or unclear test examples degrade generation performance.", "method": "The authors propose CLAST, a technique that automatically refines unit tests to increase their semantic clarity via decomposition of complex tests and a combination of program analysis and LLM-based rewriting.", "result": "CLAST significantly outperforms the previous state-of-the-art (UTgen) on multiple metrics such as compilation success rate, pass rate, test coverage, and mutation score, while maintaining test effectiveness. Over 85% of participants preferred CLAST-refined tests in terms of clarity, and its use as in-context examples improved LLM-based test generation approaches by at least 25-45% on key metrics compared to UTgen.", "conclusion": "CLAST presents a superior method for refining in-context test examples, enhancing both the clarity and effectiveness of LLM-based unit test generation. It preserves test efficacy and has strong user preference and empirical benefits, signaling its promising impact and guiding future research in software testing."}}
{"id": "2510.02002", "categories": ["cs.SE", "D.2.1; D.2.2; D.2.3; D.3.4; G.1.6"], "pdf": "https://arxiv.org/pdf/2510.02002", "abs": "https://arxiv.org/abs/2510.02002", "authors": ["Maximilian Kratz", "Steffen Zschaler", "Jens Kosiol", "Gabriele Taentzer"], "title": "Automatic Generation of Combinatorial Reoptimisation Problem Specifications: A Vision", "comment": null, "summary": "Once an optimisation problem has been solved, the solution may need\nadaptation when contextual factors change. This challenge, also known as\nreoptimisation, has been addressed in various problem domains, such as railway\ncrew rescheduling, nurse rerostering, or aircraft recovery. This requires a\nmodified problem to be solved again to ensure that the adapted solution is\noptimal in the new context. However, the new optimisation problem differs\nnotably from the original problem: (i) we want to make only minimal changes to\nthe original solution to minimise the impact; (ii) we may be unable to change\nsome parts of the original solution (e.g., because they refer to past\nallocations); and (iii) we need to derive a change script from the original\nsolution to the new solution. In this paper, we argue that Model-Driven\nEngineering (MDE) - in particular, the use of declarative modelling languages\nand model transformations for the high-level specification of optimisation\nproblems - offers new opportunities for the systematic derivation of\nreoptimisation problems from the original optimisation problem specification.\nWe focus on combinatorial reoptimisation problems and provide an initial\ncategorisation of changing problems and strategies for deriving the\ncorresponding reoptimisation specifications. We introduce an initial\nproof-of-concept implementation based on the GIPS (Graph-Based (Mixed) Integer\nLinear Programming Problem Specification) tool and apply it to an example\nresource-allocation problem: the allocation of teaching assistants to teaching\nsessions.", "AI": {"tldr": "This paper addresses the challenge of adapting optimisation solutions to changing contexts (reoptimisation), proposing that Model-Driven Engineering can systematically derive modified problems. A proof-of-concept implementation in the GIPS tool demonstrates these ideas in a resource allocation scenario.", "motivation": "The motivation lies in the need for efficient and minimally disruptive adaptation of optimisation solutions when contextual factors change, common in domains like crew scheduling or resource allocation.", "method": "The authors propose leveraging declarative modelling languages and model transformations within MDE, provide a categorisation of problem changes and strategies, and implement a proof-of-concept using the GIPS tool applied to a resource allocation example.", "result": "The work delivers an initial categorisation framework for reoptimisation specification strategies and a working prototype via the GIPS tool, showing feasibility for teaching assistant allocation problems.", "conclusion": "The paper concludes that Model-Driven Engineering (MDE) provides a systematic approach for deriving reoptimisation problems from original optimisation specifications, demonstrated via a proof-of-concept using the GIPS tool."}}
{"id": "2510.02007", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02007", "abs": "https://arxiv.org/abs/2510.02007", "authors": ["Justus Bogner", "Roberto Verdecchia"], "title": "ACM SIGSOFT SEN Empirical Software Engineering: Introducing Our New Regular Column", "comment": "Published in ACM SIGSOFT Software Engineering Notes (SIGSOFT-SEN).\n  Volume 50, Issue 4, 2025", "summary": "From its early foundations in the 1970s, empirical software engineering (ESE)\nhas evolved into a mature research discipline that embraces a plethora of\ndifferent topics, methodologies, and industrial practices. Despite its\nremarkable progress, the ESE research field still needs to keep evolving, as\nnew impediments, shortcoming, and technologies emerge. Research\nreproducibility, limited external validity, subjectivity of reviews, and\nporting research results to industrial practices are just some examples of the\ndrivers for improvements to ESE research. Additionally, several facets of ESE\nresearch are not documented very explicitly, which makes it difficult for\nnewcomers to pick them up. With this new regular ACM SIGSOFT SEN column\n(SEN-ESE), we introduce a venue for discussing meta-aspects of ESE research,\nranging from general topics such as the nature and best practices for\nreplication packages, to more nuanced themes such as statistical methods,\ninterview transcription tools, and publishing interdisciplinary research. Our\naim for the column is to be a place where we can regularly spark conversations\non ESE topics that might not often be touched upon or are left implicit.\nContributions to this column will be grounded in expert interviews, focus\ngroups, surveys, and position pieces, with the goal of encouraging reflection\nand improvement in how we conduct, communicate, teach, and ultimately improve\nESE research. Finally, we invite feedback from the ESE community on\nchallenging, controversial, or underexplored topics, as well as suggestions for\nvoices you would like to hear from. While we cannot promise to act on every\nidea, we aim to shape this column around the community interests and are\ngrateful for all contributions.", "AI": {"tldr": "This paper introduces a new regular column in ACM SIGSOFT SEN dedicated to discussing meta-level issues, challenges, and best practices in empirical software engineering (ESE). The goal is to promote reflection, conversation, and improvement in ESE through expert input and community feedback.", "motivation": "Empirical Software Engineering (ESE) has progressed significantly since the 1970s, but still faces challenges like research reproducibility, limited external validity, reviewer subjectivity, and the transfer of research to industrial practice. Additionally, many important aspects of ESE are not well-documented, making it harder for newcomers to engage with the field.", "method": "The paper introduces a regular ACM SIGSOFT SEN column (SEN-ESE), which will feature expert interviews, focus groups, surveys, and position papers. The column serves as a platform to discuss meta-aspects of ESE, covering both broad and specific topics, and encourages community engagement and feedback.", "result": "The result is the establishment of the SEN-ESE column as an open venue for ongoing discussions about ESE research practices, challenges, and improvements. It is designed to address underexplored or implicit topics and foster reflection and dialogue within the ESE community.", "conclusion": "The SEN-ESE column aims to spark regular and inclusive conversations about the meta-aspects of ESE to improve the way research is conducted, communicated, and taught. Community feedback and contributions are encouraged to better address emerging and underexplored issues within ESE."}}
{"id": "2510.02165", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02165", "abs": "https://arxiv.org/abs/2510.02165", "authors": ["Peter Wauyo", "Dalia Bwiza", "Alain Murara", "Edwin Mugume", "Eric Umuhoza"], "title": "Towards fairer public transit: Real-time tensor-based multimodal fare evasion and fraud detection", "comment": "10 pages", "summary": "This research introduces a multimodal system designed to detect fraud and\nfare evasion in public transportation by analyzing closed circuit television\n(CCTV) and audio data. The proposed solution uses the Vision Transformer for\nVideo (ViViT) model for video feature extraction and the Audio Spectrogram\nTransformer (AST) for audio analysis. The system implements a Tensor Fusion\nNetwork (TFN) architecture that explicitly models unimodal and bimodal\ninteractions through a 2-fold Cartesian product. This advanced fusion technique\ncaptures complex cross-modal dynamics between visual behaviors (e.g.,\ntailgating,unauthorized access) and audio cues (e.g., fare transaction sounds).\nThe system was trained and tested on a custom dataset, achieving an accuracy of\n89.5%, precision of 87.2%, and recall of 84.0% in detecting fraudulent\nactivities, significantly outperforming early fusion baselines and exceeding\nthe 75% recall rates typically reported in state-of-the-art transportation\nfraud detection systems. Our ablation studies demonstrate that the tensor\nfusion approach provides a 7.0% improvement in the F1 score and an 8.8% boost\nin recall compared to traditional concatenation methods. The solution supports\nreal-time detection, enabling public transport operators to reduce revenue\nloss, improve passenger safety, and ensure operational compliance.", "AI": {"tldr": "The paper presents a multimodal, transformer-based system for real-time public transport fraud detection, using advanced tensor fusion of video and audio signals. It significantly outperforms existing methods in accuracy and recall, supporting enhanced operational outcomes for transportation providers.", "motivation": "The motivation is to address revenue loss, passenger safety, and operational compliance in public transport by improving detection of fraud and fare evasion.", "method": "The paper proposes a multimodal system utilizing ViViT for video features, AST for audio analysis, and a Tensor Fusion Network (TFN) to model both unimodal and cross-modal interactions, specifically through a 2-fold Cartesian product for explicit fusion of visual and audio cues.", "result": "The system trained on a custom dataset achieved 89.5% accuracy, 87.2% precision, and 84.0% recall, outperforming early fusion baselines and typical state-of-the-art (75% recall). Ablation studies show the tensor fusion method gives a 7.0% F1 and 8.8% recall improvement over simple concatenation.", "conclusion": "The system substantially improves fraud detection in public transport, supporting real-time usage and helping operators reduce losses, improve safety, and ensure compliance."}}
{"id": "2510.02166", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02166", "abs": "https://arxiv.org/abs/2510.02166", "authors": ["Fatou Ndiaye Mbodji", "El-hacen Diallo", "Jordan Samhi", "Kui Liu", "Jacques Klein", "Tegawend\u00e9 F. Bissyande"], "title": "SIEVE: Towards Verifiable Certification for Code-datasets", "comment": "5", "summary": "Code agents and empirical software engineering rely on public code datasets,\nyet these datasets lack verifiable quality guarantees. Static 'dataset cards'\ninform, but they are neither auditable nor do they offer statistical\nguarantees, making it difficult to attest to dataset quality. Teams build\nisolated, ad-hoc cleaning pipelines. This fragments effort and raises cost. We\npresent SIEVE, a community-driven framework. It turns per-property checks into\nConfidence Cards-machine-readable, verifiable certificates with anytime-valid\nstatistical bounds. We outline a research plan to bring SIEVE to maturity,\nreplacing narrative cards with anytime-verifiable certification. This shift is\nexpected to lower quality-assurance costs and increase trust in code-datasets.", "AI": {"tldr": "Current public code datasets lack auditable quality guarantees, relying on narrative descriptions. The SIEVE framework proposes replacing these with machine-readable, verifiable 'Confidence Cards' featuring statistical guarantees, aiming to reduce costs and increase dataset trustworthiness.", "motivation": "Public code datasets are widely used in code agents and empirical software engineering. However, their quality is often unknown, as current documentation (like dataset cards) lacks verifiable or statistically guaranteed information. This leads to inconsistent, costly, and fragmented dataset cleaning by individual teams.", "method": "The paper introduces SIEVE, a community-driven framework that transforms dataset property checks into 'Confidence Cards.' These cards are machine-readable, provide verifiable certification, and deliver statistical quality guarantees that are valid anytime. The authors outline a research plan for developing SIEVE further.", "result": "SIEVE can replace traditional, narrative dataset cards with verifiable, statistical certifications. This approach promises to reduce the costs associated with quality assurance and build greater trust in public code datasets.", "conclusion": "SIEVE represents a shift in dataset quality management by providing auditable and statistically sound quality guarantees through Confidence Cards, rather than informal descriptions. This is expected to streamline quality assurance and elevate overall trust in shared code datasets."}}
{"id": "2510.02169", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.02169", "abs": "https://arxiv.org/abs/2510.02169", "authors": ["Vadim Safronov", "Anthony McCaigue", "Nicholas Allott", "Andrew Martin"], "title": "TAIBOM: Bringing Trustworthiness to AI-Enabled Systems", "comment": "This paper has been accepted at the First International Workshop on\n  Security and Privacy-Preserving AI/ML (SPAIML 2025), co-located with the 28th\n  European Conference on Artificial Intelligence (ECAI 2025)", "summary": "The growing integration of open-source software and AI-driven technologies\nhas introduced new layers of complexity into the software supply chain,\nchallenging existing methods for dependency management and system assurance.\nWhile Software Bills of Materials (SBOMs) have become critical for enhancing\ntransparency and traceability, current frameworks fall short in capturing the\nunique characteristics of AI systems -- namely, their dynamic, data-driven\nnature and the loosely coupled dependencies across datasets, models, and\nsoftware components. These challenges are compounded by fragmented governance\nstructures and the lack of robust tools for ensuring integrity, trust, and\ncompliance in AI-enabled environments.\n  In this paper, we introduce Trusted AI Bill of Materials (TAIBOM) -- a novel\nframework extending SBOM principles to the AI domain. TAIBOM provides (i) a\nstructured dependency model tailored for AI components, (ii) mechanisms for\npropagating integrity statements across heterogeneous AI pipelines, and (iii) a\ntrust attestation process for verifying component provenance. We demonstrate\nhow TAIBOM supports assurance, security, and compliance across AI workflows,\nhighlighting its advantages over existing standards such as SPDX and CycloneDX.\nThis work lays the foundation for trustworthy and verifiable AI systems through\nstructured software transparency.", "AI": {"tldr": "TAIBOM is a new framework that extends software bill of materials standards to the specifics of AI technologies, improving integrity, trust, and compliance in AI workflows where traditional SBOMs fall short.", "motivation": "The integration of open-source software and AI technologies has created complexity in software supply chains, with standard dependency management and assurance methods proving insufficient. Current SBOM frameworks do not adequately address the unique challenges posed by AI systems, such as their dynamic and data-driven nature, as well as loosely coupled dependencies.", "method": "This paper introduces the Trusted AI Bill of Materials (TAIBOM) framework that extends SBOM principles specifically to AI. TAIBOM consists of a structured model for AI dependencies, mechanisms to propagate integrity statements throughout diverse AI pipelines, and a trust attestation process to verify the provenance of components.", "result": "TAIBOM enhances assurance, security, and compliance in AI workflows. It provides better transparency and traceability than existing standards (e.g., SPDX, CycloneDX) and addresses intricate needs specific to AI systems.", "conclusion": "TAIBOM establishes a foundation for trustworthy and verifiable AI systems by adapting and extending software transparency practices with tailored approaches for AI."}}
{"id": "2510.02185", "categories": ["cs.SE", "cs.CR", "cs.MA", "D.2.4; F.3.1"], "pdf": "https://arxiv.org/pdf/2510.02185", "abs": "https://arxiv.org/abs/2510.02185", "authors": ["Paschal C. Amusuo", "Dongge Liu", "Ricardo Andres Calvo Mendez", "Jonathan Metzman", "Oliver Chang", "James C. Davis"], "title": "FalseCrashReducer: Mitigating False Positive Crashes in OSS-Fuzz-Gen Using Agentic AI", "comment": "12 pages, 2 figures", "summary": "Fuzz testing has become a cornerstone technique for identifying software bugs\nand security vulnerabilities, with broad adoption in both industry and\nopen-source communities. Directly fuzzing a function requires fuzz drivers,\nwhich translate random fuzzer inputs into valid arguments for the target\nfunction. Given the cost and expertise required to manually develop fuzz\ndrivers, methods exist that leverage program analysis and Large Language Models\nto automatically generate these drivers. However, the generated fuzz drivers\nfrequently lead to false positive crashes, especially in functions highly\nstructured input and complex state requirements. This problem is especially\ncrucial in industry-scale fuzz driver generation efforts like OSS-Fuzz-en, as\nreporting false positive crashes to maintainers impede trust in both the system\nand the team.\n  This paper presents two AI-driven strategies to reduce false positives in\nOSS-Fuzz-Gen, a multi-agent system for automated fuzz driver generation. First,\nconstraint-based fuzz driver generation proactively enforces constraints on a\nfunction's inputs and state to guide driver creation. Second, context-based\ncrash validation reactively analyzes function callers to determine whether\nreported crashes are feasible from program entry points. Using 1,500 benchmark\nfunctions from OSS-Fuzz, we show that these strategies reduce spurious crashes\nby up to 8%, cut reported crashes by more than half, and demonstrate that\nfrontier LLMs can serve as reliable program analysis agents. Our results\nhighlight the promise and challenges of integrating AI into large-scale fuzzing\npipelines.", "AI": {"tldr": "Developing fuzz drivers manually is costly and automated methods often report false positive crashes. This paper introduces AI-based strategies\u2014constraint enforcement and crash validation\u2014to reduce such errors in OSS-Fuzz-Gen. The approach cuts spurious crashes and reports by half, leveraging LLMs for reliable program analysis. AI integration advances large-scale fuzzing but brings new challenges as well.", "motivation": "Manually developing fuzz drivers for fuzz testing is resource-intensive and requires significant expertise. Automated solutions often generate drivers with high false positive crash rates, especially for functions with structured inputs and complex states. This issue undermines trust in large-scale efforts like OSS-Fuzz-Gen.", "method": "The paper introduces two AI-driven methods: (1) constraint-based fuzz driver generation, which enforces input and state constraints during driver creation, and (2) context-based crash validation, which analyzes function callers to validate whether reported crashes could occur from program entry points.", "result": "These strategies, tested on 1,500 benchmark functions from OSS-Fuzz, reduced spurious crashes by up to 8% and more than halved reported crashes. The study also shows that state-of-the-art LLMs can reliably serve as program analysis agents.", "conclusion": "AI-driven strategies can significantly reduce false positives in automated fuzz driver generation, improving the reliability and credibility of large-scale fuzzing systems like OSS-Fuzz-Gen. Integration of AI presents both promise and challenges for future fuzzing pipelines."}}
