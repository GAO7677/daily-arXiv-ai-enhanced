<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 21]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Evolution of Kernels: Automated RISC-V Kernel Optimization with Large Language Models](https://arxiv.org/abs/2509.14265)
*Siyuan Chen,Zhichao Lu,Qingfu Zhang*

Main category: cs.SE

TL;DR: The paper introduces EoK, an LLM-based framework for automated kernel design in domains with limited references like RISC-V. By leveraging mined human optimization ideas and RAG-enhanced context, EoK achieves notable speedups and outperforms experts and previous methods, demonstrating the value of human-guided LLMs in new hardware platforms.


<details>
  <summary>Details</summary>
Motivation: Existing large language model-driven kernel optimization works well in domains like CUDA due to abundant documentation and mature codebases, but its effectiveness for emerging hardware like RISC-V, which lacks reference materials, is unknown.

Method: The authors propose EoK (Evolution of Kernels), an LLM-based evolutionary program search framework. EoK mines optimization ideas (design principles and actionable thoughts) from development histories of established kernel libraries, then uses Retrieval-Augmented Generation (RAG) to enrich LLM explorations with RISC-V-specific context, focusing on proven techniques.

Result: EoK achieves a median 1.27x speedup, outperforms human experts across 80 kernel design tasks, and improves prior LLM-based methods by 20%.

Conclusion: Incorporating reusable human experience and optimization ideas via LLMs significantly enhances automated kernel design for reference-scarce domains, establishing the potential of LLM-based optimization in emerging hardware.

Abstract: Automated kernel design is critical for overcoming software ecosystem
barriers in emerging hardware platforms like RISC-V. While large language
models (LLMs) have shown promise for automated kernel optimization,
demonstrating success in CUDA domains with comprehensive technical documents
and mature codebases, their effectiveness remains unproven for reference-scarce
domains like RISC-V. We present Evolution of Kernels (EoK), a novel LLM-based
evolutionary program search framework that automates kernel design for domains
with limited reference material. EoK mitigates reference scarcity by mining and
formalizing reusable optimization ideas (general design principles + actionable
thoughts) from established kernel libraries' development histories; it then
guides parallel LLM explorations using these ideas, enriched via
Retrieval-Augmented Generation (RAG) with RISC-V-specific context, prioritizing
historically effective techniques. Empirically, EoK achieves a median 1.27x
speedup, surpassing human experts on all 80 evaluated kernel design tasks and
improving upon prior LLM-based automated kernel design methods by 20%. These
results underscore the viability of incorporating human experience into
emerging domains and highlight the immense potential of LLM-based automated
kernel optimization.

</details>


### [2] [Automated and Context-Aware Code Documentation Leveraging Advanced LLMs](https://arxiv.org/abs/2509.14273)
*Swapnil Sharma Sarker,Tanzina Taher Ifty*

Main category: cs.SE

TL;DR: This paper introduces a new dataset for template-based Javadoc generation and evaluates open-source LLMs for this task, finding LLaMA 3.1 highly effective and practical as an alternative to closed-source systems.


<details>
  <summary>Details</summary>
Motivation: Manual code documentation is tedious but essential for software maintainability. Automated documentation generation, particularly for template-based documentation (like Javadoc), is underexplored, partly due to the absence of specialized datasets.

Method: Development of a novel, context-aware dataset for Javadoc generation, covering modern language features and frameworks. Evaluation of five open-source LLMs (LLaMA-3.1, Gemma-2, Phi-3, Mistral, Qwen-2.5) in zero-shot, few-shot, and fine-tuned settings, followed by a comparative performance analysis.

Result: LLaMA 3.1 consistently outperformed other open-source LLMs and proved effective for automated Javadoc generation, providing a strong alternative to proprietary solutions.

Conclusion: The tailored dataset and LLaMA 3.1 enable practical, context-aware, automated Javadoc generation with publicly accessible models, closing a significant gap in software documentation research.

Abstract: Code documentation is essential to improve software maintainability and
comprehension. The tedious nature of manual code documentation has led to much
research on automated documentation generation. Existing automated approaches
primarily focused on code summarization, leaving a gap in template-based
documentation generation (e.g., Javadoc), particularly with publicly available
Large Language Models (LLMs). Furthermore, progress in this area has been
hindered by the lack of a Javadoc-specific dataset that incorporates modern
language features, provides broad framework/library coverage, and includes
necessary contextual information. This study aims to address these gaps by
developing a tailored dataset and assessing the capabilities of publicly
available LLMs for context-aware, template-based Javadoc generation. In this
work, we present a novel, context-aware dataset for Javadoc generation that
includes critical structural and semantic information from modern Java
codebases. We evaluate five open-source LLMs (including LLaMA-3.1, Gemma-2,
Phi-3, Mistral, Qwen-2.5) using zero-shot, few-shot, and fine-tuned setups and
provide a comparative analysis of their performance. Our results demonstrate
that LLaMA 3.1 performs consistently well and is a reliable candidate for
practical, automated Javadoc generation, offering a viable alternative to
proprietary systems.

</details>


### [3] [Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization](https://arxiv.org/abs/2509.14279)
*Robert Tjarko Lange,Qi Sun,Aaditya Prasad,Maxence Faldor,Yujin Tang,David Ha*

Main category: cs.SE

TL;DR: They introduce robust-kbench, a new benchmark for evaluating CUDA kernel performance, and a novel agentic framework that leverages LLMs to automate CUDA kernel discovery, optimization, and verification, beating PyTorch in practical tasks.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based approaches for software engineering often neglect low-level CUDA kernel optimization and rely on benchmarks with insufficient diversity and exploitable loopholes, impeding genuine assessment of kernel generalization and performance.

Method: The authors build a sequential workflow consisting of translation from PyTorch to CUDA kernels, runtime optimization via an evolutionary meta-generation process, correctness verification using LLM-based verifiers, and rigorous evaluation using their new benchmark, robust-kbench.

Result: The new approach outperforms PyTorch implementations in practical applications by producing more optimized CUDA kernels, effectively fusing operations, deploying diverse runtime strategies, and accurately classifying incorrect kernels, thereby improving hardware verification efficiency.

Conclusion: The proposed agentic framework and robust-kbench benchmark enable significant improvements in the generation, correctness, and optimization of CUDA kernels by LLMs over high-level PyTorch implementations.

Abstract: Recent advances in large language models (LLMs) demonstrate their
effectiveness in scaling test-time compute for software engineering tasks.
However, these approaches often focus on high-level solutions, with limited
attention to optimizing low-level CUDA kernel implementations. Additionally,
existing kernel generation benchmarks suffer from exploitable loopholes and
insufficient diversity in testing conditions, hindering true generalization
assessment. To address these limitations, we introduce robust-kbench, a new
benchmark for rigorous evaluation of kernel performance and correctness across
varied scenarios. Furthermore, we present a comprehensive agentic framework
that automates CUDA kernel discovery, verification, and optimization. This
pipeline enables frontier LLMs to translate torch code to CUDA kernels and
iteratively improve their runtime within our robust evaluation setting. Our
sequential workflow first translates PyTorch code into equivalent CUDA kernels.
It then optimizes their runtime using a novel evolutionary meta-generation
procedure tailored to the CUDA ecosystem, guided by LLM-based verifiers for
correctness and efficient filtering. Evaluated on robust-kbench, our approach
produces CUDA kernels outperforming torch implementations for practical
applications, including forward and backward passes. It can fuse operations and
deploy various runtime optimization strategies. The verifier workflow
accurately classifies incorrect kernels, enhancing hardware verification
efficiency.

</details>


### [4] [SCoGen: Scenario-Centric Graph-Based Synthesis of Real-World Code Problems](https://arxiv.org/abs/2509.14281)
*Xifeng Yao,Dongyu Lang,Wu Zhang,Xintong Guo,Huarui Xie,Yinhao Ni,Ping Liu,Guang Shen,Yi Bai,Dandan Tu,Changzheng Zhang*

Main category: cs.SE

TL;DR: The paper introduces a graph-based framework that synthesizes diverse, realistic code problems from real-world data (Stack Overflow, Kaggle) and improves code language model evaluation, outperforming existing models on empirical benchmarks.


<details>
  <summary>Details</summary>
Motivation: Code large language models have made great progress but are limited by a lack of authentic real-world coding problems, which constrains their further development. This paper addresses the need for a scalable method to generate realistic, diverse code problems.

Method: The paper proposes a novel problem synthesis framework that extracts domain knowledge, domain skills, and coding skills from real-world datasets like Stack Overflow and Kaggle. It maps these into a scenario-centric graph representing practical application scenarios and designs a graph-based sampling strategy to control the diversity and complexity of generated problems.

Result: Experimental results show the proposed framework outperforms current state-of-the-art large language models (coders and general-purpose) on several real-world coding benchmarks in terms of performance and problem quality.

Conclusion: The framework enables scalable creation of authentic code problems, advancing the development and evaluation of code large language models by closing the gap between real-world needs and available benchmarks.

Abstract: Significant advancements have been made in the capabilities of code large
language models, leading to their rapid adoption and application across a wide
range of domains. However, their further advancements are often constrained by
the scarcity of real-world coding problems. To bridge this gap, we propose a
novel framework for synthesizing code problems that emulate authentic
real-world scenarios. This framework systematically integrates domain
knowledge, domain skills, and coding skills, all of which are meticulously
extracted from real-world programming-related datasets, including Stack
Overflow and Kaggle. The extracted elements serve as the foundational building
blocks for constructing code problems. To align the generated problems with
practical applications, application scenarios are also mined from the
aforementioned datasets. These scenarios are then utilized to construct a
scenario-centric graph that interconnects domain knowledge, domain skills, and
coding skills. Based on this structured representation, a sampling strategy on
the graph is designed, which effectively controls the generation of a code
problem with complexity and diversity, reflects real-world challenges.
Experimental results demonstrate that the proposed method consistently achieves
superior performance over state-of-the-art open-source large language models of
varying sizes and functionalities, including both coders and general-purpose
models, across a diverse set of real-world benchmarks.

</details>


### [5] [Monitoring Machine Learning Systems: A Multivocal Literature Review](https://arxiv.org/abs/2509.14294)
*Hira Naveed,Scott Barnett,Chetan Arora,John Grundy,Hourieh Khalajzadeh,Omar Haggag*

Main category: cs.SE

TL;DR: This paper surveys 136 ML monitoring works, examining motivations, methods, tools, and limitations. It provides insights, highlights gaps, and recommends directions for future research and practice.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in the inherent instability and unpredictability of production ML environments, where runtime issues (data drift, operational changes) frequently affect model performance, making effective monitoring crucial for reliability and trust.

Method: The authors use a multivocal literature review (MLR) following Garousi's guidelines, analyzing 136 papers and studying ML monitoring from multiple dimensions, including motivations, techniques/tools, benefits, and limitations.

Result: The review provides a systematic analysis of ML monitoring literature, outlining key motivations, approaches, practical techniques, benefits, and current shortcomings. It highlights both formal and 'gray' literature, presenting insights, implications, and actionable recommendations for future work.

Conclusion: The study summarizes the state of ML monitoring, identifying practices and gaps, and offering guidance to both academics and practitioners on selecting solutions and prioritizing future research and tool development.

Abstract: Context: Dynamic production environments make it challenging to maintain
reliable machine learning (ML) systems. Runtime issues, such as changes in data
patterns or operating contexts, that degrade model performance are a common
occurrence in production settings. Monitoring enables early detection and
mitigation of these runtime issues, helping maintain users' trust and prevent
unwanted consequences for organizations. Aim: This study aims to provide a
comprehensive overview of the ML monitoring literature. Method: We conducted a
multivocal literature review (MLR) following the well established guidelines by
Garousi to investigate various aspects of ML monitoring approaches in 136
papers. Results: We analyzed selected studies based on four key areas: (1) the
motivations, goals, and context; (2) the monitored aspects, specific
techniques, metrics, and tools; (3) the contributions and benefits; and (4) the
current limitations. We also discuss several insights found in the studies,
their implications, and recommendations for future research and practice.
Conclusion: Our MLR identifies and summarizes ML monitoring practices and gaps,
emphasizing similarities and disconnects between formal and gray literature.
Our study is valuable for both academics and practitioners, as it helps select
appropriate solutions, highlights limitations in current approaches, and
provides future directions for research and tool development.

</details>


### [6] [On the Illusion of Success: An Empirical Study of Build Reruns and Silent Failures in Industrial CI](https://arxiv.org/abs/2509.14347)
*Henri Aïdasso,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: This paper is the first to empirically study 'silent failures' in CI—cases where build jobs succeed but secretly fail tasks—by analyzing job reruns and public issues. It finds silent failures are common and often unnoticed, caused by factors such as scripting errors, ignored exit codes, and caching problems. Highlighting these helps teams improve CI reliability and reduce bugs slipping into production.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address undetected reliability problems in Continuous Integration (CI) systems, specifically the issue of 'silent failures' where jobs are marked as successful but fail during execution. These failures undermine developer trust and can result in bugs reaching production, yet have not been previously studied in detail.

Method: The study empirically investigates silent CI failures by analyzing reruns of successful build jobs. It examines 142,387 jobs from 81 industrial projects, applying mixed-effects models to 32 independent variables. Additionally, the study reviews 92 public issues to categorize types of silent failures.

Result: The results show that 11% of successful CI jobs are rerun, and 35% of reruns occur more than 24 hours after the initial run. The analysis identifies key factors related to silent failures, such as static analysis and testing tasks, scripting languages (Shell), and developer rerun tendencies. 11 distinct categories of silent failures, with common causes including artifact operation errors, caching errors, and ignored exit codes, are documented.

Conclusion: Silent failures in CI are widespread and usually unnoticed, posing a significant threat to software quality and reliability. Identifying their causes and patterns enables teams to enhance CI practices, reduce unnoticed errors, and improve build outcome trust.

Abstract: Reliability of build outcomes is a cornerstone of effective Continuous
Integration (CI). Yet in practice, developers often struggle with
non-deterministic issues in the code or CI infrastructure, which undermine
trust in build results. When faced with such unexpected outcomes, developers
often repeatedly rerun jobs hoping for true success, but this practice is known
to increase CI costs and reduce productivity. While recent studies have focused
on intermittent job failures, no prior work has investigated silent failures,
where build jobs are marked as successful but fail to complete all or part of
their tasks. Such silent failures often go unnoticed, creating an illusion of
success with detrimental consequences such as bugs escaping into production.
This paper presents the first empirical study of silent failures through the
practice of rerunning successful jobs. An analysis of 142,387 jobs across 81
industrial projects shows that 11% of successful jobs are rerun, with 35% of
these reruns occurring after more than 24 hours. Using mixed-effects models on
32 independent variables (AUC of 85%), we identified key factors associated
with reruns of successful jobs, notably testing and static analysis tasks,
scripting languages like Shell, and developers prior rerun tendencies. A
further analysis of 92 public issues revealed 11 categories of silent failures
aligning with these factors, the most frequent being artifact operation errors,
caching errors, and ignored exit codes. Overall, our findings provide valuable
insights into the circumstances and causes of silent failures to raise
awareness among teams, and present solutions to improve CI reliability.

</details>


### [7] [A Taxonomy of Prompt Defects in LLM Systems](https://arxiv.org/abs/2509.14404)
*Haoye Tian,Chong Wang,BoYang Yang,Lyuye Zhang,Yang Liu*

Main category: cs.SE

TL;DR: This paper systematically classifies prompt defects for LLMs, mapping their causes and effects, and outlines practical engineering solutions, urging the field towards more reliable prompt design methodologies.


<details>
  <summary>Details</summary>
Motivation: Prompt design for LLMs is currently empirical and error-prone, with mistakes often causing unreliable and insecure model behavior. There is a need for an engineering-focused understanding and systematic approach to prompt issues.

Method: The study conducts a systematic survey of prompt defects in LLMs, organizes them into a taxonomy across six dimensions, analyzes concrete examples and root causes, and distills mitigation strategies informed by software engineering principles.

Result: The paper delivers the first structured taxonomy of prompt defects, explains their impact on real-world development, and outlines targeted strategies for mitigation. It also identifies open challenges and recommends methodological transitions in prompt engineering.

Conclusion: The paper concludes that prompt defects are diverse and impactful, proposing a comprehensive taxonomy of these defects, their causes, consequences, and mitigation strategies. It advocates for more rigorous engineering approaches in prompt design to ensure reliable and robust LLM-driven systems.

Abstract: Large Language Models (LLMs) have become key components of modern software,
with prompts acting as their de-facto programming interface. However, prompt
design remains largely empirical and small mistakes can cascade into
unreliable, insecure, or inefficient behavior. This paper presents the first
systematic survey and taxonomy of prompt defects, recurring ways that prompts
fail to elicit their intended behavior from LLMs. We organize defects along six
dimensions: (1) Specification and Intent, (2) Input and Content, (3) Structure
and Formatting, (4) Context and Memory, (5) Performance and Efficiency, and (6)
Maintainability and Engineering. Each dimension is refined into fine-grained
subtypes, illustrated with concrete examples and root cause analysis. Grounded
in software engineering principles, we show how these defects surface in real
development workflows and examine their downstream effects. For every subtype,
we distill mitigation strategies that span emerging prompt engineering
patterns, automated guardrails, testing harnesses, and evaluation frameworks.
We then summarize these strategies in a master taxonomy that links defect,
impact, and remedy. We conclude with open research challenges and a call for
rigorous engineering-oriented methodologies to ensure that LLM-driven systems
are dependable by design.

</details>


### [8] [CodeLSI: Leveraging Foundation Models for Automated Code Generation with Low-Rank Optimization and Domain-Specific Instruction Tuning](https://arxiv.org/abs/2509.14373)
*Huy Le,Phong Nguyen,Hao Do,Tuan Nguyen,Thien Pham,Anh Nguyen-Duc,Tho Quan*

Main category: cs.SE

TL;DR: CodeLSI enables secure, scalable, and domain-specific code generation by combining model optimization and custom tuning, outperforming conventional methods and reducing costs.


<details>
  <summary>Details</summary>
Motivation: While foundation models (FMs) can generate code automatically, existing solutions often lack domain specificity, are costly, and may raise security concerns when relying on third-party APIs. The motivation of the paper is to develop a code generation solution tailored for specific domains that is more secure, cost-effective, and independent of commercial APIs.

Method: The proposed framework, CodeLSI, combines low-rank adaptation techniques (to reduce computational resource costs during model pre-training and fine-tuning) with domain-specific instruction tuning (to better align outputs to organizational coding needs). It is tested using real JavaScript tasks with data from internal company projects.

Result: CodeLSI produced high-quality, context-aware code that surpassed baseline models in relevance, accuracy, and domain alignment. The method also significantly reduced training resource requirements, allowing scalable training on internal infrastructure.

Conclusion: The combination of low-rank optimization and domain-specific instruction tuning enhances the performance and practicality of FMs for custom code generation. This solution is secure, cost-efficient, and does not depend on external APIs, supporting faster and more focused software innovation.

Abstract: Context: Automated code generation using Foundation Models (FMs) offers
promising solutions for enhancing software development efficiency. However,
challenges remain in ensuring domain specificity, cost-effectiveness, and
security - especially when relying on third-party APIs. This paper introduces
CodeLSI, a framework that combines low-rank optimization and domain-specific
instruction tuning to address these challenges.
  Objectives: The aim of this study is to develop and evaluate CodeLSI, a novel
approach for generating high-quality code tailored to specific domains, using
FMs fine-tuned on company infrastructure without dependence on external APIs.
  Methods: CodeLSI applies low-rank adaptation techniques to reduce the
computational cost of model pre-training and fine-tuning. Domain-specific
instruction tuning is employed to align code generation with organizational
needs. We implemented and tested the framework on real-world JavaScript coding
tasks using datasets drawn from internal software projects.
  Results: Experimental evaluations show that CodeLSI produces high-quality,
context aware code. It outperforms baseline models in terms of relevance,
accuracy, and domain fit. The use of low-rank optimization significantly
reduced resource requirements, enabling scalable training on company-owned
infrastructure.
  Conclusion: CodeLSI demonstrates that combining low-rank optimization with
domain specific tuning can enhance the practicality and performance of FMs for
automated code generation. This approach provides a secure, cost-efficient
alternative to commercial API based solutions and supports faster, more
targeted innovation in software development.

</details>


### [9] [Automating Modelica Module Generation Using Large Language Models: A Case Study on Building Control Description Language](https://arxiv.org/abs/2509.14623)
*Hanlong Wan,Xing Lu,Yan Chen,Karthik Devaprasad,Laura Hinkle*

Main category: cs.SE

TL;DR: Automating Modelica control module generation with LLMs can save significant development time, but limitations in AI validation mean human intervention is still necessary. Careful workflow design improves success rates, pointing toward future research opportunities in simulation validation and improved grounding.


<details>
  <summary>Details</summary>
Motivation: Designing robust control modules in Modelica for dynamic energy systems is labor-intensive and requires specialized skills, limiting adoption and efficiency in control systems engineering.

Method: A structured workflow was developed that uses large language models (LLMs) to automate the generation of Control Description Language modules in Modelica. The workflow features prompt scaffolds, library-aware grounding, automated compilation, and human evaluation. Experiments tested both logic and control modules using GPT-4o and Claude Sonnet 4, as well as strategies like retrieval augmented generation and hard rule search.

Result: GPT-4o did not generate executable code in zero-shot mode, while Claude Sonnet 4 succeeded for logic blocks with careful prompts. For control modules, an 83% success rate was achieved and faulty outputs required moderate human intervention. Hard rule search outperformed retrieval-based approaches. Human evaluation surpassed AI's validation abilities. The LLM-assisted workflow cut module development time by 40–60% (from 10–20 hours to 4–6 hours).

Conclusion: LLMs can substantially accelerate Modelica module development, but current AI models struggle with simulation-based validation and behavioral correctness, requiring human oversight. Improvements in grounding, validation, and evaluation processes are needed for further automation.

Abstract: Dynamic energy systems and controls require advanced modeling frameworks to
design and test supervisory and fault tolerant strategies. Modelica is a widely
used equation based language, but developing control modules is labor intensive
and requires specialized expertise. This paper examines the use of large
language models (LLMs) to automate the generation of Control Description
Language modules in the Building Modelica Library as a case study. We developed
a structured workflow that combines standardized prompt scaffolds, library
aware grounding, automated compilation with OpenModelica, and human in the loop
evaluation. Experiments were carried out on four basic logic tasks (And, Or,
Not, and Switch) and five control modules (chiller enable/disable, bypass valve
control, cooling tower fan speed, plant requests, and relief damper control).
The results showed that GPT 4o failed to produce executable Modelica code in
zero shot mode, while Claude Sonnet 4 achieved up to full success for basic
logic blocks with carefully engineered prompts. For control modules, success
rates reached 83 percent, and failed outputs required medium level human repair
(estimated one to eight hours). Retrieval augmented generation often produced
mismatches in module selection (for example, And retrieved as Or), while a
deterministic hard rule search strategy avoided these errors. Human evaluation
also outperformed AI evaluation, since current LLMs cannot assess simulation
results or validate behavioral correctness. Despite these limitations, the LLM
assisted workflow reduced the average development time from 10 to 20 hours down
to 4 to 6 hours per module, corresponding to 40 to 60 percent time savings.
These results highlight both the potential and current limitations of LLM
assisted Modelica generation, and point to future research in pre simulation
validation, stronger grounding, and closed loop evaluation.

</details>


### [10] [SALT4Decompile: Inferring Source-level Abstract Logic Tree for LLM-Based Binary Decompilation](https://arxiv.org/abs/2509.14646)
*Yongpan Wang,Xin Xu,Xiaojie Zhu,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: SALTM leverages logic-tree abstraction to guide LLM-based decompilation, handling binary jumps/data segments missed by previous approaches. It beats existing methods in accuracy and robustness, aiding both automation and human reverse engineering.


<details>
  <summary>Details</summary>
Motivation: Recent LLM-based decompilation methods fail to accurately reconstruct source code semantics because they treat assembly as a linear sequence, ignoring complex jumps and data segments. This limits their recovery of high-level logic from binaries.

Method: The proposed method, SALTM, introduces a Source-level Abstract Logic Tree (SALT) to abstract logic features (such as jumps and operations) from binary code. SALTM fine-tunes an LLM on reconstructed logic trees to generate code, followed by error correction and symbol recovery to improve readability and correctness.

Result: SALTM significantly outperforms previous state-of-the-art decompilers across multiple benchmarks (e.g., 70.4% TCP rate on Decompile-Eval with +10.6% improvement). It is robust against common obfuscation techniques and improves human analysts' comprehension in user studies.

Conclusion: Abstracting binary logic and fine-tuning LLMs using logic trees enables more accurate and robust decompilation, enhancing both automatic semantic recovery and manual analysis.

Abstract: Decompilation is widely used in reverse engineering to recover high-level
language code from binary executables. While recent approaches leveraging Large
Language Models (LLMs) have shown promising progress, they typically treat
assembly code as a linear sequence of instructions, overlooking arbitrary jump
patterns and isolated data segments inherent to binary files. This limitation
significantly hinders their ability to correctly infer source code semantics
from assembly code. To address this limitation, we propose \saltm, a novel
binary decompilation method that abstracts stable logical features shared
between binary and source code. The core idea of \saltm is to abstract selected
binary-level operations, such as specific jumps, into a high-level logic
framework that better guides LLMs in semantic recovery. Given a binary
function, \saltm constructs a Source-level Abstract Logic Tree (\salt) from
assembly code to approximate the logic structure of high-level language. It
then fine-tunes an LLM using the reconstructed \salt to generate decompiled
code. Finally, the output is refined through error correction and symbol
recovery to improve readability and correctness. We compare \saltm to three
categories of baselines (general-purpose LLMs, commercial decompilers, and
decompilation methods) using three well-known datasets (Decompile-Eval, MBPP,
Exebench). Our experimental results demonstrate that \saltm is highly effective
in recovering the logic of the source code, significantly outperforming
state-of-the-art methods (e.g., 70.4\% TCP rate on Decompile-Eval with a 10.6\%
improvement). The results further validate its robustness against four commonly
used obfuscation techniques. Additionally, analyses of real-world software and
a user study confirm that our decompiled output offers superior assistance to
human analysts in comprehending binary functions.

</details>


### [11] [An LLM-based multi-agent framework for agile effort estimation](https://arxiv.org/abs/2509.14483)
*Thanh-Long Bui,Hoa Khanh Dam,Rashina Hoda*

Main category: cs.SE

TL;DR: This paper introduces an LLM-powered multi-agent system for agile effort estimation that not only surpasses existing methods in accuracy, but also enables interactive collaboration with human developers, addressing key limitations of current machine learning solutions.


<details>
  <summary>Details</summary>
Motivation: Effort estimation in agile software development is vital but is currently plagued by subjective judgments, resulting in inaccurate and inconsistent estimates. Existing machine learning approaches lack transparency and interactive capabilities.

Method: The paper proposes a novel multi-agent framework based on Large Language Models (LLMs) for agile effort estimation. This framework can produce estimates, coordinate, communicate, and discuss with human developers and other agents to reach consensus.

Result: On a real-life dataset, the proposed framework outperforms state-of-the-art techniques across most evaluation metrics. Additionally, a human study demonstrates a highly positive experience for practitioners interacting with the agents.

Conclusion: The LLM-based multi-agent approach effectively improves estimation accuracy and consistency in agile software development, while enhancing collaborative interactions between humans and AI agents.

Abstract: Effort estimation is a crucial activity in agile software development, where
teams collaboratively review, discuss, and estimate the effort required to
complete user stories in a product backlog. Current practices in agile effort
estimation heavily rely on subjective assessments, leading to inaccuracies and
inconsistencies in the estimates. While recent machine learning-based methods
show promising accuracy, they cannot explain or justify their estimates and
lack the capability to interact with human team members. Our paper fills this
significant gap by leveraging the powerful capabilities of Large Language
Models (LLMs). We propose a novel LLM-based multi-agent framework for agile
estimation that not only can produce estimates, but also can coordinate,
communicate and discuss with human developers and other agents to reach a
consensus. Evaluation results on a real-life dataset show that our approach
outperforms state-of-the-art techniques across all evaluation metrics in the
majority of the cases. Our human study with software development practitioners
also demonstrates an overwhelmingly positive experience in collaborating with
our agents in agile effort estimation.

</details>


### [12] [Code Less to Code More: Streamlining Language Server Protocol and Type System Development for Language Families](https://arxiv.org/abs/2509.15150)
*Federico Bruzzone,Walter Cazzola,Luca Favalli*

Main category: cs.SE

TL;DR: This paper introduces the Typelang family and several modular automation approaches for editing support in language families, demonstrating drastic reductions in code and manual effort via empirical results.


<details>
  <summary>Details</summary>
Motivation: Supporting multiple programming languages in multiple editors is complex and resource-intensive. Existing approaches lack modularity, reusability, and exploitation of type systems, making language server and plugin generation inefficient.

Method: The authors propose a suite of solutions: Typelang (domain-specific languages for modular/reusable type systems), a modular language server generation process, a variant-oriented programming paradigm, a cross-artifact coordination layer, and an LSP plugin generator. These are implemented in the Neverlang language workbench and empirically evaluated.

Result: Their approach produces modular language servers and automates LSP plugin generation for three editors. Evaluation shows a 93.48% reduction in code required for type system implementation and full automation of plugin generation, leading to significantly reduced development effort.

Conclusion: The solutions described greatly reduce the effort required to support editing for language families, especially when language artifacts can be reused. By simplifying and automating server and plugin generation, they facilitate better modularity and reusability in language workbenches.

Abstract: Developing editing support for $L$ languages in $E$ editors is complex and
time-consuming. Some languages do not provide dedicated editors, while others
offer a single native editor. The $\textit{language server protocol}$ (LSP)
reduces the language-editor combinations $L \times E$ to $L + E$, where a
single language server communicates with editors via LSP plugins. However,
overlapping implementations of linguistic components remain an issue. Existing
language workbenches struggle with modularity, reusability, and leveraging type
systems for language server generation. In this work, we propose: (i) Typelang,
a family of domain-specific languages for modular, composable, and reusable
type system implementation, (ii) a modular language server generation process,
producing servers for languages built in a modular workbench, (iii) the
variant-oriented programming paradigm and a cross-artifact coordination layer
to manage interdependent software variants, and (iv) an LSP plugin generator,
reducing $E$ to $1$ by automating plugin creation for multiple editors. To
simplify editing support for language families, each language artifact
integrates its own Typelang variant, used to generate language servers. This
reduces combinations to $T \times 1$, where $T = L$ represents the number of
type systems. Further reuse of language artifacts across languages lowers this
to $N \times 1$, where $N << T$, representing unique type systems. We implement
Typelang in Neverlang, generating language servers for each artifact and LSP
plugins for three editors. Empirical evaluation shows a 93.48% reduction in
characters needed for type system implementation and 100% automation of LSP
plugin generation, significantly lowering effort for editing support in
language families, especially when artifacts are reused.

</details>


### [13] [Evaluating the Effectiveness of Coverage-Guided Fuzzing for Testing Deep Learning Library APIs](https://arxiv.org/abs/2509.14626)
*Feiran Qin,M. M. Abid Naziri,Hengyu Ai,Saikat Dutta,Marcelo d'Amorim*

Main category: cs.SE

TL;DR: FlashFuzz uses LLMs to automatically generate harnesses for coverage guided fuzzing of deep learning libraries, achieving much higher coverage, speed, and bug detection than existing fuzzers. It sets a new baseline for testing tools in this domain.


<details>
  <summary>Details</summary>
Motivation: Deep Learning libraries like PyTorch are essential for building AI applications, but finding bugs in these libraries is particularly difficult. Traditional fuzzing techniques for these libraries lack coverage guidance, thus reducing their efficiency and effectiveness. The motivation here is to determine whether coverage guided fuzzing (CGF), particularly tools like LibFuzzer, can be successfully adapted to improve bug detection, code coverage, and scalability in DL libraries.

Method: The paper presents FlashFuzz, a novel approach that utilizes Large Language Models (LLMs) to automatically generate API-level fuzzing harnesses for DL libraries. This is achieved by synthesizing harnesses using templates, helper functions, and API documentation. FlashFuzz employs a feedback-driven strategy to iteratively create and refine these harnesses, enabling effective application of CGF to libraries like PyTorch and TensorFlow.

Result: FlashFuzz synthesizes harnesses for over 1,100 PyTorch APIs and 662 TensorFlow APIs. It achieves significantly higher coverage (up to 212.88% more) and higher input validity (up to 5.4 times more) compared to previous fuzzing tools, as well as massive speedups in input generation (up to 1182x). Moreover, FlashFuzz discovers 42 unknown bugs, 8 of which have already been resolved.

Conclusion: Coverage guided fuzzing, when enhanced with automated harness synthesis using LLMs, can be effectively applied to deep learning libraries. FlashFuzz sets a new benchmark for fuzzing these libraries, demonstrating notable improvements in code coverage, bug detection, and efficiency compared to prior methods.

Abstract: Deep Learning (DL) libraries such as PyTorch provide the core components to
build major AI-enabled applications. Finding bugs in these libraries is
important and challenging. Prior approaches have tackled this by performing
either API-level fuzzing or model-level fuzzing, but they do not use coverage
guidance, which limits their effectiveness and efficiency. This raises an
intriguing question: can coverage guided fuzzing (CGF), in particular
frameworks like LibFuzzer, be effectively applied to DL libraries, and does it
offer meaningful improvements in code coverage, bug detection, and scalability
compared to prior methods?
  We present the first in-depth study to answer this question. A key challenge
in applying CGF to DL libraries is the need to create a test harness for each
API that can transform byte-level fuzzer inputs into valid API inputs. To
address this, we propose FlashFuzz, a technique that leverages Large Language
Models (LLMs) to automatically synthesize API-level harnesses by combining
templates, helper functions, and API documentation. FlashFuzz uses a feedback
driven strategy to iteratively synthesize and repair harnesses. With this
approach, FlashFuzz synthesizes harnesses for 1,151 PyTorch and 662 TensorFlow
APIs. Compared to state-of-the-art fuzzing methods (ACETest, PathFinder, and
TitanFuzz), FlashFuzz achieves up to 101.13 to 212.88 percent higher coverage
and 1.0x to 5.4x higher validity rate, while also delivering 1x to 1182x
speedups in input generation. FlashFuzz has discovered 42 previously unknown
bugs in PyTorch and TensorFlow, 8 of which are already fixed. Our study
confirms that CGF can be effectively applied to DL libraries and provides a
strong baseline for future testing approaches.

</details>


### [14] [Wireless Communication Performance Testing: From Laboratory Environment to Research Vessel](https://arxiv.org/abs/2509.14740)
*Andrei-Raoul Morariu,Andreas Strandberg,Bogdan Iancu,Jerker Bjorkqvist*

Main category: cs.SE

TL;DR: The paper demonstrates that obstacles and dynamic environments, like those encountered on boats or in labs, degrade wireless signal transmission, emphasizing the need to consider these factors when designing communication systems.


<details>
  <summary>Details</summary>
Motivation: The study aims to determine how real-world environmental factors, such as obstacles and dynamic placement, influence the quality of wireless communication in shared spectrum settings.

Method: Signal transmission was measured both in laboratory conditions and on an electric research boat, with varying distances and object placements between the transmitter and receiver.

Result: Laboratory objects attenuated the signal when obstructing the line of sight, and placement and distance on the boat also influenced transmission efficiency, confirming that environmental factors critically affect communication.

Conclusion: Environmental obstacles and placement aboard vehicles significantly affect wireless signal transmission efficiency, indicating that communication performance can degrade in obstructed or dynamic settings.

Abstract: This study investigates signal transmission within a shared spectrum,
focusing on measurements conducted both in laboratory and outdoor environments.
The objective was to demonstrate how laboratory objects obstructing the line of
sight can attenuate the signal between a transmitter (Tx) and a receiver (Rx).
Additionally, we examined the impact of distance and placement in various
locations aboard an electric research boat on signal transmission efficiency.
These findings contribute to understanding whether the environmental factors
influence wireless communication in dynamic and obstructed environments.

</details>


### [15] [On the Use of Agentic Coding Manifests: An Empirical Study of Claude Code](https://arxiv.org/abs/2509.14744)
*Worawalan Chatlatanagulchai,Kundjanasith Thonglek,Brittany Reid,Yutaro Kashiwa,Pattara Leelaprute,Arnon Rungsawang,Bundit Manaskasemsak,Hajimu Iida*

Main category: cs.SE

TL;DR: The paper analyzed hundreds of agent manifest files, revealing simple structural patterns and content focused on operational use and technical details, emphasizing the documentation gap for developers.


<details>
  <summary>Details</summary>
Motivation: To address the challenge faced by developers due to the insufficient documentation available for creating agent manifests in agentic coding tools.

Method: Analysis of 253 Claude.md files from 242 repositories to identify recurring structural and content patterns within agent manifests.

Result: The study demonstrates that Claude.md manifests are generally organized with one main heading and several subsections, containing predominantly operational commands, technical notes, and architectural overviews.

Conclusion: Agent manifests commonly feature shallow hierarchical structures, focusing on operational commands, implementation notes, and architecture, which reflects current usage patterns but highlights a lack of comprehensive documentation.

Abstract: Agentic coding tools receive goals written in natural language as input,
break them down into specific tasks, and write/execute the actual code with
minimal human intervention. Key to this process are agent manifests,
configuration files (such as Claude.md) that provide agents with essential
project context, identity, and operational rules. However, the lack of
comprehensive and accessible documentation for creating these manifests
presents a significant challenge for developers. We analyzed 253 Claude.md
files from 242 repositories to identify structural patterns and common content.
Our findings show that manifests typically have shallow hierarchies with one
main heading and several subsections, with content dominated by operational
commands, technical implementation notes, and high-level architecture.

</details>


### [16] [On the Use of Agentic Coding: An Empirical Study of Pull Requests on GitHub](https://arxiv.org/abs/2509.14745)
*Miku Watanabe,Hao Li,Yutaro Kashiwa,Brittany Reid,Hajimu Iida,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: Agent-generated pull requests are widely accepted in open-source projects, but nearly half still require human improvement before full integration.


<details>
  <summary>Details</summary>
Motivation: To assess the practical usefulness and acceptance of autonomous AI agent-generated pull requests in real-world software development environments.

Method: Empirical analysis of 567 GitHub pull requests generated by Claude Code agent across 157 open-source projects, examining acceptance rates and revisions.

Result: 83.8% of agent-assisted PRs were accepted and merged, with 54.9% merged without further modification. 45.1% needed additional human revisions, mainly for bug fixes, documentation, and compliance with project standards.

Conclusion: Agent-assisted pull requests are highly accepted in open-source projects, with most being merged, though a significant proportion still require human revision.

Abstract: Large language models (LLMs) are increasingly being integrated into software
development processes. The ability to generate code and submit pull requests
with minimal human intervention, through the use of autonomous AI agents, is
poised to become a standard practice. However, little is known about the
practical usefulness of these pull requests and the extent to which their
contributions are accepted in real-world projects. In this paper, we
empirically study 567 GitHub pull requests (PRs) generated using Claude Code,
an agentic coding tool, across 157 diverse open-source projects. Our analysis
reveals that developers tend to rely on agents for tasks such as refactoring,
documentation, and testing. The results indicate that 83.8% of these
agent-assisted PRs are eventually accepted and merged by project maintainers,
with 54.9% of the merged PRs are integrated without further modification. The
remaining 45.1% require additional changes benefit from human revisions,
especially for bug fixes, documentation, and adherence to project-specific
standards. These findings suggest that while agent-assisted PRs are largely
acceptable, they still benefit from human oversight and refinement.

</details>


### [17] [RulER: Automated Rule-Based Semantic Error Localization and Repair for Code Translation](https://arxiv.org/abs/2509.14829)
*Shuo Jin,Songqiang Chen,Xiaoyuan Xie,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: RulER introduces a rule-based debugging method for automated code translation, effectively improving error localization and repair success rates by leveraging reusable and dynamically derived translation rules. It notably outperforms leading models and LLM-prompting approaches.


<details>
  <summary>Details</summary>
Motivation: Existing methods for automated code debugging in translation lack reliable references for code alignment and repair, reducing their effectiveness. The paper aims to overcome this by introducing a systematic and reusable approach based on translation rules.

Method: RulER is a rule-based debugging approach that derives translation rules from correct LLM-generated translations and dynamically combines these rules to align and repair code statements. RulER enables reliable code alignment and repair template construction for debugging translations.

Result: RulER outperformed BatFix and TransMap by 20% in error localization and 272% in repair success rates across Java-to-C++ and Python-to-C++ translations from four code translators. It also demonstrated superior repair capabilities compared to prompting LLMs directly.

Conclusion: RulER significantly improves error localization and repair success rates compared to existing automated debugging methods in code translation, outperforming state-of-the-art models substantially.

Abstract: Automated code translation aims to convert programs between different
programming languages while maintaining their functionality. Due to the
imperfections of code translation models, the generated translations may
contain errors that compromise their reliability. Existing automated debugging
methods for code translation rely on code alignments and repair patch templates
to locate and fix erroneous translations. However, existing methods lack
reliable references to construct code alignments and design repair patch
templates, which significantly impacts their localization accuracy and repair
effectiveness. To address these limitations, we reintroduce code translation
rules and propose a rule-based debugging method for code translation, called
RulER. RulER automatically derives code translation rules from correct
translations generated by LLMs, enabling the efficient collection of diverse
translation rules. In addition, RulER dynamically combines the existing rules
on expandable nodes like expressions and tokens to further adaptively align
more statements. These rules capture clear and detailed structural
correspondences between source and target programming languages. Therefore,
they can serve as reliable and reusable references for code alignment and
repair template design, enabling RulER to locate and fix translation errors
effectively. Our evaluation of RulER on Java-to-C++ and Python-to-C++
translations produced by four code translation models demonstrates that RulER
outperforms state-of-the-art methods, BatFix and TransMap. Our experimental
results show that RulER outperformed the best baseline by 20% and 272% in terms
of error localization rates and repair success rates, respectively. RulER
exhibits superior repair performance compared to directly prompting LLMs for
patch generation, demonstrating a promising methodology for extracting and
leveraging coding knowledge from LLMs.

</details>


### [18] [CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects](https://arxiv.org/abs/2509.14856)
*Hanyang Guo,Xunjin Zheng,Zihan Liao,Hang Yu,Peng DI,Ziyin Zhang,Hong-Ning Dai*

Main category: cs.SE

TL;DR: CodeFuse-CR-Bench introduces a comprehensive and context-rich benchmark for repository-level code review, enabling nuanced evaluation of LLMs. Results show varied strengths among models, highlighting the need for multidimensional testing to progress toward truly effective code review assistants.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for automated code review using LLMs are insufficient because they lack holistic and context-rich evaluation, which limits real-world applicability. Existing methods focus only on isolated tasks with simplified data, not capturing the complexity of genuine repository-level code review.

Method: The authors introduce CodeFuse-CR-Bench, a new benchmark that features 601 instances spanning 70 Python repositories and nine pull-request domains, each containing multifaceted real-world context. They also develop an evaluation framework combining rule-based syntax/location checks with model-based quality judgments, enabling end-to-end assessments.

Result: The benchmark reveals that no single LLM excels in all code review aspects. Gemini 2.5 Pro performs best overall but robustness and strengths vary across models, especially when handling redundant or complex context.

Conclusion: Holistic, multi-dimensional evaluation is essential for advancing practical and intelligent code review tools. This work sets vital baselines and emphasizes the need for comprehensive, context-rich evaluation methods in LLM-driven code review research.

Abstract: Automated code review (CR) is a key application for Large Language Models
(LLMs), but progress is hampered by a "reality gap": existing benchmarks
evaluate models on isolated sub-tasks using simplified, context-poor data. This
fails to reflect the holistic context-rich nature of real-world CR. To bridge
this gap, we introduce CodeFuse-CR-Bench, the first comprehensiveness-aware
benchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601
high-quality instances from 70 Python projects covering nine Pull-Request (PR)
problem domains, where each instance provides rich, multi-faceted context
including the associated issue, PR details, and repository state, enabling
end-to-end evaluation. Beyond superficial metrics, we also propose a novel
evaluation framework that combines rule-based checks for location and syntax
with model-based judgments of review quality. We present the first large-scale
assessment of state-of-the-art LLMs on this comprehensive CR task. Our results
establish crucial baselines and reveal that (1) no single LLM dominates all
aspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive
performance; and (3) different LLMs exhibit varying robustness to redundant
context. These findings highlight the necessity of holistic, multi-dimensional
evaluation and provide actionable insights for advancing truly intelligent yet
practical CR assistants.

</details>


### [19] [CARGO: A Framework for Confidence-Aware Routing of Large Language Models](https://arxiv.org/abs/2509.14899)
*Amine Barrak,Yosr Fourati,Michael Olchawa,Emna Ksontini,Khalil Zoghlami*

Main category: cs.SE

TL;DR: CARGO is a lightweight, confidence-aware framework for choosing the best LLM for a given prompt. It combines embedding-based regressors and a binary classifier to achieve high routing accuracy and expert-level performance across multiple LLMs and task categories, all without requiring human supervision.


<details>
  <summary>Details</summary>
Motivation: With the increasing variety and specialization of large language models (LLMs), it is challenging to dynamically select the best model for each user prompt while balancing performance and cost.

Method: The authors propose CARGO, a two-stage framework for model selection. CARGO uses embedding-based regressors trained on pairwise LLM comparisons to predict which model would perform best for a given prompt. When predictions are uncertain, an auxiliary binary classifier refines the decision. CARGO also incorporates category-specific regressors for different types of tasks.

Result: CARGO achieves a top-1 routing accuracy of 76.4% and win rates between 72% and 89% against individual expert models when tested on four leading LLMs across five task categories.

Conclusion: CARGO delivers expert-level performance for dynamic LLM selection, enabling more effective and cost-efficient multi-model deployments with minimal overhead and no need for human-annotated supervision.

Abstract: As large language models (LLMs) proliferate in scale, specialization, and
latency profiles, the challenge of routing user prompts to the most appropriate
model has become increasingly critical for balancing performance and cost. We
introduce CARGO (Category-Aware Routing with Gap-based Optimization), a
lightweight, confidence-aware framework for dynamic LLM selection. CARGO
employs a single embedding-based regressor trained on LLM-judged pairwise
comparisons to predict model performance, with an optional binary classifier
invoked when predictions are uncertain. This two-stage design enables precise,
cost-aware routing without the need for human-annotated supervision. To capture
domain-specific behavior, CARGO also supports category-specific regressors
trained across five task groups: mathematics, coding, reasoning, summarization,
and creative writing. Evaluated on four competitive LLMs (GPT-4o, Claude 3.5
Sonnet, DeepSeek V3, and Perplexity Sonar), CARGO achieves a top-1 routing
accuracy of 76.4% and win rates ranging from 72% to 89% against individual
experts. These results demonstrate that confidence-guided, lightweight routing
can achieve expert-level performance with minimal overhead, offering a
practical solution for real-world, multi-model LLM deployments.

</details>


### [20] ["Let it be Chaos in the Plumbing!" Usage and Efficacy of Chaos Engineering in DevOps Pipelines](https://arxiv.org/abs/2509.14931)
*Stefano Fossati,Damian Andrew Tamburri,Massimiliano Di Penta,Marco Tonnarelli*

Main category: cs.SE

TL;DR: This paper systematically reviews industry adoption of Chaos Engineering in DevOps, finding adaptation of its principles to favor control, automation, and risk reduction, and provides a framework and guidance for future improvements in resilient system design.


<details>
  <summary>Details</summary>
Motivation: Chaos Engineering is important for improving resilience in distributed systems, but there is limited systematic study of its real-world adoption and evolution within industry, especially in DevOps settings.

Method: A systematic gray literature review of 50 industry sources from 2019 to early 2024 was conducted. The authors created a classification framework that extends existing Chaos Engineering principles into ten distinct concepts.

Result: Practitioners still value the original Chaos Engineering principles, but there is growing emphasis on controlled experimentation, increased automation, and new risk mitigation strategies to suit agile, fast-changing DevOps environments.

Conclusion: The paper provides a deeper understanding of how Chaos Engineering is applied in practice, highlights evolving priorities, and offers recommendations for research and practice to improve system robustness in modern production environments.

Abstract: Chaos Engineering (CE) has emerged as a proactive method to improve the
resilience of modern distributed systems, particularly within DevOps
environments. Originally pioneered by Netflix, CE simulates real-world failures
to expose weaknesses before they impact production. In this paper, we present a
systematic gray literature review that investigates how industry practitioners
have adopted and adapted CE principles over recent years. Analyzing 50 sources
published between 2019 and early 2024, we developed a comprehensive
classification framework that extends the foundational CE principles into ten
distinct concepts. Our study reveals that while the core tenets of CE remain
influential, practitioners increasingly emphasize controlled experimentation,
automation, and risk mitigation strategies to align with the demands of agile
and continuously evolving DevOps pipelines. Our results enhance the
understanding of how CE is intended and implemented in practice, and offer
guidance for future research and industrial applications aimed at improving
system robustness in dynamic production environments.

</details>


### [21] [Orion: Fuzzing Workflow Automation](https://arxiv.org/abs/2509.15195)
*Max Bazalii,Marius Fleischer*

Main category: cs.SE

TL;DR: Orion leverages LLMs and deterministic tools to automate fuzz testing, drastically minimizing human effort and uncovering new vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Fuzz testing is highly effective for detecting software vulnerabilities, but its full workflow is labor-intensive, requiring manual work in code analysis, harness configuration, and result triage. Previous solutions addressed isolated stages, not the entire campaign.

Method: Orion is a framework that automates fuzz testing workflows by integrating large language model (LLM) reasoning for code analysis and semantic guidance with traditional deterministic tools for verification and refinement.

Result: Orion reduces human effort by 46-204x across various workflow stages and discovers two new vulnerabilities in the open-source clib library.

Conclusion: Orion successfully automates significant portions of the fuzz testing process, making large-scale campaigns feasible and greatly reducing the need for manual intervention.

Abstract: Fuzz testing is one of the most effective techniques for finding software
vulnerabilities. While modern fuzzers can generate inputs and monitor
executions automatically, the overall workflow, from analyzing a codebase, to
configuring harnesses, to triaging results, still requires substantial manual
effort. Prior attempts focused on single stages such as harness synthesis or
input minimization, leaving researchers to manually connect the pieces into a
complete fuzzing campaign.
  We introduce Orion, a framework that automates the the manual bottlenecks of
fuzzing by integrating LLM reasoning with traditional tools, allowing campaigns
to scale to settings where human effort alone was impractical. Orion uses LLMs
for code reasoning and semantic guidance, while relying on deterministic tools
for verification, iterative refinement, and tasks that require precision.
Across our benchmark suite, Orion reduces human effort by 46-204x depending on
the workflow stage, and we demonstrate its effectiveness through the discovery
of two previously unknown vulnerabilities in the widely used open-source clib
library.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [22] [DeliverC: Teaching Pointers through GenAI-Powered Game-Based Learning](https://arxiv.org/abs/2509.14496)
*Wyatt Petula,Anushcka Joshi,Peggy Tu,Amrutha Somasundar,Suman Saha*

Main category: cs.PL

TL;DR: DeliverC, a GenAI-powered educational game, improved student confidence and reduced errors for C pointers but needs clearer AI feedback and further refinement for harder tasks.


<details>
  <summary>Details</summary>
Motivation: Game-based learning is commonly applied in programming education, but there is a lack of adaptive, real-time tools specifically addressing challenging topics like C pointers. The authors aim to tackle this gap by leveraging GenAI to provide personalized support.

Method: The authors developed DeliverC, a GenAI-enhanced game that uses GPT-4-mini to generate tailored hints and pointer challenges in real time. They evaluated its effects in a pilot study with 25 undergraduate students, analyzing gameplay data and survey responses on motivation, self-efficacy, metacognition, and feedback quality.

Result: Students became more confident and reflective after using DeliverC, with reduced error rates as they progressed. However, participation declined with increased difficulty, and some feedback from the AI was considered vague.

Conclusion: DeliverC can boost engagement and understanding of difficult programming concepts like C pointers in a game-based format. However, improvements to AI feedback quality are necessary for optimal learning experiences.

Abstract: While game-based learning is widely used in programming education, few tools
offer adaptive, real-time support for complex topics, such as C pointers. We
present DeliverC, a GenAI-enhanced game that integrates GPT-4-mini to provide
personalized hints and generate pointer-related challenges on the fly. In a
pilot study involving 25 undergraduate students, we investigated the impact of
the system on learning through gameplay data and a 15-item survey that covered
constructs such as motivation, self-efficacy, metacognition, and feedback
quality. Results show that most students felt more confident and reflective
after using the tool, and error rates decreased as students progressed through
scaffolded levels. However, participation decreased with task difficulty, and
some students reported receiving unclear or vague feedback. These findings
suggest that DeliverC can enhance engagement and understanding in systems
programming, although refinement in AI-generated feedback is still needed. Our
study highlights the potential of combining GenAI with game-based learning to
support personalized and interactive practice in traditionally challenging
programming domains.

</details>


### [23] [Refinement-Types Driven Development: A study](https://arxiv.org/abs/2509.15005)
*Facundo Domínguez,Arnaud Spiwack*

Main category: cs.PL

TL;DR: The paper proposes that SMT solvers should be used more broadly in normal programming by integrating them into compiler checks through refinement types, demonstrated via case study and prototype with Liquid Haskell.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the belief that SMT solvers are underutilized in everyday programming, being traditionally reserved for formal verification tasks. The authors aim to challenge this perception and demonstrate broader benefits.

Method: The paper argues for integrating SMT solvers into static compiler checks using refinement types, specifically via Liquid Haskell. The approach is demonstrated through a case study on handling binder scopes in compilers, and supported by a prototype theory of finite maps for the SMT solver.

Result: The authors show that, by incorporating SMT solvers and refinement types into the compilation process, ordinary programming tasks—such as managing binder scopes—can be simplified. They also provide a working prototype that extends Liquid Haskell's capabilities.

Conclusion: SMT solvers, when combined with refinement types and integrated into compiler static checks, can substantially enhance everyday programming and program composition—not just formal verification. The case study and prototype support the feasibility of this approach.

Abstract: This paper advocates for the broader application of SMT solvers in everyday
programming, challenging the conventional wisdom that these tools are solely
for formal methods and verification. We claim that SMT solvers, when seamlessly
integrated into a compiler's static checks, significantly enhance the
capabilities of ordinary type checkers in program composition. Specifically, we
argue that refinement types, as embodied by Liquid Haskell, enable the use of
SMT solvers in mundane programming tasks. Through a case study on handling
binder scopes in compilers, we envision a future where ordinary programming is
made simpler and more enjoyable with the aid of refinement types and SMT
solvers. As a secondary contribution, we present a prototype implementation of
a theory of finite maps for Liquid Haskell's solver, developed to support our
case study.

</details>
