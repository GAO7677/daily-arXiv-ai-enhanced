<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 18]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [How Software Engineers Engage with AI: A Pragmatic Process Model and Decision Framework Grounded in Industry Observations](https://arxiv.org/abs/2507.17930)
*Vahid Garousi,Zafar Jafarov*

Main category: cs.SE

TL;DR: The paper studies how engineers engage with AI tools in real-world software engineering, presenting practical models for structured, effective collaboration between humans and AI, informed by industry observations.


<details>
  <summary>Details</summary>
Motivation: The authors are motivated by the increasing use of Artificial Intelligence (AI) in Software Engineering (SE), particularly with tools like GitHub Copilot and ChatGPT, which enable new development approaches. However, how engineers interact with these tools in daily practice—including trust and refinement decisions—remains insufficiently studied.

Method: The paper employs qualitative research, grounding its findings in practitioner reports and direct observations across three industry settings in Turkiye and Azerbaijan. The study synthesizes these insights into models and frameworks for guiding AI-assisted SE activities.

Result: The authors introduced a pragmatic process model covering key AI-assisted SE activities: prompt design, inspection, fallback, and refinement. Additionally, they proposed a 2D decision framework to help developers weigh the trade-offs between effort saved and the quality of AI-generated output.

Conclusion: The study concludes that the presented models provide structured, lightweight guidance that supports more effective, deliberative use of AI tools in SE, addressing a gap in practical human-AI collaboration understanding.

Abstract: Artificial Intelligence (AI) has the potential to transform Software
Engineering (SE) by enhancing productivity, efficiency, and decision support.
Tools like GitHub Copilot and ChatGPT have given rise to "vibe coding"-an
exploratory, prompt-driven development style. Yet, how software engineers
engage with these tools in daily tasks, especially in deciding whether to
trust, refine, or reject AI-generated outputs, remains underexplored. This
paper presents two complementary contributions. First, a pragmatic process
model capturing real-world AI-assisted SE activities, including prompt design,
inspection, fallback, and refinement. Second, a 2D decision framework that
could help developers reason about trade-offs between effort saved and output
quality. Grounded in practitioner reports and direct observations in three
industry settings across Turkiye and Azerbaijan, our work illustrates how
engineers navigate AI use with human oversight. These models offer structured,
lightweight guidance to support more deliberate and effective use of AI tools
in SE, contributing to ongoing discussions on practical human-AI collaboration.

</details>


### [2] [Use as Directed? A Comparison of Software Tools Intended to Check Rigor and Transparency of Published Work](https://arxiv.org/abs/2507.17991)
*Peter Eckmann,Adrian Barnett,Alexandra Bannach-Brown,Elisa Pilar Bascunan Atria,Guillaume Cabanac,Louise Delwen Owen Franzen,Małgorzata Anna Gazda,Kaitlyn Hair,James Howison,Halil Kilicoglu,Cyril Labbe,Sarah McCann,Vladislav Nachev,Martijn Roelandse,Maia Salholz-Hillel,Robert Schulz,Gerben ter Riet,Colby Vorland,Anita Bandrowski,Tracey Weissgerber*

Main category: cs.SE

TL;DR: A study compared 11 automated tools for checking transparency and rigor in scientific reporting. Results showed some tools excel at specific tasks, while tool combinations work best for others. Recommendations for future tool development are given, with open access to study resources.


<details>
  <summary>Details</summary>
Motivation: The reproducibility crisis in science is partly due to inadequate standardization and transparency in scientific reporting. While checklists like ARRIVE and CONSORT aim to improve these aspects, they are often underutilized, and peer review may not catch omissions. Automated tools have therefore been developed to help check for rigor and transparency.

Method: The authors compared 11 different automated tools designed to assess 9 rigor criteria, as part of the ScreenIT group. This comparison evaluated individual and combined tool performance across these criteria.

Result: For some criteria, such as open data detection, one tool outperformed the rest. For other criteria, like inclusion and exclusion criteria detection, combining multiple tools yielded better performance than any single tool. The study also highlighted areas where further tool development is needed.

Conclusion: Key insights and recommendations are presented for those developing rigor and transparency detection tools. The results also emphasize the value of tool combinations and identify high-priority areas for improvement. All code and data from the study is openly available.

Abstract: The causes of the reproducibility crisis include lack of standardization and
transparency in scientific reporting. Checklists such as ARRIVE and CONSORT
seek to improve transparency, but they are not always followed by authors and
peer review often fails to identify missing items. To address these issues,
there are several automated tools that have been designed to check different
rigor criteria. We have conducted a broad comparison of 11 automated tools
across 9 different rigor criteria from the ScreenIT group. We found some
criteria, including detecting open data, where the combination of tools showed
a clear winner, a tool which performed much better than other tools. In other
cases, including detection of inclusion and exclusion criteria, the combination
of tools exceeded the performance of any one tool. We also identified key areas
where tool developers should focus their effort to make their tool maximally
useful. We conclude with a set of insights and recommendations for stakeholders
in the development of rigor and transparency detection tools. The code and data
for the study is available at https://github.com/PeterEckmann1/tool-comparison.

</details>


### [3] [An Empirical Study of GenAI Adoption in Open-Source Game Development: Tools, Tasks, and Developer Challenges](https://arxiv.org/abs/2507.18029)
*Xiang Echo Chen,Wenhan Zhu,Guoshuai Albert Shi,Michael W. Godfrey*

Main category: cs.SE

TL;DR: This paper studies how generative AI is being used in open-source game development by analyzing GitHub issue discussions, comparing it to traditional AI and non-AI topics to reveal differences in adoption, challenges, and integration practices.


<details>
  <summary>Details</summary>
Motivation: Generative AI (GenAI) is transforming game design and development, but there is limited empirical research on how open-source game developers are actually adopting and integrating GenAI compared to traditional AI (TradAI) or NonAI approaches. The motivation is to fill this research gap by exploring real-world usage patterns, challenges, and integration practices of GenAI in open-source gaming.

Method: The study constructs a dataset of open-source game repositories from GitHub discussing AI topics. It uses open card sorting and thematic analysis on a stratified sample of GitHub issues, labeling each by type and content to facilitate comparative analysis between GenAI, TradAI, and NonAI issues.

Result: The comparative analysis provides insights into the tools, tasks, and challenges unique to GenAI versus TradAI and NonAI topics. The annotation and analysis offer an understanding of GenAI's impact on developer workflows and highlight specific pain points encountered by open-source game developers.

Conclusion: GenAI is being distinctly adopted and integrated within open-source game development, with unique usage patterns, developer concerns, and integration methods compared to traditional AI and non-AI methods. Understanding these differences can inform better tool development and support for open-source game communities.

Abstract: The growing capabilities of generative AI (GenAI) have begun to reshape how
games are designed and developed, offering new tools for content creation,
gameplay simulation, and design ideation. While prior research has explored
traditional uses of AI in games, such as controlling agents or generating
procedural content. There is limited empirical understanding of how GenAI is
adopted by developers in real-world contexts, especially within the open-source
community. This study aims to explore how GenAI technologies are discussed,
adopted, and integrated into open-source game development by analyzing issue
discussions on GitHub. We investigate the tools, tasks, and challenges
associated with GenAI by comparing GenAI-related issues to those involving
traditional AI (TradAI) and NonAI topics. Our goal is to uncover how GenAI
differs from other approaches in terms of usage patterns, developer concerns,
and integration practices. To address this objective, we construct a dataset of
open-source game repositories that discuss AI-related topics. We apply open
card sorting and thematic analysis to a stratified sample of GitHub issues,
labelling each by type and content. These annotations enable comparative
analysis across GenAI, TradAI, and NonAI groups, and provide insight into how
GenAI is shaping the workflows and pain points of open-source game developers.

</details>


### [4] [Your ATs to Ts: MITRE ATT&CK Attack Technique to P-SSCRM Task Mapping](https://arxiv.org/abs/2507.18037)
*Sivana Hamer,Jacob Bowen,Md Nazmul Haque,Chris Madden,Laurie Williams*

Main category: cs.SE

TL;DR: This paper presents a consensus-based mapping connecting MITRE ATT&CK supply chain attack techniques, P-SSCRM tasks, and ten major security frameworks, helping organizations strategically mitigate software supply chain risks.


<details>
  <summary>Details</summary>
Motivation: Software supply chain attacks are a growing threat, and organizations need structured ways to mitigate these risks. The motivation behind this paper is to help software organizations understand how to use the MITRE ATT&CK framework in conjunction with other established frameworks for proactive supply chain risk management.

Method: The paper utilizes four independent strategies to establish consensus-based mappings between the MITRE ATT&CK Software Supply Chain Attack Techniques and tasks from the Proactive Software Supply Chain Risk Management Framework (P-SSCRM). Each task was carefully analyzed and cross-referenced with ten different prominent frameworks.

Result: The study produces a mapping between specific P-SSCRM tasks and MITRE ATT&CK attack techniques, as well as a broader mapping that connects MITRE ATT&CK to various other well-known government and industry frameworks. This aids organizations in aligning their security practices across multiple frameworks.

Conclusion: The paper concludes that this mapping enables software organizations to better understand and mitigate software supply chain threats by leveraging consensus-based connections across several major security frameworks, thus enhancing their proactive risk management strategies.

Abstract: The MITRE Adversarial Tactics, Techniques and Common Knowledge (MITRE ATT&CK)
Attack Technique to Proactive Software Supply Chain Risk Management Framework
(P-SSCRM) Task mapping described in this document helps software organizations
to determine how different tasks mitigate the attack techniques of software
supply chain attacks. The mapping was created through four independent
strategies to find agreed-upon mappings. Because each P-SSCRM task is mapped to
one or more tasks from the 10 frameworks, the mapping we provide is also a
mapping between MITRE ATT&CK and other prominent government and industry
frameworks.

</details>


### [5] [Factors Impacting Faculty Adoption of Project-Based Learning in Computing Education: a Survey](https://arxiv.org/abs/2507.18039)
*Ahmad D. Suleiman,Yiming Tang,Daqing Hou*

Main category: cs.SE

TL;DR: Although project-based learning is seen as highly beneficial in computing education, faculty uptake is limited by support and resource barriers. This study finds that professional development, collaboration, and institutional incentives are key to broader adoption, underscoring the need for systemic changes to enable and encourage PjBL in curricula.


<details>
  <summary>Details</summary>
Motivation: Project-based learning (PjBL) is known to enhance student outcomes such as motivation, engagement, and problem-solving skills in computing education. However, the adoption of PjBL by faculty remains inconsistent due to various practical barriers.

Method: The study utilized a mixed-methods approach, collecting quantitative data through an online survey with closed-ended questions, and qualitative data via an open-ended question, from 80 computing faculty. Quantitative data was statistically analyzed and qualitative data underwent thematic analysis.

Result: Findings indicate that while PjBL is highly valued, its adoption is inconsistent due to obstacles like insufficient institutional support, time, funding, lack of training, and challenges in project design. Faculty are more likely to adopt PjBL with peer collaboration, professional development, institutional incentives, and accessible project sources (from research, industry, and peers).

Conclusion: To improve and scale PjBL adoption among computing educators, comprehensive systemic support, professional development opportunities, and better resource access are necessary. Institutional structures that promote collaboration and incentives can empower faculty to implement and sustain PjBL.

Abstract: This research full paper investigates the factors influencing computing
educators' adoption of project-based learning (PjBL) in software engineering
and computing curricula. Recognized as a student-centered pedagogical approach,
PjBL has the potential to enhance student motivation, engagement, critical
thinking, collaboration, and problem-solving skills. Despite these benefits,
faculty adoption remains inconsistent due to challenges such as insufficient
institutional support, time constraints, limited training opportunities,
designing or sourcing projects, and aligning them with course objectives. This
research explores these barriers and investigates the strategies and resources
that facilitate a successful adoption. Using a mixed-methods approach, data
from 80 computing faculty were collected through an online survey comprising
closed-ended questions to quantify barriers, enablers, and resource needs,
along with an open-ended question to gather qualitative insights. Quantitative
data were analyzed using statistical methods, while qualitative responses
underwent thematic analysis. Results reveal that while PjBL is widely valued,
its adoption is often selective and impacted by challenges in planning and
managing the learning process, designing suitable projects, and a lack of
institutional support, such as time, funding, and teaching assistants. Faculty
are more likely to adopt or sustain PjBL when they have access to peer
collaboration, professional development, and institutional incentives. In
addition, sourcing projects from research, industry partnerships, and borrowing
from peers emerged as key facilitators for new projects. These findings
underscore the need for systemic support structures to empower faculty to
experiment with and scale PjBL practices.

</details>


### [6] [An Empirical Study of Complexity, Heterogeneity, and Compliance of GitHub Actions Workflows](https://arxiv.org/abs/2507.18062)
*Edward Abrokwah,Taher A. Ghaleb*

Main category: cs.SE

TL;DR: This paper analyzes thousands of GitHub Actions workflows in open-source projects to assess complexity, structure, and compliance with best practices, finding both strong and weak points in current usage and recommending clearer guidelines for developers.


<details>
  <summary>Details</summary>
Motivation: There is a lack of empirical understanding regarding how real-world open-source CI workflows, specifically using GitHub Actions (GHA), align with recommended best practices. Many existing workflows might be unnecessarily complex and fall short of CI simplicity goals.

Method: The study analyzes a large dataset of GHA workflows from open-source repositories written in Java, Python, and C++. It examines the structure, complexity, heterogeneity, and compliance of these workflows with GHA best practices.

Result: The analysis identifies workflow complexities, recurring and heterogeneous structuring patterns, evaluates compliance with best practices, and reveals differences in CI pipeline design across programming languages.

Conclusion: There are both strong areas of adherence to GHA best practices and notable areas for improvement. The study suggests the need for clearer guidelines and more comprehensive documentation to enhance the quality and simplicity of CI workflows.

Abstract: Continuous Integration (CI) has evolved from a tooling strategy to a
fundamental mindset in modern CI engineering. It enables teams to develop,
test, and deliver software rapidly and collaboratively. Among CI services,
GitHub Actions (GHA) has emerged as a dominant service due to its deep
integration with GitHub and a vast ecosystem of reusable workflow actions.
Although GHA provides official documentation and community-supported best
practices, there appears to be limited empirical understanding of how
open-source real-world CI workflows align with such practices. Many workflows
might be unnecessarily complex and not aligned with the simplicity goals of CI
practices. This study will investigate the structure, complexity,
heterogeneity, and compliance of GHA workflows in open-source software
repositories. Using a large dataset of GHA workflows from Java, Python, and C++
repositories, our goal is to (a) identify workflow complexities, (b) analyze
recurring and heterogeneous structuring patterns, (c) assess compliance with
GHA best practices, and (d) uncover differences in CI pipeline design across
programming languages. Our findings are expected to reveal both areas of strong
adherence to best practices and areas for improvement where needed. These
insights will also have implications for CI services, as they will highlight
the need for clearer guidelines and comprehensive examples in CI documentation.

</details>


### [7] [Identifier Name Similarities: An Exploratory Study](https://arxiv.org/abs/2507.18081)
*Carol Wong,Mai Abe,Silvia De Benedictis,Marissa Halim,Anthony Peruma*

Main category: cs.SE

TL;DR: The paper introduces a preliminary taxonomy for categorizing types of identifier name similarity in software projects to support further research on code comprehension and collaboration.


<details>
  <summary>Details</summary>
Motivation: Identifier names are critical for program comprehension, yet poorly chosen or highly similar identifier names can hinder understanding and collaboration. There is a need to systematically study and categorize the different types of identifier name similarity.

Method: The study involves the development of a taxonomy categorizing various forms of identifier name similarity, supported by an exploratory analysis of their occurrence in software projects.

Result: The researchers present a preliminary taxonomy that classifies different kinds of identifier name similarity observed in codebases.

Conclusion: The initial taxonomy will help researchers analyze how identifier name similarity affects code comprehension, maintainability, and developer collaboration, while serving as a foundation for further refinement and study.

Abstract: Identifier names, which comprise a significant portion of the codebase, are
the cornerstone of effective program comprehension. However, research has shown
that poorly chosen names can significantly increase cognitive load and hinder
collaboration. Even names that appear readable in isolation may lead to
misunderstandings in contexts when they closely resemble other names in either
structure or functionality. In this exploratory study, we present our
preliminary findings on the occurrence of identifier name similarity in
software projects through the development of a taxonomy that categorizes
different forms of identifier name similarity. We envision our initial taxonomy
providing researchers with a platform to analyze and evaluate the impact of
identifier name similarity on code comprehension, maintainability, and
collaboration among developers, while also allowing for further refinement and
expansion of the taxonomy.

</details>


### [8] [Understanding the Supply Chain and Risks of Large Language Model Applications](https://arxiv.org/abs/2507.18105)
*Yujie Ma,Lili Quan,Xiaofei Xie,Qiang Hu,Jiongchi Yu,Yao Zhang,Sen Chen*

Main category: cs.SE

TL;DR: LLM systems have complex supply chains with significant, often-overlooked vulnerabilities. The authors provide a first-of-its-kind dataset to map these risks, revealing widespread dependencies and security issues. They offer recommendations for improving supply chain security in LLM applications.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the growing risks associated with the complex supply chains of Large Language Model (LLM)-based systems, which rely on interconnected pretrained models, datasets, third-party libraries, and infrastructure. Existing risk assessments often neglect these broader supply chain vulnerabilities, creating a need for systematic benchmarks to evaluate and improve the security of LLM systems.

Method: The authors introduce the first comprehensive dataset for analyzing and benchmarking LLM supply chain security. They collect 3,859 real-world LLM applications and perform interdependency analysis to identify models, datasets, and libraries involved. The study extracts model fine-tuning paths, dataset reuse, and library reliance, and gathers risk-related issues from public vulnerability databases for empirical analysis.

Result: The empirical analysis shows that LLM applications have deeply nested dependencies and face significant vulnerabilities across their supply chain components, including models, datasets, and especially third-party libraries.

Conclusion: The paper highlights the pervasive security risks present in LLM supply chains and emphasizes the importance of comprehensive security analysis. Practical recommendations are provided to guide researchers and developers toward building safer and more trustworthy LLM-enabled systems.

Abstract: The rise of Large Language Models (LLMs) has led to the widespread deployment
of LLM-based systems across diverse domains. As these systems proliferate,
understanding the risks associated with their complex supply chains is
increasingly important. LLM-based systems are not standalone as they rely on
interconnected supply chains involving pretrained models, third-party
libraries, datasets, and infrastructure. Yet, most risk assessments narrowly
focus on model or data level, overlooking broader supply chain vulnerabilities.
While recent studies have begun to address LLM supply chain risks, there
remains a lack of benchmarks for systematic research.
  To address this gap, we introduce the first comprehensive dataset for
analyzing and benchmarking LLM supply chain security. We collect 3,859
real-world LLM applications and perform interdependency analysis, identifying
109,211 models, 2,474 datasets, and 9,862 libraries. We extract model
fine-tuning paths, dataset reuse, and library reliance, mapping the ecosystem's
structure. To evaluate security, we gather 1,555 risk-related issues-50 for
applications, 325 for models, 18 for datasets, and 1,229 for libraries from
public vulnerability databases.
  Using this dataset, we empirically analyze component dependencies and risks.
Our findings reveal deeply nested dependencies in LLM applications and
significant vulnerabilities across the supply chain, underscoring the need for
comprehensive security analysis. We conclude with practical recommendations to
guide researchers and developers toward safer, more trustworthy LLM-enabled
systems.

</details>


### [9] [NoCode-bench: A Benchmark for Evaluating Natural Language-Driven Feature Addition](https://arxiv.org/abs/2507.18130)
*Le Deng,Zhonghao Jiang,Jialun Cao,Michael Pradel,Zhongxin Liu*

Main category: cs.SE

TL;DR: LLMs struggle with real-world natural language-driven no-code development (15.79% success). NoCode-bench offers a standard to measure and improve on this task, revealing that major technical challenges remain.


<details>
  <summary>Details</summary>
Motivation: Enable software development with natural language (no-code) specifications using powerful LLMs, aiming for higher productivity and wider accessibility. Assess LLMs' real capability to handle real-world NL-driven software feature additions, as documentation often serves as NL specifications.

Method: The authors introduce NoCode-bench, a benchmark dataset featuring 634 tasks pulled from 10 real-world projects, linking natural language documentation updates with corresponding code implementations and validated by developer-written test cases. A smaller, highly curated subset (NoCode-bench Verified, 114 tasks) ensures reliable evaluation. Multiple LLMs are evaluated for their ability to edit codebases following NL specifications.

Result: State-of-the-art LLMs could only achieve a task success rate of 15.79%, even with high token usage. The primary challenges identified: difficulties with cross-file editing, comprehensive codebase understanding, and correct tool invocation.

Conclusion: Despite the promise of natural language-driven no-code development, current LLMs are not yet ready for such real-world tasks. NoCode-bench provides a foundation and benchmark for future improvements in NL-driven feature addition capabilities.

Abstract: Natural language-driven no-code development allows users to specify software
functionality using natural language (NL) instead of editing source code,
promising increased productivity and democratized development. Large language
models (LLMs) show potential in enabling this paradigm. In this context,
software documentation acts as an NL specification for functionality. This work
introduces NoCode-bench, a benchmark designed to evaluate LLMs on real-world
NL-driven feature addition tasks, consisting of 634 tasks across 10 projects
and 114k code changes. Each task pairs documentation updates with corresponding
code implementations, validated by developer-written test cases. A subset of
114 high-quality, human-verified instances, NoCode-bench Verified, ensures
reliable evaluation. Our experiments reveal that, despite high token usage, the
best LLMs achieve a task success rate of only 15.79%, highlighting challenges
in cross-file editing, codebase understanding, and tool calling. These findings
indicate that LLMs are not yet ready for fully NL-driven no-code development.
NoCode-bench lays the foundation for future advances in this area.

</details>


### [10] [SMECS: A Software Metadata Extraction and Curation Software](https://arxiv.org/abs/2507.18159)
*Stephan Ferenz,Aida Jafarbigloo,Oliver Werth,Astrid Nieße*

Main category: cs.SE

TL;DR: SMECS automates and streamlines the creation of high-quality research software metadata, supporting FAIR principles and providing a user-friendly interface, as confirmed by usability testing.


<details>
  <summary>Details</summary>
Motivation: Creating high-quality metadata is essential for implementing the FAIR (Findable, Accessible, Interoperable, Reusable) principles in research software. However, generating such metadata is often resource-intensive and challenging for researchers and software engineers.

Method: The authors developed the Software Metadata Extraction and Curation Software (SMECS), which automatically extracts metadata from platforms like GitHub and offers an interactive, user-friendly interface for metadata curation and export as a CodeMeta file. They evaluated the tool through usability experiments.

Result: Experiments showed that SMECS provides a satisfactory user experience in creating and curating research software metadata.

Conclusion: SMECS facilitates the FAIRification process of research software by making metadata extraction and curation easier and more accessible for users.

Abstract: Metadata play a crucial role in adopting the FAIR principles for research
software and enables findability and reusability. However, creating
high-quality metadata can be resource-intensive for researchers and research
software engineers. To address this challenge, we developed the Software
Metadata Extraction and Curation Software (SMECS) which integrates the
extraction of metadata from existing sources together with a user-friendly
interface for metadata curation. SMECS extracts metadata from online
repositories such as GitHub and presents it to researchers through an
interactive interface for further curation and export as a CodeMeta file. The
usability of SMECS was evaluated through usability experiments which confirmed
that SMECS provides a satisfactory user experience. SMECS supports the
FAIRification of research software by simplifying metadata creation.

</details>


### [11] [GenAI for Automotive Software Development: From Requirements to Wheels](https://arxiv.org/abs/2507.18223)
*Nenad Petrovic,Fengjunjie Pan,Vahid Zolfaghari,Krzysztof Lebioda,Andre Schamschurko,Alois Knoll*

Main category: cs.SE

TL;DR: The paper presents a GenAI-powered method using LLMs and RAG to automate code and test scenario generation for ADAS, enabling consistent, regulation-aware, and efficient automotive software development.


<details>
  <summary>Details</summary>
Motivation: Development for ADAS systems is complex, requires consistency, regulatory compliance, and fast iteration. Automating code and test scenario generation from requirements addresses the need for efficiency, reliability, and compliance in automotive software development.

Method: The methodology incorporates Large Language Models for model-based summarization, code generation (Python and C++), and requirements validation. It also uses Retrieval Augmented Generation to enhance scenario generation from regulatory documents, all embedded in a workflow based on Model-Driven Engineering.

Result: The approach demonstrates that requirements can be consistently checked and automatically translated into simulation scenarios and hardware-targeted implementation, potentially achieving shorter development and testing times for ADAS.

Conclusion: The paper concludes that integrating GenAI, LLMs, and RAG into the software development workflow of automotive systems can streamline code and test generation, enhance requirement consistency, and potentially shorten development, compliance, and re-engineering cycles for ADAS capabilities.

Abstract: This paper introduces a GenAI-empowered approach to automated development of
automotive software, with emphasis on autonomous and Advanced Driver Assistance
Systems (ADAS) capabilities. The process starts with requirements as input,
while the main generated outputs are test scenario code for simulation
environment, together with implementation of desired ADAS capabilities
targeting hardware platform of the vehicle connected to testbench. Moreover, we
introduce additional steps for requirements consistency checking leveraging
Model-Driven Engineering (MDE). In the proposed workflow, Large Language Models
(LLMs) are used for model-based summarization of requirements (Ecore metamodel,
XMI model instance and OCL constraint creation), test scenario generation,
simulation code (Python) and target platform code generation (C++).
Additionally, Retrieval Augmented Generation (RAG) is adopted to enhance test
scenario generation from autonomous driving regulations-related documents. Our
approach aims shorter compliance and re-engineering cycles, as well as reduced
development and testing time when it comes to ADAS-related capabilities.

</details>


### [12] [An Empirical Study on Embodied Artificial Intelligence Robot (EAIR) Software Bugs](https://arxiv.org/abs/2507.18267)
*Zeqin Liao,Zibin Zheng,Peifan Reng,Henglong Liang,Zixu Gao,Zhixiang Chen,Wei Li,Yuhong Nan*

Main category: cs.SE

TL;DR: The paper systematically analyzes 885 bugs in Embodied AI Robots, identifying symptoms, causes, and affected modules. It reveals several EAIR-specific issues, especially linked to AI-reasoning, and provides a mapping to help researchers diagnose and repair such bugs efficiently.


<details>
  <summary>Details</summary>
Motivation: There is a lack of in-depth understanding of bugs in Embodied Artificial Intelligence Robots (EAIR) systems, which impedes the development of effective practices and tools for bug diagnosis and repair in this rapidly evolving domain.

Method: The authors conducted a systematic empirical study by collecting and manually analyzing 885 EAIR system bugs from 80 different EAIR projects. They classified the bugs according to their symptoms, underlying causes, and associated modules.

Result: The study classified EAIR bugs into 18 causes, 15 symptoms, and 13 modules. It discovered 8 symptoms and 8 underlying causes unique to EAIR, mostly related to AI-agent reasoning and decision making, and found these symptoms are characterized by severe functional failures and potential hazards. The mapping of causes to modules enables more targeted bug diagnostics.

Conclusion: This study provides the first comprehensive taxonomy and mapping of EAIR bugs, symptoms, and causes, identifying several EAIR-specific aspects. The findings can inform more precise bug prediction, detection, and repair, and help focus future research and engineering efforts on the most vulnerable modules and causes in EAIR systems.

Abstract: Embodied Artificial Intelligence Robots (EAIR) is an emerging and rapidly
evolving technological domain. Ensuring their program correctness is
fundamental to their successful deployment. However, a general and in-depth
understanding of EAIR system bugs remains lacking, which hinders the
development of practices and techniques to tackle EAIR system bugs.
  To bridge this gap, we conducted the first systematic study of 885 EAIR
system bugs collected from 80 EAIR system projects to investigate their
symptoms, underlying causes, and module distribution. Our analysis takes
considerable effort, which classifies these bugs into 18 underlying causes, 15
distinct symptoms, and identifies 13 affected modules. It reveals several new
interesting findings and implications which help shed light on future research
on tackling or repairing EAIR system bugs. First, among the 15 identified
symptoms, our findings highlight 8 symptoms specific to EAIR systems, which is
characterized by severe functional failures and potential physical hazards.
Second, within the 18 underlying causes, we define 8 EAIR-specific causes, the
majority of which stem from the intricate issues of AI- agent reasoning and
decision making. Finally, to facilitate precise and efficient bug prediction,
detection, and repair, we constructed a mapping between underlying causes and
the modules in which they most frequently occur, which enables researchers to
focus diagnostic efforts on the modules most susceptible to specific bug types.

</details>


### [13] [Scheduzz: Constraint-based Fuzz Driver Generation with Dual Scheduling](https://arxiv.org/abs/2507.18289)
*Yan Li,Wenzhang Yang,Yuekun Wang,Jian Gao,Shaohua Wang,Yinxing Xue,Lijun Zhang*

Main category: cs.SE

TL;DR: Scheduzz is an LLM-driven library fuzzing tool that intelligently generates and schedules fuzz drivers based on real usage patterns, significantly improving efficiency and bug detection over existing solutions.


<details>
  <summary>Details</summary>
Motivation: Fuzzing libraries is currently a manual and expertise-driven process, requiring extensive knowledge to create effective fuzz drivers. Existing methods for automatic driver generation often produce irrational drivers and waste computational resources, as they fail to adhere to proper usage conventions and execute all generated drivers indiscriminately.

Method: The authors propose Scheduzz, an LLM-based fuzzing tool that uses large language models to infer rational API usage patterns and generate appropriate fuzz drivers. Scheduzz utilizes a dual scheduling framework that treats driver generation and fuzzing as an online optimization problem, selectively generating and executing drivers to maximize resource efficiency and coverage.

Result: Scheduzz was evaluated on 33 real-world libraries, where it reduced computational overhead compared to existing methods and achieved higher code coverage (1.62x, 1.50x, and 1.89x more than CKGFuzzer, Promptfuzz, and OSS-Fuzz respectively). It also discovered 33 previously unknown bugs, including 3 that were assigned security CVEs.

Conclusion: Scheduzz effectively addresses the limitations of prior automatic fuzz driver generation techniques by using LLMs to understand and adhere to library usage conventions and by employing an efficient driver execution framework. It demonstrates superior coverage and bug-finding abilities while reducing wasted computational resources.

Abstract: Fuzzing a library requires experts to understand the library usage well and
craft high-quality fuzz drivers, which is tricky and tedious. Therefore, many
techniques have been proposed to automatically generate fuzz drivers. However,
they fail to generate rational fuzz drivers due to the lack of adherence to
proper library usage conventions, such as ensuring a resource is closed after
being opened. To make things worse, existing library fuzzing techniques
unconditionally execute each driver, resulting in numerous irrational drivers
that waste computational resources while contributing little coverage and
generating false positive bug reports.
  To tackle these challenges, we propose a novel automatic library fuzzing
technique, Scheduzz, an LLM-based library fuzzing technique. It leverages LLMs
to understand rational usage of libraries and extract API combination
constraints. To optimize computational resource utilization, a dual scheduling
framework is implemented to efficiently manage API combinations and fuzz
drivers. The framework models driver generation and the corresponding fuzzing
campaign as an online optimization problem. Within the scheduling loop,
multiple API combinations are selected to generate fuzz drivers, while
simultaneously, various optimized fuzz drivers are scheduled for execution or
suspension.
  We implemented Scheduzz and evaluated it in 33 real-world libraries. Compared
to baseline approaches, Scheduzz significantly reduces computational overhead
and outperforms UTopia on 16 out of 21 libraries. It achieves 1.62x, 1.50x, and
1.89x higher overall coverage than the state-of-the-art techniques CKGFuzzer,
Promptfuzz, and the handcrafted project OSS-Fuzz, respectively. In addition,
Scheduzz discovered 33 previously unknown bugs in these well-tested libraries,
3 of which have been assigned CVEs.

</details>


### [14] [YATE: The Role of Test Repair in LLM-Based Unit Test Generation](https://arxiv.org/abs/2507.18316)
*Michael Konstantinou,Renzo Degiovanni,Jie M. Zhang,Mark Harman,Mike Papadakis*

Main category: cs.SE

TL;DR: The paper presents YATE, a technique to repair incorrect tests generated by language models using static analysis and re-prompting, resulting in significantly improved code coverage and fault detection over existing methods, at similar computational cost.


<details>
  <summary>Details</summary>
Motivation: Automated test generation using language models produces many incorrect tests, which are often easily detected and discarded. However, if these tests are repaired, they can provide significant testing value and serve as useful seeds for further test generation. Hence, leveraging and fixing these incorrect tests would enhance the overall effectiveness of automated test generation.

Method: The paper proposes a technique called YATE that combines rule-based static analysis and re-prompting to repair some of the incorrect tests generated by language models. The method is evaluated on six open-source projects.

Result: YATE can generate tests that achieve, on average, 32.06% more line coverage and kill 21.77% more mutants than standard LLM-based methods. When compared to other LLM-based test generation methods (HITS, SYMPROMPT, TESTSPARK, and COVERUP), YATE achieves 22% higher line coverage, 20% higher branch coverage, and kills 20% more mutants, with a similar cost in the number of LLM calls.

Conclusion: Repairing incorrect unit tests generated by language models is a promising approach. The proposed technique YATE effectively improves test coverage and fault detection compared to existing LLM-based test generation techniques, providing more value at a comparable computational cost.

Abstract: Recent advances in automated test generation utilises language models to
produce unit tests. While effective, language models tend to generate many
incorrect tests with respect to both syntax and semantics. Although such
incorrect tests can be easily detected and discarded, they constitute a "missed
opportunity" -- if fixed, they are often valuable as they directly add testing
value (they effectively target the underlying program logic to be tested) and
indirectly form good seeds for generating additional tests. To this end, we
propose a simple technique for repairing some of these incorrect tests through
a combination of rule-based static analysis and re-prompting. We evaluate this
simple approach, named YATE, on a set of 6 open-source projects and show that
it can effectively produce tests that cover on average 32.06% more lines and
kill 21.77% more mutants than a plain LLM-based method. We also compare YATE
with four other LLM-based methods, namely HITS, SYMPROMPT, TESTSPARK and
COVERUP and show that it produces tests that cover substantially more code.
YATE achieves 22% higher line coverage, 20% higher branch coverage and kill 20%
more mutants at a comparable cost (number of calls to LLMs).

</details>


### [15] [Gotta catch 'em all! Towards File Localisation from Issues at Large](https://arxiv.org/abs/2507.18319)
*Jesse Maarleveld,Jiapan Guo,Daniel Feitosa*

Main category: cs.SE

TL;DR: The paper presents a flexible dataset creation pipeline for issue file localisation and shows that bug-focussed methods are inadequate for general issue types. Differences in localisation performance exist across issue types and projects, indicating the need for adaptable models.


<details>
  <summary>Details</summary>
Motivation: There is a growing need to localize files for all issue types—not just bugs—in software projects, as traditional research has narrowly focused on bug reports. The motivation is to address the gap by creating approaches and datasets that apply broadly to diverse issue types to better aid developer productivity.

Method: The authors developed a data pipeline to produce issue file localisation datasets robust to varying code branching and merging practices. They conducted baseline evaluations using traditional information retrieval techniques and performed statistical analyses to examine biases and the effects of specific factors on performance.

Result: Methods reliant on bug-specific heuristics were found to perform poorly on broader issue types. There are small but statistically significant variations in localisation performance depending on the issue type, and the presence of identifiers only minimally affects most types. Results also showed significant project-dependent variation, suggesting the need for project-specific approaches.

Conclusion: Existing bug-localisation approaches do not generalize well to all issue types. There is a need for new, general-purpose or project-tunable models to improve file localisation across various issue categories.

Abstract: Bug localisation, the study of developing methods to localise the files
requiring changes to resolve bugs, has been researched for a long time to
develop methods capable of saving developers' time. Recently, researchers are
starting to consider issues outside of bugs. Nevertheless, most existing
research into file localisation from issues focusses on bugs or uses other
selection methods to ensure only certain types of issues are considered as part
of the focus of the work. Our goal is to work on all issues at large, without
any specific selection.
  In this work, we provide a data pipeline for the creation of issue file
localisation datasets, capable of dealing with arbitrary branching and merging
practices. We provide a baseline performance evaluation for the file
localisation problem using traditional information retrieval approaches.
Finally, we use statistical analysis to investigate the influence of biases
known in the bug localisation community on our dataset.
  Our results show that methods designed using bug-specific heuristics perform
poorly on general issue types, indicating a need for research into general
purpose models. Furthermore, we find that there are small, but statistically
significant differences in performance between different issue types. Finally,
we find that the presence of identifiers have a small effect on performance for
most issue types. Many results are project-dependent, encouraging the
development of methods which can be tuned to project-specific characteristics.

</details>


### [16] [FMI Meets SystemC: A Framework for Cross-Tool Virtual Prototyping](https://arxiv.org/abs/2507.18339)
*Nils Bosbach,Meik Schmidt,Lukas Jünger,Matthias Berthold,Rainer Leupers*

Main category: cs.SE

TL;DR: The paper introduces a framework that merges SystemC virtual platforms with FMI-based simulation tools, allowing realistic environmental data input for software testing. This enables more comprehensive and earlier verification, benefiting certification processes such as ISO 26262.


<details>
  <summary>Details</summary>
Motivation: As system complexity increases, thorough testing and prototyping require integrating multiple simulation tools, especially to verify interactions between hardware and their operating environments. SystemC is a key technology for virtual platforms but lacks easy co-simulation with other tools due to missing FMI support. There is a need to bridge this gap for improved testing, verification, and accelerated certification processes.

Method: The paper proposes a novel framework that enables SystemC-based virtual platforms (VPs) to communicate with other simulation tools using the Functional Mock-up Interface (FMI) standard. This is demonstrated through a case study where a SystemC simulation of a temperature sensor obtains real-time environmental data from an external FMI-compliant tool.

Result: The framework successfully allows the unmodified target software in the SystemC VP to receive realistic input from external tools, facilitating more extensive and accurate software testing and verification. The case study shows practical integration between SystemC and FMI, enabling a simulated system to interact with realistic environmental inputs like temperature, velocity, or acceleration.

Conclusion: By linking SystemC-based VPs with external simulation tools via the FMI standard, this framework enables more robust and realistic virtual prototyping. This advancement allows early, thorough software validation and testing, which expedites processes such as ISO 26262 certification, as tests and verifications can be completed before physical hardware becomes available.

Abstract: As systems become more complex, the demand for thorough testing and virtual
prototyping grows. To simulate whole systems, multiple tools are usually needed
to cover different parts. These parts include the hardware of a system and the
environment with which the system interacts. The Functional Mock-up Interface
(FMI) standard for co-simulation can be used to connect these tools.
  The control part of modern systems is usually a computing unit, such as a
System-on-a-Chip (SoC) or Microcontroller Unit (MCU), which executes software
from a connected memory and interacts with peripherals. To develop software
without requiring access to physical hardware, full-system simulators, the
so-called Virtual Platforms (VPs), are commonly used. The IEEE-standardized
framework for VP development is SystemC TLM. SystemC provides interfaces and
concepts that enable modular design and model exchange. However, SystemC lacks
native FMI support, which limits the integration into broader co-simulation
environments.
  This paper presents a novel framework to control and interact with
SystemC-based VPs using the FMI. We present a case study showing how a
simulated temperature sensor in a SystemC simulation can obtain temperature
values from an external tool via FMI. This approach allows the unmodified
target software to run on the VP and receive realistic environmental input data
such as temperature, velocity, or acceleration values from other tools. Thus,
extensive software testing and verification is enabled. By having tests ready
and the software pre-tested using a VP once the physical hardware is available,
certifications like ISO 26262 can be done earlier.

</details>


### [17] [Automated Code Review Using Large Language Models with Symbolic Reasoning](https://arxiv.org/abs/2507.18476)
*Busra Icoz,Goksel Biricik*

Main category: cs.SE

TL;DR: The paper introduces a hybrid method using symbolic reasoning and LLMs for automated code review, showing improved accuracy and efficiency over LLMs alone through evaluation on the CodexGlue dataset.


<details>
  <summary>Details</summary>
Motivation: Manual code review is essential but subjective and time-consuming, making it a prime candidate for automation. Although LLMs have promise for automating code review, they lack strong logical reasoning abilities.

Method: The study proposes a hybrid approach that integrates symbolic reasoning techniques with Large Language Models (LLMs) to automate code reviews. The approach was tested using the CodexGlue dataset and compared models such as CodeT5, CodeBERT, and GraphCodeBERT with and without symbolic reasoning enhancements.

Result: The hybrid approach, combining symbolic reasoning and LLM prompting, led to improved accuracy and efficiency in automated code review compared to using LLMs alone.

Conclusion: Integrating symbolic reasoning with LLM-based approaches enhances the effectiveness of automated code review, addressing the limitations of LLMs in logical reasoning and evaluation tasks.

Abstract: Code review is one of the key processes in the software development lifecycle
and is essential to maintain code quality. However, manual code review is
subjective and time consuming. Given its rule-based nature, code review is well
suited for automation. In recent years, significant efforts have been made to
automate this process with the help of artificial intelligence. Recent
developments in Large Language Models (LLMs) have also emerged as a promising
tool in this area, but these models often lack the logical reasoning
capabilities needed to fully understand and evaluate code. To overcome this
limitation, this study proposes a hybrid approach that integrates symbolic
reasoning techniques with LLMs to automate the code review process. We tested
our approach using the CodexGlue dataset, comparing several models, including
CodeT5, CodeBERT, and GraphCodeBERT, to assess the effectiveness of combining
symbolic reasoning and prompting techniques with LLMs. Our results show that
this approach improves the accuracy and efficiency of automated code review.

</details>


### [18] [A Deep Dive into Retrieval-Augmented Generation for Code Completion: Experience on WeChat](https://arxiv.org/abs/2507.18515)
*Zezhou Yang,Ting Peng,Cuiyun Gao,Chaozheng Wang,Hailiang Huang,Yuetang Deng*

Main category: cs.SE

TL;DR: This paper investigates RAG methods for code completion on WeChat’s massive closed-source codebase. It finds that similarity-based RAG—especially with combined lexical and semantic retrieval—significantly boosts LLM code completion performance in industrial settings, validated by both quantitative results and developer feedback.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address a gap in knowledge: while retrieval-augmented generation (RAG) methods have shown promise in improving code completion with LLMs on open-source code, their performance on closed-source, industrial-scale codebases remains unexplored. Since closed-source environments may differ from open-source ones, understanding RAG's effectiveness there is important for real-world applications.

Method: The authors perform an empirical study using WeChat’s large, proprietary codebase. They examine two types of RAG methods for code completion: identifier-based and similarity-based RAG. They test 26 open-source LLMs (ranging from 0.5B to 671B parameters) and explore different retrieval strategies, including lexical (BM25) and semantic (GTE-Qwen) approaches. The study also includes a developer survey for real-world validation.

Result: Both RAG methods work effectively in the closed-source codebase, with similarity-based RAG outperforming identifier-based RAG. Advanced retrieval methods further enhance similarity-based RAG, with BM25 (lexical) and GTE-Qwen (semantic) being particularly effective. Combining lexical and semantic retrieval provides the best results. Developer feedback validates the utility of RAG in practice.

Conclusion: RAG methods, particularly those combining advanced similarity-based techniques, significantly improve code completion in industrial, closed-source settings and provide real-world value, bridging the gap between public benchmarks and proprietary software contexts.

Abstract: Code completion, a crucial task in software engineering that enhances
developer productivity, has seen substantial improvements with the rapid
advancement of large language models (LLMs). In recent years,
retrieval-augmented generation (RAG) has emerged as a promising method to
enhance the code completion capabilities of LLMs, which leverages relevant
context from codebases without requiring model retraining. While existing
studies have demonstrated the effectiveness of RAG on public repositories and
benchmarks, the potential distribution shift between open-source and
closed-source codebases presents unique challenges that remain unexplored. To
mitigate the gap, we conduct an empirical study to investigate the performance
of widely-used RAG methods for code completion in the industrial-scale codebase
of WeChat, one of the largest proprietary software systems. Specifically, we
extensively explore two main types of RAG methods, namely identifier-based RAG
and similarity-based RAG, across 26 open-source LLMs ranging from 0.5B to 671B
parameters. For a more comprehensive analysis, we employ different retrieval
techniques for similarity-based RAG, including lexical and semantic retrieval.
Based on 1,669 internal repositories, we achieve several key findings: (1) both
RAG methods demonstrate effectiveness in closed-source repositories, with
similarity-based RAG showing superior performance, (2) the effectiveness of
similarity-based RAG improves with more advanced retrieval techniques, where
BM25 (lexical retrieval) and GTE-Qwen (semantic retrieval) achieve superior
performance, and (3) the combination of lexical and semantic retrieval
techniques yields optimal results, demonstrating complementary strengths.
Furthermore, we conduct a developer survey to validate the practical utility of
RAG methods in real-world development environments.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [19] [Higher-Order Behavioural Conformances via Fibrations](https://arxiv.org/abs/2507.18509)
*Henning Urbat*

Main category: cs.PL

TL;DR: The paper introduces a categorical and highly general framework for establishing program congruence in higher-order languages—including quantitative (probabilistic) ones—by generalizing Howe's method via abstract operational semantics and behavioral fibrations, with successful application to important examples like bisimilarity and behavioral metrics.


<details>
  <summary>Details</summary>
Motivation: Coinductive reasoning is essential for verifying behavioral equivalence in higher-order programming languages. The rise of quantitative language features (e.g., probabilistic constructs) requires more sophisticated notions of behavioral conformance, such as behavioral distance. A critical challenge is ensuring that these conformances respect the structure of the language, demanding robust and adaptable proof techniques.

Method: The authors propose a uniform, categorical approach to Howe's method using two abstraction layers: (1) model the higher-order language with Abstract Higher-Order Specifications (AHOS), providing a general categorical operational semantic framework; (2) represent behavioral conformances (relations or metrics) as fibrations over the AHOS's base category. They establish soundness through a general congruence theorem under natural conditions.

Result: The main result is a fundamental congruence theorem: for any language modeled by AHOS and appropriate categorical conditions, the greatest behavioral (bi)conformance induces a program congruence. The framework unifies treatment across different languages and notions of conformance. The approach is validated by deriving congruence results for bisimilarity and behavioral pseudometrics in probabilistic higher-order languages.

Conclusion: This work generalizes and abstracts Howe's method, allowing for broader and more uniform proofs of program congruence in both classical and quantitative higher-order languages. The categorical methodology enables systematic congruence theorems for various behavioral conformances.

Abstract: Coinduction is a widely used technique for establishing behavioural
equivalence of programs in higher-order languages. In recent years, the rise of
languages with quantitative (e.g.~probabilistic) features has led to extensions
of coinductive methods to more refined types of behavioural conformances, most
notably notions of behavioural distance. To guarantee soundness of coinductive
reasoning, one needs to show that the behavioural conformance at hand forms a
program congruence, i.e. it is suitably compatible with the operations of the
language. This is usually achieved by a complex proof technique known as
\emph{Howe's method}, which needs to be carefully adapted to both the specific
language and the targeted notion of behavioural conformance. We develop a
uniform categorical approach to Howe's method that features two orthogonal
dimensions of abstraction: (1) the underlying higher-order language is modelled
by an \emph{abstract higher-order specification} (AHOS), a novel and very
general categorical account of operational semantics, and (2) notions of
behavioural conformance (such as relations or metrics) are modelled via
fibrations over the base category of an AHOS. Our main result is a fundamental
congruence theorem at this level of generality: Under natural conditions on the
categorical ingredients and the operational rules of a language modelled by an
AHOS, the greatest behavioural (bi)conformance on its operational model forms a
congruence. We illustrate our theory by deriving congruence of bisimilarity and
behavioural pseudometrics for probabilistic higher-order languages.

</details>
