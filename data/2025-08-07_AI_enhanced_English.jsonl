{"id": "2508.03830", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.03830", "abs": "https://arxiv.org/abs/2508.03830", "authors": ["Hanwen Guo", "Ben Greenman"], "title": "If-T: A Benchmark for Type Narrowing", "comment": null, "summary": "**Context:** The design of static type systems that can validate\ndynamically-typed programs (**gradually**) is an ongoing challenge. A key\ndifficulty is that dynamic code rarely follows datatype-driven design. Programs\ninstead use runtime tests to narrow down the proper usage of incoming data.\nType systems for dynamic languages thus need a **type narrowing** mechanism\nthat refines the type environment along individual control paths based on\ndominating tests, a form of flow-sensitive typing. In order to express\nrefinements, the type system must have some notion of sets and subsets. Since\nset-theoretic types are computationally and ergonomically complex, the need for\ntype narrowing raises design questions about how to balance precision and\nperformance. **Inquiry:** To date, the design of type narrowing systems has\nbeen driven by intuition, past experience, and examples from users in various\nlanguage communities. There is no standard that captures desirable and\nundesirable behaviors. Prior formalizations of narrowing are also significantly\nmore complex than a standard type system, and it is unclear how the extra\ncomplexity pays off in terms of concrete examples. This paper addresses the\nproblems through If-T, a language-agnostic **design benchmark** for type\nnarrowing that characterizes the abilities of implementations using simple\nprograms that draw attention to fundamental questions. Unlike a traditional\nperformance-focused benchmark, If-T measures a narrowing system's ability to\nvalidate correct code and reject incorrect code. Unlike a test suite, systems\nare not required to fully conform to If-T. Deviations are acceptable provided\nthey are justified by well-reasoned design considerations, such as compile-time\nperformance. **Approach:** If-T is guided by the literature on type narrowing,\nthe documentation of gradual languages such as TypeScript, and experiments with\ntypechecker implementations. We have identified a set of core technical\ndimensions for type narrowing. For each dimension, the benchmark contains a set\nof topics and (at least) two characterizing programs per topic: one that should\ntypecheck and one that should not typecheck. **Knowledge:** If-T provides a\nbaseline to measure type narrowing systems. For researchers, it provides\ncriteria to categorize future designs via its collection of positive and\nnegative examples. For language designers, the benchmark demonstrates the\npayoff of typechecker complexity in terms of concrete examples. Designers can\nuse the examples to decide whether supporting a particular example is\nworthwhile. Both the benchmark and its implementations are freely available\nonline. **Grounding:** We have implemented the benchmark for five typecheckers:\nTypeScript, Flow, Typed Racket, mypy, and Pyright. The results highlight\nimportant differences, such as the ability to track logical implications among\nprogram variables and typechecking for user-defined narrowing predicates.\n**Importance:** Type narrowing is essential for gradual type systems, but the\ntradeoffs between systems with different complexity have been unclear. If-T\nclarifies these tradeoffs by illustrating the benefits and limitations of each\nlevel of complexity. With If-T as a way to assess implementations in a fair,\ncross-language manner, future type system designs can strive for a better\nbalance among precision, annotation burden, and performance.", "AI": {"tldr": "This paper presents If-T, a language-independent benchmark for evaluating how type systems narrow types in gradually-typed languages. By applying If-T to major typecheckers, it reveals key strengths and weaknesses, helping designers balance precision, usability, and performance. If-T provides a standard, fair way to assess and guide future type system development.", "motivation": "The motivation of this paper stems from the challenge of designing static type systems that can effectively validate dynamically-typed programs, particularly by handling how such programs rely on runtime tests and not rigid datatype-driven patterns. There is a key need for type narrowing mechanisms that can account for these practical realities, but existing approaches mostly rely on intuition and lack consensus or clear benchmarks to evaluate their tradeoffs.", "method": "The paper introduces If-T, a language-agnostic design benchmark comprised of fundamental, small programs that stress-test type narrowing systems. The benchmark defines core technical dimensions, and for each, it presents programs that should and shouldn't typecheck. If-T allows deviations from its expectations if well-justified, ensuring practical flexibility. The benchmark is informed by literature review, language documentation, and practical experiments and is implemented across five popular typecheckers.", "result": "Implementing If-T on TypeScript, Flow, Typed Racket, mypy, and Pyright revealed significant differences among these systems, notably in their abilities to track logical implications and handle user-defined narrowing predicates. The results highlight where increased typechecker complexity leads to tangible benefits (or not), thereby revealing the tradeoffs of different design approaches.", "conclusion": "If-T establishes a practical, cross-language means for systematically evaluating type narrowing systems. It enables both researchers and language designers to better gauge the value of increased typechecker complexity and to make more informed, justified decisions regarding design tradeoffs in gradual typing systems. This helps clarify and standardize the evaluation process for future type system innovations."}}
{"id": "2508.03831", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.03831", "abs": "https://arxiv.org/abs/2508.03831", "authors": ["Chinmayi Prabhu Baramashetru", "Paola Giannini", "Silvia Lizeth Tapia Tarifa", "Olaf Owe"], "title": "A Type System for Data Privacy Compliance in Active Object Languages", "comment": null, "summary": "Data protection laws such as GDPR aim to give users unprecedented control\nover their personal data. Compliance with these regulations requires\nsystematically considering information flow and interactions among entities\nhandling sensitive data. Privacy-by-design principles advocate embedding data\nprotection into system architectures as a default. However, translating these\nabstract principles into concrete, explicit methods remains a significant\nchallenge. This paper addresses this gap by proposing a language-based approach\nto privacy integration, combining static and runtime techniques. By employing\ntype checking and type inference in an active object language, the framework\nenables the tracking of authorised data flows and the automatic generation of\nconstraints checked at runtime based on user consent. This ensures that\npersonal data is processed in compliance with GDPR constraints. The key\ncontribution of this work is a type system that gather the compliance checks\nand the changes to users consent and integrates data privacy compliance\nverification into system execution. The paper demonstrates the feasibility of\nthis approach through a soundness proof and several examples, illustrating how\nthe proposed language addresses common GDPR requirements, such as user consent,\npurpose limitation, and data subject rights. This work advances the state of\nthe art in privacy-aware system design by offering a systematic and automated\nmethod for integrating GDPR compliance into programming languages. This\ncapability has implications for building trustworthy systems in domains such as\nhealthcare or finance, where data privacy is crucial.", "AI": {"tldr": "This paper presents a new programming framework that systematically enforces GDPR privacy requirements using type systems and runtime checks, helping developers automate and verify data protection in software systems.", "motivation": "The paper is motivated by the difficulty of translating abstract privacy-by-design principles and data protection regulations, such as GDPR, into concrete and operationalized methods for software systems. There is a need for systematic and explicit support for GDPR compliance within programming languages and system architectures.", "method": "The authors propose a language-based approach that combines static and runtime techniques, specifically using type checking and type inference within an active object language. The framework enables tracking of authorized data flows and automates the generation of runtime constraints based on user consent. The approach incorporates a type system for gathering compliance checks and monitoring changes to user consent, integrating privacy compliance verification into system execution. The feasibility is demonstrated with a soundness proof and example applications addressing GDPR requirements.", "result": "The result is a language framework with a novel type system that supports the systematic and automated integration of GDPR compliance into system design. This includes tracking authorized data flows, enforcing user consent constraints, and addressing core GDPR principles like purpose limitation and data subject rights. Feasibility is proven by theoretical soundness and practical examples.", "conclusion": "The paper concludes that its language-based approach significantly advances privacy-aware system design by making GDPR compliance an integral, automated, and verifiable part of programming languages. This paves the way for building trustworthy systems where data privacy is essential, particularly in sensitive domains like healthcare and finance."}}
{"id": "2508.03832", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.03832", "abs": "https://arxiv.org/abs/2508.03832", "authors": ["Andreas Pointner", "Josef Pichler", "Herbert Pr\u00e4hofer"], "title": "Generating Inputs for Grammar Mining using Dynamic Symbolic Execution", "comment": null, "summary": "A vast number of software systems include components that parse and process\nstructured input. In addition to programming languages, which are analyzed by\ncompilers or interpreters, there are numerous components that process\nstandardized or proprietary data formats of varying complexity. Even if such\ncomponents were initially developed and tested based on a specification, such\nas a grammar, numerous modifications and adaptations over the course of\nsoftware evolution can make it impossible to precisely determine which inputs\nthey actually accept. In this situation, grammar mining can be used to\nreconstruct the specification in the form of a grammar. Established approaches\nalready produce useful results, provided that sufficient input data is\navailable to fully cover the input language. However, achieving this\ncompleteness is a major challenge. In practice, only input data recorded during\nthe operation of the software systems is available. If this data is used for\ngrammar mining, the resulting grammar reflects only the actual processed inputs\nbut not the complete grammar of the input language accepted by the software\ncomponent. As a result, edge cases or previously supported features that no\nlonger appear in the available input data are missing from the generated\ngrammar. This work addresses this challenge by introducing a novel approach for\nthe automatic generation of inputs for grammar mining. Although input\ngenerators have already been used for fuzz testing, it remains unclear whether\nthey are also suitable for grammar miners. Building on the grammar miner Mimid,\nthis work presents a fully automated approach to input generation. The approach\nleverages Dynamic Symbolic Execution (DSE) and extends it with two mechanisms\nto overcome the limitations of DSE regarding structured input parsers. First,\nthe search for new inputs is guided by an iterative expansion that starts with\na single-character input and gradually extends it. Second, input generation is\nstructured into a novel three-phase approach, which separates the generation of\ninputs for parser functions. The proposed method was evaluated against a\ndiverse set of eleven benchmark applications from the existing literature.\nResults demonstrate that the approach achieves precision and recall for\nextracted grammars close to those derived from state-of-the-art grammar miners\nsuch as Mimid. Notably, it successfully uncovers subtle features and edge cases\nin parsers that are typically missed by such grammar miners. The effectiveness\nof the method is supported by empirical evidence, showing that it can achieve\nhigh performance in various domains without requiring prior input samples. This\ncontribution is significant for researchers and practitioners in software\nengineering, offering an automated, scalable, and precise solution for grammar\nmining. By eliminating the need for manual input generation, the approach not\nonly reduces workload but also enhances the robustness and comprehensiveness of\nthe extracted grammars. Following this approach, software engineers can\nreconstruct specification from existing (legacy) parsers.", "AI": {"tldr": "This paper introduces an automated input generation method for grammar mining, overcoming the lack of comprehensive input samples in real-world scenarios. By extending Dynamic Symbolic Execution and using a three-phase approach, it achieves high precision in extracting grammars, including edge cases, from software parsers, thus reducing manual effort and improving reconstruction of legacy system specifications.", "motivation": "Software components that process structured input often lack updated or complete specifications due to continual evolution and modifications. Existing grammar mining methods depend heavily on the availability of comprehensive input samples, which are rare in practical scenarios, thus failing to reconstruct the full specification, especially missing edge cases or obsolete features. There is a need for an automated approach that can generate diverse and comprehensive input samples for grammar mining without manual intervention.", "method": "The proposed method builds on the grammar miner Mimid and introduces a fully automated approach for input generation using Dynamic Symbolic Execution (DSE). This method incorporates two key innovations: (1) an iterative input expansion starting from a single character and gradually increasing complexity, and (2) a novel three-phase process for generating inputs specifically for parser functions. This approach aims to overcome the limitations of DSE for structured input parsers and is evaluated on eleven benchmark applications.", "result": "The evaluation demonstrates that the proposed method achieves precision and recall in extracting grammars very close to leading grammar miners like Mimid, and importantly, it discovers subtle features and edge cases usually missed by existing methods. It performs well across different domains, and does not require any prior input samples. Empirical results verify its effectiveness, robustness, and scalability.", "conclusion": "The work presents a fully automated and scalable input generation technique for grammar mining, significantly reducing manual effort while improving the comprehensiveness and accuracy of reconstructed grammars. This innovation helps researchers and software engineers recover specifications from legacy systems, enhancing robustness and efficiency in software engineering tasks."}}
{"id": "2508.04115", "categories": ["cs.PL", "A.1; C.1.2; D.3.1; F.3.1; F.3.2"], "pdf": "https://arxiv.org/pdf/2508.04115", "abs": "https://arxiv.org/abs/2508.04115", "authors": ["Roger C. Su", "Robert J. Colvin"], "title": "Weak Memory Model Formalisms: Introduction and Survey", "comment": null, "summary": "Memory consistency models define the order in which accesses to shared memory\nin a concurrent system may be observed to occur. Such models are a necessity\nsince program order is not a reliable indicator of execution order, due to\nmicroarchitectural features or compiler transformations. Concurrent\nprogramming, already a challenging task, is thus made even harder when weak\nmemory effects must be addressed. A rigorous specification of weak memory\nmodels is therefore essential to make this problem tractable for developers of\nsafety- and security-critical, low-level software.\n  In this paper we survey the field of formalisations of weak memory models,\nincluding their specification, their effects on execution, and tools and\ninference systems for reasoning about code. To assist the discussion we also\nprovide an introduction to two styles of formal representation found commonly\nin the literature (using a much simplified version of Intel's x86 as the\nexample): a step-by-step construction of traces of the system (operational\nsemantics); and with respect to relations between memory events (axiomatic\nsemantics). The survey covers some long-standing hardware features that lead to\nobservable weak behaviours, a description of historical developments in\npractice and in theory, an overview of computability and complexity results,\nand outlines current and future directions in the field.", "AI": {"tldr": "This paper surveys formal approaches to weak memory models, discussing their specifications, reasoning tools, hardware implications, and theoretical advances. It provides an introduction to both operational and axiomatic representations, reviews historical and current developments, and points to ongoing research challenges in the field.", "motivation": "Developers of safety- and security-critical, low-level software face significant challenges when dealing with weak memory effects in concurrent programming, as program order does not reliably represent execution order. Rigorous specifications of weak memory models are crucial to manage this complexity.", "method": "This paper surveys the formalization of weak memory models by reviewing existing specifications, their effects on execution, and reasoning tools. It introduces two formal approaches\u2014operational semantics (step-by-step traces) and axiomatic semantics (relations between memory events)\u2014using simplified x86 architecture as an example. The paper also examines hardware features, historical developments, and computability/complexity results, with attention to current and future research directions.", "result": "The survey compiles and elucidates various formalizations and reasoning approaches to weak memory models, clarifies hardware phenomena behind observable weak behaviors, and contextualizes theoretical and practical evolution in the field. It highlights the progress of formal tools and identifies outstanding challenges and research frontiers.", "conclusion": "A rigorous and comprehensive understanding of weak memory models, their formal representations, reasoning tools, and practical consequences is fundamental for developers and researchers in safety- and security-critical software. This survey serves as a valuable reference by consolidating knowledge and outlining evolving directions in weak memory model research."}}
{"id": "2508.03846", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03846", "abs": "https://arxiv.org/abs/2508.03846", "authors": ["Hashini Gunatilake", "John Grundy", "Rashina Hoda", "Ingo Mueller"], "title": "Empathy Guidelines for Improving Practitioner Well-being & Software Engineering Practices", "comment": null, "summary": "Empathy is a powerful yet often overlooked element in software engineering\n(SE), supporting better teamwork, smoother communication, and effective\ndecision-making. In our previous study, we identified a range of practitioner\nstrategies for fostering empathy in SE contexts. Building on these insights,\nthis paper introduces 17 actionable empathy guidelines designed to support\npractitioners, teams, and organisations. We also explore how these guidelines\ncan be implemented in practice by examining real-world applications,\nchallenges, and strategies to overcome them shared by software practitioners.\nTo support adoption, we present a visual prioritisation framework that\ncategorises the guidelines based on perceived importance, ease of\nimplementation, and willingness to adopt. The findings offer practical and\nflexible suggestions for integrating empathy into everyday SE work, helping\nteams move from principles to sustainable action.", "AI": {"tldr": "This paper provides 17 practical empathy guidelines for software engineering, explores their real-world use, discusses implementation challenges, and offers a prioritization framework to help teams integrate sustainable empathetic practices.", "motivation": "Empathy is often overlooked in software engineering (SE), despite its significant benefits for teamwork, communication, and decision-making. The authors aim to address this gap by providing practical guidance for integrating empathy in SE workplaces.", "method": "The authors introduce 17 actionable empathy guidelines and examine their implementation in real-world SE practice. They analyze challenges, applications, and solutions shared by practitioners, and present a visual prioritization framework to assist with adoption of the guidelines.", "result": "The paper presents a set of 17 empathy guidelines, examples of their practical application, common challenges, and a framework to help practitioners prioritize and adopt these measures based on importance, ease, and willingness.", "conclusion": "Integrating empathy through practical guidelines and a prioritization tool can help SE teams translate empathy principles into sustained, everyday practices for improved teamwork and communication."}}
{"id": "2508.03856", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.03856", "abs": "https://arxiv.org/abs/2508.03856", "authors": ["Richard Hegewald", "Rebecca Beyer"], "title": "Evaluating Software Supply Chain Security in Research Software", "comment": "Accepted at conference GI SKILL 2025", "summary": "The security of research software is essential for ensuring the integrity and\nreproducibility of scientific results. However, research software security is\nstill largely unexplored. Due to its dependence on open source components and\ndistributed development practices, research software is particularly vulnerable\nto supply chain attacks. This study analyses 3,248 high-quality, largely\npeer-reviewed research software repositories using the OpenSSF Scorecard. We\nfind a generally weak security posture with an average score of 3.5/10.\nImportant practices, such as signed releases and branch protection, are rarely\nimplemented. Finally, we present actionable, low-effort recommendations that\ncan help research teams improve software security and mitigate potential\nthreats to scientific integrity.", "AI": {"tldr": "Research software is generally insecure, with common vulnerabilities due to poor implementation of important security practices. The study identifies these issues and provides straightforward recommendations for improvement.", "motivation": "Research software is crucial for scientific integrity and reproducibility, but its security remains under-explored, especially given its reliance on open source components and distributed development, which make it vulnerable to supply chain attacks.", "method": "The authors analyzed 3,248 high-quality, mostly peer-reviewed research software repositories using the OpenSSF Scorecard tool to assess their security practices.", "result": "The analysis revealed that research software generally has weak security, with an average OpenSSF Scorecard score of 3.5 out of 10. Key security practices like signed releases and branch protection are rarely used.", "conclusion": "Research software currently has insufficient security measures, exposing it to potential threats. The study offers practical, low-effort recommendations to help research teams strengthen their software security and thereby protect scientific integrity."}}
{"id": "2508.03881", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03881", "abs": "https://arxiv.org/abs/2508.03881", "authors": ["Martin Obaidi", "Kushtrim Qengaj", "Jakob Droste", "Hannah Deters", "Marc Herrmann", "Jil Kl\u00fcnder", "Elisa Schmid", "Kurt Schneider"], "title": "From App Features to Explanation Needs: Analyzing Correlations and Predictive Potential", "comment": "This paper has been accepted at the 33rd IEEE International\n  Requirements Engineering Workshop (REW 2025)", "summary": "In today's digitized world, software systems must support users in\nunderstanding both how to interact with a system and why certain behaviors\noccur. This study investigates whether explanation needs, classified from user\nreviews, can be predicted based on app properties, enabling early consideration\nduring development and large-scale requirements mining. We analyzed a gold\nstandard dataset of 4,495 app reviews enriched with metadata (e.g., app\nversion, ratings, age restriction, in-app purchases). Correlation analyses\nidentified mostly weak associations between app properties and explanation\nneeds, with moderate correlations only for specific features such as app\nversion, number of reviews, and star ratings. Linear regression models showed\nlimited predictive power, with no reliable forecasts across configurations.\nValidation on a manually labeled dataset of 495 reviews confirmed these\nfindings. Categories such as Security & Privacy and System Behavior showed\nslightly higher predictive potential, while Interaction and User Interface\nremained most difficult to predict. Overall, our results highlight that\nexplanation needs are highly context-dependent and cannot be precisely inferred\nfrom app metadata alone. Developers and requirements engineers should therefore\nsupplement metadata analysis with direct user feedback to effectively design\nexplainable and user-centered software systems.", "AI": {"tldr": "App properties and metadata can't reliably predict when users will need explanations; direct user feedback is still essential for designing explainable software.", "motivation": "The motivation is to help developers understand users' needs for explanations when interacting with software, and to identify whether these needs can be predicted early using app metadata, thus aiding requirement mining and system design.", "method": "The authors analyzed a gold standard set of 4,495 app reviews with enriched metadata. They used correlation analyses to examine relationships between app properties and explanation needs, followed by linear regression models to test predictive power, and validated their findings on a separate, manually labeled dataset of 495 reviews.", "result": "There were mostly weak associations between app properties and explanation needs, with only moderate correlations for features like app version, review count, and ratings. Predictive models had very limited reliability. Some categories (e.g., Security & Privacy, System Behavior) had slightly better predictability, but others (Interaction, UI) remained difficult to predict. Overall, metadata alone was insufficient for accurately inferring explanation needs.", "conclusion": "User explanation needs are highly context-dependent and cannot be predicted precisely using app metadata alone. Supplemental user feedback is necessary for creating explainable and user-centered systems."}}
{"id": "2508.03922", "categories": ["cs.SE", "cs.HC", "D.2.1"], "pdf": "https://arxiv.org/pdf/2508.03922", "abs": "https://arxiv.org/abs/2508.03922", "authors": ["Soroush Heydari"], "title": "A Human Centric Requirements Engineering Framework for Assessing Github Copilot Output", "comment": "8 pages", "summary": "The rapid adoption of Artificial Intelligence(AI) programming assistants such\nas GitHub Copilot introduces new challenges in how these software tools address\nhuman needs. Many existing evaluation frameworks address technical aspects such\nas code correctness and efficiency, but often overlook crucial human factors\nthat affect the successful integration of AI assistants in software development\nworkflows. In this study, I analyzed GitHub Copilot's interaction with users\nthrough its chat interface, measured Copilot's ability to adapt explanations\nand code generation to user expertise levels, and assessed its effectiveness in\nfacilitating collaborative programming experiences. I established a\nhuman-centered requirements framework with clear metrics to evaluate these\nqualities in GitHub Copilot chat. I discussed the test results and their\nimplications for future analysis of human requirements in automated\nprogramming.", "AI": {"tldr": "Traditional benchmarks miss how AI assistants fit with real users. This paper evaluates GitHub Copilot's ability to adapt to different user types and support collaboration, introducing a new framework for measuring these human aspects. Results point toward better ways to assess AI helpers' impact on actual developer workflows.", "motivation": "Although AI programming assistants like GitHub Copilot are widely adopted, current evaluation frameworks mainly focus on technical metrics (e.g., code correctness), neglecting important human factors critical for effective integration into software development workflows.", "method": "The study involved analyzing Copilot's chat-based interactions with users, specifically measuring its adaptability to varying user expertise and its efficacy in supporting collaborative programming. The author developed a human-centered requirements framework with defined metrics for evaluating these human factors.", "result": "The research assessed how well Copilot adapts its explanations and code generation according to user expertise and how effective it is in collaborative programming via its chat interface. The study provides empirical data and analysis on these aspects using the newly proposed requirements framework.", "conclusion": "The study highlights the importance of human-centered evaluation for AI programming assistants. It presents a framework and initial findings for assessing GitHub Copilot's human factor performance, offering a foundation for future studies in this area."}}
{"id": "2508.03931", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03931", "abs": "https://arxiv.org/abs/2508.03931", "authors": ["Everton Guimaraes", "Nathalia Nascimento", "Chandan Shivalingaiah", "Asish Nelapati"], "title": "Analyzing Prominent LLMs: An Empirical Study of Performance and Complexity in Solving LeetCode Problems", "comment": "11 pages, 13 figures, 29th International Conference on Evaluation and\n  Assessment in Software Engineering (EASE)", "summary": "Large Language Models (LLMs) like ChatGPT, Copilot, Gemini, and DeepSeek are\ntransforming software engineering by automating key tasks, including code\ngeneration, testing, and debugging. As these models become integral to\ndevelopment workflows, a systematic comparison of their performance is\nessential for optimizing their use in real world applications. This study\nbenchmarks these four prominent LLMs on one hundred and fifty LeetCode problems\nacross easy, medium, and hard difficulties, generating solutions in Java and\nPython. We evaluate each model based on execution time, memory usage, and\nalgorithmic complexity, revealing significant performance differences. ChatGPT\ndemonstrates consistent efficiency in execution time and memory usage, while\nCopilot and DeepSeek show variability as task complexity increases. Gemini,\nalthough effective on simpler tasks, requires more attempts as problem\ndifficulty rises. Our findings provide actionable insights into each model's\nstrengths and limitations, offering guidance for developers selecting LLMs for\nspecific coding tasks and providing insights on the performance and complexity\nof GPT-like generated solutions.", "AI": {"tldr": "This paper benchmarks ChatGPT, Copilot, Gemini, and DeepSeek on 150 LeetCode coding problems in Java and Python, revealing that ChatGPT is consistently efficient, Copilot and DeepSeek handle complexity with variability, and Gemini struggles as tasks get harder. The findings help developers make informed choices about which LLM to use for different coding scenarios.", "motivation": "With Large Language Models (LLMs) like ChatGPT, Copilot, Gemini, and DeepSeek becoming important tools in software engineering tasks (code generation, testing, debugging), there is a need to systematically compare their performance to help optimize their use in real-world scenarios.", "method": "The study benchmarks ChatGPT, Copilot, Gemini, and DeepSeek by having them solve 150 LeetCode problems of varying difficulty (easy, medium, hard) in Java and Python. The comparison focuses on execution time, memory usage, and algorithmic complexity of generated solutions.", "result": "ChatGPT shows consistent efficiency in both execution time and memory usage across different problem difficulties. Copilot and DeepSeek demonstrate greater performance variability, especially as problems get harder. Gemini performs well on easy tasks but requires more attempts as the difficulty increases.", "conclusion": "There are significant differences in how major LLMs perform on coding tasks. Developers should consider these strengths and weaknesses when choosing an LLM for specific tasks. The study offers actionable recommendations based on empirical benchmarking."}}
{"id": "2508.03949", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03949", "abs": "https://arxiv.org/abs/2508.03949", "authors": ["Md. Abdul Awal", "Mrigank Rochan", "Chanchal K. Roy"], "title": "Model Compression vs. Adversarial Robustness: An Empirical Study on Language Models for Code", "comment": null, "summary": "Transformer-based language models for code have shown remarkable performance\nin various software analytics tasks, but their adoption is hindered by high\ncomputational costs, slow inference speeds, and substantial environmental\nimpact. Model compression techniques such as pruning, quantization, and\nknowledge distillation have gained traction in addressing these challenges.\nHowever, the impact of these strategies on the robustness of compressed\nlanguage models for code in adversarial scenarios remains poorly understood.\nUnderstanding how these compressed models behave under adversarial attacks is\nessential for their safe and effective deployment in real-world applications.\nTo bridge this knowledge gap, we conduct a comprehensive evaluation of how\ncommon compression strategies affect the adversarial robustness of compressed\nmodels. We assess the robustness of compressed versions of three widely used\nlanguage models for code across three software analytics tasks, using six\nevaluation metrics and four commonly used classical adversarial attacks. Our\nfindings indicate that compressed models generally maintain comparable\nperformance to their uncompressed counterparts. However, when subjected to\nadversarial attacks, compressed models exhibit significantly reduced\nrobustness. These results reveal a trade-off between model size reduction and\nadversarial robustness, underscoring the need for careful consideration when\ndeploying compressed models in security-critical software applications. Our\nstudy highlights the need for further research into compression strategies that\nstrike a balance between computational efficiency and adversarial robustness,\nwhich is essential for deploying reliable language models for code in\nreal-world software applications.", "AI": {"tldr": "Compressing language models for code saves resources but makes them more vulnerable to adversarial attacks, highlighting a need for strategies that balance efficiency and security.", "motivation": "Transformer-based language models for code are effective for software analytics tasks, but their high computational costs and environmental impact hinder widespread adoption. Model compression techniques aim to address these issues, but their effects on adversarial robustness are not well understood.", "method": "The authors conducted a comprehensive evaluation of common model compression strategies (pruning, quantization, knowledge distillation) on three popular code language models. They tested these on three software analytics tasks, using six evaluation metrics and four different classical adversarial attacks to assess robustness.", "result": "Compressed models maintained comparable performance to uncompressed models under normal conditions. However, their robustness was significantly reduced when exposed to adversarial attacks.", "conclusion": "There is a clear trade-off between model size reduction and adversarial robustness. This trade-off must be carefully considered when deploying compressed language models for code, especially in security-critical applications. More research is needed to develop compression methods that ensure both efficiency and robustness."}}
{"id": "2508.04125", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04125", "abs": "https://arxiv.org/abs/2508.04125", "authors": ["Sangwon Hyun", "Hyunjun Kim", "Jinhyuk Jang", "Hyojin Choi", "M. Ali Babar"], "title": "Experimental Analysis of Productive Interaction Strategy with ChatGPT: User Study on Function and Project-level Code Generation Tasks", "comment": "The benchmark repository has not been publicly released yet due to\n  the IP policy in our institutions. If you would like to use the benchmark or\n  collaborate on extension, please contact \"dr.sangwon.hyun@gmail.com\"", "summary": "The application of Large Language Models (LLMs) is growing in the productive\ncompletion of Software Engineering tasks. Yet, studies investigating the\nproductive prompting techniques often employed a limited problem space,\nprimarily focusing on well-known prompting patterns and mainly targeting\nfunction-level SE practices. We identify significant gaps in real-world\nworkflows that involve complexities beyond class-level (e.g., multi-class\ndependencies) and different features that can impact Human-LLM Interactions\n(HLIs) processes in code generation. To address these issues, we designed an\nexperiment that comprehensively analyzed the HLI features regarding the code\ngeneration productivity. Our study presents two project-level benchmark tasks,\nextending beyond function-level evaluations. We conducted a user study with 36\nparticipants from diverse backgrounds, asking them to solve the assigned tasks\nby interacting with the GPT assistant using specific prompting patterns. We\nalso examined the participants' experience and their behavioral features during\ninteractions by analyzing screen recordings and GPT chat logs. Our statistical\nand empirical investigation revealed (1) that three out of 15 HLI features\nsignificantly impacted the productivity in code generation; (2) five primary\nguidelines for enhancing productivity for HLI processes; and (3) a taxonomy of\n29 runtime and logic errors that can occur during HLI processes, along with\nsuggested mitigation plans.", "AI": {"tldr": "This paper explores how different human-LLM interaction features affect productivity in complex code generation tasks, moving beyond simple function-level evaluations. Through user studies and comprehensive analysis, it identifies key features impacting productivity, proposes guidelines, and catalogs typical errors and solutions.", "motivation": "While Large Language Models (LLMs) are increasingly used for software engineering tasks, most current studies focus on simple prompts and function-level tasks, neglecting more complex scenarios such as multi-class dependencies that are closer to real-world software development. There is a need to understand the human-LLM interaction (HLI) features that impact productivity during code generation at a higher level of complexity.", "method": "The authors designed an experiment involving two project-level software engineering benchmark tasks requiring participants to interact with GPT using specific prompting patterns. A user study with 36 diverse participants was conducted, during which behavioral features and interactions were captured via screen recordings and chat logs. Both statistical and empirical analyses were performed to evaluate which HLI features affected code generation productivity.", "result": "(1) Three out of 15 analyzed HLI features were found to significantly impact productivity in code generation; (2) five key guidelines were identified to improve productivity in HLI processes; (3) a taxonomy of 29 runtime and logic errors was developed, along with proposed mitigation plans for these errors.", "conclusion": "The study demonstrates the importance of identifying and optimizing key HLI features to improve productivity in real-world software engineering tasks using LLMs. The provided guidelines and error taxonomy offer actionable insights for enhancing human-LLM collaboration in code generation beyond function-level tasks."}}
{"id": "2508.04295", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.04295", "abs": "https://arxiv.org/abs/2508.04295", "authors": ["Chaofan Wang", "Tingrui Yu", "Jie Wang", "Dong Chen", "Wenrui Zhang", "Yuling Shi", "Xiaodong Gu", "Beijun Shen"], "title": "EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust Translation", "comment": null, "summary": "Rust's compile-time safety guarantees make it ideal for safety-critical\nsystems, creating demand for translating legacy C codebases to Rust. While\nvarious approaches have emerged for this task, they face inherent trade-offs:\nrule-based solutions face challenges in meeting code safety and idiomaticity\nrequirements, while LLM-based solutions often fail to generate semantically\nequivalent Rust code, due to the heavy dependencies of modules across the\nentire codebase. Recent studies have revealed that both solutions are limited\nto small-scale programs. In this paper, we propose EvoC2Rust, an automated\nframework for converting entire C projects to equivalent Rust ones. EvoC2Rust\nemploys a skeleton-guided translation strategy for project-level translation.\nThe pipeline consists of three evolutionary stages: 1) it first decomposes the\nC project into functional modules, employs a feature-mapping-enhanced LLM to\ntransform definitions and macros and generates type-checked function stubs,\nwhich form a compilable Rust skeleton; 2) it then incrementally translates the\nfunction, replacing the corresponding stub placeholder; 3) finally, it repairs\ncompilation errors by integrating LLM and static analysis. Through evolutionary\naugmentation, EvoC2Rust combines the advantages of both rule-based and\nLLM-based solutions. Our evaluation on open-source benchmarks and six\nindustrial projects demonstrates EvoC2Rust's superior performance in\nproject-level C-to-Rust translation. On average, it achieves 17.24% and 14.32%\nimprovements in syntax and semantic accuracy over the LLM-based approaches,\nalong with a 96.79% higher code safety rate than the rule-based tools. At the\nmodule level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates\non industrial projects, even for complex codebases and long functions.", "AI": {"tldr": "EvoC2Rust is an automated framework for converting large C projects to safe, idiomatic Rust, using a multi-step process that outperforms existing methods in both safety and accuracy, making large-scale translation feasible.", "motivation": "The motivation is the high demand for translating legacy C codebases to Rust, driven by Rust's compile-time safety guarantees, especially for safety-critical systems. Existing translation approaches either produce non-idiomatic, unsafe code or fail to generate semantically equivalent Rust for large codebases due to heavy dependencies and are limited to small-scale programs.", "method": "The paper proposes EvoC2Rust, an automated framework using a skeleton-guided translation strategy. The process involves three evolutionary stages: (1) Decompose C projects into functional modules and use an LLM enhanced with feature-mapping to generate a compilable Rust skeleton; (2) Incrementally translate function bodies by replacing stubs; (3) Repair compilation errors by integrating LLM and static analysis.", "result": "EvoC2Rust outperforms prior approaches on both open-source and industrial projects. It achieves on average 17.24% and 14.32% improvements in syntax and semantic accuracy over prior LLM-based tools, and 96.79% higher code safety compared to rule-based tools. For module-level evaluation, it achieves 92.25% compilation and 89.53% test pass rates, even for large, complex industrial projects.", "conclusion": "EvoC2Rust provides a scalable, automated solution for project-level C-to-Rust translation, combining the strengths of rule-based and LLM-based approaches, significantly improving code safety and translation accuracy for complex and large-scale codebases."}}
{"id": "2508.04352", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.04352", "abs": "https://arxiv.org/abs/2508.04352", "authors": ["Dragana Sunaric", "Charlotte Verbruggen", "Dominik Bork"], "title": "Vanilla-Converter: A Tool for Converting Camunda 7 BPMN Models into Camunda 8 Models", "comment": null, "summary": "As organizations prepare for the end-of-life of Camunda 7, manual migration\nremains complex due to fundamental differences between the two platforms. We\npresent Vanilla-Converter, a command-line tool that facilitates the migration\nof BPMN models from Camunda 7 to Camunda 8. Vanilla-Converter automates the\ntransformation process, supports a wide range of BPMN elements, and produces a\ntransformed model and a detailed transformation log indicating automatic\nchanges and remaining manual conversion tasks. The tool's effectiveness is\ndemonstrated through three case studies with real industrially used Camunda 7\nmodels, confirming its ability to convert these models into valid and\nexecutable Camunda 8 models.", "AI": {"tldr": "Vanilla-Converter is a tool that automates migration of BPMN models from Camunda 7 to 8, easing the process and proving effective in practical industrial cases.", "motivation": "Organizations must migrate their BPMN models because Camunda 7 is reaching end-of-life, but manual migration is difficult due to significant platform differences.", "method": "The authors developed Vanilla-Converter, a command-line tool that automates the transformation of BPMN models from Camunda 7 to Camunda 8. It supports various BPMN elements, provides a transformed model, and generates a detailed log of the automated and remaining manual conversion tasks.", "result": "The tool was evaluated on three real-world Camunda 7 models in industrial settings, successfully converting them into valid and executable Camunda 8 models.", "conclusion": "Vanilla-Converter effectively reduces manual migration effort by automating most of the BPMN model transformation process from Camunda 7 to Camunda 8, as demonstrated in multiple case studies."}}
{"id": "2508.04408", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.04408", "abs": "https://arxiv.org/abs/2508.04408", "authors": ["Carlos Andr\u00e9s Ram\u00edrez Cata\u00f1o", "Makoto Itoh"], "title": "Breaking New Ground in Software Defect Prediction: Introducing Practical and Actionable Metrics with Superior Predictive Power for Enhanced Decision-Making", "comment": "16 pages, 2 figures, 2 formulas, 12 tables", "summary": "Software defect prediction using code metrics has been extensively researched\nover the past five decades. However, prediction harnessing non-software metrics\nis under-researched. Considering that the root cause of software defects is\noften attributed to human error, human factors theory might offer key\nforecasting metrics for actionable insights. This paper explores automated\nsoftware defect prediction at the method level based on the developers' coding\nhabits. First, we propose a framework for deciding the metrics to conduct\npredictions. Next, we compare the performance of our metrics to that of the\ncode and commit history metrics shown by research to achieve the highest\nperformance to date. Finally, we analyze the prediction importance of each\nmetric. As a result of our analyses of twenty-one critical infrastructure\nlarge-scale open-source software projects, we have presented: (1) a human\nerror-based framework with metrics useful for defect prediction at method\nlevel; (2) models using our proposed metrics achieve better average prediction\nperformance than the state-of-the-art code metrics and history measures; (3)\nthe prediction importance of all metrics distributes differently with each of\nthe novel metrics having better average importance than code and history\nmetrics; (4) the novel metrics dramatically enhance the explainability,\npracticality, and actionability of software defect prediction models,\nsignificantly advancing the field. We present a systematic approach to\nforecasting defect-prone software methods via a human error framework. This\nwork empowers practitioners to act on predictions, empirically demonstrating\nhow developer coding habits contribute to defects in software systems.", "AI": {"tldr": "By shifting from just code metrics to developer behavior-based metrics, this research offers a new, more effective, and more actionable way to predict and address software defects at the method level.", "motivation": "Traditional software defect prediction heavily relies on code metrics, but often overlooks non-software, human-related factors. Since many defects are caused by human error, incorporating developer coding habits could improve prediction and provide more actionable insights.", "method": "The paper introduces a novel framework to identify human error-related metrics for software defect prediction at the method level. It compares the predictive power of these new metrics against traditional code and commit history metrics across 21 large-scale open-source projects. The importance of each metric type is also analyzed.", "result": "The proposed human error-based metrics lead to models that outperform state-of-the-art predictors using only code and commit history metrics. The importance of these new metrics is generally higher, and they also significantly enhance the explainability, practicality, and actionability of defect prediction models.", "conclusion": "Incorporating developer coding habits\u2014viewed through a human error framework\u2014significantly improves defect prediction performance, insightfulness, and practical utility. This represents a meaningful advance in the field and provides practitioners with more actionable predictions."}}
{"id": "2508.04448", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.04448", "abs": "https://arxiv.org/abs/2508.04448", "authors": ["Damian Gnieciak", "Tomasz Szandala"], "title": "Large Language Models Versus Static Code Analysis Tools: A Systematic Benchmark for Vulnerability Detection", "comment": null, "summary": "Modern software relies on a multitude of automated testing and quality\nassurance tools to prevent errors, bugs and potential vulnerabilities. This\nstudy sets out to provide a head-to-head, quantitative and qualitative\nevaluation of six automated approaches: three industry-standard rule-based\nstatic code-analysis tools (SonarQube, CodeQL and Snyk Code) and three\nstate-of-the-art large language models hosted on the GitHub Models platform\n(GPT-4.1, Mistral Large and DeepSeek V3). Using a curated suite of ten\nreal-world C# projects that embed 63 vulnerabilities across common categories\nsuch as SQL injection, hard-coded secrets and outdated dependencies, we measure\nclassical detection accuracy (precision, recall, F-score), analysis latency,\nand the developer effort required to vet true positives. The language-based\nscanners achieve higher mean F-1 scores,0.797, 0.753 and 0.750, than their\nstatic counterparts, which score 0.260, 0.386 and 0.546, respectively. LLMs'\nadvantage originates from superior recall, confirming an ability to reason\nacross broader code contexts. However, this benefit comes with substantial\ntrade-offs: DeepSeek V3 exhibits the highest false-positive ratio, all language\nmodels mislocate issues at line-or-column granularity due to tokenisation\nartefacts. Overall, language models successfully rival traditional static\nanalysers in finding real vulnerabilities. Still, their noisier output and\nimprecise localisation limit their standalone use in safety-critical audits. We\ntherefore recommend a hybrid pipeline: employ language models early in\ndevelopment for broad, context-aware triage, while reserving deterministic\nrule-based scanners for high-assurance verification. The open benchmark and\nJSON-based result harness released with this paper lay a foundation for\nreproducible, practitioner-centric research into next-generation automated code\nsecurity.", "AI": {"tldr": "This paper shows that large language models outperform traditional static code-analysis tools in finding vulnerabilities in real-world software, mostly thanks to better recall and code context reasoning. However, LLMs generate more false positives and struggle with precise error localization, making them less suitable alone for high-assurance needs. The authors recommend combining both approaches in development pipelines and provide tools supporting reproducible future research.", "motivation": "Modern software development depends on automated testing and quality assurance tools to prevent vulnerabilities. However, the effectiveness of traditional rule-based static code-analysis tools versus newer large language model (LLM)-based approaches for vulnerability detection has not been comprehensively compared, especially on real-world projects.", "method": "The study conducts both quantitative and qualitative evaluation of six automated code analysis approaches: three rule-based static code-analysis tools (SonarQube, CodeQL, Snyk Code) and three large language models (GPT-4.1, Mistral Large, DeepSeek V3) from GitHub's platform. Ten real-world C# projects with 63 known vulnerabilities are used as benchmarks. Detection accuracy (precision, recall, F1-score), analysis latency, and developer verification effort are measured.", "result": "LLMs achieved higher F1-scores (0.797, 0.753, 0.750) than traditional static analyzers (0.260, 0.386, 0.546), mainly due to their higher recall, indicating stronger ability to reason across code context. However, LLMs also produced more false positives, struggled with precise localization of vulnerabilities due to tokenization, and required more developer effort to vet results.", "conclusion": "Modern LLMs can rival or outperform rule-based tools in finding real vulnerabilities, but their higher rate of false positives and imprecise localization currently limit their standalone use in safety-critical environments. A hybrid pipeline\u2014using LLMs for broad triage early and rule-based tools for final verification\u2014is recommended. The paper also contributes an open benchmark and result harness for future research."}}
{"id": "2508.04479", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.04479", "abs": "https://arxiv.org/abs/2508.04479", "authors": ["Hashini Gunatilake", "John Grundy", "Rashina Hoda", "Ingo Mueller"], "title": "Manifestations of Empathy in Software Engineering: How, Why, and When It Matters", "comment": null, "summary": "Empathy plays a crucial role in software engineering (SE), influencing\ncollaboration, communication, and decision-making. While prior research has\nhighlighted the importance of empathy in SE, there is limited understanding of\nhow empathy manifests in SE practice, what motivates SE practitioners to\ndemonstrate empathy, and the factors that influence empathy in SE work. Our\nstudy explores these aspects through 22 interviews and a large scale survey\nwith 116 software practitioners. Our findings provide insights into the\nexpression of empathy in SE, the drivers behind empathetic practices, SE\nactivities where empathy is perceived as useful or not, and the other factors\nthat influence empathy. In addition, we offer practical implications for SE\npractitioners and researchers, offering a deeper understanding of how to\neffectively integrate empathy into SE processes.", "AI": {"tldr": "This paper explores how empathy is shown and motivated in software engineering, using interviews and a survey, and provides insights and practical tips for integrating empathy into SE work.", "motivation": "Empathy is recognized as important in software engineering, affecting collaboration, communication, and decision-making, but there is limited detailed understanding of how empathy is expressed in SE practice and what motivates its demonstration.", "method": "The researchers conducted 22 interviews and a large-scale survey with 116 software practitioners to investigate how empathy is manifested, what motivates it, and what factors influence it in SE.", "result": "The study identified specific ways empathy is expressed in SE, the motivations behind empathetic practices, SE activities where empathy is found to be useful or not, and additional factors that influence empathy. Practical suggestions are provided for SE practitioners and researchers.", "conclusion": "The study deepens understanding of empathy in software engineering by revealing its expression, motivations, and influencing factors, while offering actionable insights for better integrating empathy into SE processes."}}
