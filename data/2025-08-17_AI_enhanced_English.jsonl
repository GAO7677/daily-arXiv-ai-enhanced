{"id": "2508.10781", "categories": ["cs.PL", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.10781", "abs": "https://arxiv.org/abs/2508.10781", "authors": ["Abtin Molavi", "Amanda Xu", "Ethan Cecchetti", "Swamit Tannu", "Aws Albarghouthi"], "title": "Generating Compilers for Qubit Mapping and Routing", "comment": null, "summary": "Quantum computers promise to solve important problems faster than classical\ncomputers, potentially unlocking breakthroughs in materials science, chemistry,\nand beyond. Optimizing compilers are key to realizing this potential, as they\nminimize expensive resource usage and limit error rates. A critical compilation\nstep is qubit mapping and routing (QMR), which finds mappings from circuit\nqubits to qubits on a target device and plans instruction execution while\nsatisfying the device's connectivity constraints. The challenge is that the\nlandscape of quantum architectures is incredibly diverse and fast-evolving.\nGiven this diversity, hundreds of papers have addressed the QMR problem for\ndifferent qubit hardware, connectivity constraints, and quantum error\ncorrection schemes.\n  We present an approach for automatically generating qubit mapping and routing\ncompilers for arbitrary quantum architectures. Though each QMR problem is\ndifferent, we identify a common core structure-device state machine-that we use\nto formulate an abstract QMR problem. Our formulation naturally leads to a\ndomain-specific language, Marol, for specifying QMR problems-for example, the\nwell-studied NISQ mapping and routing problem requires only 12 lines of Marol.\nWe demonstrate that QMR problems, defined in Marol, can be solved with a\npowerful parametric solver that can be instantiated for any Marol program. We\nevaluate our approach through case studies of important QMR problems from prior\nand recent work, covering noisy and fault-tolerant quantum architectures on all\nmajor hardware platforms. Our thorough evaluation shows that generated\ncompilers are competitive with handwritten, specialized compilers in terms of\nruntime and solution quality. We envision that our approach will simplify\ndevelopment of future quantum compilers as new quantum architectures continue\nto emerge.", "AI": {"tldr": "The paper introduces a generic framework and language (Marol) for automatically generating quantum circuit mapping and routing compilers for arbitrary architectures, showing competitive performance to manual solutions and promising faster, easier compiler development as quantum hardware diversifies.", "motivation": "Quantum architectures are rapidly evolving and diverse, presenting significant challenges for efficiently mapping and routing qubits in quantum circuits. Existing solutions are typically specialized for particular hardware and constraints, requiring extensive manual effort.", "method": "The authors developed an abstract QMR (qubit mapping and routing) problem formulation using a device state machine, enabling generalization across architectures. This led to the creation of Marol, a domain-specific language for defining QMR problems, and a parametric solver that automatically generates compilers based on Marol specifications.", "result": "Case studies across various quantum architectures (both noisy and fault-tolerant, from all major hardware platforms) demonstrate that automatically generated compilers using this approach match or exceed the performance of manually written special-purpose compilers in terms of runtime and solution quality.", "conclusion": "The proposed framework and Marol DSL significantly reduce the effort required to develop quantum compilers for new, diverse architectures, aiding future quantum computing compiler development as hardware evolves."}}
{"id": "2508.10068", "categories": ["cs.SE", "cs.CL", "cs.IR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.10068", "abs": "https://arxiv.org/abs/2508.10068", "authors": ["Xiaohan Chen", "Zhongying Pan", "Quan Feng", "Yu Tian", "Shuqun Yang", "Mengru Wang", "Lina Gong", "Yuxia Geng", "Piji Li", "Xiang Chen"], "title": "SaraCoder: Orchestrating Semantic and Structural Cues for Profit-Oriented Repository-Level Code Completion", "comment": null, "summary": "Retrieval-augmented generation (RAG) for repository-level code completion\ncommonly relies on superficial text similarity, leading to results plagued by\nsemantic misguidance, redundancy, and homogeneity, while also failing to\nresolve external symbol ambiguity. To address these challenges, we introduce\nSaracoder, a Hierarchical Feature-Optimized retrieval framework. Its core\nHierarchical Feature Optimization module systematically refines candidates by\ndistilling deep semantic relationships, pruning exact duplicates, assessing\nstructural similarity with a novel graph-based metric that weighs edits by\ntheir topological importance, and reranking results to maximize both relevance\nand diversity. Furthermore, an External-Aware Identifier Disambiguator module\naccurately resolves cross-file symbol ambiguity via dependency analysis.\nExtensive experiments on the challenging CrossCodeEval and RepoEval-Updated\nbenchmarks demonstrate that Saracoder significantly outperforms existing\nbaselines across multiple programming languages and models. Our work proves\nthat systematically refining retrieval results across multiple dimensions\nprovides a new paradigm for building more accurate and robust repository-level\ncode completion systems.", "AI": {"tldr": "The paper introduces Saracoder, a hierarchical retrieval framework that goes beyond simple text similarity for code completion by leveraging deep semantics, structural analysis, and identifier disambiguation, resulting in superior performance over prior methods.", "motivation": "Existing repository-level code completion using retrieval-augmented generation (RAG) suffers from problems such as semantic misguidance, redundancy, homogeneity, and inability to resolve external symbol ambiguity. Superficial text similarity is inadequate for effective retrieval.", "method": "Saracoder is introduced\u2014a Hierarchical Feature-Optimized retrieval framework. It employs a Hierarchical Feature Optimization module to distill deep semantic relationships, remove duplicates, assess structural similarity using a novel graph-based metric, and rerank results for relevance/diversity. An External-Aware Identifier Disambiguator resolves cross-file symbol ambiguity via dependency analysis.", "result": "Extensive experiments on CrossCodeEval and RepoEval-Updated benchmarks show that Saracoder significantly outperforms existing baselines across programming languages and models.", "conclusion": "Systematic refinement of retrieval results through multiple dimensions (semantic, structural, and cross-file analysis) establishes a new paradigm for accurate and robust repository-level code completion."}}
{"id": "2508.10059", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10059", "abs": "https://arxiv.org/abs/2508.10059", "authors": ["Yueke Zhang", "Yifan Zhang", "Kevin Leach", "Yu Huang"], "title": "FormalGrad: Integrating Formal Methods with Gradient-Based LLM Refinement", "comment": "6 Pages", "summary": "While Large Language Models (LLMs) have demonstrated remarkable capabilities\nin code generation, they often produce solutions that lack guarantees of\ncorrectness, robustness, and efficiency. The limitation is acute in domains\nrequiring strict constraints. FormalGrad introduces a principled framework that\nintegrates formal methods directly into an iterative LLM-based generation loop.\nIt uniquely treats code as a differentiable variable, converting structured\nfeedback and formal constraints into a textual pseudo-gradient. This gradient\nguides the model to iteratively refine solutions, ensuring they are not only\nfunctional but also robust and formally justified. We evaluate FormalGrad on\nthe HumanEval, HumanEval+, and LiveCodeBench benchmarks. Our implementation\noutperforms strong baselines, achieving an absolute improvement of up to 27% on\nHumanEval and a 41% relative improvement on the challenging LiveCodeBench V6.\nFormalGrad generates formally justified code that is robust and efficient,\npaving the way for reliable AI-assisted software development in high-stakes\napplications.", "AI": {"tldr": "FormalGrad combines formal methods with LLMs in code generation, resulting in more correct and robust code; it significantly outperforms current methods on major benchmarks.", "motivation": "LLMs generate code with high creativity but often lack guarantees regarding correctness, robustness, and efficiency, especially for tasks with strict constraints.", "method": "The paper proposes FormalGrad, a framework that integrates formal methods into an iterative LLM-based code generation loop. It treats code as a differentiable variable and converts formal constraints into a textual pseudo-gradient that guides LLMs to refine the generated code iteratively.", "result": "FormalGrad shows substantial performance gains on benchmarks: up to 27% absolute improvement on HumanEval and 41% relative improvement on LiveCodeBench V6 compared to baselines. The generated code is both robust and formally justified.", "conclusion": "FormalGrad enables LLMs to produce code that is not only functional but also meets formal correctness and robustness requirements, making it suitable for reliable AI-assisted software development in critical domains."}}
{"id": "2508.10074", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10074", "abs": "https://arxiv.org/abs/2508.10074", "authors": ["Ruofan Lu", "Yintong Huo", "Meng Zhang", "Yichen Li", "Michael R. Lyu"], "title": "Next Edit Prediction: Learning to Predict Code Edits from Context and Interaction History", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has led to the\nwidespread adoption of AI-powered coding assistants integrated into a\ndevelopment environment. On one hand, low-latency code completion offers\ncompletion suggestions but is fundamentally constrained to the cursor's current\nposition. On the other hand, chat-based editing can perform complex\nmodifications, yet forces developers to stop their work, describe the intent in\nnatural language, which causes a context-switch away from the code. This\ncreates a suboptimal user experience, as neither paradigm proactively predicts\nthe developer's next edit in a sequence of related edits. To bridge this gap\nand provide the seamless code edit suggestion, we introduce the task of Next\nEdit Prediction, a novel task designed to infer developer intent from recent\ninteraction history to predict both the location and content of the subsequent\nedit. Specifically, we curate a high-quality supervised fine-tuning dataset and\nan evaluation benchmark for the Next Edit Prediction task. Then, we conduct\nsupervised fine-tuning on a series of models and performed a comprehensive\nevaluation of both the fine-tuned models and other baseline models, yielding\nseveral novel findings. This work lays the foundation for a new interaction\nparadigm that proactively collaborate with developers by anticipating their\nfollowing action, rather than merely reacting to explicit instructions.", "AI": {"tldr": "Current code assistants are limited by reactive paradigms. This paper proposes 'Next Edit Prediction'\u2014a proactive model that anticipates developers' edits. Using a curated dataset and extensive evaluation, their models outperform baselines, paving the way for smarter, seamless code assistance.", "motivation": "Existing AI-powered coding assistants either offer code completion suggestions constrained to the cursor (low-latency) or allow chat-based editing that requires developers to articulate their intent, causing disruptive context-switching. Neither approach proactively anticipates the developer's next move, which impacts user experience.", "method": "The authors introduce the 'Next Edit Prediction' task, which predicts both the location and content of the developer's next code edit based on recent interaction history. They curate a supervised fine-tuning dataset and evaluation benchmark for this task, then fine-tune several models and compare the results to baseline models.", "result": "Fine-tuned models on the Next Edit Prediction task outperform baseline models in predicting developer edits, yielding new insights into intent inference and proactive assistant behavior.", "conclusion": "This research establishes Next Edit Prediction as a foundational task that enables coding assistants to anticipate developer actions, creating a seamless and collaborative interaction paradigm that's superior to merely responsive tools."}}
{"id": "2508.10157", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10157", "abs": "https://arxiv.org/abs/2508.10157", "authors": ["Ajibode Adekunle", "Abdul Ali Bangash", "Bram Adams", "Ahmed E. Hassan"], "title": "On the synchronization between Hugging Face pre-trained language models and their upstream GitHub repository", "comment": null, "summary": "Pretrained language models (PTLMs) have advanced natural language processing\n(NLP), enabling progress in tasks like text generation and translation. Like\nsoftware package management, PTLMs are trained using code and environment\nscripts in upstream repositories (e.g., GitHub, GH) and distributed as variants\nvia downstream platforms like Hugging Face (HF). Coordinating development\nbetween GH and HF poses challenges such as misaligned release timelines,\ninconsistent versioning, and limited reuse of PTLM variants. We conducted a\nmixed-method study of 325 PTLM families (904 HF variants) to examine how commit\nactivities are coordinated. Our analysis reveals that GH contributors typically\nmake changes related to specifying the version of the model, improving code\nquality, performance optimization, and dependency management within the\ntraining scripts, while HF contributors make changes related to improving model\ndescriptions, data set handling, and setup required for model inference.\nFurthermore, to understand the synchronization aspects of commit activities\nbetween GH and HF, we examined three dimensions of these activities -- lag\n(delay), type of synchronization, and intensity -- which together yielded eight\ndistinct synchronization patterns. The prevalence of partially synchronized\npatterns, such as Disperse synchronization and Sparse synchronization, reveals\nstructural disconnects in current cross-platform release practices. These\npatterns often result in isolated changes -- where improvements or fixes made\non one platform are never replicated on the other -- and in some cases,\nindicate an abandonment of one repository in favor of the other. Such\nfragmentation risks exposing end users to incomplete, outdated, or behaviorally\ninconsistent models. Hence, recognizing these synchronization patterns is\ncritical for improving oversight and traceability in PTLM release workflows.", "AI": {"tldr": "Pretrained language model development is fragmented across GitHub and Hugging Face, often lacking proper synchronization in updates and releases. This study found prevalent patterns of partial and poor synchronization, threatening model quality and user safety. Better coordination is needed for reliable PTLM releases.", "motivation": "Pretrained language models (PTLMs) have become integral to NLP, but their development and distribution spans across upstream repositories (like GitHub) and downstream platforms (like Hugging Face). Managing releases, versioning, and reuse across these sites is challenging, leading to potential fragmentation and inconsistency.", "method": "A mixed-method study was conducted, examining 325 PTLM families and 904 Hugging Face variants. The study analyzed commit activities in both GitHub and Hugging Face, investigating differences in contribution focus and synchronization patterns across three dimensions: lag (delay), type of synchronization, and intensity.", "result": "GitHub contributors mostly focused on technical aspects like versioning, code quality, performance, and dependencies, while Hugging Face contributors focused on documentation, dataset handling, and inference setup. The synchronization analysis identified eight distinct patterns, with a prevalence of partially synchronized (disperse and sparse) patterns, highlighting structural disconnects and fragmentation in release practices.", "conclusion": "The lack of synchronization between GitHub and Hugging Face leads to isolated or abandoned repositories, risking incomplete or inconsistent models for users. Recognizing and addressing these synchronization patterns is crucial for better traceability, oversight, and reliability in PTLM workflows."}}
{"id": "2508.10517", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10517", "abs": "https://arxiv.org/abs/2508.10517", "authors": ["Likai Ye", "Mengliang Li", "Dehai Zhao", "Jiamou Sun", "Xiaoxue Ren"], "title": "Bridging Solidity Evolution Gaps: An LLM-Enhanced Approach for Smart Contract Compilation Error Resolution", "comment": "International Conference on Software Maintenance and Evolution\n  (ICSME) 2025", "summary": "Solidity, the dominant smart contract language for Ethereum, has rapidly\nevolved with frequent version updates to enhance security, functionality, and\ndeveloper experience. However, these continual changes introduce significant\nchallenges, particularly in compilation errors, code migration, and\nmaintenance. Therefore, we conduct an empirical study to investigate the\nchallenges in the Solidity version evolution and reveal that 81.68% of examined\ncontracts encounter errors when compiled across different versions, with 86.92%\nof compilation errors.\n  To mitigate these challenges, we conducted a systematic evaluation of large\nlanguage models (LLMs) for resolving Solidity compilation errors during version\nmigrations. Our empirical analysis across both open-source (LLaMA3, DeepSeek)\nand closed-source (GPT-4o, GPT-3.5-turbo) LLMs reveals that although these\nmodels exhibit error repair capabilities, their effectiveness diminishes\nsignificantly for semantic-level issues and shows strong dependency on prompt\nengineering strategies. This underscores the critical need for domain-specific\nadaptation in developing reliable LLM-based repair systems for smart contracts.\n  Building upon these insights, we introduce SMCFIXER, a novel framework that\nsystematically integrates expert knowledge retrieval with LLM-based repair\nmechanisms for Solidity compilation error resolution. The architecture\ncomprises three core phases: (1) context-aware code slicing that extracts\nrelevant error information; (2) expert knowledge retrieval from official\ndocumentation; and (3) iterative patch generation for Solidity migration.\nExperimental validation across Solidity version migrations demonstrates our\napproach's statistically significant 24.24% improvement over baseline GPT-4o on\nreal-world datasets, achieving near-perfect 96.97% accuracy.", "AI": {"tldr": "Frequent Solidity updates cause serious migration and compilation issues for smart contracts. While LLMs help, their performance is limited. The new SMCFIXER framework, combining expert guidance and LLMs, markedly improves error resolution and achieves nearly perfect accuracy.", "motivation": "The motivation is to address the challenges caused by rapid evolution and frequent updates in the Solidity language, leading to significant issues with compilation errors, code migration, and maintenance for Ethereum smart contracts.", "method": "The method involves an empirical study quantifying the prevalence of compilation errors due to Solidity version changes and evaluating the capability of large language models (LLMs) to resolve these errors. The paper then proposes SMCFIXER, a framework that combines expert knowledge retrieval and LLMs in three phases: code slicing, expert knowledge retrieval, and iterative patch generation, to improve Solidity migration.", "result": "The study finds that over 81% of contracts encounter errors when compiled with different Solidity versions, and current LLMs struggle with semantic error repair and are sensitive to prompt engineering. The proposed SMCFIXER framework, however, delivers a 24.24% improvement over GPT-4o and achieves 96.97% accuracy in resolving migration-related compilation errors.", "conclusion": "Solidity's rapid evolution poses real obstacles for developers, but integrating expert knowledge with LLM-based repair significantly improves error resolution. SMCFIXER demonstrates near-perfect accuracy and outperforms state-of-the-art LLMs, highlighting the value of domain-specific approaches in smart contract maintenance and migration."}}
{"id": "2508.10852", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10852", "abs": "https://arxiv.org/abs/2508.10852", "authors": ["Souhaila Serbout", "Diana Carolina Mu\u00f1oz Hurtado", "Hassan Atwi", "Edoardo Riggio", "Cesare Pautasso"], "title": "EVOSCAT: Exploring Software Change Dynamics in Large-Scale Historical Datasets", "comment": "Submitted to VISSOFT 2025. For the hi-resolution version of the\n  paper, see https://design.inf.usi.ch/publications/2025/vissoft", "summary": "Long lived software projects encompass a large number of artifacts, which\nundergo many revisions throughout their history. Empirical software engineering\nresearchers studying software evolution gather and collect datasets with\nmillions of events, representing changes introduced to specific artifacts. In\nthis paper, we propose EvoScat, a tool that attempts addressing temporal\nscalability through the usage of interactive density scatterplot to provide a\nglobal overview of large historical datasets mined from open source\nrepositories in a single visualization. EvoScat intents to provide researchers\nwith a mean to produce scalable visualizations that can help them explore and\ncharacterize evolution datasets, as well as comparing the histories of\nindividual artifacts, both in terms of 1) observing how rapidly different\nartifacts age over multiple-year-long time spans 2) how often metrics\nassociated with each artifacts tend towards an improvement or worsening. The\npaper shows how the tool can be tailored to specific analysis needs (pace of\nchange comparison, clone detection, freshness assessment) thanks to its support\nfor flexible configuration of history scaling and alignment along the time\naxis, artifacts sorting and interactive color mapping, enabling the analysis of\nmillions of events obtained by mining the histories of tens of thousands of\nsoftware artifacts. We include in this paper a gallery showcasing datasets\ngathering specific artifacts (OpenAPI descriptions, GitHub workflow\ndefinitions) across multiple repositories, as well as diving into the history\nof specific popular open source projects.", "AI": {"tldr": "The paper introduces EvoScat, a tool that enables interactive and scalable visualizations of large software evolution datasets by using density scatterplots. It allows researchers to analyze and compare the histories of many artifacts efficiently and flexibly, supporting advanced empirical analysis of how open source software evolves over time.", "motivation": "Empirical software engineering researchers face challenges in visualizing and analyzing massive historical datasets resulting from the evolution of long-lived software projects. Traditional visualization tools often cannot scale to the large number of artifacts and revisions present in these datasets.", "method": "The paper proposes EvoScat, a tool that uses interactive density scatterplots to visualize large historical datasets from open source software repositories. EvoScat provides options for scaling and aligning artifact histories along the time axis, sorting artifacts, and applying interactive color mapping. The tool is highly configurable to suit different analysis needs such as pace of change, clone detection, and dataset freshness.", "result": "EvoScat enables scalable visualization and analysis of millions of historical change events from tens of thousands of software artifacts. The paper demonstrates its application through a gallery of datasets, including specific artifact types (OpenAPI descriptions, GitHub workflow definitions) and in-depth explorations of histories from well-known open source projects.", "conclusion": "EvoScat addresses the temporal scalability challenge in empirical software engineering by empowering researchers with interactive, customizable visualizations. This aids in understanding and comparing the evolution of software artifacts across large datasets, supporting more effective empirical studies."}}
