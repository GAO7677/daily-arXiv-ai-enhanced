<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 22]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Arguzz: Testing zkVMs for Soundness and Completeness Bugs](https://arxiv.org/abs/2509.10819)
*Christoph Hochrainer,Valentin Wüstholz,Maria Christakis*

Main category: cs.SE

TL;DR: Arguzz is a new automated tool combining metamorphic testing and fault injection to systematically detect critical bugs in zero-knowledge virtual machines. It found 11 bugs—including in previously audited systems—highlighting the need for more rigorous zkVM testing.


<details>
  <summary>Details</summary>
Motivation: Zero-knowledge virtual machines (zkVMs) enable secure and verifiable off-chain computation in decentralized applications and blockchains. However, their complexity introduces risks of soundness and completeness bugs, leading to potentially critical security vulnerabilities. Existing auditing practices may not fully uncover these issues.

Method: The paper presents Arguzz, an automated tool that tests zkVMs for soundness and completeness bugs. Arguzz innovatively combines metamorphic testing—by generating semantically equivalent program pairs—and fault injection, merging them into a single Rust program with known output and running it inside zkVMs. Faults are injected to mimic the actions of malicious or buggy provers, aiming to identify weak constraints.

Result: Arguzz was applied to six popular real-world zkVMs: RISC Zero, Nexus, Jolt, SP1, OpenVM, and Pico. It discovered eleven bugs in three zkVMs, including one in RISC Zero that resulted in a $50,000 bounty despite prior auditing, showing significant undetected vulnerabilities.

Conclusion: Arguzz provides a much-needed, systematic and automated approach for uncovering hidden bugs—particularly soundness and completeness issues—in zkVMs, demonstrating the importance of advanced testing methodologies beyond traditional audits.

Abstract: Zero-knowledge virtual machines (zkVMs) are increasingly deployed in
decentralized applications and blockchain rollups since they enable verifiable
off-chain computation. These VMs execute general-purpose programs, frequently
written in Rust, and produce succinct cryptographic proofs. However, zkVMs are
complex, and bugs in their constraint systems or execution logic can cause
critical soundness (accepting invalid executions) or completeness (rejecting
valid ones) issues.
  We present Arguzz, the first automated tool for testing zkVMs for soundness
and completeness bugs. To detect such bugs, Arguzz combines a novel variant of
metamorphic testing with fault injection. In particular, it generates
semantically equivalent program pairs, merges them into a single Rust program
with a known output, and runs it inside a zkVM. By injecting faults into the
VM, Arguzz mimics malicious or buggy provers to uncover overly weak
constraints.
  We used Arguzz to test six real-world zkVMs (RISC Zero, Nexus, Jolt, SP1,
OpenVM, and Pico) and found eleven bugs in three of them. One RISC Zero bug
resulted in a $50,000 bounty, despite prior audits, demonstrating the critical
need for systematic testing of zkVMs.

</details>


### [2] [ViScratch: Using Large Language Models and Gameplay Videos for Automated Feedback in Scratch](https://arxiv.org/abs/2509.11065)
*Yuan Si,Daming Li,Hanyuan Shi,Jialu Zhang*

Main category: cs.SE

TL;DR: ViScratch, a new debugging tool, uses both code and gameplay video to fix bugs in Scratch projects, outperforming existing tools and human testers, and highlighting the value of video in programming education debugging.


<details>
  <summary>Details</summary>
Motivation: Debugging semantic bugs in block-based programming environments like Scratch is challenging and current debugging tools rely heavily on manual input or static rules, while ignoring the visual aspects intrinsic to Scratch.

Method: ViScratch is introduced as a multimodal system that analyzes both the code and gameplay video of Scratch projects. It uses a vision-language model to align visual symptoms with code structure, identify the main bug, and then proposes minimal repairs verified by re-executing the repaired project.

Result: ViScratch was tested on real Scratch projects and outperformed state-of-the-art LLM-based tools and human testers in both bug detection and repair quality, demonstrating that gameplay video is a critical debugging signal.

Conclusion: Integrating video as a specification enhances bug diagnosis and repair in visual programming environments. ViScratch's success suggests new possibilities for multimodal, LLM-based debugging approaches.

Abstract: Block-based programming environments such as Scratch are increasingly popular
in programming education, in particular for young learners. While the use of
blocks helps prevent syntax errors, semantic bugs remain common and difficult
to debug. Existing tools for Scratch debugging rely heavily on predefined rules
or user manual inputs, and crucially, they ignore the platform's inherently
visual nature.
  We introduce ViScratch, the first multimodal feedback generation system for
Scratch that leverages both the project's block code and its generated gameplay
video to diagnose and repair bugs. ViScratch uses a two-stage pipeline: a
vision-language model first aligns visual symptoms with code structure to
identify a single critical issue, then proposes minimal, abstract syntax tree
level repairs that are verified via execution in the Scratch virtual machine.
  We evaluate ViScratch on a set of real-world Scratch projects against
state-of-the-art LLM-based tools and human testers. Results show that gameplay
video is a crucial debugging signal: ViScratch substantially outperforms prior
tools in both bug identification and repair quality, even without access to
project descriptions or goals. This work demonstrates that video can serve as a
first-class specification in visual programming environments, opening new
directions for LLM-based debugging beyond symbolic code alone.

</details>


### [3] [Quality Assessment of Tabular Data using Large Language Models and Code Generation](https://arxiv.org/abs/2509.10572)
*Ashlesha Akella,Akshar Kaul,Krishnasuri Narayanam,Sameep Mehta*

Main category: cs.SE

TL;DR: The paper introduces a novel framework leveraging clustering, LLMs, and RAG to automate and improve data quality validation for tabular datasets, achieving strong results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Traditional rule-based data validation methods for tabular datasets are inefficient, require significant human effort, and are computationally expensive, motivating the need for a more automated and scalable solution.

Method: The paper proposes a three-stage framework: (1) statistical outlier detection via clustering, (2) iterative prompting of large language models (LLMs) to generate semantically valid data quality rules, and (3) automatic generation of code validators using code-generating LLMs. Retrieval-augmented generation (RAG) is used to enhance LLM performance with external knowledge and few-shot examples, while robust guardrails maintain accuracy.

Result: Empirical evaluations on benchmark datasets demonstrate the framework's effectiveness, showing improved data quality validation with reliable rule and code generation.

Conclusion: Combining statistical methods with LLM-based rule and code creation, complemented by retrieval-augmented generation and guardrails, offers a powerful and reliable solution for automated data quality validation in tabular datasets.

Abstract: Reliable data quality is crucial for downstream analysis of tabular datasets,
yet rule-based validation often struggles with inefficiency, human
intervention, and high computational costs. We present a three-stage framework
that combines statistical inliner detection with LLM-driven rule and code
generation. After filtering data samples through traditional clustering, we
iteratively prompt LLMs to produce semantically valid quality rules and
synthesize their executable validators through code-generating LLMs. To
generate reliable quality rules, we aid LLMs with retrieval-augmented
generation (RAG) by leveraging external knowledge sources and domain-specific
few-shot examples. Robust guardrails ensure the accuracy and consistency of
both rules and code snippets. Extensive evaluations on benchmark datasets
confirm the effectiveness of our approach.

</details>


### [4] [Reasonable Experiments in Model-Based Systems Engineering](https://arxiv.org/abs/2509.10649)
*Johan Cederbladh,Loek Cleophas,Eduard Kamburjan,Lucas Lima,Rakshit Mittal,Hans Vangheluwe*

Main category: cs.SE

TL;DR: This paper introduces a case-based reasoning framework for managing and reusing experimental data in digital and physical systems engineering. By intelligently deciding when existing data applies to new questions, the framework streamlines design and reduces the need for repeated experiments, as shown in an industrial vehicular system case study.


<details>
  <summary>Details</summary>
Motivation: There is a growing need to efficiently manage and reuse experimental data in the context of digital and physical systems engineering to save time, resources, and avoid redundant experiments.

Method: The authors propose a framework based on case-based reasoning that leverages domain knowledge to decide whether existing experimental results can answer new engineering queries, thereby avoiding the need for new experiments. They present the architecture of their experiment manager and validate it using an industrial vehicular energy system case study.

Result: The framework enables efficient reuse of experimental data, preventing unnecessary and resource-consuming experiments by determining the applicability of existing results to new questions.

Conclusion: Intelligent management and reuse of experiment metadata and results accelerate the overall design process in Model-Based Systems Engineering, validated through a real-world industrial case study.

Abstract: With the current trend in Model-Based Systems Engineering towards Digital
Engineering and early Validation & Verification, experiments are increasingly
used to estimate system parameters and explore design decisions. Managing such
experimental configuration metadata and results is of utmost importance in
accelerating overall design effort. In particular, we observe it is important
to 'intelligent-ly' reuse experiment-related data to save time and effort by
not performing potentially superfluous, time-consuming, and resource-intensive
experiments. In this work, we present a framework for managing experiments on
digital and/or physical assets with a focus on case-based reasoning with domain
knowledge to reuse experimental data efficiently by deciding whether an
already-performed experiment (or associated answer) can be reused to answer a
new (potentially different) question from the engineer/user without having to
set up and perform a new experiment. We provide the general architecture for
such an experiment manager and validate our approach using an industrial
vehicular energy system-design case study.

</details>


### [5] [TPSQLi: Test Prioritization for SQL Injection Vulnerability Detection in Web Applications](https://arxiv.org/abs/2509.10920)
*Guan-Yan Yang,Farn Wang,You-Zong Gu,Ya-Wen Teng,Kuo-Hui Yeh,Ping-Hsueh Ho,Wei-Ling Wen*

Main category: cs.SE

TL;DR: The paper presents a new test prioritization method for SQL injection vulnerabilities. By adjusting defense strategies based on test results, the approach boosts efficiency and effectiveness in software security testing.


<details>
  <summary>Details</summary>
Motivation: The rise in network attacks, especially injection attacks (highlighted as a top vulnerability in the OWASP Top 10 2021), has complicated software testing and increased the need for advanced testing tools.

Method: Proposes a novel test prioritization method for detecting SQL injection vulnerabilities. It adapts defense strength vectors based on previous test outcomes, creating a dynamic and flexible framework.

Result: The method optimizes testing workflows and customizes defense mechanisms according to specific software requirements, enhancing both the efficiency and effectiveness of vulnerability detection and mitigation.

Conclusion: The proposed approach successfully improves the efficiency and effectiveness of SQL injection vulnerability testing, with dynamic adjustments and consideration of temporal factors.

Abstract: The rapid proliferation of network applications has led to a significant
increase in network attacks. According to the OWASP Top 10 Projects report
released in 2021, injection attacks rank among the top three vulnerabilities in
software projects. This growing threat landscape has increased the complexity
and workload of software testing, necessitating advanced tools to support agile
development cycles. This paper introduces a novel test prioritization method
for SQL injection vulnerabilities to enhance testing efficiency. By leveraging
previous test outcomes, our method adjusts defense strength vectors for
subsequent tests, optimizing the testing workflow and tailoring defense
mechanisms to specific software needs. This approach aims to improve the
effectiveness and efficiency of vulnerability detection and mitigation through
a flexible framework that incorporates dynamic adjustments and considers the
temporal aspects of vulnerability exposure.

</details>


### [6] [When the Code Autopilot Breaks: Why LLMs Falter in Embedded Machine Learning](https://arxiv.org/abs/2509.10946)
*Roberto Morabito,Guanghan Wu*

Main category: cs.SE

TL;DR: The paper investigates why LLMs often fail or behave unpredictably in automating embedded ML workflows, finds multiple subtle failure modes, and suggests the need for better reliability and traceability in such systems.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are widely used to automate software generation in embedded machine learning workflows, but their outputs often result in silent failures or unpredictable behavior, presenting reliability challenges.

Method: The study conducts an empirical investigation using an autopilot framework that manages data preprocessing, model conversion, and code generation for on-device inference. It analyzes how factors like prompt format, model behavior, and structural assumptions affect the success and failure of LLM outputs. Errors are taxonomized and analyzed across multiple LLMs to identify root causes and systemic fragilities.

Result: The analysis uncovers a variety of error-prone behaviors (such as format-induced misinterpretations and code that compiles but fails at runtime), outlines failure categories, and discusses common root causes and systemic weaknesses in current LLM-powered approaches.

Conclusion: There are significant challenges and systemic weaknesses in current LLM-driven code generation for embedded ML due to subtle and undetected failure modes. Improving reliability and traceability requires new approaches and awareness of these failure characteristics.

Abstract: Large Language Models (LLMs) are increasingly used to automate software
generation in embedded machine learning workflows, yet their outputs often fail
silently or behave unpredictably. This article presents an empirical
investigation of failure modes in LLM-powered ML pipelines, based on an
autopilot framework that orchestrates data preprocessing, model conversion, and
on-device inference code generation. We show how prompt format, model behavior,
and structural assumptions influence both success rates and failure
characteristics, often in ways that standard validation pipelines fail to
detect. Our analysis reveals a diverse set of error-prone behaviors, including
format-induced misinterpretations and runtime-disruptive code that compiles but
breaks downstream. We derive a taxonomy of failure categories and analyze
errors across multiple LLMs, highlighting common root causes and systemic
fragilities. Though grounded in specific devices, our study reveals broader
challenges in LLM-based code generation. We conclude by discussing directions
for improving reliability and traceability in LLM-powered embedded ML systems.

</details>


### [7] [Hardness, Structural Knowledge, and Opportunity: An Analytical Framework for Modular Performance Modeling](https://arxiv.org/abs/2509.11000)
*Omid Gheibi,Christian Kästner,Pooyan Jamshidi*

Main category: cs.SE

TL;DR: Building effective performance-influence models is tough, but using more structural system knowledge and understanding the inherent challenges (modeling hardness) can yield substantial improvement. The value of these factors depends on the modeling task—knowing the system's structure is more helpful for ranking tasks, while acknowledging modeling difficulty matters more for prediction tasks. The findings guide designers in making informed modeling choices based on system characteristics and goals.


<details>
  <summary>Details</summary>
Motivation: Performance-influence models help understand how system configurations impact performance, but building such models is hard due to exponentially large configuration spaces. While some approaches use structural knowledge of the system to help with modeling, the precise relationship between this knowledge, specific system characteristics, and model improvement is not well understood.

Method: The paper conducts a formal investigation using controlled experiments with synthetic system models. It quantifies structural aspects (like number of modules and options per module), introduces 'modeling hardness' as a concept, and constructs an analytical matrix to measure the effects of structural knowledge and system characteristics on modeling opportunities.

Result: The experiments show that modeling hardness is mainly influenced by the number of modules and configuration options per module. Greater structural knowledge and increased modeling hardness both boost opportunities for improved modular performance modeling. Their relative importance differs: structural knowledge is more crucial for ranking accuracy tasks, while modeling hardness is more important for prediction accuracy tasks.

Conclusion: The study provides practical guidance for system designers. Depending on the system’s structural aspects and the objective of the modeling task (ranking vs. prediction), designers can strategically choose where to spend their efforts and which modeling strategies to use.

Abstract: Performance-influence models are beneficial for understanding how
configurations affect system performance, but their creation is challenging due
to the exponential growth of configuration spaces. While gray-box approaches
leverage selective "structural knowledge" (like the module execution graph of
the system) to improve modeling, the relationship between this knowledge, a
system's characteristics (we call them "structural aspects"), and potential
model improvements is not well understood. This paper addresses this gap by
formally investigating how variations in structural aspects (e.g., the number
of modules and options per module) and the level of structural knowledge impact
the creation of "opportunities" for improved "modular performance modeling". We
introduce and quantify the concept of modeling "hardness", defined as the
inherent difficulty of performance modeling. Through controlled experiments
with synthetic system models, we establish an "analytical matrix" to measure
these concepts. Our findings show that modeling hardness is primarily driven by
the number of modules and configuration options per module. More importantly,
we demonstrate that both higher levels of structural knowledge and increased
modeling hardness significantly enhance the opportunity for improvement. The
impact of these factors varies by performance metric; for ranking accuracy
(e.g., in debugging task), structural knowledge is more dominant, while for
prediction accuracy (e.g., in resource management task), hardness plays a
stronger role. These results provide actionable insights for system designers,
guiding them to strategically allocate time and select appropriate modeling
approaches based on a system's characteristics and a given task's objectives.

</details>


### [8] [Rethinking Technology Stack Selection with AI Coding Proficiency](https://arxiv.org/abs/2509.11132)
*Xiaoyu Zhang,Weipeng Jiang,Juan Zhai,Shiqing Ma,Qingshuang Bao,Chenhao Lin,Chao Shen,Tianlin Li,Yang Liu*

Main category: cs.SE

TL;DR: Traditional ways of choosing tech for software development don't consider how well LLMs work with those libraries, leading to bad code and extra work. The paper shows big gaps in LLM performance depending on the library, and calls for new selection methods that include 'AI coding proficiency' to avoid narrowing tech choices and losing ecosystem diversity.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the increasing role of LLMs in software development. Traditional technology stack selection does not account for how well LLMs can use a given technology, leading to risks of low-quality code and increased technical debt.

Method: They introduce the concept of 'AI coding proficiency' and conduct an empirical study across 170 third-party libraries and 61 scenarios, evaluating six popular LLMs for their code generation capabilities.

Result: Libraries with similar functions may show up to an 84% difference in LLM-generated code quality. Different LLMs perform unevenly with the same technology, influencing developer choices and potentially threatening technology diversity.

Conclusion: AI coding proficiency should be integrated into technology selection frameworks. The community should develop strategies to preserve diversity and maintain competitive balance in an AI-driven development landscape.

Abstract: Large language models (LLMs) are now an integral part of software development
workflows and are reshaping the whole process. Traditional technology stack
selection has not caught up. Most of the existing selection methods focus
solely on the inherent attributes of the technology, overlooking whether the
LLM can effectively leverage the chosen technology. For example, when
generating code snippets using popular libraries like Selenium (one of the most
widely used test automation tools with over 33k GitHub stars), existing LLMs
frequently generate low-quality code snippets (e.g., using deprecated APIs and
methods, or containing syntax errors). As such, teams using LLM assistants risk
choosing technologies that cannot be used effectively by LLMs, yielding high
debugging effort and mounting technical debt. We foresee a practical question
in the LLM era, is a technology ready for AI-assisted development? In this
paper, we first propose the concept, AI coding proficiency, the degree to which
LLMs can utilize a given technology to generate high-quality code snippets. We
conduct the first comprehensive empirical study examining AI proficiency across
170 third-party libraries and 61 task scenarios, evaluating six widely used
LLMs. Our findings reveal that libraries with similar functionalities can
exhibit up to 84% differences in the quality score of LLM-generated code, while
different models also exhibit quality gaps among their generation results using
the same library. These gaps translate into real engineering costs and can
steer developer choices toward a narrow set of libraries with high AI coding
proficiency, threatening technological diversity in the ecosystem. We call on
the community to integrate AI proficiency assessments into technology selection
frameworks and develop mitigation strategies, preserving competitive balance in
AI-driven development.

</details>


### [9] [UserTrace: User-Level Requirements Generation and Traceability Recovery from Software Project Repositories](https://arxiv.org/abs/2509.11238)
*Dongming Jin,Zhi Jin,Yiran Zhang,Zheng Fang,Linyu Li,Yuanpeng He,Xiaohong Chen,Weisong Sun*

Main category: cs.SE

TL;DR: UserTrace is a multi-agent system that automatically generates user-level requirements and maintains live traceability from requirements to code. It outperforms existing tools in completeness and precision and helps users verify that AI-generated software matches their intended needs.


<details>
  <summary>Details</summary>
Motivation: Current automated code summarization and requirements traceability methods focus mostly on developer-oriented requirements and overlook user-level requirements and traceability as projects evolve. This hampers software maintainability and validation regarding user intent.

Method: The authors propose UserTrace, a multi-agent system consisting of four specialized agents (Code Reviewer, Searcher, Writer, Verifier) that operate in three phases: structuring repository dependencies, deriving implementation-level requirements, and synthesizing user-level requirements with domain-specific context.

Result: Comparative evaluation reveals that UserTrace generates user-level requirements with better completeness, correctness, and helpfulness than existing baseline methods, and outperforms five state-of-the-art traceability techniques in precision for trace link recovery. A user study further validates its effectiveness in helping end users ensure alignment with their intent.

Conclusion: UserTrace addresses the gap in user-level requirements and live traceability, supporting better validation of AI-generated software against user intent and improving overall software maintainability.

Abstract: Software maintainability critically depends on high-quality requirements
descriptions and explicit traceability between requirements and code. Although
automated code summarization (ACS) and requirements traceability (RT)
techniques have been widely studied, existing ACS methods mainly generate
implementation-level (i.e., developer-oriented) requirements (IRs) for
fine-grained units (e.g., methods), while RT techniques often overlook the
impact of project evolution. As a result, user-level (i.e., end user-oriented)
requirements (URs) and live trace links remain underexplored, despite their
importance for supporting user understanding and for validating whether
AI-generated software aligns with user intent. To address this gap, we propose
UserTrace, a multi-agent system that automatically generates URs and recovers
live trace links (from URs to IRs to code) from software repositories.
UserTrace coordinates four specialized agents (i.e., Code Reviewer, Searcher,
Writer, and Verifier) through a three-phase process: structuring repository
dependencies, deriving IRs for code units, and synthesizing URs with
domain-specific context. Our comparative evaluation shows that UserTrace
produces URs with higher completeness, correctness, and helpfulness than an
established baseline, and achieves superior precision in trace link recovery
compared to five state-of-the-art RT approaches. A user study further
demonstrates that UserTrace helps end users validate whether the AI-generated
repositories align with their intent.

</details>


### [10] [Beyond Autoregression: An Empirical Study of Diffusion Large Language Models for Code Generation](https://arxiv.org/abs/2509.11252)
*Chengze li,Yitong Zhang,Jia Li,Liyi Cai,Ge Li*

Main category: cs.SE

TL;DR: This paper provides the first comprehensive study of diffusion LLMs for code generation, finding them competitive and sometimes superior to traditional autoregressive LLMs, particularly for long code. Results and open-source resources can inform practical use and future research.


<details>
  <summary>Details</summary>
Motivation: Autoregressive LLMs are currently dominant for code generation but suffer from inefficiency (one token per step) and do not align well with the non-sequential nature of real-world programming. These limitations hinder advancements in code generation.

Method: The authors conduct the first empirical study of diffusion LLMs for code generation by evaluating 9 representative diffusion LLMs over 4 widely used benchmarks. They compare these models to autoregressive LLMs and analyze factors that impact their effectiveness and efficiency.

Result: The study finds that diffusion LLMs are competitive with similarly sized autoregressive LLMs, show stronger length extrapolation abilities, perform better on long code tasks, and identifies factors affecting their effectiveness and efficiency. The authors also propose future research directions and provide practical guidance.

Conclusion: Diffusion LLMs address key limitations of autoregressive LLMs in code generation by generating multiple tokens per step and allowing for flexible order, offering competitive and sometimes superior performance, especially in handling longer code. The findings provide insights and practical guidance for further development.

Abstract: LLMs have become the mainstream approaches to code generation. Existing LLMs
mainly employ autoregressive generation, i.e. generating code token-by-token
from left to right. However, the underlying autoregressive generation has two
limitations in code generation. First, autoregressive LLMs only generate a
token at each step, showing low efficiency in practice. Second, programming is
a non-sequential process involving back-and-forth editing, while autoregressive
LLMs only employ the left-to-right generation order. These two intrinsic
limitations hinder the further development of LLMs in code generation.
Recently, diffusion LLMs have emerged as a promising alternative. Diffusion
LLMs address the above limitations with two advances, including multi-token
prediction (i.e. generating multiple tokens at each step) and flexible
generation order (i.e. flexibly determining which positions to generate
tokens). However, there is no systematic study exploring diffusion LLMs in code
generation. To bridge the knowledge gap, we present the first empirical study
of diffusion LLMs for code generation. Our study involves 9 representative
diffusion LLMs and conduct experiments on 4 widely used benchmarks. Based on
the results, we summarize the following findings. (1) Existing diffusion LLMs
are competitive with autoregressive LLMs with similar sizes. (2) Diffusion LLMs
have a stronger length extrapolation ability than autoregressive LLMs and
perform better in long code understanding. (3) We explore factors impacting the
effectiveness and efficiency of diffusion LLMs, and provide practical guidance.
(4) We discuss several promising further directions to improve diffusion LLMs
on code generation. We open-source all source code, data, and results to
facilitate the following research. The code is publicly available at
https://github.com/zhangyitonggg/dllm4code.

</details>


### [11] [A Web-Based Environment for the Specification and Generation of Smart Legal Contracts](https://arxiv.org/abs/2509.11258)
*Regan Meloche,Durga Sivakumar,Amal A. Anda,Sofana Alfuhaid,Daniel Amyot,Luigi Logrippo,John Mylopoulos*

Main category: cs.SE

TL;DR: The paper introduces a web-based tool to bridge the gap between legal contracts and smart contracts, allowing user-assisted refinement of contract specifications and automatic smart contract generation for Hyperledger Fabric. This accelerates and simplifies the creation of compliance-monitoring smart contracts.


<details>
  <summary>Details</summary>
Motivation: There is a significant gap between natural language contracts (used in legal settings) and the implementation of those contracts as smart contracts, which can automate compliance monitoring and violation detection.

Method: The paper presents a web-based environment that assists users in refining Symboleo specifications derived from legal contract templates. It then automatically generates monitoring smart contracts that can be deployed on Hyperledger Fabric.

Result: The proposed environment, demonstrated through a sample contract in the transactive energy domain, shows promise in accelerating smart contract development for legal compliance purposes.

Conclusion: This environment enables more efficient and reliable translation of legal contract templates into deployable smart contracts, reducing the manual effort and potential for error in creating automated compliance monitoring solutions.

Abstract: Monitoring the compliance of contract performance against legal obligations
is important in order to detect violations, ideally, as soon as they occur.
Such monitoring can nowadays be achieved through the use of smart contracts,
which provide protection against tampering as well as some level of automation
in handling violations. However, there exists a large gap between natural
language contracts and smart contract implementations. This paper introduces a
Web-based environment that partly fills that gap by supporting the
user-assisted refinement of Symboleo specifications corresponding to legal
contract templates, followed by the automated generation of monitoring smart
contracts deployable on the Hyperledger Fabric platform. This environment,
illustrated using a sample contract from the transactive energy domain, shows
much potential in accelerating the development of smart contracts in a legal
compliance context.

</details>


### [12] [Weakly Supervised Vulnerability Localization via Multiple Instance Learning](https://arxiv.org/abs/2509.11312)
*Wenchao Gu,Yupan Chen,Yanlin Wang,Hongyu Zhang,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: WAVES is a weakly supervised method for detecting and localizing software vulnerabilities at the statement level without expensive manual labels, matching existing detection accuracy and outperforming prior methods on localization tasks.


<details>
  <summary>Details</summary>
Motivation: Current software vulnerability detection methods mostly work at coarse granularity (function or file level), requiring manual inspection by developers to locate precise vulnerable statements within code, which is costly and demands expert labeling at statement-level.

Method: The paper introduces WAVES, a weakly supervised vulnerability localization technique using multiple instance learning. WAVES trains on function-level labels by generating pseudo-labels for statements, removing the need for costly statement-level expert labeling.

Result: WAVES performs comparably to existing methods in function-level vulnerability detection and achieves state-of-the-art results in localizing vulnerable code statements, validated on three benchmark datasets.

Conclusion: WAVES significantly reduces labeling costs for vulnerability localization without sacrificing detection accuracy and advances the field to finer-grained, automated vulnerability identification.

Abstract: Software vulnerability detection has emerged as a significant concern in the
field of software security recently, capturing the attention of numerous
researchers and developers. Most previous approaches focus on coarse-grained
vulnerability detection, such as at the function or file level. However, the
developers would still encounter the challenge of manually inspecting a large
volume of code inside the vulnerable function to identify the specific
vulnerable statements for modification, indicating the importance of
vulnerability localization. Training the model for vulnerability localization
usually requires ground-truth labels at the statement-level, and labeling
vulnerable statements demands expert knowledge, which incurs high costs. Hence,
the demand for an approach that eliminates the need for additional labeling at
the statement-level is on the rise. To tackle this problem, we propose a novel
approach called WAVES for WeAkly supervised Vulnerability Localization via
multiplE inStance learning, which does not need the additional statement-level
labels during the training. WAVES has the capability to determine whether a
function is vulnerable (i.e., vulnerability detection) and pinpoint the
vulnerable statements (i.e., vulnerability localization). Specifically,
inspired by the concept of multiple instance learning, WAVES converts the
ground-truth label at the function-level into pseudo labels for individual
statements, eliminating the need for additional statement-level labeling. These
pseudo labels are utilized to train the classifiers for the function-level
representation vectors. Extensive experimentation on three popular benchmark
datasets demonstrates that, in comparison to previous baselines, our approach
achieves comparable performance in vulnerability detection and state-of-the-art
performance in statement-level vulnerability localization.

</details>


### [13] [Large Language Models (LLMs) for Requirements Engineering (RE): A Systematic Literature Review](https://arxiv.org/abs/2509.11446)
*Mohammad Amin Zadenoori,Jacek Dąbrowski,Waad Alhoshan,Liping Zhao,Alessio Ferrari*

Main category: cs.SE

TL;DR: This paper systematically reviews 74 recent studies on LLMs in Requirements Engineering, revealing shifts in focus and method, highlighting new tasks and integration opportunities, but also underscoring the lack of real-world deployment. It provides practical resources and suggests future research directions.


<details>
  <summary>Details</summary>
Motivation: As LLMs revolutionize various fields, there is a growing need to understand their applications, benefits, and research directions in Requirements Engineering, which involves complex language tasks.

Method: A systematic literature review of 74 primary studies (2023–2024), analyzing trends, activities, prompting strategies, and evaluation methods related to the application of LLMs in Requirements Engineering.

Result: LLMs are predominantly used for requirements elicitation and validation, with a shift from traditional tasks like defect detection. Research focuses on test generation and integrating RE with other SE disciplines. Most studies use GPT models and zero-/few-shot prompting, with limited real-world or industry application. The paper provides lists of tools and datasets for LLM-based RE.

Conclusion: The paper concludes that LLMs are making a significant impact on Requirements Engineering, with substantial differences from prior NLP-based approaches. However, most uses are experimental, limited to controlled settings, and call for further research, especially for real-world adoption and broader SE integration.

Abstract: Large Language Models (LLMs) are finding applications in numerous domains,
and Requirements Engineering (RE) is increasingly benefiting from their
capabilities to assist with complex, language-intensive tasks. This paper
presents a systematic literature review of 74 primary studies published between
2023 and 2024, examining how LLMs are being applied in RE. The study
categorizes the literature according to several dimensions, including
publication trends, RE activities, prompting strategies, and evaluation
methods. Our findings indicate notable patterns, among which we observe
substantial differences compared to previous works leveraging standard Natural
Language Processing (NLP) techniques. Most of the studies focus on using LLMs
for requirements elicitation and validation, rather than defect detection and
classification, which were dominant in the past. Researchers have also
broadened their focus and addressed novel tasks, e.g., test generation,
exploring the integration of RE with other software engineering (SE)
disciplines. Although requirements specifications remain the primary focus,
other artifacts are increasingly considered, including issues from issue
tracking systems, regulations, and technical manuals. The studies mostly rely
on GPT-based models, and often use Zero-shot or Few-shot prompting. They are
usually evaluated in controlled environments, with limited use in industry
settings and limited integration in complex workflows. Our study outlines
important future directions, such as leveraging the potential to expand the
influence of RE in SE, exploring less-studied tasks, improving prompting
methods, and testing in real-world environments. Our contribution also helps
researchers and practitioners use LLMs more effectively in RE, by providing a
list of identified tools leveraging LLMs for RE, as well as datasets.

</details>


### [14] [VulAgent: Hypothesis-Validation based Multi-Agent Vulnerability Detection](https://arxiv.org/abs/2509.11523)
*Ziliang Wang,Ge Li,Jia Li,Hao Zhu,Zhi Jin*

Main category: cs.SE

TL;DR: VulAgent, a multi-agent, hypothesis-based framework, significantly improves project-level vulnerability detection by guiding LLMs to validate suspected vulnerabilities more thoroughly. It outperforms previous methods with higher accuracy, better identification of fixed vulnerabilities, and reduced false positives.


<details>
  <summary>Details</summary>
Motivation: Language models face challenges in project-level vulnerability detection because they must both localize security-sensitive code accurately and analyze complex code contexts. Existing methods struggle to correlate these elements effectively, often leading to high false positive rates and missed vulnerabilities.

Method: The authors propose VulAgent, a multi-agent framework inspired by human code auditing. It consists of specialized agents focused on different security perspectives (e.g., memory, authorization). VulAgent uses a hypothesis-validation process wherein it formulates vulnerability hypotheses, identifies trigger paths, and directs the language model to verify these against relevant context and defensive checks. This targeted validation reduces errors and improves precision.

Result: VulAgent demonstrates improved performance on two datasets: it increases overall accuracy by 6.6%, boosts correct recognition of vulnerable-fixed code pairs by up to 450% (246% on average), and lowers the false positive rate by about 36% compared to state-of-the-art LLM-based methods.

Conclusion: VulAgent’s multi-agent, hypothesis-driven approach provides more precise and context-sensitive project-level vulnerability detection with substantially better accuracy and fewer false positives than previous LLM-based systems.

Abstract: The application of language models to project-level vulnerability detection
remains challenging, owing to the dual requirement of accurately localizing
security-sensitive code and correctly correlating and reasoning over complex
program context. We present VulAgent, a multi-agent vulnerability detection
framework based on hypothesis validation. Our design is inspired by how human
auditors review code: when noticing a sensitive operation, they form a
hypothesis about a possible vulnerability, consider potential trigger paths,
and then verify the hypothesis against the surrounding context. VulAgent
implements a semantics-sensitive, multi-view detection pipeline: specialized
agents, each aligned to a specific analysis perspective (e.g., memory,
authorization), collaboratively surface and precisely localize sensitive code
sites with higher coverage. Building on this, VulAgent adopts a
hypothesis-validation paradigm: for each vulnerability report, it builds
hypothesis conditions and a trigger path, steering the LLM to target the
relevant program context and defensive checks during verification, which
reduces false positives. On average across the two datasets, VulAgent improves
overall accuracy by 6.6%, increases the correct identification rate of
vulnerable--fixed code pairs by up to 450% (246% on average), and reduces the
false positive rate by about 36% compared with state-of-the-art LLM-based
baselines.

</details>


### [15] [Sedeve-Kit, a Specification-Driven Development Framework for Building Distributed Systems](https://arxiv.org/abs/2509.11566)
*Hua Guo,Yunhong Ji,Xuan Zhou*

Main category: cs.SE

TL;DR: This paper introduces a three-stage specification-driven framework for developing distributed systems: formal specification and model checking with TLA+, code generation that's tightly guided by specifications, and comprehensive testing using automatically generated test cases, together ensuring high system quality and reliable implementation.


<details>
  <summary>Details</summary>
Motivation: Distributed systems are difficult to develop due to non-deterministic concurrency and faults, necessitating better approaches to manage complexity and ensure system correctness.

Method: The authors propose a specification-driven development framework with three stages: (1) defining specifications and invariants in TLA+, model checking the algorithm and generating test cases; (2) implementing the system according to these specifications; (3) testing the system using the generated test cases to verify correctness.

Result: The proposed methodology strengthens the link between design and implementation, enabling continuous verification and higher assurance of system quality for distributed systems.

Conclusion: The specification-driven framework ensures the correctness and reliability of distributed systems by combining formal specification, model checking, and rigorous testing.

Abstract: Developing distributed systems presents significant challenges, primarily due
to the complexity introduced by non-deterministic concurrency and faults. To
address these, we propose a specification-driven development framework. Our
method encompasses three key stages. The first stage defines system
specifications and invariants using TLA${^+}$. It allows us to perform model
checking on the algorithm's correctness and generate test cases for subsequent
development phases. In the second stage, based on the established
specifications, we write code to ensure consistency and accuracy in the
implementation. Finally, after completing the coding process, we rigorously
test the system using the test cases generated in the initial stage. This
process ensures system quality by maintaining a strong connection between the
abstract design and the concrete implementation through continuous
verification.

</details>


### [16] [Automated Creation and Enrichment Framework for Improved Invocation of Enterprise APIs as Tools](https://arxiv.org/abs/2509.11626)
*Prerna Agarwal,Himanshu Gupta,Soujanya Soni,Rohith Vallam,Renuka Sindhgatta,Sameep Mehta*

Main category: cs.SE

TL;DR: The paper introduces ACE, a framework that makes enterprise APIs easier for LLM agents to use by automating enrichment of tool specs and dynamic shortlisting, addressing key pain points in accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are increasingly powerful in enterprise settings but struggle to use external APIs effectively due to poor documentation, complex schemas, and overwhelming numbers of operations. This hinders tool selection and reduces accuracy.

Method: The paper proposes ACE, an automated framework that transforms complex enterprise APIs into LLM-friendly tools. ACE enriches API specifications with parameter descriptions and examples and introduces a dynamic tool shortlisting mechanism to filter relevant tools during runtime.

Result: ACE was validated on both proprietary and open-source APIs and successfully integrated with agentic frameworks. The framework improves tool selection and invocation accuracy and reduces prompt complexity while maintaining scalability.

Conclusion: ACE is an end-to-end automated solution for creating, enriching, and dynamically selecting enterprise API tools, enabling LLM agents to interact more effectively and efficiently with complex APIs.

Abstract: Recent advancements in Large Language Models (LLMs) has lead to the
development of agents capable of complex reasoning and interaction with
external tools. In enterprise contexts, the effective use of such tools that
are often enabled by application programming interfaces (APIs), is hindered by
poor documentation, complex input or output schema, and large number of
operations. These challenges make tool selection difficult and reduce the
accuracy of payload formation by up to 25%. We propose ACE, an automated tool
creation and enrichment framework that transforms enterprise APIs into
LLM-compatible tools. ACE, (i) generates enriched tool specifications with
parameter descriptions and examples to improve selection and invocation
accuracy, and (ii) incorporates a dynamic shortlisting mechanism that filters
relevant tools at runtime, reducing prompt complexity while maintaining
scalability. We validate our framework on both proprietary and open-source APIs
and demonstrate its integration with agentic frameworks. To the best of our
knowledge, ACE is the first end-to-end framework that automates the creation,
enrichment, and dynamic selection of enterprise API tools for LLM agents.

</details>


### [17] [Do Code Semantics Help? A Comprehensive Study on Execution Trace-Based Information for Code Large Language Models](https://arxiv.org/abs/2509.11686)
*Jian Wang,Xiaofei Xie,Qiang Hu,Shangqing Liu,Yi Li*

Main category: cs.SE

TL;DR: This paper finds that including semantic information like execution traces into prompts does not meaningfully improve Code LLMs' reasoning during training or inference, challenging prior assumptions.


<details>
  <summary>Details</summary>
Motivation: Code LLMs have shown strong programming capabilities but struggle with understanding program runtime behavior and properly using semantic information like execution traces. There is a need to improve their reasoning abilities for more robust post-training and deployment.

Method: The authors introduce a general framework for integrating semantic information, such as execution traces, into code task prompts. They systematically study the effect of this semantic information on the reasoning capabilities of Code LLMs, focusing on supervised fine-tuning (SFT) and inference phases.

Result: Experimental results show that integrating semantic information is less beneficial than previously thought. Semantic information provides limited usefulness for supervised fine-tuning and test-time scaling of Code LLMs.

Conclusion: Contrary to some previous studies, adding semantic information such as execution traces does not significantly enhance the reasoning abilities of Code LLMs during SFT or inference.

Abstract: Code Large Language Models (Code LLMs) have opened a new era in programming
with their impressive capabilities. However, recent research has revealed
critical limitations in their ability to reason about runtime behavior and
understand the actual functionality of programs, which poses significant
challenges for their post-training and practical deployment. Specifically, Code
LLMs encounter two principal issues: (1) a lack of proficiency in reasoning
about program execution behavior, as they struggle to interpret what programs
actually do during runtime, and (2) the inconsistent and fragmented
representation of semantic information, such as execution traces, across
existing methods, which hinders their ability to generalize and reason
effectively. These challenges underscore the necessity for more systematic
approaches to enhance the reasoning capabilities of Code LLMs. To address these
issues, we introduce a generic framework to support integrating semantic
information~(e.g., execution trace) to code task-relevant prompts, and conduct
a comprehensive study to explore the role of semantic information in enhancing
the reasoning ability of Code LLMs accordingly. Specifically, we focus on
investigating the usefulness of trace-based semantic information in boosting
supervised fine-tuning~(SFT) and post-phase inference of Code LLMs. The
experimental results surprisingly disagree with previous works and demonstrate
that semantic information has limited usefulness for SFT and test time scaling
of Code LLM.

</details>


### [18] [AI Asset Management for Manufacturing (AIM4M): Development of a Process Model for Operationalization](https://arxiv.org/abs/2509.11691)
*Lukas Rauh,Mel-Rick Süner,Daniel Schel,Thomas Bauernhansl*

Main category: cs.SE

TL;DR: Organizations struggle to fully implement AI in manufacturing due to system complexity and lack of standards. This paper presents a refined process model based on MLOps principles, specifically designed for CPPS, enabling better lifecycle management and compliance for AI assets.


<details>
  <summary>Details</summary>
Motivation: AI adoption in manufacturing faces substantial obstacles, especially moving beyond prototypes due to technical complexity, lack of standards, and organizational fragmentation.

Method: The paper proposes a process model for AI lifecycle management, tailored to manufacturing and cyber-physical production system (CPPS) contexts. This model adapts and refines MLOps principles to meet domain-specific requirements.

Result: The proposed model enhances systematic development, deployment, and management of AI assets throughout their lifecycle, incorporating CPPS-specific constraints and regulatory needs.

Conclusion: The process model can help organizations effectively operationalize AI in manufacturing by addressing both technical and organizational challenges unique to CPPS environments.

Abstract: The benefits of adopting artificial intelligence (AI) in manufacturing are
undeniable. However, operationalizing AI beyond the prototype, especially when
involved with cyber-physical production systems (CPPS), remains a significant
challenge due to the technical system complexity, a lack of implementation
standards and fragmented organizational processes. To this end, this paper
proposes a new process model for the lifecycle management of AI assets designed
to address challenges in manufacturing and facilitate effective
operationalization throughout the entire AI lifecycle. The process model, as a
theoretical contribution, builds on machine learning operations (MLOps)
principles and refines three aspects to address the domain-specific
requirements from the CPPS context. As a result, the proposed process model
aims to support organizations in practice to systematically develop, deploy and
manage AI assets across their full lifecycle while aligning with CPPS-specific
constraints and regulatory demands.

</details>


### [19] [From Evaluation to Enhancement: Large Language Models for Zero-Knowledge Proof Code Generation](https://arxiv.org/abs/2509.11708)
*Zhantong Xue,Pingchuan Ma,Zhaoyu Wang,Shuai Wang*

Main category: cs.SE

TL;DR: This paper analyzes how well large language models (LLMs) generate zero-knowledge (ZK) programs, finding that while LLMs handle syntax well, they struggle with deeper ZK concepts. By introducing evaluation tools and an augmentation framework, the authors raise program generation success rates dramatically, making ZK development more accessible and robust.


<details>
  <summary>Details</summary>
Motivation: Authoring zero-knowledge (ZK) programs is complex and error-prone, requiring deep domain knowledge of finite field arithmetic, constraint systems, and specialized gadgets. Existing large language models show promise in general coding tasks but have not been evaluated specifically for the nuances of ZK programming.

Method: The authors develop ZK-Eval, a domain-specific evaluation pipeline that assesses language model capabilities at three levels: language knowledge, gadget competence, and full program generation. They benchmark four state-of-the-art LLMs, then introduce ZK-Coder, an agentic framework that augments LLMs with constraint sketching, guided retrieval, and interactive repair.

Result: LLMs perform well at basic syntax but frequently fail at gadget use and semantic correctness, often producing incorrect ZK programs. The ZK-Coder framework improves program success rates substantially: from 17.35% to 83.38% for Circom, and from 32.21% to 90.05% for Noir.

Conclusion: The work provides the first systematic evaluation of LLM capabilities in ZK programming and offers an augmentation framework that dramatically improves outcomes. This lowers development barriers and advances reliable, trustworthy ZK computation.

Abstract: Zero-knowledge proofs (ZKPs) are increasingly deployed in domains such as
privacy-preserving authentication, blockchain scalability, and secure finance.
However, authoring ZK programs remains challenging: unlike mainstream
programming, ZK development requires reasoning about finite field arithmetic,
constraint systems, and gadgets, making it knowledge-intensive and error-prone.
While large language models (LLMs) have demonstrated strong code generation
capabilities in general-purpose languages, their effectiveness for ZK
programming, where correctness hinges on both language mastery and gadget-level
reasoning, remains unexplored. To address this gap, we propose
\textsc{ZK-Eval}, a domain-specific evaluation pipeline that probes LLM
capabilities at three levels: language knowledge, gadget competence, and
end-to-end program generation. Our evaluation of four state-of-the-art LLMs
reveals that models excel at surface-level syntax but struggle with gadget
usage and semantic correctness, often yielding incorrect programs. Based on
these insights, we introduce \textsc{ZK-Coder}, an agentic framework that
augments LLMs with constraint sketching, guided retrieval, and interactive
repair. Experiments on Circom and Noir show substantial gains, with success
rates improving from 17.35\% to 83.38\% and from 32.21\% to 90.05\%,
respectively. With \textsc{ZK-Eval} and \textsc{ZK-Coder}, we establish a
foundation for systematically measuring and augmenting LLMs in ZK code
generation to lower barriers for practitioners and advance trustworthy
computation.

</details>


### [20] [Toward Greener Background Processes -- Measuring Energy Cost of Autosave Feature](https://arxiv.org/abs/2509.11738)
*Maria Küüsvek,Hina Anwar*

Main category: cs.SE

TL;DR: Background processes in desktop apps contribute significantly to energy use. This paper presents a three-stage method to analyze their energy behavior, demonstrated with autosave in Python text editors. Key design factors and four actionable recommendations for greener autosave are provided.


<details>
  <summary>Details</summary>
Motivation: Background processes in desktop applications, such as autosave features, are often overlooked in energy consumption studies despite their significant cumulative impact. The motivation is to address this gap and provide a structured approach to evaluating and improving their energy efficiency.

Method: The paper introduces a three-phase reusable process: 1) decomposing background features into core operations, 2) isolating these operations, and 3) performing controlled energy measurements for comparative profiling. This process is demonstrated via a case study of autosave implementations in three open-source Python text editors, with 900 energy measurement experiments.

Result: The study identifies crucial design factors affecting energy consumption, notably save frequency, buffering strategies, and auxiliary logic like change detection. These insights result in four actionable recommendations for developing greener autosave features in Python applications.

Conclusion: Energy consumption by background processes, specifically autosave features, can be systematically evaluated and optimized using the proposed process. Implementing the recommendations can lead to more sustainable software design.

Abstract: Background processes in desktop applications are often overlooked in energy
consumption studies, yet they represent continuous, automated workloads with
significant cumulative impact. This paper introduces a reusable process for
evaluating the energy behavior of such features at the level of operational
design. The process works in three phases: 1) decomposing background
functionality into core operations, 2) operational isolation, and 3) controlled
measurements enabling comparative profiling. We instantiate the process in a
case study of autosave implementations across three open-source Python-based
text editors. Using 900 empirical software-based energy measurements, we
identify key design factors affecting energy use, including save frequency,
buffering strategy, and auxiliary logic such as change detection. We give four
actionable recommendations for greener implementations of autosave features in
Python to support sustainable software practices.

</details>


### [21] [Analysing Python Machine Learning Notebooks with Moose](https://arxiv.org/abs/2509.11748)
*Marius Mignard,Steven Costiou,Nicolas Anquetil,Anne Etien*

Main category: cs.SE

TL;DR: The paper introduces Vespucci Linter, a new static analysis tool that checks ML notebooks for bad coding practices at multiple levels (Python, notebook structure, and ML-specific issues). Tested on 5,000 Kaggle notebooks, it successfully finds problems that existing tools miss, showing it can improve code quality and reliability for ML projects.


<details>
  <summary>Details</summary>
Motivation: Machine Learning notebooks frequently suffer from poor code quality due to a combination of general coding issues, notebook organizational problems, and ML-specific best practices not being followed. Existing tools fail to address all these levels, missing out on important context and semantics specific to ML.

Method: The authors propose Vespucci Linter, a static analysis tool that operates at three levels: general Python, notebook structure, and ML-specific practices. It uses a metamodeling strategy to unify the representation of notebook and Python code constructs, and implements 22 rules sourced from relevant literature. The tool is then evaluated on 5,000 Kaggle notebooks.

Result: The tool detects rule violations across all three levels in the analyzed notebooks, showing that the multi-level, context-aware approach is effective. The detected issues validate the need for multi-level linting in ML notebooks.

Conclusion: Vespucci Linter can reliably uncover a wide range of code quality problems in ML notebooks, proving the importance and advantage of integrating multiple analysis levels to address unique ML coding requirements. The tool is positioned to help improve both reliability and reproducibility in notebook-based ML development.

Abstract: Machine Learning (ML) code, particularly within notebooks, often exhibits
lower quality compared to traditional software. Bad practices arise at three
distinct levels: general Python coding conventions, the organizational
structure of the notebook itself, and ML-specific aspects such as
reproducibility and correct API usage. However, existing analysis tools
typically focus on only one of these levels and struggle to capture ML-specific
semantics, limiting their ability to detect issues. This paper introduces
Vespucci Linter, a static analysis tool with multi-level capabilities, built on
Moose and designed to address this challenge. Leveraging a metamodeling
approach that unifies the notebook's structural elements with Python code
entities, our linter enables a more contextualized analysis to identify issues
across all three levels. We implemented 22 linting rules derived from the
literature and applied our tool to a corpus of 5,000 notebooks from the Kaggle
platform. The results reveal violations at all levels, validating the relevance
of our multi-level approach and demonstrating Vespucci Linter's potential to
improve the quality and reliability of ML development in notebook environments.

</details>


### [22] [CodeCureAgent: Automatic Classification and Repair of Static Analysis Warnings](https://arxiv.org/abs/2509.11787)
*Pascal Joos,Islem Bouzenia,Michael Pradel*

Main category: cs.SE

TL;DR: CodeCureAgent is an LLM-powered tool that can automatically fix most static analysis warnings in code, outperforming existing solutions in both accuracy and efficiency. It’s cost-effective and suitable for continuous integration workflows, potentially reducing manual workload and improving overall code health.


<details>
  <summary>Details</summary>
Motivation: Static analysis tools are essential for improving code quality by detecting bugs and vulnerabilities, but their warnings require manual resolution, which is tedious and can be ignored by developers, leading to degraded code quality.

Method: The authors propose CodeCureAgent, an LLM-based agentic system that automatically analyzes, classifies, and repairs static analysis warnings in code. It operates iteratively, employing code searches and edits, and applies a three-step patch approval heuristic: building the project, verifying warning resolution without new issues, and running tests. The agent distinguishes false positives and resolves true positives.

Result: On 1,000 SonarQube warnings across 106 Java projects (291 rules), CodeCureAgent produced plausible fixes for 96.8% of cases, improving upon previous baselines by over 29%. Manual review found an 86.3% correct-fix rate. The average cost is 2.9 cents and four minutes per warning.

Conclusion: CodeCureAgent can effectively and efficiently repair static analysis warnings, outperforming prior methods, and is practical for integration into development pipelines to help maintain and improve code quality.

Abstract: Static analysis tools are widely used to detect bugs, vulnerabilities, and
code smells. Traditionally, developers must resolve these warnings manually.
Because this process is tedious, developers sometimes ignore warnings, leading
to an accumulation of warnings and a degradation of code quality. This paper
presents CodeCureAgent, an approach that harnesses LLM-based agents to
automatically analyze, classify, and repair static analysis warnings. Unlike
previous work, our method does not follow a predetermined algorithm. Instead,
we adopt an agentic framework that iteratively invokes tools to gather
additional information from the codebase (e.g., via code search) and edit the
codebase to resolve the warning. CodeCureAgent detects and suppresses false
positives, while fixing true positives when identified. We equip CodeCureAgent
with a three-step heuristic to approve patches: (1) build the project, (2)
verify that the warning disappears without introducing new warnings, and (3)
run the test suite. We evaluate CodeCureAgent on a dataset of 1,000 SonarQube
warnings found in 106 Java projects and covering 291 distinct rules. Our
approach produces plausible fixes for 96.8% of the warnings, outperforming
state-of-the-art baseline approaches by 30.7% and 29.2% in plausible-fix rate,
respectively. Manual inspection of 291 cases reveals a correct-fix rate of
86.3%, showing that CodeCureAgent can reliably repair static analysis warnings.
The approach incurs LLM costs of about 2.9 cents (USD) and an end-to-end
processing time of about four minutes per warning. We envision CodeCureAgent
helping to clean existing codebases and being integrated into CI/CD pipelines
to prevent the accumulation of static analysis warnings.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [23] [Mechanizing Synthetic Tait Computability in Istari](https://arxiv.org/abs/2509.11418)
*Runming Li,Yue Yao,Robert Harper*

Main category: cs.PL

TL;DR: The paper mechanizes synthetic Tait Computability (STC) in the Istari proof assistant, producing reusable libraries for phase distinction and demonstrating that STC constructions can be straightforwardly formalized in machine-checked proofs, thus improving type-theoretical reasoning and meta-theorem verification.


<details>
  <summary>Details</summary>
Motivation: To formalize meta-theorems such as canonicity and normalization in type theory, categorical gluing is commonly used but traditionally requires complex manual reasoning. Synthetic Tait Computability (STC) abstracts this process, and the authors aim to mechanize STC in the Istari proof assistant to streamline such formalizations and eliminate tedious transport reasoning associated with intensional proof assistants.

Method: The authors mechanize Synthetic Tait Computability (STC) within Istari—a proof assistant supporting extensional type theory with equality reflection. They create a reusable library supporting synthetic phase distinction, including modalities, extension types, and strict glue types. The method is validated through two case studies: a canonicity model for dependent type theory (with products and booleans), and a Kripke canonicity model for a cost-aware logical framework.

Result: The study shows that core STC constructions can be formalized in Istari essentially as written in paper arguments. This preserves theoretical elegance and ensures correctness by machine-checking, as demonstrated by the two case studies.

Conclusion: Mechanizing STC in Istari proof assistant preserves the elegance of traditional arguments, streamlines type-theoretical reasoning, and facilitates precise and reusable formal meta-theory libraries, as evidenced by successful case studies in type theory and cost-aware logical frameworks.

Abstract: Categorical gluing is a powerful technique for proving meta-theorems of type
theories such as canonicity and normalization. Synthetic Tait Computability
(STC) provides an abstract treatment of the complex gluing models by
internalizing the gluing category into a modal dependent type theory with a
phase distinction. This work presents a mechanization of STC in the Istari
proof assistant. Istari is a Martin-L\"{o}f-style extensional type theory with
equality reflection. Equality reflection eliminates the nuisance of transport
reasoning typically found in intensional proof assistants. This work develops a
reusable library for synthetic phase distinction, including modalities,
extension types, and strict glue types, and applies it to two case studies: (1)
a canonicity model for dependent type theory with dependent products and
booleans with large elimination, and (2) a Kripke canonicity model for the
cost-aware logical framework. Our results demonstrate that the core STC
constructions can be formalized essentially verbatim in Istari, preserving the
elegance of the on-paper arguments while ensuring machine-checked correctness.

</details>


### [24] [Expressive Power of One-Shot Control Operators and Coroutines](https://arxiv.org/abs/2509.11901)
*Kentaro Kobayashi,Yukiyoshi Kameyama*

Main category: cs.PL

TL;DR: This paper systematically compares one-shot control operators using rigorous definitions. It shows that asymmetric coroutines are more expressive than one-shot effect handlers and delimited continuations, clarifying and correcting previous claims.


<details>
  <summary>Details</summary>
Motivation: One-shot control operators are becoming important due to their balance of expressiveness and efficiency. However, their relative expressiveness compared to each other, and to other abstractions like asymmetric coroutines, is not well understood.

Method: The authors use mathematically rigorous techniques and Felleisen's concept of macro-expressiveness to formally compare different one-shot control operators. They carefully revisit previous informal arguments and develop valid macro-translations between these abstractions.

Result: They demonstrate that one-shot effect handlers and one-shot delimited control operators can be macro-expressed using asymmetric coroutines, but the converse is not possible. They also correct and clarify misunderstandings from prior informal arguments about these encodings.

Conclusion: The paper provides a rigorous foundation for the expressiveness hierarchy among one-shot control abstractions, showing asymmetric coroutines are strictly more expressive under macro-expressiveness than one-shot effect handlers or delimited control operators.

Abstract: Control operators, such as exceptions and effect handlers, provide a means of
representing computational effects in programs abstractly and modularly. While
most theoretical studies have focused on multi-shot control operators, one-shot
control operators -- which restrict the use of captured continuations to at
most once -- are gaining attention for their balance between expressiveness and
efficiency. This study aims to fill the gap. We present a mathematically
rigorous comparison of the expressive power among one-shot control operators,
including effect handlers, delimited continuations, and even asymmetric
coroutines. Following previous studies on multi-shot control operators, we
adopt Felleisen's macro-expressiveness as our measure of expressiveness. We
verify the folklore that one-shot effect handlers and one-shot
delimited-control operators can be macro-expressed by asymmetric coroutines,
but not vice versa. We explain why a previous informal argument fails, and how
to revise it to make a valid macro-translation.

</details>
