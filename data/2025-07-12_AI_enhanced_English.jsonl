{"id": "2507.07480", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.07480", "abs": "https://arxiv.org/abs/2507.07480", "authors": ["Tobias Kapp\u00e9"], "title": "On Propositional Program Equivalence (extended abstract)", "comment": null, "summary": "General program equivalence is undecidable. However, if we abstract away the\nsemantics of statements, then this problem becomes not just decidable, but\npractically feasible. For instance, a program of the form \"if $b$ then $e$ else\n$f$\" should be equivalent to \"if not $b$ then $f$ else $e$\" - no matter what\n$b$, $e$ and $f$ are. This kind of equivalence is known as propositional\nequivalence. In this extended abstract, we discuss recent developments in\npropositional program equivalence from the perspective of (Guarded) Kleene\nAlgebra with Tests, or (G)KAT.", "AI": {"tldr": "By abstracting program semantics and focusing on propositional equivalence, program equivalence problems become decidable and applicable in practice. Recent advances leverage (G)KAT as a solid foundation for this analysis.", "motivation": "Since general program equivalence is undecidable, the motivation is to find a practical, decidable approach by abstracting away from detailed semantics, leading to the study of propositional equivalence. This has practical relevance for verification and optimization of programs.", "method": "The paper uses the framework of (Guarded) Kleene Algebra with Tests, or (G)KAT, to analyze and formalize propositional program equivalence.", "result": "Recent progress has been made in defining and determining propositional program equivalence within the (G)KAT framework, making equivalence checking feasible for practical purposes.", "conclusion": "Propositional program equivalence, when ignoring detailed statement semantics, becomes a decidable and practical problem. (Guarded) Kleene Algebra with Tests ((G)KAT) provides a valuable framework for analyzing such equivalence."}}
{"id": "2507.07325", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07325", "abs": "https://arxiv.org/abs/2507.07325", "authors": ["Martin Obaidi", "Marc Herrmann", "Elisa Schmid", "Raymond Ochsner", "Kurt Schneider", "Jil Kl\u00fcnder"], "title": "A German Gold-Standard Dataset for Sentiment Analysis in Software Engineering", "comment": "This paper has been accepted at the 33rd IEEE International\n  Requirements Engineering Workshop (REW 2025)", "summary": "Sentiment analysis is an essential technique for investigating the emotional\nclimate within developer teams, contributing to both team productivity and\nproject success. Existing sentiment analysis tools in software engineering\nprimarily rely on English or non-German gold-standard datasets. To address this\ngap, our work introduces a German dataset of 5,949 unique developer statements,\nextracted from the German developer forum Android-Hilfe.de. Each statement was\nannotated with one of six basic emotions, based on the emotion model by Shaver\net al., by four German-speaking computer science students. Evaluation of the\nannotation process showed high interrater agreement and reliability. These\nresults indicate that the dataset is sufficiently valid and robust to support\nsentiment analysis in the German-speaking software engineering community.\nEvaluation with existing German sentiment analysis tools confirms the lack of\ndomain-specific solutions for software engineering. We also discuss approaches\nto optimize annotation and present further use cases for the dataset.", "AI": {"tldr": "The paper introduces a robust German dataset for developer sentiment analysis, demonstrates its validity, and highlights the scarcity of specialized tools for sentiment analysis in German software engineering.", "motivation": "Existing sentiment analysis tools for software engineering are mainly based on English or non-German datasets, leaving a gap for robust German-language resources to analyze developer sentiment.", "method": "The authors created a dataset of 5,949 unique German-language developer statements from the forum Android-Hilfe.de. Each statement was annotated with one of six basic emotions, based on Shaver et al.'s emotion model, by four German-speaking computer science students. The annotation process was evaluated for interrater agreement and reliability.", "result": "The annotation process showed high interrater agreement and reliability, confirming the validity and robustness of the dataset. Testing with current German sentiment analysis tools revealed a lack of domain-specific solutions for software engineering.", "conclusion": "The new German-language dataset provides a valuable resource for sentiment analysis in software engineering, validated by strong annotation reliability. Existing sentiment tools are insufficient for this domain, and the dataset enables further research and optimization in annotation strategies."}}
{"id": "2507.07344", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07344", "abs": "https://arxiv.org/abs/2507.07344", "authors": ["Martin Obaidi", "Jannik Fischbach", "Jakob Droste", "Hannah Deters", "Marc Herrmann", "Jil Kl\u00fcnder", "Steffen Kr\u00e4tzig", "Hugo Villamizar", "Kurt Schneider"], "title": "Automatic Generation of Explainability Requirements and Software Explanations From User Reviews", "comment": "This paper has been accepted at the 33rd IEEE International\n  Requirements Engineering Workshop (REW 2025)", "summary": "Explainability has become a crucial non-functional requirement to enhance\ntransparency, build user trust, and ensure regulatory compliance. However,\ntranslating explanation needs expressed in user feedback into structured\nrequirements and corresponding explanations remains challenging. While existing\nmethods can identify explanation-related concerns in user reviews, there is no\nestablished approach for systematically deriving requirements and generating\naligned explanations. To contribute toward addressing this gap, we introduce a\ntool-supported approach that automates this process. To evaluate its\neffectiveness, we collaborated with an industrial automation manufacturer to\ncreate a dataset of 58 user reviews, each annotated with manually crafted\nexplainability requirements and explanations. Our evaluation shows that while\nAI-generated requirements often lack relevance and correctness compared to\nhuman-created ones, the AI-generated explanations are frequently preferred for\ntheir clarity and style. Nonetheless, correctness remains an issue,\nhighlighting the importance of human validation. This work contributes to the\nadvancement of explainability requirements in software systems by (1)\nintroducing an automated approach to derive requirements from user reviews and\ngenerate corresponding explanations, (2) providing empirical insights into the\nstrengths and limitations of automatically generated artifacts, and (3)\nreleasing a curated dataset to support future research on the automatic\ngeneration of explainability requirements.", "AI": {"tldr": "This paper presents a tool that automatically extracts and explains explainability requirements from user reviews. While AI-generated explanations are usually preferred for clear communication, accuracy is still an issue, emphasizing the importance of human validation. The study also provides a new annotated dataset for future research.", "motivation": "Explainability is increasingly important for software systems to enhance transparency, user trust, and regulatory compliance. However, it is difficult to translate user feedback on explainability into structured requirements and linked explanations, and there is a lack of established, systematic approaches for this task.", "method": "The authors developed a tool-supported, automated process for extracting explainability requirements and generating corresponding explanations from user reviews. This was evaluated through a case study using 58 user reviews from an industrial automation manufacturer, with manual annotation of requirements and explanations for comparison.", "result": "AI-generated requirements often lack relevance and correctness compared to human-crafted ones, but the generated explanations are usually preferred for their clarity and style. However, there are correctness issues in both, underlining the need for human oversight.", "conclusion": "The paper introduces an automated method for deriving explainability requirements from user reviews and creating explanations, offers empirical insights into the strengths and weaknesses of automatic approaches, and releases an annotated dataset to further research in this area."}}
{"id": "2507.07468", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07468", "abs": "https://arxiv.org/abs/2507.07468", "authors": ["Sten Gr\u00fcner", "Nafise Eskandani"], "title": "Towards an Engineering Workflow Management System for Asset Administration Shells using BPMN", "comment": "7 pages, 7 figures, Accepted at IFAC EAAS 2025\n  (https://j3c.org/eaas.php)", "summary": "The integration of Industry 4.0 technologies into engineering workflows is an\nessential step toward automating and optimizing plant and process engineering\nprocesses. The Asset Administration Shell (AAS) serves as a key enabler for\ncreating interoperable Digital Twins that facilitate engineering data exchange\nand automation. This paper explores the use of AAS within engineering\nworkflows, particularly in combination with Business Process Model and Notation\n(BPMN) to define structured and automated processes. We propose a distributed\nAAS copy-on-write infrastructure that enhances security and scalability while\nenabling seamless cross organizational collaboration. We also introduce a\nworkflow management prototype automating AAS operations and engineering\nworkflows, improving efficiency and traceability.", "AI": {"tldr": "This paper proposes integrating AAS and BPMN for automated engineering workflows, introducing a secure and scalable distributed infrastructure, and demonstrates improved efficiency through a novel management prototype.", "motivation": "There is a need to automate and optimize engineering workflows in the context of Industry 4.0, especially to enhance cross-organizational collaboration, security, and scalability.", "method": "The paper explores the application of Asset Administration Shell (AAS) in conjunction with Business Process Model and Notation (BPMN). It proposes a distributed AAS copy-on-write infrastructure and presents a workflow management prototype for automating AAS operations.", "result": "The proposed distributed AAS infrastructure enables secure, scalable, and seamless collaboration across organizations. The workflow management prototype successfully automates engineering workflows and AAS operations, resulting in improved efficiency and traceability.", "conclusion": "Integrating AAS and BPMN within engineering workflows using a distributed copy-on-write approach significantly enhances automation, collaboration, and data management, making engineering processes more efficient and secure."}}
{"id": "2507.07548", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07548", "abs": "https://arxiv.org/abs/2507.07548", "authors": ["Jonathan Ullrich", "Matthias Koch", "Andreas Vogelsang"], "title": "From Requirements to Code: Understanding Developer Practices in LLM-Assisted Software Engineering", "comment": "This paper has been accepted for publication at the 33rd IEEE\n  International Requirements Engineering (RE) conference", "summary": "With the advent of generative LLMs and their advanced code generation\ncapabilities, some people already envision the end of traditional software\nengineering, as LLMs may be able to produce high-quality code based solely on\nthe requirements a domain expert feeds into the system. The feasibility of this\nvision can be assessed by understanding how developers currently incorporate\nrequirements when using LLMs for code generation-a topic that remains largely\nunexplored. We interviewed 18 practitioners from 14 companies to understand how\nthey (re)use information from requirements and other design artifacts to feed\nLLMs when generating code. Based on our findings, we propose a theory that\nexplains the processes developers employ and the artifacts they rely on. Our\ntheory suggests that requirements, as typically documented, are too abstract\nfor direct input into LLMs. Instead, they must first be manually decomposed\ninto programming tasks, which are then enriched with design decisions and\narchitectural constraints before being used in prompts. Our study highlights\nthat fundamental RE work is still necessary when LLMs are used to generate\ncode. Our theory is important for contextualizing scientific approaches to\nautomating requirements-centric SE tasks.", "AI": {"tldr": "Despite advances in LLM code generation, developers must still manually convert and enrich requirements into concrete tasks for LLM prompts. Basic requirements engineering remains crucial, and the dream of fully automated requirements-to-code generation is not yet feasible.", "motivation": "With the advancement of generative LLMs for code generation, there is speculation that traditional software engineering could become obsolete. However, it is unclear how developers incorporate requirements when using these LLMs\u2014a critical and underexplored area.", "method": "The researchers interviewed 18 practitioners from 14 companies to explore how they leverage requirements and design artifacts in prompts for LLM-based code generation. The data from these interviews was analyzed to build a grounded theory.", "result": "The study found that requirements as typically documented are too abstract for direct use in LLM prompts. Developers must decompose requirements into programming tasks and enrich them with design and architectural constraints before inputting them into LLMs. The process is not fully automated and requires significant manual effort.", "conclusion": "Traditional requirements engineering (RE) work remains necessary even with LLM-powered code generation. Developers still need to translate and adapt requirements into forms suitable for LLM consumption, meaning automation of requirements-centric tasks is not yet fully realized."}}
{"id": "2507.07682", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07682", "abs": "https://arxiv.org/abs/2507.07682", "authors": ["Kaicheng Huang", "Fanyu Wang", "Yutan Huang", "Chetan Arora"], "title": "Prompt Engineering for Requirements Engineering: A Literature Review and Roadmap", "comment": null, "summary": "Advancements in large language models (LLMs) have led to a surge of prompt\nengineering (PE) techniques that can enhance various requirements engineering\n(RE) tasks. However, current LLMs are often characterized by significant\nuncertainty and a lack of controllability. This absence of clear guidance on\nhow to effectively prompt LLMs acts as a barrier to their trustworthy\nimplementation in the RE field. We present the first roadmap-oriented\nsystematic literature review of Prompt Engineering for RE (PE4RE). Following\nKitchenham's and Petersen's secondary-study protocol, we searched six digital\nlibraries, screened 867 records, and analyzed 35 primary studies. To bring\norder to a fragmented landscape, we propose a hybrid taxonomy that links\ntechnique-oriented patterns (e.g., few-shot, Chain-of-Thought) to task-oriented\nRE roles (elicitation, validation, traceability). Two research questions, with\nfive sub-questions, map the tasks addressed, LLM families used, and prompt\ntypes adopted, and expose current limitations and research gaps. Finally, we\noutline a step-by-step roadmap showing how today's ad-hoc PE prototypes can\nevolve into reproducible, practitioner-friendly workflows.", "AI": {"tldr": "This paper systematically reviews and structures the field of prompt engineering for requirements engineering, proposing a taxonomy and a stepwise roadmap to make LLM applications in RE more trustworthy and practical.", "motivation": "Large language models have unlocked new possibilities in requirements engineering through prompt engineering, but uncertainty and lack of controllability hinder reliable implementation. Clear, evidence-based guidance is needed to leverage LLMs effectively.", "method": "The authors performed a systematic literature review using Kitchenham\u2019s and Petersen\u2019s protocol, searching six digital libraries, screening 867 records, and analyzing 35 selected primary studies in the field of prompt engineering for requirements engineering.", "result": "The review produced a hybrid taxonomy connecting prompt engineering techniques to requirements engineering roles and mapped out the tasks, LLM families, and prompt types reported in the literature. It revealed persistent limitations and gaps in current knowledge.", "conclusion": "A roadmap was outlined, guiding the transition from current ad-hoc prototypes to reproducible and user-friendly workflows, providing structure for future research and more reliable PE adoption in RE."}}
{"id": "2507.07689", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07689", "abs": "https://arxiv.org/abs/2507.07689", "authors": ["Chetan Arora", "Fanyu Wang", "Chakkrit Tantithamthavorn", "Aldeida Aleti", "Shaun Kenyon"], "title": "From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry", "comment": null, "summary": "Requirements engineering (RE) in the space industry is inherently complex,\ndemanding high precision, alignment with rigorous standards, and adaptability\nto mission-specific constraints. Smaller space organisations and new entrants\noften struggle to derive actionable requirements from extensive, unstructured\ndocuments such as mission briefs, interface specifications, and regulatory\nstandards. In this innovation opportunity paper, we explore the potential of\nRetrieval-Augmented Generation (RAG) models to support and (semi-)automate\nrequirements generation in the space domain. We present a modular, AI-driven\napproach that preprocesses raw space mission documents, classifies them into\nsemantically meaningful categories, retrieves contextually relevant content\nfrom domain standards, and synthesises draft requirements using large language\nmodels (LLMs). We apply the approach to a real-world mission document from the\nspace domain to demonstrate feasibility and assess early outcomes in\ncollaboration with our industry partner, Starbound Space Solutions. Our\npreliminary results indicate that the approach can reduce manual effort,\nimprove coverage of relevant requirements, and support lightweight compliance\nalignment. We outline a roadmap toward broader integration of AI in RE\nworkflows, intending to lower barriers for smaller organisations to participate\nin large-scale, safety-critical missions.", "AI": {"tldr": "AI-driven RAG models can automate and improve requirements engineering for the space industry, helping small organizations more easily generate and comply with mission requirements.", "motivation": "Requirements engineering (RE) in the space industry is challenging, especially for small organizations, due to the complexity, demand for precision, and need to adhere to strict standards when deriving actionable requirements from unstructured documents.", "method": "The paper proposes a modular, AI-driven approach leveraging Retrieval-Augmented Generation (RAG) models. It preprocesses mission documents, classifies them semantically, retrieves domain-standard content, and synthesizes requirements drafts via large language models (LLMs), demonstrated on a real-world case study with industry partnership.", "result": "Preliminary results show the approach reduces manual effort, improves requirements coverage, and aids compliance, making RE more accessible for smaller organizations.", "conclusion": "Integrating AI-based RAG models into the requirements engineering process can significantly benefit small space organizations by automating and streamlining requirement generation, thus enabling broader participation in critical space missions."}}
