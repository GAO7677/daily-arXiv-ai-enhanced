{"id": "2509.20380", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.20380", "abs": "https://arxiv.org/abs/2509.20380", "authors": ["Samyak Jhaveri", "Vanessa Klotzmann", "Crista Lopes"], "title": "ACCeLLiuM: Supervised Fine-Tuning for Automated OpenACC Pragma Generation", "comment": null, "summary": "The increasing ubiquity of GPUs is accompanied by the increasing complexity\nof their hardware and parallel programming frameworks. Directive-based parallel\nprogramming standards like OpenACC simplify GPU programming to some extent by\nabstracting away low-level complexities, but a fair amount of expertise is\nstill required in order to use those directives effectively.\n  We introduce ACCeLLiuM, two open weights Large Language Models specifically\nfine-tuned for generating expert OpenACC directives for data-parallel loops,\nalong with the supervised fine-tuning dataset that was used to train them. The\nACCeLLiuM SFT dataset contains 4,033 OpenACC pragma-loop pairs mined from\npublic GitHub C/C++ repositories, with 3,223 pairs for training and 810 for\ntesting. Experimental evaluations show a pronounced performance gap in\ngenerating correct OpenACC pragmas between base LLMs and our fine-tuned\nversions. On the held-out test set, base LLMs fail to consistently generate\nvalid pragmas, whereas LLMs fine-tuned on the ACCeLLiuM dataset generate valid\npragmas with the correct directive type for $87\\%$ of the data-parallel loops,\nand exact pragmas--including directives, clauses, clause order, and clause\nvariables--for $50\\%$ of the cases. Even when not exact, generated pragmas\nfrequently incorporate the correct clauses in a different order than the\nground-truth label, or include additional clauses that enable finer control\nover parallel execution, data movement, and concurrency, offering practical\nvalue beyond strict string-matching. By publicly releasing the code, models,\nand dataset as ACCeLLiuM we hope to establish a reproducible benchmark for\nLLM-powered OpenACC pragma generation, and lower the barrier to automated GPU\noffloading of serially written programs.", "AI": {"tldr": "ACCeLLiuM, two open-weight LLMs fine-tuned on a curated dataset of OpenACC pragmas, substantially improve the automated generation of correct OpenACC directives for GPU programming, outperforming base LLMs and aiding code parallelization.", "motivation": "GPU programming is complex and even with directive-based approaches like OpenACC, significant expertise is needed. The research aims to ease this process by enabling LLMs to automatically generate expert-level OpenACC directives, making GPU offloading more accessible.", "method": "The authors introduce ACCeLLiuM, two Large Language Models (LLMs) fine-tuned for generating OpenACC directives for data-parallel loops. They trained these models on a supervised dataset containing 4,033 OpenACC pragma-loop pairs sourced from public C/C++ repositories.", "result": "Fine-tuned ACCeLLiuM LLMs significantly outperform base LLMs, generating valid OpenACC pragmas with the correct type in 87% of cases and exact matches in 50%. Even non-exact outputs usually add valuable clauses or rearrange them, enhancing practical utility.", "conclusion": "The paper demonstrates that domain-specific fine-tuning enables LLMs to effectively generate correct and useful OpenACC directives, lowering barriers to automated GPU programming. The public release of code, models, and data sets a reproducible standard for future research in this area."}}
{"id": "2509.20385", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20385", "abs": "https://arxiv.org/abs/2509.20385", "authors": ["Ishara Devendra", "Chaman Wijesiriwardana", "Prasad Wimalaratne"], "title": "State-of-the-Art in Software Security Visualization: A Systematic Review", "comment": null, "summary": "Software security visualization is an interdisciplinary field that combines\nthe technical complexity of cybersecurity, including threat intelligence and\ncompliance monitoring, with visual analytics, transforming complex security\ndata into easily digestible visual formats. As software systems get more\ncomplex and the threat landscape evolves, traditional text-based and numerical\nmethods for analyzing and interpreting security concerns become increasingly\nineffective. The purpose of this paper is to systematically review existing\nresearch and create a comprehensive taxonomy of software security visualization\ntechniques through literature, categorizing these techniques into four types:\ngraph-based, notation-based, matrix-based, and metaphor-based visualization.\nThis systematic review explores over 60 recent key research papers in software\nsecurity visualization, highlighting its key issues, recent advancements, and\nprospective future research directions. From the comprehensive analysis, the\ntwo main areas were distinctly highlighted as extensive software development\nvisualization, focusing on advanced methods for depicting software\narchitecture: operational security visualization and cybersecurity\nvisualization. The findings highlight the necessity for innovative\nvisualization techniques that adapt to the evolving security landscape, with\npractical implications for enhancing threat detection, improving security\nresponse strategies, and guiding future research.", "AI": {"tldr": "This paper systematically reviews and categorizes software security visualization techniques, revealing current advancements and gaps. It underscores the need for adaptive and innovative visual tools that better support threat detection and cybersecurity operations as software systems and threats become more complex.", "motivation": "Traditional text and numeric analysis methods are increasingly unable to handle the complexity of modern software security issues. There is a need for better ways to visualize and interpret security data as threat landscapes and software systems evolve.", "method": "Systematic review of over 60 recent research papers, followed by classification of security visualization techniques into four categories: graph-based, notation-based, matrix-based, and metaphor-based.", "result": "The two main areas of focus in software security visualization were identified: extensive software development visualization and operational/cybersecurity visualization. The paper shows recent advancements and highlights ongoing key issues, emphasizing the need for adaptive, innovative visualization techniques to keep up with evolving threats.", "conclusion": "Innovative visualization techniques are required to effectively support threat detection, security strategy, and future research due to the growing complexity of software and security threats."}}
{"id": "2509.20386", "categories": ["cs.SE", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.20386", "abs": "https://arxiv.org/abs/2509.20386", "authors": ["Nishant Gaurav", "Adit Akarsh", "Ankit Ranjan", "Manoj Bajaj"], "title": "Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments", "comment": null, "summary": "We present Dynamic ReAct, a novel approach for enabling ReAct agents to ef-\nficiently operate with extensive Model Control Protocol (MCP) tool sets that\nexceed the contextual memory limitations of large language models. Our approach\naddresses the fundamental challenge of tool selection in environments\ncontaining hundreds or thousands of available tools, where loading all tools\nsimultaneously is computationally infeasible. We propose and evaluate five\ndistinct architectures that progressively refine the tool selection process,\nculminating in a search-and-load mechanism that achieves intelligent tool\nselection with minimal computational overhead. Our experimental results\ndemonstrate that the proposed approach reduces tool loading by up to 50% while\nmaintaining task completion accuracy, advancing the path towards truly\ngeneral-purpose AI agents capable of dynamically adapting to diverse task\nenvironments.", "AI": {"tldr": "Dynamic ReAct allows ReAct agents to efficiently pick the right tools from massive tool sets\u2014without overloading memory\u2014by using refined selection strategies, which halves tool loading needs while keeping performance high.", "motivation": "ReAct agents struggle with using a large number of tools due to memory and computational constraints when all tools cannot be loaded at once. There is a key challenge in efficiently selecting relevant tools from a vast pool.", "method": "The authors introduce Dynamic ReAct, which includes five architectures designed to refine the tool selection process for ReAct agents. The culminating architecture employs a 'search-and-load' strategy to select tools intelligently while minimizing computation.", "result": "Experimental results show up to 50% reduction in tool loading, with task completion accuracy maintained despite not loading all tools at once.", "conclusion": "Dynamic ReAct enables ReAct agents to effectively and efficiently use large tool sets beyond the memory limits of LLMs by improving the tool selection process, thus supporting more general-purpose and adaptable AI agents."}}
{"id": "2509.20387", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20387", "abs": "https://arxiv.org/abs/2509.20387", "authors": ["Qusai Ramadan", "Jukka Ruohonen", "Abhishek Tiwari", "Adam Alami", "Zeyd Boukhers"], "title": "Towards Systematic Specification and Verification of Fairness Requirements: A Position Paper", "comment": "Accepted at the 2025 IEEE 33rd International Requirements Engineering\n  Conference Workshops", "summary": "Decisions suggested by improperly designed software systems might be prone to\ndiscriminate against people based on protected characteristics, such as gender\nand ethnicity. Previous studies attribute such undesired behavior to flaws in\nalgorithmic design or biased data. However, these studies ignore that\ndiscrimination is often the result of a lack of well-specified fairness\nrequirements and their verification. The fact that experts' knowledge about\nfairness is often implicit makes the task of specifying precise and verifiable\nfairness requirements difficult. In related domains, such as security\nengineering, knowledge graphs have been proven to be effective in formalizing\nknowledge to assist requirements specification and verification. To address the\nlack of formal mechanisms for specifying and verifying fairness requirements,\nwe propose the development of a knowledge graph-based framework for fairness.\nIn this paper, we discuss the challenges, research questions, and a road map\ntowards addressing the research questions.", "AI": {"tldr": "Discrimination in software systems is often caused not just by bad algorithms or biased data, but by weakly defined fairness requirements. This paper suggests that knowledge graphs, used successfully in fields like security, could be used to better specify and verify fairness in software, outlining key challenges and next steps.", "motivation": "Improperly designed software systems may discriminate based on protected characteristics due to insufficient specification and verification of fairness requirements, with experts' knowledge often being implicit and hard to formalize.", "method": "The paper proposes using knowledge graphs, drawing parallels from their successful application in security engineering, to structure and verify fairness requirements. It provides a roadmap and discusses related challenges and research questions.", "result": "The paper lays out a conceptual framework and research approach, but does not report empirical results. It suggests that knowledge graphs offer a promising path for formalizing fairness requirements.", "conclusion": "The paper concludes that a knowledge graph-based framework can help formalize and verify fairness requirements in software systems, addressing current gaps in requirement specification."}}
{"id": "2509.20426", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20426", "abs": "https://arxiv.org/abs/2509.20426", "authors": ["Mahmoud Samir Fayed"], "title": "Dual-Language General-Purpose Self-Hosted Visual Language and new Textual Programming Language for Applications", "comment": "PhD thesis", "summary": "Most visual programming languages (VPLs) are domain-specific, with few\ngeneral-purpose VPLs like Programming Without Coding Technology (PWCT). These\ngeneral-purpose VPLs are developed using textual programming languages and\nimproving them requires textual programming. In this thesis, we designed and\ndeveloped PWCT2, a dual-language (Arabic/English), general-purpose,\nself-hosting visual programming language. Before doing so, we specifically\ndesigned a textual programming language called Ring for its development. Ring\nis a dynamically typed language with a lightweight implementation, offering\nsyntax customization features. It permits the creation of domain-specific\nlanguages through new features that extend object-oriented programming,\nallowing for specialized languages resembling Cascading Style Sheets (CSS) or\nSupernova language. The Ring Compiler and Virtual Machine are designed using\nthe PWCT visual programming language where the visual implementation is\ncomposed of 18,945 components that generate 24,743 lines of C code, which\nincreases the abstraction level and hides unnecessary details. Using PWCT to\ndevelop Ring allowed us to realize several issues in PWCT, which led to the\ndevelopment of the PWCT2 visual programming language using the Ring textual\nprogramming language. PWCT2 provides approximately 36 times faster code\ngeneration and requires 20 times less storage for visual source files. It also\nallows for the conversion of Ring code into visual code, enabling the creation\nof a self-hosting VPL that can be developed using itself. PWCT2 consists of\napproximately 92,000 lines of Ring code and comes with 394 visual components.\nPWCT2 is distributed to many users through the Steam platform and has received\npositive feedback, On Steam, 1772 users have launched the software, and the\ntotal recorded usage time exceeds 17,000 hours, encouraging further research\nand development.", "AI": {"tldr": "PWCT2 is a new dual-language, self-hosting visual programming language made using a custom textual language, Ring. It provides much faster code generation, efficient storage, and can be built and extended using itself, with positive user reception and promising future potential.", "motivation": "Most visual programming languages (VPLs) are domain-specific and existing general-purpose VPLs are dependent on textual languages for development and improvement, which limits accessibility and self-sufficiency.", "method": "The authors designed and developed PWCT2, a dual-language (Arabic/English), general-purpose, self-hosting visual programming language. This involved first developing Ring, a textual, dynamically typed language, and then implementing its compiler and virtual machine using the PWCT visual programming language, later leading to the development of PWCT2 in Ring.", "result": "PWCT2 provides substantial improvements: code generation is approximately 36 times faster, and visual source files require 20 times less storage. PWCT2 enables conversion between textual and visual code and is self-hosting. Its distribution has seen over 1,700 users and more than 17,000 hours of usage on Steam with positive feedback.", "conclusion": "PWCT2 demonstrates the feasibility of a self-hosting, general-purpose, visual programming language developed in a textual language designed for such extensibility. The dual-language support and code efficiency gains make it a significant advance, and its uptake among users suggests strong potential for further research and development."}}
{"id": "2509.20415", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20415", "abs": "https://arxiv.org/abs/2509.20415", "authors": ["Yu Pan", "Xiaocheng Li", "Hanzhao Wang"], "title": "Online-Optimized RAG for Tool Use and Function Calling", "comment": null, "summary": "In many applications, retrieval-augmented generation (RAG) drives tool use\nand function calling by embedding the (user) queries and matching them to\npre-specified tool/function descriptions. In this paper, we address an\nembedding misalignment issue that often arises in practical applications due to\nimperfect embedding models or noisy descriptions; such misalignment may lead to\nincorrect retrieval and task failure. We introduce Online-Optimized RAG, a\ndeployment-time framework that continually adapts retrieval embeddings from\nlive interactions using minimal feedback (e.g., task success). Online-Optimized\nRAG applies lightweight online gradient updates with negligible per-query\nlatency and requires no changes to the underlying LLM. The method is\nplug-and-play: it supports both single- and multi-hop tool use, dynamic tool\ninventories, and $K$-retrieval with re-ranking. We provide a problem-dependent\ntheoretical analysis that quantifies how the method's performance depends on\nthe initialization quality of the embeddings and other related quantities.\nAcross diverse tool-use and document-retrieval scenarios, our Online-Optimized\nRAG consistently improves tool selection accuracy and end-task success, thus\nproviding a simple, practical path to robust, self-improving RAG systems.", "AI": {"tldr": "Online-Optimized RAG is a practical framework for improving tool selection in retrieval-augmented generation systems, adapting embeddings in real-time based on minimal feedback, and strengthening system reliability and success without modifying the underlying LLM.", "motivation": "Current retrieval-augmented generation (RAG) systems often rely on embedding-based query matching to select tools or functions, but suffer from embedding misalignment due to imperfect models or noisy descriptions. This misalignment can cause retrieval errors and task failure in practical applications.", "method": "The proposed method, Online-Optimized RAG, uses online gradient-based updates during deployment to continually adapt retrieval embeddings based on minimal user feedback (like task success). It works without modifying the LLM, supports single-/multi-hop tool use, dynamic tool inventories, and improves ranking in $K$-retrieval scenarios, with negligible added latency.", "result": "Online-Optimized RAG consistently improves the accuracy of tool selection and the overall success rate across varied tool-use and document-retrieval tasks. Theoretical analysis connects performance to embedding initialization and relevant parameters.", "conclusion": "Online-Optimized RAG provides a simple, robust, and self-improving solution for RAG systems, making them more resilient to embedding misalignment in real-world deployments."}}
{"id": "2509.20534", "categories": ["cs.PL", "cs.SC", "68W30", "I.1.1; G.4"], "pdf": "https://arxiv.org/pdf/2509.20534", "abs": "https://arxiv.org/abs/2509.20534", "authors": ["Bowen Zhu", "Aayush Sabharwal", "Songchen Tan", "Yingbo Ma", "Alan Edelman", "Christopher Rackauckas"], "title": "Efficient Symbolic Computation vis Hash Consing", "comment": null, "summary": "Symbolic computation systems suffer from memory inefficiencies due to\nredundant storage of structurally identical subexpressions, commonly known as\nexpression swell, which degrades performance in both classical computer algebra\nand emerging AI-driven mathematical reasoning tools. In this paper, we present\nthe first integration of hash consing into JuliaSymbolics, a high-performance\nsymbolic toolkit in Julia, by employing a global weak-reference hash table that\ncanonicalizes expressions and eliminates duplication. This approach reduces\nmemory consumption and accelerates key operations such as differentiation,\nsimplification, and code generation, while seamlessly integrating with Julia's\nmetaprogramming and just-in-time compilation infrastructure. Benchmark\nevaluations across different computational domains reveal substantial\nimprovements: symbolic computations are accelerated by up to 3.2 times, memory\nusage is reduced by up to 2 times, code generation is up to 5 times faster,\nfunction compilation up to 10 times faster, and numerical evaluation up to 100\ntimes faster for larger models. While certain workloads with fewer duplicate\nunknown-variable expressions show more modest gains or even slight overhead in\ninitial computation stages, downstream processing consistently benefits\nsignificantly. These findings underscore the importance of hash consing in\nscaling symbolic computation and pave the way for future work integrating hash\nconsing with e-graphs for enhanced equivalence-aware expression sharing in\nAI-driven pipelines.", "AI": {"tldr": "Integrating hash consing into JuliaSymbolics significantly reduces memory usage and speeds up symbolic computations, code generation, and evaluation, especially for redundant, complex expressions. This is a key improvement for scaling symbolic tools and supporting future AI-driven mathematics.", "motivation": "Symbolic computation systems face performance and memory problems due to expression swell, caused by redundant storage of structurally identical subexpressions. This issue hampers classical computer algebra systems as well as modern AI-based mathematical tools.", "method": "The paper implements hash consing in the JuliaSymbolics toolkit by employing a global weak-reference hash table to canonicalize symbolic expressions and eliminate duplication. This solution is tightly integrated with Julia's metaprogramming and just-in-time compilation features.", "result": "Benchmark tests show that this hash consing integration significantly speeds up symbolic operations (up to 3.2x), reduces memory consumption (up to 2x), makes code generation and compilation much faster (up to 5x and 10x, respectively), and dramatically improves numerical evaluation speed for large models (up to 100x). While workloads with few duplicate expressions have smaller gains or slight overhead initially, downstream tasks consistently benefit.", "conclusion": "Hash consing in JuliaSymbolics enhances scalability and performance of symbolic computations and is particularly impactful for complex workloads with high redundancy; it also lays groundwork for future integration with e-graphs for equivalence-aware sharing in AI mathematical reasoning."}}
{"id": "2509.20421", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20421", "abs": "https://arxiv.org/abs/2509.20421", "authors": ["Reiner H\u00e4hnle", "Cosimo Laneve", "Adele Veschetti"], "title": "Formal Verification of Legal Contracts: A Translation-based Approach", "comment": null, "summary": "Stipula is a domain-specific programming language designed to model legal\ncontracts with enforceable properties, especially those involving asset\ntransfers and obligations. This paper presents a methodology to formally verify\nthe correctness of Stipula contracts through translation into Java code\nannotated with Java Modeling Language specifications. As a verification\nbackend, the deductive verification tool KeY is used. Both, the translation and\nthe verification of partial and total correctness for a large subset of Stipula\ncontracts, those with disjoint cycles, is fully automatic. Our work\ndemonstrates that a general-purpose deductive verification tool can be used\nsuccessfully in a translation approach.", "AI": {"tldr": "Stipula contracts can be automatically and formally verified for correctness by translating them into annotated Java code and using general-purpose verification tools.", "motivation": "The motivation is to ensure the correctness and enforceability of legal contracts (especially asset transfers and obligations) implemented in the Stipula programming language.", "method": "The method involves translating Stipula contracts into Java code annotated with Java Modeling Language (JML) specifications, then using the deductive verification tool KeY to formally verify contract correctness.", "result": "The approach is able to automatically verify both partial and total correctness for a large subset of Stipula contracts, specifically those with disjoint cycles.", "conclusion": "General-purpose deductive verification tools, like KeY, can be successfully applied to formally verify the correctness of domain-specific contract languages via an automatic translation approach."}}
{"id": "2509.20491", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20491", "abs": "https://arxiv.org/abs/2509.20491", "authors": ["Brahim Mahmoudi", "Naouel Moha", "Quentin Stievenert", "Florent Avellaneda"], "title": "AI-Specific Code Smells: From Specification to Detection", "comment": null, "summary": "The rise of Artificial Intelligence (AI) is reshaping how software systems\nare developed and maintained. However, AI-based systems give rise to new\nsoftware issues that existing detection tools often miss. Among these, we focus\non AI-specific code smells, recurring patterns in the code that may indicate\ndeeper problems such as unreproducibility, silent failures, or poor model\ngeneralization. We introduce SpecDetect4AI, a tool-based approach for the\nspecification and detection of these code smells at scale. This approach\ncombines a high-level declarative Domain-Specific Language (DSL) for rule\nspecification with an extensible static analysis tool that interprets and\ndetects these rules for AI-based systems. We specified 22 AI-specific code\nsmells and evaluated SpecDetect4AI on 826 AI-based systems (20M lines of code),\nachieving a precision of 88.66% and a recall of 88.89%, outperforming other\nexisting detection tools. Our results show that SpecDetect4AI supports the\nspecification and detection of AI-specific code smells through dedicated rules\nand can effectively analyze large AI-based systems, demonstrating both\nefficiency and extensibility (SUS 81.7/100).", "AI": {"tldr": "AI systems have unique code smells that aren't detected by current tools. SpecDetect4AI combines DSL-based rule specification and extensible static analysis to detect these issues with high accuracy at scale, boosting maintainability and reliability.", "motivation": "AI-based systems introduce unique software issues that current detection tools often overlook, like reproducibility problems and silent failures. Addressing these new code smells is necessary to improve AI software reliability.", "method": "SpecDetect4AI is a novel tool combining a declarative DSL for easy specification of code smell rules and a static analysis engine capable of large-scale detection in AI-based codebases. The tool supports extensibility and high-level rule creation.", "result": "SpecDetect4AI specified 22 AI-specific code smells and was tested on 826 AI systems (~20 million lines of code), achieving a precision of 88.66% and recall of 88.89%. The system improved performance compared to existing tools and received a usability score of 81.7/100.", "conclusion": "SpecDetect4AI efficiently enables the specification and scalable detection of AI-specific code smells. It outperforms prior tools in accuracy and usability and can analyze large AI systems, improving maintainability and reliability."}}
{"id": "2509.20518", "categories": ["cs.SE", "cs.PL", "D.2.3"], "pdf": "https://arxiv.org/pdf/2509.20518", "abs": "https://arxiv.org/abs/2509.20518", "authors": ["Sayed Mahbub Hasan Amiri", "Md Mainul Islam"], "title": "Enhancing Python Programming Education with an AI-Powered Code Helper: Design, Implementation, and Impact", "comment": "20 pages, 16 figures", "summary": "This is the study that presents an AI-Python-based chatbot that helps\nstudents to learn programming by demonstrating solutions to such problems as\ndebugging errors, solving syntax problems or converting abstract theoretical\nconcepts to practical implementations. Traditional coding tools like Integrated\nDevelopment Environments (IDEs) and static analyzers do not give robotic help\nwhile AI-driven code assistants such as GitHub Copilot focus on getting things\ndone. To close this gap, our chatbot combines static code analysis, dynamic\nexecution tracing, and large language models (LLMs) to provide the students\nwith relevant and practical advice, hence promoting the learning process. The\nchatbots hybrid architecture employs CodeLlama for code embedding, GPT-4 for\nnatural language interactions, and Docker-based sandboxing for secure\nexecution. Evaluated through a mixed-methods approach involving 1,500 student\nsubmissions, the system demonstrated an 85% error resolution success rate,\noutperforming standalone tools like pylint (62%) and GPT-4 (73%). Quantitative\nresults revealed a 59.3% reduction in debugging time among users, with pre- and\npost-test assessments showing a 34% improvement in coding proficiency,\nparticularly in recursion and exception handling. Qualitative feedback from 120\nstudents highlighted the chatbots clarity, accessibility, and\nconfidence-building impact, though critiques included occasional latency and\nrestrictive code sanitization. By balancing technical innovation with\npedagogical empathy, this research provides a blueprint for AI tools that\nprioritize educational equity and long-term skill retention over mere code\ncompletion. The chatbot exemplifies how AI can augment human instruction,\nfostering deeper conceptual understanding in programming education.", "AI": {"tldr": "This paper presents an AI-driven programming chatbot for students, which outperforms traditional tools in resolving code errors, improving learning efficiency, and boosting student confidence. Its hybrid architecture leverages advanced language models and secure execution, providing a practical blueprint for educational AI systems.", "motivation": "Traditional programming tools and code assistants either lack interactive, robotic, and educational support for students or focus mainly on task completion instead of learning enhancement. This study addresses the need for an AI-based system that emphasizes student learning and understanding.", "method": "The proposed solution is an AI-Python-based chatbot that integrates static code analysis, dynamic execution tracing, and large language models (LLMs) for personalized feedback. It uses CodeLlama for code embedding, GPT-4 for natural language exchanges, and Docker-based sandboxing for security. The system was evaluated with a mixed-methods approach over 1,500 student code submissions, complemented by pre- and post-tests and qualitative surveys from 120 students.", "result": "The chatbot achieved an 85% error resolution rate, surpassing traditional tools like pylint (62%) and standalone GPT-4 (73%). Students using the chatbot showed a 59.3% reduction in debugging time and a 34% improvement in coding proficiency, especially in complex areas like recursion and exception handling. Student feedback noted strengths in clarity, usability, and increased learning confidence, with minor concerns about latency and security constraints.", "conclusion": "The study demonstrates that combining technical innovation with educational goals can lead to highly effective AI-powered learning tools. The chatbot serves as a model for future AI systems that prioritize educational impact, equity, and student skill development rather than only code completion."}}
{"id": "2509.20497", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20497", "abs": "https://arxiv.org/abs/2509.20497", "authors": ["Ahmed Aljohani", "Hyunsook Do"], "title": "PromptDebt: A Comprehensive Study of Technical Debt Across LLM Projects", "comment": "Accepted at Proceedings of the 2025 Evaluation and Assessment in\n  Software Engineering (EASE '25)", "summary": "Large Language Models (LLMs) are increasingly embedded in software via APIs\nlike OpenAI, offering powerful AI features without heavy infrastructure. Yet\nthese integrations bring their own form of self-admitted technical debt (SATD).\nIn this paper, we present the first large-scale empirical study of LLM-specific\nSATD: its origins, prevalence, and mitigation strategies. By analyzing 93,142\nPython files across major LLM APIs, we found that 54.49% of SATD instances stem\nfrom OpenAI integrations and 12.35% from LangChain use. Prompt design emerged\nas the primary source of LLM-specific SATD, with 6.61% of debt related to\nprompt configuration and optimization issues, followed by hyperparameter tuning\nand LLM-framework integration. We further explored which prompt techniques\nattract the most debt, revealing that instruction-based prompts (38.60%) and\nfew-shot prompts (18.13%) are particularly vulnerable due to their dependence\non instruction clarity and example quality. Finally, we release a comprehensive\nSATD dataset to support reproducibility and offer practical guidance for\nmanaging technical debt in LLM-powered systems.", "AI": {"tldr": "The paper presents a large-scale study of technical debt (SATD) tied to integrating Large Language Models (LLMs) in software. Prompt design is the main debt source, with instruction- and few-shot prompts being particularly problematic. Over half the issues stem from OpenAI API usage. The authors provide a dataset and practical guidance to help developers address technical debt in AI-driven systems.", "motivation": "With the increasing adoption of Large Language Models (LLMs) via APIs like OpenAI, there's a growing concern about technical debt specific to LLM integrations, especially due to rapid development cycles, evolving prompt strategies, and complex configurations. Understanding and managing this technical debt is crucial for robust software engineering.", "method": "The authors conducted a large-scale empirical analysis by examining 93,142 Python files that utilize major LLM APIs. They classified and quantified Self-Admitted Technical Debt (SATD) instances, analyzed the origins and types of LLM-specific SATD, and studied which prompt techniques are most susceptible to technical debt.", "result": "54.49% of LLM-specific SATD originated from OpenAI API use and 12.35% from LangChain. The main source of technical debt was prompt design, with 6.61% related to prompt configuration/optimization, and others stemming from hyperparameter tuning and framework integration. Instruction-based prompts (38.6%) and few-shot prompts (18.13%) were found to be most vulnerable to technical debt due to their reliance on clarity and quality. The authors also released a SATD dataset to enable further research.", "conclusion": "LLM integrations introduce distinctive technical debt, primarily from prompt design and configuration. Certain prompt techniques, especially instruction-based and few-shot approaches, attract more debt. Awareness and strategic management of this unique technical debt are necessary for reliable LLM-powered systems."}}
{"id": "2509.20552", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20552", "abs": "https://arxiv.org/abs/2509.20552", "authors": ["Xinyu Shi", "Zhenhao Li", "An Ran Chen"], "title": "Enhancing LLM-based Fault Localization with a Functionality-Aware Retrieval-Augmented Generation Framework", "comment": null, "summary": "Fault localization (FL) is a critical but time-consuming task in software\ndebugging, aiming to identify faulty code elements. While recent advances in\nlarge language models (LLMs) have shown promise for FL, they often struggle\nwith complex systems due to the lack of project-specific knowledge and the\ndifficulty of navigating large projects. To address these limitations, we\npropose FaR-Loc, a novel framework that enhances method-level FL by integrating\nLLMs with retrieval-augmented generation (RAG). FaR-Loc consists of three key\ncomponents: LLM Functionality Extraction, Semantic Dense Retrieval, and LLM\nRe-ranking. First, given a failed test and its associated stack trace, the LLM\nFunctionality Extraction module generates a concise natural language\ndescription that captures the failing behavior. Next, the Semantic Dense\nRetrieval component leverages a pre-trained code-understanding encoder to embed\nboth the functionality description (natural language) and the covered methods\n(code) into a shared semantic space, enabling the retrieval of methods with\nsimilar functional behavior. Finally, the LLM Re-ranking module reorders the\nretrieved methods based on their contextual relevance. Our experiments on the\nwidely used Defects4J benchmark show that FaR-Loc outperforms state-of-the-art\nLLM-based baselines SoapFL and AutoFL, by 14.6% and 9.1% in Top-1 accuracy, by\n19.2% and 22.1% in Top-5 accuracy, respectively. It also surpasses all\nlearning-based and spectrum-based baselines across all Top-N metrics without\nrequiring re-training. Furthermore, we find that pre-trained code embedding\nmodels that incorporate code structure, such as UniXcoder, can significantly\nimprove fault localization performance by up to 49.0% in Top-1 accuracy.\nFinally, we conduct a case study to illustrate the effectiveness of FaR-Loc and\nto provide insights for its practical application.", "AI": {"tldr": "FaR-Loc, a framework combining LLMs and retrieval-augmented generation, markedly improves fault localization in software by efficiently leveraging semantic retrieval and code structure-aware embeddings, surpassing multiple baseline methods without retraining.", "motivation": "Fault localization is crucial but slow and difficult in large, complex software systems. Recent LLMs help but lack project-specific knowledge and struggle with large codebases.", "method": "FaR-Loc framework integrates LLMs with retrieval-augmented generation (RAG) for method-level fault localization. It includes three main components: LLM-based functionality extraction from failed tests, semantic dense retrieval using code-understanding encoders to embed both descriptions and methods in a shared semantic space, and LLM-based re-ranking of retrieved method candidates.", "result": "FaR-Loc significantly outperforms state-of-the-art LLM-based (SoapFL and AutoFL), learning-based, and spectrum-based fault localization methods on the Defects4J benchmark, showing Top-1 and Top-5 accuracy improvements. Using code-structure-aware models (such as UniXcoder) boosts fault localization accuracy by up to 49%.", "conclusion": "Integrating LLMs with retrieval-augmented generation and code structure-aware embeddings leads to substantial improvements in fault localization, making the process more efficient and effective in complex software projects."}}
{"id": "2509.20631", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20631", "abs": "https://arxiv.org/abs/2509.20631", "authors": ["Michael Zhang", "Yuan Tian", "Mariam Guizani"], "title": "Design, Implementation and Evaluation of a Novel Programming Language Topic Classification Workflow", "comment": null, "summary": "As software systems grow in scale and complexity, understanding the\ndistribution of programming language topics within source code becomes\nincreasingly important for guiding technical decisions, improving onboarding,\nand informing tooling and education. This paper presents the design,\nimplementation, and evaluation of a novel programming language topic\nclassification workflow. Our approach combines a multi-label Support Vector\nMachine (SVM) with a sliding window and voting strategy to enable fine-grained\nlocalization of core language concepts such as operator overloading, virtual\nfunctions, inheritance, and templates. Trained on the IBM Project CodeNet\ndataset, our model achieves an average F1 score of 0.90 across topics and 0.75\nin code-topic highlight. Our findings contribute empirical insights and a\nreusable pipeline for researchers and practitioners interested in code analysis\nand data-driven software engineering.", "AI": {"tldr": "The paper introduces a new SVM-based workflow for detecting and localizing programming language concepts in code, achieving strong F1 scores, and presenting a practical tool for software engineering analysis.", "motivation": "As software projects become larger and more complex, understanding how different programming language concepts are distributed in the code helps in making technical decisions, onboarding new developers, and improving tools and educational content.", "method": "The paper develops a workflow that uses a multi-label Support Vector Machine (SVM) model in combination with a sliding window and voting strategy. This approach enables detailed identification and localization of programming language topics within source code. The model is trained using the IBM Project CodeNet dataset.", "result": "The approach achieves an average F1 score of 0.90 for topic classification and 0.75 for detailed, code-topic highlighting, demonstrating good performance.", "conclusion": "The workflow provides empirical insights and a reusable toolchain for code analysis. It is useful for both researchers and practitioners in software engineering who are focused on data-driven analysis."}}
{"id": "2509.20780", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20780", "abs": "https://arxiv.org/abs/2509.20780", "authors": ["Daniela Grassi", "Fabio Calefato", "Darja Smite", "Nicole Novielli", "Filippo Lanubile"], "title": "Exploring Engagement in Hybrid Meetings", "comment": null, "summary": "Background. The widespread adoption of hybrid work following the COVID-19\npandemic has fundamentally transformed software development practices,\nintroducing new challenges in communication and collaboration as organizations\ntransition from traditional office-based structures to flexible working\narrangements. This shift has established a new organizational norm where even\ntraditionally office-first companies now embrace hybrid team structures. While\nremote participation in meetings has become commonplace in this new\nenvironment, it may lead to isolation, alienation, and decreased engagement\namong remote team members. Aims. This study aims to identify and characterize\nengagement patterns in hybrid meetings through objective measurements, focusing\non the differences between co-located and remote participants. Method. We\nstudied professionals from three software companies over several weeks,\nemploying a multimodal approach to measure engagement. Data were collected\nthrough self-reported questionnaires and physiological measurements using\nbiometric devices during hybrid meetings to understand engagement dynamics.\nResults. The regression analyses revealed comparable engagement levels between\nonsite and remote participants, though remote participants show lower\nengagement in long meetings regardless of participation mode. Active roles\npositively correlate with higher engagement, while larger meetings and\nafternoon sessions are associated with lower engagement. Conclusions. Our\nresults offer insights into factors associated with engagement and\ndisengagement in hybrid meetings, as well as potential meeting improvement\nrecommendations. These insights are potentially relevant not only for software\nteams but also for knowledge-intensive organizations across various sectors\nfacing similar hybrid collaboration challenges.", "AI": {"tldr": "This study found that remote and onsite workers have similar engagement in hybrid meetings, except remote workers disengage more in longer meetings. Active participation boosts engagement, while big and late-day meetings reduce it. These results inform better meeting practices for hybrid teams.", "motivation": "The shift to hybrid work after COVID-19 has created new communication and collaboration challenges, especially in software development. There are concerns that remote participation in meetings leads to isolation and disengagement, so understanding engagement in hybrid meetings is crucial.", "method": "A multimodal approach measuring engagement was used with professionals in three software companies. Data collection included self-reported questionnaires and physiological (biometric) data during hybrid meetings over several weeks.", "result": "Regression analyses showed that onsite and remote participants generally had similar engagement levels, but remote participants were less engaged in long meetings. Having an active role improves engagement, while larger meetings and afternoon sessions lower it.", "conclusion": "Insights into engagement and disengagement factors in hybrid meetings were provided, along with recommendations for improvements. These findings are relevant beyond software teams, applicable to other knowledge-intensive organizations dealing with hybrid collaboration."}}
{"id": "2509.20837", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20837", "abs": "https://arxiv.org/abs/2509.20837", "authors": ["Srishti Gureja", "Elena Tommasone", "Jingyi He", "Sara Hooker", "Matthias Gall\u00e9", "Marzieh Fadaee"], "title": "Verification Limits Code LLM Training", "comment": null, "summary": "Large language models for code generation increasingly rely on synthetic\ndata, where both problem solutions and verification tests are generated by\nmodels. While this enables scalable data creation, it introduces a previously\nunexplored bottleneck: the verification ceiling, in which the quality and\ndiversity of training data are fundamentally constrained by the capabilities of\nsynthetic verifiers. In this work, we systematically study how verification\ndesign and strategies influence model performance. We investigate (i) what we\nverify by analyzing the impact of test complexity and quantity: richer test\nsuites improve code generation capabilities (on average +3 pass@1), while\nquantity alone yields diminishing returns, (ii) how we verify by exploring\nrelaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By\nallowing for relaxed thresholds or incorporating LLM-based soft verification,\nwe can recover valuable training data, leading to a 2-4 point improvement in\npass@1 performance. However, this benefit is contingent upon the strength and\ndiversity of the test cases used, and (iii) why verification remains necessary\nthrough controlled comparisons of formally correct versus incorrect solutions\nand human evaluation: retaining diverse correct solutions per problem yields\nconsistent generalization gains. Our results show that Verification as\ncurrently practiced is too rigid, filtering out valuable diversity. But it\ncannot be discarded, only recalibrated. By combining calibrated verification\nwith diverse, challenging problem-solution pairs, we outline a path to break\nthe verification ceiling and unlock stronger code generation models.", "AI": {"tldr": "Rigid automated verification restricts code generation models by filtering out valuable diverse solutions. Strategic recalibration\u2014especially through more varied tests, relaxed criteria, and soft verification\u2014unlocks greater model capability and data diversity, breaking the limits imposed by current practices.", "motivation": "Current code generation models depend heavily on synthetic data, which are validated using automated verifiers. However, the quality and diversity of synthetic training data are limited by the capabilities of these verifiers, creating a 'verification ceiling.' This paper aims to systematically examine how verification strategies influence code generation model performance and to identify ways to improve this process.", "method": "The study analyzes three aspects of verification: 1) the impact of test complexity and quantity on code generation; 2) the effect of relaxing pass thresholds and adopting LLM-based soft verification; and 3) controlled comparisons of correct versus incorrect solutions combined with human evaluation. Experiments are conducted to assess each strategy and its contribution to code model performance.", "result": "Enriched and diverse test suites improve code generation performance (+3 pass@1), while just increasing quantity offers diminishing returns. Relaxed verification thresholds or soft LLM-based verification recover valuable training data and improve pass@1 scores by 2-4 points, but this depends on strong and diverse test cases. Keeping multiple diverse correct solutions leads to better generalization. Rigid verification practices filter out valuable diversity but removing verification entirely is not viable; recalibration is necessary.", "conclusion": "Verification practices for synthetic code data are currently too restrictive, which limits the effectiveness of code generation models. By recalibrating verification strategies (using more diverse and challenging tests, relaxing criteria, and utilizing soft verifiers) and retaining solution diversity, it is possible to break the verification ceiling and significantly improve code model performance."}}
{"id": "2509.20881", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20881", "abs": "https://arxiv.org/abs/2509.20881", "authors": ["Yixuan Li", "Xinyi Liu", "Weidong Yang", "Ben Fei", "Shuhao Li", "Mingjie Zhou", "Lipeng Ma"], "title": "PseudoBridge: Pseudo Code as the Bridge for Better Semantic and Logic Alignment in Code Retrieval", "comment": null, "summary": "Code search aims to precisely find relevant code snippets that match natural\nlanguage queries within massive codebases, playing a vital role in software\ndevelopment. Recent advances leverage pre-trained language models (PLMs) to\nbridge the semantic gap between unstructured natural language (NL) and\nstructured programming languages (PL), yielding significant improvements over\ntraditional information retrieval and early deep learning approaches. However,\nexisting PLM-based methods still encounter key challenges, including a\nfundamental semantic gap between human intent and machine execution logic, as\nwell as limited robustness to diverse code styles. To address these issues, we\npropose PseudoBridge, a novel code retrieval framework that introduces\npseudo-code as an intermediate, semi-structured modality to better align NL\nsemantics with PL logic. Specifically, PseudoBridge consists of two stages.\nFirst, we employ an advanced large language model (LLM) to synthesize\npseudo-code, enabling explicit alignment between NL queries and pseudo-code.\nSecond, we introduce a logic-invariant code style augmentation strategy and\nemploy the LLM to generate stylistically diverse yet logically equivalent code\nimplementations with pseudo-code, then align the code snippets of different\nstyles with pseudo-code, enhancing model robustness to code style variation. We\nbuild PseudoBridge across 10 different PLMs and evaluate it on 6 mainstream\nprogramming languages. Extensive experiments demonstrate that PseudoBridge\nconsistently outperforms baselines, achieving significant gains in retrieval\naccuracy and generalization, particularly under zero-shot domain transfer\nscenarios such as Solidity and XLCoST datasets. These results demonstrate the\neffectiveness of explicit logical alignment via pseudo-code and highlight\nPseudoBridge's potential as a robust, generalizable solution for code\nretrieval.", "AI": {"tldr": "PseudoBridge uses pseudo-code as an intermediate layer between natural language queries and code, boosting code search accuracy and robustness across different languages, styles, and domains.", "motivation": "Code search is crucial for software development but still faces challenges bridging the semantic gap between natural language queries and programming languages, and is sensitive to diverse coding styles.", "method": "PseudoBridge, the proposed framework, introduces pseudo-code as an intermediate step. It uses large language models (LLMs) to synthesize pseudo-code from natural language, then generates logically equivalent but stylistically diverse code to improve robustness. Code retrieval is then performed by aligning NL, pseudo-code, and code across styles.", "result": "Experiments were conducted using 10 different pre-trained language models and tested on 6 programming languages. PseudoBridge outperformed baseline models, particularly in zero-shot transfer to new domains like Solidity and XLCoST.", "conclusion": "PseudoBridge effectively bridges natural language and code using pseudo-code, achieving better retrieval accuracy and robustness to code style variations. It demonstrates strong generalization, suggesting its promise as a solution for robust code search."}}
{"id": "2509.21067", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.21067", "abs": "https://arxiv.org/abs/2509.21067", "authors": ["Oka Kurniawan", "Erick Chandra", "Christopher M. Poskitt", "Yannic Noller", "Kenny Tsu Wei Choo", "Cyrille Jegourel"], "title": "Designing for Novice Debuggers: A Pilot Study on an AI-Assisted Debugging Tool", "comment": "Accepted by the 25th Koli Calling International Conference on\n  Computing Education Research (Koli Calling 2025)", "summary": "Debugging is a fundamental skill that novice programmers must develop.\nNumerous tools have been created to assist novice programmers in this process.\nRecently, large language models (LLMs) have been integrated with automated\nprogram repair techniques to generate fixes for students' buggy code. However,\nmany of these tools foster an over-reliance on AI and do not actively engage\nstudents in the debugging process. In this work, we aim to design an intuitive\ndebugging assistant, CodeHinter, that combines traditional debugging tools with\nLLM-based techniques to help novice debuggers fix semantic errors while\npromoting active engagement in the debugging process. We present findings from\nour second design iteration, which we tested with a group of undergraduate\nstudents. Our results indicate that the students found the tool highly\neffective in resolving semantic errors and significantly easier to use than the\nfirst version. Consistent with our previous study, error localization was the\nmost valuable feature. Finally, we conclude that any AI-assisted debugging tool\nshould be personalized based on user profiles to optimize their interactions\nwith students.", "AI": {"tldr": "This paper introduces CodeHinter, a hybrid debugging assistant for novices that merges traditional methods with LLM-based AI support. Tested with undergraduates, it improved error fixing and usability, especially through its error localization feature. The study emphasizes the need for personalized AI tools to boost learning and engagement in debugging.", "motivation": "Novice programmers struggle with debugging, and AI-based automated code repair tools can lead to passive learning and over-reliance on technology. There is a need for a tool that supports active engagement in debugging while leveraging the advantages of AI.", "method": "The paper proposes an intuitive debugging assistant called CodeHinter, which combines traditional debugging tools with large language model (LLM)-based techniques. The tool is iteratively designed and tested with undergraduate students to evaluate its effectiveness and usability.", "result": "Students found CodeHinter highly effective for fixing semantic errors and easier to use compared to the first version. Error localization remains the most valued feature. The study also highlights the importance of personalizing AI-assisted debugging tools to user profiles.", "conclusion": "AI-assisted debugging tools should be personalized for different user profiles to maximize student engagement and effectiveness. CodeHinter demonstrates that combining traditional and AI techniques can facilitate active learning in debugging."}}
{"id": "2509.21068", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21068", "abs": "https://arxiv.org/abs/2509.21068", "authors": ["Nek Dil Khan", "Javed Ali Khan", "Mobashir Husain", "Muhammad Sohail Khan", "Arif Ali Khan", "Muhammad Azeem Akbar", "Shahid Hussain"], "title": "An Improved Quantum Software Challenges Classification Approach using Transfer Learning and Explainable AI", "comment": null, "summary": "Quantum Software Engineering (QSE) is a research area practiced by tech\nfirms. Quantum developers face challenges in optimizing quantum computing and\nQSE concepts. They use Stack Overflow (SO) to discuss challenges and label\nposts with specialized quantum tags, which often refer to technical aspects\nrather than developer posts. Categorizing questions based on quantum concepts\ncan help identify frequent QSE challenges. We conducted studies to classify\nquestions into various challenges. We extracted 2829 questions from Q&A\nplatforms using quantum-related tags. Posts were analyzed to identify frequent\nchallenges and develop a novel grounded theory. Challenges include Tooling,\nTheoretical, Learning, Conceptual, Errors, and API Usage. Through content\nanalysis and grounded theory, discussions were annotated with common challenges\nto develop a ground truth dataset. ChatGPT validated human annotations and\nresolved disagreements. Fine-tuned transformer algorithms, including BERT,\nDistilBERT, and RoBERTa, classified discussions into common challenges. We\nachieved an average accuracy of 95% with BERT DistilBERT, compared to\nfine-tuned Deep and Machine Learning (D&ML) classifiers, including Feedforward\nNeural Networks (FNN), Convolutional Neural Networks (CNN), and Long Short-Term\nMemory networks (LSTM), which achieved accuracies of 89%, 86%, and 84%,\nrespectively. The Transformer-based approach outperforms the D&ML-based\napproach with a 6\\% increase in accuracy by processing actual discussions,\ni.e., without data augmentation. We applied SHAP (SHapley Additive\nexPlanations) for model interpretability, revealing how linguistic features\ndrive predictions and enhancing transparency in classification. These findings\ncan help quantum vendors and forums better organize discussions for improved\naccess and readability. However,empirical evaluation studies with actual\ndevelopers and vendors are needed.", "AI": {"tldr": "Transformer-based NLP models (BERT, DistilBERT) accurately classify QSE discussion challenges, outperforming traditional ML methods, and SHAP enhances interpretability. Results suggest improved organization of quantum forums, but further empirical studies are needed.", "motivation": "Quantum developers face persistent challenges in optimizing quantum computing and engineering practices, and need better ways to identify and categorize frequent challenges discussed in technical forums for enhanced support and resource organization.", "method": "The study extracted 2829 QSE-related questions from Q&A platforms using quantum tags, performed content and grounded theory analysis for annotation, used ChatGPT for validation, and applied fine-tuned transformer algorithms (BERT, DistilBERT, RoBERTa) for classification. Comparative experiments with deep/machine learning models and SHAP for interpretability were also conducted.", "result": "Transformer-based models achieved a classification accuracy of 95%, outperforming deep/machine learning models (FNN: 89%, CNN: 86%, LSTM: 84%). SHAP analysis improved model interpretability by linking linguistic features to predictions.", "conclusion": "Transformer-based algorithms, specifically BERT and DistilBERT, effectively classify quantum software engineering (QSE) challenges in developer discussions with higher accuracy (95%) compared to traditional deep/machine learning approaches. SHAP further improves model transparency."}}
{"id": "2509.21170", "categories": ["cs.SE", "cs.AI", "D.2.3; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.21170", "abs": "https://arxiv.org/abs/2509.21170", "authors": ["Yongda Yu", "Guohao Shi", "Xianwei Wu", "Haochuan He", "XueMing Gu", "Qianqian Zhao", "Kui Liu", "Qiushi Wang", "Zhao Tian", "Haifeng Shen", "Guoping Rong"], "title": "Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach", "comment": "22 pages", "summary": "Large Language Models (LLMs) have shown great potential in supporting\nautomated code review due to their impressive capabilities in context\nunderstanding and reasoning. However, these capabilities are still limited\ncompared to human-level cognition because they are heavily influenced by the\ntraining data. Recent research has demonstrated significantly improved\nperformance through fine-tuning LLMs with code review data. However, compared\nto human reviewers who often simultaneously analyze multiple dimensions of code\nreview to better identify issues, the full potential of these methods is\nhampered by the limited or vague information used to fine-tune the models. This\npaper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that\ntrains LLMs with an impressive reasoning ability to analyze multiple dimensions\nof code review by harnessing long COT techniques to provide rich structured\ninformation. To address context loss and reasoning logic loss issues that\nfrequently occur when LLMs process long COT prompts, we propose a solution that\ncombines the Maximum Entropy (ME) modeling principle with pre-defined reasoning\npathways in MelcotCR to enable more effective utilization of in-context\nknowledge within long COT prompts while strengthening the logical tightness of\nthe reasoning process. Empirical evaluations on our curated MelcotCR dataset\nand the public CodeReviewer dataset reveal that a low-parameter base model,\nsuch as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art\nmethods in terms of the accuracy of detecting and describing code issues, with\nits performance remarkably on par with that of the 671B DeepSeek-R1 model.", "AI": {"tldr": "MelcotCR improves LLM code review by training models to follow detailed reasoning paths, yielding superior code issue detection even in smaller models. It matches or exceeds larger models' performance by solving context and logic loss in long prompts.", "motivation": "Current LLMs have shown promise in code review automation but lack human-level multidimensional reasoning, mainly due to limited or insufficiently structured fine-tuning data.", "method": "The authors propose MelcotCR, a chain-of-thought fine-tuning approach, which trains LLMs to analyze multiple dimensions of code review using long and structured reasoning pathways. It combines Maximum Entropy modeling with pre-defined reasoning paths to address context and logic losses in long prompts.", "result": "Empirical evaluations show that a low-parameter base model (14B Qwen2.5), fine-tuned with MelcotCR, outperforms state-of-the-art methods and achieves performance close to much larger models (671B DeepSeek-R1).", "conclusion": "MelcotCR enables efficient and accurate code review capabilities for LLMs, even with lower-parameter models, by enhancing multidimensional reasoning through structured chain-of-thought fine-tuning."}}
{"id": "2509.21292", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.21292", "abs": "https://arxiv.org/abs/2509.21292", "authors": ["Ronivaldo Ferreira", "Guilherme da Silva", "Carla Rocha", "Gustavo Pinto"], "title": "Semantic Clustering of Civic Proposals: A Case Study on Brazil's National Participation Platform", "comment": "12 pages, in Portuguese language", "summary": "Promoting participation on digital platforms such as Brasil Participativo has\nemerged as a top priority for governments worldwide. However, due to the sheer\nvolume of contributions, much of this engagement goes underutilized, as\norganizing it presents significant challenges: (1) manual classification is\nunfeasible at scale; (2) expert involvement is required; and (3) alignment with\nofficial taxonomies is necessary. In this paper, we introduce an approach that\ncombines BERTopic with seed words and automatic validation by large language\nmodels. Initial results indicate that the generated topics are coherent and\ninstitutionally aligned, with minimal human effort. This methodology enables\ngovernments to transform large volumes of citizen input into actionable data\nfor public policy.", "AI": {"tldr": "Organizing citizen input on public platforms is hard due to volume and classification needs. This paper proposes a semi-automated method using BERTopic and large language models, yielding coherent, policy-aligned topics with minimal human labor, thus making public input usable for policymaking.", "motivation": "Digital participation platforms generate large amounts of public input, but much of it remains underused because it's difficult to organize at scale, requires expert involvement, and must align with official classifications.", "method": "The authors propose a method that uses BERTopic combined with seed words and automated validation using large language models to classify and structure citizen contributions efficiently.", "result": "Initial results show that the generated topics are coherent, aligned with institutional requirements, and require minimal human intervention.", "conclusion": "Their approach can help governments effectively process and leverage massive citizen input to support public policy decision-making."}}
