<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 13]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Pattern-Based File and Data Access with Python Glob: A Comprehensive Guide for Computational Research](https://arxiv.org/abs/2509.08843)
*Sidney Shapiro*

Main category: cs.SE

TL;DR: This paper positions Python's glob module as a core tool for scalable, reproducible file access in data science and AI research, illustrating its practical benefits with real-world examples.


<details>
  <summary>Details</summary>
Motivation: File pattern matching is fundamental but under-documented; there is a need to clarify and promote best practices for file access in computational research to support reproducibility and scalability.

Method: Concrete Python examples are provided using popular libraries (pandas, scikit-learn, matplotlib) to demonstrate glob's integration and utility in file traversal and analytical pipelines.

Result: Glob significantly streamlines data ingestion, analysis, AI dataset construction, and supports robust and reproducible research through efficient file management.

Conclusion: The glob module is an essential methodological tool for pattern-based file access, facilitating reproducible and scalable workflows in Python-driven research and analytics.

Abstract: Pattern-based file access is a fundamental but often under-documented aspect
of computational research. The Python glob module provides a simple yet
powerful way to search, filter, and ingest files using wildcard patterns,
enabling scalable workflows across disciplines. This paper introduces glob as a
versatile tool for data science, business analytics, and artificial
intelligence applications. We demonstrate use cases including large-scale data
ingestion, organizational data analysis, AI dataset construction, and
reproducible research practices. Through concrete Python examples with widely
used libraries such as pandas,scikit-learn, and matplotlib, we show how glob
facilitates efficient file traversal and integration with analytical pipelines.
By situating glob within the broader context of reproducible research and data
engineering, we highlight its role as a methodological building block. Our goal
is to provide researchers and practitioners with a concise reference that
bridges foundational concepts and applied practice, making glob a default
citation for file pattern matching in Python-based research workflows.

</details>


### [2] [A Systematic Mapping Study on Chatbots in Programming Education](https://arxiv.org/abs/2509.08857)
*Marcelino Garcia,Renato Garcia,Arthur Parizotto,Andre Mendes,Pedro Valle,Ricardo Vilela,Renato Balancieri,Williamson Silva*

Main category: cs.SE

TL;DR: A systematic review of 54 studies shows most educational chatbots for programming teach Python and basic concepts, using diverse teaching methods and technology setups; insights from this review can inform the creation of better programming education tools.


<details>
  <summary>Details</summary>
Motivation: Educational chatbots have become prominent tools to support programming instruction, especially in introductory courses. There is a need to understand how these tools are developed and applied in educational contexts.

Method: The authors conducted a Systematic Mapping Study (SMS) by reviewing an initial set of 3,216 publications, selecting and analyzing 54 studies using five research subquestions related to chatbot types, programming languages, educational content, interaction models, and application contexts.

Result: The study found a dominance of chatbots designed for teaching Python, centered on fundamental programming concepts. It observed a variety of pedagogical approaches and technical architectures used in these agents.

Conclusion: There are clear trends (Python, fundamental concepts, varied approaches) and gaps in the literature regarding educational chatbots for programming, and these findings can guide the future development of instructional tools.

Abstract: Educational chatbots have gained prominence as support tools for teaching
programming, particularly in introductory learning contexts. This paper
presents a Systematic Mapping Study (SMS) that investigated how such agents
have been developed and applied in programming education. From an initial set
of 3,216 publications, 54 studies were selected and analyzed based on five
research subquestions, addressing chatbot types, programming languages used,
educational content covered, interaction models, and application contexts. The
results reveal a predominance of chatbots designed for Python instruction,
focusing on fundamental programming concepts, and employing a wide variety of
pedagogical approaches and technological architectures. In addition to
identifying trends and gaps in the literature, this study provides insights to
inform the development of new educational tools for programming instruction.

</details>


### [3] [GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial Analysis-Function Calling vs Code Generation](https://arxiv.org/abs/2509.08863)
*Qianqian Luo,Liuchang Xu,Qingming Lin,Sensen Wu,Ruichen Mao,Chao Wang,Hailin Feng,Bo Huang,Zhenhong Du*

Main category: cs.SE

TL;DR: This paper presents GeoJSON Agents, a novel multi-agent LLM architecture designed to automate GIS tasks by translating natural language into structured spatial commands. It compares Function Calling and Code Generation, finding that both techniques far outperform general-purpose models in task accuracy. Code Generation is more flexible, while Function Calling is more stable, providing a new path forward for advanced GeoAI systems.


<details>
  <summary>Details</summary>
Motivation: LLMs have advanced in task automation and natural language understanding, but lack expertise in Geographic Information Systems (GIS), limiting their effectiveness for GIS-specific tasks. Addressing this gap is crucial for expanding LLM capabilities to spatial domains.

Method: The paper proposes GeoJSON Agents, a multi-agent LLM architecture that transforms natural language tasks into structured GeoJSON operation commands. It utilizes two enhancement techniques: Function Calling and Code Generation. The system features three components: task parsing, agent collaboration (with Planner and Worker agents), and result integration. Spatial data is processed by either calling predefined functions or dynamically generating Python code. The outputs are standardized into GeoJSON files. The methods are systematically benchmarked using 70 GIS tasks with GPT-4o as the core model.

Result: Function Calling-based GeoJSON Agents achieved 85.71% accuracy; Code Generation-based Agents reached 97.14% accuracyâ€”both substantially higher than general-purpose models (48.57%). Code Generation provides more flexibility, while Function Calling offers greater stability.

Conclusion: The study introduces the first LLM multi-agent framework for GeoJSON data, demonstrating the superior performance of specialized enhancement techniques in GIS automation. It sheds light on the respective strengths and weaknesses of Function Calling and Code Generation, offering new insights for future GeoAI system improvements.

Abstract: LLMs have made substantial progress in task automation and natural language
understanding.However,without expertise in GIS,they continue to encounter
limitations.To address these issues, we propose GeoJSON Agents-a multi-agent
LLM architecture.This framework transforms natural language tasks into
structured GeoJSON operation commands and processes spatial data using two
widely adopted LLM enhancement techniques:Function Calling and Code
Generation.The architecture consists of three components-task parsing,agent
collaboration,and result integration-aimed at enhancing both the performance
and scalability of GIS automation.The Planner agent interprets natural language
tasks into structured GeoJSON commands.Then,specialized Worker agents
collaborate according to assigned roles to perform spatial data processing and
analysis,either by invoking predefined function APIs or by dynamically
generating and executing Python-based spatial analysis code.Finally,the system
integrates the outputs from multiple execution rounds into
reusable,standards-compliant GeoJSON files.To systematically evaluate the
performance of the two approaches,we constructed a benchmark dataset of 70
tasks with varying complexity and conducted experiments using OpenAI's GPT-4o
as the core model.Results indicate that the Function Calling-based GeoJSON
Agent achieved an accuracy of 85.71%,while the Code Generation-based agent
reached 97.14%,both significantly outperforming the best-performing
general-purpose model (48.57%).Further analysis reveals that the Code
Generation provides greater flexibility,whereas the Function Calling approach
offers more stable execution.This study is the first to introduce an LLM
multi-agent framework for GeoJSON data and to compare the strengths and
limitations of two mainstream LLM enhancement methods,offering new perspectives
for improving GeoAI system performance.

</details>


### [4] [TraceRAG: A LLM-Based Framework for Explainable Android Malware Detection and Behavior Analysis](https://arxiv.org/abs/2509.08865)
*Guangyu Zhang,Xixuan Wang,Shiyu Sun,Peiyan Xiao,Kun Sun,Yanhai Xiong*

Main category: cs.SE

TL;DR: TraceRAG is a new framework leveraging large language models to improve Android malware detection. It links natural language questions to code, achieves high detection and behavior identification accuracy, and generates expert-validated, human-readable reports.


<details>
  <summary>Details</summary>
Motivation: Malicious Android applications use advanced evasion tactics and complex behaviors to hide their malicious logic within legitimate functions, making detection challenging. Existing analysis tools often cannot uncover deeply hidden behaviors or provide clear human-readable explanations for their findings.

Method: TraceRAG, a framework inspired by large language models, utilizes retrieval-augmented generation (RAG) to link natural language queries with Java code. It first summarizes method-level code, stores these summaries in a vector database, retrieves relevant snippets in response to behavior-driven queries, and finally generates detailed, readable reports about malicious behaviors and their implementation.

Result: TraceRAG achieves 96% malware detection accuracy and 83.81% behavior identification accuracy, validated by both VirusTotal scans and manual verification. Experts also found its reports to be effective and practical.

Conclusion: TraceRAG offers a robust solution for Android malware detection and explanation by bridging natural language and code. It not only identifies deeply hidden malicious behaviors with high accuracy but also provides clear, actionable reports for human analysts.

Abstract: Sophisticated evasion tactics in malicious Android applications, combined
with their intricate behavioral semantics, enable attackers to conceal
malicious logic within legitimate functions, underscoring the critical need for
robust and in-depth analysis frameworks. However, traditional analysis
techniques often fail to recover deeply hidden behaviors or provide
human-readable justifications for their decisions. Inspired by advances in
large language models (LLMs), we introduce TraceRAG, a retrieval-augmented
generation (RAG) framework that bridges natural language queries and Java code
to deliver explainable malware detection and analysis. First, TraceRAG
generates summaries of method-level code snippets, which are indexed in a
vector database. At query time, behavior-focused questions retrieve the most
semantically relevant snippets for deeper inspection. Finally, based on the
multi-turn analysis results, TraceRAG produces human-readable reports that
present the identified malicious behaviors and their corresponding code
implementations. Experimental results demonstrate that our method achieves 96\%
malware detection accuracy and 83.81\% behavior identification accuracy based
on updated VirusTotal (VT) scans and manual verification. Furthermore, expert
evaluation confirms the practical utility of the reports generated by TraceRAG.

</details>


### [5] [Benchmarking Energy Efficiency of Large Language Models Using vLLM](https://arxiv.org/abs/2509.08867)
*K. Pronk,Q. Zhao*

Main category: cs.SE

TL;DR: The paper presents a new benchmark that measures LLM energy efficiency under realistic use conditions, helping developers create more sustainable AI solutions.


<details>
  <summary>Details</summary>
Motivation: Rising utilization of Large Language Models (LLMs) is increasing their climate impact due to high energy consumption. Current energy efficiency benchmarks for LLMs do not accurately simulate real-world production environments, necessitating better information for developers.

Method: The paper introduces the LLM Efficiency Benchmark, which is designed to simulate realistic deployment scenarios. It uses vLLM, a highly efficient LLM serving backend, to evaluate the impact of model size, architecture, and concurrent request volume on inference energy efficiency.

Result: The study shows that it is feasible to develop more realistic energy efficiency benchmarks for LLMs. These benchmarks reveal how various factors influence energy efficiency in practical conditions, offering actionable insights for optimizing sustainability.

Conclusion: By adopting benchmarks that mirror production conditions, developers can gain more relevant data on LLM energy usage, ultimately advancing the development of more sustainable AI systems.

Abstract: The prevalence of Large Language Models (LLMs) is having an growing impact on
the climate due to the substantial energy required for their deployment and
use. To create awareness for developers who are implementing LLMs in their
products, there is a strong need to collect more information about the energy
efficiency of LLMs. While existing research has evaluated the energy efficiency
of various models, these benchmarks often fall short of representing realistic
production scenarios. In this paper, we introduce the LLM Efficiency Benchmark,
designed to simulate real-world usage conditions. Our benchmark utilizes vLLM,
a high-throughput, production-ready LLM serving backend that optimizes model
performance and efficiency. We examine how factors such as model size,
architecture, and concurrent request volume affect inference energy efficiency.
Our findings demonstrate that it is possible to create energy efficiency
benchmarks that better reflect practical deployment conditions, providing
valuable insights for developers aiming to build more sustainable AI systems.

</details>


### [6] [CLARA: A Developer's Companion for Code Comprehension and Analysis](https://arxiv.org/abs/2509.09072)
*Ahmed Adnan,Mushfiqur Rahman,Saad Sakib Noor,Kazi Sakib*

Main category: cs.SE

TL;DR: CLARA is a browser extension helping developers and researchers effectively understand and analyze open-source code with minimal setup. It was found useful and accurate in both dataset-based evaluation and user studies.


<details>
  <summary>Details</summary>
Motivation: Developers and researchers frequently need to comprehend and analyze code in open-source projects, but existing tools are cumbersome, requiring setup, lacking contextual understanding, and demanding manual effort.

Method: CLARA, a browser extension, leverages a state-of-the-art inference model to provide code comprehension, refactoring support, and code quality attribute detection. Its performance was evaluated qualitatively using existing datasets and a user study involving 10 participants.

Result: CLARA was found to be useful, accurate, and practical for code comprehension and analysis tasks according to the qualitative evaluation and user study.

Conclusion: CLARA is an effective, open-source browser extension that assists developers and researchers in code comprehension and analysis without requiring complex setup, making the process more efficient.

Abstract: Code comprehension and analysis of open-source project codebases is a task
frequently performed by developers and researchers. However, existing tools
that practitioners use for assistance with such tasks often require prior
project setup, lack context-awareness, and involve significant manual effort.
To address this, we present CLARA, a browser extension that utilizes a
state-of-the-art inference model to assist developers and researchers in: (i)
comprehending code files and code fragments, (ii) code refactoring, and (iii)
code quality attribute detection. We qualitatively evaluated CLARA's inference
model using existing datasets and methodology, and performed a comprehensive
user study with 10 developers and academic researchers to assess its usability
and usefulness. The results show that CLARA is useful, accurate, and practical
in code comprehension and analysis tasks. CLARA is an open-source tool
available at https://github.com/SaadNoor555/CLARA_tool_demo. A video showing
the full capabilities of CLARA can be found at
https://youtu.be/VDKVXvIH41Q?si=qBFsmS_Y4m_9x3YH.

</details>


### [7] [Probing Pre-trained Language Models on Code Changes: Insights from ReDef, a High-Confidence Just-in-Time Defect Prediction Dataset](https://arxiv.org/abs/2509.09192)
*Doha Nam,Taehyoun Kim,Duksan Ryu,Jongmoon Baik*

Main category: cs.SE

TL;DR: The paper introduces a high-confidence, function-level code modification dataset for defect prediction and shows that while language models excel with certain input encodings, they do not fundamentally understand code changes as intended.


<details>
  <summary>Details</summary>
Motivation: Existing JIT software defect prediction datasets are noisy and imprecise, making it hard to accurately evaluate or improve models. The study aims to provide better defect labels and understand how pre-trained language models reason about code changes.

Method: The authors built a new dataset (ReDef) with high-confidence labels using revert commits for defects, history checks for clean modifications, and GPT-assisted triage to filter ambiguity. They then systematically evaluated various pre-trained language models under multiple input encodings, including robustness tests with counterfactual edits.

Result: A reliable dataset of 3,164 defective and 10,268 clean code modifications was created. PLMs performed best with diff-style encodings, but counterfactual tests revealed that models relied on superficial cues rather than deep semantic understanding of code changes.

Conclusion: While the new dataset improves defect prediction research, current PLMs do not truly comprehend code modifications; their robustness is more apparent than real, based on shallow patterns rather than actual edit semantics.

Abstract: Just-in-Time software defect prediction (JIT-SDP) plays a critical role in
prioritizing risky code changes during code review and continuous integration.
However, existing datasets often suffer from noisy labels and low precision in
identifying bug-inducing commits. To address this, we present ReDef
(Revert-based Defect dataset), a high-confidence benchmark of function-level
modifications curated from 22 large-scale C/C++ projects. Defective cases are
anchored by revert commits, while clean cases are validated through post-hoc
history checks. Ambiguous instances are conservatively filtered out via a
GPT-assisted triage process involving multiple votes and audits. This pipeline
yields 3,164 defective and 10,268 clean modifications, offering substantially
more reliable labels than prior existing resources. Beyond dataset
construction, we provide the first systematic evaluation of how pre-trained
language models (PLMs) reason about code modifications -- specifically, which
input encodings most effectively expose change information, and whether models
genuinely capture edit semantics. We fine-tune CodeBERT, CodeT5+, and UniXcoder
under five encoding strategies, and further probe their sensitivity through
counterfactual perturbations that swap added/deleted blocks, invert diff
polarity, or inject spurious markers. Our results show that compact diff-style
encodings consistently outperform whole-function formats across all PLMs, with
statistical tests confirming large, model-independent effects. However, under
counterfactual tests, performance degrades little or not at all -- revealing
that what appears to be robustness in fact reflects reliance on superficial
cues rather than true semantic understanding. These findings indicate that,
unlike in snapshot-based tasks, current PLMs remain limited in their ability to
genuinely comprehend code modifications.

</details>


### [8] [On Integrating Large Language Models and Scenario-Based Programming for Improving Software Reliability](https://arxiv.org/abs/2509.09194)
*Ayelet Berzack,Guy Katz*

Main category: cs.SE

TL;DR: LLMs can speed up software development but are error-prone. Combining LLMs with Scenario-Based Programming enables developers to control, verify, and improve LLM-generated code. In a Connect4 case study, this method produced a strong, verifiable agent and demonstrated practical benefits for developers.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) offer potential in software development but also introduce significant errors, thus requiring more reliable methodologies for their use.

Method: The paper proposes combining LLMs with traditional software engineering techniques, specifically using the Scenario-Based Programming (SBP) paradigm to help developers leverage LLMs while verifying program correctness and reducing errors.

Result: A case study was conducted, developing the Connect4 game by integrating LLMs and SBP. The resulting agent outperformed existing agents and, in some cases, its correctness was formally verified. The methodology showed promising ease-of-use.

Conclusion: Integrating LLMs with SBP in a structured process enhances reliability, streamlines development, and allows for verification of software properties, making it a promising approach for safely deploying LLMs in software engineering.

Abstract: Large Language Models (LLMs) are fast becoming indispensable tools for
software developers, assisting or even partnering with them in crafting complex
programs. The advantages are evident -- LLMs can significantly reduce
development time, generate well-organized and comprehensible code, and
occasionally suggest innovative ideas that developers might not conceive on
their own. However, despite their strengths, LLMs will often introduce
significant errors and present incorrect code with persuasive confidence,
potentially misleading developers into accepting flawed solutions.
  In order to bring LLMs into the software development cycle in a more reliable
manner, we propose a methodology for combining them with ``traditional''
software engineering techniques in a structured way, with the goal of
streamlining the development process, reducing errors, and enabling users to
verify crucial program properties with increased confidence. Specifically, we
focus on the Scenario-Based Programming (SBP) paradigm -- an event-driven,
scenario-based approach for software engineering -- to allow human developers
to pour their expert knowledge into the LLM, as well as to inspect and verify
its outputs.
  To evaluate our methodology, we conducted a significant case study, and used
it to design and implement the Connect4 game. By combining LLMs and SBP we were
able to create a highly-capable agent, which could defeat various strong
existing agents. Further, in some cases, we were able to formally verify the
correctness of our agent. Finally, our experience reveals interesting insights
regarding the ease-of-use of our proposed approach. The full code of our
case-study will be made publicly available with the final version of this
paper.

</details>


### [9] [Altered Histories in Version Control System Repositories: Evidence from the Trenches](https://arxiv.org/abs/2509.09294)
*Solal Rapaport,Laurent Pautet,Samuel Tardieu,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: Altering history in public Git repositories is widespread and poses risks to users, project governance, and security. This paper presents the first extensive analysis of such alterations, categorizes their types and impacts, and introduces GitHistorianâ€”a tool to automatically detect and describe history changes in public repositories.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to understand and address the risks associated with history alterations in public Git repositories. Such alterations can disrupt workflows, compromise integrity and reproducibility, and facilitate supply chain attacks, especially when performed on branches used by downstream users.

Method: The authors conduct a large-scale empirical study by analyzing 111 million repositories archived by Software Heritage, which preserves version control histories even after alterations. They identify and categorize changes, and perform two targeted case studies to assess the impact of specific types of history alterations. Additionally, they develop an automated tool called GitHistorian to detect history alterations.

Result: The study reveals that 1.22 million repositories had altered histories, amounting to 8.7 million instances of history rewriting. The alterations include both changes to files and commit metadata, often involving retroactive license changes or removal of sensitive data such as private keys. These practices are considered bad for project governance and security management.

Conclusion: History alterations in public Git repositories are prevalent and potentially dangerous, frequently involving problematic practices that downstream users and software recipients may wish to detect and avoid. The GitHistorian tool provides an automated solution for identifying and describing such alterations, contributing to improved project governance and security.

Abstract: Version Control Systems (VCS) like Git allow developers to locally rewrite
recorded history, e.g., to reorder and suppress commits or specific data in
them. These alterations have legitimate use cases, but become problematic when
performed on public branches that have downstream users: they break push/pull
workflows, challenge the integrity and reproducibility of repositories, and
create opportunities for supply chain attackers to sneak into them nefarious
changes. We conduct the first large-scale investigation of Git history
alterations in public code repositories. We analyze 111 M (millions)
repositories archived by Software Heritage, which preserves VCS histories even
across alterations. We find history alterations in 1.22 M repositories, for a
total of 8.7 M rewritten histories. We categorize changes by where they happen
(which repositories, which branches) and what is changed in them (files or
commit metadata). Conducting two targeted case studies we show that altered
histories recurrently change licenses retroactively, or are used to remove
''secrets'' (e.g., private keys) committed by mistake. As these behaviors
correspond to bad practices-in terms of project governance or security
management, respectively-that software recipients might want to avoid, we
introduce GitHistorian, an automated tool, that developers can use to spot and
describe history alterations in public Git repositories.

</details>


### [10] [Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on Open & Industry Data](https://arxiv.org/abs/2509.09313)
*Moritz Mock,Thomas Forrer,Barbara Russo*

Main category: cs.SE

TL;DR: This paper evaluates and adapts deep learning-based vulnerability detection tools for industry, finding that with fine-tuning and integration strategies like undersampling and CI/CD workflows, performance and developer acceptance can be significantly improved.


<details>
  <summary>Details</summary>
Motivation: There is a gap between academic research and industrial adoption of deep learning-based vulnerability detection due to challenges in trustworthiness, legacy systems, workflow integration, and expertise disparities.

Method: Performance evaluation of CodeBERT for vulnerability detection in both industrial and open-source software, cross-domain generalization analysis, handling class imbalance, and development of a CI/CD-integrated recommender system (AI-DO) with practical survey assessment.

Result: Industrial-trained models perform well within the same domain but struggle with open-source code. Open data fine-tuning with undersampling enhances vulnerability detection. The AI-DO tool is perceived as useful by IT professionals in the company.

Conclusion: Effective cross-domain vulnerability detection by deep learning models requires specific training and data strategies. CI/CD integration via practical tools like AI-DO can bridge the research-to-industry gap and gain positive user reception.

Abstract: Deep learning solutions for vulnerability detection proposed in academic
research are not always accessible to developers, and their applicability in
industrial settings is rarely addressed. Transferring such technologies from
academia to industry presents challenges related to trustworthiness, legacy
systems, limited digital literacy, and the gap between academic and industrial
expertise. For deep learning in particular, performance and integration into
existing workflows are additional concerns. In this work, we first evaluate the
performance of CodeBERT for detecting vulnerable functions in industrial and
open-source software. We analyse its cross-domain generalisation when
fine-tuned on open-source data and tested on industrial data, and vice versa,
also exploring strategies for handling class imbalance. Based on these results,
we develop AI-DO(Automating vulnerability detection Integration for Developers'
Operations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated
recommender system that uses fine-tuned CodeBERT to detect and localise
vulnerabilities during code review without disrupting workflows. Finally, we
assess the tool's perceived usefulness through a survey with the company's IT
professionals. Our results show that models trained on industrial data detect
vulnerabilities accurately within the same domain but lose performance on
open-source code, while a deep learner fine-tuned on open data, with
appropriate undersampling techniques, improves the detection of
vulnerabilities.

</details>


### [11] [ORCA: Unveiling Obscure Containers In The Wild](https://arxiv.org/abs/2509.09322)
*Jacopo Bufalino,Agathe Blaise,Stefano Secci*

Main category: cs.SE

TL;DR: Open-source and cloud SCA tools often miss obscure files in container images, leading to security gaps. This paper introduces ORCA, a tool achieving 40% better file coverage on challenging containers, improving reliability and security analysis.


<details>
  <summary>Details</summary>
Motivation: Modern software development relies heavily on open-source libraries and containers, which introduces security risks when vulnerable or outdated components are unintentionally deployed. Existing Software Composition Analysis (SCA) tools struggle to reliably analyze containers with obscure or incompletely modified filesystems.

Method: The paper conducts an empirical analysis of 600 popular container images to evaluate the effectiveness of cloud-based and open-source SCA tools on obscure container images. It then proposes a new methodology and introduces ORCA, an open-source tool designed to be resilient to filesystem obscuration issues in containers.

Result: The study finds that obscure containers are prevalent even among trusted registries, and many existing SCA tools fail to fully analyze them. ORCA demonstrates superior performance, achieving a median 40% improvement in file coverage compared to established tools like Docker Scout and Syft.

Conclusion: Obscure container images pose a significant challenge to the reliability of current SCA tools. The proposed ORCA methodology and tool offer a practical solution by improving the detection and analysis of such containers, thereby enhancing security and dependency management.

Abstract: Modern software development increasingly depends on open-source libraries and
third-party components, which are often encapsulated into containerized
environments. While improving the development and deployment of applications,
this approach introduces security risks, particularly when outdated or
vulnerable components are inadvertently included in production environments.
Software Composition Analysis (SCA) is a critical process that helps identify
and manage packages and dependencies inside a container. However, unintentional
modifications to the container filesystem can lead to incomplete container
images, which compromise the reliability of SCA tools. In this paper, we
examine the limitations of both cloud-based and open-source SCA tools when
faced with such obscure images. An analysis of 600 popular containers revealed
that obscure containers exist in well-known registries and trusted images and
that many tools fail to analyze such containers. To mitigate these issues, we
propose an obscuration-resilient methodology for container analysis and
introduce ORCA (Obscuration-Resilient Container Analyzer), its open-source
implementation. We reported our findings to all vendors using their appropriate
channels. Our results demonstrate that ORCA effectively detects the content of
obscure containers and achieves a median 40% improvement in file coverage
compared to Docker Scout and Syft.

</details>


### [12] [LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering](https://arxiv.org/abs/2509.09614)
*Jielin Qiu,Zuxin Liu,Zhiwei Liu,Rithesh Murthy,Jianguo Zhang,Haolin Chen,Shiyu Wang,Ming Zhu,Liangwei Yang,Juntao Tan,Zhepeng Cen,Cheng Qian,Shelby Heinecke,Weiran Yao,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.SE

TL;DR: LoCoBench is a new benchmark tailored for evaluating large language models' capabilities in understanding and reasoning about entire codebases and complex software systems with long contexts. Initial tests show that even advanced LLMs perform poorly on these challenging, realistic software development tasks, revealing a significant area for improvement in long-context model understanding.


<details>
  <summary>Details</summary>
Motivation: Current code evaluation benchmarks for language models primarily focus on short-context tasks or single-function completions, failing to assess models' abilities in handling entire codebases or reasoning across multiple files. The emergence of long-context LLMs with much larger context windows creates a need for proper evaluation of their abilities in realistic, large-scale software development scenarios.

Method: The authors propose LoCoBench, a benchmark specifically designed for evaluating long-context LLMs. LoCoBench includes 8,000 evaluation scenarios across 10 programming languages, with context lengths ranging from 10,000 to 1,000,000 tokens. It covers 8 essential long-context task categories and uses a 5-phase pipeline to generate high-quality test cases. The evaluation framework introduces 17 metrics across 4 dimensions, including 8 new metrics, and produces an aggregate LoCoBench Score (LCBS).

Result: Evaluation of state-of-the-art long-context models with LoCoBench shows significant performance gaps, indicating that these models still struggle with complex, large-scale software development tasks and that long-context understanding remains an unsolved challenge in the field.

Conclusion: LoCoBench fills a critical gap by providing a comprehensive, realistic benchmark for assessing the long-context abilities of language models in software development. Early results highlight that advancing LLMsâ€™ performance on long-context tasks is a major open research problem.

Abstract: The emergence of long-context language models with context windows extending
to millions of tokens has created new opportunities for sophisticated code
understanding and software development evaluation. We propose LoCoBench, a
comprehensive benchmark specifically designed to evaluate long-context LLMs in
realistic, complex software development scenarios. Unlike existing code
evaluation benchmarks that focus on single-function completion or short-context
tasks, LoCoBench addresses the critical evaluation gap for long-context
capabilities that require understanding entire codebases, reasoning across
multiple files, and maintaining architectural consistency across large-scale
software systems. Our benchmark provides 8,000 evaluation scenarios
systematically generated across 10 programming languages, with context lengths
spanning 10K to 1M tokens, a 100x variation that enables precise assessment of
long-context performance degradation in realistic software development
settings. LoCoBench introduces 8 task categories that capture essential
long-context capabilities: architectural understanding, cross-file refactoring,
multi-session development, bug investigation, feature implementation, code
comprehension, integration testing, and security analysis. Through a 5-phase
pipeline, we create diverse, high-quality scenarios that challenge LLMs to
reason about complex codebases at unprecedented scale. We introduce a
comprehensive evaluation framework with 17 metrics across 4 dimensions,
including 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our
evaluation of state-of-the-art long-context models reveals substantial
performance gaps, demonstrating that long-context understanding in complex
software development represents a significant unsolved challenge that demands
more attention. LoCoBench is released at:
https://github.com/SalesforceAIResearch/LoCoBench.

</details>


### [13] [I Know Who Clones Your Code: Interpretable Smart Contract Similarity Detection](https://arxiv.org/abs/2509.09630)
*Zhenguang Liu,Lixun Ma,Zhongzheng Mu,Chengkun Wei,Xiaojun Xu,Yingying Jiao,Kui Ren*

Main category: cs.SE

TL;DR: SmartDetector is a novel tool for detecting similar smart contract functions at a fine-grained level. It breaks down function code into smaller trees for better structural analysis and uses an efficient hyperparameter optimization method. The approach outperforms previous methods, achieving an F1-score of 95.88% and a 14% improvement over existing solutions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of bug propagation in smart contracts due to the widespread reuse of open-source code and to improve upon limitations of existing methods for detecting similar smart contract functions, particularly with respect to handling complex syntax structures and achieving interpretability.

Method: The proposed method, SmartDetector, decomposes the abstract syntax tree (AST) of a smart contract function into smaller statement trees to capture structural code elements. It applies a classifier to compute similarity scores between function pairs, comparing their respective statement trees. The method incorporates a mathematically derived cosine-wise diffusion process to efficiently search for optimal classifier hyperparameters.

Result: SmartDetector demonstrates superior performance, outperforming state-of-the-art approaches with an average F1-score improvement of 14.01%, and achieves an overall average F1-score of 95.88% on three large real-world datasets.

Conclusion: SmartDetector effectively enhances similar function detection in smart contracts by enabling fine-grained, interpretable comparisons and overcoming the limitations of AST and deep learning-based existing methods.

Abstract: Widespread reuse of open-source code in smart contract development boosts
programming efficiency but significantly amplifies bug propagation across
contracts, while dedicated methods for detecting similar smart contract
functions remain very limited. Conventional abstract-syntax-tree (AST) based
methods for smart contract similarity detection face challenges in handling
intricate tree structures, which impedes detailed semantic comparison of code.
Recent deep-learning based approaches tend to overlook code syntax and
detection interpretability, resulting in suboptimal performance.
  To fill this research gap, we introduce SmartDetector, a novel approach for
computing similarity between smart contract functions, explainable at the
fine-grained statement level. Technically, SmartDetector decomposes the AST of
a smart contract function into a series of smaller statement trees, each
reflecting a structural element of the source code. Then, SmartDetector uses a
classifier to compute the similarity score of two functions by comparing each
pair of their statement trees. To address the infinite hyperparameter space of
the classifier, we mathematically derive a cosine-wise diffusion process to
efficiently search optimal hyperparameters. Extensive experiments conducted on
three large real-world datasets demonstrate that SmartDetector outperforms
current state-of-the-art methods by an average improvement of 14.01% in
F1-score, achieving an overall average F1-score of 95.88%.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [14] [Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs](https://arxiv.org/abs/2509.09019)
*Mohit Tekriwal,John Sarracino*

Main category: cs.PL

TL;DR: This paper demonstrates how formal verification can ensure the correctness of compiler-level floating-point optimizations, specifically the Fused-Multiply-Add operation, using the Verified LLVM framework and Rocq theorem prover, with potential for further generalization.


<details>
  <summary>Details</summary>
Motivation: Compiler optimizations, especially floating-point optimizations, are widely used in scientific computing for performance gains, but their correctness must be guaranteed to maintain result integrity.

Method: The study uses the Verified LLVM framework within the Rocq theorem prover to formally verify the correctness of Fused-Multiply-Add (FMA) optimization at the LLVM IR level for the expression a * b + c.

Result: The preliminary verification shows that using Rocq's theorem-proving capabilities, the correctness of FMA optimization for a basic arithmetic block can be formally proven. The paper also suggests expanding the approach to other fast math optimizations and program features.

Conclusion: Formal verification using theorem provers like Rocq with Verified LLVM shows promise in ensuring the correctness of floating-point optimizations, particularly FMA, and the methodology can be extended to broader cases.

Abstract: Scientific computing programs often undergo aggressive compiler optimization
to achieve high performance and efficient resource utilization. While
performance is critical, we also need to ensure that these optimizations are
correct. In this paper, we focus on a specific class of optimizations,
floating-point optimizations, notably due to fast math, at the LLVM IR level.
We present a preliminary work, which leverages the Verified LLVM framework in
the Rocq theorem prover, to prove the correctness of Fused-Multiply-Add (FMA)
optimization for a basic block implementing the arithmetic expression $a * b +
c$ . We then propose ways to extend this preliminary results by adding more
program features and fast math floating-point optimizations.

</details>


### [15] [Dependent-Type-Preserving Memory Allocation](https://arxiv.org/abs/2509.09059)
*Paulette Koronkevich,William J. Bowman*

Main category: cs.PL

TL;DR: Dependently typed languages lose their guarantees when linking with external programs after compilation. This work introduces a typed intermediate language and type-preserving compiler pass that support memory safety and maintain dependent types, thereby helping prevent violations of program specifications during linking.


<details>
  <summary>Details</summary>
Motivation: Dependently typed languages enable strong program specifications and proofs, but these guarantees can be violated after compilation, especially through interactions with external programs. Specifically, dependencies on erased types during compilation and unsafe linking can undermine type-safety.

Method: The authors propose the use of type-preserving compilation. They develop a typed intermediate language that supports dependent memory allocation and introduce a compiler pass that preserves dependent types related to memory allocation, enabling type checking even during linking.

Result: The paper provides an ongoing development of a type system and compiler pass that maintains dependent types throughout compilation, aiming to prevent specification violations caused by interaction with ill-typed external programs.

Conclusion: Type-preserving compilation, along with a typed intermediate language, can help enforce program specifications even when linking with external programs, by ensuring ill-typed linkages are ruled out.

Abstract: Dependently typed programming languages such as Coq, Agda, Idris, and F*,
allow programmers to write detailed specifications of their programs and prove
their programs meet these specifications. However, these specifications can be
violated during compilation since they are erased after type checking. External
programs linked with the compiled program can violate the specifications of the
original program and change the behavior of the compiled program -- even when
compiled with a verified compiler. For example, since Coq does not allow
explicitly allocating memory, a programmer might link their Coq program with a
C program that can allocate memory. Even if the Coq program is compiled with a
verified compiler, the external C program can still violate the memory-safe
specification of the Coq program by providing an uninitialized pointer to
memory. This error could be ruled out by type checking in a language expressive
enough to indicate whether memory is initialized versus uninitialized. Linking
with a program with an uninitialized pointer could be considered ill-typed, and
our linking process could prevent linking with ill-typed programs. To
facilitate type checking during linking, we can use type-preserving
compilation, which preserves the types through the compilation process. In this
ongoing work, we develop a typed intermediate language that supports dependent
memory allocation, as well as a dependent-type-preserving compiler pass for
memory allocation.

</details>
