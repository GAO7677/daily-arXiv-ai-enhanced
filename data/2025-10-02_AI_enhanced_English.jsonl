{"id": "2510.00002", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00002", "abs": "https://arxiv.org/abs/2510.00002", "authors": ["Dong Liu"], "title": "PBFD and PDFD: Formally Defined and Verified Methodologies and Empirical Evaluation for Scalable Full-Stack Software Engineering", "comment": "184 pages; 35 figures; A DOI-linked version of this paper and all\n  supplementary materials are available on Zenodo at\n  https://zenodo.org/records/16883985", "summary": "This paper introduces Primary Breadth-First Development (PBFD) and Primary\nDepth-First Development (PDFD), two formally defined and verified methodologies\nfor scalable, industrial-grade full-stack software engineering. These\napproaches bridge a longstanding gap between formal methods and real-world\ndevelopment practice by enforcing structural correctness through\ngraph-theoretic modeling. Unlike prior graph-based approaches, PBFD and PDFD\noperate over layered directed graphs and are formalized using unified state\nmachines and Communicating Sequential Processes (CSP) to ensure critical\nproperties, including bounded-refinement termination and structural\ncompleteness. To coordinate hierarchical data at scale, we propose Three-Level\nEncapsulation (TLE) - a novel, bitmask-based encoding scheme that delivers\nprovably constant-time updates. TLE's formal guarantees underpin PBFD's\nindustrial-scale performance and scalability. PBFD was empirically validated\nthrough an eight-year enterprise deployment, demonstrating over 20x faster\ndevelopment than Salesforce OmniScript and 7-8x faster query performance\ncompared to conventional relational models. Additionally, both methodologies\nare supported by open-source MVPs, with PDFD's implementation conclusively\ndemonstrating its correctness-first design principles. Together, PBFD and PDFD\nestablish a reproducible, transparent framework that integrates formal\nverification into practical software development. All formal specifications,\nMVPs, and datasets are publicly available to foster academic research and\nindustrial-grade adoption.", "AI": {"tldr": "This paper presents two formally verified software development methodologies, PBFD and PDFD, that use advanced graph and state machine models to enforce structural correctness and scalability in industrial settings. With the novel Three-Level Encapsulation for fast hierarchical data management, PBFD has shown dramatic performance improvements in real-world enterprise deployment. Both approaches are supported by public tools and datasets to enable transparent academic and commercial use.", "motivation": "There is a longstanding disconnect between formal methods and practical, scalable software engineering in industry. Existing graph-based methodologies lack rigorous enforcement of correctness in real-world scenarios and do not scale efficiently.", "method": "The paper introduces Primary Breadth-First Development (PBFD) and Primary Depth-First Development (PDFD), two methodologies for full-stack software engineering, using layered directed graphs, unified state machines, and Communicating Sequential Processes (CSP) for formal modeling. Additionally, it proposes Three-Level Encapsulation (TLE), a bitmask-based encoding scheme for constant-time hierarchical data updates.", "result": "PBFD achieved over 20 times faster development and 7-8 times faster query performance compared to popular industry alternatives, as validated through an eight-year enterprise use. Both PBFD and PDFD were implemented as open-source MVPs, and PDFD's implementation validated its correctness-first approach.", "conclusion": "PBFD and PDFD present rigorously defined, formally verified, and scalable methodologies for full-stack software engineering, successfully bridging the gap between formal methods and practical development. Their formal specifications and open-source tools demonstrate both improved speed and structural correctness, supporting academic and industrial adoption."}}
{"id": "2510.00003", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00003", "abs": "https://arxiv.org/abs/2510.00003", "authors": ["Malte Hansen", "Jens Bamberg", "Noe Baumann", "Wilhelm Hasselbring"], "title": "Semantic Zoom and Mini-Maps for Software Cities", "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Software visualization tools can facilitate program comprehension by\nproviding visual metaphors, or abstractions that reduce the amount of textual\ndata that needs to be processed mentally. One way they do this is by enabling\ndevelopers to build an internal representation of the visualized software and\nits architecture. However, as the amount of displayed data in the visualization\nincreases, the visualization itself can become more difficult to comprehend.\nThe ability to display small and large amounts of data in visualizations is\ncalled visual scalability.\n  In this paper, we present two approaches to address the challenge of visual\nscalability in 3D software cities. First, we present an approach to semantic\nzoom, in which the graphical representation of the software landscape changes\nbased on the virtual camera's distance from visual objects. Second, we augment\nthe visualization with a miniature two-dimensional top-view projection called\nmini-map. We demonstrate our approach using an open-source implementation in\nour software visualization tool ExplorViz. ExplorViz is web-based and uses the\n3D city metaphor, focusing on live trace visualization.\n  We evaluated our approaches in two separate user studies. The results\nindicate that semantic zoom and the mini-map are both useful additions. User\nfeedback indicates that semantic zoom and mini-maps are especially useful for\nlarge software landscapes and collaborative software exploration. The studies\nindicate a good usability of our implemented approaches. However, some\nshortcomings in our implementations have also been discovered, to be addressed\nin future work.\n  Video URL: https://youtu.be/LYtUeWvizjU", "AI": {"tldr": "The paper improves visual scalability in 3D software city visualizations using semantic zoom and a mini-map, both proven useful in user studies. These features make it easier to navigate and understand large software systems, though further improvements are necessary.", "motivation": "As visualizations for software architectures become larger and more complex, it's increasingly difficult for developers to comprehend them. There's a need for improved visual scalability so that both small and large amounts of data can be effectively displayed and understood.", "method": "The paper presents two main methodological approaches: (1) semantic zoom, where the graphical representation dynamically changes with the camera's distance to objects; and (2) a 2D mini-map offering a top-down overview. These were implemented in a web-based tool called ExplorViz and evaluated via two user studies.", "result": "The user studies demonstrated that both semantic zoom and mini-map are useful for navigating and comprehending large software visualizations. They received positive feedback, especially for use with large landscapes and collaborative tasks. Some implementation shortcomings were also identified for future improvement.", "conclusion": "Semantic zoom and mini-map features significantly enhance the usability and scalability of 3D software visualization tools, supporting better program comprehension. The approaches are practical and well-received, though future refinements are needed."}}
{"id": "2510.00004", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00004", "abs": "https://arxiv.org/abs/2510.00004", "authors": ["Malte Hansen", "David Moreno-Lumbreras", "Wilhelm Hasselbring"], "title": "HTML Structure Exploration in 3D Software Cities", "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Software visualization, which uses data from dynamic program analysis, can\nhelp to explore and understand the behavior of software systems. It is common\nthat large software systems offer a web interface for user interaction.\nUsually, available web interfaces are not regarded in software visualization\ntools. This paper introduces additions to the web-based live tracing software\nvisualization tool ExplorViz: We add an embedded web view for instrumented\napplications in the 3D visualization to ease interaction with the given\napplications and enable the exploration of the thereby displayed HTML content.\nNamely, the Document Object Model (DOM) is visualized via a three-dimensional\nrepresentation of the HTML structure in same-origin contexts.\n  Our visualization approach is evaluated in a preliminary user study. The\nstudy results give insights into the potential use cases, benefits, and\nshortcomings of our implemented approach. Based on our study results, we\npropose directions for further research to support the visual exploration of\nweb interfaces and explore use cases for the combined visualization of software\ncities and HTML structure.\n  Video URL: https://youtu.be/wBWKlbvzOOE", "AI": {"tldr": "The paper improves the ExplorViz tool by integrating web interface visualization directly into 3D software analysis. Through a user study, it demonstrates benefits and shortcomings, suggesting future research to further support web interface exploration in software visualization.", "motivation": "Existing software visualization tools typically exclude web interfaces, which are common in large software systems. There's a need to bridge this gap to facilitate better understanding and exploration of systems via their web interfaces.", "method": "The paper extends ExplorViz, a web-based live tracing software visualization tool, by adding an embedded web view and a 3D visualization of the Document Object Model (DOM) for instrumented applications. This enables interaction with web interfaces and visual exploration of HTML content in same-origin contexts. A preliminary user study is performed to evaluate the approach.", "result": "The user study provides insights into how the enhanced tool can be used, its benefits, and its shortcomings. It identifies potential use cases and areas for further development in the visual exploration of web interfaces and joint visualization of software cities and HTML structure.", "conclusion": "Visualizing web interfaces within software visualization tools like ExplorViz enhances user interaction and aids in understanding software systems. There remains scope for improving the visualization and exploring further use cases based on user feedback."}}
{"id": "2510.00031", "categories": ["cs.SE", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.00031", "abs": "https://arxiv.org/abs/2510.00031", "authors": ["Shun-ichiro Hayashi", "Koki Morita", "Daichi Mukunoki", "Tetsuya Hoshino", "Takahiro Katagiri"], "title": "VibeCodeHPC: An Agent-Based Iterative Prompting Auto-Tuner for HPC Code Generation Using LLMs", "comment": null, "summary": "We propose VibeCodeHPC, an automatic tuning system for HPC programs based on\nmulti-agent LLMs for code generation. VibeCodeHPC tunes programs through\nmulti-agent role allocation and iterative prompt refinement. We describe the\nsystem configuration with four roles: Project Manager (PM), System Engineer\n(SE), Programmer (PG), and Continuous Delivery (CD). We introduce dynamic agent\ndeployment and activity monitoring functions to facilitate effective\nmulti-agent collaboration. In our case study, we convert and optimize CPU-based\nmatrix-matrix multiplication code written in C to GPU code using CUDA. The\nmulti-agent configuration of VibeCodeHPC achieved higher-quality code\ngeneration per unit time compared to a solo-agent configuration. Additionally,\nthe dynamic agent deployment and activity monitoring capabilities facilitated\nmore effective identification of requirement violations and other issues.", "AI": {"tldr": "VibeCodeHPC automatically tunes HPC code using multi-agent LLMs, assigning distinct roles to maximize collaboration. Tested on CPU-to-GPU code conversion, it yields higher-quality results faster, while its dynamic system improves error detection over using a single agent.", "motivation": "Optimizing HPC programs often requires significant manual effort. Automatic tuning and code generation, especially for heterogeneous systems like CPUs and GPUs, are challenging tasks that could benefit from intelligent automation.", "method": "VibeCodeHPC utilizes multi-agent Large Language Models (LLMs), each assigned specific roles\u2014Project Manager, System Engineer, Programmer, and Continuous Delivery. These agents collaborate iteratively, dynamically deploying resources and monitoring their activity to automatically generate and optimize code.", "result": "In a case study, VibeCodeHPC successfully converted a CPU-based matrix multiplication code in C to optimized GPU code using CUDA. The multi-agent (collaborative) approach outperformed single-agent setups in both code quality and efficiency. Dynamic agent deployment also improved issue and requirement violation detection.", "conclusion": "VibeCodeHPC, by leveraging multi-agent LLM collaboration and dynamic resource management, provides a more effective method for automatic tuning and code generation in HPC compared to traditional or solo-agent approaches."}}
{"id": "2510.00092", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00092", "abs": "https://arxiv.org/abs/2510.00092", "authors": ["Shufeng Chen", "Mariat James Elizebeth", "Robab Aghazadeh Chakherlou", "Xingyu Zhao", "Eric Barbier", "Siddartha Khastgir", "Paul Jennings"], "title": "A Scalable Framework for Safety Assurance of Self-Driving Vehicles based on Assurance 2.0", "comment": null, "summary": "Assurance 2.0 is a modern framework developed to address the assurance\nchallenges of increasingly complex, adaptive, and autonomous systems. Building\non the traditional Claims-Argument-Evidence (CAE) model, it introduces reusable\nassurance theories and explicit counterarguments (defeaters) to enhance rigor,\ntransparency, and adaptability. It supports continuous, incremental assurance,\nenabling innovation without compromising safety. However, limitations persist\nin confidence measurement, residual doubt management, automation support, and\nthe practical handling of defeaters and confirmation bias. This paper presents\n\\textcolor{black}{a set of decomposition frameworks to identify a complete set\nof safety arguments and measure their corresponding evidence.} Grounded in the\nAssurance 2.0 paradigm, the framework is instantiated through a structured\ntemplate and employs a three-tiered decomposition strategy. \\textcolor{black}{A\ncase study regarding the application of the decomposition framework in the\nend-to-end (E2E) AI-based Self-Driving Vehicle (SDV) development is also\npresented in this paper.} At the top level, the SDV development is divided into\nthree critical phases: Requirements Engineering (RE), Verification and\nValidation (VnV), and Post-Deployment (PD). Each phase is further decomposed\naccording to its Product Development Lifecycle (PDLC). To ensure comprehensive\ncoverage, each PDLC is analyzed using an adapted 5M1E model (Man, Machine,\nMethod, Material, Measurement, and Environment). Originally developed for\nmanufacturing quality control, the 5M1E model is reinterpreted and contextually\nmapped to the assurance domain. This enables a multi-dimensional decomposition\nthat supports fine-grained traceability of safety claims, evidence, and\npotential defeaters.", "AI": {"tldr": "The paper introduces Assurance 2.0, a framework for assuring complex autonomous systems using structured decomposition and adapted models like 5M1E. It enables rigorous and transparent assurance across the development lifecycle, but some challenges like confidence measurement and automation remain.", "motivation": "Modern autonomous systems present complex assurance challenges due to their adaptability and autonomy. Traditional models lack flexibility and rigor when addressing safety and transparency in these systems, motivating the development of a new framework.", "method": "The paper builds on the Claims-Argument-Evidence (CAE) model and introduces reusable assurance theories and explicit counterarguments (defeaters). It utilizes a decomposition framework instantiated through a structured template and a three-tiered decomposition strategy. In a case study, the framework is applied to the self-driving vehicle development lifecycle, decomposing it into Requirements Engineering, Verification and Validation, and Post-Deployment phases, and further analyzing each using an adapted 5M1E model.", "result": "The decomposition frameworks identify a comprehensive set of safety arguments and measure corresponding evidence for assurance in autonomous system development. The use of the adapted 5M1E model ensures thorough coverage and fine-grained traceability across all stages of the product lifecycle.", "conclusion": "Assurance 2.0, with its new decomposition frameworks and adapted 5M1E model, enhances rigor, transparency, and adaptability in assuring complex autonomous systems. However, challenges remain in confidence measurement, doubt management, automation, and the practical handling of defeaters and biases."}}
{"id": "2510.00197", "categories": ["cs.SE", "D.2.11"], "pdf": "https://arxiv.org/pdf/2510.00197", "abs": "https://arxiv.org/abs/2510.00197", "authors": ["Diogo Maia", "Filipe Correia", "Andr\u00e9 Restivo", "Paulo Queiroz"], "title": "Container Orchestration Patterns for Optimizing Resource Use", "comment": null, "summary": "Service-based architectures provide substantial benefits, yet service\norchestration remains a challenge, particularly for newcomers. While various\nresources on orchestration techniques exist, they often lack clarity and\nstandardization, making best practices difficult to implement and limiting\ntheir adoption within the software industry.\n  To address this gap, we analyzed existing literature and tools to identify\ncommon orchestration practices. Based on our findings, we define three key\norchestration resource optimization patterns: {\\sc Preemptive Scheduling}, {\\sc\nService Balancing}, and {\\sc Garbage Collection}. {\\sc Preemptive Scheduling}\nallows the allocation of sufficient resources for services of higher priority\nin stressful situations, while {\\sc Service Balancing} enables a restructuring\nof the nodes to allow better resource usage. To end, {\\sc Garbage Collection}\ncreates cleanup mechanisms to better understand the system's resource usage and\noptimize it. These patterns serve as foundational elements for improving\norchestration practices and fostering broader adoption in service-based\narchitectures.", "AI": {"tldr": "The paper addresses the complexity of service orchestration in service-based architectures by identifying and defining three core resource optimization patterns\u2014Preemptive Scheduling, Service Balancing, and Garbage Collection\u2014through analysis of literature and tools. These patterns aim to standardize best practices, improve resource management, and encourage wider industry adoption.", "motivation": "Service orchestration in service-based architectures is challenging, especially for newcomers, due to lack of clarity and standardization in existing resources and practices.", "method": "The authors analyzed existing literature and tools to identify and define common orchestration practices.", "result": "Three key orchestration resource optimization patterns were identified and defined: Preemptive Scheduling, Service Balancing, and Garbage Collection. Each pattern addresses a distinct optimization challenge in service orchestration.", "conclusion": "These foundational patterns can improve orchestration practices and facilitate broader adoption of service-based architectures by providing clear, standardized approaches."}}
{"id": "2510.00324", "categories": ["cs.SE", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00324", "abs": "https://arxiv.org/abs/2510.00324", "authors": ["Lucas Roberts", "Denisa Roberts"], "title": "Which Programming Language and Model Work Best With LLM-as-a-Judge For Code Retrieval?", "comment": "Accepted as a full paper at SIGIR-AP 2025", "summary": "Code search is an important information retrieval application. Benefits of\nbetter code search include faster new developer on-boarding, reduced software\nmaintenance, and ease of understanding for large repositories. Despite\nimprovements in search algorithms and search benchmarks, the domain of code\nsearch has lagged behind. One reason is the high cost of human annotation for\ncode queries and answers. While humans may annotate search results in general\ntext QA systems, code annotations require specialized knowledge of a\nprogramming language (PL), as well as domain specific software engineering\nknowledge. In this work we study the use of Large Language Models (LLMs) to\nretrieve code at the level of functions and to generate annotations for code\nsearch results. We compare the impact of the retriever representation (sparse\nvs. semantic), programming language, and LLM by comparing human annotations\nacross several popular languages (C, Java, Javascript, Go, and Python). We\nfocus on repositories that implement common data structures likely to be\nimplemented in any PLs. For the same human annotations, we compare several\nLLM-as-a-Judge models to evaluate programming language and other affinities\nbetween LLMs. We find that the chosen retriever and PL exhibit affinities that\ncan be leveraged to improve alignment of human and AI relevance determinations,\nwith significant performance implications. We also find differences in\nrepresentation (sparse vs. semantic) across PLs that impact alignment of human\nand AI relevance determinations. We propose using transpilers to bootstrap\nscalable code search benchmark datasets in other PLs and in a case study\ndemonstrate that human-AI relevance agreement rates largely match the (worst\ncase) human-human agreement under study. The application code used in this work\nis available at \\href{https://github.com/rlucas7/code-searcher/}{this github\nrepo}.", "AI": {"tldr": "This paper explores how large language models can automate code search and annotation, reducing reliance on costly human expertise. Key findings highlight the impact of retriever types and programming languages on AI-human agreement in relevance judgments and propose transpilers to expand benchmarks. The proposed techniques can make code search more scalable and effective across various languages.", "motivation": "Code search is crucial for developers but hampered by the high cost and expertise required for annotating search results. Current advancements in natural language search have outpaced code search mainly due to these annotation challenges.", "method": "The authors leverage Large Language Models (LLMs) to retrieve code snippets at the function level and automatically generate annotations for search results. They study both sparse and semantic representations and evaluate LLM performance across multiple programming languages using human annotations and LLM-as-a-Judge models. They also propose using transpilers to create scalable code search benchmark datasets for different languages.", "result": "The study finds that both the choice of code retriever and programming language affects the alignment between human and AI relevance judgments. Sparse and semantic representations show varying performance across languages. Using transpilers can effectively expand benchmark datasets, and human-AI agreement rates are comparable to human-human agreement rates in the worst case scenario.", "conclusion": "LLMs, combined with appropriate retriever representations and transpilers, can significantly improve scalability and relevance alignment in code search tasks across programming languages. This work demonstrates practical and scalable methods for benchmarking and improving code search tools with AI annotation."}}
{"id": "2510.00328", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00328", "abs": "https://arxiv.org/abs/2510.00328", "authors": ["Ahmed Fawzy", "Amjed Tahir", "Kelly Blincoe"], "title": "Vibe Coding in Practice: Motivations, Challenges, and a Future Outlook -- a Grey Literature Review", "comment": null, "summary": "AI code generation tools are transforming software development, especially\nfor novice and non-software developers, by enabling them to write code and\nbuild applications faster and with little to no human intervention. Vibe coding\nis the practice where users rely on AI code generation tools through intuition\nand trial-and-error without necessarily understanding the underlying code.\nDespite widespread adoption, no research has systematically investigated why\nusers engage in vibe coding, what they experience while doing so, and how they\napproach quality assurance (QA) and perceive the quality of the AI-generated\ncode. To this end, we conduct a systematic grey literature review of 101\npractitioner sources, extracting 518 firsthand behavioral accounts about vibe\ncoding practices, challenges, and limitations. Our analysis reveals a\nspeed-quality trade-off paradox, where vibe coders are motivated by speed and\naccessibility, often experiencing rapid ``instant success and flow'', yet most\nperceive the resulting code as fast but flawed. QA practices are frequently\noverlooked, with many skipping testing, relying on the models' or tools'\noutputs without modification, or delegating checks back to the AI code\ngeneration tools. This creates a new class of vulnerable software developers,\nparticularly those who build a product but are unable to debug it when issues\narise. We argue that vibe coding lowers barriers and accelerates prototyping,\nbut at the cost of reliability and maintainability. These insights carry\nimplications for tool designers and software development teams. Understanding\nhow vibe coding is practiced today is crucial for guiding its responsible use\nand preventing a broader QA crisis in AI-assisted development.", "AI": {"tldr": "AI code generation tools enable fast prototyping by non-experts, but users (vibe coders) often overlook code quality assurance, leading to unreliable software. Understanding these practices is crucial to prevent quality crises in AI-driven development.", "motivation": "AI code generation tools have become widely used by novices and non-developers for faster application development, but little is known about users' motivations, experiences, and QA practices.", "method": "A systematic grey literature review of 101 practitioner sources, analyzing 518 firsthand accounts of vibe coding practices, challenges, and limitations.", "result": "The study reveals that vibe coders prioritize speed and accessibility, leading to quick results but flawed code. Most users neglect QA, often skipping testing or relying solely on the AI's output, resulting in vulnerable software and developers unable to debug issues.", "conclusion": "Vibe coding accelerates prototyping and lowers entry barriers but compromises reliability and maintainability, posing risks for software quality and necessitating attention from tool designers and teams."}}
{"id": "2510.00450", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00450", "abs": "https://arxiv.org/abs/2510.00450", "authors": ["Sheikh Md. Mushfiqur Rahman", "Nasir Eisty"], "title": "Beyond Pass/Fail: The Story of Learning-Based Testing", "comment": null, "summary": "Learning-Based Testing (LBT) merges learning and testing processes to achieve\nboth testing and behavioral adequacy. LBT utilizes active learning to infer the\nmodel of the System Under Test (SUT), enabling scalability for large and\ncomplex programs by requiring only a minimal set of initial test cases. The\ncore principle of LBT is that the SUT's behavior can be thoroughly inferred by\nprogressively generating test cases and subjecting the SUT to testing, thereby\nensuring comprehensive testing. Despite being in its early stages, LBT has a\nsolid foundation of theoretical research demonstrating its efficacy in testing\nboth procedural and reactive programs. This paper provides a systematic\nliterature review of various LBT implementations across different program types\nand evaluates the current state of research in this field. We explore diverse\ntheoretical frameworks, existing tools, and libraries within the LBT domain to\nillustrate the concept's evolution and current research status. Additionally,\nwe examine case studies involving the application of LBT tools in industrial\nsettings, highlighting their potential and effectiveness in commercial software\ntesting. This systematic literature review aims to offer researchers a\ncomprehensive perspective on the inception and development of LBT, presenting\nit as a promising technique in software testing. By unveiling LBT's\nunderutilized potential, this paper seeks to significantly benefit the\npractitioners and research community.", "AI": {"tldr": "This paper reviews the current state and potential of Learning-Based Testing (LBT), summarizing its theoretical backing, practical applications, and underused value in software testing. The findings suggest that LBT is a promising technique for effective and efficient software testing, with proven benefits in both academia and industry.", "motivation": "The motivation behind this paper is to provide a comprehensive overview of Learning-Based Testing (LBT), a promising approach that combines learning and software testing to improve test coverage and behavioral adequacy, especially for large and complex software systems. As LBT is relatively underutilized and still in its early stages, the authors aim to highlight its development, effectiveness, and potential benefits for researchers and practitioners.", "method": "This paper employs a systematic literature review methodology. The authors collect, review, and synthesize existing research, implementations, frameworks, tools, and case studies related to LBT across different types of programs (procedural and reactive). They also evaluate the evolution, state-of-the-art, and industrial application of LBT tools.", "result": "The paper presents a detailed synthesis of LBT's theoretical foundations and practical applications. It highlights successful case studies in industrial contexts, confirms LBT's efficacy for commercial software testing, and shows a variety of available tools and frameworks. The review demonstrates LBT's evolution and underexploited potential in the software testing field.", "conclusion": "The paper concludes that LBT is a promising and effective approach for comprehensive software testing, especially suitable for complex and large systems. Despite its early development stage, LBT's theoretical and practical evidence suggests significant benefits for both researchers and industry practitioners. The authors encourage further exploration and adoption of LBT to realize its potential in software quality assurance."}}
{"id": "2510.00476", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.00476", "abs": "https://arxiv.org/abs/2510.00476", "authors": ["Arushi Sharma", "Vedant Pungliya", "Christopher J. Quinn", "Ali Jannesari"], "title": "Analyzing Latent Concepts in Code Language Models", "comment": null, "summary": "Interpreting the internal behavior of large language models trained on code\nremains a critical challenge, particularly for applications demanding trust,\ntransparency, and semantic robustness. We propose Code Concept Analysis\n(CoCoA): a global post-hoc interpretability framework that uncovers emergent\nlexical, syntactic, and semantic structures in a code language model's\nrepresentation space by clustering contextualized token embeddings into\nhuman-interpretable concept groups. We propose a hybrid annotation pipeline\nthat combines static analysis tool-based syntactic alignment with\nprompt-engineered large language models (LLMs), enabling scalable labeling of\nlatent concepts across abstraction levels. We analyse the distribution of\nconcepts across layers and across three finetuning tasks. Emergent concept\nclusters can help identify unexpected latent interactions and be used to\nidentify trends and biases within the model's learned representations. We\nfurther integrate LCA with local attribution methods to produce\nconcept-grounded explanations, improving the coherence and interpretability of\ntoken-level saliency. Empirical evaluations across multiple models and tasks\nshow that LCA discovers concepts that remain stable under semantic-preserving\nperturbations (average Cluster Sensitivity Index, CSI = 0.288) and evolve\npredictably with fine-tuning. In a user study, concept-augmented explanations\ndisambiguate token roles. In a user study on the programming-language\nclassification task, concept-augmented explanations disambiguated token roles\nand improved human-centric explainability by 37 percentage points compared with\ntoken-level attributions using Integrated Gradients.", "AI": {"tldr": "This paper presents a novel interpretability tool, CoCoA, for code-related language models, which organizes internal representations into human-friendly concept clusters, enables better explanations, and shows improved user understanding over traditional attribution techniques.", "motivation": "Interpreting the behaviors of large language models (LLMs) trained on code is difficult, yet essential for ensuring trust, transparency, and robustness in high-stakes applications.", "method": "The paper introduces Code Concept Analysis (CoCoA), a global post-hoc interpretability framework. CoCoA clusters contextualized token embeddings into human-interpretable concept groups using a hybrid annotation pipeline that combines static analysis with prompt-engineered LLMs for scalable labeling. It analyzes the distribution of concepts across model layers and fine-tuning tasks, and integrates concept analysis with local attribution methods to ground explanations.", "result": "CoCoA identifies stable and interpretable concept clusters related to code structure and semantics, revealing latent model interactions and biases. The framework's concept-augmented explanations improve the coherence and interpretability of token-level saliency, outperforming standard attribution methods (Integrated Gradients) by significantly increasing explainability in user studies.", "conclusion": "Code Concept Analysis provides a scalable and effective method for interpreting code LLMs, discovering meaningful semantic and syntactic concepts that persist across perturbations and tasks, and improving human understanding of model predictions."}}
{"id": "2510.00501", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00501", "abs": "https://arxiv.org/abs/2510.00501", "authors": ["Kaixin Wang", "Tianlin Li", "Xiaoyu Zhang", "Aishan Liu", "Xianglong Liu", "Ziqi Liu", "Zhiqiang Zhang", "Jun Zhou", "and Bin Shi"], "title": "CodeChemist: Functional Knowledge Transfer for Low-Resource Code Generation via Test-Time Scaling", "comment": null, "summary": "Code Large Language Models (CodeLLMs) are increasingly used in code\ngeneration tasks across a wide range of applications. However, their\nperformance is often inconsistent across different programming languages (PLs),\nwith low-resource PLs suffering the most due to limited training data. In this\npaper, we present CodeChemist, a novel and efficient framework for test-time\nscaling that enables functional knowledge transfer from high-resource to\nlow-resource PLs using generated test cases. CodeChemist first generates and\nexecutes code in high-resource PLs to create test cases that encapsulate\nfunctional knowledge. It then uses multi-temperature hedged sampling to\ngenerate code snippets in the low-resource PL and selects the best one based on\nthe pass rate of the test cases. Our extensive experiments show that\nCodeChemist outperforms existing test-time scaling approaches, boosting the\nperformance of code generation for low-resource PLs without requiring any model\nretraining.", "AI": {"tldr": "CodeChemist transfers knowledge from high-resource to low-resource programming languages at test time by generating test cases and intelligently sampling code, significantly improving CodeLLM code generation for low-resource languages without retraining the model.", "motivation": "CodeLLMs show uneven performance across programming languages, particularly underperforming for low-resource languages due to limited training data. Addressing this discrepancy is crucial for broader applicability of code generation models.", "method": "CodeChemist introduces a test-time scaling framework that transfers functional knowledge from high-resource to low-resource languages. It generates and executes code in high-resource languages to create test cases, then uses multi-temperature hedged sampling to generate code in low-resource languages. Code snippets are evaluated based on the pass rate of the test cases, with the best one selected.", "result": "Extensive experiments demonstrate that CodeChemist outperforms existing test-time scaling approaches, leading to significant performance improvements in code generation for low-resource programming languages. Importantly, this is achieved without any model retraining.", "conclusion": "CodeChemist offers an efficient and effective solution for improving CodeLLM performance in low-resource languages by leveraging functional knowledge transfer from high-resource languages at test time, enhancing code generation without extra training."}}
{"id": "2510.00519", "categories": ["cs.SE", "cs.AI", "D.2.4; D.2.11"], "pdf": "https://arxiv.org/pdf/2510.00519", "abs": "https://arxiv.org/abs/2510.00519", "authors": ["Hadiza Umar Yusuf", "Khouloud Gaaloul"], "title": "Architectural Transformations and Emerging Verification Demands in AI-Enabled Cyber-Physical Systems", "comment": null, "summary": "In the world of Cyber-Physical Systems (CPS), a captivating real-time fusion\noccurs where digital technology meets the physical world. This synergy has been\nsignificantly transformed by the integration of artificial intelligence (AI), a\nmove that dramatically enhances system adaptability and introduces a layer of\ncomplexity that impacts CPS control optimization and reliability. Despite\nadvancements in AI integration, a significant gap remains in understanding how\nthis shift affects CPS architecture, operational complexity, and verification\npractices. The extended abstract addresses this gap by investigating\narchitectural distinctions between AI-driven and traditional control models\ndesigned in Simulink and their respective implications for system verification.", "AI": {"tldr": "This paper highlights how adding AI to CPS changes system structure and verification needs compared to traditional models, emphasizing new complexities and adaptation requirements.", "motivation": "The motivation is to address the lack of understanding regarding the effects of integrating AI into CPS, specifically in terms of architecture, operational complexity, and verification practices.", "method": "The paper investigates architectural differences between AI-driven and traditional control models, specifically those designed in Simulink. It analyzes how these differences impact system verification.", "result": "The study reveals specific distinctions between AI-driven and traditional CPS control architectures and discusses their respective implications for verification.", "conclusion": "Integrating AI into CPS significantly alters architectural design and system complexity, necessitating changes in verification approaches."}}
{"id": "2510.00532", "categories": ["cs.SE", "cs.CR", "D.2.5"], "pdf": "https://arxiv.org/pdf/2510.00532", "abs": "https://arxiv.org/abs/2510.00532", "authors": ["Hengcheng Zhu", "Songqiang Chen", "Valerio Terragni", "Lili Wei", "Jiarong Wu", "Yepang Liu", "Shing-Chi Cheung"], "title": "LSPFuzz: Hunting Bugs in Language Servers", "comment": "This paper has been accepted for publication in The 40th IEEE/ACM\n  International Conference on Automated Software Engineering (ASE 2025)", "summary": "The Language Server Protocol (LSP) has revolutionized the integration of code\nintelligence in modern software development. There are approximately 300 LSP\nserver implementations for various languages and 50 editors offering LSP\nintegration. However, the reliability of LSP servers is a growing concern, as\ncrashes can disable all code intelligence features and significantly impact\nproductivity, while vulnerabilities can put developers at risk even when\nediting untrusted source code. Despite the widespread adoption of LSP, no\nexisting techniques specifically target LSP server testing. To bridge this gap,\nwe present LSPFuzz, a grey-box hybrid fuzzer for systematic LSP server testing.\nOur key insight is that effective LSP server testing requires holistic mutation\nof source code and editor operations, as bugs often manifest from their\ncombinations. To satisfy the sophisticated constraints of LSP and effectively\nexplore the input space, we employ a two-stage mutation pipeline: syntax-aware\nmutations to source code, followed by context-aware dispatching of editor\noperations. We evaluated LSPFuzz on four widely used LSP servers. LSPFuzz\ndemonstrated superior performance compared to baseline fuzzers, and uncovered\npreviously unknown bugs in real-world LSP servers. Of the 51 bugs we reported,\n42 have been confirmed, 26 have been fixed by developers, and two have been\nassigned CVE numbers. Our work advances the quality assurance of LSP servers,\nproviding both a practical tool and foundational insights for future research\nin this domain.", "AI": {"tldr": "LSPFuzz is a specialized fuzzer that systematically tests LSP servers by mutating both source code and editor actions, uncovering dozens of previously unknown bugs and vulnerabilities. This tool surpasses conventional fuzzers, enhances server reliability, and sets new directions for research in LSP server testing.", "motivation": "The widespread adoption of the Language Server Protocol (LSP) has improved code intelligence integration in modern software development, but the reliability and security of LSP servers remain major concerns. Crashes can disable vital features and vulnerabilities can expose developers to risks, especially when editing untrusted code. Prior to this work, no specific testing techniques targeted LSP server robustness.", "method": "The paper introduces LSPFuzz, a grey-box hybrid fuzzer designed specifically for LSP server testing. LSPFuzz employs a two-stage mutation pipeline: first, it applies syntax-aware mutations to source code, and then performs context-aware dispatching of editor operations. This holistic approach effectively tests combinations of code and editor actions that can trigger bugs.", "result": "LSPFuzz was evaluated on four popular LSP servers and demonstrated better effectiveness than traditional fuzzers. It discovered 51 previously unknown bugs, of which 42 were confirmed by developers, 26 were fixed, and two were assigned CVE numbers, highlighting the tool's impact on improving LSP server reliability and security.", "conclusion": "LSPFuzz provides a practical and effective solution for systematically testing LSP servers. It not only improves code intelligence reliability but also lays a foundation for future research in LSP server quality assurance. The results affirm the value of targeted, holistic mutation-based fuzzing for complex protocol implementations."}}
{"id": "2510.00591", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00591", "abs": "https://arxiv.org/abs/2510.00591", "authors": ["Liyi Cai", "Yijie Ren", "Yitong Zhang", "Jia Li"], "title": "AI-Driven Self-Evolving Software: A Promising Path Toward Software Automation", "comment": null, "summary": "Software automation has long been a central goal of software engineering,\nstriving for software development that proceeds without human intervention.\nRecent efforts have leveraged Artificial Intelligence (AI) to advance software\nautomation with notable progress. However, current AI functions primarily as\nassistants to human developers, leaving software development still dependent on\nexplicit human intervention. This raises a fundamental question: Can AI move\nbeyond its role as an assistant to become a core component of software, thereby\nenabling genuine software automation? To investigate this vision, we introduce\nAI-Driven Self-Evolving Software, a new form of software that evolves\ncontinuously through direct interaction with users. We demonstrate the\nfeasibility of this idea with a lightweight prototype built on a multi-agent\narchitecture that autonomously interprets user requirements, generates and\nvalidates code, and integrates new functionalities. Case studies across\nmultiple representative scenarios show that the prototype can reliably\nconstruct and reuse functionality, providing early evidence that such software\nsystems can scale to more sophisticated applications and pave the way toward\ntruly automated software development. We make code and cases in this work\npublicly available at https://anonymous.4open.science/r/live-software.", "AI": {"tldr": "This paper proposes and tests a self-evolving software system powered by AI that autonomously develops and maintains software based on user input. The results show promising scalability and automation, suggesting a path toward fully automated software engineering.", "motivation": "The motivation is to address the limitations of current AI-driven software development, which still largely requires explicit human intervention. The authors aim to explore whether AI can move beyond being just an assistant to developers, to become a core, autonomous component of software, enabling true end-to-end automation.", "method": "This paper introduces the concept of AI-Driven Self-Evolving Software and demonstrates its feasibility through a prototype. The prototype employs a multi-agent architecture designed to autonomously interpret user requirements, generate and validate code, and integrate new functionalities based on direct user interaction.", "result": "The prototype successfully constructs and reuses functionalities across varied scenarios. Case studies indicate the system reliably scales up to more sophisticated applications.", "conclusion": "The results provide early evidence that AI-Driven Self-Evolving Software can lead to genuine automated software development, moving AI from an assistant to an integral, autonomous role in software engineering. Publicly sharing code and cases helps further research and validation."}}
{"id": "2510.00674", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00674", "abs": "https://arxiv.org/abs/2510.00674", "authors": ["Konstantinos Karakatsanis", "Georgios Alexopoulos", "Ioannis Karyotakis", "Foivos Timotheos Proestakis", "Evangelos Talos", "Panos Louridas", "Dimitris Mitropoulos"], "title": "PyTrim: A Practical Tool for Reducing Python Dependency Bloat", "comment": "Accepted at ASE 2025 (Tool Demonstration Track)", "summary": "Dependency bloat is a persistent challenge in Python projects, which\nincreases maintenance costs and security risks. While numerous tools exist for\ndetecting unused dependencies in Python, removing these dependencies across the\nsource code and configuration files of a project requires manual effort and\nexpertise.\n  To tackle this challenge we introduce PYTRIM, an end-to-end system to\nautomate this process. PYTRIM eliminates unused imports and package\ndeclarations across a variety of file types, including Python source and\nconfiguration files such as requirements.txt and setup.py. PYTRIM's modular\ndesign makes it agnostic to the source of dependency bloat information,\nenabling integration with any detection tool. Beyond its contribution when it\ncomes to automation, PYTRIM also incorporates a novel dynamic analysis\ncomponent that improves dependency detection recall.\n  Our evaluation of PYTRIM's end-to-end effectiveness on a ground-truth dataset\nof 37 merged pull requests from prior work, shows that PYTRIM achieves 98.3%\naccuracy in replicating human-made changes. To show its practical impact, we\nrun PYTRIM on 971 open-source packages, identifying and trimming bloated\ndependencies in 39 of them. For each case, we submit a corresponding pull\nrequest, 6 of which have already been accepted and merged. PYTRIM is available\nas an open-source project, encouraging community contributions and further\ndevelopment.\n  Video demonstration: https://youtu.be/LqTEdOUbJRI\n  Code repository: https://github.com/TrimTeam/PyTrim", "AI": {"tldr": "PYTRIM is an open-source system that automates the removal of unused dependencies from Python projects, achieving high accuracy and practical impact by eliminating manual cleanup, and is ready for community contributions.", "motivation": "Dependency bloat in Python projects leads to higher maintenance costs and security risks, yet removing unused dependencies remains a manual and expertise-intensive process despite the existence of detection tools.", "method": "The authors introduce PYTRIM, an automated, end-to-end system that removes unused dependencies\u2014including imports and package declarations\u2014from both source code and configuration files. PYTRIM employs a modular architecture to accept input from any dependency bloat detection tool and uniquely incorporates a dynamic analysis component for better recall.", "result": "PYTRIM was evaluated on a dataset of 37 pull requests, achieving 98.3% accuracy in reproducing human changes. In practical use, PYTRIM trimmed dependencies in 39 out of 971 open-source packages, with 6 pull request contributions accepted and merged upstream.", "conclusion": "PYTRIM effectively automates the removal of unused dependencies in Python projects, improving accuracy and reducing manual effort, and is openly available for community use and improvement."}}
{"id": "2510.00680", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00680", "abs": "https://arxiv.org/abs/2510.00680", "authors": ["Hang Cui", "Jingjing Li", "Haotian Si", "Quan Zhou", "Changhua Pei", "Gaogang Xie", "Dan Pei"], "title": "TShape: Rescuing Machine Learning Models from Complex Shapelet Anomalies", "comment": null, "summary": "Time series anomaly detection (TSAD) is critical for maintaining the\nreliability of modern IT infrastructures, where complex anomalies frequently\narise in highly dynamic environments. In this paper, we present TShape, a novel\nframework designed to address the challenges in industrial time series anomaly\ndetection. Existing methods often struggle to detect shapelet anomalies that\nmanifest as complex shape deviations, which appear obvious to human experts but\nprove challenging for machine learning algorithms. TShape introduces a\npatch-wise dual attention mechanism with multi-scale convolution to model\nintricate sub-sequence variations by balancing local, fine-grained shape\nfeatures with global contextual dependencies. Our extensive evaluation on five\ndiverse benchmarks demonstrates that TShape outperforms existing\nstate-of-the-art models, achieving an average 10\\% F1 score improvement in\nanomaly detection. Additionally, ablation studies and attention visualizations\nconfirm the essential contributions of each component, highlighting the\nrobustness and adaptability of TShape to complex shapelet shapes in time series\ndata.", "AI": {"tldr": "TShape is a new anomaly detection framework that leverages dual attention and multi-scale convolution to better detect complex shape anomalies in time series data, outperforming existing methods with a significant F1 score improvement and increased robustness.", "motivation": "Accurate detection of complex anomalies in time series data is fundamental for reliable IT infrastructure operations. Current machine learning methods have difficulties identifying 'shapelet anomalies'\u2014subtle but critical shape changes easily identified by experts, yet missed by algorithms.", "method": "The paper proposes TShape, a framework using a patch-wise dual attention mechanism combined with multi-scale convolution. This approach enables modeling of both local, fine-grained shape features and broader contextual dependencies, which improves the detection of intricate sub-sequence anomalies.", "result": "TShape achieves, on average, a 10% improvement in F1 score compared to state-of-the-art methods across five diverse benchmarks for industrial time series anomaly detection. Ablation studies and attention visualizations further support the effectiveness and robustness of its individual components.", "conclusion": "TShape demonstrates superior performance and adaptability in identifying complex shapelet anomalies in time series data, making it suitable for use in unpredictable and highly dynamic environments found in IT infrastructures."}}
{"id": "2510.00730", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.00730", "abs": "https://arxiv.org/abs/2510.00730", "authors": ["Larissa Schmid", "Elias Lundell", "Yogya Gamage", "Benoit Baudry", "Martin Monperrus"], "title": "Maven-Lockfile: High Integrity Rebuild of Past Java Releases", "comment": null, "summary": "Modern software projects depend on many third-party libraries, complicating\nreproducible and secure builds. Several package managers address this with the\ngeneration of a lockfile that freezes dependency versions and can be used to\nverify the integrity of dependencies. Yet, Maven, one of the most important\npackage managers in the Java ecosystem, lacks native support for a lockfile. We\npresent Maven-Lockfile to generate and update lockfiles, with support for\nrebuilding projects from past versions. Our lockfiles capture all direct and\ntransitive dependencies with their checksums, enabling high integrity builds.\nOur evaluation shows that Maven-Lockfile can reproduce builds from historical\ncommits and is able to detect tampered artifacts. With minimal configuration,\nMaven-Lockfile equips Java projects with modern build integrity and build\nreproducibility, and fosters future research on software supply chain security\nin Java.", "AI": {"tldr": "Maven-Lockfile is a tool that brings modern lockfile support to Java's Maven, enabling secure and reproducible builds by capturing dependencies and their checksums. It works with minimal setup and detects tampering, addressing a major ecosystem gap.", "motivation": "Maven, a widely used package manager for Java, does not natively support lockfiles, making it harder to achieve reproducible and secure builds in projects that depend on many third-party libraries.", "method": "The authors introduce 'Maven-Lockfile', a tool that generates and updates lockfiles, capturing direct and transitive dependencies with checksums to ensure build integrity.", "result": "Maven-Lockfile enables reproducible builds from historical commits and detects tampered artifacts, requiring minimal configuration.", "conclusion": "Maven-Lockfile provides Java projects with improved build integrity and reproducibility, and offers a foundation for future research in Java software supply chain security."}}
{"id": "2510.00762", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.00762", "abs": "https://arxiv.org/abs/2510.00762", "authors": ["Rudrajit Choudhuri", "Carmen Badea", "Christian Bird", "Jenna Butler", "Rob DeLine", "Brian Houck"], "title": "AI Where It Matters: Where, Why, and How Developers Want AI Support in Daily Work", "comment": null, "summary": "Generative AI is reshaping software work, yet we lack clear guidance on where\ndevelopers most need and want support, and how to design it responsibly. We\nreport a large-scale, mixed-methods study of N=860 developers that examines\nwhere, why, and how they seek or limit AI help, providing the first task-aware,\nempirically validated mapping from developers' perceptions of their tasks to AI\nadoption patterns and responsible AI priorities. Using cognitive appraisal\ntheory, we show that task evaluations predict openness to and use of AI,\nrevealing distinct patterns: strong current use and a desire for improvement in\ncore work (e.g., coding, testing); high demand to reduce toil (e.g.,\ndocumentation, operations); and clear limits for identity- and\nrelationship-centric work (e.g., mentoring). Priorities for responsible AI\nsupport vary by context: reliability and security for systems-facing tasks;\ntransparency, alignment, and steerability to maintain control; and fairness and\ninclusiveness for human-facing work. Our results offer concrete, contextual\nguidance for delivering AI where it matters to developers and their work.", "AI": {"tldr": "The paper maps out when, why, and how developers want or limit AI support across different software tasks, finding that needs and priorities for responsible AI vary by context (code, ops, mentoring), and offering tailored recommendations to guide future AI tool design.", "motivation": "There is a lack of clear guidance on where developers most need and want support from generative AI, and on how such support should be designed responsibly.", "method": "A large-scale mixed-methods study involving 860 developers, utilizing cognitive appraisal theory to analyze task evaluations and correlate them with AI adoption patterns and responsible AI priorities.", "result": "Developers show strong current use of AI in coding and testing, seek to reduce toil in documentation and operations, and place clear boundaries for AI involvement in mentoring and relationship-building. Responsible AI priorities differ by task: reliability for systems-facing tasks, transparency and control for core work, and fairness for human-facing work.", "conclusion": "The study provides a contextual, empirical mapping of developers' needs and preferences for AI assistance in software work, highlighting where AI adoption is suitable and where responsible design is critical."}}
{"id": "2510.00881", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.00881", "abs": "https://arxiv.org/abs/2510.00881", "authors": ["Patrizio Migliarini", "Mashal Afzal Memon", "Marco Autili", "Paola Inverardi"], "title": "Advancing Automated Ethical Profiling in SE: a Zero-Shot Evaluation of LLM Reasoning", "comment": "Accepted at ASE 2025", "summary": "Large Language Models (LLMs) are increasingly integrated into software\nengineering (SE) tools for tasks that extend beyond code synthesis, including\njudgment under uncertainty and reasoning in ethically significant contexts. We\npresent a fully automated framework for assessing ethical reasoning\ncapabilities across 16 LLMs in a zero-shot setting, using 30 real-world\nethically charged scenarios. Each model is prompted to identify the most\napplicable ethical theory to an action, assess its moral acceptability, and\nexplain the reasoning behind their choice. Responses are compared against\nexpert ethicists' choices using inter-model agreement metrics. Our results show\nthat LLMs achieve an average Theory Consistency Rate (TCR) of 73.3% and Binary\nAgreement Rate (BAR) on moral acceptability of 86.7%, with interpretable\ndivergences concentrated in ethically ambiguous cases. A qualitative analysis\nof free-text explanations reveals strong conceptual convergence across models\ndespite surface-level lexical diversity. These findings support the potential\nviability of LLMs as ethical inference engines within SE pipelines, enabling\nscalable, auditable, and adaptive integration of user-aligned ethical\nreasoning. Our focus is the Ethical Interpreter component of a broader\nprofiling pipeline: we evaluate whether current LLMs exhibit sufficient\ninterpretive stability and theory-consistent reasoning to support automated\nprofiling.", "AI": {"tldr": "The paper evaluates how well 16 large language models can reason about ethical dilemmas, finding high consistency with expert ethicists and suggesting LLMs could be useful for ethical decision-making in software engineering tools.", "motivation": "Large Language Models (LLMs) are increasingly used in software engineering tools not just for coding tasks, but also for making judgments under uncertainty and in ethically relevant situations. Assessing whether these models can reason about ethics in a reliable way is essential for their responsible deployment.", "method": "The authors introduce a fully automated framework to assess the ethical reasoning abilities of 16 LLMs. In a zero-shot setting, each model is evaluated using 30 real-world ethical dilemmas and asked to: (1) pick the most applicable ethical theory, (2) assess moral acceptability of actions, and (3) explain their reasoning. Model outputs are compared to expert ethicists' opinions using inter-model agreement metrics and qualitative analysis of explanations.", "result": "LLMs demonstrated an average Theory Consistency Rate (TCR) of 73.3% and a Binary Agreement Rate (BAR) of 86.7% regarding moral acceptability, aligning closely with expert judgments. Divergences appeared mainly in ambiguous cases, but different models still showed strong conceptual convergence in their textual explanations.", "conclusion": "Current LLMs have promising ethical reasoning capabilities, showing interpretable and theory-consistent outputs in most tested scenarios. They can potentially serve as scalable, auditable ethical inference engines in software engineering pipelines, though ambiguity in some cases warrants further attention."}}
{"id": "2510.00920", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00920", "abs": "https://arxiv.org/abs/2510.00920", "authors": ["Songqiang Chen", "Congying Xu", "Jingyi Chen", "Jialun Cao", "Jiarong Wu", "Shing-Chi Cheung"], "title": "On Effective Semantic Translation for Code: A Study Based on Pseudocode", "comment": null, "summary": "Large language models (LLMs) show great potential in code translation.\nHowever, accurate translation remains challenging when using the commonly\nadopted direct code-to-code translation approach, which converts a program into\nthe target programming language (PL) in a single step. Inspired by the success\nof incorporating intermediate steps to guide LLMs in resolving challenging\ntasks, we explore pseudocode-based code translation, which emulates the human\nsemantic translation by first interpreting the program's intent and logic into\npseudocode and then implementing it in the target PL. We find that\npseudocode-based translation helps translate programs that direct translation\nstruggles to handle. Nonetheless, the effectiveness, advantages, and\nlimitations of this approach remain underexplored. To bridge this gap, we\npresent an empirical study on pseudocode-based code translation, aiming to\ninvestigate its effectiveness in enhancing the direct translation approach,\nilluminate its effective usage, and identify limitations hindering its\npotential benefits. By comparing direct and pseudocode-based translation\napproaches on 9,690 translation tasks across six PLs with five popular LLMs, we\ndemonstrate that pseudocode-based translation can effectively complement direct\ntranslation, particularly when translating from flexible to rigid PLs or\ndealing with low-resource Rust. Based on these findings, we suggest adopting\nstrategies that combine the complementary strengths of both approaches to\nenhance code translation accuracy. We also reveal the advantages of\npseudocode-based translation in disentangling translations of complicated\nprograms and mitigating distractions from detailed implementations in original\nprograms, as well as its limitations due to incorrect, incomplete, or ambiguous\npseudocode.", "AI": {"tldr": "This paper empirically shows that generating pseudocode as an intermediate step improves code translation by LLMs, especially for tough translations and low-resource languages, though the method depends on producing sufficiently clear and correct pseudocode. Hybrid strategies leveraging both direct and pseudocode-based approaches are recommended for best results.", "motivation": "Large language models can translate code, but direct code-to-code translation often lacks accuracy. Inspired by intermediate-step methods in other tasks, this paper explores using pseudocode as a bridge to improve translation.", "method": "The authors conducted an empirical study, comparing direct translation with pseudocode-based translation across 9,690 tasks involving six programming languages and five LLMs. They evaluated effectiveness, usage patterns, and limitations.", "result": "Pseudocode-based translation improves code translation, especially when converting from flexible to rigid languages or in low-resource situations (e.g., Rust). It helps in translating complex logic and mitigating implementation distractions, but is limited by the quality of generated pseudocode (e.g., errors, incompleteness, ambiguity).", "conclusion": "Combining direct and pseudocode-based approaches leads to better code translation accuracy. Pseudocode-based methods are notably advantageous in handling complex or low-resource scenarios, despite some limitations."}}
{"id": "2510.00946", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00946", "abs": "https://arxiv.org/abs/2510.00946", "authors": ["Shiza Andleeb", "Brandon Kantorski", "Jeffrey C. Carver"], "title": "ChatGPT in Introductory Programming: Counterbalanced Evaluation of Code Quality, Conceptual Learning, and Student Perceptions", "comment": "Accepted to SIGCITE'25", "summary": "Background: Large language models (LLMs) such as ChatGPT are increasingly\nused in introductory programming courses to provide real-time code generation,\ndebugging, and explanations. While these tools can boost productivity and code\nquality, concerns remain about over-reliance and potential impacts on\nconceptual learning. Objective: To investigate how ChatGPT access affects code\nquality, conceptual understanding, task completion times, and student\nperceptions in a CS1 course. Methods: We conducted a counterbalanced,\nquasi-experimental study in which students alternated between ChatGPT and\nnon-ChatGPT conditions across two programming assignments in C (functions and\nstructures). We evaluated their code submissions using multidimensional\nrubrics, conceptual post-surveys, and task completion time. Results: Students\nwho had access to ChatGPT produced significantly higher rubric scores for code\nquality and completed tasks in less time compared to those without access.\nHowever, gains in conceptual understanding were mixed, lower for the functions\ntopic but higher for the structures topic. Students reported positive\nexperiences with ChatGPT, citing its value for debugging and practice, while\nexpressing concerns about accuracy and long-term skill development.\nConclusions: ChatGPT can enhance code quality and efficiency for novice\nprogrammers, but may not uniformly improve conceptual understanding. Structured\nintegration and complementary instructional strategies are recommended to\nfoster independent problem-solving skills.", "AI": {"tldr": "Using ChatGPT in beginner programming courses improves code quality and speed, but doesn't always help students understand concepts. Students appreciate ChatGPT for practical help but worry about becoming dependent, so teachers should thoughtfully combine it with traditional teaching.", "motivation": "To understand the effects of ChatGPT usage on beginner programmers' code quality, conceptual understanding, time efficiency, and attitudes in an introductory CS course.", "method": "A quasi-experimental, counterbalanced study where students alternated between using and not using ChatGPT for two programming assignments in C. Code quality, conceptual knowledge, completion time, and student feedback were assessed using rubrics, surveys, and task timing.", "result": "Access to ChatGPT led to higher code quality scores and quicker task completion. Conceptual understanding gains were inconsistent: lower for functions, higher for structures. Students liked ChatGPT for support, but worried about accuracy and skill development.", "conclusion": "ChatGPT boosts code quality and productivity for beginners, but does not consistently improve conceptual understanding. It should be used alongside instructional methods that promote independent learning."}}
{"id": "2510.00957", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.00957", "abs": "https://arxiv.org/abs/2510.00957", "authors": ["Shiza Andleeb", "Teo Mendoza", "Lucas Cordova", "Gursimran Walia", "Jeffrey C. Carver"], "title": "Enhancing Software Testing Education: Understanding Where Students Struggle", "comment": "Accepted to SIGCITE'25", "summary": "Effective software testing is critical for producing reliable and secure\nsoftware, yet many computer science students struggle to master the\nfoundational concepts required to construct comprehensive test suites. While\nautomated feedback tools are widely used to support student learning, it\nremains unclear which testing concepts are most frequently misunderstood and\nhow these misunderstandings are reflected in students' test suite revisions.\nThis study examines the specific testing concepts that lead students to make\nineffective changes, those that fail to improve code coverage, during test\nsuite development. Leveraging an automated feedback tool in a senior-level\nsoftware testing course, we analyzed student submissions from two assignments\nto identify prevalent conceptual gaps and patterns of unproductive\nmodification. Our results reveal that decision coverage and exception handling\nare persistent challenges, and that students most often make superficial or\nmethod-level changes that do not enhance coverage. These findings provide\nactionable insights for educators, researchers, and tool designers. By\npinpointing the concepts that most often contribute to poor testing outcomes,\nwe can refine feedback systems, target instruction to address persistent\nmisconceptions, and more effectively support students in developing robust,\nmaintainable test suites.", "AI": {"tldr": "Many students struggle with software testing concepts, especially decision coverage and exception handling. Analysis of assignment revisions in a senior software testing course shows students often make ineffective test suite changes. The insights point to the need for targeted instruction and improved feedback tools to better support student learning.", "motivation": "Many computer science students have difficulty mastering software testing concepts necessary to build strong test suites. There is a need to understand which testing concepts are most frequently misunderstood and how misconceptions show up in students' revision behavior, especially as automated feedback tools are commonly used to support learning.", "method": "The authors analyzed student submissions from two assignments in a senior-level software testing course, where an automated feedback tool was used. The research focused on examining the conceptual gaps and modification patterns when students revise their test suites.", "result": "The study found that decision coverage and exception handling are persistent conceptual challenges for students. Most ineffective test suite revisions involved superficial or method-level changes that did not significantly improve code coverage.", "conclusion": "Educators, researchers, and tool designers can use these findings to refine feedback systems and instructional approaches, specifically targeting persistent misconceptions. Addressing these issues can help students develop stronger and more maintainable test suites."}}
{"id": "2510.01002", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01002", "abs": "https://arxiv.org/abs/2510.01002", "authors": ["Chengran Yang", "Ting Zhang", "Jinfeng Jiang", "Xin Zhou", "Haoye Tian", "Jieke Shi", "Junkai Chen", "Yikun Li", "Eng Lieh Ouh", "Lwin Khin Shar", "David Lo"], "title": "Semantics-Aligned, Curriculum-Driven, and Reasoning-Enhanced Vulnerability Repair Framework", "comment": null, "summary": "Current learning-based Automated Vulnerability Repair (AVR) approaches, while\npromising, often fail to generalize effectively in real-world scenarios. Our\ndiagnostic analysis reveals three fundamental weaknesses in state-of-the-art\nAVR approaches: (1) limited cross-repository generalization, with performance\ndrops on unseen codebases; (2) inability to capture long-range dependencies,\ncausing a performance degradation on complex, multi-hunk repairs; and (3)\nover-reliance on superficial lexical patterns, leading to significant\nperformance drops on vulnerabilities with minor syntactic variations like\nvariable renaming.\n  To address these limitations, we propose SeCuRepair, a semantics-aligned,\ncurriculum-driven, and reasoning-enhanced framework for vulnerability repair.\nAt its core, SeCuRepair adopts a reason-then-edit paradigm, requiring the model\nto articulate why and how a vulnerability should be fixed before generating the\npatch. This explicit reasoning enforces a genuine understanding of repair logic\nrather than superficial memorization of lexical patterns. SeCuRepair also moves\nbeyond traditional supervised fine-tuning and employs semantics-aware\nreinforcement learning, rewarding patches for their syntactic and semantic\nalignment with the oracle patch rather than mere token overlap. Complementing\nthis, a difficulty-aware curriculum progressively trains the model, starting\nwith simple fixes and advancing to complex, multi-hunk coordinated edits.\n  We evaluate SeCuRepair on strict, repository-level splits of BigVul and newly\ncrafted PrimeVul_AVR datasets. SeCuRepair significantly outperforms all\nbaselines, surpassing the best-performing baselines by 34.52% on BigVul and\n31.52% on PrimeVul\\textsubscript{AVR} in terms of CodeBLEU, respectively.\nComprehensive ablation studies further confirm that each component of our\nframework contributes to its final performance.", "AI": {"tldr": "SeCuRepair, a novel curriculum-driven and reasoning-enhanced framework for vulnerability repair, tackles key weaknesses in current AVR models. It outperforms existing methods by over 30% on major benchmarks by promoting semantic understanding and progressive learning.", "motivation": "Current learning-based Automated Vulnerability Repair (AVR) methods show promise but struggle to generalize in real-world software, specifically due to limited cross-repository performance, inability to handle long-range code dependencies, and excessive reliance on simple lexical patterns.", "method": "The authors propose SeCuRepair, a new framework that is semantics-aligned, curriculum-driven, and reasoning-enhanced. SeCuRepair introduces a 'reason-then-edit' approach where the model explains the fix before patching, uses semantics-aware reinforcement learning that rewards patches for syntactic and semantic alignment, and applies a difficulty-aware curriculum to train progressively from simple to complex repairs.", "result": "SeCuRepair is evaluated on repository-level splits of BigVul and PrimeVul_AVR. It achieves significant improvements over baselines, outperforming the best previous methods by 34.52% on BigVul and 31.52% on PrimeVul_AVR (CodeBLEU metric). Ablation studies indicate each framework component is beneficial.", "conclusion": "SeCuRepair effectively addresses major limitations of prior AVR methods, achieving much better generalization, handling complex repairs, and reducing reliance on lexical patterns. The approach offers a promising direction for robust automated vulnerability repair."}}
{"id": "2510.01003", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01003", "abs": "https://arxiv.org/abs/2510.01003", "authors": ["Boshi Wang", "Weijian Xu", "Yunsheng Li", "Mei Gao", "Yujia Xie", "Huan Sun", "Dongdong Chen"], "title": "Improving Code Localization with Repository Memory", "comment": "15 pages, 8 figures", "summary": "Code localization is a fundamental challenge in repository-level software\nengineering tasks such as bug fixing. While existing methods equip language\nagents with comprehensive tools/interfaces to fetch information from the\nrepository, they overlook the critical aspect of memory, where each instance is\ntypically handled from scratch assuming no prior repository knowledge. In\ncontrast, human developers naturally build long-term repository memory, such as\nthe functionality of key modules and associations between various bug types and\ntheir likely fix locations. In this work, we augment language agents with such\nmemory by leveraging a repository's commit history - a rich yet underutilized\nresource that chronicles the codebase's evolution. We introduce tools that\nallow the agent to retrieve from a non-parametric memory encompassing recent\nhistorical commits and linked issues, as well as functionality summaries of\nactively evolving parts of the codebase identified via commit patterns. We\ndemonstrate that augmenting such a memory can significantly improve LocAgent, a\nstate-of-the-art localization framework, on both SWE-bench-verified and the\nmore recent SWE-bench-live benchmarks. Our research contributes towards\ndeveloping agents that can accumulate and leverage past experience for\nlong-horizon tasks, more closely emulating the expertise of human developers.", "AI": {"tldr": "This paper enhances language agents for code localization by enabling them to remember and utilize repository commit history, leading to significant performance gains and making them behave more like experienced human developers.", "motivation": "Code localization remains a key challenge in software engineering, especially for tasks like bug fixing. Existing language agent methods lack memory and therefore cannot build up repository knowledge over time, unlike human developers who rely on long-term memories to be more effective.", "method": "This work augments language agents with memory by using a repository\u2019s commit history, linked issues, and summaries of actively changed code sections. Tools are introduced to let agents retrieve and utilize this historical information, forming a non-parametric memory of recent commits and functionality changes, recognized through commit patterns.", "result": "Augmenting memory in this manner significantly improves the performance of LocAgent, a state-of-the-art localization framework, on benchmarks including SWE-bench-verified and SWE-bench-live.", "conclusion": "Integrating long-term, repository-specific memory into language agents makes them more capable at code localization tasks and allows them to more closely emulate the expertise of human developers, especially for long-duration tasks."}}
{"id": "2510.01024", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01024", "abs": "https://arxiv.org/abs/2510.01024", "authors": ["Elvis J\u00fanior", "Alan Valejo", "Jorge Valverde-Rebaza", "V\u00e2nia de Oliveira Neves"], "title": "GenIA-E2ETest: A Generative AI-Based Approach for End-to-End Test Automation", "comment": "Preprint of a paper published at the 39th Brazilian Symposium on\n  Software Engineering (SBES 2025). Please cite the published version:\n  https://sol.sbc.org.br/index.php/sbes/article/view/37006", "summary": "Software testing is essential to ensure system quality, but it remains\ntime-consuming and error-prone when performed manually. Although recent\nadvances in Large Language Models (LLMs) have enabled automated test\ngeneration, most existing solutions focus on unit testing and do not address\nthe challenges of end-to-end (E2E) testing, which validates complete\napplication workflows from user input to final system response. This paper\nintroduces GenIA-E2ETest, which leverages generative AI to generate executable\nE2E test scripts from natural language descriptions automatically. We evaluated\nthe approach on two web applications, assessing completeness, correctness,\nadaptation effort, and robustness. Results were encouraging: the scripts\nachieved an average of 77% for both element metrics, 82% for precision of\nexecution, 85% for execution recall, required minimal manual adjustments\n(average manual modification rate of 10%), and showed consistent performance in\ntypical web scenarios. Although some sensitivity to context-dependent\nnavigation and dynamic content was observed, the findings suggest that\nGenIA-E2ETest is a practical and effective solution to accelerate E2E test\nautomation from natural language, reducing manual effort and broadening access\nto automated testing.", "AI": {"tldr": "Manual E2E software testing is inefficient. GenIA-E2ETest uses generative AI to turn natural language into test scripts for web apps, achieving high correctness and recall with little manual adjustment, making E2E test automation easier and more widespread.", "motivation": "Manual software testing is time-consuming and error-prone. Although LLMs have automated unit test generation, effective solutions for end-to-end (E2E) testing are lacking.", "method": "The authors introduce GenIA-E2ETest, a system that uses generative AI to automatically generate executable E2E test scripts from natural language descriptions. They evaluate their system on two web applications, measuring metrics such as completeness, correctness, precision, recall, manual modification rate, and robustness.", "result": "GenIA-E2ETest achieved an average of 77% in element metrics, 82% in execution precision, and 85% in execution recall, required only about 10% manual modification, and showed robust performance in common web scenarios. Some limitations were noted with context-dependent navigation and dynamic content.", "conclusion": "GenIA-E2ETest demonstrates that generative AI can feasibly and effectively automate E2E web testing from natural language, substantially reducing manual work and making automated testing more accessible."}}
{"id": "2510.01077", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01077", "abs": "https://arxiv.org/abs/2510.01077", "authors": ["Daniele Bifolco", "Guido Annicchiarico", "Pierluigi Barbiero", "Massimiliano Di Penta", "Fiorella Zampetti"], "title": "CodeGenLink: A Tool to Find the Likely Origin and License of Automatically Generated Code", "comment": "Proceedings of the 40th IEEE/ACM International Conference on\n  Automated Software Engineering (ASE 2025), November 16-20 2025, Seoul, South\n  Korea", "summary": "Large Language Models (LLMs) are widely used in software development tasks\nnowadays. Unlike reusing code taken from the Web, for LLMs' generated code,\ndevelopers are concerned about its lack of trustworthiness and possible\ncopyright or licensing violations, due to the lack of code provenance\ninformation. This paper proposes CodeGenLink, a GitHub CoPilot extension for\nVisual Studio Code aimed at (i) suggesting links containing code very similar\nto automatically generated code, and (ii) whenever possible, indicating the\nlicense of the likely origin of the code. CodeGenLink retrieves candidate links\nby combining LLMs with their web search features and then performs similarity\nanalysis between the generated and retrieved code. Preliminary results show\nthat CodeGenLink effectively filters unrelated links via similarity analysis\nand provides licensing information when available. Tool URL:\nhttps://github.com/danielebifolco/CodeGenLink Tool Video:\nhttps://youtu.be/M6nqjBf9_pw", "AI": {"tldr": "CodeGenLink helps developers trace the origins and licenses of code generated by LLMs by linking it to similar web code and filtering out unrelated sources, thus improving trust and legal safety.", "motivation": "Developers are increasingly using LLMs for automated code generation, but they worry about trustworthiness, copyright, and the lack of provenance and licensing information compared to code sourced from the Web.", "method": "The paper proposes CodeGenLink, a GitHub CoPilot extension for Visual Studio Code. It suggests URLs with code similar to generated snippets, and provides the likely license info. It combines LLM-powered searches with similarity analysis between generated and retrieved code.", "result": "Preliminary results show CodeGenLink filters out unrelated URLs effectively and, when possible, supplies license details for the detected code origins.", "conclusion": "CodeGenLink addresses developers' concerns by linking generated code to its probable web origin and providing license info, increasing the trust and traceability of LLM-generated code."}}
{"id": "2510.01096", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01096", "abs": "https://arxiv.org/abs/2510.01096", "authors": ["Nathan Wintersgill", "Trevor Stalnaker", "Daniel Otten", "Laura A. Heymann", "Oscar Chaparro", "Massimiliano Di Penta", "Daniel M. German", "Denys Poshyvanyk"], "title": "Developers' Perspectives on Software Licensing: Current Practices, Challenges, and Tools", "comment": null, "summary": "Most modern software products incorporate open-source components, requiring\ndevelopment teams to maintain compliance with each component's licenses.\nNoncompliance can lead to significant financial, legal, and reputational\nrepercussions. While some organizations may seek advice from legal\npractitioners to assist with licensing tasks, developers still play a key role\nin such a process. To this end, it is essential to understand how developers\napproach license compliance tasks, the challenges they encounter, and the tools\nthat they use. This work studies these aspects of software licensing practices\nthrough a study - conducted by a joint team of software engineering and legal\nresearchers - consisting of a survey with 58 software developers and seven\nfollow-up interviews. The study resulted in 15 key findings regarding the\ncurrent state of practice. We discuss the implications of our findings and\noffer directions for future research as well as actionable recommendations for\nlicensing tools.", "AI": {"tldr": "Through surveys and interviews, this study identifies major challenges and practices in software license compliance, providing actionable insights and suggestions for improving compliance tools and guiding future research.", "motivation": "Modern software heavily relies on open-source components, requiring developers to ensure compliance with varied licenses. Noncompliance can result in serious legal, financial, and reputational consequences, highlighting the importance of understanding developer practices in managing licensing.", "method": "A joint team of software engineering and legal researchers conducted a survey with 58 developers, followed by seven in-depth interviews, to examine current practices, challenges, and tool usage in license compliance.", "result": "The research produced 15 key findings about current practices, challenges, and tool use in software license compliance.", "conclusion": "The paper discusses the significance of these findings, offers recommendations for improving licensing tools, and proposes future research directions."}}
{"id": "2510.01182", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01182", "abs": "https://arxiv.org/abs/2510.01182", "authors": ["Shuqing Li", "Chenran Zhang", "Binchang Li", "Cuiyun Gao", "Michael R. Lyu"], "title": "When Shared Worlds Break: Demystifying Defects in Multi-User Extended Reality Software Systems", "comment": null, "summary": "Multi-user Extended Reality (XR) systems enable transformative shared\nexperiences but introduce unique software defects that compromise user\nexperience. Understanding software defects in multi-user XR systems is crucial\nfor enhancing system reliability, yet remains underexplored. To fill the gap,\nthis paper presents the first large-scale empirical study of multi-user XR\ndefects, analyzing 2,649 real-world bug reports from diverse sources, including\ndeveloper forums, GitHub repositories, and app reviews on mainstream XR app\nstores. Through rigorous qualitative analysis using iterative open coding, we\ndevelop a comprehensive taxonomy that classifies multi-user XR bugs along three\ndimensions: Symptom Manifestation, Root Cause Origin, and Consequence Severity.\nOur findings reveal that synchronization inconsistencies and avatar-related\nanomalies are the most prevalent symptoms, while network/synchronization logic\ndefects and session management flaws emerge as dominant root causes.\nCritically, over 34% of analyzed bugs lead to severe consequences that\nfundamentally break the shared experience, including system crashes, persistent\ndisconnections, and complete interaction breakdowns, etc. We also identify\nconcerning privacy and health implications unique to multi-user XR contexts.\nBased on our findings of defect analysis, we provide actionable recommendations\nfor developers, platform vendors, and researchers. Our results demonstrate that\nmulti-user XR systems face distinct challenges at the intersection of\ndistributed systems, real-time 3D interaction, and immersive experiences,\nnecessitating specialized approaches to testing, debugging, and quality\nassurance.", "AI": {"tldr": "This paper presents a first-of-its-kind empirical study of 2,649 multi-user XR bug reports, categorizing XR defects, highlighting major issues like sync failures and avatar glitches, revealing substantial system-breaking consequences, and offering guidance for more reliable XR experiences.", "motivation": "Multi-user XR systems enable shared immersive experiences but suffer from unique software defects that impact user reliability and experience. Understanding these defects is necessary but currently under-researched.", "method": "The authors conducted a large-scale empirical study by analyzing 2,649 real-world bug reports from developer forums, GitHub repositories, and mainstream XR app store reviews. They used qualitative methods with iterative open coding to develop a taxonomy categorizing XR bugs along three axes: symptoms, root causes, and severity.", "result": "The study revealed that synchronization inconsistencies and avatar-related anomalies are the most common defects. Network/synchronization and session management flaws are the primary root causes. Over one-third of bugs have severe consequences that disrupt shared XR experiences, with additional concerns regarding privacy and health.", "conclusion": "Multi-user XR systems encounter unique and severe software defects predominantly in synchronization and session management. The paper offers actionable recommendations for stakeholders and emphasizes the need for specialized testing and quality assurance methods for XR systems."}}
