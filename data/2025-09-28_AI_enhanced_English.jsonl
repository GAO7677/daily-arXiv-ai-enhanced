{"id": "2509.20426", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20426", "abs": "https://arxiv.org/abs/2509.20426", "authors": ["Mahmoud Samir Fayed"], "title": "Dual-Language General-Purpose Self-Hosted Visual Language and new Textual Programming Language for Applications", "comment": "PhD thesis", "summary": "Most visual programming languages (VPLs) are domain-specific, with few\ngeneral-purpose VPLs like Programming Without Coding Technology (PWCT). These\ngeneral-purpose VPLs are developed using textual programming languages and\nimproving them requires textual programming. In this thesis, we designed and\ndeveloped PWCT2, a dual-language (Arabic/English), general-purpose,\nself-hosting visual programming language. Before doing so, we specifically\ndesigned a textual programming language called Ring for its development. Ring\nis a dynamically typed language with a lightweight implementation, offering\nsyntax customization features. It permits the creation of domain-specific\nlanguages through new features that extend object-oriented programming,\nallowing for specialized languages resembling Cascading Style Sheets (CSS) or\nSupernova language. The Ring Compiler and Virtual Machine are designed using\nthe PWCT visual programming language where the visual implementation is\ncomposed of 18,945 components that generate 24,743 lines of C code, which\nincreases the abstraction level and hides unnecessary details. Using PWCT to\ndevelop Ring allowed us to realize several issues in PWCT, which led to the\ndevelopment of the PWCT2 visual programming language using the Ring textual\nprogramming language. PWCT2 provides approximately 36 times faster code\ngeneration and requires 20 times less storage for visual source files. It also\nallows for the conversion of Ring code into visual code, enabling the creation\nof a self-hosting VPL that can be developed using itself. PWCT2 consists of\napproximately 92,000 lines of Ring code and comes with 394 visual components.\nPWCT2 is distributed to many users through the Steam platform and has received\npositive feedback, On Steam, 1772 users have launched the software, and the\ntotal recorded usage time exceeds 17,000 hours, encouraging further research\nand development.", "AI": {"tldr": "The paper introduces PWCT2, a dual-language, general-purpose self-hosting visual programming language developed with and for easy future extension. Using the newly developed Ring textual language, PWCT2 is shown to be dramatically more efficient and space-saving compared to its predecessor. User feedback from a broad public release demonstrates its practicality and appeal, pushing VPLs closer to mainstream adoption.", "motivation": "Most existing visual programming languages (VPLs) are domain-specific and are developed or improved using textual programming, presenting a barrier to non-programmers. There is a need for a more accessible, general-purpose, and self-sustaining VPL.", "method": "The researchers first designed a new textual programming language called Ring, characterized by dynamic typing, lightweight implementation, and extensive syntax customization. Then, they used the original PWCT visual language to develop the Ring compiler and virtual machine, and subsequently developed PWCT2 (a self-hosting VPL) using the Ring language.", "result": "PWCT2 achieved approximately 36 times faster code generation and required 20 times less storage for visual source files compared to its predecessor. It includes a mechanism to convert Ring code to visual code (self-hosting). PWCT2 consists of around 92,000 lines of Ring code and 394 visual components. It has been distributed on Steam, launched by 1,772 users with over 17,000 total hours recorded, and received positive user feedback.", "conclusion": "PWCT2 provides a significant advancement in general-purpose, self-hosting visual programming languages, making development easier and more accessible. The integration of the Ring language and the move towards self-sustainability demonstrate major improvements in VPL capabilities, efficiency, and user adoption."}}
{"id": "2509.20534", "categories": ["cs.PL", "cs.SC", "68W30", "I.1.1; G.4"], "pdf": "https://arxiv.org/pdf/2509.20534", "abs": "https://arxiv.org/abs/2509.20534", "authors": ["Bowen Zhu", "Aayush Sabharwal", "Songchen Tan", "Yingbo Ma", "Alan Edelman", "Christopher Rackauckas"], "title": "Efficient Symbolic Computation vis Hash Consing", "comment": null, "summary": "Symbolic computation systems suffer from memory inefficiencies due to\nredundant storage of structurally identical subexpressions, commonly known as\nexpression swell, which degrades performance in both classical computer algebra\nand emerging AI-driven mathematical reasoning tools. In this paper, we present\nthe first integration of hash consing into JuliaSymbolics, a high-performance\nsymbolic toolkit in Julia, by employing a global weak-reference hash table that\ncanonicalizes expressions and eliminates duplication. This approach reduces\nmemory consumption and accelerates key operations such as differentiation,\nsimplification, and code generation, while seamlessly integrating with Julia's\nmetaprogramming and just-in-time compilation infrastructure. Benchmark\nevaluations across different computational domains reveal substantial\nimprovements: symbolic computations are accelerated by up to 3.2 times, memory\nusage is reduced by up to 2 times, code generation is up to 5 times faster,\nfunction compilation up to 10 times faster, and numerical evaluation up to 100\ntimes faster for larger models. While certain workloads with fewer duplicate\nunknown-variable expressions show more modest gains or even slight overhead in\ninitial computation stages, downstream processing consistently benefits\nsignificantly. These findings underscore the importance of hash consing in\nscaling symbolic computation and pave the way for future work integrating hash\nconsing with e-graphs for enhanced equivalence-aware expression sharing in\nAI-driven pipelines.", "AI": {"tldr": "The paper shows that introducing hash consing to JuliaSymbolics significantly cuts memory use and speeds up symbolic computation tasks, helping both classic algebra and AI systems run more efficiently by avoiding redundant subexpression storage.", "motivation": "Symbolic computation systems often struggle with 'expression swell'\u2014the redundant storage of structurally identical subexpressions, which leads to inefficiencies in memory usage and slows down performance. This is a significant problem in both traditional computer algebra systems and new AI-driven mathematical reasoning tools.", "method": "The paper introduces the integration of hash consing into JuliaSymbolics, a symbolic computation library in Julia. By using a global weak-reference hash table, the system canonicalizes expressions and removes duplicates, ensuring only unique subexpressions are stored in memory. This method seamlessly supports Julia\u2019s metaprogramming and just-in-time compilation features.", "result": "Benchmarking across diverse computational domains shows substantial performance improvements: up to 3.2x faster symbolic computation, 2x less memory used, up to 5x faster code generation, 10x faster function compilation, and up to 100x faster numerical evaluation on larger models. While scenarios with fewer duplicate expressions may see smaller benefits or slight overheads initially, later computation stages still realize significant gains.", "conclusion": "Incorporating hash consing into JuliaSymbolics meaningfully reduces memory usage and accelerates many core symbolic operations. This strengthens the scalability of symbolic computations and lays groundwork for further enhancements\u2014such as integrating with e-graphs to enable smarter expression sharing in AI applications."}}
{"id": "2509.20380", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.20380", "abs": "https://arxiv.org/abs/2509.20380", "authors": ["Samyak Jhaveri", "Vanessa Klotzmann", "Crista Lopes"], "title": "ACCeLLiuM: Supervised Fine-Tuning for Automated OpenACC Pragma Generation", "comment": null, "summary": "The increasing ubiquity of GPUs is accompanied by the increasing complexity\nof their hardware and parallel programming frameworks. Directive-based parallel\nprogramming standards like OpenACC simplify GPU programming to some extent by\nabstracting away low-level complexities, but a fair amount of expertise is\nstill required in order to use those directives effectively.\n  We introduce ACCeLLiuM, two open weights Large Language Models specifically\nfine-tuned for generating expert OpenACC directives for data-parallel loops,\nalong with the supervised fine-tuning dataset that was used to train them. The\nACCeLLiuM SFT dataset contains 4,033 OpenACC pragma-loop pairs mined from\npublic GitHub C/C++ repositories, with 3,223 pairs for training and 810 for\ntesting. Experimental evaluations show a pronounced performance gap in\ngenerating correct OpenACC pragmas between base LLMs and our fine-tuned\nversions. On the held-out test set, base LLMs fail to consistently generate\nvalid pragmas, whereas LLMs fine-tuned on the ACCeLLiuM dataset generate valid\npragmas with the correct directive type for $87\\%$ of the data-parallel loops,\nand exact pragmas--including directives, clauses, clause order, and clause\nvariables--for $50\\%$ of the cases. Even when not exact, generated pragmas\nfrequently incorporate the correct clauses in a different order than the\nground-truth label, or include additional clauses that enable finer control\nover parallel execution, data movement, and concurrency, offering practical\nvalue beyond strict string-matching. By publicly releasing the code, models,\nand dataset as ACCeLLiuM we hope to establish a reproducible benchmark for\nLLM-powered OpenACC pragma generation, and lower the barrier to automated GPU\noffloading of serially written programs.", "AI": {"tldr": "ACCeLLiuM introduces fine-tuned large language models for generating accurate OpenACC GPU directives. These outperform base LLMs, correctly producing valid pragmas in most cases. Dataset, models, and code are publicly released to foster reproducible research and lower GPU programming barriers.", "motivation": "GPU programming is becoming widespread but remains complex. Directive-based frameworks like OpenACC help, but considerable expertise is still needed to use these directives correctly.", "method": "The authors introduce ACCeLLiuM, two large language models (open weights) specifically fine-tuned for generating expert-level OpenACC directives for data-parallel loops. They created and publicly released a supervised fine-tuning dataset (ACCeLLiuM SFT dataset) consisting of 4,033 OpenACC pragma-loop pairs sourced from GitHub C/C++ repositories.", "result": "Fine-tuned LLMs on the ACCeLLiuM dataset outperform base LLMs in generating valid OpenACC pragmas. On the test set, fine-tuned models correctly generate pragmas for 87% of data-parallel loops, with exact matches (directives, clauses, order, variables) in 50% of cases.", "conclusion": "ACCeLLiuM LLMs significantly improve automated generation of OpenACC pragmas, thereby lowering the barrier for GPU offloading in serial programs. The public release of code, models, and dataset aims to establish a reproducible benchmark for LLM-powered OpenACC pragma generation."}}
{"id": "2509.20518", "categories": ["cs.SE", "cs.PL", "D.2.3"], "pdf": "https://arxiv.org/pdf/2509.20518", "abs": "https://arxiv.org/abs/2509.20518", "authors": ["Sayed Mahbub Hasan Amiri", "Md Mainul Islam"], "title": "Enhancing Python Programming Education with an AI-Powered Code Helper: Design, Implementation, and Impact", "comment": "20 pages, 16 figures", "summary": "This is the study that presents an AI-Python-based chatbot that helps\nstudents to learn programming by demonstrating solutions to such problems as\ndebugging errors, solving syntax problems or converting abstract theoretical\nconcepts to practical implementations. Traditional coding tools like Integrated\nDevelopment Environments (IDEs) and static analyzers do not give robotic help\nwhile AI-driven code assistants such as GitHub Copilot focus on getting things\ndone. To close this gap, our chatbot combines static code analysis, dynamic\nexecution tracing, and large language models (LLMs) to provide the students\nwith relevant and practical advice, hence promoting the learning process. The\nchatbots hybrid architecture employs CodeLlama for code embedding, GPT-4 for\nnatural language interactions, and Docker-based sandboxing for secure\nexecution. Evaluated through a mixed-methods approach involving 1,500 student\nsubmissions, the system demonstrated an 85% error resolution success rate,\noutperforming standalone tools like pylint (62%) and GPT-4 (73%). Quantitative\nresults revealed a 59.3% reduction in debugging time among users, with pre- and\npost-test assessments showing a 34% improvement in coding proficiency,\nparticularly in recursion and exception handling. Qualitative feedback from 120\nstudents highlighted the chatbots clarity, accessibility, and\nconfidence-building impact, though critiques included occasional latency and\nrestrictive code sanitization. By balancing technical innovation with\npedagogical empathy, this research provides a blueprint for AI tools that\nprioritize educational equity and long-term skill retention over mere code\ncompletion. The chatbot exemplifies how AI can augment human instruction,\nfostering deeper conceptual understanding in programming education.", "AI": {"tldr": "An AI-powered chatbot using CodeLlama and GPT-4 was developed to help students with programming, especially debugging and understanding concepts. It outperformed existing tools, improved coding proficiency and speed, and was positively reviewed by students. This showcases how AI can be effectively used in education to foster deeper learning.", "motivation": "Traditional coding tools and existing AI-driven assistants lack interactive, learning-focused support for students\u2014IDEs/static analyzers are non-robotic, and tools like GitHub Copilot focus on code completion rather than learning. The paper aims to fill this gap by creating an AI chatbot that assists in debugging, syntax issues, and conceptual translation, supporting student learning rather than just task completion.", "method": "The method involves integrating static code analysis, dynamic execution tracing, and large language models (LLMs) in a hybrid architecture. CodeLlama is used for code embedding, GPT-4 handles natural language interaction, and a Docker-based sandbox ensures secure code execution. The system is evaluated with mixed methods: performance on 1,500 student submissions, quantitative analysis of debugging time and proficiency, and qualitative feedback from 120 students.", "result": "The chatbot achieved an 85% error resolution success rate, outperforming tools like pylint (62%) or standalone GPT-4 (73%). Students using the chatbot reduced debugging time by 59.3% and saw a 34% improvement in coding proficiency, particularly in recursion and exception handling. Qualitative feedback praised the chatbot's clarity and confidence-building, despite occasional latency and restrictive sanitization.", "conclusion": "This research shows that an AI chatbot designed with both technical innovation and pedagogical sensitivity can significantly improve programming education, promoting conceptual understanding and educational equity instead of just code completion. The system demonstrates how AI can successfully augment human instruction."}}
{"id": "2509.20385", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20385", "abs": "https://arxiv.org/abs/2509.20385", "authors": ["Ishara Devendra", "Chaman Wijesiriwardana", "Prasad Wimalaratne"], "title": "State-of-the-Art in Software Security Visualization: A Systematic Review", "comment": null, "summary": "Software security visualization is an interdisciplinary field that combines\nthe technical complexity of cybersecurity, including threat intelligence and\ncompliance monitoring, with visual analytics, transforming complex security\ndata into easily digestible visual formats. As software systems get more\ncomplex and the threat landscape evolves, traditional text-based and numerical\nmethods for analyzing and interpreting security concerns become increasingly\nineffective. The purpose of this paper is to systematically review existing\nresearch and create a comprehensive taxonomy of software security visualization\ntechniques through literature, categorizing these techniques into four types:\ngraph-based, notation-based, matrix-based, and metaphor-based visualization.\nThis systematic review explores over 60 recent key research papers in software\nsecurity visualization, highlighting its key issues, recent advancements, and\nprospective future research directions. From the comprehensive analysis, the\ntwo main areas were distinctly highlighted as extensive software development\nvisualization, focusing on advanced methods for depicting software\narchitecture: operational security visualization and cybersecurity\nvisualization. The findings highlight the necessity for innovative\nvisualization techniques that adapt to the evolving security landscape, with\npractical implications for enhancing threat detection, improving security\nresponse strategies, and guiding future research.", "AI": {"tldr": "As cybersecurity challenges grow, traditional security analysis methods are less effective. This paper systematically reviews recent research, categorizes visualization techniques, and defines key trends in software security visualization. Its findings spotlight the urgent need for new, adaptable visualization methods to enhance threat detection and security management.", "motivation": "Traditional methods of analyzing software security\u2014text-based and numerical approaches\u2014are increasingly ineffective as software systems and threats become more complex. There is a need for more accessible and effective ways to interpret and act on security data.", "method": "This paper conducts a systematic review of over 60 recent research papers related to software security visualization and develops a taxonomy categorizing existing visualization techniques into four types: graph-based, notation-based, matrix-based, and metaphor-based.", "result": "The review underscores the evolution of software security visualization, identifies the strengths and weaknesses of various techniques, and structures the field into two primary areas: software development visualization (focusing on architectural depiction) and cybersecurity visualization (focusing on operational and threat-focused aspects). The study also highlights the necessity for continued innovation in visualization methods to keep pace with evolving security threats.", "conclusion": "Innovative and adaptive visualization techniques are essential for effective software security management. The taxonomy and trends identified in this systematic review provide practical insights for improving threat detection, optimizing security responses, and guiding future research in this dynamic field."}}
{"id": "2509.20386", "categories": ["cs.SE", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.20386", "abs": "https://arxiv.org/abs/2509.20386", "authors": ["Nishant Gaurav", "Adit Akarsh", "Ankit Ranjan", "Manoj Bajaj"], "title": "Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments", "comment": null, "summary": "We present Dynamic ReAct, a novel approach for enabling ReAct agents to ef-\nficiently operate with extensive Model Control Protocol (MCP) tool sets that\nexceed the contextual memory limitations of large language models. Our approach\naddresses the fundamental challenge of tool selection in environments\ncontaining hundreds or thousands of available tools, where loading all tools\nsimultaneously is computationally infeasible. We propose and evaluate five\ndistinct architectures that progressively refine the tool selection process,\nculminating in a search-and-load mechanism that achieves intelligent tool\nselection with minimal computational overhead. Our experimental results\ndemonstrate that the proposed approach reduces tool loading by up to 50% while\nmaintaining task completion accuracy, advancing the path towards truly\ngeneral-purpose AI agents capable of dynamically adapting to diverse task\nenvironments.", "AI": {"tldr": "Dynamic ReAct introduces a set of architectures that allow AI agents to intelligently manage and select from large toolsets, reducing resource usage while preserving performance.", "motivation": "The motivation is to solve the problem faced by ReAct agents when dealing with numerous tools in Model Control Protocol environments, where loading every tool is computationally infeasible due to the memory limitations of large language models.", "method": "Five distinct architectures are proposed and evaluated, culminating in a search-and-load mechanism for dynamic and efficient tool selection.", "result": "The approach reduces tool loading by up to 50% while maintaining task completion accuracy.", "conclusion": "The work advances general-purpose AI agents that can dynamically adapt to diverse task environments, demonstrating effective scalable tool selection."}}
{"id": "2509.20387", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20387", "abs": "https://arxiv.org/abs/2509.20387", "authors": ["Qusai Ramadan", "Jukka Ruohonen", "Abhishek Tiwari", "Adam Alami", "Zeyd Boukhers"], "title": "Towards Systematic Specification and Verification of Fairness Requirements: A Position Paper", "comment": "Accepted at the 2025 IEEE 33rd International Requirements Engineering\n  Conference Workshops", "summary": "Decisions suggested by improperly designed software systems might be prone to\ndiscriminate against people based on protected characteristics, such as gender\nand ethnicity. Previous studies attribute such undesired behavior to flaws in\nalgorithmic design or biased data. However, these studies ignore that\ndiscrimination is often the result of a lack of well-specified fairness\nrequirements and their verification. The fact that experts' knowledge about\nfairness is often implicit makes the task of specifying precise and verifiable\nfairness requirements difficult. In related domains, such as security\nengineering, knowledge graphs have been proven to be effective in formalizing\nknowledge to assist requirements specification and verification. To address the\nlack of formal mechanisms for specifying and verifying fairness requirements,\nwe propose the development of a knowledge graph-based framework for fairness.\nIn this paper, we discuss the challenges, research questions, and a road map\ntowards addressing the research questions.", "AI": {"tldr": "Current approaches to fairness in software overlook requirement specification and verification. This paper proposes using knowledge graphs to formalize fairness requirements, discusses related challenges, and presents a plan for future research.", "motivation": "Software systems can inadvertently discriminate against people based on protected attributes like gender and ethnicity. Prior research focuses on biased algorithms or data but overlooks the absence of well-specified fairness requirements and their verification. Experts' knowledge about fairness is often implicit, complicating the specification and verification process.", "method": "The paper proposes the development of a knowledge graph-based framework to formalize fairness requirements. It suggests leveraging methodologies from related areas, such as security engineering, where knowledge graphs assist in formalizing and verifying requirements.", "result": "The main result is the identification of challenges and research questions around specifying and verifying fairness requirements using knowledge graphs. The paper outlines a road map for future research but does not present experimental results.", "conclusion": "A knowledge graph-based framework is a promising approach to specify and verify fairness requirements in software systems, addressing shortcomings in current practices."}}
{"id": "2509.20415", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20415", "abs": "https://arxiv.org/abs/2509.20415", "authors": ["Yu Pan", "Xiaocheng Li", "Hanzhao Wang"], "title": "Online-Optimized RAG for Tool Use and Function Calling", "comment": null, "summary": "In many applications, retrieval-augmented generation (RAG) drives tool use\nand function calling by embedding the (user) queries and matching them to\npre-specified tool/function descriptions. In this paper, we address an\nembedding misalignment issue that often arises in practical applications due to\nimperfect embedding models or noisy descriptions; such misalignment may lead to\nincorrect retrieval and task failure. We introduce Online-Optimized RAG, a\ndeployment-time framework that continually adapts retrieval embeddings from\nlive interactions using minimal feedback (e.g., task success). Online-Optimized\nRAG applies lightweight online gradient updates with negligible per-query\nlatency and requires no changes to the underlying LLM. The method is\nplug-and-play: it supports both single- and multi-hop tool use, dynamic tool\ninventories, and $K$-retrieval with re-ranking. We provide a problem-dependent\ntheoretical analysis that quantifies how the method's performance depends on\nthe initialization quality of the embeddings and other related quantities.\nAcross diverse tool-use and document-retrieval scenarios, our Online-Optimized\nRAG consistently improves tool selection accuracy and end-task success, thus\nproviding a simple, practical path to robust, self-improving RAG systems.", "AI": {"tldr": "Embedding misalignment in retrieval-augmented generation systems causes retrieval errors and task failures. This paper proposes Online-Optimized RAG, which updates embeddings in real time with minimal feedback, boosting tool selection accuracy and overall performance in diverse applications, without modifying the underlying LLM.", "motivation": "Retrieval-augmented generation (RAG) systems are commonly used for tool use and function calling by embedding user queries and matching them to tool descriptions. However, in practice, embedding misalignment caused by imperfect models or noisy descriptions can lead to incorrect retrieval results and task failures.", "method": "The paper introduces Online-Optimized RAG, a deployment-time framework that adapts retrieval embeddings using minimal feedback from live interactions (such as task success). The system applies lightweight online gradient updates with negligible latency, requiring no changes to the underlying language model.", "result": "Across various tool-use and document-retrieval scenarios, Online-Optimized RAG consistently enhances tool selection accuracy and end-task success rates.", "conclusion": "Online-Optimized RAG offers a practical and robust solution for improving retrieval-alignments in RAG systems, is plug-and-play, and enables self-improving tool use without altering the core LLM."}}
{"id": "2509.20421", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20421", "abs": "https://arxiv.org/abs/2509.20421", "authors": ["Reiner H\u00e4hnle", "Cosimo Laneve", "Adele Veschetti"], "title": "Formal Verification of Legal Contracts: A Translation-based Approach", "comment": null, "summary": "Stipula is a domain-specific programming language designed to model legal\ncontracts with enforceable properties, especially those involving asset\ntransfers and obligations. This paper presents a methodology to formally verify\nthe correctness of Stipula contracts through translation into Java code\nannotated with Java Modeling Language specifications. As a verification\nbackend, the deductive verification tool KeY is used. Both, the translation and\nthe verification of partial and total correctness for a large subset of Stipula\ncontracts, those with disjoint cycles, is fully automatic. Our work\ndemonstrates that a general-purpose deductive verification tool can be used\nsuccessfully in a translation approach.", "AI": {"tldr": "The paper introduces a method to automatically verify Stipula legal contracts by translating them into annotated Java code and checking them with the KeY tool, proving this approach works well for many contract types.", "motivation": "Legal contracts, especially involving asset transfers and obligations, require reliable and enforceable modeling tools. There is a need for formal verification methods that ensure the correctness of these contracts.", "method": "The methodology involves translating Stipula contracts into Java code with annotations in the Java Modeling Language. The deductive verification tool KeY is used as the backend to verify the correctness.", "result": "For a large subset of Stipula contracts, specifically those with disjoint cycles, both the translation and verification processes are fully automatic.", "conclusion": "A general-purpose deductive verification tool, such as KeY, can be effectively applied in an automatic translation-then-verification approach to ensure the correctness of domain-specific contract languages like Stipula."}}
