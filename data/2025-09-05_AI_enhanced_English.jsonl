{"id": "2509.03541", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.03541", "abs": "https://arxiv.org/abs/2509.03541", "authors": ["Chong Wang", "Haoning Wu", "Peng Liang", "Maya Daneva", "Marten van Sinderen"], "title": "Towards the Datasets Used in Requirements Engineering of Mobile Apps: Preliminary Findings from a Systematic Mapping Study", "comment": null, "summary": "[Background] Research on requirements engineering (RE) for mobile apps\nemploys datasets formed by app users, developers or vendors. However, little is\nknown about the sources of these datasets in terms of platforms and the RE\nactivities that were researched with the help of the respective datasets.\n[Aims] The goal of this paper is to investigate the state-of-the-art of the\ndatasets of mobile apps used in existing RE research. [Method] We carried out a\nsystematic mapping study by following the guidelines of Kitchenham et al.\n[Results] Based on 43 selected papers, we found that Google Play and Apple App\nStore provide the datasets for more than 90% of published research in RE for\nmobile apps. We also found that the most investigated RE activities - based on\ndatasets, are requirements elicitation and requirements analysis. [Conclusions]\nOur most important conclusions are: (1) there is a growth in the use of\ndatasets for RE research of mobile apps since 2012, (2) the RE knowledge for\nmobile apps might be skewed due to the overuse of Google Play and Apple App\nStore, (3) there are attempts to supplement reviews of apps from repositories\nwith other data sources, (4) there is a need to expand the alternative sources\nand experiments with complimentary use of multiple sources, if the community\nwants more generalizable results. Plus, it is expected to expand the research\non other RE activities, beyond elicitation and analysis.", "AI": {"tldr": "Most mobile app RE research relies on Google Play and Apple App Store datasets, mainly focusing on elicitation and analysis activities. There\u2019s a growing trend to use more diverse data sources, but further efforts are needed for broader and more generalizable findings.", "motivation": "The motivation for this paper is the lack of understanding about the sources of datasets used in requirements engineering (RE) research for mobile apps, specifically regarding the platforms they originate from and the specific RE activities they support.", "method": "The authors conducted a systematic mapping study following the guidelines of Kitchenham et al., analyzing 43 selected research papers to investigate dataset sources and their application to RE activities in mobile app research.", "result": "The study found that over 90% of these studies use datasets from Google Play or Apple App Store. It also revealed that requirements elicitation and analysis are the most researched RE activities using these datasets. There is a recent trend of supplementing app review data with other sources.", "conclusion": "The conclusions drawn are: (1) increasing use of datasets from 2012 onward; (2) potential bias in RE knowledge for mobile apps due to the heavy reliance on Google Play and Apple App Store datasets; (3) attempts to incorporate additional data sources exist, but more are needed; (4) expanding both data sources and focus on diverse RE activities is necessary for more generalizable research outcomes."}}
{"id": "2509.03554", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.03554", "abs": "https://arxiv.org/abs/2509.03554", "authors": ["Cheng-Yang Tsai", "Tzu-Wei Huang", "Jen-Wei Shih", "I-Hsiang Wang", "Yu-Cheng Lin", "Rung-Bin Lin"], "title": "A Multi-stage Error Diagnosis for APB Transaction", "comment": null, "summary": "Functional verification and debugging are critical bottlenecks in modern\nSystem-on-Chip (SoC) design, with manual detection of Advanced Peripheral Bus\n(APB) transaction errors in large Value Change Dump (VCD) files being\ninefficient and error-prone. Addressing the 2025 ICCAD Contest Problem D, this\nstudy proposes an automated error diagnosis framework using a hierarchical\nRandom Forest-based architecture. The multi-stage error diagnosis employs four\npre-trained binary classifiers to sequentially detect Out-of-Range Access,\nAddress Corruption, and Data Corruption errors, prioritizing high-certainty\naddress-related faults before tackling complex data errors to enhance\nefficiency. Experimental results show an overall accuracy of 91.36%, with\nnear-perfect precision and recall for address errors and robust performance for\ndata errors. Although the final results of the ICCAD 2025 CAD Contest are yet\nto be announced as of the submission date, our team achieved first place in the\nbeta stage, highlighting the method's competitive strength. This research\nvalidates the potential of hierarchical machine learning as a powerful\nautomated tool for hardware debugging in Electronic Design Automation (EDA).", "AI": {"tldr": "This paper presents a hierarchical Random Forest-based approach to automate APB error diagnosis in SoC design, achieving high accuracy and top performance in a major CAD contest, proving ML's potential for EDA hardware debugging tasks.", "motivation": "Manual detection of APB transaction errors in large VCD files during SoC design is inefficient and prone to mistakes. Faster, reliable automated methods are needed for verification and debugging.", "method": "A hierarchical Random Forest-based architecture is used, employing four pre-trained binary classifiers in a multi-stage error diagnosis framework. The classifiers sequentially detect Out-of-Range Access, Address Corruption, and Data Corruption errors, focusing first on address errors before data errors for efficiency.", "result": "The proposed method achieves 91.36% accuracy, with near-perfect precision and recall for address-related errors and robust data error detection. The team achieved first place in the beta stage of the ICCAD 2025 CAD Contest.", "conclusion": "Hierarchical machine learning can provide powerful and automated solutions for hardware error diagnosis in EDA, significantly improving efficiency and reliability over manual methods."}}
{"id": "2509.03668", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.03668", "abs": "https://arxiv.org/abs/2509.03668", "authors": ["Matt Rau", "Chris Brown", "John Edwards"], "title": "Parse Tree Tracking Through Time for Programming Process Analysis at Scale", "comment": null, "summary": "Background and Context: Programming process data can be utilized to\nunderstand the processes students use to write computer programming\nassignments. Keystroke- and line-level event logs have been used in the past in\nvarious ways, primarily in high-level descriptive statistics (e.g., timings,\ncharacter deletion rate, etc). Analysis of behavior in context (e.g., how much\ntime students spend working on loops) has been cumbersome because of our\ninability to automatically track high-level code representations, such as\nabstract syntax trees, through time and unparseable states.\n  Objective: Our study has two goals. The first is to design the first\nalgorithm that tracks parse tree nodes through time. Second, we utilize this\nalgorithm to perform a partial replication study of prior work that used manual\ntracking of code representations, as well as other novel analyses of student\nprogramming behavior that can now be done at scale.\n  Method: We use two algorithms presented in this paper to track parse tree\nnodes through time and construct tree representations for unparseable code\nstates. We apply these algorithms to a public keystroke data from student\ncoursework in a 2021 CS1 course and conduct analysis on the resulting parse\ntrees.\n  Findings: We discover newly observable statistics at scale, including that\ncode is deleted at similar rates inside and outside of conditionals and loops,\na third of commented out code is eventually restored, and that frequency with\nwhich students jump around in their code may not be indicative of struggle.\n  Implications: The ability to track parse trees through time opens the door to\nunderstanding new dimensions of student programming, such as best practices of\nstructural development of code over time, quantitative measurement of what\nsyntactic constructs students struggle most with, refactoring behavior, and\nattention shifting within the code.", "AI": {"tldr": "This paper introduces algorithms to automatically track parse tree changes in student code over time, enabling large-scale and detailed analysis of student programming behavior, and unlocks new avenues for measuring code development, difficulties, and habits.", "motivation": "Traditional programming process data focuses mainly on keystrokes, deletions, or high-level descriptive statistics. More contextual behavioral analysis, such as time spent on specific syntactic constructs (loops, conditionals), has been difficult due to the challenge of tracking high-level code representations (parse/abstract syntax trees) over time, especially through unparseable code states.", "method": "The researchers designed two algorithms: one to track parse tree nodes over time and another to create tree representations even when student code is unparseable. They implemented these algorithms on public keystroke data collected from a 2021 CS1 course and conducted various analyses on the resulting parse trees.", "result": "They identified new, previously unobservable statistics, such as similar code deletion rates inside and outside loops/conditionals, about one-third of commented out code is eventually restored, and that frequent code navigation by students may not necessarily indicate struggle.", "conclusion": "Automatic tracking of parse trees over time enables scalable and in-depth analyses of student programming processes, paving the way for understanding structural code development, quantifying student struggles with specific constructs, and analyzing code editing behavior and focus shifts."}}
{"id": "2509.03848", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.03848", "abs": "https://arxiv.org/abs/2509.03848", "authors": ["Rodrigo Oliveira Zacarias", "Rodrigo Pereira dos Santos", "Patricia Lago"], "title": "Towards an Understanding of Developer Experience-Driven Transparency in Software Ecosystems", "comment": "36 pages Submitted to the ACM Transactions on Software Engineering\n  and Methodology. 2025", "summary": "Software ecosystems (SECO) have become a dominant paradigm in the software\nindustry, enabling third-party developers to co-create value through\ncomplementary components and services. While Developer Experience (DX) is\nincreasingly recognized as critical for sustainable SECO, transparency remains\nan underexplored factor shaping how developers perceive and interact with\necosystems. Existing studies acknowledge transparency as essential for trust,\nfairness, and engagement, yet its relationship with DX has not been\nsystematically conceptualized. Hence, this work aims to advance the\nunderstanding of transparency in SECO from a developer-centered perspective. To\nthis end, we propose SECO-TransDX (Transparency in Software Ecosystems from a\nDeveloper Experience Perspective), a conceptual model that introduces the\nnotion of DX-driven transparency. The model identifies 63 interrelated\nconcepts, including conditioning factors, ecosystem procedures, artifacts, and\nrelational dynamics that influence how transparency is perceived and\nconstructed during developer interactions. SECO-TransDX was built upon prior\nresearch and refined through a Delphi study with experts from academia and\nindustry. It offers a structured lens to examine how transparency mediates DX\nacross technical, social, and organizational layers. For researchers, it lays\nthe groundwork for future studies and tool development; for practitioners, it\nsupports the design of trustworthy, developer-centered platforms that improve\ntransparency and foster long-term engagement in SECO.", "AI": {"tldr": "The paper proposes a comprehensive conceptual model (SECO-TransDX) to systematically understand how transparency impacts developer experience in software ecosystems, offering insights for both researchers and practitioners to enhance trust and engagement.", "motivation": "While software ecosystems (SECO) are central to modern software development, the role of transparency in shaping Developer Experience (DX) is not well understood or systematically conceptualized, even though transparency is noted as vital for trust, fairness, and engagement.", "method": "The authors proposed SECO-TransDX, a conceptual model built upon prior research and refined via a Delphi study involving experts from both academia and industry. The model identifies and organizes 63 concepts affecting transparency and developer experience in SECO.", "result": "SECO-TransDX offers a detailed, structured perspective on the factors influencing transparency from a developer experience standpoint, categorizing conditioning factors, ecosystem procedures, artifacts, and relational dynamics. The framework clarifies how transparency operates at technical, social, and organizational layers within SECO.", "conclusion": "The work introduces a robust conceptual model that advances the theoretical foundation of transparency in software ecosystems from a developer-centered view, facilitating future research and practical improvements in platform and tool design, ultimately intended to enhance developer trust and long-term engagement."}}
{"id": "2509.04253", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.04253", "abs": "https://arxiv.org/abs/2509.04253", "authors": ["Siyuan He", "Songlin Jia", "Yuyan Bao", "Tiark Rompf"], "title": "When Lifetimes Liberate: A Type System for Arenas with Higher-Order Reachability Tracking", "comment": null, "summary": "Static resource management in higher-order functional languages remains\nelusive due to tensions between control, expressiveness, and flexibility.\nRegion-based systems [Grossman et al. 2002; Tofte et al. 2001] offer control\nover lifetimes and expressive in-region sharing, but restrict resources to\nlexical scopes. Rust, an instance of ownership types [Clarke et al. 2013],\noffers non-lexical lifetimes and robust safety guarantees, yet its global\ninvariants make common sharing patterns hard to express. Reachability types\n[Wei et al. 2024] enable reasoning about sharing and separation, but lack\npractical tools for controlling resource lifetimes.\n  In this work, we try to unify their strengths. Our solution enables grouping\nresources as arenas for arbitrary sharing and static guarantees of lexically\nscoped lifetimes. Crucially, arenas and lexical lifetimes are not the only\nchoice: users may also manage resources individually, with non-lexical\nlifetimes. Regardless of mode, resources share the same type, preserving the\nhigher-order parametric nature of the language.\n  Obtaining static safety guarantee in a higher-order language with flexible\nsharing is nontrivial. To this end, we propose two new extensions atop\nreachability types [Wei et al. 2024]. First, A<: features a novel\ntwo-dimensional store model to enable coarse-grained reachability tracking for\narbitrarily shared resources within arenas. Building on this, {A}<: establishes\nlexical lifetime control with static guarantees. As the first reachability\nformalism presented for lifetime control, {A}<: avoids the complication of\nflow-sensitive reasoning and retains expressive power and simplicity. Both\ncalculi are formalized and proven type safe in Rocq.", "AI": {"tldr": "The paper introduces a unified static resource management system for higher-order functional languages, drawing on the strengths of regions, ownership, and reachability types. New type extensions allow flexible sharing and lifetime control while preserving type safety and expressiveness.", "motivation": "Effective static resource management in higher-order functional languages is difficult due to the need to balance control, expressiveness, and flexibility. Existing approaches like region-based systems, ownership types (e.g., Rust), and reachability types address parts of the problem, but each has significant limitations\u2014such as restricted lifetimes, inflexible sharing, or lack of practical lifetime control tools.", "method": "The authors unify strengths from region-based systems, ownership types, and reachability types to create a new approach. They introduce a system that allows resources to be grouped into arenas for shared management with lexical lifetimes, or individually managed with non-lexical lifetimes, while preserving type uniformity. The core technical innovations are two extensions to reachability types: (1) A<: with a two-dimensional store for reachability tracking within arenas, and (2) {A}<: that supports lexical lifetime control with static guarantees. Both extensions are formalized and proven type safe in the Rocq language.", "result": "The proposed approach successfully combines flexible resource grouping (through arenas) and individual management, supporting both lexical and non-lexical lifetimes. The new reachability type extensions (A<: and {A}<:) enable expressive yet statically safe resource management in higher-order languages. The calculus is formalized and its type safety is proven.", "conclusion": "By extending reachability types with support for both arenas and lexical lifetimes, and formalizing these in the Rocq language, the authors provide a unified, type-safe foundation for static resource management with both flexibility and expressiveness in higher-order functional languages."}}
{"id": "2509.03875", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.03875", "abs": "https://arxiv.org/abs/2509.03875", "authors": ["Ziyou Jiang", "Mingyang Li", "Guowei Yang", "Lin Shi", "Qing Wang"], "title": "VulRTex: A Reasoning-Guided Approach to Identify Vulnerabilities from Rich-Text Issue Report", "comment": "25 pages, 7 figures, submitting to TOSEM journal", "summary": "Software vulnerabilities exist in open-source software (OSS), and the\ndevelopers who discover these vulnerabilities may submit issue reports (IRs) to\ndescribe their details. Security practitioners need to spend a lot of time\nmanually identifying vulnerability-related IRs from the community, and the time\ngap may be exploited by attackers to harm the system. Previously, researchers\nhave proposed automatic approaches to facilitate identifying these\nvulnerability-related IRs, but these works focus on textual descriptions but\nlack the comprehensive analysis of IR's rich-text information. In this paper,\nwe propose VulRTex, a reasoning-guided approach to identify\nvulnerability-related IRs with their rich-text information. In particular,\nVulRTex first utilizes the reasoning ability of the Large Language Model (LLM)\nto prepare the Vulnerability Reasoning Database with historical IRs. Then, it\nretrieves the relevant cases from the prepared reasoning database to generate\nreasoning guidance, which guides LLM to identify vulnerabilities by reasoning\nanalysis on target IRs' rich-text information. To evaluate the performance of\nVulRTex, we conduct experiments on 973,572 IRs, and the results show that\nVulRTex achieves the highest performance in identifying the\nvulnerability-related IRs and predicting CWE-IDs when the dataset is\nimbalanced, outperforming the best baseline with +11.0% F1, +20.2% AUPRC, and\n+10.5% Macro-F1, and 2x lower time cost than baseline reasoning approaches.\nFurthermore, VulRTex has been applied to identify 30 emerging vulnerabilities\nacross 10 representative OSS projects in 2024's GitHub IRs, and 11 of them are\nsuccessfully assigned CVE-IDs, which illustrates VulRTex's practicality.", "AI": {"tldr": "The paper introduces VulRTex, a reasoning-guided method that leverages large language models to identify vulnerability-related issue reports in open-source software by utilizing rich-text information, surpassing prior methods in accuracy and efficiency and proving practical value in real-world OSS projects.", "motivation": "It is time-consuming for security practitioners to manually identify vulnerability-related issue reports (IRs) in open-source software communities, causing potential security risks due to the delay. Existing automatic solutions focus mainly on simple textual descriptions and overlook the comprehensive analysis of IRs that contain rich-text information.", "method": "VulRTex first uses a large language model (LLM) to build a Vulnerability Reasoning Database from historical issue reports. It then retrieves relevant cases from this database to produce reasoning guidance for the LLM, which uses this guidance to analyze rich-text information in target IRs and accurately identify vulnerabilities.", "result": "VulRTex significantly outperforms the best baseline methods for identifying vulnerability-related IRs, achieving +11.0% higher F1, +20.2% higher AUPRC, and +10.5% higher Macro-F1. It also runs at half the time cost of comparable reasoning approaches and has successfully identified new vulnerabilities in real-world OSS projects, with 11 receiving CVE-IDs.", "conclusion": "VulRTex effectively and efficiently identifies vulnerability-related issue reports in vast and imbalanced open-source repositories, outperforming existing methods both in accuracy and processing speed. Its real-world utility in OSS security is demonstrated by its successful identification and assignment of CVE-IDs to newly emerging vulnerabilities."}}
{"id": "2509.03876", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.03876", "abs": "https://arxiv.org/abs/2509.03876", "authors": ["Xingchu Chen", "Chengwei Liu", "Jialun Cao", "Yang Xiao", "Xinyue Cai", "Yeting Li", "Jingyi Shi", "Tianqi Sun", "Haiming Chen ang Wei Huo"], "title": "Vulnerability-Affected Versions Identification: How Far Are We?", "comment": null, "summary": "Identifying which software versions are affected by a vulnerability is\ncritical for patching, risk mitigation.Despite a growing body of tools, their\nreal-world effectiveness remains unclear due to narrow evaluation scopes often\nlimited to early SZZ variants, outdated techniques, and small or\ncoarse-graineddatasets. In this paper, we present the first comprehensive\nempirical study of vulnerability affected versions identification. We curate a\nhigh quality benchmark of 1,128 real-world C/C++ vulnerabilities and\nsystematically evaluate 12 representative tools from both tracing and matching\nparadigms across four dimensions: effectiveness at both vulnerability and\nversion levels, root causes of false positives and negatives, sensitivity to\npatch characteristics, and ensemble potential. Our findings reveal fundamental\nlimitations: no tool exceeds 45.0% accuracy, with key challenges stemming from\nheuristic dependence, limited semantic reasoning, and rigid matching logic.\nPatch structures such as add-only and cross-file changes further hinder\nperformance. Although ensemble strategies can improve results by up to 10.1%,\noverall accuracy remains below 60.0%, highlighting the need for fundamentally\nnew approaches. Moreover, our study offers actionable insights to guide tool\ndevelopment, combination strategies, and future research in this critical area.\nFinally, we release the replicated code and benchmark on our website to\nencourage future contributions.outdated techniques, and small or coarse grained\ndatasets.", "AI": {"tldr": "Current tools for identifying affected software versions by vulnerabilities are fundamentally limited, achieving less than 45% accuracy individually and below 60% even with ensemble strategies. A new approach is needed, with the authors providing a comprehensive benchmark and actionable insights for the community.", "motivation": "Identifying which software versions are affected by a vulnerability is crucial for patching and risk mitigation. However, existing tools have unclear real-world effectiveness due to limited evaluation scopes, outdated techniques, and small or coarse-grained datasets.", "method": "The authors curate a benchmark of 1,128 real-world C/C++ vulnerabilities and systematically evaluate 12 representative tools from both tracing and matching paradigms. They measure effectiveness at vulnerability and version levels, analyze false positives/negatives, examine patch sensitivity, and explore ensemble strategies.", "result": "No tool exceeds 45.0% accuracy, with most challenges arising from reliance on heuristics, limited semantic reasoning, and rigid matching logic. Patch structures such as add-only and cross-file changes further degrade tool performance. Ensemble approaches can increase accuracy by up to 10.1%, but overall accuracy remains below 60%.", "conclusion": "The study reveals fundamental limitations in current tools for identifying affected software versions. There is a clear need for new approaches, as even the best existing techniques achieve limited accuracy. The released benchmark and insights can guide future research and tool development."}}
{"id": "2509.03896", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.03896", "abs": "https://arxiv.org/abs/2509.03896", "authors": ["Zushuai Zhang", "Elliott Wen", "Ewan Tempero"], "title": "Analyzing Variations in Dependency Distributions Due to Code Smell Interactions", "comment": null, "summary": "The existence of dependencies between modules, such as classes, can mean that\nchanging a module triggers ripple effects that make maintenance complex and\ncostly, so the advice is to minimize dependencies between modules. It is\ntherefore important to understand the circumstances that can lead to increased\ndependencies. Recent studies suggest that code smells, which are\ncharacteristics of code that indicate potential design issues, may interact in\nways that increase dependencies between modules. In this study, we aim to\nconfirm previous observations and investigate whether and how the distribution\nof static dependencies changes in the presence of code smell interactions. We\nconducted a dependency analysis on 116 open-source Java systems to quantify the\ninteractions, comparing interactions among code smells and interactions between\ncode smells and non-code smells. Our results suggest that while interactions\nbetween code smell pairs are associated with increases in certain dependencies\nand decreases in others, overall, they are associated with an increase in total\ndependencies. For example, the median number of dependencies between Feature\nEnvy methods and Data Classes is seven times as many as when the methods are\nnon-Feature Envy methods, increasing from 1 to 7. This implies that developers\nshould prioritize addressing code smells that interact with each other, rather\nthan code smells that exist only in isolation.", "AI": {"tldr": "The study finds that when code smells interact, they substantially increase dependencies between software modules, making maintenance harder. Developers are advised to prioritize fixing interacting code smells over isolated ones.", "motivation": "To understand how code smell interactions affect dependencies, potentially complicating maintenance, and to confirm prior observations.", "method": "Dependency analysis on 116 open-source Java systems, quantifying and comparing dependencies caused by interacting code smells versus individual smells.", "result": "Code smell pairs tend to significantly increase dependencies; for instance, Feature Envy methods interacting with Data Classes show seven times more dependencies than non-interacting cases.", "conclusion": "Interactions between code smells tend to increase overall module dependencies, making software maintenance more complex."}}
{"id": "2509.03900", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.03900", "abs": "https://arxiv.org/abs/2509.03900", "authors": ["Yuvraj Agrawal"], "title": "The Auth Shim: A Lightweight Architectural Pattern for Integrating Enterprise SSO with Standalone Open-Source Applications", "comment": null, "summary": "Open-source software OSS is widely adopted in enterprise settings, but\nstandalone tools often lack native support for protocols like SAML or OIDC,\ncreating a critical security integration gap. This paper introduces and\nformalizes the Auth Shim, a lightweight architectural pattern designed to solve\nthis problem. The Auth Shim is a minimal, external proxy service that acts as a\ncompatibility layer, translating requests from an enterprise Identity Provider\nIdP into the native session management mechanism of a target application. A key\nprerequisite for this pattern is that the target application must expose a\nprogrammatic, secure administrative API. We present a case study of the\npattern's implementation at Adobe to integrate a popular OSS BI tool with Okta\nSAML, which enabled automated Role-Based Access Control RBAC via IAM group\nmapping and eliminated manual user provisioning. By defining its components,\ninteractions, and production deployment considerations, this paper provides a\nreusable, secure, and cost-effective blueprint for integrating any standalone\nOSS tool into an enterprise SSO ecosystem, thereby enabling organizations to\nembrace open-source innovation without compromising on security governance.", "AI": {"tldr": "This paper introduces the Auth Shim, a lightweight external service that enables secure SSO integration for open-source tools lacking built-in SAML/OIDC support. A case study at Adobe shows its practicality, security, and ability to automate user management, providing a blueprint for similar enterprise use cases.", "motivation": "Many open-source software (OSS) tools lack native support for security protocols like SAML or OIDC, making integration with enterprise identity providers challenging and potentially leading to security risks.", "method": "The paper proposes and formalizes the 'Auth Shim,' an external proxy service that translates identity provider authentication into the application's native session management, assuming the application has a secure administrative API. The pattern is illustrated by an implementation case at Adobe.", "result": "The Auth Shim enabled secure, automated single sign-on integration for a popular OSS BI tool and Okta SAML, including automated RBAC and eliminating manual user provision. It demonstrated practical, secure, and scalable integration in production.", "conclusion": "The Auth Shim architectural pattern offers a reusable and secure solution for integrating standalone OSS tools with enterprise SSO systems, addressing key adoption barriers and security integration gaps while enabling greater flexibility and innovation in enterprise environments."}}
{"id": "2509.04078", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04078", "abs": "https://arxiv.org/abs/2509.04078", "authors": ["Jingjing Liu", "Zeming Liu", "Zihao Cheng", "Mengliang He", "Xiaoming Shi", "Yuhang Guo", "Xiangrong Zhu", "Yuanfang Guo", "Yunhong Wang", "Haifeng Wang"], "title": "RepoDebug: Repository-Level Multi-Task and Multi-Language Debugging Evaluation of Large Language Models", "comment": "30 pages, 12 figures, EMNLP 2025 Findings", "summary": "Large Language Models (LLMs) have exhibited significant proficiency in code\ndebugging, especially in automatic program repair, which may substantially\nreduce the time consumption of developers and enhance their efficiency.\nSignificant advancements in debugging datasets have been made to promote the\ndevelopment of code debugging. However, these datasets primarily focus on\nassessing the LLM's function-level code repair capabilities, neglecting the\nmore complex and realistic repository-level scenarios, which leads to an\nincomplete understanding of the LLM's challenges in repository-level debugging.\nWhile several repository-level datasets have been proposed, they often suffer\nfrom limitations such as limited diversity of tasks, languages, and error\ntypes. To mitigate this challenge, this paper introduces RepoDebug, a\nmulti-task and multi-language repository-level code debugging dataset with 22\nsubtypes of errors that supports 8 commonly used programming languages and 3\ndebugging tasks. Furthermore, we conduct evaluation experiments on 10 LLMs,\nwhere Claude 3.5 Sonnect, the best-performing model, still cannot perform well\nin repository-level debugging.", "AI": {"tldr": "Most code debugging datasets for LLMs only test simple cases. This paper presents RepoDebug, a more complex, real-world dataset, showing that even top LLMs cannot yet effectively debug at the repository level.", "motivation": "Existing code debugging datasets using LLMs are limited because they focus mainly on function-level repairs rather than the more complex repository-level scenarios. This leads to an incomplete understanding of LLMs' real-world debugging capabilities. Repository-level datasets already proposed have limitations in task, language, and error diversity.", "method": "The paper introduces RepoDebug, a new dataset for repository-level code debugging. RepoDebug features 22 error subtypes, supports 8 programming languages, and provides 3 distinct debugging tasks. The researchers evaluate 10 large language models using this dataset.", "result": "RepoDebug offers a diverse, multi-language, repository-level dataset that allows broader assessment of LLMs in real-world debugging. Experimental results show that even the best-performing LLM (Claude 3.5 Sonnect) struggles with repository-level debugging tasks.", "conclusion": "Focusing on repository-level rather than function-level debugging reveals persistent LLM deficiencies in real-world debugging. RepoDebug serves as a more comprehensive benchmark, but even advanced models are not yet proficient in repository-level debugging."}}
{"id": "2509.04260", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.04260", "abs": "https://arxiv.org/abs/2509.04260", "authors": ["Haowei Quan", "Junjie Wang", "Xinzhe Li", "Terry Yue Zhuo", "Xiao Chen", "Xiaoning Du"], "title": "An Empirical Study of Vulnerabilities in Python Packages and Their Detection", "comment": null, "summary": "In the rapidly evolving software development landscape, Python stands out for\nits simplicity, versatility, and extensive ecosystem. Python packages, as units\nof organization, reusability, and distribution, have become a pressing concern,\nhighlighted by the considerable number of vulnerability reports. As a scripting\nlanguage, Python often cooperates with other languages for performance or\ninteroperability. This adds complexity to the vulnerabilities inherent to\nPython packages, and the effectiveness of current vulnerability detection tools\nremains underexplored. This paper addresses these gaps by introducing PyVul,\nthe first comprehensive benchmark suite of Python-package vulnerabilities.\nPyVul includes 1,157 publicly reported, developer-verified vulnerabilities,\neach linked to its affected packages. To accommodate diverse detection\ntechniques, it provides annotations at both commit and function levels. An\nLLM-assisted data cleansing method is incorporated to improve label accuracy,\nachieving 100% commit-level and 94% function-level accuracy, establishing PyVul\nas the most precise large-scale Python vulnerability benchmark. We further\ncarry out a distribution analysis of PyVul, which demonstrates that\nvulnerabilities in Python packages involve multiple programming languages and\nexhibit a wide variety of types. Moreover, our analysis reveals that\nmulti-lingual Python packages are potentially more susceptible to\nvulnerabilities. Evaluation of state-of-the-art detectors using this benchmark\nreveals a significant discrepancy between the capabilities of existing tools\nand the demands of effectively identifying real-world security issues in Python\npackages. Additionally, we conduct an empirical review of the top-ranked CWEs\nobserved in Python packages, to diagnose the fine-grained limitations of\ncurrent detection tools and highlight the necessity for future advancements in\nthe field.", "AI": {"tldr": "This paper introduces PyVul, a large, precise benchmark of 1,157 Python package vulnerabilities, showing that current detection tools struggle with real-world issues. PyVul exposes gaps, especially in multi-language scenarios, and will drive future tool improvements.", "motivation": "The proliferation of Python package vulnerabilities, coupled with the complexity gained from multi-language integrations, calls for better benchmarks and analyses of vulnerability detection within Python's ecosystem. Existing tools may not sufficiently address these needs, and there is a lack of comprehensive datasets.", "method": "The paper introduces PyVul, the first comprehensive benchmark suite for Python-package vulnerabilities, containing 1,157 real-world, publicly reported, and developer-verified vulnerabilities at both commit and function levels. The dataset is meticulously annotated, and LLM-assisted data cleansing is used to improve labeling accuracy to 100% at the commit level and 94% at the function level. The benchmark is used to analyze the distribution and types of vulnerabilities and to evaluate state-of-the-art vulnerability detection tools.", "result": "PyVul establishes itself as the most accurate and comprehensive large-scale benchmark for Python package vulnerabilities. The data analysis shows that vulnerabilities are prevalent in multi-lingual packages and exhibit diverse types. Benchmarking state-of-the-art tools against PyVul exposes a significant gap between the tools' detection capabilities and real-world needs. An in-depth empirical review of common weakness enumerations (CWEs) in Python packages identifies specific areas where detection tools fall short.", "conclusion": "PyVul provides a crucial resource for evaluating and advancing security tools for Python packages, making clear the limitations of current solutions and highlighting the need for new approaches to vulnerability detection, particularly in multi-lingual and complex Python packages."}}
{"id": "2509.04328", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.04328", "abs": "https://arxiv.org/abs/2509.04328", "authors": ["Amine Barrak", "Emna Ksontini", "Ridouane Atike", "Fehmi Jaafar"], "title": "FaaSGuard: Secure CI/CD for Serverless Applications -- An OpenFaaS Case Study", "comment": "IEEE International Conference on Source Code Analysis & Manipulation\n  (SCAM 2025)", "summary": "Serverless computing significantly alters software development by abstracting\ninfrastructure management and enabling rapid, modular, event-driven\ndeployments. Despite its benefits, the distinct characteristics of serverless\nfunctions, such as ephemeral execution and fine-grained scalability, pose\nunique security challenges, particularly in open-source platforms like\nOpenFaaS. Existing approaches typically address isolated phases of the\nDevSecOps lifecycle, lacking an integrated and comprehensive security strategy.\nTo bridge this gap, we propose FaaSGuard, a unified DevSecOps pipeline\nexplicitly designed for open-source serverless environments. FaaSGuard\nsystematically embeds lightweight, fail-closed security checks into every stage\nof the development lifecycle-planning, coding, building, deployment, and\nmonitoring-effectively addressing threats such as injection attacks, hard-coded\nsecrets, and resource exhaustion. We validate our approach empirically through\na case study involving 20 real-world serverless functions from public GitHub\nrepositories. Results indicate that FaaSGuard effectively detects and prevents\ncritical vulnerabilities, demonstrating high precision (95%) and recall (91%)\nwithout significant disruption to established CI/CD practices.", "AI": {"tldr": "FaaSGuard is a comprehensive DevSecOps pipeline for open-source serverless platforms like OpenFaaS, automating security checks throughout development. It was empirically tested, showing high precision and recall in detecting and preventing vulnerabilities, with little impact on existing workflows.", "motivation": "Serverless computing, while advantageous, introduces new security risks that are not well covered by current solutions, especially in open-source environments like OpenFaaS. Existing security approaches in DevSecOps are fragmented and insufficiently comprehensive.", "method": "The authors propose FaaSGuard, a unified DevSecOps pipeline tailored for open-source serverless platforms. FaaSGuard inserts lightweight, fail-closed security checks at every phase of the software development lifecycle, from planning to monitoring. Empirical validation through a case study on 20 open-source serverless functions was conducted.", "result": "FaaSGuard successfully detected and prevented critical vulnerabilities, such as injection attacks, hard-coded secrets, and resource exhaustion. It achieved high precision (95%) and recall (91%) during empirical validation, while not causing major disruptions to the CI/CD workflow.", "conclusion": "FaaSGuard offers an effective, comprehensive, and non-disruptive security solution for serverless environments, providing robust protection at all stages of the DevSecOps pipeline in open-source platforms."}}
{"id": "2509.04423", "categories": ["cs.SE", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.04423", "abs": "https://arxiv.org/abs/2509.04423", "authors": ["Fatima Zulfiqar Ali", "Atrooba Ilyas"], "title": "Design and Development of a Web Platform for Blood Donation Management", "comment": "10 pages, 6 figures, conference", "summary": "Blood donation is a critical component of healthcare, yet locating suitable\ndonors in emergencies often presents significant challenges. This paper\npresents the design and development of a Blood Donation Web Platform, a\nweb-based system that connects patients, donors, and administrators within a\ncentralized digital space. The platform allows interested donors to register\ntheir personal information, including blood group, contact details, and\navailability. Patients can search for donors based on blood group and location,\nand the system provides a list of nearby donors who are ready to donate. The\nplatform design was guided by use case, database, class, and sequence diagrams\nto ensure a well-structured and efficient system architecture. Modern web\ntechnologies, including PHP (Laravel framework), HTML, CSS, Bootstrap, and\nMySQL, supported by XAMPP and Visual Studio Code, were employed to implement a\ndynamic, interactive, and user-friendly platform. By streamlining donor\nrefgistration, blood requests, and communication, the proposed system reduces\ndelays and complexities in emergencies, improving timely accessibility of blood\nand enhancing overall efficiency in blood donation services.", "AI": {"tldr": "The paper introduces a web platform that connects patients, blood donors, and administrators to streamline blood donor registration and matching in emergencies, using modern web technologies and a structured system design.", "motivation": "Blood donation is vital for healthcare, but finding suitable donors during emergencies is challenging. There is a need for a system that simplifies and streamlines the process of connecting patients and donors.", "method": "The authors designed and developed a centralized Blood Donation Web Platform using modern web technologies (PHP Laravel, HTML, CSS, Bootstrap, MySQL) and utilized use case, database, class, and sequence diagrams for system architecture. Implementation was done with XAMPP and Visual Studio Code.", "result": "The resulting platform enables easy donor registration, search based on blood group and location, and effective communication between patients, donors, and administrators, reducing delays and complexities.", "conclusion": "The web platform improves accessibility and efficiency of blood donation services, particularly in emergencies."}}
