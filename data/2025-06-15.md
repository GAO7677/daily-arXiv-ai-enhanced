<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 24]
- [cs.PL](#cs.PL) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [TrioXpert: An automated incident management framework for microservice system](https://arxiv.org/abs/2506.10043)
*Yongqian Sun,Yu Luo,Xidao Wen,Yuan Yuan,Xiaohui Nie,Shenglin Zhang,Tong Liu,Xi Luo*

Main category: cs.SE

TL;DR: TrioXpert is a new framework for automated incident management in microservice systems that uses multiple data types and large language models to improve performance and interpretability across multiple tasks, showing strong empirical results.


<details>
  <summary>Details</summary>
Motivation: Existing incident management methods for microservice systems typically utilize only single-modal data (such as metrics, logs, or traces), which limits their ability to simultaneously handle multiple tasks like anomaly detection, failure triage, and root cause localization. Furthermore, these methods often lack clear reasoning evidence, resulting in poor interpretability.

Method: The paper proposes TrioXpert, an end-to-end incident management framework that leverages multimodal data. It features three independent data processing pipelines tailored to different data modalities (numerical and textual), providing a comprehensive view of system operation. A collaborative reasoning mechanism powered by large language models (LLMs) allows the framework to handle multiple tasks concurrently with enhanced interpretability through clear reasoning evidence.

Result: TrioXpert was extensively evaluated on two popular microservice datasets. The results showed significant improvements across all tasks: anomaly detection (improvement of 4.7%–57.7%), failure triage (improvement of 2.1%–40.6%), and root cause localization (improvement of 1.6%–163.1%).

Conclusion: TrioXpert effectively addresses the limitations of single-modal and less interpretable incident management systems. By integrating multimodal data processing and collaborative reasoning through LLMs, TrioXpert achieves superior performance and interpretability in critical incident management tasks for microservice systems.

Abstract: Automated incident management plays a pivotal role in large-scale
microservice systems. However, many existing methods rely solely on
single-modal data (e.g., metrics, logs, and traces) and struggle to
simultaneously address multiple downstream tasks, including anomaly detection
(AD), failure triage (FT), and root cause localization (RCL). Moreover, the
lack of clear reasoning evidence in current techniques often leads to
insufficient interpretability. To address these limitations, we propose
TrioXpert, an end-to-end incident management framework capable of fully
leveraging multimodal data. TrioXpert designs three independent data processing
pipelines based on the inherent characteristics of different modalities,
comprehensively characterizing the operational status of microservice systems
from both numerical and textual dimensions. It employs a collaborative
reasoning mechanism using large language models (LLMs) to simultaneously handle
multiple tasks while providing clear reasoning evidence to ensure strong
interpretability. We conducted extensive evaluations on two popular
microservice system datasets, and the experimental results demonstrate that
TrioXpert achieves outstanding performance in AD (improving by 4.7% to 57.7%),
FT (improving by 2.1% to 40.6%), and RCL (improving by 1.6% to 163.1%) tasks.

</details>


### [2] [Reward Models Enable Scalable Code Verification by Trading Accuracy for Throughput](https://arxiv.org/abs/2506.10056)
*Gabriel Orlanski,Nicholas Roberts,Aws Albarghouthi,Frederic Sala*

Main category: cs.SE

TL;DR: The paper shows that using outcome reward models (ORMs) for faster, rough pruning of incorrect code solutions enables large speed gains with only minor accuracy loss in LLM coding tasks, challenging the assumption that only comprehensive verifiers should be used.


<details>
  <summary>Details</summary>
Motivation: The paper challenges the prevailing belief that comprehensive verifiers should always be prioritized over outcome reward models (ORMs) in program synthesis using large language models (LLMs), with little consideration for the trade-offs involved.

Method: The authors systematically explore the trade-offs between speed and accuracy by evaluating the role of ORMs in the verification and ranking steps of LLM-generated code solutions. They analyze a generate-prune-then-rank approach that employs a fast but less accurate verifier to prune incorrect solutions before ranking.

Result: The study finds that ORMs are essential for scaling verification by trading off some accuracy for significant speed gains. Specifically, their generate-prune-then-rank approach results in an 11.65x speed-up while sacrificing only 8.33% in accuracy compared to using a comprehensive test suite.

Conclusion: ORMs provide substantial benefits for scalable LLM-based program synthesis, especially when integrated as part of a generate-prune-then-rank pipeline, balancing execution speed and accuracy.

Abstract: The standard paradigm for solving coding tasks via large language models
(LLMs) is to generate-then-rank programs, where the latter step uses a verifier
in the ranking process. The growing consensus is that a comprehensive verifier
(e.g., a full test suite) should be prioritized over an outcome reward model
(ORM) whenever possible, with little consideration given to the trade-offs
involved. We aim to challenge this assumption by systematically exploring the
tradeoff between speed and accuracy. We find that ORMs play a crucial role in
scaling verification through trading accuracy for speed, even when a
comprehensive verifier is available. Their value becomes especially apparent
when used in a generate-prune-then-rank approach, where a faster but less
accurate verifier removes incorrect solutions prior to ranking -- leading to a
system that is 11.65x faster while only being 8.33% less accurate than the full
test suite. We analyze the generate-prune-then-rank approach and show that it
works by filtering out incorrect but highly ranked solutions. These findings
enable the design of scalable and accurate program ranking systems.

</details>


### [3] [Online Discovery of Simulation Models for Evolving Business Processes (Extended Version)](https://arxiv.org/abs/2506.10049)
*Francesco Vinci,Gyunam Park,Wil van der Aalst,Massimiliano de Leoni*

Main category: cs.SE

TL;DR: A new real-time simulation model discovery method uses streaming data and machine learning to adapt to ongoing business process changes, yielding more stable and robust simulations than previous techniques.


<details>
  <summary>Details</summary>
Motivation: Organizations are constantly changing their business processes to improve efficiency and customer satisfaction. Existing approaches for discovering business process simulation models do not effectively adapt to ongoing, real-time changes in processes.

Method: The paper introduces a streaming process simulation discovery technique that combines Incremental Process Discovery and Online Machine Learning. The approach emphasizes recent event data while maintaining historical context, allowing the simulation model to evolve with the process.

Result: Experiments on four event logs show that the proposed method produces more stable simulations, better adapts to process changes (concept drift), and balances recent and historical data for accurate simulation.

Conclusion: The proposed technique effectively adapts business process simulation models in real time, leading to robust and stable simulations even as the underlying processes evolve.

Abstract: Business Process Simulation (BPS) refers to techniques designed to replicate
the dynamic behavior of a business process. Many approaches have been proposed
to automatically discover simulation models from historical event logs,
reducing the cost and time to manually design them. However, in dynamic
business environments, organizations continuously refine their processes to
enhance efficiency, reduce costs, and improve customer satisfaction. Existing
techniques to process simulation discovery lack adaptability to real-time
operational changes. In this paper, we propose a streaming process simulation
discovery technique that integrates Incremental Process Discovery with Online
Machine Learning methods. This technique prioritizes recent data while
preserving historical information, ensuring adaptation to evolving process
dynamics. Experiments conducted on four different event logs demonstrate the
importance in simulation of giving more weight to recent data while retaining
historical knowledge. Our technique not only produces more stable simulations
but also exhibits robustness in handling concept drift, as highlighted in one
of the use cases.

</details>


### [4] [Solving Package Management via Hypergraph Dependency Resolution](https://arxiv.org/abs/2506.10803)
*Ryan Gibb,Patrick Ferris,David Allsopp,Michael Winston Dales,Mark Elvers,Thomas Gazagnaire,Sadiq Jaffer,Thomas Leonard,Jon Ludlam,Anil Madhavapeddy*

Main category: cs.SE

TL;DR: This paper proposes HyperRes, a hypergraph-based system for cross-ecosystem dependency resolution, enabling precise and interoperable package management without requiring changes to existing user tooling.


<details>
  <summary>Details</summary>
Motivation: There is a lack of interoperability among package managers across different programming languages and operating systems. This makes it difficult for multi-lingual projects to accurately manage dependencies and for external system and hardware dependencies to be explicitly versioned and maintained.

Method: The authors introduce HyperRes, a formal hypergraph-based system for expressing and resolving versioned dependencies. They define translations from various existing package managers into HyperRes, enabling cross-ecosystem dependency resolution. The solution retains compatibility with users' current package managers by translating packaging metadata and solving dependency constraints specialized for particular environments.

Result: HyperRes successfully models many existing package manager ecosystems and enables cross-ecosystem dependency resolution. The system demonstrates comprehensive compatibility and resolution capabilities without requiring users to abandon their existing tools.

Conclusion: HyperRes provides a rigorous foundation and practical translations that bridge package management systems, allowing precise, versioned, and interoperable dependency management across language and system boundaries.

Abstract: Package managers are everywhere, with seemingly every language and operating
system implementing their own solution. The lack of interoperability between
these systems means that multi-lingual projects are unable to express precise
dependencies across language ecosystems, and external system and hardware
dependencies are typically implicit and unversioned. We define HyperRes, a
formal system for describing versioned dependency resolution using a hypergraph
that is expressive enough to model many ecosystems and solve dependency
constraints across them. We define translations from dozens of existing package
managers to HyperRes and comprehensively demonstrate that dependency resolution
can work across ecosystems that are currently distinct. This does not require
users to shift their choice of package managers; instead, HyperRes allows for
the translation of packaging metadata between ecosystems, and for solving to be
precisely specialised to a particular deployment environment.

</details>


### [5] [The Effects of GitHub Copilot on Computing Students' Programming Effectiveness, Efficiency, and Processes in Brownfield Programming Tasks](https://arxiv.org/abs/2506.10051)
*Md Istiak Hossain Shihab,Christopher Hundhausen,Ahsun Tariq,Summit Haque,Yunhan Qiao,Brian Mulanda*

Main category: cs.SE

TL;DR: GitHub Copilot helps students work faster and achieve more in unfamiliar code bases, but may reduce their understanding of the code, suggesting new educational strategies are needed to balance productivity and learning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand how generative AI coding assistants such as GitHub Copilot impact undergraduate students during brownfield (legacy code base) programming tasks, given that such environments are common in industry but understudied in educational contexts.

Method: The authors conducted a controlled experiment with 10 undergraduate computer science students, who performed similar brownfield programming tasks both with and without GitHub Copilot, using a mixed-methods approach including performance analysis, behavioral analysis, and exit interviews.

Result: With Copilot, students completed tasks 35% faster and made 50% more progress on solutions. They spent 11% less time writing code manually and 12% less time searching the web. However, students were concerned about not understanding the AI-generated suggestions.

Conclusion: GitHub Copilot significantly improves student efficiency and task progress in brownfield programming, but may negatively impact students’ understanding of the code. The findings highlight the need for new teaching approaches that integrate GenAI tools while ensuring students still comprehend and reflect on the code they use.

Abstract: When graduates of computing degree programs enter the software industry, they
will most likely join teams working on legacy code bases developed by people
other than themselves. In these so-called brownfield software development
settings, generative artificial intelligence (GenAI) coding assistants like
GitHub Copilot are rapidly transforming software development practices, yet the
impact of GenAI on student programmers performing brownfield development tasks
remains underexplored. This paper investigates how GitHub Copilot influences
undergraduate students' programming performance, behaviors, and understanding
when completing brownfield programming tasks in which they add new code to an
unfamiliar code base. We conducted a controlled experiment in which 10
undergraduate computer science students completed highly similar brownfield
development tasks with and without Copilot in a legacy web application. Using a
mixed-methods approach combining performance analysis, behavioral analysis, and
exit interviews, we found that students completed tasks 35% faster (p < 0.05)
and made 50% more solution progress p (< 0.05) when using Copilot. Moreover,
our analysis revealed that, when using Copilot, students spent 11% less time
manually writing code (p < 0.05), and 12% less time conducting web searches (p
< 0.05), providing evidence of a fundamental shift in how they engaged in
programming. In exit interviews, students reported concerns about not
understanding how or why Copilot suggestions work. This research suggests the
need for computing educators to develop new pedagogical approaches that
leverage GenAI assistants' benefits while fostering reflection on how and why
GenAI suggestions address brownfield programming tasks. Complete study results
and analysis are presented at https://ghcopilot-icer.github.io/.

</details>


### [6] [Prompt Variability Effects On LLM Code Generation](https://arxiv.org/abs/2506.10204)
*Andrei Paleyes,Radzim Sendyka,Diana Robinson,Christian Cabrera,Neil D. Lawrence*

Main category: cs.SE

TL;DR: The paper introduces task- and model-agnostic methods to systematically evaluate how users' backgrounds influence code generated by LLMs, validating their utility through experiments and sharing supporting code with the community.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are increasingly used for code generation, but the effectiveness and quality of generated code greatly depend on prompt quality, which itself is influenced by the user's familiarity with software development. Therefore, it is crucial to systematically evaluate how LLM performance varies with such input variations.

Method: The authors propose a synthetic evaluation pipeline for code generation using LLMs and introduce a persona-based evaluation approach. These methods analyze how LLM responses differ qualitatively depending on the prospective user's software development background. Importantly, their approach is independent of specific programming tasks and models, making it broadly applicable.

Result: Experimental evidence demonstrates the utility of the proposed evaluation methods. Additionally, the authors provide the implementation code for these methods, making them available to the research community.

Conclusion: The proposed evaluation frameworks help systematically quantify and understand the sensitivity of code-generating LLMs to user backgrounds and prompt variations, offering valuable, task-agnostic tools for benchmarking and improving LLM-based code generation systems.

Abstract: Code generation is one of the most active areas of application of Large
Language Models (LLMs). While LLMs lower barriers to writing code and
accelerate development process, the overall quality of generated programs
depends on the quality of given prompts. Specifically, functionality and
quality of generated code can be sensitive to user's background and familiarity
with software development. It is therefore important to quantify LLM's
sensitivity to variations in the input. To this end we propose a synthetic
evaluation pipeline for code generation with LLMs, as well as a systematic
persona-based evaluation approach to expose qualitative differences of LLM
responses dependent on prospective user background. Both proposed methods are
completely independent from specific programming tasks and LLMs, and thus are
widely applicable. We provide experimental evidence illustrating utility of our
methods and share our code for the benefit of the community.

</details>


### [7] [AI-Based Software Vulnerability Detection: A Systematic Literature Review](https://arxiv.org/abs/2506.10280)
*Samiha Shimmi,Hamed Okhravi,Mona Rahimi*

Main category: cs.SE

TL;DR: This paper reviews software vulnerability detection methods from 2018-2023, highlighting the dominance of AI and graph-based models, current challenges, and future opportunities in emerging AI techniques.


<details>
  <summary>Details</summary>
Motivation: There is a need for improved software vulnerability detection due to the serious cybersecurity risks posed by vulnerabilities in source code. Traditional detection methods are insufficient, prompting the exploration of AI-driven techniques.

Method: The study systematically reviews software vulnerability detection research published between 2018 and 2023. It constructs a taxonomy of detection techniques, feature representations, and embedding methods. The review analyzes the prevalence of various AI-based approaches and discusses their strengths and limitations.

Result: The analysis found that 91% of SVD studies use AI-based methods, with graph-based models being the most common. Key limitations include dataset quality, reproducibility, and interpretability. The study also identifies emerging methods such as federated learning and quantum neural networks that present new research opportunities.

Conclusion: AI-driven approaches dominate recent SVD research, particularly graph-based models. Despite advances, challenges remain with data and interpretability. New techniques like federated learning and quantum neural networks offer promising directions for future work.

Abstract: Software vulnerabilities in source code pose serious cybersecurity risks,
prompting a shift from traditional detection methods (e.g., static analysis,
rule-based matching) to AI-driven approaches. This study presents a systematic
review of software vulnerability detection (SVD) research from 2018 to 2023,
offering a comprehensive taxonomy of techniques, feature representations, and
embedding methods. Our analysis reveals that 91% of studies use AI-based
methods, with graph-based models being the most prevalent. We identify key
limitations, including dataset quality, reproducibility, and interpretability,
and highlight emerging opportunities in underexplored techniques such as
federated learning and quantum neural networks, providing a roadmap for future
research.

</details>


### [8] [Minimizing False Positives in Static Bug Detection via LLM-Enhanced Path Feasibility Analysis](https://arxiv.org/abs/2506.10322)
*Xueying Du,Kai Yu,Chong Wang,Yi Zou,Wentai Deng,Zuoyu Ou,Xin Peng,Lingming Zhang,Yiling Lou*

Main category: cs.SE

TL;DR: LLM4PFA uses advanced LLM-based techniques to greatly reduce false positives in static bug detection for large codebases, outperforming current solutions and maintaining high accuracy for real bug identification.


<details>
  <summary>Details</summary>
Motivation: Existing static bug analyzers exhibit high false positive rates, particularly in large codebases with complex control flows and data dependencies. Current LLM-based solutions have limited effectiveness due to insufficient constraint analysis and scalability issues.

Method: The authors propose LLM4PFA, an iterative path feasibility analysis framework that leverages LLM agent-based targeted constraint reasoning and context-aware analysis driven by agent planning to enhance the precision of inter-procedural path feasibility analysis.

Result: Evaluation results show LLM4PFA filters out 72% to 96% of false positives in static bug detection, outperforming baselines by 41.1% to 105.7%. It only misses 3 real bugs out of 45 true positives.

Conclusion: LLM4PFA significantly improves static bug analysis by reducing false positives while maintaining high true positive detection, making it more reliable than existing approaches.

Abstract: Static bug analyzers play a crucial role in ensuring software quality.
However, existing analyzers for bug detection in large codebases often suffer
from high false positive rates. This is primarily due to the limited
capabilities of analyzers in path feasibility validation with multiple
conditional branches and complex data dependencies. While current LLM-based
approaches attempt to address this issue, their effectiveness remains limited
due to insufficient constraint cascade analysis and scalability challenges in
large projects. To address this challenge, we propose an iterative path
feasibility analysis framework LLM4PFA. By leveraging LLM agent based targeted
constraint reasoning, and key context-aware analysis driven by agent planning,
LLM4PFA effectively enhances complex inter-procedural path feasibility analysis
for minimizing false positives in static bug detection. Evaluation results show
that LLM4PFA precisely filters out 72% to 96% false positives reported during
static bug detection, significantly outperforming all the baselines by 41.1% -
105.7% improvements; meanwhile LLM4PFA only misses 3 real bugs of 45 true
positives.

</details>


### [9] [Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements](https://arxiv.org/abs/2506.10330)
*Seyed Moein Abtahi,Akramul Azim*

Main category: cs.SE

TL;DR: The study shows that using LLMs, smart prompting strategies, and augmented knowledge retrieval can automatically fix coding issues identified by static analysis, significantly improving code quality while reducing manual effort and resource use.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to improve code quality and efficiency in software development by automating code issue detection and revision. Existing static code analysis tools can detect issues but often require extensive manual intervention for code correction.

Method: The authors integrate Large Language Models (LLMs) (e.g., GPT-3.5 Turbo and GPT-4o) into a software development workflow together with a static code analysis framework. They employ iterative prompt engineering and retrieval-augmented generation (RAG) to refine code revisions. Additionally, a custom 'Code Comparison App' is used to address hallucinations by identifying and correcting erroneous LLM-generated code changes.

Result: Integration of LLMs with static code analysis and RAG substantially reduced the number of code issues in large software projects. Follow-up scans confirmed that the quality of code improved and error rates decreased, validating the approach.

Conclusion: Combining LLMs, advanced prompt engineering, RAG, and comparative tools can automate and enhance coding revision tasks, leading to more efficient software development, improved code quality, and lower costs.

Abstract: This study examined code issue detection and revision automation by
integrating Large Language Models (LLMs) such as OpenAI's GPT-3.5 Turbo and
GPT-4o into software development workflows. A static code analysis framework
detects issues such as bugs, vulnerabilities, and code smells within a
large-scale software project. Detailed information on each issue was extracted
and organized to facilitate automated code revision using LLMs. An iterative
prompt engineering process is applied to ensure that prompts are structured to
produce accurate and organized outputs aligned with the project requirements.
Retrieval-augmented generation (RAG) is implemented to enhance the relevance
and precision of the revisions, enabling LLM to access and integrate real-time
external knowledge. The issue of LLM hallucinations - where the model generates
plausible but incorrect outputs - is addressed by a custom-built "Code
Comparison App," which identifies and corrects erroneous changes before
applying them to the codebase. Subsequent scans using the static code analysis
framework revealed a significant reduction in code issues, demonstrating the
effectiveness of combining LLMs, static analysis, and RAG to improve code
quality, streamline the software development process, and reduce time and
resource expenditure.

</details>


### [10] [AutoGEEval++: A Multi-Level and Multi-Geospatial-Modality Automated Evaluation Framework for Large Language Models in Geospatial Code Generation on Google Earth Engine](https://arxiv.org/abs/2506.10365)
*Shuyang Hou,Zhangxiao Shen,Huayi Wu,Haoyue Jiao,Ziqi Liu,Lutong Xie,Chang Liu,Jianyuan Liang,Yaxian Qing,Xiaopu Zhang,Dehua Peng,Zhipeng Gui,Xuefeng Guan*

Main category: cs.SE

TL;DR: AutoGEEval++ introduces a robust, automated benchmark for evaluating how large language models generate geospatial code using Google Earth Engine, providing much-needed standardization and revealing performance differences across top models.


<details>
  <summary>Details</summary>
Motivation: Geospatial code generation is advancing quickly at the intersection of AI and geo-scientific analysis, but lacks standardized and automated evaluation methods. As large language models start to generate geospatial code, an objective, scalable assessment tool is crucial for progress.

Method: The authors introduce AutoGEEval++, an enhanced framework based on AutoGEEval. It offers a benchmark dataset with 6,365 cases, a submission program, and a judge module, creating an automated end-to-end evaluation pipeline using the Google Earth Engine Python API. The framework evaluates using multi-dimensional metrics such as accuracy, resource usage, runtime efficiency, and error categorization.

Result: The study evaluated 24 state-of-the-art large language models on geospatial code generation tasks in Google Earth Engine. The results showed significant differences in model performance, error types, stability, and efficiency, across different task categories and model types. The findings validated AutoGEEval++'s effectiveness and the need for such a standardized evaluation protocol.

Conclusion: AutoGEEval++ is the first standardized, automated benchmark and evaluation framework for GEE-based LLM geospatial code generation. It enables fair, systematic, and scalable comparison of model performance, setting a foundation for further advances in domain-specific, AI-driven code generation.

Abstract: Geospatial code generation is becoming a key frontier in integrating
artificial intelligence with geo-scientific analysis, yet standardised
automated evaluation tools for this task remain absent. This study presents
AutoGEEval++, an enhanced framework building on AutoGEEval, and the first
automated assessment system for large language models (LLMs) generating
geospatial code on Google Earth Engine (GEE). It supports diverse data
modalities and varying task complexities. Built on the GEE Python API,
AutoGEEval++ features a benchmark dataset-AutoGEEval++-Bench-with 6,365 test
cases across 26 data types and three task categories: unit, combo, and theme
tests. It includes a submission programme and a judge module to realise an
end-to-end automated evaluation pipeline from code generation to
execution-based validation. The framework adopts multi-dimensional
metrics-accuracy, resource usage, run-time efficiency, and error
types-balancing hallucination control and efficiency, and enabling boundary
testing and error pattern analysis. Using AutoGEEval++, we evaluate 24
state-of-the-art LLMs (as of June 2025), including general-purpose,
reasoning-enhanced, code-centric, and geoscience-specific models. Results
reveal clear performance, stability, and error differences across task types,
model designs, and deployment settings, confirming AutoGEEval++'s practical
value and scalability in vertical-domain code generation. This work establishes
the first standardised evaluation protocol and foundational benchmark for
GEE-based LLM code generation, providing a unified basis for performance
comparison and a methodological framework for systematic, domain-specific code
evaluation.

</details>


### [11] [MLLM-Based UI2Code Automation Guided by UI Layout Information](https://arxiv.org/abs/2506.10376)
*Fan Wu,Cuiyun Gao,Shuqing Li,Xin-Cheng Wen,Qing Liao*

Main category: cs.SE

TL;DR: LayoutCoder is a new MLLM-based system that converts web UI images to accurate code, using novel layout-aware modules and outperforming top existing methods.


<details>
  <summary>Details</summary>
Motivation: Converting UI designs into code is a key but tedious step in web development, often requiring significant manual effort. Existing deep learning solutions have problems generalizing to real-world designs and rely heavily on labeled data.

Method: The paper introduces LayoutCoder, an MLLM-based framework with three modules: (1) Element Relation Construction to identify and group similar UI components, (2) UI Layout Parsing to create layout trees for code generation, and (3) Layout-Guided Code Fusion for accurate, layout-preserving code generation. It uses two datasets for evaluation, including a new benchmark called Snap2Code.

Result: LayoutCoder demonstrates superior performance compared to existing methods, achieving up to 10.14% higher BLEU scores and 3.95% higher CLIP scores across multiple datasets.

Conclusion: LayoutCoder offers a more efficient and accurate approach to UI-to-code conversion, outperforming previous state-of-the-art models, and is validated with a new real-world dataset.

Abstract: Converting user interfaces into code (UI2Code) is a crucial step in website
development, which is time-consuming and labor-intensive. The automation of
UI2Code is essential to streamline this task, beneficial for improving the
development efficiency. There exist deep learning-based methods for the task;
however, they heavily rely on a large amount of labeled training data and
struggle with generalizing to real-world, unseen web page designs. The advent
of Multimodal Large Language Models (MLLMs) presents potential for alleviating
the issue, but they are difficult to comprehend the complex layouts in UIs and
generate the accurate code with layout preserved. To address these issues, we
propose LayoutCoder, a novel MLLM-based framework generating UI code from
real-world webpage images, which includes three key modules: (1) Element
Relation Construction, which aims at capturing UI layout by identifying and
grouping components with similar structures; (2) UI Layout Parsing, which aims
at generating UI layout trees for guiding the subsequent code generation
process; and (3) Layout-Guided Code Fusion, which aims at producing the
accurate code with layout preserved. For evaluation, we build a new benchmark
dataset which involves 350 real-world websites named Snap2Code, divided into
seen and unseen parts for mitigating the data leakage issue, besides the
popular dataset Design2Code. Extensive evaluation shows the superior
performance of LayoutCoder over the state-of-the-art approaches. Compared with
the best-performing baseline, LayoutCoder improves 10.14% in the BLEU score and
3.95% in the CLIP score on average across all datasets.

</details>


### [12] [Bug Classification in Quantum Software: A Rule-Based Framework and Its Evaluation](https://arxiv.org/abs/2506.10397)
*Mir Mohammad Yousuf,Shabir Ahmad Sofi*

Main category: cs.SE

TL;DR: This paper introduces and validates a rule-based automated framework for classifying quantum software bugs, demonstrating high accuracy and reliability in most categories, and provides insights into bug characteristics across Qiskit repositories.


<details>
  <summary>Details</summary>
Motivation: There is an increasing need to improve software quality in quantum computing projects, but the unique challenges of quantum software bugs require effective methods for accurate bug classification.

Method: The authors developed a rule-based automated classification framework that uses keyword and heuristic-based methods tailored to quantum software. They validated its reliability by manually classifying a stratified sample of 4,984 issues and comparing the automated results against the manual ground truth using standard evaluation metrics (accuracy, precision, recall, F1-score). Statistical validation included paired t-tests and Cohen's Kappa analysis.

Result: The framework achieved up to 85.21% accuracy, with F1-scores between 0.7075 and 0.8393 for different classification types. Cohen's Kappa values indicated strong agreement for most categories except severity. Analysis showed that most bugs are classical (67.2%), with quantum-specific bugs making up 27.3%. Compatibility, functional, and quantum-specific defects were common categories. Quality attributes most impacted were usability, maintainability, and interoperability. Most issues were low severity.

Conclusion: The proposed framework provides an effective automated solution for classifying quantum software bugs, achieving high reliability for most classification categories. However, further improvement is needed in severity classification. The analysis provides actionable insights into the nature and distribution of bugs in quantum software.

Abstract: Accurate classification of software bugs is essential for improving software
quality. This paper presents a rule-based automated framework for classifying
issues in quantum software repositories by bug type, category, severity, and
impacted quality attributes, with additional focus on quantum-specific bug
types. The framework applies keyword and heuristic-based techniques tailored to
quantum computing. To assess its reliability, we manually classified a
stratified sample of 4,984 issues from a dataset of 12,910 issues across 36
Qiskit repositories. Automated classifications were compared with ground truth
using accuracy, precision, recall, and F1-score. The framework achieved up to
85.21% accuracy, with F1-scores ranging from 0.7075 (severity) to 0.8393
(quality attribute). Statistical validation via paired t-tests and Cohen's
Kappa showed substantial to almost perfect agreement for bug type (k = 0.696),
category (k = 0.826), quality attribute (k = 0.818), and quantum-specific bug
type (k = 0.712). Severity classification showed slight agreement (k = 0.162),
suggesting room for improvement. Large-scale analysis revealed that classical
bugs dominate (67.2%), with quantum-specific bugs at 27.3%. Frequent bug
categories included compatibility, functional, and quantum-specific defects,
while usability, maintainability, and interoperability were the most impacted
quality attributes. Most issues (93.7%) were low severity; only 4.3% were
critical. A detailed review of 1,550 quantum-specific bugs showed that over
half involved quantum circuit-level problems, followed by gate errors and
hardware-related issues.

</details>


### [13] [Towards Understanding Bugs in Distributed Training and Inference Frameworks for Large Language Models](https://arxiv.org/abs/2506.10426)
*Xiao Yu,Haoxuan Chen,Feifei Niu,Xing Hu,Jacky Wai Keung,Xin Xia*

Main category: cs.SE

TL;DR: This paper analyzes 308 bugs in major distributed LLM frameworks, identifying common bug types, causes, and fix patterns. Nearly half of the fixes are simple and suitable for automation, pointing to opportunities for more robust frameworks and LLM-driven debugging tools.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of large language models (LLMs) demands scalable distributed training and inference frameworks. These frameworks, while powerful, have become increasingly complex, resulting in non-trivial software bugs that can degrade performance, cause failures, and waste significant resources. Understanding the characteristics of these bugs is necessary for effective quality assurance and improved debugging.

Method: The authors conduct a large-scale empirical analysis of 308 fixed bugs across three widely used distributed training/inference frameworks: DeepSpeed, Megatron-LM, and Colossal-AI. They examine bug symptoms, root causes, identification/fixing efforts, and common fixing strategies.

Result: The study reveals unique bug root causes specific to distributed frameworks, such as allocation strategy errors and distributed communication errors. Many bug fixes (48%) require minimal code changes (<=10 LOC) and involve simple strategies, indicating a potential for automation. Diagnosing and fixing complex bugs remains challenging due to complexities like symptom-root cause disconnects, high reproduction costs, and interactions across multiple system components.

Conclusion: The findings emphasize the importance of understanding bug characteristics to improve debugging and framework reliability. The authors highlight possibilities for automating simple bug fixes and suggest that LLM-based tools can further aid automated debugging and repair, benefiting both frameworks and dependent projects.

Abstract: With the rapid development of large language models (LLMs), distributed
training and inference frameworks like DeepSpeed have become essential for
scaling model training and inference across multiple GPUs or nodes. However,
the increasing complexity of these frameworks brings non-trivial software bugs,
which may degrade training performance, cause unexpected failures, and result
in significant resource waste. Understanding framework bugs' characteristics is
fundamental for quality assurance, allowing the design of more effective
debugging and repair methods. Thus, our paper conducts the first large-scale
empirical analysis of 308 fixed bugs across three popular distributed
training/inference frameworks: DeepSpeed, Megatron-LM, and Colossal-AI. We
examine bug symptoms, root causes, bug identification and fixing efforts, and
common low-effort fixing strategies. Additionally, the distributed nature of
these frameworks introduces unique bug root causes, such as allocation strategy
error and distributed communication error. Diagnosing and fixing complex bugs
remains challenging due to factors like the disconnect between symptoms and
root causes, high bug reproduction costs, and low-level or cross-component
interactions. Interestingly, we observe that 48% of bug fixes require minimal
code changes (<=10 LOC) and follow simple strategies such as conditional logic
optimization, parameter handling enhancement, or version compatibility
handling, indicating potential for automation. Based on these insights, we
offer several implications for improving the reliability of both distributed
training and inference frameworks and their dependent LLM projects, while also
identifying opportunities to leverage LLM-based tools for automated debugging
and repair.

</details>


### [14] [EXPEREPAIR: Dual-Memory Enhanced LLM-based Repository-Level Program Repair](https://arxiv.org/abs/2506.10484)
*Fangwen Mu,Junjie Wang,Lin Shi,Song Wang,Shoubin Li,Qing Wang*

Main category: cs.SE

TL;DR: ExpeRepair uses dual memory systems to dynamically enhance LLM-based software repair, outperforming previous methods on a standard benchmark.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based software repair methods lack the ability to learn from previous repairs and mainly use static prompts, which limits their effectiveness and adaptability.

Method: The authors introduce ExpeRepair, a novel LLM-based approach that uses dual memory systems inspired by human cognition—episodic memory stores concrete repair demonstrations and semantic memory stores abstract insights. During inference, the system retrieves relevant data from both memories to produce dynamic, context-aware prompts.

Result: Experiments on the SWE-bench Lite benchmark show that ExpeRepair with Claude 3.7 Sonnet achieves a pass@1 score of 49.3%, surpassing all current state-of-the-art open-source approaches.

Conclusion: ExpeRepair successfully leverages both episodic and semantic memories for dynamic prompt composition, enabling more effective and generalized software issue repair than existing LLM-based solutions.

Abstract: Automatically repairing software issues remains a fundamental challenge at
the intersection of software engineering and AI. Although recent advancements
in Large Language Models (LLMs) have demonstrated potential for
repository-level repair tasks, current methodologies exhibit two notable
limitations: (1) they often address issues in isolation, neglecting to
incorporate insights from previously resolved issues, and (2) they rely on
static and rigid prompting strategies, which constrain their ability to
generalize across diverse and evolving issue scenarios. Inspired by the dual
memory systems of human cognition, where episodic and semantic memories work
synergistically to support human reasoning and decision-making, we propose
ExpeRepair, a novel LLM-based approach that continuously learns from historical
repair experiences through dual-channel knowledge accumulation. ExpeRepair
organizes historical repair experiences into two complementary memories: an
episodic memory that stores concrete repair demonstrations, and a semantic
memory that encodes abstract reflective insights. At inference time, ExpeRepair
activates both memory systems by retrieving relevant demonstrations from
episodic memory and recalling high-level repair insights from semantic memory.
It further enhances adaptability through dynamic prompt composition,
synergistically integrating both memory types to replace static prompts with
context-aware, experience-driven prompts. Experiments on the SWE-bench Lite
benchmark demonstrate that ExpeRepair achieves a pass@1 score of 49.3% with
Claude 3.7 Sonnet, outperforming all state-of-the-art open-source methods.

</details>


### [15] [BugGen: A Self-Correcting Multi-Agent LLM Pipeline for Realistic RTL Bug Synthesis](https://arxiv.org/abs/2506.10501)
*Surya Jasper,Minh Luu,Evan Pan,Aakash Tyagi,Michael Quinn,Jiang Hu,David Kebo Houngninou*

Main category: cs.SE

TL;DR: BugGen uses LLMs to autonomously generate realistic hardware bugs, greatly improving the speed, quality, and utility of bug datasets for verification and ML-based debugging.


<details>
  <summary>Details</summary>
Motivation: There is increasing hardware complexity, but current manual and automated methods for inserting hardware bugs don't scale well or produce diverse, realistic bug datasets for machine learning-assisted debugging.

Method: The authors propose BugGen, an autonomous pipeline that uses Large Language Models (LLMs) and multi-agent systems to partition hardware modules, select mutation targets, and iteratively generate, insert, and validate syntactically correct and functionally realistic bugs in RTL code.

Result: BugGen created 500 unique bugs with a 94% functional accuracy rate and validated 17.7 bugs per hour, significantly outperforming manual methods. It found 104 previously undetected bugs in OpenTitan regressions and had over twice the syntactic accuracy of Certitude, with more complex and meaningful bugs. ML models trained on BugGen datasets achieved high classification accuracy (88.1%-93.2%).

Conclusion: BugGen offers a scalable and automated approach for generating high-quality, realistic bug datasets, boosting hardware verification efficiency and enabling practical ML-assisted debugging.

Abstract: Hardware complexity continues to strain verification resources, motivating
the adoption of machine learning (ML) methods to improve debug efficiency.
However, ML-assisted debugging critically depends on diverse and scalable bug
datasets, which existing manual or automated bug insertion methods fail to
reliably produce. We introduce BugGen, a first of its kind, fully autonomous,
multi-agent pipeline leveraging Large Language Models (LLMs) to systematically
generate, insert, and validate realistic functional bugs in RTL. BugGen
partitions modules, selects mutation targets via a closed-loop agentic
architecture, and employs iterative refinement and rollback mechanisms to
ensure syntactic correctness and functional detectability. Evaluated across
five OpenTitan IP blocks, BugGen produced 500 unique bugs with 94% functional
accuracy and achieved a throughput of 17.7 validated bugs per hour-over five
times faster than typical manual expert insertion. Additionally, BugGen
identified 104 previously undetected bugs in OpenTitan regressions,
highlighting its utility in exposing verification coverage gaps. Compared
against Certitude, BugGen demonstrated over twice the syntactic accuracy,
deeper exposure of testbench blind spots, and more functionally meaningful and
complex bug scenarios. Furthermore, when these BugGen-generated datasets were
employed to train ML-based failure triage models, we achieved high
classification accuracy (88.1%-93.2%) across different IP blocks, confirming
the practical utility and realism of generated bugs. BugGen thus provides a
scalable solution for generating high-quality bug datasets, significantly
enhancing verification efficiency and ML-assisted debugging.

</details>


### [16] [AdaptiveLLM: A Framework for Selecting Optimal Cost-Efficient LLM for Code-Generation Based on CoT Length](https://arxiv.org/abs/2506.10525)
*Junhang Cheng,Fang Liu,Chengru Wu,Li Zhang*

Main category: cs.SE

TL;DR: AdaptiveLLM is a framework for dynamically selecting the most suitable LLM for code tasks based on automatically estimated difficulty, improving accuracy and cost-efficiency over existing methods, and outperforming both baseline and single-model approaches.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) enhance code generation but struggle to balance performance and inference costs for diverse programming tasks. Existing dynamic model selection methods are resource-heavy and often depend on human-annotated task difficulty labels, which are impractical or misaligned with LLM assessments.

Method: The paper introduces AdaptiveLLM, a framework that automatically assesses coding task difficulty using Chain-of-Thought (CoT) reasoning lengths, clusters these with k-means, and fine-tunes CodeBERT for difficulty-sensitive feature embedding. An XGBoost classifier then chooses the most suitable LLM for each task, aiming for an optimal performance-cost balance.

Result: Experimental results show AdaptiveLLM improves pass@1 scores by 7.86% and reduces resource consumption by 88.9% compared to ComplexityNet. Against a single-model approach, AdaptiveLLM increases accuracy by about 15% without increasing costs. CoT-based difficulty assessment provides more reliable selection than human labels.

Conclusion: AdaptiveLLM offers a cost-efficient and effective solution for dynamic LLM selection in code generation by leveraging automated difficulty assessment, leading to significant performance gains and resource savings.

Abstract: While Large Language Models (LLMs) have significantly advanced code
generation efficiency, they face inherent challenges in balancing performance
and inference costs across diverse programming tasks. Dynamically selecting the
optimal LLM based on task difficulty and resource constraints offers a
promising approach to achieve an optimal balance between efficiency and
performance. However, existing model selection methods are resource-intensive
and often neglect cost efficiency. Moreover, these approaches rely on
human-annotated difficulty labels that are frequently inaccessible in
real-world settings and may not align with the LLM's own assessment of task
difficulty. In this paper, we introduce AdaptiveLLM, a framework that
dynamically selects optimal LLMs for a given coding task by automatically
assessing task difficulty. Our framework first estimates task difficulty using
Chain-of-Thought lengths generated by reasoning model, clusters these into
three difficulty levels via k-means, and fine-tunes CodeBERT to embed
difficulty-aware features. A trained XGBoost classifier then selects the best
model for each problem, optimizing the performance-cost trade-off. Experimental
results show that AdaptiveLLM achieves a 7.86% improvement in pass@1 score
while reducing resource consumption by 88.9% compared to baseline method
ComplexityNet. When compared to a single model, AdaptiveLLM demonstrates an
approximately 15% accuracy improvement, while maintaining the same level of
cost consumption. Apart from that, the difficulty assessment using CoT provides
more reliable selection criteria than human evaluation. Our replication package
is available at https://github.com/cjhCoder7/AdaptiveLLM.

</details>


### [17] [Scalable Software Testing in Fast Virtual Platforms: Leveraging SystemC, QEMU and Containerization](https://arxiv.org/abs/2506.10624)
*Lukas Jünger,Jan Henrik Weinstock,Tim Kraus*

Main category: cs.SE

TL;DR: The paper presents a containerized approach for deploying Virtual Platforms in the cloud using open-source tools, enabling scalable, fast, and license-free HW/SW testing before physical hardware is available. A case study shows this method can accelerate and simplify development, especially in critical fields like automotive.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of hardware/software (HW/SW) systems, especially in safety-critical fields like automotive, makes thorough testing both crucial and challenging. One major obstacle is the late availability of hardware, which delays early-stage software development and testing.

Method: The study proposes using containerization to encapsulate Virtual Platforms (VPs) that adhere to the SystemC TLM-2.0 standard. This approach lowers environment dependencies and enables deployment in the cloud for fast and parallel test execution. The method also leverages open-source technologies (e.g., QEMU, VCML) to eliminate costly licensing needs. A case study involving an Artificial Intelligence (AI) accelerator VP is used to demonstrate the approach.

Result: The proposed approach enables pre-silicon execution and robust testing of unmodified target software, accelerates HW/SW co-development, and reduces both costs and environmental restrictions for large-scale parallel verification. The case study showcases practical improvements in test speed and flexibility.

Conclusion: Encapsulating VPs using containerization, and employing open-source technologies, presents an effective and scalable solution to HW/SW complexity, facilitating early, parallel, and cloud-based test execution. This has significant implications for accelerating and improving HW/SW co-development in complex and safety-critical domains.

Abstract: The ever-increasing complexity of HW/SW systems presents a persistent
challenge, particularly in safety-critical domains like automotive, where
extensive testing is imperative. However, the availability of hardware often
lags behind, hindering early-stage software development. To address this,
Virtual Platforms (VPs) based on the SystemC TLM-2.0 standard have emerged as a
pivotal solution, enabling pre-silicon execution and testing of unmodified
target software. In this study, we propose an approach leveraging
containerization to encapsulate VPs in order to reduce environment dependencies
and enable cloud deployment for fast, parallelized test execution, as well as
open-source VP technologies such as QEMU and VCML to obviate the need for seat
licenses. To demonstrate the efficacy of our approach, we present an Artificial
Intelligence (AI) accelerator VP case study. Through our research, we offer a
robust solution to address the challenges posed by the complexity of HW/SW
systems, with practical implications for accelerating HW/SW co-development.

</details>


### [18] [Not One to Rule Them All: Mining Meaningful Code Review Orders From GitHub](https://arxiv.org/abs/2506.10654)
*Abir Bouraffa,Carolin Brandt,Andy Zaidmann,Walid Maalej*

Main category: cs.SE

TL;DR: Reviewers often don't follow alphabetical order in code reviews, instead using meaningful strategies like reviewing big changes or test files first. Better tool support is needed to match these real reviewing habits, particularly for large pull requests.


<details>
  <summary>Details</summary>
Motivation: Code review is a critical activity in software development, commonly facilitated through tools like GitHub pull requests. File changes are usually shown in alphabetical order, but this may not align with how reviewers prefer to navigate and comment on code. Understanding actual reviewer navigation can help improve code review process and tool support.

Method: The authors mined and analyzed code review comments from 23,241 pull requests across 100 popular Java and Python repositories on GitHub. They specifically studied the sequence in which reviewers commented on files within pull requests, identifying whether this order was alphabetical or followed other meaningful strategies.

Result: 44.6% of pull requests had reviewers commenting in a non-alphabetical order. Within these, reviewers followed significant strategies such as largest-diff-first (20.6%), similarity to PR title and description (17.6%), and a test-first order (29% in P.R.s with both production and test changes). Non-alphabetical reviews covered a higher proportion of changed files but received slightly fewer approvals.

Conclusion: Reviewers frequently deviate from alphabetical file order, adopting complex file review strategies in larger pull requests. This suggests a need for better support in code review tools to cater to these navigation preferences and strategies, especially for larger PRs.

Abstract: Developers use tools such as GitHub pull requests to review code, discuss
proposed changes, and request modifications. While changed files are commonly
presented in alphabetical order, this does not necessarily coincide with the
reviewer's preferred navigation sequence. This study investigates the different
navigation orders developers follow while commenting on changes submitted in
pull requests. We mined code review comments from 23,241 pull requests in 100
popular Java and Python repositories on GitHub to analyze the order in which
the reviewers commented on the submitted changes. Our analysis shows that for
44.6% of pull requests, the reviewers comment in a non-alphabetical order.
Among these pull requests, we identified traces of alternative meaningful
orders: 20.6% (2,134) followed a largest-diff-first order, 17.6% (1,827) were
commented in the order of the files' similarity to the pull request's title and
description, and 29% (1,188) of pull requests containing changes to both
production and test files adhered to a test-first order. We also observed that
the proportion of reviewed files to total submitted files was significantly
higher in non-alphabetically ordered reviews, which also received slightly
fewer approvals from reviewers, on average. Our findings highlight the need for
additional support during code reviews, particularly for larger pull requests,
where reviewers are more likely to adopt complex strategies rather than
following a single predefined order.

</details>


### [19] [Formalising Software Requirements using Large Language Models](https://arxiv.org/abs/2506.10704)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: The VERIFAI project investigates leveraging NLP, ontologies, reuse, LLMs, and AI to automate the traceability and formal specification of software requirements, improving verification and development processes.


<details>
  <summary>Details</summary>
Motivation: Traceability and verification of natural language software requirements are challenging, particularly in ensuring that requirements are correctly captured, formally specified, and maintained throughout the development process.

Method: The project explores Natural Language Processing, ontology-based domain modeling, similarity-based artifact reuse, large language models for specification identification, and artificial intelligence guidance.

Result: The project aims to provide tools for automatic formal specification generation and traceability of requirements throughout software development and verification.

Conclusion: Combining NLP, ontologies, software artefact reuse, LLMs, and AI can enhance requirement traceability and verification, from design to implementation.

Abstract: This paper is a brief introduction to our recently initiated project named
VERIFAI: Traceability and verification of natural language requirements. The
project addresses the challenges in the traceability and verification of formal
specifications through providing support for the automatic generation of the
formal specifications and the traceability of the requirements from the initial
software design stage through the systems implementation and verification.
Approaches explored in this project include Natural Language Processing, use of
ontologies to describe the software system domain, reuse of existing software
artefacts from similar systems (i.e. through similarity based reuse) and large
language models to identify and declare the specifications as well as use of
artificial intelligence to guide the process.

</details>


### [20] [From Tea Leaves to System Maps: Context-awareness in Monitoring Operational Machine Learning Models](https://arxiv.org/abs/2506.10770)
*Joran Leest,Claudia Raibulet,Patricia Lago,Ilias Gerostathopoulos*

Main category: cs.SE

TL;DR: ML model failures in production often stem from contextual misalignments rather than statistical input anomalies. This paper systematically reviews 94 studies to synthesize how contextual information can enhance ML monitoring. It introduces the C-SAR framework and 20 reusable patterns, guiding the move from basic statistical monitoring to systematic, contextual root-cause analysis and better ML model reliability.


<details>
  <summary>Details</summary>
Motivation: Machine learning models in production often fail not because of simple statistical anomalies in input data, but due to misalignments between the production environment and the models' training assumptions. Addressing these failures requires monitoring practices that incorporate rich contextual information for meaningful alerts and root-cause analysis, but it is unclear how contextual information should be used in current monitoring research.

Method: The authors conducted a systematic review of 94 primary studies spanning the domains of data mining, databases, software engineering, and machine learning. They analyzed these studies to characterize and structure different types of contextual information relevant to ML monitoring.

Result: The paper proposes the Contextual System--Aspect--Representation (C-SAR) framework as a conceptual model derived from the analysis. Additionally, it identifies 20 recurring, reusable patterns combining system, aspect, and representation types, and connects them to relevant ML monitoring activities.

Conclusion: There is a lack of consensus in the field regarding the use of contextual information in ML monitoring. The C-SAR framework and the identified patterns offer a structured approach to incorporating context, shifting industry practices from superficial statistical analyses to systematic, context-driven monitoring.

Abstract: Machine learning (ML) models in production do not fail due to statistical
anomalies in their input data; they fail due to contextual misalignment -- when
their environment deviates from training assumptions, leading to unreliable
predictions. Effective ML monitoring requires rich contextual information to
move beyond detecting statistical shifts toward meaningful alerts and
systematic root-cause analysis. Yet, surprisingly, despite extensive research
in ML monitoring and related disciplines (drift detection, data validation,
out-of-distribution detection), there is no shared understanding of how to use
contextual information -- striking, given that monitoring involves
interpretation of information in context. In response, this paper presents a
systematic review to characterize and structure the various types of contextual
information in this domain. Our analysis examines 94 primary studies across
data mining, databases, software engineering, and ML. We introduce the
Contextual System--Aspect--Representation (C-SAR) framework, a conceptual model
that synthesizes our findings. We also identify 20 recurring and potentially
reusable patterns of specific system, aspect, and representation combinations,
and map them to the monitoring activities they support. This study provides a
new perspective on ML monitoring: from interpreting "tea leaves" of
observational statistics into constructing and managing "system maps" that
enable systematic and reliable ML monitoring practices.

</details>


### [21] [What Users Value and Critique: Large-Scale Analysis of User Feedback on AI-Powered Mobile Apps](https://arxiv.org/abs/2506.10785)
*Vinaik Chhetri,Krishna Upadhyay,A. B. Siddique,Umar Farooq*

Main category: cs.SE

TL;DR: A large-scale, validated analysis pipeline using LLMs enables nuanced understanding of user feedback for AI-powered mobile apps, revealing key positive and negative themes that are often missed by traditional analysis approaches.


<details>
  <summary>Details</summary>
Motivation: Despite the rapid proliferation of AI-powered features in mobile apps, there is limited understanding of how users perceive and critique these features, mainly due to the sheer volume and complexity of user feedback data.

Method: The authors curated a dataset of 292 AI-driven mobile apps across 14 categories, collecting 894,000 AI-specific Google Play reviews. They developed a multi-stage analysis pipeline, starting from a human-labeled benchmark to systematically evaluate large language models (LLMs) and prompting strategies for review classification, aspect-sentiment extraction, and clustering. Each pipeline stage was validated for accuracy and consistency.

Result: The pipeline enabled scalable, high-precision extraction of over one million aspect-sentiment pairs, grouped into 18 positive and 15 negative user topics. Findings showed that users focus on a few key themes: positives like productivity, reliability, and personalized assistance; and negatives such as technical failures, pricing, and limited language support. The approach can detect mixed sentiments within the same review, which are often overlooked by traditional methods. Category-aware analysis also uncovered both universal and domain-specific user satisfaction and frustrations.

Conclusion: The proposed multi-stage analytical pipeline offers a more accurate and nuanced understanding of real-world user experiences with AI-powered apps, outperforming traditional, coarser feedback analysis methods and highlighting both broad and app-specific satisfaction and pain points.

Abstract: Artificial Intelligence (AI)-powered features have rapidly proliferated
across mobile apps in various domains, including productivity, education,
entertainment, and creativity. However, how users perceive, evaluate, and
critique these AI features remains largely unexplored, primarily due to the
overwhelming volume of user feedback. In this work, we present the first
comprehensive, large-scale study of user feedback on AI-powered mobile apps,
leveraging a curated dataset of 292 AI-driven apps across 14 categories with
894K AI-specific reviews from Google Play. We develop and validate a
multi-stage analysis pipeline that begins with a human-labeled benchmark and
systematically evaluates large language models (LLMs) and prompting strategies.
Each stage, including review classification, aspect-sentiment extraction, and
clustering, is validated for accuracy and consistency. Our pipeline enables
scalable, high-precision analysis of user feedback, extracting over one million
aspect-sentiment pairs clustered into 18 positive and 15 negative user topics.
Our analysis reveals that users consistently focus on a narrow set of themes:
positive comments emphasize productivity, reliability, and personalized
assistance, while negative feedback highlights technical failures (e.g.,
scanning and recognition), pricing concerns, and limitations in language
support. Our pipeline surfaces both satisfaction with one feature and
frustration with another within the same review. These fine-grained,
co-occurring sentiments are often missed by traditional approaches that treat
positive and negative feedback in isolation or rely on coarse-grained analysis.
To this end, our approach provides a more faithful reflection of the real-world
user experiences with AI-powered apps. Category-aware analysis further uncovers
both universal drivers of satisfaction and domain-specific frustrations.

</details>


### [22] [Evaluating Large Language Models on Non-Code Software Engineering Tasks](https://arxiv.org/abs/2506.10833)
*Fabian C. Peña,Steffen Herbold*

Main category: cs.SE

TL;DR: The paper introduces SELU, a benchmark covering 17 non-code software engineering tasks to evaluate LLMs. Moderate-size decoder-only models outperform others, and pre-training on code offers limited extra benefit for non-code tasks. SELU aids in choosing LLMs for broader SE workflows.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) have impressive code processing skills, but their abilities on non-code software engineering (SE) tasks are not well understood. There is a need for a benchmark to evaluate LLMs in these broader SE contexts.

Method: The authors establish SELU, the first comprehensive benchmark for evaluating LLMs on 17 non-code SE tasks (e.g., requirement classification, effort estimation), utilizing classification, regression, NER, and MLM tasks. They fine-tune 22 open-source LLMs, prompt two proprietary LLMs, and train two traditional baselines. Performance is evaluated using various metrics and compared using the Bayesian signed-rank test.

Result: Moderate-scale decoder-only models consistently perform best across tasks, with strong average results and stable performance. Code-focused pre-training provides only minor gains when adapting to non-code SE tasks.

Conclusion: SELU provides a robust evaluation framework for LLMs in non-code SE tasks. Findings suggest that moderate-size, decoder-only models are most effective here, and domain adaptation from code does not significantly improve non-code task performance. These insights inform model selection and future expansions of the SELU benchmark.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
code understanding and generation; however, their effectiveness on non-code
Software Engineering (SE) tasks remains underexplored. We present the first
comprehensive benchmark, which we name `Software Engineering Language
Understanding' (SELU), for evaluating LLMs on 17 non-code tasks, spanning from
identifying whether a requirement is functional or non-functional to estimating
the effort and complexity of backlog items. SELU covers classification,
regression, Named Entity Recognition (NER), and Masked Language Modeling (MLM)
targets, with data drawn from diverse sources such as code repositories, issue
tracking systems, and developer forums. We fine-tune 22 open-source LLMs,
prompt two proprietary alternatives, and train two baselines. Performance is
measured using metrics such as F1-macro, SMAPE, F1-micro, and accuracy, and
compared via the Bayesian signed-rank test. Our results show that
moderate-scale decoder-only models consistently form a top-tier, exhibiting
high mean performance and low across-task variance, while domain adaptation via
code-focused pre-training might yield only modest improvements. These insights
guide model selection for non-code SE workflows and highlight directions for
expanding SELU to generative and design-oriented scenarios.

</details>


### [23] [MultiCoSim: A Python-based Multi-Fidelity Co-Simulation Framework](https://arxiv.org/abs/2506.10869)
*Quinn Thibeault,Giulia Pedrielli*

Main category: cs.SE

TL;DR: MultiCoSim is a new Python-based framework that streamlines simulation for complex cyber-physical systems by enabling flexible, modular, and automated component integration, overcoming limitations of traditional simulation tools.


<details>
  <summary>Details</summary>
Motivation: As cyber-physical systems (CPS) become more complex and require the integration of hardware, software, and physical processes, existing simulation tools face challenges like rigidity, lack of automation, limited portability, and difficulty in composing heterogeneous components for reusable benchmarks and flexible evaluation.

Method: The authors introduce MultiCoSim, a Python-based framework for CPS simulation. MultiCoSim allows programmatic definition, composition, and configuration of simulation components, supporting distributed and component-based co-simulation with easy substitution and reconfiguration. The framework's capabilities are demonstrated through case studies including custom controllers and integration with platforms like PX4 autopilot.

Result: MultiCoSim is shown to enable flexible and modular co-simulations, supporting easier and more automated composition of heterogeneous components. Case studies illustrate its ability to integrate both custom and off-the-shelf simulation elements, enhancing the efficiency and scalability of CPS simulation pipelines.

Conclusion: MultiCoSim addresses fundamental limitations of existing CPS simulation tools, providing a more versatile and automated approach for analysis and development. Its flexibility enables its application across diverse scenarios, supporting research and development needs in safety-critical and learning-enabled CPS.

Abstract: Simulation is a foundational tool for the analysis and testing of
cyber-physical systems (CPS), underpinning activities such as algorithm
development, runtime monitoring, and system verification. As CPS grow in
complexity and scale, particularly in safety-critical and learning-enabled
settings, accurate analysis and synthesis increasingly rely on the rapid use of
simulation experiments. Because CPS inherently integrate hardware, software,
and physical processes, simulation platforms must support co-simulation of
heterogeneous components at varying levels of fidelity. Despite recent advances
in high-fidelity modeling of hardware, firmware, and physics, co-simulation in
diverse environments remains challenging. These limitations hinder the
development of reusable benchmarks and impede the use of simulation for
automated and comparative evaluation.
  Existing simulation tools often rely on rigid configurations, lack automation
support, and present obstacles to portability and modularity. Many are
configured through static text files or impose constraints on how simulation
components are represented and connected, making it difficult to flexibly
compose systems or integrate components across platforms.
  To address these challenges, we introduce MultiCoSim, a Python-based
simulation framework that enables users to define, compose, and configure
simulation components programmatically. MultiCoSim supports distributed,
component-based co-simulation and allows seamless substitution and
reconfiguration of components. We demonstrate the flexibility of MultiCoSim
through case studies that include co-simulations involving custom
automaton-based controllers, as well as integration with off-the-shelf
platforms like the PX4 autopilot for aerial robotics. These examples highlight
MultiCoSim's capability to streamline CPS simulation pipelines for research and
development.

</details>


### [24] [SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks](https://arxiv.org/abs/2506.10954)
*Lianghong Guo,Yanlin Wang,Caihua Li,Pengyu Yang,Jiachi Chen,Wei Tao,Yingtian Zou,Duyu Tang,Zibin Zheng*

Main category: cs.SE

TL;DR: SWE-Factory is a novel automated pipeline that simplifies and scales the creation of GitHub issue resolution datasets, achieving high accuracy and efficiency in environment setup, grading, and validation, and is available publicly for community use.


<details>
  <summary>Details</summary>
Motivation: Constructing large-scale datasets for evaluating and training Large Language Models (LLMs) on GitHub issue resolution is essential but highly labor-intensive and complex, particularly in environment setup, grading, and validation.

Method: The authors propose SWE-Factory, an automated pipeline composed of: (1) SWE-Builder, a multi-agent system for constructing evaluation environments, (2) a standardized, exit-code-based grading approach, and (3) automated fail2pass validation using exit codes.

Result: Experiments on 671 issues in four programming languages demonstrate that SWE-Factory can generate valid dataset instances efficiently and at low cost, achieving up to 100% grading accuracy and high validation recall and precision.

Conclusion: SWE-Factory significantly streamlines the creation of large-scale, high-quality GitHub issue resolution datasets, thereby facilitating better evaluation and training of LLMs for software engineering tasks.

Abstract: Constructing large-scale datasets for the GitHub issue resolution task is
crucial for both training and evaluating the software engineering capabilities
of Large Language Models (LLMs). However, the traditional process for creating
such benchmarks is notoriously challenging and labor-intensive, particularly in
the stages of setting up evaluation environments, grading test outcomes, and
validating task instances. In this paper, we propose SWE-Factory, an automated
pipeline designed to address these challenges. To tackle these issues, our
pipeline integrates three core automated components. First, we introduce
SWE-Builder, a multi-agent system that automates evaluation environment
construction, which employs four specialized agents that work in a
collaborative, iterative loop and leverages an environment memory pool to
enhance efficiency. Second, we introduce a standardized, exit-code-based
grading method that eliminates the need for manually writing custom parsers.
Finally, we automate the fail2pass validation process using these reliable exit
code signals. Experiments on 671 issues across four programming languages show
that our pipeline can effectively construct valid task instances; for example,
with GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at $0.045 per
instance, while with Gemini-2.5-flash, it achieves comparable performance at
the lowest cost of $0.024 per instance. We also demonstrate that our
exit-code-based grading achieves 100% accuracy compared to manual inspection,
and our automated fail2pass validation reaches a precision of 0.92 and a recall
of 1.00. We hope our automated pipeline will accelerate the collection of
large-scale, high-quality GitHub issue resolution datasets for both training
and evaluation. Our code and datasets are released at
https://github.com/DeepSoftwareAnalytics/swe-factory.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [25] [From Tool Calling to Symbolic Thinking: LLMs in a Persistent Lisp Metaprogramming Loop](https://arxiv.org/abs/2506.10021)
*Jordi de la Torre*

Main category: cs.PL

TL;DR: The paper proposes an architecture combining LLMs with a live Lisp REPL, enabling stateful memory, reflection, and dynamic tool creation through programmatic interaction. It offers a design framework for future interactive, symbolic-neural AI systems.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack strong capabilities for interacting with dynamic, persistent programming environments and typically cannot evolve their toolsets over time. There's an increasing desire to integrate symbolic programming (like Lisp) with neural language models to enhance interaction and tool creation.

Method: The paper introduces an architecture that connects LLMs with a live Lisp REPL via a middleware. It embeds Lisp expressions in generated text, intercepts and executes them, which lets the LLM define and adapt its own tools programmatically while maintaining state.

Result: This approach enables LLMs to use external memory, perform reflective programming, and dynamically create new tools within an interactive environment. The authors also provide a framework and design principles for building such AI systems in the future.

Conclusion: Integrating LLMs with a persistent, interactive symbolic environment (like Lisp REPL) broadens their capabilities, allowing for more sophisticated programming, reflection, and evolving tool use. The proposed architecture serves as a blueprint for more interactive, dynamic AI systems that combine neural and symbolic methods.

Abstract: We propose a novel architecture for integrating large language models (LLMs)
with a persistent, interactive Lisp environment. This setup enables LLMs to
define, invoke, and evolve their own tools through programmatic interaction
with a live REPL. By embedding Lisp expressions within generation and
intercepting them via a middleware layer, the system allows for stateful
external memory, reflective programming, and dynamic tool creation. We present
a design framework and architectural principles to guide future implementations
of interactive AI systems that integrate symbolic programming with neural
language generation.

</details>


### [26] [A Language-Agnostic Logical Relation for Message-Passing Protocols](https://arxiv.org/abs/2506.10026)
*Tesla Zhang,Sonya Simkin,Rui Li,Yue Yao,Stephanie Balzer*

Main category: cs.PL

TL;DR: This paper presents a Coq-mechanized, language-independent logical relation for verifying protocol compliance in heterogeneous, message-passing systems, overcoming limitations of traditional, language-dependent methods.


<details>
  <summary>Details</summary>
Motivation: Modern distributed and heterogeneous systems such as cloud computing and IoT involve concurrent, message-passing applications interfacing with foreign code and devices. Traditional verification methods relying on a common language or type system are inadequate in these settings, motivating the need for new approaches to verify protocol compliance regardless of implementation details.

Method: The paper proposes a framework that utilizes a language-agnostic logical relation founded on labelled transition-based semantics. This approach enables reasoning about protocol compliance for both typed and untyped systems, including foreign objects. The framework and its verification scenarios are mechanized in the Coq theorem prover.

Result: The paper successfully defines and mechanizes the first language-agnostic logical relation for protocol compliance in the Coq theorem prover. The framework is demonstrated through two case studies: verification for specific system instances (including hardware) and universal verification for all well-typed programs within a given type system.

Conclusion: The framework enables rigorous, language-agnostic certification of protocol compliance in heterogeneous message-passing systems, supporting both per-instance and once-and-for-all verification strategies. The work addresses the limitations of prior methods and offers mechanized proofs via Coq.

Abstract: Today's computing landscape has been gradually shifting to applications
targeting distributed and *heterogeneous* systems, such as cloud computing and
Internet of Things (IoT) applications. These applications are predominantly
*concurrent*, employ *message-passing*, and interface with *foreign objects*,
ranging from externally implemented code to actual physical devices such as
sensors. Verifying that the resulting systems adhere to the intended protocol
of interaction is challenging -- the usual assumption of a common
implementation language, let alone a type system, no longer applies, ruling out
any verification method based on them. This paper develops a framework for
certifying *protocol compliance* of heterogeneous message-passing systems. It
contributes the first mechanization of a *language-agnostic logical relation*,
asserting that its inhabitants comply with the protocol specified. This
definition relies entirely on a labelled transition-based semantics,
accommodating arbitrary inhabitants, typed and untyped alike, including foreign
objects. As a case study, the paper considers two scenarios: (1) *per-instance
verification* of a specific application or hardware device, and (2)
*once-and-for-all verification* of well-typed applications for a given type
system. The logical relation and both scenarios are mechanized in the Coq
theorem prover.

</details>


### [27] [Hazel Deriver: A Live Editor for Constructing Rule-Based Derivations](https://arxiv.org/abs/2506.10781)
*Zhiyao Zhong,Cyrus Omar*

Main category: cs.PL

TL;DR: Hazel Deriver is a web-based tool that helps students build derivation trees more easily by providing real-time feedback and structured support, shown to increase understanding and reduce difficulty in learning, though questions remain about the right level of guidance.


<details>
  <summary>Details</summary>
Motivation: Students often find it challenging to construct rule-based derivation trees in programming languages and formal logic courses, hindered by the complexity of inference rules, lack of immediate feedback, and the cumbersome nature of handwritten proofs.

Method: The authors developed Hazel Deriver, a live, web-based editor built on the Hazel programming environment, providing layered scaffolding, interactive experience, and real-time feedback to assist students in constructing derivations.

Result: A preliminary user study with former students demonstrates that Hazel Deriver reduces the perceived difficulty of derivation tasks and improves both conceptual understanding and student engagement.

Conclusion: Hazel Deriver offers an effective, structured environment for learning how to construct derivation trees, supporting greater engagement and understanding, while also raising important questions about finding a balance between system guidance and learner autonomy.

Abstract: Students in programming languages and formal logic courses often struggle
with constructing rule-based derivation trees due to the complexity of applying
inference rules, the lack of immediate feedback, and the manual effort required
for handwritten proofs. We present Hazel Deriver, a live, web-based editor
designed to scaffold derivation construction through multiple layers of
support. Built on the Hazel live programming environment, it provides a
structured, interactive experience that encourages iterative exploration and
real-time feedback. A preliminary user study with former students suggests that
Hazel Deriver reduces the perceived difficulty of derivation tasks while
improving conceptual understanding and engagement. We discuss the design of its
layered scaffolding features and raise questions about balancing system
guidance with learner autonomy.

</details>


### [28] [Choreographic Quick Changes: First-Class Location (Set) Polymorphism](https://arxiv.org/abs/2506.10913)
*Ashley Samuelson,Andrew K. Hirsch,Ethan Cecchetti*

Main category: cs.PL

TL;DR: The paper presents λ_{QC}, a new choreographic language for concurrent systems that allows dynamic decision-making and complex data handling, providing greater expressiveness and safety guarantees, all formally verified.


<details>
  <summary>Details</summary>
Motivation: Existing choreographic programming languages for concurrent systems lack features such as dynamic decision-making on which node performs computations and communication of such decisions to others. Modern applications require more expressive power and flexibility.

Method: The paper introduces λ_{QC}, a new typed choreographic language that supports first-class process names, polymorphism over types and node locations, algebraic and recursive data types, and values located at multiple nodes. The formal properties and correctness of λ_{QC} are mechanically verified in the Rocq framework.

Result: λ_{QC} fills the gap by enabling dynamic task assignment and communication, increasing expressiveness with advanced type features, and maintains strong safety guarantees such as deadlock freedom, as verified in Rocq.

Conclusion: λ_{QC} is the first choreographic language to incorporate advanced features needed for real-world concurrent systems without sacrificing correctness and safety properties, making it significantly more practical and expressive than previous languages in this area.

Abstract: Choreographic programming is a promising new paradigm for programming
concurrent systems where a developer writes a single centralized program that
compiles to individual programs for each node. Existing choreographic
languages, however, lack critical features integral to modern systems, like the
ability of one node to dynamically compute who should perform a computation and
send that decision to others. This work addresses this gap with $\lambda_{QC}$,
the first typed choreographic language with \emph{first class process names}
and polymorphism over both types and (sets of) locations. $\lambda_{QC}$ also
improves expressive power over previous work by supporting algebraic and
recursive data types as well as multiply-located values. We formalize and
mechanically verify our results in Rocq, including the standard choreographic
guarantee of deadlock freedom.

</details>
