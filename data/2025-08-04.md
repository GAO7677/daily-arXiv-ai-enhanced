<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 19]
- [cs.PL](#cs.PL) [Total: 6]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Git Context Controller: Manage the Context of LLM-based Agents like Git](https://arxiv.org/abs/2508.00031)
*Junde Wu*

Main category: cs.SE

TL;DR: The paper presents GCC, a Git-inspired context manager for LLM agents that significantly enhances their ability to handle long-term, complex tasks by using versioned memory operations. This approach beats existing baselines for bug fixing and task resolution in software development workflows.


<details>
  <summary>Details</summary>
Motivation: Standard LLM-based agents struggle with context management in long, complex tasks such as large software projects, limiting their effectiveness in these scenarios.

Method: The paper introduces Git-Context-Controller (GCC), a context management framework inspired by version control systems like Git. GCC treats agent memory as a versioned, hierarchical file system, and introduces operations such as COMMIT, BRANCH, MERGE, and CONTEXT to structure and manage context and memory for LLM agents.

Result: GCC-equipped agents outperform 26 other competitive systems on the SWE-Bench-Lite benchmark, resolving 48% of software bugs. In a self-replication case study, agents with GCC built a new CLI agent from scratch, resolving 40.7% of tasks versus only 11.7% without GCC.

Conclusion: Structured, versioned context management substantially improves LLM agent performance on long-horizon and complex software tasks. GCC enables efficient milestone tracking, alternative solution exploration, memory isolation, and transfer, leading to state-of-the-art results. The framework is openly available for further research and application.

Abstract: Large language model (LLM) based agents have shown impressive capabilities by
interleaving internal reasoning with external tool use. However, as these
agents are deployed in long-horizon workflows, such as coding for a big,
long-term project, context management becomes a critical bottleneck. We
introduce Git-Context-Controller (GCC), a structured context management
framework inspired by software version control systems. GCC elevates context as
versioned memory hierarchy like Git. It structures agent memory as a persistent
file system with explicit operations: COMMIT, BRANCH, MERGE, and CONTEXT,
enabling milestone-based checkpointing, exploration of alternative plans, and
structured reflection. Our approach empowers agents to manage long-term goals,
isolate architectural experiments, and recover or hand off memory across
sessions and agents. Empirically, agents equipped with GCC achieve
state-of-the-art performance on the SWE-Bench-Lite benchmark, resolving 48.00
of software bugs, outperforming 26 competitive systems. In a self-replication
case study, a GCC-augmented agent builds a new CLI agent from scratch,
achieving 40.7 task resolution, compared to only 11.7 without GCC. The code is
released at: https://github.com/theworldofagents/GCC

</details>


### [2] [GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries](https://arxiv.org/abs/2508.00033)
*Nuno Fachada,Daniel Fernandes,Carlos M. Fernandes,Bruno D. Ferreira-Saraiva,Jo√£o P. Matos-Carvalho*

Main category: cs.SE

TL;DR: Most LLMs struggle with unfamiliar Python APIs for complex scientific code tasks, except for GPT-4.1. Success depends on both LLM capabilities and library documentation, highlighting areas for improvement in automation for scientific research.


<details>
  <summary>Details</summary>
Motivation: To evaluate how well current large language models (LLMs) can generate correct and functional Python code when working with unfamiliar APIs in scientific contexts, a scenario relevant to automated scientific research.

Method: The authors benchmark several state-of-the-art LLMs using two challenging tasks: conversational data analysis with the ParShift library, and synthetic data generation plus clustering with pyclugen and scikit-learn. Zero-shot, structured prompts are used with no in-context examples, and model outputs are assessed quantitatively (for correctness and prompt compliance) and qualitatively (by analyzing errors).

Result: Few LLMs could consistently generate correct, executable code for the specified tasks. Only GPT-4.1 succeeded in every attempt. The evaluation also surfaced issues in the third-party libraries themselves.

Conclusion: Current LLMs have significant limitations in automated scientific code generation, especially when dealing with unfamiliar or poorly documented APIs. Improved prompt design, better documentation, and further LLM development are needed for reliable scientific automation.

Abstract: Large Language Models (LLMs) have advanced rapidly as tools for automating
code generation in scientific research, yet their ability to interpret and use
unfamiliar Python APIs for complex computational experiments remains poorly
characterized. This study systematically benchmarks a selection of
state-of-the-art LLMs in generating functional Python code for two increasingly
challenging scenarios: conversational data analysis with the \textit{ParShift}
library, and synthetic data generation and clustering using \textit{pyclugen}
and \textit{scikit-learn}. Both experiments use structured, zero-shot prompts
specifying detailed requirements but omitting in-context examples. Model
outputs are evaluated quantitatively for functional correctness and prompt
compliance over multiple runs, and qualitatively by analyzing the errors
produced when code execution fails. Results show that only a small subset of
models consistently generate correct, executable code, with GPT-4.1 standing
out as the only model to always succeed in both tasks. In addition to
benchmarking LLM performance, this approach helps identify shortcomings in
third-party libraries, such as unclear documentation or obscure implementation
bugs. Overall, these findings highlight current limitations of LLMs for
end-to-end scientific automation and emphasize the need for careful prompt
design, comprehensive library documentation, and continued advances in language
model capabilities.

</details>


### [3] [Machine Learning Pipeline for Software Engineering: A Systematic Literature Review](https://arxiv.org/abs/2508.00045)
*Samah Kansab*

Main category: cs.SE

TL;DR: This paper reviews recent advances in machine learning pipelines for software engineering, highlighting effective methods, best practices, and evaluation metrics. It finds that robust preprocessing and ensemble models improve outcomes and offers guidance to boost software quality and efficiency using ML.


<details>
  <summary>Details</summary>
Motivation: Software engineering systems have become increasingly complex, making traditional quality assurance and efficiency approaches inadequate. The paper is motivated by the need to address longer debugging times, inefficient defect detection, and high resource usage. It explores how machine learning (ML) can address these challenges in the software engineering lifecycle.

Method: The paper conducts a systematic literature review (SLR) focused on machine learning pipelines in software engineering. It surveys the literature to consolidate best practices, challenges, and knowledge gaps in the application of ML to software engineering tasks such as defect prediction and code review.

Result: Robust preprocessing techniques (like SMOTE and SZZ-based feature selection) significantly improve ML model reliability. Ensemble methods (e.g., Random Forest, Gradient Boosting) are top performers across various tasks. While simpler models offer efficiency and interpretability, ensemble models usually yield better predictive performance. Standard evaluation metrics are AUC, F1-score, and precision, but new ones like Best Arithmetic Mean are being explored. Bootstrapping is frequently used to validate model stability and generalizability.

Conclusion: Well-constructed ML pipelines are crucial for improving software quality and efficiency. The study offers actionable recommendations for optimizing these pipelines, identifies key trends and gaps, and lays the groundwork for future ML adoption and innovation in software engineering.

Abstract: The rapid advancement of software development practices has introduced
challenges in ensuring quality and efficiency across the software engineering
(SE) lifecycle. As SE systems grow in complexity, traditional approaches often
fail to scale, resulting in longer debugging times, inefficient defect
detection, and resource-heavy development cycles. Machine Learning (ML) has
emerged as a key solution, enabling automation in tasks such as defect
prediction, code review, and release quality estimation. However, the
effectiveness of ML in SE depends on the robustness of its pipeline, including
data collection, preprocessing, feature engineering, algorithm selection,
validation, and evaluation.
  This systematic literature review (SLR) examines state-of-the-art ML
pipelines designed for SE, consolidating best practices, challenges, and gaps.
Our findings show that robust preprocessing, such as SMOTE for data balancing
and SZZ-based algorithms for feature selection, improves model reliability.
Ensemble methods like Random Forest and Gradient Boosting dominate performance
across tasks, while simpler models such as Naive Bayes remain valuable for
efficiency and interpretability. Evaluation metrics including AUC, F1-score,
and precision are most common, with new metrics like Best Arithmetic Mean (BAM)
emerging in niche applications. Validation techniques such as bootstrapping are
widely used to ensure model stability and generalizability.
  This SLR highlights the importance of well-designed ML pipelines for
addressing SE challenges and provides actionable insights for researchers and
practitioners seeking to optimize software quality and efficiency. By
identifying gaps and trends, this study sets a foundation for advancing ML
adoption and fostering innovation in increasingly complex development
environments.

</details>


### [4] [Functional vs. Object-Oriented: Comparing How Programming Paradigms Affect the Architectural Characteristics of Systems](https://arxiv.org/abs/2508.00244)
*Briza Mel Dias de Sousa,Renato Cordeiro Ferreira,Alfredo Goldman*

Main category: cs.SE

TL;DR: This paper compares OOP (Kotlin) and FP (Scala) via a Digital Wallet system. Using both qualitative and quantitative methods, it shows how each paradigm impacts software architecture, helping developers pick the best approach for future projects.


<details>
  <summary>Details</summary>
Motivation: Functional programming (FP) is gaining traction after years of object-oriented programming (OOP) dominance. The paper aims to understand how each paradigm affects software architecture, providing insights for more informed development choices.

Method: The study compares OOP and FP by implementing a Digital Wallet system in Kotlin (OOP) and Scala (FP). It employs qualitative (self-ethnography) and quantitative (surveys) analyses to evaluate the architectural impact of each paradigm.

Result: Qualitative analysis offers a direct side-by-side comparison of coding experience and design implementation. The quantitative survey reveals how developers with different backgrounds perceive and evaluate the resulting code.

Conclusion: The findings illustrate distinct influences of OOP and FP on architectural characteristics, aiding developers and organizations in choosing the most suitable paradigm for their needs.

Abstract: After decades of dominance by object-oriented programming (OOP), functional
programming (FP) is gaining increasing attention in the software industry. This
study compares the impact of OOP and FP on the architectural characteristics of
software systems. For that, it examines the design and implementation of a
Digital Wallet system, developed in Kotlin (representing OOP) and Scala
(representing FP). The comparison is made through both qualitative and
quantitative analyses to explore how each paradigm influences the system's
architectural characteristics. The self-ethnographic qualitative analysis
provides a side-by-side comparison of both implementations, revealing the
perspective of those writing such code. The survey-based quantitative analysis
gathers feedback from developers with diverse backgrounds, showing their
impressions of those reading this code. Hopefully, these results may be useful
for developers or organizations seeking to make more informed decisions about
which paradigm is best suited for their next project.

</details>


### [5] [A Survey on Code Generation with LLM-based Agents](https://arxiv.org/abs/2508.00083)
*Yihong Dong,Xue Jiang,Jiaru Qian,Tian Wang,Kechi Zhang,Zhi Jin,Ge Li*

Main category: cs.SE

TL;DR: This paper surveys the quickly developing field of code generation agents powered by large language models, outlining their evolution, technical approaches, real-world applications, evaluation practices, key tools, and future challenges. It serves as a comprehensive guide and roadmap for researchers and practitioners interested in this transformative technology.


<details>
  <summary>Details</summary>
Motivation: The motivation is to synthesize and clarify the fast-evolving field of LLM-based code generation agents, which are transforming software development by introducing greater autonomy, broader capabilities, and a focus on solving engineering challenges. With rapid growth and increasing practical application, a comprehensive survey is needed to organize current progress and outline future research directions.

Method: The paper takes a systematic survey approach, tracing the historical development of LLM-based code generation agents, categorizing key techniques (including single-agent and multi-agent systems), reviewing applications throughout the software development lifecycle, summarizing benchmarks and metrics, cataloging representative tools, and analyzing challenges to recommend research directions.

Result: The study provides a structured overview of how LLM-based agents have evolved, describes core technical categories, details their application across the SDLC, presents mainstream evaluation methods and tools, and identifies both current obstacles and prospective research directions for continued advancement.

Conclusion: The survey concludes that LLM-based code generation agents are rapidly advancing and hold significant promise for revolutionizing software engineering practices. By systematically categorizing the field and highlighting challenges, the paper establishes a foundation for future research and practical development in this area.

Abstract: Code generation agents powered by large language models (LLMs) are
revolutionizing the software development paradigm. Distinct from previous code
generation techniques, code generation agents are characterized by three core
features. 1) Autonomy: the ability to independently manage the entire workflow,
from task decomposition to coding and debugging. 2) Expanded task scope:
capabilities that extend beyond generating code snippets to encompass the full
software development lifecycle (SDLC). 3) Enhancement of engineering
practicality: a shift in research emphasis from algorithmic innovation toward
practical engineering challenges, such as system reliability, process
management, and tool integration. This domain has recently witnessed rapid
development and an explosion in research, demonstrating significant application
potential. This paper presents a systematic survey of the field of LLM-based
code generation agents. We trace the technology's developmental trajectory from
its inception and systematically categorize its core techniques, including both
single-agent and multi-agent architectures. Furthermore, this survey details
the applications of LLM-based agents across the full SDLC, summarizes
mainstream evaluation benchmarks and metrics, and catalogs representative
tools. Finally, by analyzing the primary challenges, we identify and propose
several foundational, long-term research directions for the future work of the
field.

</details>


### [6] [Desyan: A Platform for Seamless Value-Flow and Symbolic Analysis](https://arxiv.org/abs/2508.00508)
*Panagiotis Diamantakis,Thanassis Avgerinos,Yannis Smaragdakis*

Main category: cs.SE

TL;DR: Desyan is a new program analysis platform that effectively combines high-performance value-flow analysis with advanced symbolic reasoning (including SMT solving), delivering significant speedups and offering flexible integration of analysis techniques.


<details>
  <summary>Details</summary>
Motivation: There are two dominant types of static program analyses: value-flow analysis (like data-flow or points-to analysis) and symbolic analysis (like symbolic execution). Despite each method's success, they remain separated due to the lack of a unified platform that efficiently integrates both high-performance data-flow with symbolic reasoning.

Method: The authors introduce Desyan, a platform that integrates value-flow and symbolic reasoning. Desyan enhances the Souffl√© Datalog engine with support for SMT solving and offers constructs for efficiently handling common program analysis patterns. It supports multiple modes: high-performance Datalog evaluation, integration with external SMT solvers for complex symbolic reasoning, and fast, Datalog-native symbolic reasoning for simpler cases.

Result: Desyan enables seamless blending of value-flow and symbolic reasoning. It achieves state-of-the-art performance for value-flow analyses, with speedups over 20x, and approaches to symbolic reasoning that are significantly faster (over 2x) than always resorting to SMT solvers. It provides efficiency and flexibility for complex program analyses requiring both analytic paradigms.

Conclusion: Desyan successfully bridges the gap between value-flow and symbolic analysis, providing a powerful, efficient, and flexible platform for advanced program analysis tasks. The approach leverages the strengths of both paradigms and offers substantial performance improvements.

Abstract: Over the past two decades, two different types of static analyses have
emerged as dominant paradigms both in academia and industry: value-flow
analysis (e.g., data-flow analysis or points-to analysis) and symbolic analysis
(e.g., symbolic execution). Despite their individual successes in numerous
application fields, the two approaches have remained largely separate; an
artifact of the simple reality that there is no broadly adopted unifying
platform for effortless and efficient integration of symbolic techniques with
high-performance data-flow reasoning.
  To bridge this gap, we introduce Desyan: a platform for writing program
analyses with seamless integration of value-flow and symbolic reasoning. Desyan
expands a production-ready Datalog fixpoint engine (Souffl\'e) with
full-fledged SMT solving invoking industry-leading SMT engines. Desyan provides
constructs for automatically (and efficiently!) handling typical patterns that
come up in program analysis. At the same time, the integration is agnostic with
respect to the solving technology, and supports Datalog-native symbolic
reasoning, via a bottom-up algebraic reasoning module.
  The result is an engine that allows blending different kinds of reasoning, as
needed for the underlying analysis. For value-flow analysis, the engine is the
best-in-class Datalog evaluator (often by a factor of over 20x in execution
time); for applications that require full SMT (e.g., a concolic execution
engine or other symbolic evaluator that needs to solve arbitrarily complex
conditions), the engine is leveraging the leading SMT solvers; for lightweight
symbolic evaluation (e.g., solving simple conditionals in the context of a
path-sensitive analysis), the engine can use Datalog-native symbolic reasoning,
achieving large speedups (often of over 2x) compared to eagerly appealing to an
SMT solver.

</details>


### [7] [How Quantization Impacts Privacy Risk on LLMs for Code?](https://arxiv.org/abs/2508.00128)
*Md Nazmul Haque,Hua Yang,Zhou Yang,Bowen Xu*

Main category: cs.SE

TL;DR: This paper shows that quantization (compressing) large language models for code both lowers privacy risk (membership inference) and maintains good task performance, revealing a tradeoff between the two. Larger, quantized models can be a better choice than smaller, uncompressed ones for balancing privacy and utility.


<details>
  <summary>Details</summary>
Motivation: Large language models for code (LLMs4Code) are trained on massive datasets that can include sensitive information, raising significant privacy concerns. As quantization is widely used to compress these models for efficient deployment, it is crucial to understand whether such compression methods impact the models' tendency to leak training data, assessed via membership inference attacks.

Method: The authors conduct the first empirical study on the impact of quantization on both task performance and privacy risk in LLMs4Code. They apply static and dynamic quantization techniques to three representative model families (Pythia, CodeGen, GPTNeo), and evaluate the models using membership inference attacks to assess privacy risks. The study examines the relationship between quantization level, model size, performance, and privacy across different architectures and MI methods.

Result: Quantization significantly reduces the privacy risk of LLMs4Code compared to the original full-precision models. The study finds a positive correlation between model performance and privacy risk, highlighting a tradeoff. Larger, quantized models can strike a better balance between performance and privacy than smaller, full-precision models. These results generalize across architectures, sizes, and MI attack methods.

Conclusion: Quantization not only provides computational benefits but also reduces the privacy risk in LLMs4Code. Model compression thus supports safer deployment in privacy-sensitive environments, but there is a tradeoff between maintaining model performance and minimizing privacy leakage. Careful model selection and quantization strategies can optimize both.

Abstract: Large language models for code (LLMs4Code) rely heavily on massive training
data, including sensitive data, such as cloud service credentials of the
projects and personal identifiable information of the developers, raising
serious privacy concerns. Membership inference (MI) has recently emerged as an
effective tool for assessing privacy risk by identifying whether specific data
belong to a model's training set. In parallel, model compression techniques,
especially quantization, have gained traction for reducing computational costs
and enabling the deployment of large models. However, while quantized models
still retain knowledge learned from the original training data, it remains
unclear whether quantization affects their ability to retain and expose privacy
information. Answering this question is of great importance to understanding
privacy risks in real-world deployments. In this work, we conduct the first
empirical study on how quantization influences task performance and privacy
risk simultaneously in LLMs4Code. To do this, we implement widely used
quantization techniques (static and dynamic) to three representative model
families, namely Pythia, CodeGen, and GPTNeo. Our results demonstrate that
quantization has a significant impact on reducing the privacy risk relative to
the original model. We also uncover a positive correlation between task
performance and privacy risk, indicating an underlying tradeoff. Moreover, we
reveal the possibility that quantizing larger models could yield better balance
than using full-precision small models. Finally, we demonstrate that these
findings generalize across different architectures, model sizes and MI methods,
offering practical guidance for safeguarding privacy when deploying compressed
LLMs4Code.

</details>


### [8] [From Code to Career: Assessing Competitive Programmers for Industry Placement](https://arxiv.org/abs/2508.00772)
*Md Imranur Rahman Akib,Fathima Binthe Muhammed,Umit Saha,Md Fazlul Karim Patwary,Mehrin Anannya,Md Alomgeer Hussein,Md Biplob Hosen*

Main category: cs.SE

TL;DR: This paper presents a machine learning-driven system that predicts a programmer's job readiness using Codeforces data. By analyzing coding metrics and applying a Random Forest classifier, the model effectively categorizes users into employability levels. This approach proves reliable and could help automate career assessments in tech.


<details>
  <summary>Details</summary>
Motivation: There is a demand for tools that objectively assess a programmer's job readiness using data from their coding performance, given the competitive and rapidly evolving tech industry.

Method: The study collects user activity data from Codeforces using its API, extracts key performance metrics, and applies a Random Forest classifier to predict users' employability across four levels. The model is built into a web-based system using Flask and hosted on Render for real-time predictions.

Result: The model successfully classifies users into different employability tiers, effectively differentiating skill levels based on coding activity and proficiency. The system provides real-time assessment to predict the likelihood of securing jobs at varying tiers of the tech industry.

Conclusion: Machine learning models, specifically Random Forests, can reliably evaluate and distinguish programmer job readiness using competitive programming data, setting the stage for broader applications in technical career assessment.

Abstract: In today's fast-paced tech industry, there is a growing need for tools that
evaluate a programmer's job readiness based on their coding performance. This
study focuses on predicting the potential of Codeforces users to secure various
levels of software engineering jobs. The primary objective is to analyze how a
user's competitive programming activity correlates with their chances of
obtaining positions, ranging from entry-level roles to jobs at major tech
companies. We collect user data using the Codeforces API, process key
performance metrics, and build a prediction model using a Random Forest
classifier. The model categorizes users into four levels of employability,
ranging from those needing further development to those ready for top-tier tech
jobs. The system is implemented using Flask and deployed on Render for
real-time predictions. Our evaluation demonstrates that the approach
effectively distinguishes between different skill levels based on coding
proficiency and participation. This work lays a foundation for the use of
machine learning in career assessment and could be extended to predict job
readiness in broader technical fields.

</details>


### [9] [Testing the Untestable? An Empirical Study on the Testing Process of LLM-Powered Software Systems](https://arxiv.org/abs/2508.00198)
*Cleyton Magalhaes,Italo Santos,Brody Stuart-Verner,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: Real-world testing of LLM-powered software blends manual and automated techniques, faces unique challenges (like hallucinations and prompt sensitivity), and demands new verification approaches beyond traditional methods.


<details>
  <summary>Details</summary>
Motivation: While LLMs are increasingly used in software systems for various tasks, there has been little research on how such systems are tested in real-world development, especially at the full system level.

Method: An exploratory case study was conducted, analyzing 99 reports from students who developed and deployed LLM-powered applications within a university course. Thematic analysis and structured coding were used to assess the testing approaches described in these reports.

Result: Developers used a mix of manual and automated testing strategies, including exploratory testing, unit testing, and prompt iteration, to test both traditional system logic and the unique behavior of LLMs. Common challenges included integration issues, unpredictable model outputs, prompt sensitivity, hallucinations, and difficulty verifying correctness.

Conclusion: Testing LLM-based systems requires rethinking traditional verification by combining code-level checks with assessments of LLM behavior. The study sheds light on practical strategies and challenges in testing generative AI within software systems.

Abstract: Background: Software systems powered by large language models are becoming a
routine part of everyday technologies, supporting applications across a wide
range of domains. In software engineering, many studies have focused on how
LLMs support tasks such as code generation, debugging, and documentation.
However, there has been limited focus on how full systems that integrate LLMs
are tested during development. Aims: This study explores how LLM-powered
systems are tested in the context of real-world application development.
Method: We conducted an exploratory case study using 99 individual reports
written by students who built and deployed LLM-powered applications as part of
a university course. Each report was independently analyzed using thematic
analysis, supported by a structured coding process. Results: Testing strategies
combined manual and automated methods to evaluate both system logic and model
behavior. Common practices included exploratory testing, unit testing, and
prompt iteration. Reported challenges included integration failures,
unpredictable outputs, prompt sensitivity, hallucinations, and uncertainty
about correctness. Conclusions: Testing LLM-powered systems required
adaptations to traditional verification methods, blending source-level
reasoning with behavior-aware evaluations. These findings provide evidence on
the practical context of testing generative components in software systems.

</details>


### [10] [Leveraging Large Language Model for Information Retrieval-based Bug Localization](https://arxiv.org/abs/2508.00253)
*Moumita Asad,Rafed Muhammad Yasir,Armin Geramirad,Sam Malek*

Main category: cs.SE

TL;DR: GenLoc, an LLM-driven bug localization system, significantly surpasses existing methods in accuracy by intelligently analyzing and exploring code, tackling a major challenge in the field.


<details>
  <summary>Details</summary>
Motivation: Existing Information Retrieval-based bug localization techniques struggle due to vocabulary mismatches between bug reports and source code, limiting their effectiveness.

Method: The paper proposes GenLoc, an LLM-based bug localization approach. GenLoc leverages a Large Language Model with code-exploration functions to iteratively analyze the codebase and identify buggy files. It can also retrieve semantically relevant files using vector embeddings for better context.

Result: GenLoc was evaluated on 9,000+ real-world bug reports from six large-scale Java projects. It consistently outperformed five state-of-the-art techniques, with more than 60% improvement in Accuracy@1 across multiple metrics.

Conclusion: GenLoc, the proposed LLM-based approach, effectively addresses vocabulary mismatch problems in bug localization and achieves notable accuracy improvements compared to leading methods.

Abstract: Information Retrieval-based Bug Localization aims to identify buggy source
files for a given bug report. While existing approaches -- ranging from vector
space models to deep learning models -- have shown potential in this domain,
their effectiveness is often limited by the vocabulary mismatch between bug
reports and source code. To address this issue, we propose a novel Large
Language Model (LLM) based bug localization approach, called GenLoc. Given a
bug report, GenLoc leverages an LLM equipped with code-exploration functions to
iteratively analyze the code base and identify potential buggy files. To gather
better context, GenLoc may optionally retrieve semantically relevant files
using vector embeddings. GenLoc has been evaluated on over 9,000 real-world bug
reports from six large-scale Java projects. Experimental results show that
GenLoc outperforms five state-of-the-art bug localization techniques across
multiple metrics, achieving an average improvement of more than 60\% in
Accuracy@1.

</details>


### [11] [Accurate and Consistent Graph Model Generation from Text with Large Language Models](https://arxiv.org/abs/2508.00255)
*Boqi Chen,Ou Wei,Bingzhou Zheng,Gunter Mussbacher*

Main category: cs.SE

TL;DR: The paper proposes an abstraction-concretization framework that aggregates and refines multiple LLM outputs for graph model generation, significantly improving consistency and quality beyond standard LLM approaches.


<details>
  <summary>Details</summary>
Motivation: LLM-based graph model generation suffers from issues of syntax violations, constraint inconsistencies, and inaccuracies due to inherent uncertainty in large language models. While syntax issues can often be addressed, problems with constraint violations and inaccuracies remain unsolved, motivating the need for a new approach.

Method: The authors propose a novel abstraction-concretization framework. This framework aggregates multiple candidate outputs from an LLM into a probabilistic partial model and then refines it into a concrete model that satisfies all domain constraints, improving consistency and quality.

Result: Experimental evaluation over several LLMs and datasets shows that this framework significantly enhances the consistency and overall quality of automatically generated graph models compared to standard LLM-based approaches.

Conclusion: The abstraction-concretization framework effectively addresses constraint inconsistencies and inaccuracies in LLM-generated graph models, yielding more reliable and higher-quality outputs.

Abstract: Graph model generation from natural language description is an important task
with many applications in software engineering. With the rise of large language
models (LLMs), there is a growing interest in using LLMs for graph model
generation. Nevertheless, LLM-based graph model generation typically produces
partially correct models that suffer from three main issues: (1) syntax
violations: the generated model may not adhere to the syntax defined by its
metamodel, (2) constraint inconsistencies: the structure of the model might not
conform to some domain-specific constraints, and (3) inaccuracy: due to the
inherent uncertainty in LLMs, the models can include inaccurate, hallucinated
elements. While the first issue is often addressed through techniques such as
constraint decoding or filtering, the latter two remain largely unaddressed.
Motivated by recent self-consistency approaches in LLMs, we propose a novel
abstraction-concretization framework that enhances the consistency and quality
of generated graph models by considering multiple outputs from an LLM. Our
approach first constructs a probabilistic partial model that aggregates all
candidate outputs and then refines this partial model into the most appropriate
concrete model that satisfies all constraints. We evaluate our framework on
several popular open-source and closed-source LLMs using diverse datasets for
model generation tasks. The results demonstrate that our approach significantly
improves both the consistency and quality of the generated graph models.

</details>


### [12] [Benchmarking LLMs for Unit Test Generation from Real-World Functions](https://arxiv.org/abs/2508.00408)
*Dong Huang,Jie M. Zhang,Mark Harman,Qianru Zhang,Mingzhe Du,See-Kiong Ng*

Main category: cs.SE

TL;DR: The paper introduces ULT, a new, realistic benchmark for unit test generation using LLMs, and shows that LLMs perform much worse on it than on existing benchmarks, revealing prior overestimations of LLM capabilities due to simpler, contaminated datasets.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) have potential to automate unit test generation, but there is a need for reliable benchmarks that reflect real-world complexity and avoid biases such as data contamination and overly simple code.

Method: The authors introduce ULT (UnLeakedTestbench), a new benchmark for unit test generation from real-world Python functions, created through a multi-stage curation process to ensure higher code complexity and avoid test-case contamination. They also provide PLT (PreLeakedTestbench), a paired benchmark with intentionally leaked tests, for analyzing LLM memorization versus reasoning capabilities. The performance of LLMs is evaluated across ULT, PLT, and existing benchmarks.

Result: LLMs achieve significantly lower performance on ULT (e.g., 41.32% accuracy, 45.10% statement coverage) compared to TestEval and PLT, demonstrating that ULT is more challenging and that previous benchmarks may overestimate LLM capabilities due to contamination and simplicity.

Conclusion: ULT provides a more challenging and realistic benchmark for evaluating LLMs' test generation abilities, highlighting the limitations of earlier benchmarks and emphasizing the need for robust, contamination-free evaluation environments.

Abstract: Recently, large language models (LLMs) have shown great promise in automating
unit test generation, significantly reducing the manual effort required by
developers. To effectively evaluate the capabilities of LLMs in this domain, it
is crucial to have a well-designed benchmark that accurately reflects
real-world scenarios and mitigates common pitfalls. Existing LLM test
generation benchmarks are limited by two critical drawbacks: data contamination
and structurally simple function code. As a result, we often cannot rely on the
validity of scientific conclusions drawn from empirical studies using these
limited benchmarks. The empirical evidence presented may be biased due to
contamination and may fail to generalize beyond toy programs due to structural
simplicity.
  To address these problems, we introduce ULT (UnLeakedTestbench), a new
benchmark specifically designed for function-level unit test generation from
real-world Python functions. ULT is constructed through a multi-stage curation
process that ensures high cyclomatic complexity and mitigates test case
contamination. With 3,909 carefully selected function-level tasks, ULT provides
a more realistic and challenging evaluation of LLMs' test generation
capabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT
with leaked tests designed to enable a controlled analysis of memorization
versus reasoning in test generation. Our evaluation results demonstrate that
ULT is significantly more challenging. For example, test cases generated by
LLMs only achieve 41.32\%, 45.10\%, 30.22\%, and 40.21\% for accuracy,
statement coverage, branch coverage, and mutation score on average for all
LLMs, respectively. These results are substantially lower than the
corresponding metrics on TestEval (91.79\%, 92.18\%, 82.04\%, and 49.69\%) and
PLT (47.07\%, 55.13\%, 40.07\%, and 50.80\%).

</details>


### [13] [Managing Power Gaps as a Topic of Pair Programming Skill: A Grounded Theory](https://arxiv.org/abs/2508.00462)
*Linus Ververs,Lutz Prechelt*

Main category: cs.SE

TL;DR: Pair programming sessions in industry often display 'Power Gaps'‚Äîunequal participation due to power dynamics‚Äîwhich hurt knowledge sharing and code quality. Avoiding hierarchical behaviors and encouraging equal participation are key to improving outcomes.


<details>
  <summary>Details</summary>
Motivation: Pair programming is widely used in professional software development, but there is limited understanding of power dynamics within pairs and how these affect outcomes. The study aims to identify and analyze power-related phenomena to offer practical advice for improving pair programming practices.

Method: The researchers analyzed 22 industrial pair programming sessions using Grounded Theory Methodology to develop a theory of power-related behaviors. They then conducted a survey with 292 participants to validate the generalizability and frequency of the identified phenomena.

Result: The study identifies the phenomenon of the 'Power Gap,' which is a perceived difference in participation opportunities between pair members. Power Gaps are associated with negative effects on knowledge transfer, code quality, and process efficiency. The study also differentiates between Hierarchical Behaviors (which create/increase Power Gaps) and Equalizing Behaviors (which prevent/reduce them). Survey results confirm these phenomena are common in real practice.

Conclusion: To improve pair programming outcomes, practitioners should focus on minimizing Power Gaps. This involves avoiding Hierarchical Behaviors and actively engaging in Equalizing Behaviors to ensure equal participation and reduce the negative impacts associated with Power Gaps.

Abstract: Context: Pair Programming as a work mode is used (occasionally or frequently)
throughout professional software development. Objective: Understand what
power-related phenomena occur in pair programming as it is used in industry;
give advice to practitioners on how to do better pair programming. Method:
Analyze 22 industrial pair programming sessions using Grounded Theory
Methodology. Formulate a Grounded Theory on power-related behaviors. Run a
survey with 292 participants about that theory. Use it to demonstrate that the
phenomena are common. Results: Our theory describes the phenomenon of Power
Gap: a perceived difference in participation opportunities. The theory shows
the behaviors that create a Power Gap or result from it. Power Gaps tend to
damage knowledge transfer, code quality, and process effi ciency. The survey
results show that all concepts from our theory are frequent in practice. They
also provide more grounding for concepts that are observable only indirectly.
Conclusions: It is a valuable component of pair programming skill to be able to
avoid Power Gaps. Specifically, pair partners need to avoid Hierarchical
Behavior (which tends to create or increase a Power Gap) and should perform
enough Equalizing Behavior (which prevents or reduces a Power Gap).

</details>


### [14] [SPENCER: Self-Adaptive Model Distillation for Efficient Code Retrieval](https://arxiv.org/abs/2508.00546)
*Wenchao Gu,Zongyi Lyu,Yanlin Wang,Hongyu Zhang,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: SPENCER improves code retrieval by merging efficient dual-encoders and accurate cross-encoders with adaptive model distillation, significantly boosting performance and efficiency compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional code retrieval methods predominantly use dual-encoder models due to their efficiency. However, these models lack deep interaction between code and description, limiting retrieval performance. There is a need to improve effectiveness without sacrificing efficiency.

Method: The proposed framework, SPENCER, combines a dual-encoder to efficiently narrow the search space with a cross-encoder for increased retrieval accuracy. Additionally, a novel model distillation technique and an adaptive teaching assistant selection strategy are introduced to improve efficiency and maintain high performance.

Result: Experiments show that SPENCER significantly outperforms models using only dual-encoders for code retrieval. The model distillation technique reduces inference time by 70% and retains over 98% of model performance.

Conclusion: Combining dual-encoder and cross-encoder architectures with self-adaptive model distillation offers a highly efficient yet accurate code retrieval approach. The adaptive selection of teaching assistant models during distillation ensures robustness across different pre-trained models.

Abstract: Code retrieval aims to provide users with desired code snippets based on
users' natural language queries. With the development of deep learning
technologies, adopting pre-trained models for this task has become mainstream.
Considering the retrieval efficiency, most of the previous approaches adopt a
dual-encoder for this task, which encodes the description and code snippet into
representation vectors, respectively. However, the model structure of the
dual-encoder tends to limit the model's performance, since it lacks the
interaction between the code snippet and description at the bottom layer of the
model during training. To improve the model's effectiveness while preserving
its efficiency, we propose a framework, which adopts Self-AdaPtive Model
Distillation for Efficient CodE Retrieval, named SPENCER. SPENCER first adopts
the dual-encoder to narrow the search space and then adopts the cross-encoder
to improve accuracy. To improve the efficiency of SPENCER, we propose a novel
model distillation technique, which can greatly reduce the inference time of
the dual-encoder while maintaining the overall performance. We also propose a
teaching assistant selection strategy for our model distillation, which can
adaptively select the suitable teaching assistant models for different
pre-trained models during the model distillation to ensure the model
performance. Extensive experiments demonstrate that the combination of
dual-encoder and cross-encoder improves overall performance compared to solely
dual-encoder-based models for code retrieval. Besides, our model distillation
technique retains over 98% of the overall performance while reducing the
inference time of the dual-encoder by 70%.

</details>


### [15] [Can User Feedback Help Issue Detection? An Empirical Study on a One-billion-user Online Service System](https://arxiv.org/abs/2508.00593)
*Shuyao Jiang,Jiazhen Gu,Wujie Zheng,Yangfan Zhou,Michael R. Lyu*

Main category: cs.SE

TL;DR: Mass user feedback often contains irrelevant information, so effective filtering is essential. Severe issues aren't always obvious from feedback characteristics alone, but stable topic patterns mean machine learning can help. These findings support better feedback-based issue detection in big online systems.


<details>
  <summary>Details</summary>
Motivation: The motivation for this study comes from the challenge of detecting severe issues from massive amounts of natural language user feedback in large-scale online service systems. As user feedback can potentially assist in issue detection, understanding its nature and how it can be effectively analyzed is vital for improving feedback-based detection methods.

Method: The authors conducted an empirical study using 50,378,766 user feedback items collected from six real-world services within a large online system. They analyzed what users report in their feedback, examined features that might indicate severe issues, and evaluated the feasibility of using machine learning techniques for feedback analysis.

Result: The study found that much user feedback is irrelevant to system issues, making it crucial to filter out such noise. Some severe issues are not easily identified from feedback characteristics alone. They also observed that topic distributions in feedback are consistent over time, suggesting that machine learning approaches for detecting issues from feedback are feasible.

Conclusion: Their empirical findings lay the groundwork for future feedback-based issue detection systems in large-scale service environments. The insights provided guide the development of more practical and robust detection approaches.

Abstract: Background: It has long been suggested that user feedback, typically written
in natural language by end-users, can help issue detection. However, for
large-scale online service systems that receive a tremendous amount of
feedback, it remains a challenging task to identify severe issues from user
feedback. Aims: To develop a better feedback-based issue detection approach, it
is crucial first to gain a comprehensive understanding of the characteristics
of user feedback in real production systems. Method: In this paper, we conduct
an empirical study on 50,378,766 user feedback items from six real-world
services in a one-billion-user online service system. We first study what users
provide in their feedback. We then examine whether certain features of feedback
items can be good indicators of severe issues. Finally, we investigate whether
adopting machine learning techniques to analyze user feedback is reasonable.
Results: Our results show that a large proportion of user feedback provides
irrelevant information about system issues. As a result, it is crucial to
filter out issue-irrelevant information when processing user feedback.
Moreover, we find severe issues that cannot be easily detected based solely on
user feedback characteristics. Finally, we find that the distributions of the
feedback topics in different time intervals are similar. This confirms that
designing machine learning-based approaches is a viable direction for better
analyzing user feedback. Conclusions: We consider that our findings can serve
as an empirical foundation for feedback-based issue detection in large-scale
service systems, which sheds light on the design and implementation of
practical issue detection approaches.

</details>


### [16] [MCeT: Behavioral Model Correctness Evaluation using Large Language Models](https://arxiv.org/abs/2508.00630)
*Khaled Ahmed,Jialing Song,Boqi Chen,Ou Wei,Bingzhou Zheng*

Main category: cs.SE

TL;DR: The paper introduces MCeT, an automated tool using LLMs for more accurate evaluation of sequence diagrams against requirements. By breaking down the process into atomic comparisons and applying checks for consistency, MCeT detects more issues and offers clear improvements for both human engineers and AI assistants.


<details>
  <summary>Details</summary>
Motivation: With the rise of Large Language Models (LLMs) as AI assistants in generating behavioral model diagrams, there is a growing need for automated tools to evaluate the correctness of such models compared to requirements documentation. Existing manual and automatic (LLM-based) evaluations are insufficient, often missing many issues.

Method: The authors propose MCeT, an automated tool that evaluates the correctness of sequence diagrams against requirement texts using LLMs. Their method involves splitting both the diagrams and requirements into atomic elements, performing fine-grained, multi-perspective comparisons, and implementing a self-consistency check to reduce hallucinated issues by LLMs.

Result: MCeT substantially improves the evaluation process: precision increases from 0.58 to 0.81 over the direct LLM-based approach. The combined approach detects 90% more issues found by experienced engineers, and on average, identifies 6 additional new issues per diagram.

Conclusion: MCeT is an effective, fully automated tool for evaluating the correctness of behavioral sequence diagrams against textual requirements. By decomposing models and requirements into atomic elements and employing self-consistency checks, MCeT significantly outperforms direct LLM evaluation, aiding both engineers and LLM-based modeling assistants.

Abstract: Behavioral model diagrams, e.g., sequence diagrams, are an essential form of
documentation that are typically designed by system engineers from requirements
documentation, either fully manually or assisted by design tools. With the
growing use of Large Language Models (LLM) as AI modeling assistants, more
automation will be involved in generating diagrams. This necessitates the
advancement of automatic model correctness evaluation tools. Such a tool can be
used to evaluate both manually and AI automatically generated models; to
provide feedback to system engineers, and enable AI assistants to self-evaluate
and self-enhance their generated models.
  In this paper, we propose MCeT, the first fully automated tool to evaluate
the correctness of a behavioral model, sequence diagrams in particular, against
its corresponding requirements text and produce a list of issues that the model
has. We utilize LLMs for the correctness evaluation tasks as they have shown
outstanding natural language understanding ability. However, we show that
directly asking an LLM to compare a diagram to requirements finds less than 35%
of issues that experienced engineers can find. We propose to supplement the
direct check with a fine-grained, multi-perspective approach; we split the
diagram into atomic, non-divisible interactions, and split the requirements
text into atomic, self-contained items. We compare the diagram with atomic
requirements and each diagram-atom with the requirements. We also propose a
self-consistency checking approach that combines perspectives to mitigate LLM
hallucinated issues. Our combined approach improves upon the precision of the
direct approach from 0.58 to 0.81 in a dataset of real requirements. Moreover,
the approach finds 90% more issues that the experienced engineers found than
the direct approach, and reports an average of 6 new issues per diagram.

</details>


### [17] [Is LLM-Generated Code More Maintainable \& Reliable than Human-Written Code?](https://arxiv.org/abs/2508.00700)
*Alfred Santa Molison,Marcia Moraes,Glaucia Melo,Fabio Santos,Wesley K. G. Assuncao*

Main category: cs.SE

TL;DR: LLMs generate code with generally fewer bugs and lower fix effort, but in complex scenarios, can introduce serious structural problems. Careful evaluation is essential when using LLMs for advanced coding tasks.


<details>
  <summary>Details</summary>
Motivation: The adoption of Large Language Models (LLMs) for code generation is growing, but it is unclear how the quality of code generated by LLMs compares to that written by humans, specifically regarding software quality attributes.

Method: An empirical study was conducted that used datasets containing Python code solutions (across introductory, interview, and competition difficulty levels), compared three LLM configurations (zero-shot, few-shot, and fine-tuning), and employed SonarQube for quality assessment. Key metrics measured included maintainability, reliability, and the effort required to address code issues.

Result: LLM-generated code had fewer bugs and required less corrective effort overall. Fine-tuning LLMs reduced high-severity code issues but lowered overall performance. For competition-level problems, LLM solutions sometimes introduced unique structural issues not present in human code.

Conclusion: LLMs can generate code with good quality, often introducing fewer bugs and less effort to fix them. However, for complex tasks, LLMs may introduce critical issues, emphasizing the need for thorough evaluation. This study highlights both the strengths and the limitations of LLM-based code generation.

Abstract: Background: The rise of Large Language Models (LLMs) in software development
has opened new possibilities for code generation. Despite the widespread use of
this technology, it remains unclear how well LLMs generate code solutions in
terms of software quality and how they compare to human-written code. Aims:
This study compares the internal quality attributes of LLM-generated and
human-written code. Method: Our empirical study integrates datasets of coding
tasks, three LLM configurations (zero-shot, few-shot, and fine-tuning), and
SonarQube to assess software quality. The dataset comprises Python code
solutions across three difficulty levels: introductory, interview, and
competition. We analyzed key code quality metrics, including maintainability
and reliability, and the estimated effort required to resolve code issues.
Results: Our analysis shows that LLM-generated code has fewer bugs and requires
less effort to fix them overall. Interestingly, fine-tuned models reduced the
prevalence of high-severity issues, such as blocker and critical bugs, and
shifted them to lower-severity categories, but decreased the model's
performance. In competition-level problems, the LLM solutions sometimes
introduce structural issues that are not present in human-written code.
Conclusion: Our findings provide valuable insights into the quality of
LLM-generated code; however, the introduction of critical issues in more
complex scenarios highlights the need for a systematic evaluation and
validation of LLM solutions. Our work deepens the understanding of the
strengths and limitations of LLMs for code generation.

</details>


### [18] [Tool-Assisted Conformance Checking to Reference Process Models](https://arxiv.org/abs/2508.00738)
*Bernhard Rumpe,Max Stachon,Sebastian St√ºber,Valdes Voufo*

Main category: cs.SE

TL;DR: This paper proposes and tests an automated, semantics-based tool for verifying process models‚Äô adherence to reference models, improving on previous, less expressive methods.


<details>
  <summary>Details</summary>
Motivation: Reference models are important for maintaining best practices and standards in processes, but current methods for checking if other models conform to these references often lack automation and semantic expressiveness. There is a need for improved ways to verify conformance that are both accurate and efficient.

Method: The authors developed an algorithm based on causal dependency analysis to compare process models semantically, integrated it within a broader framework, and performed a case study to evaluate its effectiveness.

Result: The paper presents an automated method for checking conformance of process models against reference models using causal dependency analysis. It is integrated into a semantic framework and validated through a case study, demonstrating improvements in flexibility and accuracy.

Conclusion: The approach enhances the accuracy and flexibility of conformance checking between process models and standards, representing progress over previous methods mainly focused on execution traces. The research delivers practical, tool-supported verification.

Abstract: Reference models convey best practices and standards. The reference
frameworks necessitate conformance checks to ensure adherence to established
guidelines and principles, which is crucial for maintaining quality and
consistency in various processes. This paper explores automated conformance
checks for concrete process models against reference models using causal
dependency analysis of tasks and events. Existing notions of conformance
checking for process models focus on verifying process execution traces and
lack the expressiveness and automation needed for semantic model comparison,
leaving this question unresolved. We integrate our approach into a broader
semantic framework for defining reference model conformance. We outline an
algorithm for reference process model conformance checking, evaluate it through
a case study, and discuss its strengths and limitations. Our research provides
a tool-assisted solution enhancing accuracy and flexibility in process model
conformance verification.

</details>


### [19] [Dynamic Symbolic Execution for Semantic Difference Analysis of Component and Connector Architectures](https://arxiv.org/abs/2508.00749)
*Johanna Grahl,Bernhard Rumpe,Max Stachon,Sebastian St√ºber*

Main category: cs.SE

TL;DR: Dynamic Symbolic Execution can uncover important differences in evolving component architectures but currently struggles with scalability, highlighting the need for further work to make it practical for larger systems.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to ensure the correctness and consistency of evolving models in model-driven development, focusing on the challenge of semantic difference analysis for component-and-connector architectures.

Method: The authors use Dynamic Symbolic Execution (DSE) to analyze the semantic differences in MontiArc models. They enhance the MontiArc-to-Java generator to collect symbolic and concrete execution data during runtime, such as transition conditions, visited states, and internal variables. They then use this data to identify execution traces and evaluate multiple execution strategies.

Result: The results show that DSE provides valuable insights and is promising for semantic difference analysis in component-and-connector architectures. However, scalability is a significant challenge, limiting its use in larger systems.

Conclusion: The paper concludes that DSE is a promising technique for semantic difference analysis in component-and-connector architectures, but further research is needed to address scalability issues for application in larger models.

Abstract: In the context of model-driven development, ensuring the correctness and
consistency of evolving models is paramount. This paper investigates the
application of Dynamic Symbolic Execution (DSE) for semantic difference
analysis of component-and-connector architectures, specifically utilizing
MontiArc models. We have enhanced the existing MontiArc-to-Java generator to
gather both symbolic and concrete execution data at runtime, encompassing
transition conditions, visited states, and internal variables of automata. This
data facilitates the identification of significant execution traces that
provide critical insights into system behavior. We evaluate various execution
strategies based on the criteria of runtime efficiency, minimality, and
completeness, establishing a framework for assessing the applicability of DSE
in semantic difference analysis. Our findings indicate that while DSE shows
promise for analyzing component and connector architectures, scalability
remains a primary limitation, suggesting further research is needed to enhance
its practical utility in larger systems.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [20] [Modelling Program Spaces in Program Synthesis with Constraints](https://arxiv.org/abs/2508.00005)
*Tilman Hinnerichs,Bart Swinkels,Jaap de Jong,Reuben Gardos Reid,Tudor Magirescu,Neil Yorke-Smith,Sebastijan Dumancic*

Main category: cs.PL

TL;DR: This paper proposes using syntactic constraints to reduce the program space in program synthesis. The BART solver is introduced, which eliminates up to 99% of unwanted programs and significantly speeds up program enumeration tasks.


<details>
  <summary>Details</summary>
Motivation: Program synthesis faces the challenge of navigating a vast space of possible programs. Current methods focus on using constraints to express semantics rather than as a means to prune irrelevant or unwanted programs effectively. There is a need for more potent constraint handling to improve the efficiency of program synthesis.

Method: This paper introduces the use of syntactic constraints to model and constrain the program search space, not only ensuring feasibility but also enhancing the likelihood of producing useful solutions. The authors present BART, a solver leveraging these constraints, enabling efficient propagation and solving without the need for program execution.

Result: BART was evaluated on program space enumeration tasks, demonstrating its ability to eliminate up to 99% of the program space using syntactic constraints. This significant pruning resulted in a notable reduction in enumeration time compared to previous approaches.

Conclusion: Utilizing syntactic constraints in program synthesis, as realized by BART, drastically reduces the search space and boosts efficiency. Syntactic constraints prove to be a powerful tool in modeling and managing possible programs, outperforming traditional semantic-only constraint approaches.

Abstract: A core challenge in program synthesis is taming the large space of possible
programs. Since program synthesis is essentially a combinatorial search, the
community has sought to leverage powerful combinatorial constraint solvers.
Here, constraints are used to express the program semantics, but not as a
potentially potent tool to remove unwanted programs. Recent inductive logic
programming approaches introduce constraints on the program's syntax to be
synthesized. These syntactic constraints allow for checking and propagating a
constraint without executing the program, and thus for arbitrary operators. In
this work, we leverage syntactic constraints to model program spaces, defining
not just solutions that are feasible, but also ones that are likely useful. To
demonstrate this idea, we introduce BART, a solver that efficiently propagates
and solves these constraints. We evaluate BART on program space enumeration
tasks, finding that the constraints eliminate up to 99 percent of the program
space, and that modeling program spaces significantly reduces enumeration time.

</details>


### [21] [From Provable Correctness to Probabilistic Generation: A Comparative Review of Program Synthesis Paradigms](https://arxiv.org/abs/2508.00013)
*Zurabi Kobaladze,Anna Arnania,Tamar Sanikidze*

Main category: cs.PL

TL;DR: This thesis reviews the history and paradigms of program synthesis, comparing logic-driven, inductive, schema-based, large language model, and hybrid neuro-symbolic methods, highlighting how the field is moving toward combining symbolic and neural techniques for more scalable and reliable automated coding.


<details>
  <summary>Details</summary>
Motivation: Program synthesis has been a core pursuit in computer science for decades, aiming to automate the generation of code from high-level specifications. There has been significant evolution in methodologies, and there is a need to comprehensively understand the trajectory and trade-offs in the field.

Method: The thesis conducts a comparative literature review, systematically analyzing five principal paradigms: logic-based (deductive), inductive (example-based), sketch/schema-based, large language model-based, and neuro-symbolic hybrid approaches. It examines foundational principles, notable tools, and application domains for each.

Result: The review outlines the features, advantages, and limitations of each synthesis paradigm. It traces developments from formally verified systems (like KIDS and Coq) to recent data-driven methods (such as Codex), analyzing their trade-offs regarding guarantees, specification requirements, search complexity, and expressiveness.

Conclusion: There has been a marked transition from purely symbolic to neuro-symbolic and large language model-based methods in program synthesis. The thesis identifies ongoing challenges and future research directions to make synthesis both reliable and scalable.

Abstract: Program synthesis--the automated generation of executable code from
high-level specifications--has been a central goal of computer science for over
fifty years. This thesis provides a comparative literature review of the main
paradigms that have shaped the field, tracing its evolution from formal logic
based methods to recent advances using large scale neural models. We examine
five key approaches: logic based (deductive) synthesis, inductive (example
based) synthesis, sketch/schema based synthesis, large language model based
synthesis, and neuro-symbolic hybrids. For each, we analyze foundational
principles, notable systems, and practical applications, highlighting trade
offs between correctness guarantees, specification requirements, search
complexity, and expressive power. By reviewing developments from formally
verified synthesis tools such as KIDS and Coq to data driven models generating
probabilistic code from natural language like Codex, we present a comprehensive
narrative of progress and ongoing challenges. This work emphasizes the
transition from symbolic to hybrid neuro-symbolic methods and outlines future
directions for reliable and scalable program synthesis.

</details>


### [22] [Extended Abstract: Mutable Objects with Several Implementations](https://arxiv.org/abs/2508.00016)
*Matt Kaufmann,Yahya Sohail,Warren A. Hunt Jr*

Main category: cs.PL

TL;DR: The paper presents the 'attach-stobj' ACL2 feature, enabling users to change executable operations of abstract stobjs without recertifying related formal content, thereby streamlining the verification process.


<details>
  <summary>Details</summary>
Motivation: In ACL2, maintaining flexibility in executable operations for abstract stobjs is challenging due to the need for recertifying related books and theorems when changes occur.

Method: The paper introduces and describes the 'attach-stobj' feature in ACL2, detailing its user-facing application as well as some aspects of its implementation.

Result: With the 'attach-stobj' feature, users can now specify different executable behaviors for a given abstract stobj without having to recertify the book or any associated theorems, enhancing workflow efficiency and flexibility.

Conclusion: Attach-stobj in ACL2 significantly reduces overhead for users needing to modify executable operations for abstract stobjs, improving usability and modularity in formal verification tasks.

Abstract: This extended abstract outlines an ACL2 feature, attach-stobj, that first
appeared in ACL2 Version 8.6 (October, 2024). This feature supports different
executable operations for a given abstract stobj, without requiring
recertification of the book that introduces that stobj or theorems about it.
The paper provides background as well as a user-level overview and some
implementation notes.

</details>


### [23] [Automated Type Annotation in Python Using Large Language Models](https://arxiv.org/abs/2508.00422)
*Varun Bharti,Shashwat Jha,Dhruv Kumar,Pankaj Jalote*

Main category: cs.PL

TL;DR: LLMs can automatically generate and refine type annotations for Python code with high accuracy and consistency. The proposed pipeline offers competitive results versus traditional methods, without needing extensive labeled datasets or special training.


<details>
  <summary>Details</summary>
Motivation: Type annotations in Python improve code quality but are tedious and error-prone to generate manually. Existing automation methods have limitations such as small type vocabularies, over-approximation, and dependence on large annotated datasets. The paper seeks to address these challenges.

Method: The authors propose a 'generate-check-repair' pipeline using Large Language Models (LLMs) aided by Concrete Syntax Trees (CST). LLMs initially generate type annotations, which are checked with the static type checker Mypy; errors trigger automatic iterative refinements. Four LLM variants are evaluated on 6000 Python code snippets from the ManyTypes4Py benchmark.

Result: General-purpose and reasoning-optimized LLMs (GPT 4.1mini and O3Mini) achieved high consistency rates (~88.6%) and strong annotation accuracies (up to 70.5% exact match, 79.1% base type). Most cases required fewer than one repair iteration. LLMs performed competitively with traditional deep learning, despite not using any task-specific fine-tuning or extra training data.

Conclusion: The 'generate-check-repair' pipeline leveraging LLMs is effective for Python type annotation, rivaling deep learning approaches that require extensive labeled data. The approach is also extensible to other languages with optional typing, such as Ruby.

Abstract: Type annotations in Python enhance maintainability and error detection.
However, generating these annotations manually is error prone and requires
extra effort. Traditional automation approaches like static analysis, machine
learning, and deep learning struggle with limited type vocabularies, behavioral
over approximation, and reliance on large labeled datasets. In this work, we
explore the use of LLMs for generating type annotations in Python. We develop a
generate check repair pipeline: the LLM proposes annotations guided by a
Concrete Syntax Tree representation, a static type checker (Mypy) verifies
them, and any errors are fed back for iterative refinement. We evaluate four
LLM variants: GPT 4oMini, GPT 4.1mini (general-purpose), and O3Mini, O4Mini
(reasoning optimized), on 6000 code snippets from the ManyTypes4Py benchmark.
We first measure the proportion of code snippets annotated by LLMs for which
MyPy reported no errors (i.e., consistent results): GPT 4oMini achieved
consistency on 65.9% of cases (34.1% inconsistent), while GPT 4.1mini, O3Mini,
and O4Mini each reached approximately 88.6% consistency (around 11.4%
failures). To measure annotation quality, we then compute exact-match and
base-type match accuracies over all 6000 snippets: GPT 4.1mini and O3Mini
perform the best, achieving up to 70.5% exact match and 79.1% base type
accuracy, requiring under one repair iteration on average. Our results
demonstrate that general-purpose and reasoning optimized LLMs, without any task
specific fine tuning or additional training can be effective in generating
consistent type annotations.They perform competitively with traditional deep
learning techniques which require large labeled dataset for training. While our
work focuses on Python, the pipeline can be extended to other optionally typed
imperative languages like Ruby

</details>


### [24] [Semantic Subtyping for Maps in Erlang](https://arxiv.org/abs/2508.00482)
*Erdem Yildirim,Albert Schimpf,Stefan Wehr,Annette Bieniusa*

Main category: cs.PL

TL;DR: This paper introduces a set-theoretic model for Erlang's rich type system, defining a new semantic subtyping relation that, for the first time, formally describes subtyping for parametric map types.


<details>
  <summary>Details</summary>
Motivation: To provide a formal, set-theoretic foundation for the complex type system used in Erlang, particularly focusing on map types with parameters, where no comprehensive semantic subtyping model existed before.

Method: The authors construct a set-theoretic model for types, including type variables, base types, set-theoretic types, and map types. They define a semantic subtyping relation by using set containment as the formal basis and extend this to cover parametric map types.

Result: A new formal semantic subtyping relation is established, specifically addressing the challenge of subtyping for parametric map types in Erlang's type system.

Conclusion: The paper presents a novel semantic model for Erlang map types, providing a sound and formal approach to subtyping, especially for parametric maps, which had not been previously addressed.

Abstract: In this paper we will construct a set-theoretic model of types featuring type
variables, base types, set-theoretic types and map types. Syntax of map types
spans all the map types available in Erlang. The model of types is used to
define a semantic subtyping relation based on set containment. The novelty of
this work is the definition of subtyping over parameteric map types.

</details>


### [25] [Towards a unified framework for programming paradigms: A systematic review of classification formalisms and methodological foundations](https://arxiv.org/abs/2508.00534)
*Mikel Vandeloise*

Main category: cs.PL

TL;DR: Traditional ways to classify programming paradigms are outdated due to the rise of hybrid languages. This literature review highlights a shift toward mathematically sound, compositional frameworks for understanding and unifying programming paradigms, with an emphasis on type theory, category theory, and the unifying theories of programming.


<details>
  <summary>Details</summary>
Motivation: Multi-paradigm programming languages are blurring traditional boundaries, creating practical software engineering challenges such as interoperability issues. There is a need to understand and improve the way programming paradigms are classified and formally described.

Method: This paper conducts a systematic literature review (SLR) of 74 primary studies to assess the current state of classification formalisms for programming paradigms, synthesize their limitations, and identify foundational conceptual and mathematical approaches for a more robust framework.

Result: The review finds that current taxonomies are limited by conceptual vagueness, lack of unified formalism, and poor handling of hybrid languages. However, research is moving toward reconstructive frameworks that use minimal, orthogonal primitives and mathematical foundations such as Type theory, Category theory, and UTP, enabling better compositional reasoning.

Conclusion: There is a major academic shift from static classification of programming paradigms toward compositional, formally grounded frameworks. The paper charts this movement and outlines a research agenda focused on unifying these emerging formal approaches.

Abstract: The rise of multi-paradigm languages challenges traditional classification
methods, leading to practical software engineering issues like interoperability
defects. This systematic literature review (SLR) maps the formal foundations of
programming paradigms. Our objective is twofold: (1) to assess the state of the
art of classification formalisms and their limitations, and (2) to identify the
conceptual primitives and mathematical frameworks for a more powerful,
reconstructive approach.
  Based on a synthesis of 74 primary studies, we find that existing taxonomies
lack conceptual granularity, a unified formal basis, and struggle with hybrid
languages. In response, our analysis reveals a strong convergence toward a
compositional reconstruction of paradigms. This approach identifies a minimal
set of orthogonal, atomic primitives and leverages mathematical frameworks,
predominantly Type theory, Category theory and Unifying Theories of Programming
(UTP), to formally guarantee their compositional properties.
  We conclude that the literature reflects a significant intellectual shift
away from classification towards these promising formal, reconstructive
frameworks. This review provides a map of this evolution and proposes a
research agenda for their unification.

</details>
