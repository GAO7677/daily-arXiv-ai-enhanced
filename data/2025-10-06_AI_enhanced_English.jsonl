{"id": "2510.02387", "categories": ["cs.SE", "cs.AI", "cs.LG", "68T07", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.02387", "abs": "https://arxiv.org/abs/2510.02387", "authors": ["FAIR CodeGen team", "Quentin Carbonneaux", "Gal Cohen", "Jonas Gehring", "Jacob Kahn", "Jannik Kossen", "Felix Kreuk", "Emily McMilin", "Michel Meyer", "Yuxiang Wei", "David Zhang", "Kunhao Zheng", "Jordi Armengol-Estap\u00e9", "Pedram Bashiri", "Maximilian Beck", "Pierre Chambon", "Abhishek Charnalia", "Chris Cummins", "Juliette Decugis", "Zacharias V. Fisches", "Fran\u00e7ois Fleuret", "Fabian Gloeckle", "Alex Gu", "Michael Hassid", "Daniel Haziza", "Badr Youbi Idrissi", "Christian Keller", "Rahul Kindi", "Hugh Leather", "Gallil Maimon", "Aram Markosyan", "Francisco Massa", "Pierre-Emmanuel Mazar\u00e9", "Vegard Mella", "Naila Murray", "Keyur Muzumdar", "Peter O'Hearn", "Matteo Pagliardini", "Dmitrii Pedchenko", "Tal Remez", "Volker Seeker", "Marco Selvi", "Oren Sultan", "Sida Wang", "Luca Wehrstedt", "Ori Yoran", "Lingming Zhang", "Taco Cohen", "Yossi Adi", "Gabriel Synnaeve"], "title": "CWM: An Open-Weights LLM for Research on Code Generation with World Models", "comment": "58 pages", "summary": "We release Code World Model (CWM), a 32-billion-parameter open-weights LLM,\nto advance research on code generation with world models. To improve code\nunderstanding beyond what can be learned from training on static code alone, we\nmid-train CWM on a large amount of observation-action trajectories from Python\ninterpreter and agentic Docker environments, and perform extensive multi-task\nreasoning RL in verifiable coding, math, and multi-turn software engineering\nenvironments. With CWM, we provide a strong testbed for researchers to explore\nthe opportunities world modeling affords for improving code generation with\nreasoning and planning in computational environments. We present first steps of\nhow world models can benefit agentic coding, enable step-by-step simulation of\nPython code execution, and show early results of how reasoning can benefit from\nthe latter. CWM is a dense, decoder-only LLM trained with a context size of up\nto 131k tokens. Independent of its world modeling capabilities, CWM offers\nstrong performance on general coding and math tasks: it reaches pass@1 scores\nof 65.8% on SWE-bench Verified (with test-time scaling), 68.6% on\nLiveCodeBench, 96.6% on Math-500, and 76.0% on AIME 2024. To support further\nresearch on code world modeling, we release model checkpoints after\nmid-training, SFT, and RL.", "AI": {"tldr": "Code World Model (CWM) is a large open-weights LLM designed to advance code generation by incorporating world modeling and reasoning from dynamic environments, demonstrating strong results on benchmarks and supporting further research through released checkpoints.", "motivation": "The motivation is to improve code generation by going beyond static code understanding, leveraging world modeling and reasoning using dynamic computational environments.", "method": "CWM was mid-trained on observation-action trajectories from Python interpreter and Docker environments, followed by multi-task reasoning reinforcement learning (RL) in coding, math, and software engineering tasks. Model checkpoints were released at various stages: after mid-training, supervised fine-tuning (SFT), and RL.", "result": "CWM shows high scores on coding and math tasks, such as 65.8% pass@1 on SWE-bench Verified, 68.6% on LiveCodeBench, 96.6% on Math-500, and 76.0% on AIME 2024. It also enables step-by-step Python execution simulation and agentic coding.", "conclusion": "The release of the 32B-parameter Code World Model (CWM) provides a powerful tool for advancing code generation with reasoning and planning capabilities. CWM demonstrates strong performance on coding and math benchmarks and serves as a testbed for future research in agentic coding and code world modeling."}}
{"id": "2510.02389", "categories": ["cs.SE", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02389", "abs": "https://arxiv.org/abs/2510.02389", "authors": ["Haoran Xi", "Minghao Shao", "Brendan Dolan-Gavitt", "Muhammad Shafique", "Ramesh Karri"], "title": "From Trace to Line: LLM Agent for Real-World OSS Vulnerability Localization", "comment": null, "summary": "Large language models show promise for vulnerability discovery, yet\nprevailing methods inspect code in isolation, struggle with long contexts, and\nfocus on coarse function- or file-level detections - offering limited\nactionable guidance to engineers who need precise line-level localization and\ntargeted patches in real-world software development. We present T2L-Agent\n(Trace-to-Line Agent), a project-level, end-to-end framework that plans its own\nanalysis and progressively narrows scope from modules to exact vulnerable\nlines. T2L-Agent couples multi-round feedback with an Agentic Trace Analyzer\n(ATA) that fuses runtime evidence - crash points, stack traces, and coverage\ndeltas - with AST-based code chunking, enabling iterative refinement beyond\nsingle pass predictions and translating symptoms into actionable, line-level\ndiagnoses. To benchmark line-level vulnerability discovery, we introduce\nT2L-ARVO, a diverse, expert-verified 50-case benchmark spanning five crash\nfamilies and real-world projects. T2L-ARVO is specifically designed to support\nboth coarse-grained detection and fine-grained localization, enabling rigorous\nevaluation of systems that aim to move beyond file-level predictions. On\nT2L-ARVO, T2L-Agent achieves up to 58.0% detection and 54.8% line-level\nlocalization, substantially outperforming baselines. Together, the framework\nand benchmark push LLM-based vulnerability detection from coarse identification\ntoward deployable, robust, precision diagnostics that reduce noise and\naccelerate patching in open-source software workflows.", "AI": {"tldr": "T2L-Agent is a new LLM-driven framework for vulnerability detection that moves beyond file-level predictions to line-level diagnosis and patch recommendations, achieving significantly better results on a rigorous new benchmark compared to prior approaches.", "motivation": "Current large language model approaches to vulnerability discovery are limited by their coarse detection granularity, isolated code analysis, and struggles with long context sizes. This provides limited actionable information to engineers who require precise localization and targeted patch suggestions for effective real-world software development.", "method": "The proposed method introduces T2L-Agent, an end-to-end framework that leverages an Agentic Trace Analyzer (ATA) which combines runtime evidence (e.g., crash points, stack traces, coverage deltas) with AST-based code chunking. The system utilizes multi-round feedback and progressive narrowing from modules to exact code lines, enabling iterative refinement and translation of symptoms into line-level diagnoses.", "result": "On the new T2L-ARVO benchmark\u2014a collection of 50 diverse, expert-verified vulnerability cases spanning five crash families and various real-world projects\u2014T2L-Agent achieves up to 58.0% detection accuracy and 54.8% line-level localization accuracy, substantially outperforming baseline methods.", "conclusion": "T2L-Agent, coupled with the T2L-ARVO benchmark, significantly advances LLM-based vulnerability detection from coarse-grained identification to precise, actionable line-level diagnostics, achieving notable improvements over existing methods and promising to accelerate patching in real-world software development."}}
{"id": "2510.02393", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02393", "abs": "https://arxiv.org/abs/2510.02393", "authors": ["Jianqing Zhang", "Wei Xia", "Hande Dong", "Qiang Lin", "Jian Cao"], "title": "AP2O: Correcting LLM-Generated Code Errors Type by Type Like Humans via Adaptive Progressive Preference Optimization", "comment": null, "summary": "LLMs' code generation capabilities have yielded substantial improvements in\nthe effectiveness of programming tasks. However, LLM-generated code still\nsuffers from compilation and runtime errors. Existing offline preference\noptimization methods primarily focus on enhancing LLMs' coding abilities using\npass/fail signals in the preference data, overlooking the deep-level error\ntypes in the failed codes. To address this, we propose Adaptively Progressive\nPreference Optimization (AP2O) for coding (i.e., AP2O-Coder), a method that\nguides LLMs adaptively and methodically to reduce code errors for code\ngeneration. Specifically, we construct an error notebook from failed codes and\nprogressively optimize the LLM to correct errors type by type. Furthermore, we\nadaptively replay error types to tailor to the LLM's changing weaknesses\nthroughout the training process. Through extensive experiments on both code and\ngeneral LLMs (Llama, Qwen, and DeepSeek series) with parameters ranging from\n0.5B to 34B, our AP2O-Coder improves code generation performance by up to 3% in\npass@k while using less preference data. Code: https://github.com/TsingZ0/AP2O", "AI": {"tldr": "AP2O-Coder is a new training method that uses specific error-type feedback and adaptive replay to help LLMs generate better code, improving accuracy and efficiency compared to existing approaches.", "motivation": "LLMs have greatly improved code generation, but generated code still often contains compilation and runtime errors. Existing optimization methods mainly use pass/fail signals and do not consider specific error types, limiting performance gains.", "method": "The proposed method, AP2O-Coder, constructs an error notebook from failed code samples and progressively optimizes the LLM to correct different error types. It also adaptively replays error types during training to match the model's evolving weaknesses.", "result": "AP2O-Coder achieves up to 3% improvement in code generation performance (measured by pass@k) across multiple LLMs (Llama, Qwen, DeepSeek) and size scales, while requiring less preference data.", "conclusion": "Error-type-aware and adaptive replay optimization can reduce code errors in LLM-generated code more efficiently than traditional pass/fail-based optimization methods."}}
{"id": "2510.02404", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02404", "abs": "https://arxiv.org/abs/2510.02404", "authors": ["Siddharth Agarwal", "Maria A. Rodriguez", "Rajkumar Buyya"], "title": "Dynamic Function Configuration and its Management in Serverless Computing: A Taxonomy and Future Directions", "comment": "34 pages, 2 figures, 2 tables, journal", "summary": "The serverless cloud computing model offers a framework where the service\nprovider abstracts the underlying infrastructure management from developers. In\nthis serverless model, FaaS provides an event-driven, function-oriented\ncomputing service characterised by fine-grained, usage-based pricing that\neliminates cost for idle resources. Platforms like AWS Lambda, Azure Functions,\nand Cloud Run Functions require developers to configure their function(s) with\nminimum operational resources for its successful execution. This resource\nallocation influences both the operational expense and the performance quality\nof these functions. However, a noticeable lack of platform transparency forces\ndevelopers to rely on expert knowledge or experience-based ad-hoc decisions to\nrequest desired function resources. This makes optimal resource configuration a\nnon-trivial task while adhering to performance constraints. Furthermore, while\ncommercial platforms often scale resources like CPU and network bandwidth\nproportional to memory, open-source frameworks permit independent configuration\nof function resources, introducing additional complexity for developers aiming\nto optimise their functions. These complexities have directed researchers to\nresolve developer challenges and advance towards an efficient server-less\nexecution model. In this article, we identify different aspects of resource\nconfiguration techniques in FaaS settings and propose a taxonomy of factors\nthat influence function design, configuration, run-time cost, and performance\nguarantees. We conduct an analysis of existing literature on resource\nconfiguration to present a comprehensive review of current studies on function\nconfiguration. We also identify existing research gaps and suggest future\nresearch directions to enhance function configuration and strengthen the\ncapabilities of serverless computing environments to drive its broader\nadoption.", "AI": {"tldr": "This paper surveys and categorizes factors affecting resource configuration in serverless Function-as-a-Service platforms, highlighting platform-specific challenges and the need for improved, automated configuration tools. It outlines current research, identifies gaps, and suggests future directions to simplify and optimize function configuration in cloud environments.", "motivation": "Serverless cloud computing simplifies infrastructure management for developers, but optimal resource configuration (especially in Function-as-a-Service) remains challenging due to limited platform transparency and complex, platform-dependent resource relationships. Both over- and under-provisioning can affect cost and performance, and developers lack practical guidelines.", "method": "The authors propose a taxonomy of factors influencing FaaS resource configuration, analyze and review existing literature on the topic, and identify research gaps and future directions in function configuration in serverless environments.", "result": "The paper presents a comprehensive taxonomy categorizing the factors affecting FaaS resource configuration, and systematically reviews existing studies. It identifies key research gaps and open challenges, providing suggestions for future research that could improve configuration processes and serverless platform capabilities.", "conclusion": "Optimal resource configuration in serverless environments is non-trivial and heavily influenced by platform characteristics. There is a need for more transparent, automated, and intelligent configuration tools to help developers optimize cost and performance, and to support broader serverless adoption. Research must address these gaps."}}
{"id": "2510.02579", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.02579", "abs": "https://arxiv.org/abs/2510.02579", "authors": ["Santiago Cu\u00e9llar", "Naomi Spargo", "Jonathan Daugherty", "David Darais"], "title": "Designing Walrus: Relational Programming with Rich Types, On-Demand Laziness, and Structured Traces", "comment": "20 pages, miniKanren 2025", "summary": "We present Walrus, a functional relational programming language embedded in\nHaskell that extends the miniKanren model with type-polymorphic unification,\non-demand laziness, and a range of usability features aimed at practical\ndevelopment. These include use of Haskell Generics for boilerplate reduction,\nstructured debugging traces, and ergonomic support for product types. We\ndescribe the design and implementation of Walrus through the lens of our\nexperience developing bidirectional compilers, and reflect on key design\ndecisions and recurring usability challenges encountered in practice.", "AI": {"tldr": "Walrus, a new relational language in Haskell, adds practical features to miniKanren, improving type safety, debugging, and usability for complex application development like bidirectional compilers.", "motivation": "Relational programming languages like miniKanren have great expressive power but suffer from practical challenges when building real-world applications, such as usability, debugging, and integration with host languages.", "method": "Walrus is implemented as an embedded language in Haskell, extending miniKanren with features like type-polymorphic unification, structured debugging, on-demand laziness, use of Haskell Generics, and product type support.", "result": "Walrus demonstrates enhanced practical usability in developing bidirectional compilers and offers improvements including better type handling, boilerplate reduction, debugging, and ergonomic support for data structures.", "conclusion": "Walrus improves the miniKanren relational programming paradigm with type safety, usability enhancements, and practical debugging, making it more viable for real-world software development."}}
{"id": "2510.02504", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02504", "abs": "https://arxiv.org/abs/2510.02504", "authors": ["Mara Ulloa", "Jenna L. Butler", "Sankeerti Haniyur", "Courtney Miller", "Barrett Amos", "Advait Sarkar", "Margaret-Anne Storey"], "title": "Product Manager Practices for Delegating Work to Generative AI: \"Accountability must not be delegated to non-human actors\"", "comment": "12 pages, 4 figures, 1 table", "summary": "Generative AI (GenAI) is changing the nature of knowledge work, particularly\nfor Product Managers (PMs) in software development teams. While much software\nengineering research has focused on developers' interactions with GenAI, there\nis less understanding of how the work of PMs is evolving due to GenAI. To\naddress this gap, we conducted a mixed-methods study at Microsoft, a large,\nmultinational software company: surveying 885 PMs, analyzing telemetry data for\na subset of PMs (N=731), and interviewing a subset of 15 PMs. We contribute:\n(1) PMs' current GenAI adoption rates, uses cases, and perceived benefits and\nbarriers and; (2) a framework capturing how PMs assess which tasks to delegate\nto GenAI; (3) PMs adaptation practices for integrating GenAI into their roles\nand perceptions of how their role is evolving. We end by discussing\nimplications on the broader GenAI workflow adoption process and software\ndevelopment roles.", "AI": {"tldr": "This paper analyzes how Generative AI is transforming the role and practices of Product Managers in software development, offering a detailed view of adoption, adaptation, and implications for future workflows.", "motivation": "Generative AI is significantly impacting knowledge work, particularly for Product Managers in software development. Previous research has mainly focused on developers, leaving a gap in understanding how GenAI affects PMs specifically. The paper aims to address this lack of knowledge and provide actionable insights.", "method": "The authors conducted a mixed-methods study at Microsoft, combining a survey of 885 PMs, telemetry data analysis for 731 PMs, and interviews with 15 PMs.", "result": "The study provides: 1) GenAI adoption rates among PMs, their use cases, benefits, and barriers; 2) a framework for how PMs decide which tasks to delegate to GenAI; and 3) insights into PM adaptation and evolving perceptions of their role due to GenAI integration.", "conclusion": "PMs are adopting GenAI with varying rates and strategies, leveraging it for several tasks while facing specific benefits and challenges. Their roles are evolving as they integrate GenAI, which has broader implications on workflow and software development roles."}}
{"id": "2510.03170", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.03170", "abs": "https://arxiv.org/abs/2510.03170", "authors": ["Rafaello Sanna", "William E. Byrd", "Nada Amin"], "title": "Beyond Cons: Purely Relational Data Structures", "comment": "17 pages, 6 figures, Source code available at\n  https://www.github.com/rvs314/faster-clpset-minikanren . To be published in\n  the 7th Workshop on miniKanren and Relational Programming (miniKanren'25)", "summary": "We present {Kanren} (read: set-Kanren), an extension to miniKanren with\nconstraints for reasoning about sets and association lists. {Kanren} includes\nfirst-class set objects, a functionally complete family of set-theoretic\nconstraints (including membership, union, and disjointedness), and new\nconstraints for reasoning about association lists with shadowing and scoped\nlookup. These additions allow programmers to describe collections declaratively\nand lazily, without relying on structural encodings and eager search over\nrepresentation spaces. The result is improved expressiveness and operational\nbehavior in programs that manipulate abstract data -- particularly interpreters\n-- by supporting set equality based on contents, enabling finite failure. We\ndescribe the design and implementation of {Kanren} in a constraint-enabled\nminiKanren system and illustrate its use in representative examples.", "AI": {"tldr": "Kanren extends miniKanren with powerful and expressive constraints for sets and association lists, enabling better abstract data manipulation without the need for complex encodings or inefficient searches.", "motivation": "Reasoning about collections such as sets and association lists declaratively is challenging and often requires cumbersome structural encodings or inefficient representation searches. There was a need for more expressive and operationally efficient support for these data structures in logic programming systems like miniKanren.", "method": "The authors developed Kanren as an extension to miniKanren, adding first-class support for sets and association lists along with a complete family of set-theoretic constraints and scoped lookup. They describe its design, implementation, and give representative examples to showcase its utility.", "result": "Kanren allows for declarative and lazy manipulation of sets and association lists, supports set equality based on contents, enables finite failure, and delivers improved expressiveness and behavior for programs that manipulate abstract data.", "conclusion": "Kanren significantly enhances miniKanren's capabilities for abstract data manipulation, particularly in writing interpreters, by enabling expressive, declarative, and efficient reasoning about sets and association lists."}}
{"id": "2510.02534", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02534", "abs": "https://arxiv.org/abs/2510.02534", "authors": ["Mohsen Iranmanesh", "Sina Moradi Sabet", "Sina Marefat", "Ali Javidi Ghasr", "Allison Wilson", "Iman Sharafaldin", "Mohammad A. Tayebi"], "title": "ZeroFalse: Improving Precision in Static Analysis with LLMs", "comment": null, "summary": "Static Application Security Testing (SAST) tools are integral to modern\nsoftware development, yet their adoption is undermined by excessive false\npositives that weaken developer trust and demand costly manual triage. We\npresent ZeroFalse, a framework that integrates static analysis with large\nlanguage models (LLMs) to reduce false positives while preserving coverage.\nZeroFalse treats static analyzer outputs as structured contracts, enriching\nthem with flow-sensitive traces, contextual evidence, and CWE-specific\nknowledge before adjudication by an LLM. This design preserves the systematic\nreach of static analysis while leveraging the reasoning capabilities of LLMs.\nWe evaluate ZeroFalse across both benchmarks and real-world projects using ten\nstate-of-the-art LLMs. Our best-performing models achieve F1-scores of 0.912 on\nthe OWASP Java Benchmark and 0.955 on the OpenVuln dataset, maintaining recall\nand precision above 90%. Results further show that CWE-specialized prompting\nconsistently outperforms generic prompts, and reasoning-oriented LLMs provide\nthe most reliable precision-recall balance. These findings position ZeroFalse\nas a practical and scalable approach for enhancing the reliability of SAST and\nsupporting its integration into real-world CI/CD pipelines.", "AI": {"tldr": "ZeroFalse uses LLMs to smartly filter and enrich static analyzer results, slashing false positives and maintaining high precision/recall. It is practical for real-world software pipelines and demonstrates that tailored prompting and reasoning-optimized models are most effective.", "motivation": "Static Application Security Testing (SAST) tools are widely used in software development but suffer from high rates of false positives. This erodes developer trust and incurs significant manual effort to triage alerts, preventing more effective adoption of these tools.", "method": "ZeroFalse is introduced as a framework that combines static analysis outputs with large language models (LLMs). It formats analyzer findings as structured contracts, adds flow-sensitive traces, contextual evidence, and Common Weakness Enumeration (CWE)-specific knowledge, then provides these enriched outputs to an LLM for adjudication. The framework was tested using ten state-of-the-art LLMs on both benchmarks and real-world projects.", "result": "ZeroFalse achieved F1-scores of 0.912 on the OWASP Java Benchmark and 0.955 on the OpenVuln dataset, with both recall and precision above 90%. CWE-specialized prompting improved performance over generic prompts, and reasoning-oriented LLMs provided the best balance of precision and recall.", "conclusion": "ZeroFalse significantly reduces false positives while maintaining high detection coverage, improving SAST trustworthiness and enabling more practical integration into CI/CD pipelines. Specialized prompting and reasoning-focused models are key to its effectiveness."}}
{"id": "2510.02585", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02585", "abs": "https://arxiv.org/abs/2510.02585", "authors": ["Majid Dashtbani", "Ladan Tahvildari"], "title": "Key Considerations for Auto-Scaling: Lessons from Benchmark Microservices", "comment": null, "summary": "Microservices have become the dominant architectural paradigm for building\nscalable and modular cloud-native systems. However, achieving effective\nauto-scaling in such systems remains a non-trivial challenge, as it depends not\nonly on advanced scaling techniques but also on sound design, implementation,\nand deployment practices. Yet, these foundational aspects are often overlooked\nin existing benchmarks, making it difficult to evaluate autoscaling methods\nunder realistic conditions. In this paper, we identify a set of practical\nauto-scaling considerations by applying several state-of-the-art autoscaling\nmethods to widely used microservice benchmarks. To structure these findings, we\nclassify the issues based on when they arise during the software lifecycle:\nArchitecture, Implementation, and Deployment. The Architecture phase covers\nhigh-level decisions such as service decomposition and inter-service\ndependencies. The Implementation phase includes aspects like initialization\noverhead, metrics instrumentation, and error propagation. The Deployment phase\nfocuses on runtime configurations such as resource limits and health checks. We\nvalidate these considerations using the Sock-Shop benchmark and evaluate\ndiverse auto-scaling strategies, including threshold-based, control-theoretic,\nlearning-based, black-box optimization, and dependency-aware approaches. Our\nfindings show that overlooking key lifecycle concerns can degrade autoscaler\nperformance, while addressing them leads to more stable and efficient scaling.\nThese results underscore the importance of lifecycle-aware engineering for\nunlocking the full potential of auto-scaling in microservice-based systems.", "AI": {"tldr": "Effective auto-scaling in microservices requires considering architectural, implementation, and deployment factors throughout the software lifecycle. Ignoring these leads to poor performance, but lifecycle-aware practices improve autoscaling stability and efficiency.", "motivation": "Auto-scaling in microservice-based cloud systems is challenging because it requires not only advanced technical solutions but also robust lifecycle management in architecture, implementation, and deployment. Existing benchmarks often neglect these foundational areas, limiting practical evaluation and progress.", "method": "The authors identify practical auto-scaling considerations by applying state-of-the-art autoscaling techniques to popular microservice benchmarks. They organize the findings by lifecycle stage\u2014architecture, implementation, and deployment\u2014and validate their analysis with the Sock-Shop benchmark. Multiple autoscaling strategies (threshold-based, control-theoretic, learning-based, optimization, and dependency-aware) are evaluated.", "result": "The study finds that ignoring lifecycle-specific concerns negatively impacts autoscaler performance. Addressing these considerations improves both automation stability and resource efficiency.", "conclusion": "Lifecycle-aware engineering practices are essential for effective auto-scaling in microservice environments. Attention to architectural, implementation, and deployment issues enables scalable, robust, and efficient scaling solutions. Existing benchmarks should evolve to better reflect these lifecycle realities."}}
{"id": "2510.02609", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02609", "abs": "https://arxiv.org/abs/2510.02609", "authors": ["Chengquan Guo", "Chulin Xie", "Yu Yang", "Zhaorun Chen", "Zinan Lin", "Xander Davies", "Yarin Gal", "Dawn Song", "Bo Li"], "title": "RedCodeAgent: Automatic Red-teaming Agent against Diverse Code Agents", "comment": null, "summary": "Code agents have gained widespread adoption due to their strong code\ngeneration capabilities and integration with code interpreters, enabling\ndynamic execution, debugging, and interactive programming capabilities. While\nthese advancements have streamlined complex workflows, they have also\nintroduced critical safety and security risks. Current static safety benchmarks\nand red-teaming tools are inadequate for identifying emerging real-world risky\nscenarios, as they fail to cover certain boundary conditions, such as the\ncombined effects of different jailbreak tools. In this work, we propose\nRedCodeAgent, the first automated red-teaming agent designed to systematically\nuncover vulnerabilities in diverse code agents. With an adaptive memory module,\nRedCodeAgent can leverage existing jailbreak knowledge, dynamically select the\nmost effective red-teaming tools and tool combinations in a tailored toolbox\nfor a given input query, thus identifying vulnerabilities that might otherwise\nbe overlooked. For reliable evaluation, we develop simulated sandbox\nenvironments to additionally evaluate the execution results of code agents,\nmitigating potential biases of LLM-based judges that only rely on static code.\nThrough extensive evaluations across multiple state-of-the-art code agents,\ndiverse risky scenarios, and various programming languages, RedCodeAgent\nconsistently outperforms existing red-teaming methods, achieving higher attack\nsuccess rates and lower rejection rates with high efficiency. We further\nvalidate RedCodeAgent on real-world code assistants, e.g., Cursor and Codeium,\nexposing previously unidentified security risks. By automating and optimizing\nred-teaming processes, RedCodeAgent enables scalable, adaptive, and effective\nsafety assessments of code agents.", "AI": {"tldr": "RedCodeAgent is a new automated tool that systematically finds security vulnerabilities in code agents, outperforming existing methods by using adaptive techniques and simulated environments. It reveals hidden risks in real code assistants, making safety assessments more efficient and thorough.", "motivation": "Code agents have become widely used for their powerful code generation and dynamic interaction but bring safety and security risks, with current benchmarks failing to capture new, complex vulnerabilities.", "method": "The authors introduce RedCodeAgent, an automated red-teaming agent with an adaptive memory module that selects and combines jailbreak and red-teaming tools intelligently for each input. It evaluates vulnerabilities using dynamic simulation sandboxes rather than static code inspection.", "result": "RedCodeAgent outperforms current red-teaming techniques, achieving higher attack success rates and lower rejection rates across multiple code agents, risky scenarios, and programming languages. It even exposes previously unknown security issues in popular real-world code assistants.", "conclusion": "RedCodeAgent offers a scalable and systematic approach to uncovering vulnerabilities in code agents, exceeding the capabilities of static benchmarks and previous tools, and provides effective automated safety assessments."}}
{"id": "2510.02634", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02634", "abs": "https://arxiv.org/abs/2510.02634", "authors": ["Hanlong Wan", "Weili Xu", "Michael Rosenberg", "Jian Zhang", "Aysha Siddika"], "title": "Automatic Building Code Review: A Case Study", "comment": null, "summary": "Building officials, particularly those in resource-constrained or rural\njurisdictions, face labor-intensive, error-prone, and costly manual reviews of\ndesign documents as projects increase in size and complexity. The growing\nadoption of Building Information Modeling (BIM) and Large Language Models\n(LLMs) presents opportunities for automated code review (ACR) solutions. This\nstudy introduces a novel agent-driven framework that integrates BIM-based data\nextraction with automated verification using both retrieval-augmented\ngeneration (RAG) and Model Context Protocol (MCP) agent pipelines. The\nframework employs LLM-enabled agents to extract geometry, schedules, and system\nattributes from heterogeneous file types, which are then processed for building\ncode checking through two complementary mechanisms: (1) direct API calls to the\nUS Department of Energy COMcheck engine, providing deterministic and\naudit-ready outputs, and (2) RAG-based reasoning over rule provisions, enabling\nflexible interpretation where coverage is incomplete or ambiguous.\n  The framework was evaluated through case demonstrations, including automated\nextraction of geometric attributes (such as surface area, tilt, and insulation\nvalues), parsing of operational schedules, and validation of lighting\nallowances under ASHRAE Standard 90.1-2022. Comparative performance tests\nacross multiple LLMs showed that GPT-4o achieved the best balance of efficiency\nand stability, while smaller models exhibited inconsistencies or failures.\nResults confirm that MCP agent pipelines outperform RAG reasoning pipelines in\nrigor and reliability. This work advances ACR research by demonstrating a\nscalable, interoperable, and production-ready approach that bridges BIM with\nauthoritative code review tools.", "AI": {"tldr": "This paper introduces an agent-driven framework linking BIM data extraction with automated code verification using LLMs, RAG, and MCP agents. It demonstrates scalable, reliable, and efficient automated building code reviews, with GPT-4o as the top-performing model and MCP pipelines outperforming RAG reasoning.", "motivation": "Manual building code reviews in rural or resource-limited jurisdictions are labor-intensive, error-prone, and expensive, especially as building projects grow in complexity. There is a need to automate the review process using emerging technologies like BIM and LLMs.", "method": "The proposed framework integrates BIM data extraction with automated verification using two techniques: retrieval-augmented generation (RAG), and Model Context Protocol (MCP) agent pipelines. It uses LLM-driven agents to extract data from various files and checks compliance using the US DOE COMcheck engine (via API) for deterministic results and RAG reasoning for ambiguous cases. Multiple LLMs are evaluated for performance.", "result": "Automated attribute extraction, schedule parsing, and validation against ASHRAE Standard 90.1-2022 were successfully demonstrated. GPT-4o was the most efficient and stable LLM. MCP agent pipelines were more rigorous and reliable than RAG. The framework proved scalable, interoperable, and production-ready for bridging BIM with code review.", "conclusion": "The integration of BIM and LLM technologies via the agent-driven framework effectively automates building code review, especially benefitting resource-constrained jurisdictions. MCP agent pipelines provide a reliable, rigorous approach, enabling scalable, production-ready solutions for automated code review (ACR)."}}
{"id": "2510.02718", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02718", "abs": "https://arxiv.org/abs/2510.02718", "authors": ["Ali Ghanbari", "Sasan Tavakkol"], "title": "Using Fourier Analysis and Mutant Clustering to Accelerate DNN Mutation Testing", "comment": "2025 40th IEEE/ACM International Conference on Automated Software\n  Engineering (ASE)", "summary": "Deep neural network (DNN) mutation analysis is a promising approach to\nevaluating test set adequacy. Due to the large number of generated mutants that\nmust be tested on large datasets, mutation analysis is costly. In this paper,\nwe present a technique, named DM#, for accelerating DNN mutation testing using\nFourier analysis. The key insight is that DNN outputs are real-valued functions\nsuitable for Fourier analysis that can be leveraged to quantify mutant behavior\nusing only a few data points. DM# uses the quantified mutant behavior to\ncluster the mutants so that the ones with similar behavior fall into the same\ngroup. A representative from each group is then selected for testing, and the\nresult of the test, e.g., whether the mutant is killed or survived, is reused\nfor all other mutants represented by the selected mutant, obviating the need\nfor testing other mutants. 14 DNN models of sizes ranging from thousands to\nmillions of parameters, trained on different datasets, are used to evaluate DM#\nand compare it to several baseline techniques. Our results provide empirical\nevidence on the effectiveness of DM# in accelerating mutation testing by\n28.38%, on average, at the average cost of only 0.72% error in mutation score.\nMoreover, on average, DM# incurs 11.78, 15.16, and 114.36 times less mutation\nscore error compared to random mutant selection, boundary sample selection, and\nrandom sample selection techniques, respectively, while generally offering\ncomparable speed-up.", "AI": {"tldr": "The paper introduces DM#, a Fourier analysis-based method to cluster and select DNN mutants for faster mutation testing. DM# accelerates testing by 28%, keeps error under 1%, and outperforms several existing selection strategies.", "motivation": "Mutation analysis is a powerful way to assess the adequacy of test sets for deep neural networks (DNNs), but its high computational cost hampers practical use, due to the large numbers of mutants and dataset sizes involved.", "method": "The authors propose DM#, a technique using Fourier analysis to quantify the behavior of DNN mutants with only a few data points. Mutants are clustered based on this quantified behavior, and representative mutants from each cluster are selected for testing, which reduces the overall testing burden by generalizing test results to similar mutants.", "result": "When evaluated on 14 DNN models of various sizes and datasets, DM# accelerated mutation testing by 28.38% on average with only a 0.72% average error in mutation score. It also yielded significantly less mutation score error compared to traditional random, boundary sample, and random sample selection techniques, while achieving comparable speed-up.", "conclusion": "DM# is effective at reducing the cost and error in mutation testing of DNNs, allowing for practical evaluation of test set adequacy with greater efficiency and accuracy."}}
{"id": "2510.02773", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.02773", "abs": "https://arxiv.org/abs/2510.02773", "authors": ["Tamjid Al Rahat", "Yanju Chen", "Yu Feng", "Yuan Tian"], "title": "Automated Repair of OpenID Connect Programs (Extended Version)", "comment": "This is an extended version. The original paper is accepted to ASE\n  2025", "summary": "OpenID Connect has revolutionized online authentication based on single\nsign-on (SSO) by providing a secure and convenient method for accessing\nmultiple services with a single set of credentials. Despite its widespread\nadoption, critical security bugs in OpenID Connect have resulted in significant\nfinancial losses and security breaches, highlighting the need for robust\nmitigation strategies. Automated program repair presents a promising solution\nfor generating candidate patches for OpenID implementations. However,\nchallenges such as domain-specific complexities and the necessity for precise\nfault localization and patch verification must be addressed. We propose\nAuthFix, a counterexample-guided repair engine leveraging LLMs for automated\nOpenID bug fixing. AuthFix integrates three key components: fault localization,\npatch synthesis, and patch verification. By employing a novel Petri-net-based\nmodel checker, AuthFix ensures the correctness of patches by effectively\nmodeling interactions. Our evaluation on a dataset of OpenID bugs demonstrates\nthat AuthFix successfully generated correct patches for 17 out of 23 bugs\n(74%), with a high proportion of patches semantically equivalent to\ndeveloper-written fixes.", "AI": {"tldr": "AuthFix is an automated repair tool leveraging LLMs and Petri-nets to fix OpenID Connect security bugs. It correctly patched 74% of tested issues, often matching developer-written solutions, showing strong potential for enhancing authentication security.", "motivation": "Despite OpenID Connect's role in enhancing online authentication security and convenience, it remains vulnerable to critical security bugs, resulting in financial losses and breaches. There is a pressing need for automated, robust mitigation and repair solutions tailored to the unique challenges of OpenID security flaws.", "method": "The paper introduces AuthFix, an automated program repair engine guided by counterexamples and powered by large language models (LLMs). AuthFix comprises three main components: fault localization, patch synthesis, and patch verification. The verification uses a novel Petri-net-based model checker to accurately model protocol interactions and ensure correctness of generated patches.", "result": "AuthFix was evaluated against a dataset of OpenID bugs. It successfully generated correct patches for 17 out of 23 bugs (74%), and many of these patches were semantically equivalent to those written by developers.", "conclusion": "The study demonstrates that LLM-guided, counterexample-based automated repair tools like AuthFix can effectively address real-world OpenID security bugs. This approach offers a viable and efficient method to improve the security and reliability of authentication systems."}}
{"id": "2510.02854", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02854", "abs": "https://arxiv.org/abs/2510.02854", "authors": ["Boshuai Ye", "Arif Ali Khan", "Teemu Pihkakoski", "Peng Liang", "Muhammad Azeem Akbar", "Matti Silveri", "Lauri Malmi"], "title": "C2|Q>: A Robust Framework for Bridging Classical and Quantum Software Development", "comment": "46 pages, 8 images, 14 tables, Manuscript submitted to a Journal\n  (2025)", "summary": "Quantum Software Engineering (QSE) is emerging as a critical discipline to\nmake quantum computing accessible to a broader developer community; however,\nmost quantum development environments still require developers to engage with\nlow-level details across the software stack - including problem encoding,\ncircuit construction, algorithm configuration, hardware selection, and result\ninterpretation - making them difficult for classical software engineers to use.\nTo bridge this gap, we present C2|Q>: a hardware-agnostic quantum software\ndevelopment framework that translates classical specifications (code) into\nquantum-executable programs while preserving methodological rigor. The\nframework applies modular software engineering principles by classifying the\nworkflow into three core modules: an encoder that classifies problems, produces\nQuantum-Compatible Formats (QCFs), and constructs quantum circuits, a\ndeployment module that generates circuits and recommends hardware based on\nfidelity, runtime, and cost, and a decoder that interprets quantum outputs into\nclassical solutions. In evaluation, the encoder module achieved a 93.8%\ncompletion rate, the hardware recommendation module consistently selected the\nappropriate quantum devices for workloads scaling up to 56 qubits, and the full\nC2|Q>: workflow successfully processed classical specifications (434 Python\nsnippets and 100 JSON inputs) with completion rates of 93.8% and 100%,\nrespectively. For case study problems executed on publicly available NISQ\nhardware, C2|Q>: reduced the required implementation effort by nearly 40X\ncompared to manual implementations using low-level quantum software development\nkits (SDKs), with empirical runs limited to small- and medium-sized instances\nconsistent with current NISQ capabilities. The open-source implementation of\nC2|Q>: is available at https://github.com/C2-Q/C2Q", "AI": {"tldr": "C2|Q> is an open-source framework that lets classical developers write quantum programs by translating classical code into quantum-executable tasks. It automates problem encoding, circuit creation, hardware selection, and result decoding, achieving high completion rates and massively reducing manual effort. C2|Q> helps make quantum computing more accessible and practical for conventional software engineers.", "motivation": "Quantum software engineering is hampered by the complexity and low-level nature of quantum development environments, making them hard for classical software engineers to adopt. There is a need for frameworks that bridge the gap between classical and quantum program specifications, bringing accessibility and methodological rigor.", "method": "The paper presents C2|Q>, a modular quantum software development framework. C2|Q> is hardware-agnostic and processes classical code into quantum-executable programs by separating the workflow into three modules: encoder (problem classification, generation of quantum circuit), deployment (circuit generation, hardware recommendation based on performance metrics), and decoder (result interpretation). The system is evaluated on typical quantum workloads using classical specifications and public quantum hardware.", "result": "C2|Q> achieved high completion rates (93.8% for Python and 100% for JSON inputs), successfully recommending appropriate hardware for up to 56 qubits and reducing developer effort by up to 40 times in case studies relative to manual SDK implementations. It enables practical execution of small to medium quantum tasks on NISQ hardware.", "conclusion": "C2|Q> significantly lowers barriers for classical software engineers to develop quantum programs, streamlines the quantum workflow, improves productivity, and can automatically map classical specifications to quantum solutions across current quantum hardware."}}
{"id": "2510.02887", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02887", "abs": "https://arxiv.org/abs/2510.02887", "authors": ["Zhao Zhang", "Qingyuan Liang", "Zeyu Sun", "Yizhou Chen", "Guoqing Wang", "Yican Sun", "Lu Zhang", "Ge Li", "Yingfei Xiong"], "title": "GramTrans: A Better Code Representation Approach in Code Generation", "comment": null, "summary": "Code generation has shown great promise in assisting software development. A\nfundamental yet underexplored question is how the choice of code representation\naffects model performance. While existing studies employ various\nrepresentations, such as treating code as plain text, grammar rule sequences,\nor syntax tree sequences, they lack a principled understanding of the\nrelationship between parsing difficulty and model effectiveness. This paper\nproposes a conjecture: the easier a representation is to parse, the better\nperformance the model achieves. We formalize this idea using grammar classes,\nwhere representations in simpler classes (e.g., LL(1)) are easier to parse.\nThrough a controlled experiment on a Python-based DSL, we show that parsing\ndifficulty strongly correlates with model performance. Motivated by this\nfinding, we present GramTrans, a general approach that automatically transforms\na context-free language into a representation within the LL(1) class. GramTrans\nintroduces a novel hierarchical conflict elimination algorithm, enabling a\nflexible trade-off between syntactic simplicity and token efficiency. We\nevaluate GramTrans on both Python and Java using three code generation models:\nStarCoder 1B, DeepSeek-Coder 1.3B, and Qwen2.5 1.5B. Across multiple\nbenchmarks, GramTrans consistently delivers significant improvements over\nbaseline representations. Furthermore, our analysis of existing representations\nreconfirms the strong alignment between parsing difficulty and model\nperformance, providing additional support for the conjecture.", "AI": {"tldr": "Simpler parsing leads to better code generation: transforming code into easier-to-parse representations significantly boosts model performance, as demonstrated by the authors' GramTrans tool and comprehensive experiments.", "motivation": "The paper is motivated by the lack of a principled understanding of how the choice of code representation impacts the performance of code generation models, specifically regarding the relationship between parsing difficulty and model effectiveness.", "method": "The authors formalize the conjecture using grammar classes to measure parsing difficulty and perform controlled experiments using a Python-based DSL. They introduce GramTrans, a tool that automatically transforms context-free languages into LL(1) representations, employing a hierarchical conflict elimination algorithm. Evaluations were conducted on Python and Java code using three code generation models.", "result": "GramTrans significantly improves model performance over baseline representations across several benchmarks for Python and Java code. The results show a strong correlation between easier-to-parse representations and higher model effectiveness. Analysis of other representations also supports their conjecture.", "conclusion": "The paper concludes that parsing difficulty is strongly aligned with code generation model performance, and that transforming code into simpler grammar classes like LL(1) can enhance overall effectiveness. GramTrans provides a practical approach for achieving these improvements."}}
{"id": "2510.02917", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02917", "abs": "https://arxiv.org/abs/2510.02917", "authors": ["Kriz Tahimic", "Charibeth Cheng"], "title": "Mechanistic Interpretability of Code Correctness in LLMs via Sparse Autoencoders", "comment": null, "summary": "As Large Language Models become integral to software development, with\nsubstantial portions of AI-suggested code entering production, understanding\ntheir internal correctness mechanisms becomes critical for safe deployment. We\napply sparse autoencoders to decompose LLM representations, identifying\ndirections that correspond to code correctness. We select predictor directions\nusing t-statistics and steering directions through separation scores from base\nmodel representations, then analyze their mechanistic properties through\nsteering, attention analysis, and weight orthogonalization. We find that code\ncorrectness directions in LLMs reliably predict incorrect code, while\ncorrection capabilities, though statistically significant, involve tradeoffs\nbetween fixing errors and preserving correct code. Mechanistically, successful\ncode generation depends on attending to test cases rather than problem\ndescriptions. Moreover, directions identified in base models retain their\neffectiveness after instruction-tuning, suggesting code correctness mechanisms\nlearned during pre-training are repurposed during fine-tuning. Our mechanistic\ninsights suggest three practical applications: prompting strategies should\nprioritize test examples over elaborate problem descriptions, predictor\ndirections can serve as error alarms for developer review, and these same\npredictors can guide selective steering, intervening only when errors are\nanticipated to prevent the code corruption from constant steering.", "AI": {"tldr": "The paper uses sparse autoencoders to uncover how LLMs internally process code correctness. Directions linked to correctness reliably forecast errors and correction, especially when more attention is paid to test cases than problem descriptions. These insights power strategies for better prompting, error alarms, and improved model steering, demonstrating that code awareness mechanisms learned in pre-training persist after fine-tuning.", "motivation": "Large language models (LLMs) are increasingly used in software engineering, and their output often goes directly into production. Therefore, it is vital to understand how LLMs internally assess and ensure code correctness to enhance safety and reliability.", "method": "The study applies sparse autoencoders to decompose LLM representations and identify directions associated with code correctness. These directions are selected using t-statistics and separation scores on base model representations. The authors further analyze these directions using techniques like steering, attention inspection, and weight orthogonalization to understand their mechanistic roles.", "result": "The analysis reveals that certain directions in LLM representations consistently predict incorrect code. The models' ability to correct errors is statistically significant, but optimizing for both error correction and code preservation involves tradeoffs. Error correction often works best when the model attends to test cases instead of just problem descriptions. Directions identified during pre-training generally remain effective after fine-tuning (instruction-tuning).", "conclusion": "Mechanistic insights from this approach lead to practical applications: (1) prompting should focus on providing test examples rather than verbose descriptions, (2) predictor directions may help serve as error alarms for developer oversight, and (3) selective steering should target anticipated errors to avoid negatively impacting correct code."}}
{"id": "2510.02934", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02934", "abs": "https://arxiv.org/abs/2510.02934", "authors": ["Thanh Trong Vu", "Tuan-Dung Bui", "Thu-Trang Nguyen", "Son Nguyen", "Hieu Dinh Vo"], "title": "Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode generation and are increasingly integrated into the software development\nprocess. However, ensuring the correctness of LLM-generated code remains a\ncritical concern. Prior work has shown that the internal representations of\nLLMs encode meaningful signals for assessing code correctness. Nevertheless,\nthe existing methods rely on representations from pre-selected/fixed layers and\ntoken positions, which could limit its generalizability across diverse model\narchitectures and tasks. In this work, we introduce AUTOPROBE, a novel\nmodel-agnostic approach that dynamically selects the most informative internal\nrepresentations for code correctness assessment. AUTOPROBE employs an\nattention-based mechanism to learn importance scores for hidden states,\nenabling it to focus on the most relevant features. These weighted\nrepresentations are then aggregated and passed to a probing classifier to\npredict code correctness across multiple dimensions, including compilability,\nfunctionality, and security. To evaluate the performance of AUTOPROBE, we\nconduct extensive experiments across multiple benchmarks and code LLMs. Our\nexperimental results show that AUTOPROBE consistently outperforms the\nbaselines. For security assessment, AUTOPROBE surpasses the state-of-the-art\nwhite-box approach by 18%. For compilability and functionality assessment,\nAUTOPROBE demonstrates its highest robustness to code complexity, with the\nperformance higher than the other approaches by up to 19% and 111%,\nrespectively. These findings highlight that dynamically selecting important\ninternal signals enables AUTOPROBE to serve as a robust and generalizable\nsolution for assessing the correctness of code generated by various LLMs.", "AI": {"tldr": "AUTOPROBE is a new attention-based, model-agnostic method for assessing LLM-generated code correctness by dynamically selecting informative internal signals. It robustly outperforms existing approaches in security, compilability, and functionality assessment across diverse models and benchmarks.", "motivation": "Though LLMs are powerful at generating code, ensuring the correctness (compilability, functionality, security) of their outputs remains challenging. Prior methods for correctness assessment use fixed model layer/position representations, which limits adaptability to different LLM architectures and tasks.", "method": "AUTOPROBE is introduced as a model-agnostic, attention-based mechanism that dynamically selects the most informative hidden states from LLMs. It learns importance scores for representations, aggregates them, and uses a probing classifier to assess correctness across multiple dimensions.", "result": "AUTOPROBE outperforms current baselines across code correctness benchmarks and LLMs. It achieves 18% better security assessment than the previous best white-box approach, shows up to 19% stronger compilability assessment, and provides up to 111% better functionality assessment, maintaining high robustness across code complexities.", "conclusion": "The dynamic, attention-guided selection of internal signals in AUTOPROBE enables broad, robust, and generalizable assessment of code correctness from various LLMs, improving performance beyond static and fixed probing approaches."}}
{"id": "2510.02991", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02991", "abs": "https://arxiv.org/abs/2510.02991", "authors": ["Carlos Albuquerque", "Filipe F. Correia"], "title": "Tracing and Metrics Design Patterns for Monitoring Cloud-native Applications", "comment": "Accepted for publication in the EuroPLoP 2025 proceedings", "summary": "Observability helps ensure the reliability and maintainability of\ncloud-native applications. As software architectures become increasingly\ndistributed and subject to change, it becomes a greater challenge to diagnose\nsystem issues effectively, often having to deal with fragmented observability\nand more difficult root cause analysis. This paper builds upon our previous\nwork and introduces three design patterns that address key challenges in\nmonitoring cloud-native applications. Distributed Tracing improves visibility\ninto request flows across services, aiding in latency analysis and root cause\ndetection, Application Metrics provides a structured approach to instrumenting\napplications with meaningful performance indicators, enabling real-time\nmonitoring and anomaly detection, and Infrastructure Metrics focuses on\nmonitoring the environment in which the system is operated, helping teams\nassess resource utilization, scalability, and operational health. These\npatterns are derived from industry practices and observability frameworks and\naim to offer guidance for software practitioners.", "AI": {"tldr": "Cloud-native apps face observability challenges due to fragmentation and complexity. This paper introduces three patterns\u2014Distributed Tracing, Application Metrics, and Infrastructure Metrics\u2014each improving reliability, diagnosing issues, and monitoring performance based on successful industry practices.", "motivation": "As cloud-native applications become more distributed and rapidly evolving, traditional observability methods struggle with fragmented monitoring and complicated root cause analysis.", "method": "The paper builds on previous research and proposes three main design patterns\u2014Distributed Tracing, Application Metrics, and Infrastructure Metrics\u2014for monitoring cloud-native environments. These are distilled from industry practices and observability frameworks.", "result": "The paper defines and illustrates three patterns: Distributed Tracing for analyzing request flows and latency, Application Metrics for structured performance monitoring and anomaly detection, and Infrastructure Metrics for understanding resource utilization and system health.", "conclusion": "The introduction and description of these three observability patterns provides practical guidance for improving reliability and maintainability of cloud-native applications."}}
{"id": "2510.03005", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03005", "abs": "https://arxiv.org/abs/2510.03005", "authors": ["Daniel Pinho", "Petr P\u00edcha", "Filipe Correia", "P\u0159emek Brada"], "title": "Patterns for Teaching Agile with Student Projects -- Team and Project Setup", "comment": "Accepted for publication in the EuroPLoP 2025 proceedings", "summary": "Higher education courses teaching about agile software development (ASD) have\nincreased in commonality as the ideas behind the Agile Manifesto became more\ncommonplace in the industry. However, a lot of the literature on how ASD is\napplied in the classroom does not provide much actionable advice, focusing on\nframeworks or even moving beyond the software development area into teaching in\nan agile way. We, therefore, showcase early work on a pattern language that\nfocuses on teaching ASD practices to university students, which stems from our\nown experiences as educators in higher education contexts. We present five\npatterns, specifically focused on team and project setup phase: Capping Team\nSize, Smaller Project Scope, Business Non-Critical Project, Self-assembling\nTeams, and Team Chooses Topic as a starting point for developing the overall\npattern language.", "AI": {"tldr": "Most literature on teaching Agile Software Development is too theoretical for direct classroom use. This paper presents practical, experience-based patterns for better team and project setups in ASD courses, offering more actionable guidance for educators.", "motivation": "Many higher education courses teach Agile Software Development (ASD), but much of the literature lacks actionable, practical guidance for classroom use, particularly for teaching ASD practices directly to university students.", "method": "The authors draw from their own experience as educators to develop and present a pattern language\u2014a structured approach\u2014focused on the team and project setup phase for teaching ASD. They introduce five specific patterns as initial examples.", "result": "Five patterns are identified and described to help set up teams and projects in ASD education: Capping Team Size, Smaller Project Scope, Business Non-Critical Project, Self-assembling Teams, and Team Chooses Topic. These are proposed as foundational elements for an overall pattern language for teaching ASD.", "conclusion": "The paper provides practical, experience-based patterns as actionable advice for educators to use when teaching Agile Software Development, especially during the team and project setup phases. This work represents an initial effort toward a larger pattern language aimed at improving ASD education."}}
{"id": "2510.03029", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03029", "abs": "https://arxiv.org/abs/2510.03029", "authors": ["Debalina Ghosh Paul", "Hong Zhu", "Ian Bayley"], "title": "Investigating The Smells of LLM Generated Code", "comment": null, "summary": "Context: Large Language Models (LLMs) are increasingly being used to generate\nprogram code. Much research has been reported on the functional correctness of\ngenerated code, but there is far less on code quality.\n  Objectives: In this study, we propose a scenario-based method of evaluating\nthe quality of LLM-generated code to identify the weakest scenarios in which\nthe quality of LLM generated code should be improved.\n  Methods: The method measures code smells, an important indicator of code\nquality, and compares them with a baseline formed from reference solutions of\nprofessionally written code. The test dataset is divided into various subsets\naccording to the topics of the code and complexity of the coding tasks to\nrepresent different scenarios of using LLMs for code generation. We will also\npresent an automated test system for this purpose and report experiments with\nthe Java programs generated in response to prompts given to four\nstate-of-the-art LLMs: Gemini Pro, ChatGPT, Codex, and Falcon.\n  Results: We find that LLM-generated code has a higher incidence of code\nsmells compared to reference solutions. Falcon performed the least badly, with\na smell increase of 42.28%, followed by Gemini Pro (62.07%), ChatGPT (65.05%)\nand finally Codex (84.97%). The average smell increase across all LLMs was\n63.34%, comprising 73.35% for implementation smells and 21.42% for design\nsmells. We also found that the increase in code smells is greater for more\ncomplex coding tasks and for more advanced topics, such as those involving\nobject-orientated concepts.\n  Conclusion: In terms of code smells, LLM's performances on various coding\ntask complexities and topics are highly correlated to the quality of human\nwritten code in the corresponding scenarios. However, the quality of LLM\ngenerated code is noticeably poorer than human written code.", "AI": {"tldr": "This paper systematically explores the code quality\u2014specifically code smells\u2014of Java programs generated by state-of-the-art LLMs compared to professional code. Code generated by LLMs shows substantially more quality issues, especially for complex tasks and advanced topics, although the trend mirrors human coding patterns. Falcon exhibited the least quality degradation among the tested models, but overall, machine-generated code remains inferior in quality to human-written code.", "motivation": "While much research focuses on the functional correctness of code generated by Large Language Models (LLMs), there is comparatively little investigation into their code quality. This paper aims to fill that gap by systematically evaluating the code quality of LLM-generated programs.", "method": "The authors present a scenario-based evaluation method, which measures code smells as an indicator of code quality. LLM outputs are compared to professionally written reference solutions. The dataset is divided by topic and coding complexity to test LLM performance across various scenarios. An automated test system is developed and experiments are conducted using Java code generated by four prominent LLMs: Gemini Pro, ChatGPT, Codex, and Falcon.", "result": "LLM-generated code consistently shows a higher rate of code smells than professional reference solutions. Among tested models, Falcon exhibits the smallest increase in code smells (42.28%), followed by Gemini Pro (62.07%), ChatGPT (65.05%), and Codex (84.97%). On average, LLM-generated code shows a 63.34% increase in code smells, more pronounced in implementation (73.35%) than design (21.42%) categories. More complex and advanced coding scenarios further exacerbate quality issues.", "conclusion": "LLMs' code quality, as measured by code smells, is closely tied to task complexity and topic, mirroring patterns seen in human-coded solutions. Nonetheless, the overall quality of LLM-generated code remains significantly lower than that of human-written code across all tested scenarios."}}
{"id": "2510.03050", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03050", "abs": "https://arxiv.org/abs/2510.03050", "authors": ["Rita Peixoto", "Filipe F. Correia", "Thatiane Rosa", "Eduardo Guerra", "Alfredo Goldman"], "title": "Refactoring Towards Microservices: Preparing the Ground for Service Extraction", "comment": "Accepted for publication in the EuroPLoP 2025 proceedings", "summary": "As organizations increasingly transition from monolithic systems to\nmicroservices, they aim to achieve higher availability, automatic scaling,\nsimplified infrastructure management, enhanced collaboration, and streamlined\ndeployments. However, this migration process remains largely manual and\nlabour-intensive. While existing literature offers various strategies for\ndecomposing monoliths, these approaches primarily focus on architecture-level\nguidance, often overlooking the code-level challenges and dependencies that\ndevelopers must address during the migration. This article introduces a\ncatalogue of seven refactorings specifically designed to support the transition\nto a microservices architecture with a focus on handling dependencies. The\ncatalogue provides developers with a systematic guide that consolidates\nrefactorings identified in the literature and addresses the critical gap in\nsystematizing the process at the code level. By offering a structured,\nstep-by-step approach, this work simplifies the migration process and lays the\ngroundwork for its potential automation, empowering developers to implement\nthese changes efficiently and effectively.", "AI": {"tldr": "This paper tackles the manual, code-level challenges of migrating from monoliths to microservices by presenting a structured catalogue of seven refactorings, making the process more systematic and easier for developers.", "motivation": "Many organizations want to migrate from monolithic systems to microservices to gain numerous advantages, but the migration is mostly manual and code-level challenges are under-addressed in literature.", "method": "The authors introduce a catalogue of seven refactorings focused at the code-level, especially dealing with dependencies, consolidating approaches from previous literature and systematizing them for practical use.", "result": "A systematic, step-by-step guide for code-level refactorings is presented, making the migration process more straightforward and paving the way for potential automation.", "conclusion": "The work fills a critical gap by providing developers with practical tools for code-level migration to microservices, simplifying an otherwise labour-intensive process and fostering future automation."}}
{"id": "2510.03071", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03071", "abs": "https://arxiv.org/abs/2510.03071", "authors": ["Facundo Molina", "Nazareno Aguirre", "Alessandra Gorla"], "title": "State Field Coverage: A Metric for Oracle Quality", "comment": null, "summary": "The effectiveness of testing in uncovering software defects depends not only\non the characteristics of the test inputs and how thoroughly they exercise the\nsoftware, but also on the quality of the oracles used to determine whether the\nsoftware behaves as expected. Therefore, assessing the quality of oracles is\ncrucial to improve the overall effectiveness of the testing process. Existing\nmetrics have been used for this purpose, but they either fail to provide a\ncomprehensive basis for guiding oracle improvement, or they are tailored to\nspecific types of oracles, thus limiting their generality.\n  In this paper, we introduce state field coverage, a novel metric for\nassessing oracle quality. This metric measures the proportion of an object's\nstate, as statically defined by its class fields, that an oracle may access\nduring test execution. The main intuition of our metric is that oracles with a\nhigher state field coverage are more likely to detect faults in the software\nunder analysis, as they inspect a larger portion of the object states to\ndetermine whether tests pass or not.\n  We implement a mechanism to statically compute the state field coverage\nmetric. Being statically computed, the metric is efficient and provides direct\nguidance for improving test oracles by identifying state fields that remain\nunexamined. We evaluate state field coverage through experiments involving 273\nrepresentation invariants and 249,027 test assertions. The results show that\nstate field coverage is a well-suited metric for assessing oracle quality, as\nit strongly correlates with the oracles' fault-detection ability, measured by\nmutation score.", "AI": {"tldr": "This paper introduces 'state field coverage,' a new metric for evaluating software test oracles, showing it efficiently and reliably predicts oracles' ability to detect faults, thereby guiding improvements in software testing.", "motivation": "The paper addresses the crucial role of test oracles in determining software testing effectiveness, pointing out the limitations of current metrics for assessing oracle quality. Existing metrics either lack general applicability or fail to guide oracle improvement comprehensively.", "method": "The authors propose 'state field coverage,' a new metric that measures how much of an object's state is examined by test oracles, based on statically defined class fields. They implement a static computation mechanism for this metric to make it efficient and actionable.", "result": "Through extensive experiments with 273 representation invariants and over 249,000 test assertions, the paper demonstrates that state field coverage strongly correlates with fault detection capability, as measured by mutation score.", "conclusion": "State field coverage is an effective and general metric for assessing and improving oracle quality, providing practical guidance on increasing the effectiveness of software testing."}}
{"id": "2510.03178", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03178", "abs": "https://arxiv.org/abs/2510.03178", "authors": ["Cuong Chi Le", "Minh V. T. Pham", "Cuong Duc Van", "Hoang N. Phan", "Huy N. Phan", "Tien N. Nguyen"], "title": "When Names Disappear: Revealing What LLMs Actually Understand About Code", "comment": null, "summary": "Large Language Models (LLMs) achieve strong results on code tasks, but how\nthey derive program meaning remains unclear. We argue that code communicates\nthrough two channels: structural semantics, which define formal behavior, and\nhuman-interpretable naming, which conveys intent. Removing the naming channel\nseverely degrades intent-level tasks such as summarization, where models\nregress to line-by-line descriptions. Surprisingly, we also observe consistent\nreductions on execution tasks that should depend only on structure, revealing\nthat current benchmarks reward memorization of naming patterns rather than\ngenuine semantic reasoning. To disentangle these effects, we introduce a suite\nof semantics-preserving obfuscations and show that they expose identifier\nleakage across both summarization and execution. Building on these insights, we\nrelease ClassEval-Obf, an obfuscation-enhanced benchmark that systematically\nsuppresses naming cues while preserving behavior. Our results demonstrate that\nClassEval-Obf reduces inflated performance gaps, weakens memorization\nshortcuts, and provides a more reliable basis for assessing LLMs' code\nunderstanding and generalization.", "AI": {"tldr": "LLMs often rely on memorizing code naming conventions rather than true semantic understanding; a new obfuscation-based benchmark reveals these weaknesses and offers a more reliable assessment of LLMs' code reasoning skills.", "motivation": "The motivation is to investigate how large language models (LLMs) understand code, specifically whether their strong performance on code tasks is genuinely based on semantic reasoning or is driven by memorization of naming conventions. There is a need for benchmarks that accurately assess true code understanding without the confounding influence of human-interpretable names.", "method": "The paper analyzes the effects of removing human-interpretable naming from code, applying semantics-preserving obfuscations, and measuring performance of LLMs on both summarization (intent-level) and execution (structural) tasks. The study introduces ClassEval-Obf, a benchmark that obfuscates naming cues while preserving the underlying program behavior.", "result": "The results show that removing naming significantly degrades model performance, even on execution tasks that should depend only on structural semantics. This finding suggests current benchmarks may overstate LLMs' code understanding due to memorization of naming patterns. The obfuscation-enhanced benchmark ClassEval-Obf reduces these inflated gaps and exposes how much models rely on superficial naming cues.", "conclusion": "The study concludes that existing LLM benchmarks are susceptible to identifier leakage and memorization shortcuts. ClassEval-Obf provides a more accurate measure of LLMs' true code understanding and promotes generalization rather than memorization."}}
{"id": "2510.03217", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03217", "abs": "https://arxiv.org/abs/2510.03217", "authors": ["Jos\u00e9 Cambronero", "Michele Tufano", "Sherry Shi", "Renyao Wei", "Grant Uy", "Runxiang Cheng", "Chin-Jung Liu", "Shiying Pan", "Satish Chandra", "Pat Rondon"], "title": "Abstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic Program Repair", "comment": null, "summary": "Agentic Automated Program Repair (APR) is increasingly tackling complex,\nrepository-level bugs in industry, but ultimately agent-generated patches still\nneed to be reviewed by a human before committing them to ensure they address\nthe bug. Showing unlikely patches to developers can lead to substantial noise,\nwasting valuable developer time and eroding trust in automated code changes. We\nintroduce two complementary LLM-based policies to reduce such noise: bug\nabstention and patch validation policies. Bug abstention excludes bugs that the\nagentic APR system is unlikely to fix. Patch validation rejects patches that\nare unlikely to be a good fix for the given bug. We evaluate both policies on\nthree sets of bugs from Google's codebase, and their candidate patches\ngenerated by an internal agentic APR system. On a set of 174 human-reported\nbugs, removing bugs and patch trajectories rejected by our policies can raise\nsuccess rates by up to 13 percentage points and 15 percentage points,\nrespectively, and by up to 39 percentage points in combination. On null pointer\nexceptions and sanitizer-reported bugs with machine-generated bug reports,\npatch validation also improves average single-sample success rates. This\ntwo-policy approach provides a practical path to the reliable, industrial-scale\ndeployment of agentic APR systems.", "AI": {"tldr": "Introducing LLM-based bug abstention and patch validation policies greatly reduces noise and increases the success rate of automated program repair systems, making them more suitable for large-scale industrial deployment.", "motivation": "Automated Program Repair (APR) systems are advancing to resolve complex bugs in industrial codebases, but their outputs often require manual review due to the risk of proposing ineffective or spurious patches. Excessive noise from unlikely patches can waste developer time and diminish trust in automation.", "method": "The paper introduces two LLM-based (Large Language Model) policies: bug abstention, which filters out bugs unlikely to be fixed by the APR system, and patch validation, which rejects patches that are unlikely to effectively resolve a given bug. Both methods are evaluated on various bug datasets from Google's codebase, using patches generated by an internal APR system.", "result": "Implementing bug abstention and patch validation individually can increase successful patch rates by up to 13 and 15 percentage points, respectively, and up to 39 percentage points when combined. The patch validation policy also improves results for machine-generated bug reports (e.g., null pointer exceptions and sanitizer reports).", "conclusion": "The combination of bug abstention and patch validation policies significantly improves the reliability and practicality of agentic APR systems for industrial use. This approach facilitates more trusted, scalable deployment of automated patch generation."}}
