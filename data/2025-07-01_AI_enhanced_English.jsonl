{"id": "2506.22656", "categories": ["cs.SE", "cs.AI", "68-04", "D.2.3; I.2.7"], "pdf": "https://arxiv.org/pdf/2506.22656", "abs": "https://arxiv.org/abs/2506.22656", "authors": ["Jiangping Huang", "Dongming Jin", "Weisong Sun", "Yang Liu", "Zhi Jin"], "title": "Knowledge-Guided Multi-Agent Framework for Automated Requirements Development: A Vision", "comment": null, "summary": "This paper envisions a knowledge-guided multi-agent framework named KGMAF for\nautomated requirements development. KGMAF aims to address gaps in current\nautomation systems for SE, which prioritize code development and overlook the\ncomplexities of requirements tasks. KGMAF is composed of six specialized agents\nand an artifact pool to improve efficiency and accuracy. Specifically, KGMAF\noutlines the functionality, actions, and knowledge of each agent and provides\nthe conceptual design of the artifact pool. Our case study highlights the\npotential of KGMAF in real-world scenarios. Finally, we outline several\nresearch opportunities for implementing and enhancing automated requirements\ndevelopment using multi-agent systems. We believe that KGMAF will play a\npivotal role in shaping the future of automated requirements development in the\nera of LLMs.", "AI": {"tldr": "KGMAF is a knowledge-driven multi-agent system designed to automate and improve requirements development in software engineering, offering enhanced efficiency through specialized agents and an artifact pool. A case study shows its promise, pointing to future research potential in this area as LLMs become more prevalent.", "motivation": "Current software engineering automation systems focus mainly on code development and do not sufficiently address the complex tasks involved in requirements development. There is a need for frameworks that improve the efficiency and accuracy of these critical upstream tasks.", "method": "The paper proposes KGMAF, a knowledge-guided multi-agent framework, consisting of six specialized agents and an artifact pool. The design details each agent's function, actions, and knowledge. They also present a conceptual design for the artifact pool and validate the framework with a case study.", "result": "The case study demonstrates the potential effectiveness of KGMAF in real-world software engineering scenarios and identifies research opportunities for advancing multi-agent automated requirements development.", "conclusion": "KGMAF is positioned as a pivotal solution for advancing automated requirements development in software engineering, particularly in the era of large language models. The framework addresses gaps in current automation systems by providing specialized agents and a structured approach to complex requirements tasks."}}
{"id": "2506.22688", "categories": ["cs.SE", "D.2.11; D.2.2"], "pdf": "https://arxiv.org/pdf/2506.22688", "abs": "https://arxiv.org/abs/2506.22688", "authors": ["Humberto Cervantes", "Rick Kazman", "Yuanfang Cai"], "title": "An LLM-assisted approach to designing software architectures using ADD", "comment": "30 pages, 12 figures, 7 tables", "summary": "Designing effective software architectures is a complex, iterative process\nthat traditionally relies on expert judgment. This paper proposes an approach\nfor Large Language Model (LLM)-assisted software architecture design using the\nAttribute-Driven Design (ADD) method. By providing an LLM with an explicit\ndescription of ADD, an architect persona, and a structured iteration plan, our\nmethod guides the LLM to collaboratively produce architecture artifacts with a\nhuman architect. We validate the approach through case studies, comparing\ngenerated designs against proven solutions and evaluating them with\nprofessional architects. Results show that our LLM-assisted ADD process can\ngenerate architectures closely aligned with established solutions and partially\nsatisfying architectural drivers, highlighting both the promise and current\nlimitations of using LLMs in architecture design. Our findings emphasize the\nimportance of human oversight and iterative refinement when leveraging LLMs in\nthis domain.", "AI": {"tldr": "The paper introduces and assesses an LLM-assisted method for software architecture design using the ADD process, showing promising but imperfect results. LLMs can help generate plausible architectures but still depend on expert human guidance for optimal outcomes.", "motivation": "Software architecture design is a complex task, historically dependent on expert opinion. With the rise of Large Language Models (LLMs), there is a need to evaluate their effectiveness and role in supporting this process, especially using established methods like Attribute-Driven Design (ADD).", "method": "The authors propose an LLM-assisted design method based on ADD. The approach gives the LLMs a structured description of ADD, an architect persona, and a step-by-step iteration plan, enabling the LLM to collaboratively develop architecture artifacts with a human architect. They validate this approach through case studies, comparing LLM-generated designs to existing solutions and seeking evaluation from professional architects.", "result": "The LLM-assisted ADD approach can generate software architectures that are similar to established solutions and partially satisfy required architectural drivers. However, there are limitations, such as incomplete satisfaction of drivers, indicating the need for ongoing human involvement and oversight.", "conclusion": "LLMs show significant potential to assist in software architecture design when guided through explicit methods like ADD. Human expertise and iterative refinement remain essential to achieving high-quality results, as LLMs currently have limitations in fully understanding and satisfying complex architectural requirements."}}
{"id": "2506.22703", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22703", "abs": "https://arxiv.org/abs/2506.22703", "authors": ["Wali Mohammad Abdullah", "Azmain Kabir"], "title": "P4OMP: Retrieval-Augmented Prompting for OpenMP Parallelism in Serial Code", "comment": null, "summary": "We present P4OMP, a retrieval-augmented framework for transforming serial\nC/C++ code into OpenMP-annotated parallel code using large language models\n(LLMs). To our knowledge, this is the first system to apply retrieval-based\nprompting for OpenMP pragma correctness without model fine-tuning or compiler\ninstrumentation. P4OMP leverages Retrieval-Augmented Generation (RAG) with\nstructured instructional knowledge from OpenMP tutorials to improve the\nreliability of prompt-driven code generation. By grounding generation in the\nretrieved context, P4OMP improves syntactic correctness compared to baseline\nprompting with GPT-3.5-Turbo. We evaluate P4OMP against a baseline,\nGPT-3.5-Turbo without retrieval, on a comprehensive benchmark of 108 real-world\nC++ programs drawn from Stack Overflow, PolyBench, and NAS benchmark suites.\nP4OMP achieves 100% compilation success on all parallelizable cases, while the\nbaseline fails to compile in 20 out of 108 cases. Six cases that rely on\nnon-random-access iterators or thread-unsafe constructs are excluded due to\nfundamental OpenMP limitations. A detailed analysis demonstrates how P4OMP\nconsistently avoids scoping errors, syntactic misuse, and invalid directive\ncombinations that commonly affect baseline-generated code. We further\ndemonstrate strong runtime scaling across seven compute-intensive benchmarks on\nan HPC cluster. P4OMP offers a robust, modular pipeline that significantly\nimproves the reliability and applicability of LLM-generated OpenMP code.", "AI": {"tldr": "P4OMP is a framework that uses retrieval-augmented prompts to guide LLMs in generating syntactically correct OpenMP-parallelized code from serial C/C++ sources. It achieves higher compilation and runtime reliability than baseline LLM prompting, offering a robust, practical tool for automatic code parallelization.", "motivation": "Current LLMs often produce incorrect OpenMP pragma annotations when converting serial C/C++ code to parallel code, mainly due to a lack of structured guidance or context, leading to syntactic and scoping errors. Improving prompt reliability and output correctness for parallel code generation is an unsolved and important challenge.", "method": "P4OMP is a retrieval-augmented generation (RAG) framework that feeds instructional knowledge, retrieved from OpenMP tutorials, into the prompting context of large language models (specifically, GPT-3.5-Turbo) for code generation. No model fine-tuning or compiler instrumentation is involved. Performance was benchmarked against GPT-3.5-Turbo without retrieval across 108 real-world C++ programs, focusing on compilation correctness and runtime scaling.", "result": "P4OMP achieved 100% compilation success for all parallelizable test cases (after excluding those fundamentally incompatible with OpenMP), whereas the baseline (no retrieval augmentation) failed in 20/108 cases. P4OMP effectively prevented common errors like scoping mistakes and invalid directive usage. Additionally, the system demonstrated strong runtime performance and scalability across various compute-intensive benchmarks.", "conclusion": "By incorporating retrieval-based prompting with OpenMP instructional content, P4OMP significantly enhances the correctness and reliability of LLM-generated OpenMP parallel code. The modular pipeline also promotes wider applicability and more robust code generation, addressing a critical gap that limits the practical use of LLMs for code parallelization."}}
{"id": "2506.22742", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22742", "abs": "https://arxiv.org/abs/2506.22742", "authors": ["Wali Mohammad Abdullah", "Md. Morshedul Islam", "Devraj Parmar", "Happy Hasmukhbhai Patel", "Sindhuja Prabhakaran", "Baidya Saha"], "title": "RAILS: Retrieval-Augmented Intelligence for Learning Software Development", "comment": null, "summary": "Large Language Models (LLMs) like GPT-3.5-Turbo are increasingly used to\nassist software development, yet they often produce incomplete code or\nincorrect imports, especially when lacking access to external or\nproject-specific documentation. We introduce RAILS (Retrieval-Augmented\nIntelligence for Learning Software Development), a framework that augments LLM\nprompts with semantically retrieved context from curated Java resources using\nFAISS and OpenAI embeddings. RAILS incorporates an iterative validation loop\nguided by compiler feedback to refine suggestions. We evaluated RAILS on 78\nreal-world Java import error cases spanning standard libraries, GUI APIs,\nexternal tools, and custom utilities. Despite using the same LLM, RAILS\noutperforms baseline prompting by preserving intent, avoiding hallucinations,\nand surfacing correct imports even when libraries are unavailable locally.\nFuture work will integrate symbolic filtering via PostgreSQL and extend support\nto other languages and IDEs.", "AI": {"tldr": "RAILS boosts LLM-powered software development by semantically retrieving relevant Java documentation and refining code suggestions via compiler feedback, leading to fewer errors and more accurate imports compared to basic prompting.", "motivation": "Large Language Models (LLMs) like GPT-3.5-Turbo assist in software development but frequently generate incomplete code or incorrect imports, mainly due to limited access to external or project-specific documentation. There is a need for methods that can improve the reliability and accuracy of LLM-generated code in such settings.", "method": "The authors introduce RAILS, a Retrieval-Augmented Intelligence framework that enhances LLM prompts by semantically retrieving context from curated Java resources using FAISS and OpenAI embeddings. It also includes an iterative validation loop that utilizes compiler feedback to refine the suggestions.", "result": "In testing with 78 real-world Java import error cases, RAILS showed improved performance over baseline prompting. It was better at preserving developer intent, avoiding hallucinated code, and providing correct imports, even for libraries not available locally.", "conclusion": "RAILS significantly augments the capabilities of LLMs in code generation by effectively sourcing relevant context and iteratively validating suggestions. It outperforms traditional prompting, especially regarding Java import errors, and future enhancements are planned for broader applicability."}}
{"id": "2506.23058", "categories": ["cs.PL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2506.23058", "abs": "https://arxiv.org/abs/2506.23058", "authors": ["Nikolaj Hey Hinnerskov", "Robert Schenck", "Cosmin E. Oancea"], "title": "Verifying Properties of Index Arrays in a Purely-Functional Data-Parallel Language", "comment": null, "summary": "This paper presents a novel approach to automatically verify properties of\npure data-parallel programs with non-linear indexing -- expressed as pre- and\npost-conditions on functions. Programs consist of nests of second-order array\ncombinators (e.g., map, scan, and scatter) and loops. The key idea is to\nrepresent arrays as index functions: programs are index function\ntransformations over which properties are propagated and inferred. Our\nframework proves properties on index functions by distilling them into\nalgebraic (in)equalities and discharging them to a Fourier-Motzkin-based\nsolver. The framework is practical and accessible: properties are not\nrestricted to a decidable logic, but instead are carefully selected to express\npractically useful guarantees that can be automatically reasoned about and\ninferred. These guarantees extend beyond program correctness and can be\nexploited by the entire compiler pipeline for optimization. We implement our\nsystem in the pure data-parallel language Futhark and demonstrate its\npracticality on seven applications, reporting an average verification time of 1\nsecond. Two case studies show how eliminating dynamic verification in GPU\nprograms results in significant speedups.", "AI": {"tldr": "A novel verification method for data-parallel programs with non-linear indexing using index functions and algebraic reasoning, implemented in Futhark. Delivers fast and practical verification, improving both correctness and performance (notably for GPU programs) without restricting to decidable logics.", "motivation": "Automatically verifying properties of data-parallel programs, especially those with non-linear indexing and higher-order array constructs, is challenging yet highly desirable for correctness and optimization.", "method": "The authors represent arrays as index functions and express program transformations as manipulations over these index functions. Properties are propagated and inferred by transforming them into algebraic inequalities (mainly (in)equalities) which are then checked using a Fourier-Motzkin-based solver. The framework is implemented in the Futhark programming language.", "result": "The system proved practical and effective, with an average verification time of 1 second across seven applications. In two case studies, eliminating the need for dynamic verification in GPU programs led to significant speedups.", "conclusion": "The framework enables automatic property verification for pure data-parallel programs with non-linear indexing, supports practically useful properties, and can be exploited beyond correctness (e.g., for compiler optimization). Its implementation in Futhark and experimental results demonstrate practicality and performance gains."}}
{"id": "2506.22752", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.22752", "abs": "https://arxiv.org/abs/2506.22752", "authors": ["Havvanur Dervi\u015fo\u011flu", "Ru\u015fen Halepmollas\u0131", "Elif Eyvaz"], "title": "Privacy-Preserving Methods for Bug Severity Prediction", "comment": null, "summary": "Bug severity prediction is a critical task in software engineering as it\nenables more efficient resource allocation and prioritization in software\nmaintenance. While AI-based analyses and models significantly require access to\nextensive datasets, industrial applications face challenges due to data-sharing\nconstraints and the limited availability of labeled data. In this study, we\ninvestigate method-level bug severity prediction using source code metrics and\nLarge Language Models (LLMs) with two widely used datasets. We compare the\nperformance of models trained using centralized learning, federated learning,\nand synthetic data generation. Our experimental results, obtained using two\nwidely recognized software defect datasets, indicate that models trained with\nfederated learning and synthetic data achieve comparable results to centrally\ntrained models without data sharing. Our finding highlights the potential of\nprivacy-preserving approaches such as federated learning and synthetic data\ngeneration to enable effective bug severity prediction in industrial context\nwhere data sharing is a major challenge.\n  The source code and dataset are available at our GitHub repository:\nhttps://github.com/drvshavva/EASE2025-Privacy-Preserving-Methods-for-Bug-Severity-Prediction.", "AI": {"tldr": "Federated learning and synthetic data can predict bug severity as effectively as traditional centralized approaches, while preserving data privacy\u2014important for real-world industrial applications where sharing data is a challenge.", "motivation": "Bug severity prediction helps allocate resources efficiently in software maintenance. However, in industrial settings, it is difficult to share or obtain large labeled datasets due to privacy and availability issues, thereby limiting the effectiveness of AI models.", "method": "This study investigates bug severity prediction at the method level using source code metrics and Large Language Models (LLMs). It compares centralized learning, federated learning, and synthetic data generation methods for training models, evaluating them on two well-known software defect datasets.", "result": "Federated learning and models using synthetic data performed comparably to traditionally trained (centralized) models, even without data sharing.", "conclusion": "Privacy-preserving methods like federated learning and synthetic data generation can provide effective bug severity prediction in situations where data sharing is restricted, making them promising for industrial applications."}}
{"id": "2506.23320", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2506.23320", "abs": "https://arxiv.org/abs/2506.23320", "authors": ["Nicola Assolini", "Alessandra Di Pierro"], "title": "A Denotational Semantics for Quantum Loops", "comment": "17 pages", "summary": "Programming a quantum computer, i.e., implementing quantum algorithms on a\nquantum processor-based copmputer architecture, is a task that can be addressed\n(just as for classical computers) at different levels of abstraction. This\npaper proposes a denotational semantics for high-level quantum programming\nconstructs, focusing on the conceptual meaning of quantum-controlled branching\nand iteration. We introduce a denotational domain where a mathematical meaning\nof a quantum control flow with loops can be defined, which reflects the\ncoherent evolution of the quantum system implementing the program.", "AI": {"tldr": "The paper develops a mathematical framework to define and analyze high-level quantum programming constructs, especially quantum-controlled branching and loops, advancing the foundations for quantum programming languages.", "motivation": "Programming quantum computers involves various abstraction levels, similar to classical computers. High-level quantum programming constructs do not yet have a well-established mathematical semantics, especially concerning quantum-controlled branching and iteration.", "method": "The authors propose a denotational semantics for high-level quantum programming constructs. They introduce a specific denotational domain capable of capturing the mathematical meaning of quantum control flow, including loops, reflecting the coherent (unitary) evolution of quantum systems.", "result": "A framework is developed that provides a mathematical (denotational) semantics for quantum control flow with loops, supporting the conceptual understanding and development of quantum programming languages.", "conclusion": "The paper clarifies the meaning of quantum-controlled branching and looping in high-level quantum programming, offering a semantic domain suitable for expressing and analyzing such constructs."}}
{"id": "2506.22776", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2506.22776", "abs": "https://arxiv.org/abs/2506.22776", "authors": ["Sen Fang", "Weiyuan Ding", "Antonio Mastropaolo", "Bowen Xu"], "title": "Smaller = Weaker? Benchmarking Robustness of Quantized LLMs in Code Generation", "comment": "13 pages, 6 figures", "summary": "Quantization has emerged as a mainstream method for compressing Large\nLanguage Models (LLMs), reducing memory requirements and accelerating inference\nwithout architectural modifications. While existing research primarily focuses\non evaluating the effectiveness of quantized LLMs compared to their original\ncounterparts, the impact on robustness remains largely unexplored.In this\npaper, we present the first systematic investigation of how quantization\naffects the robustness of LLMs in code generation tasks. Through extensive\nexperiments across four prominent LLM families (LLaMA, DeepSeek, CodeGen, and\nStarCoder) with parameter scales ranging from 350M to 33B, we evaluate\nrobustness from dual perspectives: adversarial attacks on input prompts and\nnoise perturbations on model architecture. Our findings challenge conventional\nwisdom by demonstrating that quantized LLMs often exhibit superior robustness\ncompared to their full-precision counterparts, with 51.59% versus 42.86% of our\nadversarial experiments showing better resilience in quantized LLMs. Similarly,\nour noise perturbation experiments also confirm that LLMs after quantitation\ngenerally withstand higher levels of weight disturbances. These results suggest\nthat quantization not only reduces computational requirements but can actually\nenhance LLMs' reliability in code generation tasks, providing valuable insights\nfor developing more robust and efficient LLM deployment strategies.", "AI": {"tldr": "Quantizing large language models for code generation not only reduces computational needs but also improves robustness to adversarial attacks and noise, outperforming original models in most tested scenarios.", "motivation": "While quantization is commonly used to compress Large Language Models (LLMs) and speed up inference, its impact on model robustness, especially for code generation tasks, has not been systematically explored. Given the importance of both efficiency and reliability in model deployment, understanding the trade-offs, if any, between quantization and robustness is crucial.", "method": "The authors conduct a comprehensive evaluation of quantized versus full-precision LLMs across four major model families (LLaMA, DeepSeek, CodeGen, and StarCoder) at multiple parameter scales (350M to 33B). They measure robustness from two perspectives: (1) susceptibility to adversarial attacks on input prompts, and (2) resilience to random noise perturbations applied to the model weights.", "result": "Quantized LLMs demonstrate higher robustness than their full-precision counterparts in code generation tasks. Specifically, 51.59% of adversarial attack scenarios favored quantized models (compared to 42.86% for full-precision), and noise perturbation tests showed quantized LLMs generally tolerated higher levels of weight disturbance.", "conclusion": "Contrary to expectations, quantization not only compresses LLMs and accelerates inference but also enhances their robustness in code generation tasks. These findings inform strategies for deploying more robust and efficient LLMs."}}
{"id": "2506.23407", "categories": ["cs.PL", "quant-ph"], "pdf": "https://arxiv.org/pdf/2506.23407", "abs": "https://arxiv.org/abs/2506.23407", "authors": ["Marcus Edwards"], "title": "Compiling a Q# Subset to QASM 3.0 in TypeScript via a JSON Based IR", "comment": null, "summary": "We implement a compile toolchain from Q# to QASM 3.0 including a\nfull-featured lexer and parser implementation, as well as a compiler that\nsupports a subset of Q# features. The lexer, parser and compiler are shown to\nwork with various input Q# programs and the implementation is compared against\nexisting Q# compile tools. Unlike the Microsoft implementation of the official\nQ# compile toolchain, our implementation is written in TypeScript in order to\nport functionality to web environments.", "AI": {"tldr": "The paper presents a TypeScript-based Q# to QASM 3.0 compiler toolchain, demonstrating its functionality on multiple Q# programs and compatibility with web environments, thus expanding application scenarios beyond the official Microsoft tools.", "motivation": "Existing Q# compile toolchains, such as Microsoft's official implementation, are not suitable for web environments and are not written in TypeScript. There is a need for a toolchain that can be used more flexibly, such as in the browser.", "method": "The authors implemented a compile toolchain from Q# to QASM 3.0. This included building a full-featured lexer, parser, and a compiler that supports a subset of Q# features. The solution was implemented in TypeScript, and its functionality was tested with various Q# programs and compared to existing Q# toolchains.", "result": "The implementation works with various input Q# programs and offers a functional alternative to the Microsoft Q# compile toolchain. It is fully written in TypeScript, making it suitable for web and browser-based environments.", "conclusion": "Porting the Q# compile toolchain to TypeScript enables use cases in the web environment, providing a functional compiler from Q# to QASM 3.0 and broadening accessibility beyond the Microsoft toolchain."}}
{"id": "2506.23014", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23014", "abs": "https://arxiv.org/abs/2506.23014", "authors": ["Wilder Baldwin", "Shashank Chintakuntla", "Shreyah Parajuli", "Ali Pourghasemi", "Ryan Shanz", "Sepideh Ghanavati"], "title": "Generating Privacy Stories From Software Documentation", "comment": "Accepted to RENext!'25 at the 33rd IEEE International Requirements\n  Engineering 2025 conference", "summary": "Research shows that analysts and developers consider privacy as a security\nconcept or as an afterthought, which may lead to non-compliance and violation\nof users' privacy. Most current approaches, however, focus on extracting legal\nrequirements from the regulations and evaluating the compliance of software and\nprocesses with them. In this paper, we develop a novel approach based on\nchain-of-thought prompting (CoT), in-context-learning (ICL), and Large Language\nModels (LLMs) to extract privacy behaviors from various software documents\nprior to and during software development, and then generate privacy\nrequirements in the format of user stories. Our results show that most commonly\nused LLMs, such as GPT-4o and Llama 3, can identify privacy behaviors and\ngenerate privacy user stories with F1 scores exceeding 0.8. We also show that\nthe performance of these models could be improved through parameter-tuning. Our\nfindings provide insight into using and optimizing LLMs for generating privacy\nrequirements given software documents created prior to or throughout the\nsoftware development lifecycle.", "AI": {"tldr": "The paper introduces a method using LLMs to extract privacy information from software documents and generate privacy user stories, achieving high accuracy and showing promise for integrating privacy into software development.", "motivation": "Privacy is often treated as a security issue or considered as an afterthought by analysts and developers, leading to non-compliant and privacy-violating software. Current methods focus mainly on legal compliance rather than embedding privacy requirements during development.", "method": "The paper proposes a new approach that uses chain-of-thought prompting, in-context learning, and Large Language Models (LLMs) like GPT-4o and Llama 3 to automatically extract privacy behaviors from software documents and generate privacy requirements in the form of user stories.", "result": "The approach allows popular LLMs to generate privacy user stories from software documents with F1 scores above 0.8. The study also shows that model performance can be further improved via parameter tuning.", "conclusion": "LLMs can reliably identify privacy behaviors and generate corresponding privacy user stories during software development. Optimizing these models can further enhance their effectiveness, offering a powerful tool for integrating privacy requirements early in the development lifecycle."}}
{"id": "2506.23034", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.23034", "abs": "https://arxiv.org/abs/2506.23034", "authors": ["Hao Yan", "Swapneel Suhas Vaidya", "Xiaokuan Zhang", "Ziyu Yao"], "title": "Guiding AI to Fix Its Own Flaws: An Empirical Study on LLM-Driven Secure Code Generation", "comment": null, "summary": "Large Language Models (LLMs) have become powerful tools for automated code\ngeneration. However, these models often overlook critical security practices,\nwhich can result in the generation of insecure code that contains\nvulnerabilities-weaknesses or flaws in the code that attackers can exploit to\ncompromise a system. However, there has been limited exploration of strategies\nto guide LLMs in generating secure code and a lack of in-depth analysis of the\neffectiveness of LLMs in repairing code containing vulnerabilities. In this\npaper, we present a comprehensive evaluation of state-of-the-art LLMs by\nexamining their inherent tendencies to produce insecure code, their capability\nto generate secure code when guided by self-generated vulnerability hints, and\ntheir effectiveness in repairing vulnerabilities when provided with different\nlevels of feedback. Our study covers both proprietary and open-weight models\nacross various scales and leverages established benchmarks to assess a wide\nrange of vulnerability types. Through quantitative and qualitative analyses, we\nreveal that although LLMs are prone to generating insecure code, advanced\nmodels can benefit from vulnerability hints and fine-grained feedback to avoid\nor fix vulnerabilities. We also provide actionable suggestions to developers to\nreduce vulnerabilities when using LLMs for code generation.", "AI": {"tldr": "LLMs are prone to producing insecure code, but their security improves noticeably when given vulnerability hints and detailed feedback. The paper offers insights and recommendations to help developers reduce code vulnerabilities when using LLMs.", "motivation": "Despite LLMs' success in automating code generation, they often overlook security, leading to vulnerable code. There is a lack of research on how to guide LLMs toward generating secure code or repairing insecure code, necessitating a thorough study of their security capabilities and improvement strategies.", "method": "The paper presents a comprehensive evaluation using established benchmarks, comparing both proprietary and open-weight LLMs at different scales, analyzing their code generation with and without vulnerability hints, and assessing their vulnerability repair effectiveness with varying feedback levels. Both quantitative and qualitative analyses are performed.", "result": "LLMs have a tendency to produce insecure code by default. However, state-of-the-art models demonstrate a clear ability to generate more secure code and fix vulnerabilities when provided with vulnerability hints and fine-grained feedback. The paper also offers practical recommendations for safer LLM-based code generation.", "conclusion": "LLMs often generate insecure code by default, but advanced models can improve their security with guidance such as vulnerability hints and detailed feedback. Developers can take practical steps to reduce vulnerabilities when relying on LLMs for code generation."}}
{"id": "2506.23281", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2506.23281", "abs": "https://arxiv.org/abs/2506.23281", "authors": ["Xintong Zhou", "Zhenyang Xu", "Chengnian Sun"], "title": "On the Feasibility of Deduplicating Compiler Bugs with Bisection", "comment": null, "summary": "Random testing has proven to be an effective technique for compiler\nvalidation. However, the debugging of bugs identified through random testing\npresents a significant challenge due to the frequent occurrence of duplicate\ntest programs that expose identical compiler bugs. The process to identify\nduplicates is a practical research problem known as bug deduplication. Prior\nmethodologies for compiler bug deduplication primarily rely on program analysis\nto extract bug-related features for duplicate identification, which can result\nin substantial computational overhead and limited generalizability. This paper\ninvestigates the feasibility of employing bisection, a standard debugging\nprocedure largely overlooked in prior research on compiler bug deduplication,\nfor this purpose. Our study demonstrates that the utilization of bisection to\nlocate failure-inducing commits provides a valuable criterion for\ndeduplication, albeit one that requires supplementary techniques for more\naccurate identification. Building on these results, we introduce BugLens, a\nnovel deduplication method that primarily uses bisection, enhanced by the\nidentification of bug-triggering optimizations to minimize false negatives.\nEmpirical evaluations conducted on four real-world datasets demonstrate that\nBugLens significantly outperforms the state-of-the-art analysis-based\nmethodologies Tamer and D3 by saving an average of 26.98% and 9.64% human\neffort to identify the same number of distinct bugs. Given the inherent\nsimplicity and generalizability of bisection, it presents a highly practical\nsolution for compiler bug deduplication in real-world applications.", "AI": {"tldr": "BugLens uses bisection to make compiler bug deduplication simpler and more effective, requiring less human effort than previous methods.", "motivation": "Random testing finds many compiler bugs, but debugging is hard because many test cases are duplicates of the same bug. Existing deduplication techniques are computationally expensive and lack generalizability.", "method": "The authors study using bisection\u2014a debugging technique\u2014to pinpoint failure-inducing commits as a criterion for bug deduplication. They develop BugLens, a tool that improves deduplication by combining bisection with the identification of bug-triggering optimizations.", "result": "BugLens was empirically evaluated on four real-world datasets and significantly outperformed two state-of-the-art methods (Tamer and D3), reducing human effort by 26.98% and 9.64% on average to deduplicate the same number of unique bugs.", "conclusion": "Bisection, due to its simplicity and generalizability, offers a practical alternative for compiler bug deduplication. BugLens leverages bisection with enhancements to effectively outperform analysis-based methods in reducing deduplication effort."}}
{"id": "2506.23063", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.23063", "abs": "https://arxiv.org/abs/2506.23063", "authors": ["Guangfa Lyu", "Zhenzhong Cao", "Xiaofei Ren", "Fengyu Wang"], "title": "HF-DGF: Hybrid Feedback Guided Directed Grey-box Fuzzing", "comment": null, "summary": "Directed Grey-box Fuzzing (DGF) has emerged as a widely adopted technique for\ncrash reproduction and patch testing, leveraging its capability to precisely\nnavigate toward target locations and exploit vulnerabilities. However, current\nDGF tools are constrained by insufficient runtime feedback, limiting their\nefficiency in reaching targets and exploring state spaces. This study presents\nHF-DGF, a novel directed grey-box fuzzing framework. Its seed scheduling is\nguided by a hybrid feedback mechanism integrating control-flow distance,\nvalue-flow influence score, and slice coverage. To enable precise control-flow\ndistance feedback, we propose a backward-stepping algorithm to calculate basic\nblock-level seed distances on a virtual inter-procedural control-flow graph\n(ICFG). For effective state space exploration, we introduce value-flow\ninfluence and a corresponding metric, the value-flow influence score.\nAdditionally, to mitigate runtime overhead from hybrid feedback, we adopt a\nnovel selective instrumentation strategy. Evaluations on 41 real-world\nvulnerabilities show HF-DGF outperforms existing tools: it achieves crash\nreproduction 5.05 times faster than AFL, 5.79 times faster than AFLGo, 73.75\ntimes faster than WindRanger, 2.56 times faster than DAFL, and 8.45 times\nfaster than Beacon on average. Notably, when all fuzzers triggered crashes,\nHF-DGF exhibited the lowest code coverage, demonstrating superior\ndirectionality and efficiency. It also surpasses AFLGo, WindRanger, DAFL, and\nBeacon in static analysis efficiency.", "AI": {"tldr": "HF-DGF is a new directed grey-box fuzzing framework that combines multiple feedback metrics for improved crash reproduction. It is significantly faster and more efficient than leading tools like AFL, AFLGo, and WindRanger, thanks to its hybrid feedback and selective instrumentation approach.", "motivation": "Directed Grey-box Fuzzing (DGF) faces limitations due to insufficient runtime feedback, reducing its effectiveness in crash reproduction and patch testing. There is a need for improved efficiency and target reachability in DGF tools.", "method": "HF-DGF, a new DGF framework, integrates three feedback signals: control-flow distance (using a backward-stepping algorithm on a virtual ICFG), value-flow influence score, and slice coverage. A hybrid feedback guides seed scheduling, and selective instrumentation reduces runtime overhead.", "result": "HF-DGF surpasses existing DGF tools on 41 real-world vulnerabilities, reproducing crashes significantly faster than AFL, AFLGo, WindRanger, DAFL, and Beacon. It achieved the lowest code coverage among fuzzers that found crashes, indicating greater directionality and efficiency, and outperformed competitors in static analysis efficiency.", "conclusion": "By integrating hybrid feedback mechanisms and novel analysis techniques, HF-DGF offers significant improvements in speed, efficiency, and directionality for crash reproduction and vulnerability detection over previous DGF tools."}}
{"id": "2506.23696", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2506.23696", "abs": "https://arxiv.org/abs/2506.23696", "authors": ["Francisco Oliveira", "Alexandra Mendes", "Carolina Carreira"], "title": "What Challenges Do Developers Face When Using Verification-Aware Programming Languages?", "comment": null, "summary": "Software reliability is critical in ensuring that the digital systems we\ndepend on function correctly. In software development, increasing software\nreliability often involves testing. However, for complex and critical systems,\ndevelopers can use Design by Contract (DbC) methods to define precise\nspecifications that software components must satisfy. Verification-Aware (VA)\nprogramming languages support DbC and formal verification at compile-time or\nrun-time, offering stronger correctness guarantees than traditional testing.\nHowever, despite the strong guarantees provided by VA languages, their adoption\nremains limited. In this study, we investigate the barriers to adopting VA\nlanguages by analyzing developer discussions on public forums using topic\nmodeling techniques. We complement this analysis with a developer survey to\nbetter understand the practical challenges associated with VA languages. Our\nfindings reveal key obstacles to adoption, including steep learning curves and\nusability issues. Based on these insights, we identify actionable\nrecommendations to improve the usability and accessibility of VA languages. Our\nfindings suggest that simplifying tool interfaces, providing better educational\nmaterials, and improving integration with everyday development environments\ncould improve the usability and adoption of these languages. Our work provides\nactionable insights for improving the usability of VA languages and making\nverification tools more accessible.", "AI": {"tldr": "The paper analyzes why Verification-Aware programming languages, which improve software correctness, are underused. By examining forum discussions and surveying developers, it finds that complexity, poor usability, and lack of good integration are main barriers. The authors recommend better interfaces, educational materials, and integration to improve adoption.", "motivation": "The motivation behind this paper is to address the limited adoption of Verification-Aware (VA) programming languages, which offer strong software reliability guarantees through Design by Contract (DbC) and formal verification, yet remain underutilized in practice.", "method": "The authors used a mixed-methods approach, analyzing developer discussions from public forums using topic modeling techniques and supplementing these findings with a survey of developers to identify barriers and challenges in adopting VA languages.", "result": "Key obstacles to VA language adoption identified include steep learning curves and usability issues. The study reveals that users struggle with complex interfaces, insufficient educational materials, and poor integration into development environments.", "conclusion": "By synthesizing findings from discussion analysis and surveys, the study concludes that improving tool interfaces, offering better educational resources, and enhancing integration with standard development environments can make VA languages more usable and accessible, potentially increasing their adoption."}}
{"id": "2506.23100", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.23100", "abs": "https://arxiv.org/abs/2506.23100", "authors": ["Jiayi Zhang", "Kai Huang", "Jian Zhang", "Yang Liu", "Chunyang Chen"], "title": "Repair Ingredients Are All You Need: Improving Large Language Model-Based Program Repair via Repair Ingredients Search", "comment": "Accepted by ICSE 2026. Jiayi Zhang and Kai Huang contributed equally\n  to this work", "summary": "Automated Program Repair (APR) techniques aim to automatically fix buggy\nprograms. Among these, Large Language Model-based (LLM-based) approaches have\nshown great promise. Recent advances demonstrate that directly leveraging LLMs\ncan achieve leading results. However, these techniques remain suboptimal in\ngenerating contextually relevant and accurate patches, as they often overlook\nrepair ingredients crucial for practical program repair. In this paper, we\npropose ReinFix, a novel framework that enables LLMs to autonomously search for\nrepair ingredients throughout both the reasoning and solution phases of bug\nfixing. In the reasoning phase, ReinFix integrates static analysis tools to\nretrieve internal ingredients, such as variable definitions, to assist the LLM\nin root cause analysis when it encounters difficulty understanding the context.\nDuring the solution phase, when the LLM lacks experience in fixing specific\nbugs, ReinFix searches for external ingredients from historical bug fixes with\nsimilar bug patterns, leveraging both the buggy code and its root cause to\nguide the LLM in identifying appropriate repair actions, thereby increasing the\nlikelihood of generating correct patches. Evaluations on two popular benchmarks\n(Defects4J V1.2 and V2.0) demonstrate the effectiveness of our approach over\nSOTA baselines. Notably, ReinFix fixes 146 bugs, which is 32 more than the\nbaselines on Defects4J V1.2. On Defects4J V2.0, ReinFix fixes 38 more bugs than\nthe SOTA. Importantly, when evaluating on the recent benchmarks that are free\nof data leakage risk, ReinFix also maintains the best performance.", "AI": {"tldr": "ReinFix improves automated program repair by helping LLMs find relevant code info and past fixes, outperforming previous methods on major benchmarks.", "motivation": "Existing LLM-based Automated Program Repair (APR) techniques show promise but often fail to generate contextually relevant and accurate patches due to the neglect of important repair ingredients.", "method": "ReinFix, a new framework, enables LLMs to search for both internal and external repair ingredients. In the reasoning phase, it uses static analysis tools to retrieve relevant code information. In the solution phase, it searches historical bug fixes with similar patterns to inform patch generation.", "result": "ReinFix outperforms state-of-the-art baselines, fixing 32 more bugs on Defects4J V1.2 and 38 more on Defects4J V2.0. It also maintains top performance on data leakage-free benchmarks.", "conclusion": "ReinFix effectively enhances LLM-based APR by integrating ingredient search mechanisms, providing greater contextual relevance and accuracy in automated bug fixing."}}
{"id": "2506.23234", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.23234", "abs": "https://arxiv.org/abs/2506.23234", "authors": ["Peerachai Banyongrakkul", "Mansooreh Zahedi", "Patanamon Thongtanunam", "Christoph Treude", "Haoyu Gao"], "title": "From Release to Adoption: Challenges in Reusing Pre-trained AI Models for Downstream Developers", "comment": "Recently accepted at ICSME 2025", "summary": "Pre-trained models (PTMs) have gained widespread popularity and achieved\nremarkable success across various fields, driven by their groundbreaking\nperformance and easy accessibility through hosting providers. However, the\nchallenges faced by downstream developers in reusing PTMs in software systems\nare less explored. To bridge this knowledge gap, we qualitatively created and\nanalyzed a dataset of 840 PTM-related issue reports from 31 OSS GitHub\nprojects. We systematically developed a comprehensive taxonomy of PTM-related\nchallenges that developers face in downstream projects. Our study identifies\nseven key categories of challenges that downstream developers face in reusing\nPTMs, such as model usage, model performance, and output quality. We also\ncompared our findings with existing taxonomies. Additionally, we conducted a\nresolution time analysis and, based on statistical tests, found that\nPTM-related issues take significantly longer to be resolved than issues\nunrelated to PTMs, with significant variation across challenge categories. We\ndiscuss the implications of our findings for practitioners and possibilities\nfor future research.", "AI": {"tldr": "The paper identifies and categorizes key challenges developers face in reusing pre-trained models in software projects, showing such issues are harder and slower to resolve compared to others. This highlights a need for better support and targeted research for PTM reuse.", "motivation": "Pre-trained models (PTMs) are widely adopted due to their strong performance and accessibility, but the software engineering challenges faced by downstream developers who integrate these models are not well understood.", "method": "The authors collected and qualitatively analyzed 840 PTM-related issue reports from 31 open-source GitHub projects, developed a taxonomy of PTM-related challenges, compared these with existing taxonomies, and analyzed issue resolution times using statistical tests.", "result": "Seven main categories of challenges were identified, including model usage, model performance, and output quality. PTM-related issues take significantly longer to resolve than non-PTM-related issues, with resolution time varying by category.", "conclusion": "There are systematic, under-explored challenges for downstream developers using PTMs, as evidenced by prolonged issue resolution and distinct challenge types. The findings suggest areas for improved support, guidance, and future research directions."}}
{"id": "2506.23534", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.23534", "abs": "https://arxiv.org/abs/2506.23534", "authors": ["Siyu Chen", "Jiongyi Yang", "Xiang Chen", "Menglin Zheng", "Minnan Wei", "Xiaolin Ju"], "title": "Improving vulnerability type prediction and line-level detection via adversarial training-based data augmentation and multi-task learning", "comment": null, "summary": "Context: Software vulnerabilities pose a significant threat to modern\nsoftware systems, as evidenced by the growing number of reported\nvulnerabilities and cyberattacks. These escalating trends underscore the urgent\nneed for effective approaches that can automatically detect and understand\nsoftware vulnerabilities. Objective: However, the scarcity of labeled samples\nand the class imbalance issue in vulnerability datasets present significant\nchallenges for both Vulnerability Type Prediction (VTP) and Line-level\nVulnerability Detection (LVD), especially for rare yet critical vulnerability\ntypes. Moreover, most existing studies treat VTP and LVD as independent tasks,\noverlooking their inherent correlation, which limits the potential to leverage\nshared semantic patterns across tasks. Methods: To address these limitations,\nwe propose a unified approach that integrates Embedding-Layer Driven\nAdversarial Training (EDAT) with Multi-task Learning (MTL). Specifically, EDAT\nenhances model robustness by introducing adversarial perturbations to\nidentifier embeddings, guided by semantic importance. Meanwhile, MTL improves\noverall performance by leveraging shared representations and inter-task\ncorrelations between VTP and LVD. Results: Extensive experiments demonstrate\nthat our proposed approach outperforms state-of-the-art baselines on both VTP\nand LVD tasks. For VTP, it yields notable improvements in accuracy, precision,\nrecall, and F1-score, particularly in identifying rare vulnerability types.\nSimilarly, for LVD, our approach enhances line-level detection accuracy while\nsignificantly reducing false positives. Conclusion: Our study demonstrates that\ncombining EDAT with MTL provides a unified solution that improves performance\non both tasks and warrants further investigation.", "AI": {"tldr": "This paper presents a unified adversarial multi-task learning method that significantly enhances both vulnerability type prediction and line-level detection in software security, especially for rare vulnerability types, and outperforms previous approaches.", "motivation": "Software vulnerabilities continue to rise, threatening modern systems and underscoring the need for effective automated solutions. The challenges addressed include scarcity of labeled samples, class imbalance in datasets, and the commonly neglected correlation between vulnerability type prediction (VTP) and line-level vulnerability detection (LVD).", "method": "The paper proposes a unified approach using Embedding-Layer Driven Adversarial Training (EDAT) combined with Multi-task Learning (MTL). EDAT introduces adversarial perturbations to identifier embeddings (guided by semantic importance) to improve robustness, while MTL leverages shared representations and inter-task correlations between VTP and LVD.", "result": "The proposed approach outperforms state-of-the-art baselines on both VTP and LVD. It boosts accuracy, precision, recall, and F1-score, especially for rare vulnerability types in VTP, and improves line-level detection accuracy with fewer false positives in LVD.", "conclusion": "Combining EDAT with MTL yields a unified, effective solution for both VTP and LVD, improving performance and highlighting the value of joint task modeling for future research."}}
{"id": "2506.23535", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.23535", "abs": "https://arxiv.org/abs/2506.23535", "authors": ["Malik Muhammad Umer"], "title": "Comparative Analysis of the Code Generated by Popular Large Language Models (LLMs) for MISRA C++ Compliance", "comment": null, "summary": "Safety-critical systems are engineered systems whose failure or malfunction\ncould result in catastrophic consequences. The software development for\nsafety-critical systems necessitates rigorous engineering practices and\nadherence to certification standards like DO-178C for avionics. DO-178C is a\nguidance document which requires compliance to well-defined software coding\nstandards like MISRA C++ to enforce coding guidelines that prevent the use of\nambiguous, unsafe, or undefined constructs. Large Language Models (LLMs) have\ndemonstrated significant capabilities in automatic code generation across a\nwide range of programming languages, including C++. Despite their impressive\nperformance, code generated by LLMs in safety-critical domains must be\ncarefully analyzed for conformance to MISRA C++ coding standards. In this\npaper, I have conducted a comparative analysis of the C++ code generated by\npopular LLMs including: OpenAI ChatGPT, Google Gemini, DeepSeek, Meta AI, and\nMicrosoft Copilot for compliance with MISRA C++.", "AI": {"tldr": "This paper checks how well code generated by various leading Large Language Models (LLMs) such as ChatGPT and Gemini complies with strict safety standards (MISRA C++) needed in domains like avionics. LLM-generated code is compared and assessed for adherence, underlining the importance of oversight in safety-critical applications.", "motivation": "Safety-critical systems, such as those in avionics, require highly reliable software due to their potential for catastrophic failures. With increasing interest in using Large Language Models (LLMs) for automatic code generation, it is important to examine if these AI-generated codes adhere to strict industry standards like MISRA C++.", "method": "The paper conducts a comparative analysis of C++ code generated by popular LLMs (OpenAI ChatGPT, Google Gemini, DeepSeek, Meta AI, and Microsoft Copilot) to determine their compliance with the MISRA C++ coding standards, which are essential for safety-critical software development.", "result": "The study evaluates and compares the extent to which output from different LLMs meets or violates the MISRA C++ coding guidelines. Specific results in terms of compliance rates or common violations are not included in the abstract.", "conclusion": "The analysis highlights the need for careful evaluation of LLM-generated code in the context of safety-critical systems, as adherence to established standards like MISRA C++ is crucial for certification and system reliability."}}
{"id": "2506.23644", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.23644", "abs": "https://arxiv.org/abs/2506.23644", "authors": ["Junze Hu", "Xiangyu Jin", "Yizhe Zeng", "Yuling Liu", "Yunpeng Li", "Dan Du", "Kaiyu Xie", "Hongsong Zhu"], "title": "QLPro: Automated Code Vulnerability Discovery via LLM and Static Code Analysis Integration", "comment": null, "summary": "We introduce QLPro, a vulnerability detection framework that systematically\nintegrates LLMs and static analysis tools to enable comprehensive vulnerability\ndetection across entire open-source projects.We constructed a new dataset,\nJavaTest, comprising 10 open-source projects from GitHub with 62 confirmed\nvulnerabilities. CodeQL, a state-of-the-art static analysis tool, detected only\n24 of these vulnerabilities while QLPro detected 41. Furthermore, QLPro\ndiscovered 6 previously unknown vulnerabilities, 2 of which have been confirmed\nas 0-days.", "AI": {"tldr": "QLPro combines LLMs with static analysis, outperforming CodeQL in vulnerability detection and discovering new 0-days in open-source Java code.", "motivation": "Existing static analysis tools like CodeQL miss a significant number of vulnerabilities in open-source projects.", "method": "QLPro systematically integrates Large Language Models (LLMs) with static analysis for broader vulnerability detection. The framework is evaluated using JavaTest, a dataset with 10 open-source projects and 62 known vulnerabilities.", "result": "QLPro detected 41 vulnerabilities compared to CodeQL's 24, and found 6 previously unknown vulnerabilities (including 2 confirmed 0-days).", "conclusion": "QLPro significantly improves vulnerability detection rates over current state-of-the-art tools and can uncover previously unknown security issues."}}
{"id": "2506.23715", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.23715", "abs": "https://arxiv.org/abs/2506.23715", "authors": ["Benoit Combemale"], "title": "Towards a Science of Developer eXperience (DevX)", "comment": null, "summary": "As software continues to permeate nearly every facet of modern life, the\ncomplexity and ubiquity of digital services underscore the need for\nsustainable, effective, and inclusive software development practices. Although\nsoftware engineering has made significant progress in technical challenges\nsince its inception, the human experience of those involved in software\ncreation, broadly defined as developers, remains underexplored. This column\nadvocates for the formal recognition of Developer eXperience (DevX) as a\ndistinct research field. We argue that DevX profoundly influences critical\ndevelopment activities and overall productivity, especially as development\nbecomes increasingly collaborative and diverse in terms of application domains.\nBuilding on existing efforts to measure and enhance DevX, we identify key\nrationales, scientific enablers, and interdisciplinary intersections that\nsupport this emerging discipline. We also outline the core scientific\nchallenges ahead, aiming to call for actions from the research community and to\npromote more human-centered approaches to software engineering.", "AI": {"tldr": "The paper argues for establishing Developer eXperience (DevX) as a dedicated research field in software engineering, contending that focusing on developers\u2019 human experiences is vital for sustainable and productive software development. It highlights motivations, enabling factors, challenges, and calls for more research and human-centered practices.", "motivation": "Software plays an increasingly crucial role in everyday life, making sustainable, effective, and inclusive development practices more important than ever. However, the personal and collaborative experiences of developers (Developer eXperience, or DevX) remain underexplored, despite significant advances in technical aspects of software engineering.", "method": "The paper advocates for recognizing DevX as its own research field. It synthesizes existing efforts to measure and improve DevX, identifies rationales and enabling factors, explores interdisciplinary connections, and outlines unresolved scientific challenges. This is a conceptual, analytical, and agenda-setting paper rather than an empirical study.", "result": "The authors identify key rationales for focusing on DevX, describe the interdisciplinary nature of the field, and highlight scientific challenges. They call on the research community to address these challenges and adopt more human-centered approaches in software engineering.", "conclusion": "DevX should be formally recognized as a distinct research field because it critically affects development effectiveness and productivity. Focusing on DevX can drive more sustainable, inclusive, and human-centered software engineering practices."}}
{"id": "2506.23749", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.23749", "abs": "https://arxiv.org/abs/2506.23749", "authors": ["Boyang Yang", "Zijian Cai", "Fengling Liu", "Bach Le", "Lingming Zhang", "Tegawend\u00e9 F. Bissyand\u00e9", "Yang Liu", "Haoye Tian"], "title": "A Survey of LLM-based Automated Program Repair: Taxonomies, Design Paradigms, and Applications", "comment": null, "summary": "Large language models (LLMs) are reshaping automated program repair (APR). We\ncategorize the recent 63 LLM-based APR systems published from January 2022 to\nJune 2025 into four paradigms, and show how retrieval- or analysis-augmented\ncontexts strengthen any of them. This taxonomy clarifies key trade-offs:\nfine-tuning delivers strong task alignment at high training cost; prompting\nenables rapid deployment but is limited by prompt design and context windows;\nprocedural pipelines offer reproducible control with moderate overhead; agentic\nframeworks tackle multi-hunk or cross-file bugs at the price of increased\nlatency and complexity. Persistent challenges include verifying semantic\ncorrectness beyond test suites, repairing repository-scale defects, and\nlowering the costs of LLMs. We outline research directions that combine\nlightweight human feedback, repository-aware retrieval, code analysis, and\ncost-aware planning to advance reliable and efficient LLM-based APR.", "AI": {"tldr": "The paper categorizes 63 recent LLM-based program repair systems into four paradigms, analyzing their trade-offs and challenges. It highlights the importance of context augmentation and suggests future research directions for more reliable, efficient, and cost-effective LLM-powered repair solutions.", "motivation": "Large language models (LLMs) are being increasingly used for automated program repair (APR), but there is a need to categorize and understand recent advancements and persisting challenges in this field.", "method": "The authors surveyed and categorized 63 LLM-based APR systems published between January 2022 and June 2025 into four paradigms. They analyzed the strengths, limitations, and trade-offs of each approach, focusing on the impact of retrieval- or analysis-augmented contexts.", "result": "They identified four main paradigms: fine-tuning, prompting, procedural pipelines, and agentic frameworks, each with distinct benefits and drawbacks. Augmented contexts enhance the performance of any paradigm. Key challenges remain, such as ensuring semantic correctness, addressing large-scale defects, and minimizing costs.", "conclusion": "The taxonomy clarifies important trade-offs in LLM-based APR. The paper highlights persistent challenges and suggests directions for future research, including human feedback integration, smarter retrieval, code analysis, and cost-effective strategies."}}
{"id": "2506.23762", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.23762", "abs": "https://arxiv.org/abs/2506.23762", "authors": ["Hongzhou Rao", "Yanjie Zhao", "Xinyi Hou", "Shenao Wang", "Haoyu Wang"], "title": "Software Engineering for Large Language Models: Research Status, Challenges and the Road Ahead", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has redefined\nartificial intelligence (AI), pushing the boundaries of AI research and\nenabling unbounded possibilities for both academia and the industry. However,\nLLM development faces increasingly complex challenges throughout its lifecycle,\nyet no existing research systematically explores these challenges and solutions\nfrom the perspective of software engineering (SE) approaches. To fill the gap,\nwe systematically analyze research status throughout the LLM development\nlifecycle, divided into six phases: requirements engineering, dataset\nconstruction, model development and enhancement, testing and evaluation,\ndeployment and operations, and maintenance and evolution. We then conclude by\nidentifying the key challenges for each phase and presenting potential research\ndirections to address these challenges. In general, we provide valuable\ninsights from an SE perspective to facilitate future advances in LLM\ndevelopment.", "AI": {"tldr": "This paper systematically reviews the challenges in large language model (LLM) development using software engineering concepts, mapping out the key issues and research directions across the lifecycle from requirements to maintenance.", "motivation": "Large language models (LLMs) are rapidly advancing, but the complexity of their development lifecycle presents significant challenges that have not been systematically studied from a software engineering (SE) perspective. There is a lack of comprehensive frameworks addressing these issues across all stages of LLM development.", "method": "The authors conduct a systematic analysis of the LLM development lifecycle, breaking it down into six phases: requirements engineering, dataset construction, model development and enhancement, testing and evaluation, deployment and operations, and maintenance and evolution. For each phase, they identify current research statuses, key challenges, and propose potential research directions.", "result": "The paper presents a structured overview of LLM development from the SE perspective, highlighting the main challenges in each phase and suggesting potential research opportunities to address them. This approach provides a valuable framework for understanding and improving LLM development processes.", "conclusion": "By applying an SE lens to the LLM lifecycle, this research fills an existing gap, providing systematic insights and directions that could facilitate more robust and successful large language model development in the future."}}
{"id": "2506.23898", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.23898", "abs": "https://arxiv.org/abs/2506.23898", "authors": ["Diogo Lemos", "Ademar Aguiar", "Neil B. Harrison"], "title": "Requirements for Active Assistance of Natural Questions in Software Architecture", "comment": null, "summary": "Natural questions are crucial to shaping key architectural decisions and\npreserving architectural knowledge. They arise organically during the\narchitectural design process, often resulting from the existing architectural\nexperience of the designer and the distinctive characteristics of the system\nbeing designed. However, natural questions are often mismanaged or ignored,\nwhich can lead to architectural drift, knowledge loss, inefficient resource\nuse, or poor understandability of the system's architecture. We aim to better\nunderstand the lifecycle of natural questions, its key requirements, challenges\nand difficulties, and then to envision an assisted environment to properly\nsupport it. The environment should be adaptable and responsive to real-world\nconstraints and uncertainties by seamlessly integrating knowledge management\ntools and artificial intelligence techniques into software development\nworkflows. Based on existing literature, a requirements workshop, and three\ndesign iterations, we proposed a lifecycle for natural questions and elicited\nessential functional and non-functional requirements for such an environment.\nAt last, the results of a survey conducted with experts helped to analyze and\nvalidate the elicited requirements and proposed features for the environment to\nenhance collaboration, decision-making, and the preservation of architectural\nknowledge more effectively than conventional methods.", "AI": {"tldr": "Mismanagement of natural questions during software architecture design can lead to knowledge loss and inefficiencies. This paper proposes a lifecycle and environment supported by AI and knowledge tools, validated by expert feedback, to better manage these questions and enhance collaboration and knowledge preservation.", "motivation": "Natural questions are vital to the architectural design process, influencing key decisions and preserving knowledge. However, these questions are often mismanaged or ignored, which can result in architectural drift, inefficiency, and knowledge loss. The motivation is to improve the handling of these questions to enhance architecture design and knowledge retention.", "method": "The authors conducted a literature review, a requirements workshop, and three design iterations to propose a lifecycle for natural questions. They elicited both functional and non-functional requirements for an assisted environment. Finally, they validated the proposed requirements and features through a survey with experts.", "result": "A lifecycle model for natural questions was proposed. Essential requirements for an assisted environment were elicited, focusing on adaptability, seamless integration of knowledge management, and AI techniques. Survey results from experts validated and analyzed these requirements and proposed features, showing improved collaboration, decision-making, and knowledge preservation.", "conclusion": "Addressing and managing natural questions in architectural workflows is necessary to avoid knowledge loss and poor decision-making. The proposed environment and lifecycle model, validated by expert surveys, offer a viable way to enhance architectural knowledge, collaboration, and system understanding beyond current conventional methods."}}
{"id": "2506.23967", "categories": ["cs.SE", "cs.CY", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.23967", "abs": "https://arxiv.org/abs/2506.23967", "authors": ["Geerd-Dietger Hoffmann", "Verena Majuntke"], "title": "Green Metrics Tool: Measuring for fun and profit", "comment": null, "summary": "The environmental impact of software is gaining increasing attention as the\ndemand for computational resources continues to rise. In order to optimize\nsoftware resource consumption and reduce carbon emissions, measuring and\nevaluating software is a first essential step. In this paper we discuss what\nmetrics are important for fact base decision making. We introduce the Green\nMetrics Tool (GMT), a novel framework for accurately measuring the resource\nconsumption of software. The tool provides a containerized, controlled, and\nreproducible life cycle-based approach, assessing the resource use of software\nduring key phases. Finally, we discuss GMT features like visualization,\ncomparability and rule- and LLM-based optimisations highlighting its potential\nto guide developers and researchers in reducing the environmental impact of\ntheir software.", "AI": {"tldr": "The paper introduces GMT, a tool for accurately measuring and optimizing software's environmental impact, enabling informed efforts to reduce resource consumption and carbon emissions.", "motivation": "As software usage and computational demands increase, so does their environmental impact, especially in terms of resource consumption and carbon emissions. There's a growing need to accurately measure and optimize software's environmental footprint.", "method": "The authors introduce the Green Metrics Tool (GMT), a framework that uses a containerized, controlled, and reproducible life cycle-based method to measure software resource consumption during key phases. The tool also offers features such as visualization, comparability, and rule-/LLM-based optimizations.", "result": "GMT provides accurate measurement of resource use, aids in fact-based decision making, and offers optimization recommendations to minimize the software's environmental impact.", "conclusion": "The GMT framework enables developers and researchers to better assess, compare, and optimize the environmental performance of software, supporting efforts to reduce carbon emissions and resource consumption."}}
{"id": "2506.23995", "categories": ["cs.SE", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.23995", "abs": "https://arxiv.org/abs/2506.23995", "authors": ["Mingfei Cheng", "Renzhi Wang", "Xiaofei Xie", "Yuan Zhou", "Lei Ma"], "title": "STCLocker: Deadlock Avoidance Testing for Autonomous Driving Systems", "comment": null, "summary": "Autonomous Driving System (ADS) testing is essential to ensure the safety and\nreliability of autonomous vehicles (AVs) before deployment. However, existing\ntechniques primarily focus on evaluating ADS functionalities in single-AV\nsettings. As ADSs are increasingly deployed in multi-AV traffic, it becomes\ncrucial to assess their cooperative performance, particularly regarding\ndeadlocks, a fundamental coordination failure in which multiple AVs enter a\ncircular waiting state indefinitely, resulting in motion planning failures.\nDespite its importance, the cooperative capability of ADSs to prevent deadlocks\nremains insufficiently underexplored. To address this gap, we propose the first\ndedicated Spatio-Temporal Conflict-Guided Deadlock Avoidance Testing technique,\nSTCLocker, for generating DeadLock Scenarios (DLSs), where a group of AVs\ncontrolled by the ADS under test are in a circular wait state. STCLocker\nconsists of three key components: Deadlock Oracle, Conflict Feedback, and\nConflict-aware Scenario Generation. Deadlock Oracle provides a reliable\nblack-box mechanism for detecting deadlock cycles among multiple AVs within a\ngiven scenario. Conflict Feedback and Conflict-aware Scenario Generation\ncollaborate to actively guide AVs into simultaneous competition over spatial\nconflict resources (i.e., shared passing regions) and temporal competitive\nbehaviors (i.e., reaching the conflict region at the same time), thereby\nincreasing the effectiveness of generating conflict-prone deadlocks. We\nevaluate STCLocker on two types of ADSs: Roach, an end-to-end ADS, and OpenCDA,\na module-based ADS supporting cooperative communication. Experimental results\nshow that, on average, STCLocker generates more DLS than the best-performing\nbaseline.", "AI": {"tldr": "STCLocker is a new testing method that actively creates and detects deadlock scenarios among multiple autonomous vehicles, outperforming existing methods in identifying coordination failures.", "motivation": "Autonomous vehicles are increasingly operating in multi-AV environments where cooperation is critical. Deadlocks, where AVs enter a circular wait state and cannot proceed, represent a significant coordination failure that is insufficiently explored in current testing procedures focused on single-AV performance.", "method": "The authors propose STCLocker, a novel testing technique specifically designed for deadlock avoidance testing in multi-AV settings. It includes three components: Deadlock Oracle for detecting deadlocks, Conflict Feedback for guiding AVs towards competitive interactions, and Conflict-aware Scenario Generation for creating scenarios that stimulate spatial and temporal conflicts among AVs.", "result": "STCLocker was evaluated on two ADSs\u2014Roach and OpenCDA\u2014and was found to generate more deadlock scenarios than the strongest existing alternative technique.", "conclusion": "STCLocker is a highly effective tool for uncovering and testing deadlock-prone situations in multi-AV autonomous driving systems, outperforming current baselines in generating challenging deadlock scenarios."}}
{"id": "2506.24015", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.24015", "abs": "https://arxiv.org/abs/2506.24015", "authors": ["Ramtin Ehsani", "Esteban Parra", "Sonia Haiduc", "Preetha Chatterjee"], "title": "Bug Fixing with Broader Context: Enhancing LLM-Based Program Repair via Layered Knowledge Injection", "comment": null, "summary": "Prompting LLMs with bug-related context (e.g., error messages, stack traces)\nimproves automated program repair, but many bugs still remain unresolved. In\nreal-world projects, developers often rely on broader repository and\nproject-level context beyond the local code to resolve such bugs. In this\npaper, we investigate how automatically extracting and providing such knowledge\ncan improve LLM-based program repair. We propose a layered knowledge injection\nframework that incrementally augments LLMs with structured context. It starts\nwith the Bug Knowledge Layer, which includes information such as the buggy\nfunction and failing tests; expands to the Repository Knowledge Layer, which\nadds structural dependencies, related files, and commit history; and finally\ninjects the Project Knowledge Layer, which incorporates relevant details from\ndocumentation and previously fixed bugs. We evaluate this framework on a\ndataset of 314 bugs from BugsInPy using two LLMs (Llama 3.3 and GPT-4o-mini),\nand analyze fix rates across six bug types. By progressively injecting\nknowledge across layers, our approach achieves a fix rate of 79% (250/314)\nusing Llama 3.3, a significant improvement of 23% over previous work. All bug\ntypes show improvement with the addition of repository-level context, while\nonly a subset benefit further from project-level knowledge, highlighting that\ndifferent bug types require different levels of contextual information for\neffective repair. We also analyze the remaining unresolved bugs and find that\nmore complex and structurally isolated bugs, such as Program Anomaly and GUI\nbugs, remain difficult even after injecting all available information. Our\nresults show that layered context injection improves program repair and suggest\nthe need for interactive and adaptive APR systems.", "AI": {"tldr": "Providing LLMs with incrementally more powerful context\u2014starting from local bug data and extending to repository and project knowledge\u2014substantially increases automated program repair success rates, though some complex bugs still resist resolution. The method suggests future APR systems should be adaptive and interactive.", "motivation": "Many software bugs remain unresolved by automated program repair (APR) using LLMs, even when provided with local bug-related context. In practice, developers use broader project and repository-level knowledge to fix bugs. The paper investigates how systematically extracting and injecting such hierarchical knowledge into LLMs can enhance APR effectiveness.", "method": "The authors propose a layered knowledge injection framework for LLM-based program repair. The framework incrementally provides LLMs with (1) bug-specific knowledge, (2) repository-level context (e.g., dependencies, related files, commit history), and (3) project-level knowledge (documentation, previously fixed bugs). They evaluate this approach on 314 real bugs from BugsInPy using Llama 3.3 and GPT-4o-mini, measuring fix rates across six different bug types.", "result": "The layered knowledge injection framework boosts the program repair fix rate to 79% (250 out of 314) with Llama 3.3, which is a 23% improvement over prior work. Most bug types benefit from repository-level context, though only some see further enhancements with project-level knowledge. However, complex and structurally isolated bugs (e.g., Program Anomaly, GUI bugs) are still challenging to fix, even with all available knowledge.", "conclusion": "Injecting structured, multi-layered contextual knowledge (bug, repository, project) significantly improves automated program repair with LLMs, but certain complex bug types remain unresolved. The results highlight the value of adaptive and interactive repair systems tailored to the context needs of different bug categories."}}
