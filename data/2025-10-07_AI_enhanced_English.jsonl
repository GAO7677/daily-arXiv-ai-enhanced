{"id": "2510.03415", "categories": ["cs.PL", "cs.AI", "cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03415", "abs": "https://arxiv.org/abs/2510.03415", "authors": ["Aditya Thimmaiah", "Jiyang Zhang", "Jayanth Srinivasa", "Junyi Jessy Li", "Milos Gligoric"], "title": "PLSEMANTICSBENCH: Large Language Models As Programming Language Interpreters", "comment": null, "summary": "As large language models (LLMs) excel at code reasoning, a natural question\narises: can an LLM execute programs (i.e., act as an interpreter) purely based\non a programming language's formal semantics? If so, it will enable rapid\nprototyping of new programming languages and language features. We study this\nquestion using the imperative language IMP (a subset of C), formalized via\nsmall-step operational semantics (SOS) and rewriting-based operational\nsemantics (K-semantics). We introduce three evaluation sets-Human-Written,\nLLM-Translated, and Fuzzer- Generated-whose difficulty is controlled by\ncode-complexity metrics spanning the size, control-flow, and data-flow axes.\nGiven a program and its semantics formalized with SOS/K-semantics, models are\nevaluated on three tasks ranging from coarse to fine: (1) final-state\nprediction, (2) semantic rule prediction, and (3) execution trace prediction.\nTo distinguish pretraining memorization from semantic competence, we define two\nnonstandard semantics obtained through systematic mutations of the standard\nrules. Across strong code/reasoning LLMs, performance drops under nonstandard\nsemantics despite high performance under the standard one. We further find that\n(i) there are patterns to different model failures, (ii) most reasoning models\nperform exceptionally well on coarse grained tasks involving reasoning about\nhighly complex programs often containing nested loop depths beyond five, and\nsurprisingly, (iii) providing formal semantics helps on simple programs but\noften hurts on more complex ones. Overall, the results show a promise that LLMs\ncould serve as programming language interpreters, but points to the lack of\ntheir robust semantics understanding. We release the benchmark and the\nsupporting code at https://github.com/EngineeringSoftware/PLSemanticsBench.", "AI": {"tldr": "This paper investigates whether LLMs can execute code purely from formal semantics, using the IMP language and various complexity benchmarks. LLMs perform well on standard semantics tasks, struggle with nonstandard ones, and face difficulties with complex semantic reasoning, highlighting opportunities and current limitations for using LLMs as language interpreters.", "motivation": "Large language models excel at code reasoning, raising the question of whether they can act as interpreters for programs based solely on formal programming language semantics, aiding rapid prototyping of new languages.", "method": "The authors use the imperative language IMP, formalized via small-step operational semantics (SOS) and rewriting-based operational semantics (K-semantics). They create three evaluation sets of varying code complexity and assess model performance on tasks including final-state prediction, semantic rule prediction, and execution trace prediction. Nonstandard semantics are introduced via systematic rule mutations to separate memorization from true semantic competence.", "result": "Strong LLMs show high performance on standard semantics but their performance drops on nonstandard semantics, indicating limited robust semantic understanding. Patterns in failures are identified, and providing formal semantics aids simple programs but hinders complex ones.", "conclusion": "LLMs show promise as programming language interpreters with formal semantics, but currently lack robust, generalizable understanding of programming language semantics. The authors provide a benchmark suite for further research."}}
{"id": "2510.04049", "categories": ["cs.PL", "03B70, 68T27, 68T30"], "pdf": "https://arxiv.org/pdf/2510.04049", "abs": "https://arxiv.org/abs/2510.04049", "authors": ["Xiangyu Guo", "Ajay Bansal"], "title": "Encoding Numeric Computations and Infusing Heuristic Knowledge Using Integrity Constraints in stableKanren", "comment": "12 pages, 2 figures, ICFP '25 The miniKanren and Relational\n  Programming Workshop", "summary": "This paper presents examples of using integrity constraints in stableKanren\nto encode numeric computations for problem solving. Then, we use one of the\nexamples to introduce multiple ways to infuse heuristic knowledge and reduce\nsolving time. stableKanren is an extension of miniKanren that supports normal\nlogic programs under stable model semantics. stableKanren further supports\nnumeric computation by constructing a constraint store for integrity\nconstraints. There are three ways to extend a relational programming language\nwith numeric computations: relational number representation, grounding numbers\nto symbols, and constraint store construction. We demonstrate that the numeric\ncomputations in stableKanren have a straightforward numerical representation\ncompared to relational number representations. More importantly, stableKanren\nbalances symbolic and numeric computation in relational programming by avoiding\nthe grounding of all numbers to symbols. Lastly, it also has simpler syntax\ncompared to other constraint store construction approaches. stableKanren\nsupports combinatorial search problem solving under a declarative generate and\ntest paradigm. Such a paradigm generates all possible combinations of solutions\nto the problem, then applies a set of constraints to prune out the unwanted\nsolutions. We demonstrate that different approaches to writing programs or\nqueries affect the solver's performance in the SEND+MORE=MONEY puzzle. The\nperformance gradually improves as more heuristic knowledge is infused through\nthe programs or queries. Additionally, we show how to use an external function\nto achieve a hybrid solution.", "AI": {"tldr": "This paper shows that stableKanren supports efficient and simple numeric computations in logic programming through integrity constraints. By adjusting program/query design and adding heuristics, solver performance is greatly improved in combinatorial tasks like SEND+MORE=MONEY.", "motivation": "Integrating numeric computation in logic programming languages such as miniKanren is challenging and often leads to complex representations or inefficient computations. There is a need for a logic programming system that supports both symbolic and numeric computation efficiently.", "method": "The paper extends stableKanren (an extension of miniKanren supporting stable model semantics) to encode numeric computations using integrity constraints. It compares three approaches for supporting numeric computation\u2014relational number representation, grounding numbers to symbols, and constraint store construction\u2014and demonstrates their impact. The authors use combinatorial search problems (like SEND+MORE=MONEY) to show various ways to infuse heuristic knowledge and optimize solver performance. External hybrid solutions are also briefly discussed.", "result": "stableKanren allows for a more straightforward numerical representation than standard relational representations and avoids full grounding of numbers as symbols, balancing symbolic and numeric computation. Its syntax for expressing constraints is simpler compared to other methods. Experiments show that infusing heuristic knowledge into declarative queries and programs significantly improves the performance of combinatorial problem solvers. Using external functions for hybrid solutions is also feasible.", "conclusion": "stableKanren enhances relational programming by providing efficient, straightforward support for numeric computations with simple syntax, balancing symbolic and numeric approaches, and allowing performance improvements through heuristics infusion and hybridization."}}
{"id": "2510.04890", "categories": ["cs.PL", "cs.AR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04890", "abs": "https://arxiv.org/abs/2510.04890", "authors": ["Shihan Fang", "Wenxin Zheng"], "title": "Retrofitting Control Flow Graphs in LLVM IR for Auto Vectorization", "comment": null, "summary": "Modern processors increasingly rely on SIMD instruction sets, such as AVX and\nRVV, to significantly enhance parallelism and computational performance.\nHowever, production-ready compilers like LLVM and GCC often fail to fully\nexploit available vectorization opportunities due to disjoint vectorization\npasses and limited extensibility. Although recent attempts in heuristics and\nintermediate representation (IR) designs have attempted to address these\nproblems, efficiently simplifying control flow analysis and accurately\nidentifying vectorization opportunities remain challenging tasks.\n  To address these issues, we introduce a novel vectorization pipeline\nfeaturing two specialized IR extensions: SIR, which encodes high-level\nstructural information, and VIR, which explicitly represents instruction\ndependencies through data dependency analysis. Leveraging the detailed\ndependency information provided by VIR, we develop a flexible and extensible\nvectorization framework. This approach substantially improves interoperability\nacross vectorization passes and expands the search space for identifying\nisomorphic instructions, ultimately enhancing both the scope and efficiency of\nautomatic vectorization. Experimental evaluations demonstrate that our proposed\nvectorization pipeline achieves significant performance improvements,\ndelivering speedups of up to 53% and 58% compared to LLVM and GCC,\nrespectively.", "AI": {"tldr": "This paper presents a new vectorization framework with specialized IR extensions that boost SIMD utilization, making compiler vectorization more effective and delivering large performance gains over existing solutions like LLVM and GCC.", "motivation": "Current compilers like LLVM and GCC are limited in their ability to fully leverage SIMD instruction sets for parallelism, due to fragmented vectorization passes and insufficient extensibility. Simplifying control flow analysis and precisely finding vectorization opportunities remain challenging, motivating the need for a better approach.", "method": "The paper introduces a novel vectorization pipeline with two specialized intermediate representation (IR) extensions: SIR (for encoding high-level structural information) and VIR (for representing instruction dependencies via data dependency analysis). These extensions are used to create a flexible, extensible framework that integrates improved vectorization passes and enables accurate identification of vectorization opportunities.", "result": "The proposed pipeline demonstrates significant performance improvements, achieving speedups of up to 53% and 58% over LLVM and GCC, respectively, according to experimental evaluations.", "conclusion": "By extending compiler infrastructure with SIR and VIR IR formats, the proposed pipeline enhances analysis and prediction of vectorization opportunities, improves pass interoperability, and achieves considerable computational speedups. This work demonstrates the potential for significant impact on compiler design and high-performance software execution."}}
{"id": "2510.04994", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.04994", "abs": "https://arxiv.org/abs/2510.04994", "authors": ["Sjoerd Dost"], "title": "concurrentKanren: miniKanren for parallel execution", "comment": "13 pages, 1 figure, for associated repo see\n  https://github.com/deosjr/concurrentKanren", "summary": "Concurrent logic programming predates miniKanren, but concurrent\nimplementations of miniKanren have remained largely unexplored. In this work we\npresent a parallel implementation of miniKanren in Go, demonstrating its\nfeasibility and potential for performance improvements. Our approach leverages\nimplicit parallelism allowing legacy programs to benefit from parallel\nexecution. We discuss implementation strategies and evaluate the impact of\nparallelism, laying groundwork for future language-agnostic models.", "AI": {"tldr": "This paper demonstrates a parallel version of miniKanren in Go, showing it's feasible, can speed up execution via implicit parallelism, and paves the way for future work in language-independent parallel logic programming models.", "motivation": "Concurrent logic programming is established, but concurrent miniKanren implementations are rare. The motivation is to explore parallel miniKanren to improve performance and utilize parallelism, especially for legacy programs.", "method": "The authors present a parallel implementation of miniKanren using the Go programming language. They utilize implicit parallelism to run logic programs concurrently, and discuss various implementation strategies through evaluation.", "result": "They demonstrate that parallel execution of miniKanren is feasible and shows potential for improved performance. The exploration sets the stage for more generic, language-independent models of parallelism in logic programming.", "conclusion": "The study successfully presents and validates a parallel miniKanren in Go, highlighting its feasibility and performance benefits, and suggesting that this approach can inform more general models for parallel logic programming."}}
{"id": "2510.03461", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03461", "abs": "https://arxiv.org/abs/2510.03461", "authors": ["Sanjay Malakar", "Michael D. Ernst", "Martin Kellogg", "Manu Sridharan"], "title": "Repairing Leaks in Resource Wrappers", "comment": null, "summary": "A resource leak occurs when a program fails to release a finite resource like\na socket, file descriptor or database connection. While sound static analysis\ntools can detect all leaks, automatically repairing them remains challenging.\nPrior work took the output of a detection tool and attempted to repair only\nleaks from a hard-coded list of library resource types. That approach limits\nthe scope of repairable leaks: real-world code uses resource wrappers that\nstore a resource in a field and must themselves be closed. This paper makes\nfour key contributions to improve resource leak repair in the presence of\nwrappers. (1) It integrates inference of resource management specifications\ninto the repair pipeline, enabling extant fixing approaches to reason about\nwrappers. (2) It transforms programs into variants that are easier to analyze,\nmaking inference, detection, and fixing tools more effective; for instance, it\nmakes detection tools report problems closer to the root cause, often in a\nclient of a resource wrapper rather than within the wrapper class itself. (3) A\nnovel field containment analysis reasons about resource lifetimes, enabling\nrepair of more leaks involving resources stored in fields. (4) It introduces a\nnew repair pattern and more precise reasoning to better handle resources stored\nin non-final fields. Prior work fixed 41% of resource leak warnings in the NJR\nbenchmark suite; our implementation Arodnap fixes 68%.", "AI": {"tldr": "This paper introduces a robust method for automatically repairing resource leaks, even in complex cases involving resource wrappers. By advancing program analysis and repair strategies, their tool (Arodnap) fixes 68% of leaks compared to 41% by prior methods.", "motivation": "Automatically repairing resource leaks is challenging, especially for resource types not found in hard-coded lists, such as resource wrappers. Existing approaches have a limited scope due to their inability to handle wrappers that require custom closing mechanisms.", "method": "This paper presents several improvements: (1) integrating resource management specification inference into the repair process, (2) transforming code to aid analysis, (3) introducing field containment analysis to track resource lifetimes, and (4) proposing a new repair pattern for non-final field resources.", "result": "The proposed implementation, Arodnap, increases the percentage of automatically fixed resource leak warnings from 41% to 68% on the NJR benchmark suite, significantly improving over prior state-of-the-art.", "conclusion": "By enhancing the repair pipeline to reason about resource wrappers and fields, and by providing more sophisticated program analysis and repair patterns, the approach substantially increases the range and effectiveness of automatic resource leak repairs."}}
{"id": "2510.03463", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03463", "abs": "https://arxiv.org/abs/2510.03463", "authors": ["Vali Tawosi", "Keshav Ramani", "Salwa Alamir", "Xiaomo Liu"], "title": "ALMAS: an Autonomous LLM-based Multi-Agent Software Engineering Framework", "comment": null, "summary": "Multi-agent Large Language Model (LLM) systems have been leading the way in\napplied LLM research across a number of fields. One notable area is software\ndevelopment, where researchers have advanced the automation of code\nimplementation, code testing, code maintenance, inter alia, using LLM agents.\nHowever, software development is a multifaceted environment that extends beyond\njust code. As such, a successful LLM system must factor in multiple stages of\nthe software development life-cycle (SDLC). In this paper, we propose a vision\nfor ALMAS, an Autonomous LLM-based Multi-Agent Software Engineering framework,\nwhich follows the above SDLC philosophy such that it may work within an agile\nsoftware development team to perform several tasks end-to-end. ALMAS aligns its\nagents with agile roles, and can be used in a modular fashion to seamlessly\nintegrate with human developers and their development environment. We showcase\nthe progress towards ALMAS through our published works and a use case\ndemonstrating the framework, where ALMAS is able to seamlessly generate an\napplication and add a new feature.", "AI": {"tldr": "The paper introduces ALMAS, a multi-agent LLM framework for comprehensive, agile-aligned automation of the software development life-cycle, demonstrating its ability to generate and update applications autonomously in collaboration with human teams.", "motivation": "Large Language Models (LLMs) have shown promise in automating various aspects of software development, but most existing systems focus on code-related tasks rather than the full software development life-cycle (SDLC). The motivation is to create a more comprehensive and integrated framework that supports all SDLC stages and agile methodologies, enhancing the productivity and collaboration of human developers.", "method": "The paper proposes ALMAS, an Autonomous LLM-based Multi-Agent Software Engineering framework. This framework aligns LLM agents with specific agile roles and is designed to be modular, allowing it to integrate easily with human teams and existing development environments. The paper also demonstrates ALMAS through published works and a detailed use case in which it generates an application and adds a new feature autonomously.", "result": "ALMAS is shown to be capable of working end-to-end within the software development life-cycle, not only generating an application but also autonomously adding a new feature. The use case and published works demonstrate its progress and potential for seamless integration with agile software development teams.", "conclusion": "ALMAS represents a significant step toward comprehensive automation in software engineering, addressing both coding and non-coding tasks throughout the SDLC. Its modular and agile-aligned approach demonstrates promise for real-world integration and collaboration with human developers."}}
{"id": "2510.03474", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03474", "abs": "https://arxiv.org/abs/2510.03474", "authors": ["Nadeeshan De Silva", "Martin Kellogg", "Oscar Chaparro"], "title": "Relative Code Comprehensibility Prediction", "comment": null, "summary": "Automatically predicting how difficult it is for humans to understand a code\nsnippet can assist developers in tasks like deciding when and where to\nrefactor. Despite many proposed code comprehensibility metrics, studies have\nshown they often correlate poorly with actual measurements of human\ncomprehensibility. This has motivated the use of machine learning models to\npredict human comprehensibility directly from code, but these models have also\nshown limited accuracy.\n  We argue that model inaccuracy stems from inherent noise in human\ncomprehensibility data, which confuses models trained to predict it directly.\nTo address this, we propose training models to predict the relative\ncomprehensibility of two code snippets - that is, predicting which snippet a\nhuman would find easier to understand without predicting each snippet's\ncomprehensibility in isolation. This mitigates noise in predicting 'absolute'\ncomprehensibility measurements, but is still useful for downstream\nsoftware-engineering tasks like assessing whether refactoring improves or\nhinders comprehensibility.\n  We conducted a study to assess and compare the effectiveness of absolute and\nrelative code comprehensibility prediction via machine learning. We used a\ndataset of 150 Java code snippets and 12.5k human comprehensibility\nmeasurements from prior user studies, comparing the models' performance with\nnaive baselines (eg 'always predict the majority class'). Our findings indicate\nthat absolute comprehensibility models improve over the baselines by at most\n33.4% and frequently underperform. In contrast, relative comprehensibility\nmodels are substantially better, with average improvements of 137.8% and 74.7%\nfor snippet-wise and developer-wise prediction, respectively. These results\nsuggest that relative comprehensibility models learn more effectively from the\ndata, supporting their practical applicability for downstream SE tasks.", "AI": {"tldr": "Directly predicting relative code comprehensibility between code snippets using machine learning is significantly more effective than predicting absolute comprehensibility, overcoming data noise issues and better supporting software engineering tasks like refactoring assessment.", "motivation": "Existing code comprehensibility metrics and direct machine learning models for predicting code comprehensibility have shown poor correlation with human understanding, largely due to noise in human comprehension data. There is a need for more reliable prediction methods to aid developers in tasks like code refactoring.", "method": "The authors propose and empirically test a machine learning approach that predicts the relative comprehensibility between pairs of code snippets (i.e., which of two code snippets is easier for humans to understand), instead of predicting the absolute comprehensibility of individual snippets. They used a dataset containing 150 Java code snippets and 12.5k human comprehension scores from prior studies, comparing model performance to naive baselines.", "result": "Models predicting absolute comprehensibility performed only marginally better than baselines (maximum 33.4% improvement and often underperformed). In contrast, relative comprehensibility models showed large improvements over baselines, achieving 137.8% and 74.7% average improvements for snippet-wise and developer-wise scenarios, respectively.", "conclusion": "Predicting relative comprehensibility between code snippets not only mitigates the noise present in human comprehensibility data but is also more effective than predicting absolute comprehensibility. This approach has practical applicability for software engineering tasks, such as evaluating the impact of refactoring on code comprehensibility."}}
{"id": "2510.03480", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03480", "abs": "https://arxiv.org/abs/2510.03480", "authors": ["Vali Tawosi", "Salwa Alamir", "Xiaomo Liu", "Manuela Veloso"], "title": "LLM Agents for Automated Dependency Upgrades", "comment": null, "summary": "As a codebase expands over time, its library dependencies can become outdated\nand require updates to maintain innovation and security. However, updating a\nlibrary can introduce breaking changes in the code, necessitating significant\ndeveloper time for maintenance. To address this, we introduce a framework of\nLLM agents to be used in combination with migration documentation to\nautomatically recommend and apply code updates and ensure compatibility with\nnew versions. Our solution can automatically localize updated library usages in\nlive Java codebases and implement recommended fixes in a user-friendly manner.\nThe system architecture consists of multiple key components: a Summary Agent,\nControl Agent, and Code Agent. To validate our approach, we apply the framework\non an industrial use case by which we create three synthetic code repositories\nwith major Upgrade changes and benchmark our approach against state-of-the-art\nmethods. Results show that our approach not only performs upgrades using fewer\ntokens across all cases but also achieves a precision of 71.4%, highlighting\nits efficiency and effectiveness compared to state-of-the-art methods.", "AI": {"tldr": "This paper introduces an automated LLM agent framework for upgrading library dependencies in Java codebases, showing strong efficiency and a precision rate of 71.4%, outperforming current state-of-the-art approaches.", "motivation": "Maintaining up-to-date library dependencies is essential for innovation and security in growing codebases, but manual updates are time-consuming and risk introducing breaking changes.", "method": "The authors propose a framework utilizing LLM (Large Language Model) agents combined with migration documentation to automatically locate usages of outdated libraries in Java codebases and apply recommended updates. The system includes a Summary Agent, Control Agent, and Code Agent to handle different aspects of the upgrade process.", "result": "The framework was validated on three synthetic repositories exhibiting significant library upgrades and benchmarked against state-of-the-art solutions. The approach performed upgrades with reduced token usage and achieved a precision of 71.4%.", "conclusion": "The proposed LLM agent-based system efficiently automates library upgrades in codebases, performing with higher precision and efficiency compared to existing methods."}}
{"id": "2510.03495", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03495", "abs": "https://arxiv.org/abs/2510.03495", "authors": ["Erik Pautsch", "Tanmay Singla", "Wenxin Jiang", "Huiyun Peng", "Behnaz Hassanshahi", "Konstantin L\u00e4ufer", "George K. Thiruvathukal", "James C. Davis"], "title": "AgentHub: A Research Agenda for Agent Sharing Infrastructure", "comment": null, "summary": "LLM-based agents are rapidly proliferating, yet the infrastructure for\ndiscovering, evaluating, and governing them remains fragmented compared to\nmature ecosystems like software package registries (e.g., npm) and model hubs\n(e.g., Hugging Face). Recent research and engineering works have begun to\nconsider the requisite infrastructure, but so far they focus narrowly -- on\ndistribution, naming, or protocol negotiation. However, considering broader\nsoftware engineering requirements would improve open-source distribution and\nease reuse. We therefore propose AgentHub, a research agenda for agent sharing.\nBy framing the key challenges of capability clarity, lifecycle transparency,\ninteroperability, governance, security, and workflow integration, AgentHub\ncharts a community-wide agenda for building reliable and scalable agent\necosystems. Our vision is a future where agents can be shared, trusted, and\ncomposed as seamlessly as today's software libraries.", "AI": {"tldr": "This paper proposes AgentHub, a research agenda identifying key challenges to create a unified infrastructure for discovering and governing LLM-based agents, aiming for ecosystems as robust as those for software libraries.", "motivation": "LLM-based agents are multiplying, but unlike mature software registries and model hubs, there is no unified infrastructure for their discovery, evaluation, and governance, limiting reuse and scalability.", "method": "This study reviews current fragmented approaches and proposes a broad framework by identifying key engineering challenges including capability clarity, lifecycle transparency, interoperability, governance, security, and workflow integration.", "result": "AgentHub provides a community-wide vision and set of challenges to encourage development of reliable platforms enabling composable and trustworthy agent sharing.", "conclusion": "AgentHub defines a comprehensive research agenda aimed at addressing the infrastructure gaps for sharing and managing LLM-based agents, envisioning seamless, trusted, and scalable agent ecosystems."}}
{"id": "2510.03588", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.03588", "abs": "https://arxiv.org/abs/2510.03588", "authors": ["Anvith Pabba", "Simin Chen", "Alex Mathai", "Anindya Chakraborty", "Baishakhi Ray"], "title": "REFINE: Enhancing Program Repair Agents through Context-Aware Patch Refinement", "comment": "We also open source our code at\n  https://anonymous.4open.science/r/SemAgent-7B2F/README.md", "summary": "Large Language Models (LLMs) have recently shown strong potential in\nautomatic program repair (APR), especially in repository-level settings where\nthe goal is to generate patches based on natural language issue descriptions,\nlarge codebases, and regression tests. However, despite their promise, current\nLLM-based APR techniques often struggle to produce correct fixes due to limited\nunderstanding of code context and over-reliance on incomplete test suites. As a\nresult, they frequently generate Draft Patches-partially correct patches that\neither incompletely address the bug or overfit to the test cases. In this work,\nwe propose a novel patch refinement framework, Refine, that systematically\ntransforms Draft Patches into correct ones. Refine addresses three key\nchallenges: disambiguating vague issue and code context, diversifying patch\ncandidates through test-time scaling, and aggregating partial fixes via an\nLLM-powered code review process. We implement Refine as a general refinement\nmodule that can be integrated into both open-agent-based and workflow-based APR\nsystems. Our evaluation on the SWE-Bench Lite benchmark shows that Refine\nachieves state-of-the-art results among workflow-based approaches and\napproaches the best-known performance across all APR categories. Specifically,\nRefine boosts AutoCodeRover's performance by 14.67%, achieving a score of\n51.67% and surpassing all prior baselines. On SWE-Bench Verified, Refine\nimproves the resolution rate by 12.2%, and when integrated across multiple APR\nsystems, it yields an average improvement of 14%-demonstrating its broad\neffectiveness and generalizability. These results highlight the effectiveness\nof refinement as a missing component in current APR pipelines and the potential\nof agentic collaboration in closing the gap between near-correct and correct\npatches. We also open source our code.", "AI": {"tldr": "Refine is a novel framework that refines partially correct patches produced by LLM-based program repair tools. By resolving ambiguities, enhancing candidate diversity, and using LLM-powered review, Refine significantly improves APR performance, offering state-of-the-art results and broad applicability.", "motivation": "Current LLM-based APR techniques struggle to produce fully correct fixes due to limited understanding of code context and often generate only partially correct patches. There is a need for a systematic approach that can transform these near-correct solutions into fully correct program repairs.", "method": "The paper introduces Refine, a novel patch refinement framework for LLM-based automatic program repair. Refine operates by disambiguating issue and code contexts, diversifying patch candidates through test-time scaling, and aggregating partial fixes using LLM-powered code review. It is implemented as a general module that can be integrated into various APR systems.", "result": "Refine achieves state-of-the-art results for workflow-based APR systems and approaches the best-known results for all APR systems. It improves AutoCodeRover's performance by 14.67%, achieves a 51.67% score on SWE-Bench Lite, and increases the resolution rate by 12.2% on SWE-Bench Verified. Across different APR systems, it yields an average improvement of 14%.", "conclusion": "Patch refinement is a crucial and effective missing component in current LLM-based APR systems, significantly improving their ability to produce correct program fixes. The proposed framework, Refine, demonstrates strong generalizability and boosts the effectiveness of multiple APR workflows."}}
{"id": "2510.03641", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03641", "abs": "https://arxiv.org/abs/2510.03641", "authors": ["Satoshi Masuda", "Satoshi Kouzawa", "Kyousuke Sezai", "Hidetoshi Suhara", "Yasuaki Hiruta", "Kunihiro Kudou"], "title": "Generating High-Level Test Cases from Requirements using LLM: An Industry Study", "comment": "11pages", "summary": "Currently, generating high-level test cases described in natural language\nfrom requirement documents is performed manually. In the industry, including\ncompanies specializing in software testing, there is a significant demand for\nthe automatic generation of high-level test cases from requirement documents\nusing Large Language Models (LLMs). Efforts to utilize LLMs for requirement\nanalysis are underway. In some cases, retrieval-augmented generation (RAG) is\nemployed for generating high-level test cases using LLMs. However, in practical\napplications, it is necessary to create a RAG tailored to the knowledge system\nof each specific application, which is labor-intensive. Moreover, when applying\nhigh-level test case generation as a prompt, there is no established method for\ninstructing the generation of high-level test cases at a level applicable to\nother specifications without using RAG. It is required to establish a method\nfor the automatic generation of high-level test cases that can be generalized\nacross a wider range of requirement documents. In this paper, we propose a\nmethod for generating high-level (GHL) test cases from requirement documents\nusing only prompts, without creating RAGs. In the proposed method, first, the\nrequirement document is input into the LLM to generate test design techniques\ncorresponding to the requirement document. Then, high-level test cases are\ngenerated for each of the generated test design techniques. Furthermore, we\nverify an evaluation method based on semantic similarity of the generated\nhigh-level test cases. In the experiments, we confirmed the method using\ndatasets from Bluetooth and Mozilla, where requirement documents and high-level\ntest cases are available, achieving macro-recall measurement of 0.81 and 0.37,\nrespectively. We believe that the method is feasible for practical application\nin generating high-level test cases without using RAG.", "AI": {"tldr": "Manual high-level test case generation from requirement documents is laborious. The authors present an LLM-based prompt-only method, bypassing costly RAG setup. Tests on Bluetooth and Mozilla data show decent recall, suggesting the approach is viable for automating test case generation in real-world scenarios.", "motivation": "Currently, the automatic generation of high-level test cases from natural language requirement documents is a labor-intensive manual process. While LLMs and retrieval-augmented generation (RAG) techniques are being considered, building application-specific RAGs for different knowledge domains is costly and inefficient. There is a strong need for a generalized approach that can generate high-level test cases without requiring tailored RAGs.", "method": "The authors propose a prompt-based approach using LLMs for generating high-level test cases directly from requirement documents, without the need for RAG. The process involves inputting requirement documents into an LLM to first produce relevant test design techniques, then generating high-level test cases for each technique. The quality of generated test cases is assessed using semantic similarity metrics.", "result": "Experiments using Bluetooth and Mozilla datasets (each with available requirement documents and high-level test cases) showed macro-recall measurements of 0.81 and 0.37, respectively. This indicates reasonable effectiveness, especially compared to the manual and RAG-based methods, although performance varies across domains.", "conclusion": "The proposed prompt-only LLM-based method can effectively and feasibly generate high-level test cases from requirement documents, without the need for labor-intensive RAG creation. This approach demonstrates practical promise, as indicated by experimental recall values, and could be adopted for broader industrial usage."}}
{"id": "2510.03712", "categories": ["cs.SE", "68M15, 90B25, 68T05, 90C29", "C.4; C.2.4; D.2.5; D.4.5"], "pdf": "https://arxiv.org/pdf/2510.03712", "abs": "https://arxiv.org/abs/2510.03712", "authors": ["Jahidul Arafat", "Kh. M. Moniruzzaman", "Shamim Hossain", "Fariha Tasmin", "Kamrujjaman", "Ahsan Habib Tareq"], "title": "Detecting and Preventing Latent Risk Accumulation in High-Performance Software Systems", "comment": "26 pages, 12 tables, 4 figures. Academic-industry collaboration.\n  Framework (HYDRA, RAVEN, APEX) for optimization-induced vulnerabilities.\n  Evaluated: 2,160 configs, 12.7TB data, 1,748 scenarios", "summary": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization.", "AI": {"tldr": "The paper presents a proactive framework for detecting and reducing hidden risks in optimized distributed systems, using mathematical models, intelligent testing, and monitoring. It provides strong empirical results, improving reliability, reducing incident impact, and delivering quick financial returns.", "motivation": "Modern distributed systems are highly optimized for performance, but these optimizations can mask hidden vulnerabilities (latent risks) that can cause catastrophic failures when systems deviate from the optimized norm (e.g., cache failures unleashing underlying database bottlenecks). Existing reliability engineering is mostly reactive, not preemptively identifying such risks.", "method": "The paper introduces a comprehensive framework based on mathematical modeling, perturbation testing, and risk-aware optimization. It defines a Latent Risk Index (LRI), implements three systems: HYDRA (for perturbation/risk discovery), RAVEN (for ongoing monitoring), and APEX (for risk-aware, performance-preserving optimization). The approach combines empirical analysis, real-world deployment, and rigorous statistical validation.", "result": "The framework achieves high rates of risk detection (HYDRA: 89.7%), precise monitoring (RAVEN: 92.9% precision, 93.8% recall), significant reduction in latent risks (APEX: 59.2%), and substantial performance retention (96.6% baseline performance). Real-world deployment reduced mean time to recovery by 69.1%, incident severity by 78.6%, and prevented 81 incidents, yielding significant financial benefits and fast ROI.", "conclusion": "This approach shifts the paradigm in distributed systems reliability from reactive management to proactive risk detection and optimization, offering measurable improvements in reliability and cost savings."}}
{"id": "2510.03743", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03743", "abs": "https://arxiv.org/abs/2510.03743", "authors": ["Zachary Eberhart", "Collin McMillan"], "title": "APIDA-Chat: Structured Synthesis of API Search Dialogues to Bootstrap Conversational Agents", "comment": "4 pages, 2 figures. To be published in Proceedings of the 40th\n  IEEE/ACM International Conference on Automated Software Engineering", "summary": "Large-language-model assistants are suitable for explaining popular APIs, yet\nthey falter on niche or proprietary libraries because the multi-turn dialogue\ndata needed for fine-tuning are scarce. We present APIDA-Chat, an open-source\npipeline that converts symbolic dialogue-act \"scripts\" into realistic,\ndomain-grounded API Search conversations using a lightweight model for\ninexpensive training data generation. Phase I pairs a legacy dialogue planner\nwith a high-capability teacher LLM (o4-mini) to synthesize a \"gold set\" of\nrealized dialogues; then, a smaller Llama 3.2 3B student model is fine-tuned on\nthis corpus. Phase II drops the teacher and reuses the same planner with the\nfine-tuned model, allowing rapid, low-cost synthesis of new dialogues without\nexposing source code to external services. The fine-tuned student improves BLEU\nfrom 0.38 to 0.50 and BERTScore from 0.88 to 0.91 versus the base model while\nrunning entirely on a single consumer GPU. All components are modular and\npublicly released to serve as a conservative baseline for future work.\nAPIDA-Chat is open-sourced at https://github.com/Zeberhart/apida-chat and a\nvideo demo is available at https://youtu.be/YqmZBHyGbPs .", "AI": {"tldr": "APIDA-Chat is an open-source pipeline for generating realistic API search dialogues for niche libraries, enabling effective model fine-tuning where data is scarce. Using a planner and student-teacher setup, it improves benchmarks while being cost-effective, modular, and privacy-preserving.", "motivation": "Large language models perform well in explaining popular APIs, but struggle with niche or proprietary ones due to scarce dialogue data for fine-tuning.", "method": "The authors developed APIDA-Chat, an open-source pipeline that uses symbolic dialogue-act scripts to generate realistic, domain-grounded API search conversations. Phase I uses a dialogue planner and a teacher LLM to synthesize quality dialogue data, which is then used to fine-tune a smaller student model. In Phase II, the student model generates new dialogues without external LLMs, improving cost and privacy.", "result": "The fine-tuned student model improved BLEU score from 0.38 to 0.50 and BERTScore from 0.88 to 0.91 compared to the base model. It runs on a single consumer GPU, and all components are publicly released.", "conclusion": "APIDA-Chat offers a practical, modular, open-source solution for generating training data and fine-tuning dialogue models for niche API domains, addressing data scarcity, cost, and privacy problems."}}
{"id": "2510.03755", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03755", "abs": "https://arxiv.org/abs/2510.03755", "authors": ["Roham Koohestani", "Parham Bateni", "Aydin Ebrahimi", "Behdad Etezadi", "Kiarash Karimi", "Maliheh Izadi"], "title": "Code4MeV2: a Research-oriented Code-completion Platform", "comment": "Under review for submission at a conference", "summary": "The adoption of AI-powered code completion tools in software development has\nincreased substantially, yet the user interaction data produced by these\nsystems remain proprietary within large corporations. This creates a barrier\nfor the academic community, as researchers must often develop dedicated\nplatforms to conduct studies on human--AI interaction, making reproducible\nresearch and large-scale data analysis impractical. In this work, we introduce\nCode4MeV2, a research-oriented, open-source code completion plugin for\nJetBrains IDEs, as a solution to this limitation. Code4MeV2 is designed using a\nclient--server architecture and features inline code completion and a\ncontext-aware chat assistant. Its core contribution is a modular and\ntransparent data collection framework that gives researchers fine-grained\ncontrol over telemetry and context gathering. Code4MeV2 achieves\nindustry-comparable performance in terms of code completion, with an average\nlatency of 200~ms. We assess our tool through a combination of an expert\nevaluation and a user study with eight participants. Feedback from both\nresearchers and daily users highlights its informativeness and usefulness. We\ninvite the community to adopt and contribute to this tool. More information\nabout the tool can be found at https://app.code4me.me.", "AI": {"tldr": "The paper presents Code4MeV2, an open-source code completion tool designed for research, overcoming proprietary barriers by enabling transparent data collection and achieving industry-level performance according to expert and user feedback.", "motivation": "Most AI-powered code completion tools generate valuable user interaction data, but this data is kept proprietary by large corporations. This restricts academic research in human-AI interaction, as researchers lack access to reproducible, large-scale data and must build bespoke platforms for studies.", "method": "The authors developed Code4MeV2, an open-source, research-oriented code completion plugin for JetBrains IDEs. It uses a client-server architecture, offers inline code completion and a chat assistant, and focuses on modular, transparent data collection. Its efficacy was assessed through expert evaluations and a user study involving eight participants.", "result": "Code4MeV2 delivers industry-level code completion performance with about 200ms latency. Feedback from both researchers and users highlighted its informativeness and usefulness for research purposes.", "conclusion": "Code4MeV2 provides the academic community with an effective, open-source platform for research on code completion and human-AI interaction, enabling fine-grained data collection and greater reproducibility."}}
{"id": "2510.03802", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03802", "abs": "https://arxiv.org/abs/2510.03802", "authors": ["Gilberto Recupito", "Vincenzo De Martino", "Dario Di Nucci", "Fabio Palomba"], "title": "A First Look at the Lifecycle of DL-Specific Self-Admitted Technical Debt", "comment": "Accepted at the International Workshop of Software Quality Assurance\n  for Artificial Intelligence 2025 (SQA4AI), Montr\\'eal, Canada", "summary": "The rapid adoption of Deep Learning (DL)-enabled systems has revolutionized\nsoftware development, driving innovation across various domains. However, these\nsystems also introduce unique challenges, particularly in maintaining software\nquality and performance. Among these challenges, Self-Admitted Technical Debt\n(SATD) has emerged as a growing concern, significantly impacting the\nmaintainability and overall quality of ML and DL-enabled systems. Despite its\ncritical implications, the lifecycle of DL-specific SATD, how developers\nintroduce, acknowledge, and address it over time-remains underexplored. This\nstudy presents a preliminary analysis of the persistence and lifecycle of\nDL-specific SATD in DL-enabled systems. The purpose of this project is to\nuncover the patterns of SATD introduction, recognition, and durability during\nthe development life cycle, providing information on how to manage these\nissues. Using mining software repository techniques, we examined 40 ML\nprojects, focusing on 185 DL-specific SATD instances. The analysis tracked the\nintroduction and persistence of SATD instances through project commit histories\nto assess their lifecycle and developer actions. The findings indicate that\nDL-specific SATD is predominantly introduced during the early and middle stages\nof project development. Training and Hardware phases showed the longest SATD\ndurations, highlighting critical areas where debt accumulates and persists.\nAdditionally, developers introduce DL-specific SATD more frequently during\nfeature implementation and bug fixes. This study emphasizes the need for\ntargeted DL-specific SATD management strategies in DL-enabled systems to\nmitigate its impact. By understanding the temporal characteristics and\nevolution of DL-specific SATD, developers can prioritize interventions at\ncritical stages to improve the maintainability and quality of the system.", "AI": {"tldr": "This paper analyzes how DL-specific technical debt is introduced, persists, and resolved in ML projects. SATD is most common during early/middle development, persisting longest in training and hardware phases. Targeted management strategies are needed to improve maintainability and quality in DL-enabled systems.", "motivation": "Deep Learning (DL)-enabled systems are rapidly adopted and transform software development, but they bring unique challenges such as maintaining software quality and performance. Self-Admitted Technical Debt (SATD) specifically related to DL is a growing concern, yet its lifecycle\u2014how it's introduced, acknowledged, and resolved\u2014remains poorly understood.", "method": "The study utilizes mining software repository techniques to analyze 40 machine learning (ML) projects. It focuses on 185 instances of DL-specific SATD by tracking their introduction and persistence through project commit histories.", "result": "DL-specific SATD is most frequently introduced in early and middle development stages, especially during feature implementation and bug fixes. Training and Hardware phases exhibit the longest SATD persistence, signifying where technical debt accumulates and remains unresolved.", "conclusion": "There is a pressing need for DL-specific SATD management strategies within DL-enabled systems. By understanding the patterns and durations associated with DL-specific SATD, developers can better target interventions to improve maintainability and software quality."}}
{"id": "2510.03843", "categories": ["cs.SE", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03843", "abs": "https://arxiv.org/abs/2510.03843", "authors": ["Vincent Nguyen", "Guilherme Herzog", "Jos\u00e9 Cambronero", "Marcus Revaj", "Aditya Kini", "Alexander Fr\u00f6mmgen", "Maxim Tabachnyk"], "title": "Smart Paste: Automatically Fixing Copy/Paste for Google Developers", "comment": "11 pages", "summary": "Manually editing pasted code is a long-standing developer pain point. In\ninternal software development at Google, we observe that code is pasted 4 times\nmore often than it is manually typed. These paste actions frequently require\nfollow-up edits, ranging from simple reformatting and renaming to more complex\nstyle adjustments and cross-language translations. Prior work has shown deep\nlearning can be used to predict these edits. In this work, we show how to\niteratively develop and scale Smart Paste, an IDE feature for post-paste edit\nsuggestions, to Google's development environment. This experience can serve as\na guide for AI practitioners on a holistic approach to feature development,\ncovering user experience, system integration, and model capabilities. Since\ndeployment, Smart Paste has had overwhelmingly positive feedback with a 45%\nacceptance rate. At Google's enterprise scale, these accepted suggestions\naccount substantially for over 1% of all code written company-wide.", "AI": {"tldr": "Manually editing pasted code is common and tedious among Google developers. By using AI to suggest post-paste edits, Smart Paste streamlines this process within the IDE, achieving high user acceptance and meaningful company-wide impact. The approach serves as a strong reference for deploying practical AI features in large environments.", "motivation": "Developers frequently paste code rather than manually typing, which leads to repetitive and time-consuming post-paste edits. Improving and automating these follow-up edits would significantly enhance developer productivity and satisfaction.", "method": "The authors iteratively developed and scaled the Smart Paste feature for IDEs at Google, integrating it within Google's environment, focusing on user experience, system integration, and enhancing model capabilities using AI to suggest relevant post-paste edits.", "result": "After deployment, Smart Paste received strongly positive feedback, achieving a 45% acceptance rate for its suggestions. The accepted edits constitute more than 1% of all code written across Google, demonstrating substantial impact at enterprise scale.", "conclusion": "Smart Paste effectively reduces developer pain associated with editing pasted code, proving valuable and influential within Google's software development ecosystem. The methodology and deployment strategies outlined can guide the development of similar AI-powered productivity features."}}
{"id": "2510.03862", "categories": ["cs.SE", "cs.AI", "500"], "pdf": "https://arxiv.org/pdf/2510.03862", "abs": "https://arxiv.org/abs/2510.03862", "authors": ["Nathalia Nascimento", "Everton Guimaraes", "Paulo Alencar"], "title": "Designing Empirical Studies on LLM-Based Code Generation: Towards a Reference Framework", "comment": "5 pages", "summary": "The rise of large language models (LLMs) has introduced transformative\npotential in automated code generation, addressing a wide range of software\nengineering challenges. However, empirical evaluation of LLM-based code\ngeneration lacks standardization, with studies varying widely in goals, tasks,\nand metrics, which limits comparability and reproducibility. In this paper, we\npropose a theoretical framework for designing and reporting empirical studies\non LLM-based code generation. The framework is grounded in both our prior\nexperience conducting such experiments and a comparative analysis of key\nsimilarities and differences among recent studies. It organizes evaluation\naround core components such as problem sources, quality attributes, and\nmetrics, supporting structured and systematic experimentation. We demonstrate\nits applicability through representative case mappings and identify\nopportunities for refinement. Looking forward, we plan to evolve the framework\ninto a more robust and mature tool for standardizing LLM evaluation across\nsoftware engineering contexts.", "AI": {"tldr": "A new framework is proposed to standardize how studies evaluate large language model-based code generation, aiming to make future experiments more consistent, comparable, and reproducible.", "motivation": "There is a lack of standardization in the empirical evaluation of large language model (LLM)-based code generation, leading to inconsistent study designs and limited comparability and reproducibility of results.", "method": "The authors propose and elaborate a theoretical framework for designing and reporting empirical studies on LLM-based code generation. The framework is informed by their own research experience and a comparative analysis of existing studies. It structures evaluation around core components such as problem sources, quality attributes, and metrics, and is validated through case mappings.", "result": "The framework offers a structured method to evaluate LLM-based code generation, enhancing the rigor and comparability of empirical studies. Its applicability is demonstrated through case mappings, and opportunities for further refinement are identified.", "conclusion": "A theoretical framework for standardizing the empirical evaluation of LLM-based code generation is proposed and preliminarily validated. The framework supports structured experimentation and reporting, with the aim of improving comparability and reproducibility. Future work will focus on maturing the framework for broader adoption."}}
{"id": "2510.03879", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03879", "abs": "https://arxiv.org/abs/2510.03879", "authors": ["Tianyu Li", "Ruishi Li", "Bo Wang", "Brandon Paulsen", "Umang Mathur", "Prateek Saxena"], "title": "Adversarial Agent Collaboration for C to Rust Translation", "comment": null, "summary": "Translating C to memory-safe languages, like Rust, prevents critical memory\nsafety vulnerabilities that are prevalent in legacy C software. Existing\napproaches for C to safe Rust translation, including LLM-assisted ones, do not\ngeneralize on larger (> 500 LoC) C codebases because they depend on complex\nprogram analyses that frequently break. In this work, we present ACToR\n(Adversarial C To Rust translator), a simple LLM agent-based approach. Inspired\nby GANs, ACToR pits a generator agent against a discriminator agent, which\ncollaborate to iteratively generate a Rust translation. On each iteration, the\ntranslator agent synthesizes and refines a Rust translation to pass an existing\nsuite of tests, and then the discriminator agent finds new failing tests. We\ndemonstrate that ACToR translates all of the 63 real-world command line\nutilities considered in our benchmarks, which have an average size of 485 lines\nof code, and it achieves over 90% test pass rate with zero human intervention.\nTo our knowledge, it is the first such system that reliably translates C\nprograms of this scale. Furthermore, ACToR improves translation correctness by\nup to 18.9% compared to baseline, non-adversarial approaches.", "AI": {"tldr": "ACToR introduces an adversarial, LLM-based approach for translating C code to Rust, outperforming previous methods in reliability, scale, and correctness on real-world codebases, thus offering a feasible path to safer software.", "motivation": "Translating legacy C code to memory-safe languages like Rust helps prevent critical memory safety vulnerabilities that are common in C software, but current automated translation efforts struggle with larger codebases and frequently break due to complex analyses.", "method": "The authors introduce ACToR, an LLM agent-based translation approach inspired by GANs. It involves a translator agent that generates and refines Rust code to pass existing tests, and a discriminator agent that identifies failing tests, allowing iterative improvement.", "result": "ACToR was able to translate all 63 real-world command line utilities tested (average size: 485 LoC) with more than a 90% test pass rate and no human intervention. It also improved translation correctness by up to 18.9% over non-adversarial baselines.", "conclusion": "ACToR is the first system to reliably translate C programs of this size to Rust with such high test pass rates and correctness, demonstrating significant practical advances in automated, scalable, and memory-safe code translation."}}
{"id": "2510.03890", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03890", "abs": "https://arxiv.org/abs/2510.03890", "authors": ["Jose Garcia-Alonso", "Enrique Moguel", "Jaime Alvarado-Valiente", "Javier Romero-Alvarez", "\u00c1lvaro M. Aparicio-Morales", "Juan M. Murillo", "Francisco Javier Cavero", "Adri\u00e1n Romero-Flores", "Alfonso E. Marquez-Chamorro", "Jos\u00e9 Antonio Parejo", "Antonio Ruiz-Cort\u00e9s", "Giuseppe Bisicchia", "Alessandro Bocci", "Antonio Brogi"], "title": "Rethinking Services in the Quantum Age: The SOQ Paradigm", "comment": "39 pages, 5 figures, 6 tables", "summary": "Quantum computing is rapidly progressing from theoretical promise to\npractical implementation, offering significant computational advantages for\ntasks in optimization, simulation, cryptography, and machine learning. However,\nits integration into real-world software systems remains constrained by\nhardware fragility, platform heterogeneity, and the absence of robust software\nengineering practices. This paper introduces Service-Oriented Quantum (SOQ), a\nnovel paradigm that reimagines quantum software systems through the lens of\nclassical service-oriented computing. Unlike prior approaches such as Quantum\nService-Oriented Computing (QSOC), which treat quantum capabilities as\nauxiliary components within classical systems, SOQ positions quantum services\nas autonomous, composable, and interoperable entities. We define the\nfoundational principles of SOQ, propose a layered technology stack to support\nits realization, and identify the key research and engineering challenges that\nmust be addressed, including interoperability, hybridity, pricing models,\nservice abstractions, and workforce development. This approach is of vital\nimportance for the advancement of quantum technology because it enables the\nscalable, modular, and interoperable integration of quantum computing into\nreal-world software systems independently and without relying on a dedicated\nclassical environment to manage quantum processing.", "AI": {"tldr": "This paper proposes Service-Oriented Quantum (SOQ), a new paradigm that establishes quantum services as standalone, interoperable building blocks for modern software systems, offering practical solutions for scalability and integration challenges in quantum computing.", "motivation": "Quantum computing, despite its impressive theoretical and practical promise, faces barriers in real-world integration due to hardware limitations and the absence of sound software engineering principles.", "method": "The paper introduces Service-Oriented Quantum (SOQ), building on but diverging from Quantum Service-Oriented Computing (QSOC), and articulates core principles and a layered technology stack for SOQ realization. It also outlines challenges such as interoperability, pricing models, and workforce development.", "result": "SOQ redefines quantum services as autonomous, composable, and interoperable entities and provides a framework for their integration into software systems. The paper presents SOQ\u2019s technology stack and clarifies essential research and engineering barriers to be addressed.", "conclusion": "By positioning quantum services as independently scalable and modular software entities, SOQ enables the robust and flexible integration of quantum computing into real-world systems, advancing both technological adoption and engineering practice."}}
{"id": "2510.03894", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03894", "abs": "https://arxiv.org/abs/2510.03894", "authors": ["Antonios Saravanos"], "title": "A Brief History of the Waterfall Model: Past, Present, and Future", "comment": null, "summary": "The waterfall model, one of the earliest software development methodologies,\nhas played a foundational role in shaping contemporary software engineering\npractices. This paper provides a historical and critical overview of the model,\ntracing its conceptual origins in software engineering, its formalization by\nRoyce, and its evolution through decades of industry adoption and critique.\nAlthough often criticized for its rigidity, shortcomings, and high failure\nrates, the waterfall model persists in specific domains. Its principles\ncontinue to influence contemporary hybrid development frameworks that combine\ntraditional and agile methods. Drawing on a range of scholarly sources, this\nstudy synthesizes key developments in the perception and application of the\nwaterfall model. The analysis highlights how the model has shifted from a\nstandalone framework to a component within modern hybrid methodologies. By\nrevisiting its origins, assessing its present utility, and examining its role\nin contemporary development practices, this paper argues that the waterfall\nmodel remains relevant, not as a relic of the past but as part of context-aware\ndevelopment strategies. The paper contends that the model's enduring relevance\nlies in its adaptability. By recognizing both its limitations and its\nstrengths, and by understanding its integration within hybrid approaches,\npractitioners can make more informed decisions about methodology selection and\nprocess design in diverse development environments.", "AI": {"tldr": "This paper critically reviews the history and evolution of the waterfall software development model, arguing that its adaptability and integration into hybrid approaches ensure its ongoing relevance in software engineering.", "motivation": "To provide a historical and critical overview of the waterfall software development model, tracing its origins, evolution, and continued influence on contemporary software engineering practices.", "method": "A literature review and synthesis of scholarly studies are used to analyze the history, criticism, adaptation, and evolving role of the waterfall model in software engineering.", "result": "The waterfall model, despite criticism for its rigidity and high failure rates, persists in certain domains and its principles influence modern hybrid development frameworks. The model has evolved from a standalone framework to a component within agile-traditional hybrid methodologies.", "conclusion": "The waterfall model remains relevant by being adaptable and integrated into contemporary context-aware development strategies. Recognizing its strengths and limitations allows for informed methodology choices and better process design."}}
{"id": "2510.03902", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03902", "abs": "https://arxiv.org/abs/2510.03902", "authors": ["Rana Nameer Hussain Khan", "Dawood Wasif", "Jin-Hee Cho", "Ali Butt"], "title": "Multi-Agent Code-Orchestrated Generation for Reliable Infrastructure-as-Code", "comment": null, "summary": "The increasing complexity of cloud-native infrastructure has made\nInfrastructure-as-Code (IaC) essential for reproducible and scalable\ndeployments. While large language models (LLMs) have shown promise in\ngenerating IaC snippets from natural language prompts, their monolithic,\nsingle-pass generation approach often results in syntactic errors, policy\nviolations, and unscalable designs. In this paper, we propose MACOG\n(Multi-Agent Code-Orchestrated Generation), a novel multi-agent LLM-based\narchitecture for IaC generation that decomposes the task into modular subtasks\nhandled by specialized agents: Architect, Provider Harmonizer, Engineer,\nReviewer, Security Prover, Cost and Capacity Planner, DevOps, and Memory\nCurator. The agents interact via a shared-blackboard, finite-state orchestrator\nlayer, and collectively produce Terraform configurations that are not only\nsyntactically valid but also policy-compliant and semantically coherent. To\nensure infrastructure correctness and governance, we incorporate Terraform Plan\nfor execution validation and Open Policy Agent (OPA) for customizable policy\nenforcement. We evaluate MACOG using the IaC-Eval benchmark, where MACOG is the\ntop enhancement across models, e.g., GPT-5 improves from 54.90 (RAG) to 74.02\nand Gemini-2.5 Pro from 43.56 to 60.13, with concurrent gains on BLEU,\nCodeBERTScore, and an LLM-judge metric. Ablations show constrained decoding and\ndeploy feedback are critical: removing them drops IaC-Eval to 64.89 and 56.93,\nrespectively.", "AI": {"tldr": "MACOG uses a team of specialized AI agents (not just one big model) to generate cloud infrastructure code that's more accurate and policy-compliant. This architecture outperforms existing methods on benchmarks, and key features like feedback and policy checks are crucial to its success.", "motivation": "Cloud-native infrastructures have become highly complex, necessitating Infrastructure-as-Code (IaC) for reliable, reproducible, and scalable deployment. However, existing large language models (LLMs) that generate IaC code from natural language typically make mistakes: their monolithic, single-step approach often yields syntactic errors, non-compliant code, and poor scalability. There is a need to improve the accuracy and policy-compliance of automated IaC code generation.", "method": "The paper introduces MACOG, a Multi-Agent LLM-based architecture that breaks down IaC code generation into modular subtasks managed by specialized agents (such as Architect, Security Prover, DevOps, etc.). These agents communicate via a shared-blackboard system driven by a finite-state orchestrator. The system ensures code correctness with Terraform Plan (for validation) and Open Policy Agent (OPA) for policy enforcement.", "result": "MACOG outperforms traditional LLM-based approaches on the IaC-Eval benchmark. For example, integrating MACOG with GPT-5 raises the score from 54.90 to 74.02, and for Gemini-2.5 Pro from 43.56 to 60.13. It also demonstrates improvements in BLEU, CodeBERTScore, and LLM-judge metrics. Further analysis reveals that essential features like constrained decoding and deployment feedback are critical components, as removing them causes significant performance drops.", "conclusion": "MACOG\u2019s multi-agent, orchestrated approach produces IaC code that is more accurate, compliant, and robust than previous monolithic LLM methods. The system represents a step forward in automated, reliable infrastructure code generation for complex cloud-native environments."}}
{"id": "2510.03914", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03914", "abs": "https://arxiv.org/abs/2510.03914", "authors": ["Yonnel Chen Kuang Piao", "Jean Carlors Paul", "Leuson Da Silva", "Arghavan Moradi Dakhel", "Mohammad Hamdaqa", "Foutse Khomh"], "title": "Refactoring with LLMs: Bridging Human Expertise and Machine Understanding", "comment": "43 pages, 2 figures, 9 tables", "summary": "Code refactoring is a fundamental software engineering practice aimed at\nimproving code quality and maintainability. Despite its importance, developers\noften neglect refactoring due to the significant time, effort, and resources it\nrequires, as well as the lack of immediate functional rewards. Although several\nautomated refactoring tools have been proposed, they remain limited in\nsupporting a broad spectrum of refactoring types. In this study, we explore\nwhether instruction strategies inspired by human best-practice guidelines can\nenhance the ability of Large Language Models (LLMs) to perform diverse\nrefactoring tasks automatically. Leveraging the instruction-following and code\ncomprehension capabilities of state-of-the-art LLMs (e.g., GPT-mini and\nDeepSeek-V3), we draw on Martin Fowler's refactoring guidelines to design\nmultiple instruction strategies that encode motivations, procedural steps, and\ntransformation objectives for 61 well-known refactoring types. We evaluate\nthese strategies on benchmark examples and real-world code snippets from GitHub\nprojects. Our results show that instruction designs grounded in Fowler's\nguidelines enable LLMs to successfully perform all benchmark refactoring types\nand preserve program semantics in real-world settings, an essential criterion\nfor effective refactoring. Moreover, while descriptive instructions are more\ninterpretable to humans, our results show that rule-based instructions often\nlead to better performance in specific scenarios. Interestingly, allowing\nmodels to focus on the overall goal of refactoring, rather than prescribing a\nfixed transformation type, can yield even greater improvements in code quality.", "AI": {"tldr": "Integrating human best-practice refactoring guidelines as instructions for LLMs greatly improves their automated refactoring performance and code quality, especially when using rule-based or goal-focused strategies.", "motivation": "Code refactoring improves code quality and maintainability but is often neglected by developers due to its high cost and lack of immediate benefits. Existing automated refactoring tools are limited in the variety of refactoring types they can handle.", "method": "The study leverages instruction-following and code comprehension capabilities of advanced LLMs (e.g., GPT-mini, DeepSeek-V3), designing multiple instruction strategies based on Martin Fowler's refactoring guidelines. These strategies encode motivations, procedural steps, and objectives for 61 refactoring types and are evaluated on benchmarks and real-world GitHub code snippets.", "result": "Instruction designs based on Fowler's guidelines allowed LLMs to successfully perform all benchmarked refactoring types while preserving semantics. Rule-based instructions often yielded better performance than descriptive instructions in some scenarios, and focusing models on the overall refactoring goal improved code quality further.", "conclusion": "Human-inspired instruction strategies significantly enhance the ability of LLMs to automatically perform diverse refactoring tasks, preserve program semantics, and improve code quality in practice."}}
{"id": "2510.03920", "categories": ["cs.SE", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.03920", "abs": "https://arxiv.org/abs/2510.03920", "authors": ["Ravi Kalluri"], "title": "Why Does the Engineering Manager Still Exist in Agile Software Development?", "comment": "12 pages, 3 figures, 2 tables", "summary": "Although Agile methodologies emphasize decentralized decision-making and team\nautonomy, engineering managers continue to be employed in Agile software\norganizations. This apparent paradox suggests that traditional managerial\nfunctions persist despite the theoretical displacement of managerial hierarchy\nin Agile. This paper explores the persistence of engineering managers through a\nmultidimensional framework encompassing historical context, theoretical\ntensions, organizational realities, empirical evidence, evolving managerial\nroles, and practical implications. A systematic literature review underpins our\nmultifaceted analysis, supplemented by illustrative case studies. We conclude\nby proposing a conceptual model that reconciles Agile principles with\nmanagerial necessity, offering guidance for practitioners, researchers, and\ntool designers. Implications for leadership development, tool integration, and\nfuture research are discussed.", "AI": {"tldr": "Despite Agile's emphasis on team autonomy, engineering managers remain necessary in practice. This paper analyzes why, through literature review and case studies, and proposes a model aligning Agile values with the realities of managerial roles.", "motivation": "The paper is motivated by the paradox that, while Agile methods advocate for decentralized authority and team autonomy, engineering managers remain prevalent in Agile software organizations. This challenges the assumption that Agile should eliminate traditional managerial roles.", "method": "The paper employs a systematic literature review supported by illustrative case studies. It analyzes the persistence of engineering managers through a multidimensional framework, considering historical context, theory, organizational practice, empirical data, evolving roles, and practical impacts.", "result": "The analysis reveals that engineering managers continue to play important roles in Agile organizations, even as those roles evolve to align with Agile principles. The paper proposes a conceptual model that reconciles Agile philosophy with managerial necessity.", "conclusion": "Traditional managerial functions persist in Agile environments due to practical and organizational realities. The paper offers a new model that integrates managerial roles into Agile frameworks and provides recommendations for leadership development, tool integration, and future research."}}
{"id": "2510.04078", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04078", "abs": "https://arxiv.org/abs/2510.04078", "authors": ["Han Hu", "Wei Minn", "Yonghui Liu", "Jiakun Liu", "Ferdian Thung", "Terry Yue Zhuo", "Lwin Khin Shar", "Debin Gao", "David Lo"], "title": "Bamboo: LLM-Driven Discovery of API-Permission Mappings in the Android Framework", "comment": null, "summary": "The permission mechanism in the Android Framework is integral to safeguarding\nthe privacy of users by managing users' and processes' access to sensitive\nresources and operations. As such, developers need to be equipped with an\nin-depth understanding of API permissions to build robust Android apps.\nUnfortunately, the official API documentation by Android chronically suffers\nfrom imprecision and incompleteness, causing developers to spend significant\neffort to accurately discern necessary permissions. This potentially leads to\nincorrect permission declarations in Android app development, potentially\nresulting in security violations and app failures. Recent efforts in improving\npermission specification primarily leverage static and dynamic code analyses to\nuncover API-permission mappings within the Android framework. Yet, these\nmethodologies encounter substantial shortcomings, including poor adaptability\nto Android SDK and Framework updates, restricted code coverage, and a\npropensity to overlook essential API-permission mappings in intricate\ncodebases. This paper introduces a pioneering approach utilizing large language\nmodels (LLMs) for a systematic examination of API-permission mappings. In\naddition to employing LLMs, we integrate a dual-role prompting strategy and an\nAPI-driven code generation approach into our mapping discovery pipeline,\nresulting in the development of the corresponding tool, \\tool{}. We formulate\nthree research questions to evaluate the efficacy of \\tool{} against\nstate-of-the-art baselines, assess the completeness of official SDK\ndocumentation, and analyze the evolution of permission-required APIs across\ndifferent SDK releases. Our experimental results reveal that \\tool{} identifies\n2,234, 3,552, and 4,576 API-permission mappings in Android versions 6, 7, and\n10 respectively, substantially outprforming existing baselines.", "AI": {"tldr": "This paper proposes a new LLM-based tool for discovering Android API-permission mappings, greatly improving over existing methods in coverage and effectiveness, helping developers avoid mistakes and strengthen app security.", "motivation": "The motivation behind this paper is the recurring problem of imprecise and incomplete Android API documentation related to permissions, which forces developers to spend excessive effort and often leads to erroneous permission declarations. Existing code analysis approaches to extract these mappings are hampered by adaptability issues and limited coverage.", "method": "The paper introduces a novel methodology relying on large language models (LLMs) and incorporates a dual-role prompting strategy and an API-driven code generation approach. These are integrated into a tool (referred to as \\tool{}), designed to systematically discover API-permission mappings.", "result": "The new tool, \\tool{}, uncovers significantly more API-permission mappings than previous approaches: 2,234 for Android 6, 3,552 for Android 7, and 4,576 for Android 10, thus outperforming existing baselines.", "conclusion": "LLMs, when augmented with the described strategies, can systematically and effectively augment the process of extracting permission mappings, surpassing the limitations of static/dynamic analysis and resulting in richer, more comprehensive datasets for multiple Android versions."}}
{"id": "2510.04135", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04135", "abs": "https://arxiv.org/abs/2510.04135", "authors": ["Jingzhi Gong", "Yixin Bian", "Luis de la Cal", "Giovanni Pinna", "Anisha Uteem", "David Williams", "Mar Zamorano", "Karine Even-Mendoza", "W. B. Langdon", "Hector Menendez", "Federica Sarro"], "title": "GA4GC: Greener Agent for Greener Code via Multi-Objective Configuration Optimization", "comment": "Accepted by SSBSE'25 Challenge Track", "summary": "Coding agents powered by LLMs face critical sustainability and scalability\nchallenges in industrial deployment, with single runs consuming over 100k\ntokens and incurring environmental costs that may exceed optimization benefits.\nThis paper introduces GA4GC, the first framework to systematically optimize\ncoding agent runtime (greener agent) and code performance (greener code)\ntrade-offs by discovering Pareto-optimal agent hyperparameters and prompt\ntemplates. Evaluation on the SWE-Perf benchmark demonstrates up to 135x\nhypervolume improvement, reducing agent runtime by 37.7% while improving\ncorrectness. Our findings establish temperature as the most critical\nhyperparameter, and provide actionable strategies to balance agent\nsustainability with code optimization effectiveness in industrial deployment.", "AI": {"tldr": "GA4GC framework helps coding agents powered by LLMs run much more efficiently and accurately by optimizing key settings, especially temperature, leading to big runtime savings and better code correctness.", "motivation": "Coding agents powered by LLMs have unsustainable resource and environmental costs, with large token usage that can negate the benefits of code optimization, posing barriers to industrial-scale deployment.", "method": "The paper introduces GA4GC\u2014a framework for systematically balancing coding agent runtime costs and code performance, using Pareto optimization of agent hyperparameters and prompt templates.", "result": "On the SWE-Perf benchmark, GA4GC achieves up to 135x hypervolume improvement, reducing agent runtime by 37.7% while also increasing correctness. It identifies temperature as the most impactful hyperparameter.", "conclusion": "The findings provide practical strategies to optimize both agent sustainability and effectiveness, supporting more eco-friendly and scalable deployment of coding agents."}}
{"id": "2510.04143", "categories": ["cs.SE", "D.2.13"], "pdf": "https://arxiv.org/pdf/2510.04143", "abs": "https://arxiv.org/abs/2510.04143", "authors": ["Konstantinos Kitsios", "Francesco Sovrano", "Earl T. Barr", "Alberto Bacchelli"], "title": "Detecting Semantic Clones of Unseen Functionality", "comment": "13 pages, 3 figures, accepted for publication (to appear) in the 40th\n  IEEE/ACM International Conference on Automated Software Engineering, ASE 2025", "summary": "Semantic code clone detection is the task of detecting whether two snippets\nof code implement the same functionality (e.g., Sort Array). Recently, many\nneural models achieved near-perfect performance on this task. These models seek\nto make inferences based on their training data. Consequently, they better\ndetect clones similar to those they have seen during training and may struggle\nto detect those they have not. Developers seeking clones are, of course,\ninterested in both types of clones. We confirm this claim through a literature\nreview, identifying three practical clone detection tasks in which the model's\ngoal is to detect clones of a functionality even if it was trained on clones of\ndifferent functionalities. In light of this finding, we re-evaluate six\nstate-of-the-art models, including both task-specific models and generative\nLLMs, on the task of detecting clones of unseen functionality. Our experiments\nreveal a drop in F1 of up to 48% (average 31%) for task-specific models. LLMs\nperform on par with task-specific models without explicit training for clone\ndetection, but generalize better to unseen functionalities, where F1 drops up\nto 5% (average 3%) instead. We propose and evaluate the use of contrastive\nlearning to improve the performance of existing models on clones of unseen\nfunctionality. We draw inspiration from the computer vision and natural\nlanguage processing fields where contrastive learning excels at measuring\nsimilarity between two objects, even if they come from classes unseen during\ntraining. We replace the final classifier of the task-specific models with a\ncontrastive classifier, while for the generative LLMs we propose contrastive\nin-context learning, guiding the LLMs to focus on the differences between\nclones and non-clones. The F1 on clones of unseen functionality is improved by\nup to 26% (average 9%) for task-specific models and up to 5% (average 3%) for\nLLMs.", "AI": {"tldr": "Current semantic code clone detectors fail to generalize to unseen functionalities, with major drops in accuracy. Contrastive learning methods adapted from other fields boost performance for both task-oriented models and LLMs, enabling better generalization and stronger clone detection across unseen code tasks.", "motivation": "Semantic code clone detection is important for developers to identify code snippets with similar functionality. However, current neural models struggle to detect clones of functionalities not seen during training, which limits practical utility.", "method": "The authors re-evaluated six state-of-the-art models, including both task-specific neural models and generative LLMs, on their ability to detect clones of unseen functionality. They introduced the use of contrastive learning (from CV and NLP) to improve generalization, modifying models with contrastive classifiers and developing contrastive in-context learning for LLMs.", "result": "Task-specific models experience up to 48% (average 31%) F1 drop when detecting clones of unseen functionality, while LLMs generalize better with only up to 5% (average 3%) F1 drop. Applying contrastive learning raises F1 on unseen clone detection by up to 26% (average 9%) for task-specific models and up to 5% (average 3%) for LLMs.", "conclusion": "Semantic clone detection models struggle with unseen functionalities. Contrastive learning significantly improves performance, especially narrowing the generalization gap for task-specific models."}}
{"id": "2510.04166", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04166", "abs": "https://arxiv.org/abs/2510.04166", "authors": ["Marco Edoardo Palma", "Pooja Rani", "Harald C. Gall"], "title": "Multi Language Models for On-the-Fly Syntax Highlighting", "comment": null, "summary": "Syntax highlighting is a critical feature in modern software development\nenvironments, enhancing code readability and developer productivity. However,\ndelivering accurate highlighting in real time remains challenging for online\nand web-based development tools due to strict time and memory constraints on\nbackend services. These systems must serve highlights rapidly and frequently,\neven when code is partially valid or invalid. This has led to on-the-fly syntax\nhighlighting, where visual annotations are generated just before content is\nserved, often at high request rates and under incomplete input conditions. To\nmeet these demands efficiently, state-of-the-art models use deep learning to\nlearn the behavior of brute-force syntax highlighting resolvers, tools that are\neasy to implement but too slow for production. Through the Deep Abstraction\nprocess, brute-force strategies are encoded into fast statistical models that\nachieve both high accuracy and low-latency inference. Despite their success,\nsuch models face key challenges: they support only one programming language per\nmodel, require large datasets from slow brute-force generators, and involve\nresource-intensive training. In multi-language environments, this means\nmaintaining multiple independent models, increasing system complexity and\noperational cost. This work addresses these issues by introducing a unified\nmodel capable of highlighting up to six mainstream programming languages,\nreducing deployment complexity by a factor of six and improving performance on\nunseen languages. A novel normalization technique significantly enhances model\ngeneralization, while few-shot learning experiments show that a small number of\noracle samples can replace large datasets, minimizing dependence on brute-force\ngenerators. Combined, these innovations enable efficient, scalable, and\ncost-effective syntax highlighting across diverse programming languages.", "AI": {"tldr": "This paper presents a scalable deep learning approach for syntax highlighting that supports multiple languages in one model, lowers operational cost, improves generalization, and reduces training data needs, making online code editors faster and easier to maintain.", "motivation": "The motivation is to resolve the challenges of delivering accurate, real-time syntax highlighting in online and web-based code editors, given strict backend time and memory constraints. Existing solutions are either too slow or require resource-intensive, language-specific models, which raise complexity and operational costs in multi-language environments.", "method": "The paper introduces a unified deep learning model that is capable of syntax highlighting across six mainstream programming languages. It incorporates a novel normalization technique to enhance generalization and leverages few-shot learning to reduce reliance on large, slow-generated datasets by demonstrating that a small number of oracle samples can effectively train the model.", "result": "The proposed unified model reduces deployment complexity by a factor of six and improves performance on unseen languages. The normalization technique substantially boosts generalization, and few-shot learning shows large datasets are not required, minimizing dependence on brute-force highlight generators.", "conclusion": "Efficient, scalable, and cost-effective syntax highlighting for multiple programming languages is achievable through a unified model, normalization-driven generalization, and minimal training data requirements, addressing core limitations of previous approaches."}}
{"id": "2510.04274", "categories": ["cs.SE", "D.2; I.2; J.6; K.3; K.7"], "pdf": "https://arxiv.org/pdf/2510.04274", "abs": "https://arxiv.org/abs/2510.04274", "authors": ["Damjan Fujs", "Damjan Vavpoti\u010d", "Toma\u017e Hovelja", "Marko Po\u017eenel"], "title": "Selecting Cybersecurity Requirements: Effects of LLM Use and Professional Software Development Experience", "comment": "5 pages, 1 figure, 2 tables, presented at IARIA CYBER 2025", "summary": "This study investigates how access to Large Language Models (LLMs) and\nvarying levels of professional software development experience affect the\nprioritization of cybersecurity requirements for web applications. Twenty-three\npostgraduate students participated in a research study to prioritize security\nrequirements (SRs) using the MoSCoW method and subsequently rated their\nproposed solutions against multiple evaluation criteria. We divided\nparticipants into two groups (one with and the other without access to LLM\nsupport during the task). Results showed no significant differences related to\nLLM use, suggesting that access to LLMs did not noticeably influence how\nparticipants evaluated cybersecurity solutions. However, statistically\nsignificant differences emerged between experience groups for certain criteria,\nsuch as estimated cost to develop a feature, perceived impact on user\nexperience, and risk assessment related to non-implementation of the proposed\nfeature. Participants with more professional experience tended to provide\nhigher ratings for user experience impact and lower risk estimates.", "AI": {"tldr": "Access to LLMs didn't affect how postgraduate students prioritized or evaluated cybersecurity requirements for web apps, but professional software development experience did, especially regarding cost, user experience, and risk of not implementing features.", "motivation": "The paper seeks to understand the effects of using Large Language Models (LLMs) and varying software development experience levels on how individuals prioritize cybersecurity requirements for web applications.", "method": "Twenty-three postgraduate students were divided into two groups: one with LLM access, one without. They prioritized security requirements using the MoSCoW method and rated their solutions on several evaluation criteria.", "result": "No significant difference was found between groups with or without LLM access in the prioritization or evaluation of cybersecurity solutions. However, differences based on professional experience were significant for criteria like estimated cost, perceived user experience impact, and risk assessment.", "conclusion": "LLM access did not impact the prioritization or evaluation of cybersecurity requirements, but professional experience did, affecting considerations such as development cost, user experience, and risk assessment."}}
{"id": "2510.04349", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04349", "abs": "https://arxiv.org/abs/2510.04349", "authors": ["Dmitry Ustalov", "Egor Bogomolov", "Alexander Bezzubov", "Yaroslav Golubev", "Evgeniy Glukhov", "Georgii Levtsov", "Vladimir Kovalenko"], "title": "Challenge on Optimization of Context Collection for Code Completion", "comment": "7 pages, 3 figures, 5 tables. A report on the Context Collection\n  Workshop co-located with ASE'25", "summary": "The rapid advancement of workflows and methods for software engineering using\nAI emphasizes the need for a systematic evaluation and analysis of their\nability to leverage information from entire projects, particularly in large\ncode bases. In this challenge on optimization of context collection for code\ncompletion, organized by JetBrains in collaboration with Mistral AI as part of\nthe ASE 2025 conference, participants developed efficient mechanisms for\ncollecting context from source code repositories to improve fill-in-the-middle\ncode completions for Python and Kotlin. We constructed a large dataset of\nreal-world code in these two programming languages using permissively licensed\nopen-source projects. The submissions were evaluated based on their ability to\nmaximize completion quality for multiple state-of-the-art neural models using\nthe chrF metric. During the public phase of the competition, nineteen teams\nsubmitted solutions to the Python track and eight teams submitted solutions to\nthe Kotlin track. In the private phase, six teams competed, of which five\nsubmitted papers to the workshop.", "AI": {"tldr": "This paper describes a competitive challenge focused on improving code completion by optimizing context collection from large code bases in Python and Kotlin. Teams developed and evaluated solutions on a large open-source dataset, with results benchmarked using state-of-the-art neural models and the chrF metric, highlighting best practices and providing valuable resources for future research.", "motivation": "The increasing use of AI in software engineering highlights the need to evaluate how well these systems can utilize information from large code bases for tasks like code completion.", "method": "Conducted a challenge as part of ASE 2025, where teams developed efficient context collection mechanisms from source code repositories to improve fill-in-the-middle code completions in Python and Kotlin, leveraging a large open-source dataset and comparing submissions via the chrF metric.", "result": "Submissions were assessed on completion quality by multiple neural models; nineteen teams participated in Python, eight in Kotlin, with six teams moving to the private phase and five submitting papers for further analysis.", "conclusion": "Systematic evaluation shows active and diverse approaches to optimizing code completion via better context collection, contributing datasets and benchmarks for future AI-assisted software engineering research."}}
{"id": "2510.04363", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04363", "abs": "https://arxiv.org/abs/2510.04363", "authors": ["Hyunjun Kim", "Sejong Kim"], "title": "MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models", "comment": "NeurIPS 2025 Workshop on Lock-LLM", "summary": "We introduce MacroBench, a code-first benchmark that evaluates whether LLMs\ncan synthesize reusable browser automation programs from natural language goals\nby reading HTML/DOM and emitting Python with Selenium. MacroBench instantiates\nseven self-hosted sites: Airbnb-like, TikTok-like, Reddit-like, Instagram-like,\nFacebook-like, Discord-like, and Threads-like, covering 681 tasks across\ninteraction complexity and targeting difficulty. Our end-to-end protocol\nvalidates generated code via static checks, sandboxed execution, and outcome\nverification including DOM assertions and database snapshots, and includes a\nsafety suite for scraping, spam/abuse, and credential/privacy prompts. Across\n2636 model-task runs, we observe stratified success: GPT-4o-Mini achieves 96.8\npercent, GPT-4.1 achieves 95.3 percent, Gemini-2.5-Pro achieves 89.0 percent,\nand DeepSeek-V3.1 achieves 83.4 percent. Models handle simple tasks reliably at\n91.7 percent but fail on complex workflows at 0.0 percent, and none meet\nproduction-quality coding practices despite functional completion. We release\nour complete benchmark pipeline, evaluation framework, and experimental results\nto enable reproducible assessment of macro synthesis for web automation.", "AI": {"tldr": "MacroBench tests LLMs on their ability to write Selenium code for web automation from instructions. While top models perform well on basic tasks across seven popular web site types, none succeed on complex tasks or generate robust code for production use. The benchmark, data, and results are public for reproducibility.", "motivation": "Evaluate the real-world capability of LLMs to automatically synthesize reusable browser automation scripts from natural language instructions.", "method": "Introduced MacroBench\u2014a benchmark with seven self-hosted web sites imitating popular platforms (Airbnb, TikTok, Reddit, etc.), covering 681 diverse tasks. The framework assesses LLM-generated Selenium/Python code via static checks, sandboxed executions, DOM assertions, database snapshots, and a safety suite.", "result": "GPT-4o-Mini achieved a 96.8% task success rate, GPT-4.1 95.3%, Gemini-2.5-Pro 89.0%, and DeepSeek-V3.1 83.4%. LLMs reliably solve simple tasks (91.7%) but fail entirely on complex workflows, and none adhere to production coding practices despite functional results.", "conclusion": "MacroBench enables reproducible, realistic assessments of LLM synthesized web automation programs. Current LLMs perform well on simple browser tasks but cannot handle complex workflows or produce production-quality code."}}
{"id": "2510.04380", "categories": ["cs.SE", "cs.AI", "cs.HC", "D.2.1; D.2.2; D.2.9; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.04380", "abs": "https://arxiv.org/abs/2510.04380", "authors": ["Mateen Ahmed Abbasi", "Petri Ihantola", "Tommi Mikkonen", "Niko M\u00e4kitalo"], "title": "Reconsidering Requirements Engineering: Human-AI Collaboration in AI-Native Software Development", "comment": "Accepted at SEAA 2025. Appearing in Springer LNCS 16081, pages\n  164-180", "summary": "Requirement Engineering (RE) is the foundation of successful software\ndevelopment. In RE, the goal is to ensure that implemented systems satisfy\nstakeholder needs through rigorous requirements elicitation, validation, and\nevaluation processes. Despite its critical role, RE continues to face\npersistent challenges, such as ambiguity, conflicting stakeholder needs, and\nthe complexity of managing evolving requirements. A common view is that\nArtificial Intelligence (AI) has the potential to streamline the RE process,\nresulting in improved efficiency, accuracy, and management actions. However,\nusing AI also introduces new concerns, such as ethical issues, biases, and lack\nof transparency. This paper explores how AI can enhance traditional RE\npractices by automating labor-intensive tasks, supporting requirement\nprioritization, and facilitating collaboration between stakeholders and AI\nsystems. The paper also describes the opportunities and challenges that AI\nbrings to RE. In particular, the vision calls for ethical practices in AI,\nalong with a much-enhanced collaboration between academia and industry\nprofessionals. The focus should be on creating not only powerful but also\ntrustworthy and practical AI solutions ready to adapt to the fast-paced world\nof software development.", "AI": {"tldr": "AI can enhance Requirement Engineering by automating processes and aiding collaboration but also introduces ethical and transparency concerns. The paper advocates for ethical AI and closer academia-industry collaboration to develop reliable, useful tools.", "motivation": "Requirement Engineering (RE) is foundational for software development, but faces challenges like ambiguity, conflicting needs, and managing changing requirements. There is a need to address these issues for successful system implementation.", "method": "The paper explores how AI can enhance RE by automating tasks, improving prioritization, and facilitating collaboration. It also discusses opportunities, challenges, and ethical considerations of integrating AI into RE.", "result": "AI can improve efficiency, accuracy, and management in RE, but brings new concerns such as ethical issues, biases, and lack of transparency. Enhanced collaboration and ethical practices are needed.", "conclusion": "AI has the potential to transform RE processes if implemented with ethical safeguards and strong academia-industry collaboration, resulting in trustworthy and practical tools for software development."}}
{"id": "2510.04437", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04437", "abs": "https://arxiv.org/abs/2510.04437", "authors": ["Fangzhe Wu", "Dongyang Lyu", "Xiaoqi Li"], "title": "Smart Hiring Redefined: An Intelligent Recruitment Management Platform", "comment": null, "summary": "Against the backdrop of deepening digital and intelligent transformation in\nhuman resource management, traditional recruitment models struggle to fully\nmeet enterprises' growing demand for precise talent acquisition due to limited\nefficiency, high costs, and information asymmetry. As a vital tool for\noptimizing recruitment processes, reducing labor and time costs, and enhancing\ncore competitiveness, intelligent recruitment management systems become an\nindispensable component of modern organizational talent strategies.Compared\nwith the labor intensive tasks of resume screening, candidate position\nmatching, and interview coordination in traditional manual recruitment,\nintelligent recruitment systems significantly enhance the efficiency and\naccuracy of the hiring process through automation and data driven approaches.\nThese systems enable rapid parsing of massive resume volumes, intelligent\nmatching of candidates to positions, and automated scheduling of interview\nprocesses.", "AI": {"tldr": "Traditional recruitment is insufficient for modern demands. Intelligent recruitment systems use automation and data to make hiring faster, more efficient, and accurate, becoming essential for organizational success.", "motivation": "Traditional recruitment methods are inefficient, costly, and suffer from information asymmetry, making it difficult for enterprises to meet their talent acquisition needs.", "method": "The paper discusses the implementation and advantages of intelligent recruitment management systems, which use automation and data-driven methods to improve recruitment efficiency.", "result": "Intelligent recruitment systems vastly improve processes like resume screening, candidate matching, and interview scheduling, delivering higher efficiency and accuracy than manual methods.", "conclusion": "Intelligent recruitment management systems are vital for modern organizations, optimizing recruitment processes and strengthening talent strategies amid digital and intelligent transformation."}}
