{"id": "2506.22370", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2506.22370", "abs": "https://arxiv.org/abs/2506.22370", "authors": ["Carolina Carreira", "\u00c1lvaro Silva", "Alexandre Abreu", "Alexandra Mendes"], "title": "Can Large Language Models Help Students Prove Software Correctness? An Experimental Study with Dafny", "comment": null, "summary": "Students in computing education increasingly use large language models (LLMs)\nsuch as ChatGPT. Yet, the role of LLMs in supporting cognitively demanding\ntasks, like deductive program verification, remains poorly understood. This\npaper investigates how students interact with an LLM when solving formal\nverification exercises in Dafny, a language that supports functional\ncorrectness, by allowing programmers to write formal specifications and\nautomatically verifying that the implementation satisfies the specification. We\nconducted a mixed-methods study with master's students enrolled in a formal\nmethods course. Each participant completed two verification problems, one with\naccess to a custom ChatGPT interface, that logged all interactions, and the\nother without. We identified strategies used by successful students and\nassessed the level of trust students place in LLMs. %\\todo{Our findings show\nthat something here} Our findings show that students perform significantly\nbetter when using ChatGPT; however, performance gains are tied to prompt\nquality. We conclude with practical recommendations for integrating LLMs into\nformal methods courses more effectively, including designing LLM-aware\nchallenges that promote learning rather than substitution.", "AI": {"tldr": "Access to ChatGPT significantly enhances students' performance on formal verification problems if they use high-quality prompts. The paper provides insights and recommendations for leveraging LLMs in computing education effectively.", "motivation": "Students in computing education are increasingly using large language models (LLMs) like ChatGPT, but their effectiveness in supporting complex tasks such as deductive program verification is not well understood.", "method": "A mixed-methods study was conducted with master's students in a formal methods course. Each participant tackled two Dafny verification problems\u2014one with a custom ChatGPT interface that logged interactions and one without LLM access. Strategies and trust levels were analyzed.", "result": "Students performed significantly better when using ChatGPT, but performance improvements depended on the quality of their prompts. Successful student strategies and levels of trust in LLMs were identified.", "conclusion": "LLMs can support student performance in formal verification tasks, but their benefit depends on how well students engage with them. Recommendations are provided for educators to integrate LLMs into courses in ways that enhance learning instead of simply substituting student effort."}}
{"id": "2506.21634", "categories": ["cs.SE", "D.2.0; A.m; K.m"], "pdf": "https://arxiv.org/pdf/2506.21634", "abs": "https://arxiv.org/abs/2506.21634", "authors": ["Lutz Prechelt", "Lloyd Montgomery", "Julian Frattini", "Franz Zieris"], "title": "How (Not) To Write a Software Engineering Abstract", "comment": "16 pages, 11 figures, 2 tables", "summary": "Background: Abstracts are a particularly valuable element in a software\nengineering research article. However, not all abstracts are as informative as\nthey could be. Objective: Characterize the structure of abstracts in\nhigh-quality software engineering venues. Observe and quantify deficiencies.\nSuggest guidelines for writing informative abstracts. Methods: Use qualitative\nopen coding to derive concepts that explain relevant properties of abstracts.\nIdentify the archetypical structure of abstracts. Use quantitative content\nanalysis to objectively characterize abstract structure of a sample of 362\nabstracts from five presumably high-quality venues. Use exploratory data\nanalysis to find recurring issues in abstracts. Compare the archetypical\nstructure to actual structures. Infer guidelines for producing informative\nabstracts. Results: Only 29% of the sampled abstracts are complete, i.e.,\nprovide background, objective, method, result, and conclusion information. For\nstructured abstracts, the ratio is twice as big. Only 4% of the abstracts are\nproper, i.e., they also have good readability (Flesch-Kincaid score) and have\nno informativeness gaps, understandability gaps, nor highly ambiguous\nsentences. Conclusions: (1) Even in top venues, a large majority of abstracts\nare far from ideal. (2) Structured abstracts tend to be better than\nunstructured ones. (3) Artifact-centric works need a different structured\nformat. (4) The community should start requiring conclusions that generalize,\nwhich currently are often missing in abstracts.", "AI": {"tldr": "The paper finds that most software engineering abstracts\u2014even in top venues\u2014lack completeness and informativeness. Structured abstracts fare better than unstructured ones. The authors recommend new guidelines for writing more informative, readable abstracts, calling for required generalizing conclusions and specialized formats for certain research types.", "motivation": "Abstracts are a crucial component of software engineering research articles, but many are insufficiently informative. There is a need to characterize the ideal structure of abstracts in high-quality venues and identify common deficiencies.", "method": "The study used qualitative open coding to derive concepts explaining abstract properties and identified an archetypical structure for abstracts. It quantitatively analyzed the content of 362 abstracts from five high-quality venues, performed exploratory data analysis to detect recurring issues, compared real abstracts\u2019 structures to the archetype, and inferred guidelines for more informative abstracts.", "result": "Only 29% of sampled abstracts were complete (providing background, objective, method, result, and conclusion). This rate was twice as high for structured abstracts. Just 4% were deemed proper, showing good readability and no gaps in informativeness or understandability. Structured abstracts outperformed unstructured ones; artifact-centric papers need a distinct format.", "conclusion": "Most abstracts, even in top venues, do not meet ideal standards\u2014being neither complete nor proper. Structured abstracts generally perform better, yet current practices often lack required generalizing conclusions. The community should encourage better structures and conclusions in abstracts, with different formats for artifact-centric research."}}
{"id": "2506.21654", "categories": ["cs.SE", "cs.MS"], "pdf": "https://arxiv.org/pdf/2506.21654", "abs": "https://arxiv.org/abs/2506.21654", "authors": ["Wolfgang Bangerth"], "title": "Experience converting a large mathematical software package written in C++ to C++20 modules", "comment": null, "summary": "Mathematical software has traditionally been built in the form of \"packages\"\nthat build on each other. A substantial fraction of these packages is written\nin C++ and, as a consequence, the interface of a package is described in the\nform of header files that downstream packages and applications can then\n#include. C++ has inherited this approach towards exporting interfaces from C,\nbut the approach is clunky, unreliable, and slow. As a consequence, C++20 has\nintroduced a \"module\" system in which packages explicitly export declarations\nand code that compilers then store in machine-readable form and that downstream\nusers can \"import\" -- a system in line with what many other programming\nlanguages have used for decades.\n  Herein, I explore how one can convert large mathematical software packages\nwritten in C++ to this system, using the deal.II finite element library with\nits around 800,000 lines of code as an example. I describe an approach that\nallows providing both header-based and module-based interfaces from the same\ncode base, discuss the challenges one encounters, and how modules actually work\nin practice in a variety of technical and human metrics. The results show that\nwith a non-trivial, but also not prohibitive effort, the conversion to modules\nis possible, resulting in a reduction in compile time for the converted library\nitself; on the other hand, for downstream projects, compile times show no clear\ntrend. I end with thoughts about long-term strategies for converting the entire\necosystem of mathematical software over the coming years or decades.", "AI": {"tldr": "Converting large C++ mathematical software to C++20 modules improves library compile times, but downstream impacts vary. Transition is practical but requires careful, sustained effort.", "motivation": "Mathematical software packages traditionally use C++ header files to define interfaces. This approach, inherited from C, is problematic due to being clunky, unreliable, and slow. The introduction of the C++20 module system aims to address these issues, but practical strategies and outcomes for converting large, real-world mathematical software to modules remain to be explored.", "method": "The author uses the deal.II finite element library (about 800,000 lines of code) as a case study. They describe methods to provide both header-based and module-based interfaces from the same codebase, detail the migration process, and evaluate the practical challenges and effects on both human and technical metrics.", "result": "The conversion from header files to modules is achievable with a significant, yet manageable, effort. Compile times for the converted library decreased, but compile times for downstream projects did not show a consistent improvement.", "conclusion": "Migrating large C++ mathematical software packages to the C++20 module system is feasible and beneficial for the packages themselves, primarily by reducing compile times. However, benefits for downstream projects are less clear, and a gradual, long-term transition for the broader ecosystem is recommended."}}
{"id": "2506.21693", "categories": ["cs.SE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.21693", "abs": "https://arxiv.org/abs/2506.21693", "authors": ["Ali Nouri", "Beatriz Cabrero-Daniel", "Fredrik T\u00f6rner", "Christian Berger"], "title": "The DevSafeOps Dilemma: A Systematic Literature Review on Rapidity in Safe Autonomous Driving Development and Operation", "comment": "Accepted for publication in the Journal of Systems and Software (JSS)", "summary": "Developing autonomous driving (AD) systems is challenging due to the\ncomplexity of the systems and the need to assure their safe and reliable\noperation. The widely adopted approach of DevOps seems promising to support the\ncontinuous technological progress in AI and the demand for fast reaction to\nincidents, which necessitate continuous development, deployment, and\nmonitoring. We present a systematic literature review meant to identify,\nanalyse, and synthesise a broad range of existing literature related to usage\nof DevOps in autonomous driving development. Our results provide a structured\noverview of challenges and solutions, arising from applying DevOps to\nsafety-related AI-enabled functions. Our results indicate that there are still\nseveral open topics to be addressed to enable safe DevOps for the development\nof safe AD.", "AI": {"tldr": "This paper systematically reviews literature on using DevOps in autonomous driving, highlighting both current solutions and persistent challenges, particularly around handling safety in AI-driven systems. Significant issues still need resolution to achieve truly safe DevOps in this field.", "motivation": "Developing autonomous driving systems is highly complex and requires assurance of safety and reliability. Traditional development approaches struggle to keep up with continuous AI advancements and the need for rapid response to incidents. This drives interest in DevOps as a continuous development, deployment, and monitoring methodology for autonomous driving.", "method": "The authors conducted a systematic literature review focused on identifying, analyzing, and synthesizing existing studies related to the use of DevOps for autonomous driving system development, especially in safety-related, AI-enabled contexts.", "result": "The review provides a structured summary of current challenges and solutions associated with adopting DevOps for safety-critical, AI-enabled autonomous driving functions. It identifies several unresolved issues that hinder the full realization of safe and reliable DevOps practices in this domain.", "conclusion": "While DevOps offers promise for improving autonomous driving system development, especially in terms of rapid progress and reliable operation, there remain significant open challenges concerning its safe application to AI-enabled safety-critical functions."}}
{"id": "2506.21703", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.21703", "abs": "https://arxiv.org/abs/2506.21703", "authors": ["Victoria Jackson", "Susannah Liu", "Andre van der Hoek"], "title": "Using Generative AI in Software Design Education: An Experience Report", "comment": "12 pages, 1 figure", "summary": "With the rapid adoption of Generative AI (GenAI) tools, software engineering\neducators have grappled with how best to incorporate them into the classroom.\nWhile some research discusses the use of GenAI in the context of learning to\ncode, there is little research that explores the use of GenAI in the classroom\nfor other areas of software development. This paper provides an experience\nreport on introducing GenAI into an undergraduate software design class.\nStudents were required to use GenAI (in the form of ChatGPT) to help complete a\nteam-based assignment. The data collected consisted of the ChatGPT conversation\nlogs and students' reflections on using ChatGPT for the assignment.\nSubsequently, qualitative analysis was undertaken on the data. Students\nidentified numerous ways ChatGPT helped them in their design process while\nrecognizing the need to critique the response before incorporating it into\ntheir design. At the same time, we identified several key lessons for educators\nin how to deploy GenAI in a software design class effectively. Based on our\nexperience, we believe students can benefit from using GenAI in software design\neducation as it helps them design and learn about the strengths and weaknesses\nof GenAI.", "AI": {"tldr": "This paper reports on the integration of ChatGPT into an undergraduate software design class, showing it helps students with design while teaching them to critically appraise AI-generated content. Key lessons were identified for educators on effective GenAI deployment.", "motivation": "There is growing use of Generative AI (GenAI) tools in software engineering, but limited research on their integration into areas beyond coding, such as software design education.", "method": "An experience report was conducted in which undergraduate students were required to use ChatGPT during a team-based software design assignment. Data from ChatGPT conversation logs and students\u2019 reflections were collected and analyzed qualitatively.", "result": "Students found several benefits in using ChatGPT during their design process, but also emphasized the importance of critically evaluating AI-generated responses before using them in their work. The study identified key lessons for educators regarding the effective integration of GenAI in software design classes.", "conclusion": "GenAI tools like ChatGPT can be beneficial in software design education by assisting students in design tasks and exposing them to both the strengths and limitations of generative AI. Successful classroom adoption requires guidance to ensure critical engagement with AI outputs."}}
{"id": "2506.22037", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.22037", "abs": "https://arxiv.org/abs/2506.22037", "authors": ["Jiawei Li", "Zan Liang", "Guoxin Wang", "Jinzhi Lu", "Yan Yan", "Shouxuan Wu", "Hao Wang"], "title": "KARMA Approach supporting Development Process Reconstruction in Model-based Systems Engineering", "comment": "12 pages, 9 figures, submitted to the 15th international Complex\n  Systems Design & Management (CSD&M) conference", "summary": "Model reconstruction is a method used to drive the development of complex\nsystem development processes in model-based systems engineering. Currently,\nduring the iterative design process of a system, there is a lack of an\neffective method to manage changes in development requirements, such as\ndevelopment cycle requirements and cost requirements, and to realize the\nreconstruction of the system development process model. To address these\nissues, this paper proposes a model reconstruction method to support the\ndevelopment process model. Firstly, the KARMA language, based on the GOPPRR-E\nmetamodeling method, is utilized to uniformly formalize the process models\nconstructed based on different modeling languages. Secondly, a model\nreconstruction framework is introduced. This framework takes a structured\ndevelopment requirements based natural language as input, employs natural\nlanguage processing techniques to analyze the development requirements text,\nand extracts structural and optimization constraint information. Then, after\nstructural reorganization and algorithm optimization, a development process\nmodel that meets the development requirements is obtained. Finally, as a case\nstudy, the development process of the aircraft onboard maintenance system is\nreconstructed. The results demonstrate that this method can significantly\nenhance the design efficiency of the development process.", "AI": {"tldr": "This paper presents a novel method that uses natural language processing and metamodeling to formalize and reconstruct system development process models in response to changing requirements, enhancing efficiency as demonstrated with an aircraft maintenance system case study.", "motivation": "There is a lack of effective methods to manage changes in development requirements and to reconstruct system development process models during iterative design in model-based systems engineering.", "method": "The paper utilizes the KARMA language based on the GOPPRR-E metamodeling method to formalize process models uniformly. It introduces a model reconstruction framework where structured development requirements in natural language are analyzed using natural language processing, extracting structural and optimization constraints, followed by structural reorganization and algorithm optimization to generate an updated process model. A case study on the aircraft onboard maintenance system development process is presented.", "result": "The method significantly enhances the design efficiency of the system development process model.", "conclusion": "The proposed model reconstruction method effectively manages evolving requirements and reconstructs development process models, supporting efficient model-based systems engineering."}}
{"id": "2506.22185", "categories": ["cs.SE", "cs.AI", "cs.DC", "cs.NI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.22185", "abs": "https://arxiv.org/abs/2506.22185", "authors": ["Matteo Esposito", "Alexander Bakhtin", "Noman Ahmad", "Mikel Robredo", "Ruoyu Su", "Valentina Lenarduzzi", "Davide Taibi"], "title": "Autonomic Microservice Management via Agentic AI and MAPE-K Integration", "comment": null, "summary": "While microservices are revolutionizing cloud computing by offering\nunparalleled scalability and independent deployment, their decentralized nature\nposes significant security and management challenges that can threaten system\nstability. We propose a framework based on MAPE-K, which leverages agentic AI,\nfor autonomous anomaly detection and remediation to address the daunting task\nof highly distributed system management. Our framework offers practical,\nindustry-ready solutions for maintaining robust and secure microservices.\nPractitioners and researchers can customize the framework to enhance system\nstability, reduce downtime, and monitor broader system quality attributes such\nas system performance level, resilience, security, and anomaly management,\namong others.", "AI": {"tldr": "Microservices improve scalability but complicate management and security. The authors introduce a MAPE-K and agentic AI-based framework for automated anomaly detection and remediation, offering practical tools to boost system stability and security in distributed environments.", "motivation": "Microservices, while transformative for cloud computing, introduce complex security and management issues due to their decentralized nature, making system stability difficult to maintain.", "method": "The paper proposes a framework utilizing the MAPE-K architectural model and agentic AI for autonomous anomaly detection and remediation in microservices environments.", "result": "The proposed framework provides customizable and industry-ready solutions for system stability, reduced downtime, and improved monitoring of various system quality attributes (including security and resilience) in microservices.", "conclusion": "By applying MAPE-K and agentic AI to microservices management, the framework enables practitioners and researchers to autonomously detect and respond to anomalies, thus ensuring robust and secure cloud systems."}}
{"id": "2506.22390", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.22390", "abs": "https://arxiv.org/abs/2506.22390", "authors": ["Ramtin Ehsani", "Sakshi Pathak", "Esteban Parra", "Sonia Haiduc", "Preetha Chatterjee"], "title": "What Makes ChatGPT Effective for Software Issue Resolution? An Empirical Study of Developer-ChatGPT Conversations in GitHub", "comment": null, "summary": "Conversational large-language models are extensively used for issue\nresolution tasks. However, not all developer-LLM conversations are useful for\neffective issue resolution. In this paper, we analyze 686 developer-ChatGPT\nconversations shared within GitHub issue threads to identify characteristics\nthat make these conversations effective for issue resolution. First, we analyze\nthe conversations and their corresponding issues to distinguish helpful from\nunhelpful conversations. We begin by categorizing the types of tasks developers\nseek help with to better understand the scenarios in which ChatGPT is most\neffective. Next, we examine a wide range of conversational, project, and\nissue-related metrics to uncover factors associated with helpful conversations.\nFinally, we identify common deficiencies in unhelpful ChatGPT responses to\nhighlight areas that could inform the design of more effective developer-facing\ntools. We found that only 62% of the ChatGPT conversations were helpful for\nsuccessful issue resolution. ChatGPT is most effective for code generation and\ntools/libraries/APIs recommendations, but struggles with code explanations.\nHelpful conversations tend to be shorter, more readable, and exhibit stronger\nsemantic and linguistic alignment. Larger, more popular projects and more\nexperienced developers benefit more from ChatGPT. At the issue level, ChatGPT\nperforms best on simpler problems with limited developer activity and faster\nresolution, typically well-scoped tasks like compilation errors. The most\ncommon deficiencies in unhelpful ChatGPT responses include incorrect\ninformation and lack of comprehensiveness. Our findings have wide implications\nincluding guiding developers on effective interaction strategies for issue\nresolution, informing the development of tools or frameworks to support optimal\nprompt design, and providing insights on fine-tuning LLMs for issue resolution\ntasks.", "AI": {"tldr": "Analysis of 686 GitHub developer-ChatGPT conversations shows ChatGPT excels at code gen and API suggestions, but not explanations. 62% of interactions aid issue resolution; shortcomings are often due to incorrect or incomplete info. Findings can help design better LLM tools and inform developer usage strategies.", "motivation": "Despite the growing use of conversational LLMs for developer support, not all interactions with models like ChatGPT are beneficial for resolving issues. This paper seeks to understand what makes some developer-LLM conversations more effective than others for practical issue resolution.", "method": "The authors collected and analyzed 686 developer-ChatGPT conversations from GitHub issue threads. They categorized types of tasks, compared helpful versus unhelpful conversations, explored relevant project and issue metrics, and identified deficiencies in ineffective ChatGPT responses.", "result": "62% of ChatGPT conversations were deemed helpful for issue resolution. ChatGPT was most effective for code generation and API/tool recommendations but less useful for code explanations. Helpful conversations were generally shorter, more readable, and better aligned in language. Benefit was higher in larger, popular projects and with experienced developers, especially for simpler, well-scoped issues. Unhelpful responses often contained incorrect or incomplete information.", "conclusion": "The study reveals that while ChatGPT can significantly aid in issue resolution, its effectiveness is variable. Improving ChatGPT's response accuracy and comprehensiveness, along with guiding developers on prompt design, can enhance outcomes. These insights are valuable for both LLM tool design and developer strategies in leveraging conversational AI."}}
