<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 11]
- [cs.PL](#cs.PL) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Decision Models for Selecting Architecture Patterns and Strategies in Quantum Software Systems](https://arxiv.org/abs/2507.11671)
*Mst Shamima Aktar,Peng Liang,Muhammad Waseem,Amjed Tahir,Mojtaba Shahin,Muhammad Azeem Akbar,Arif Ali Khan,Aakash Ahmad,Musengamana Jean de Dieu,Ruiyin Li*

Main category: cs.SE

TL;DR: The paper proposes and validates decision models to help quantum software developers choose appropriate architectural patterns and strategies, making quantum software design easier and more effective.


<details>
  <summary>Details</summary>
Motivation: Quantum software development is complex and lacks guidelines for selecting appropriate architectural patterns and strategies. Practitioners face significant challenges due to the novelty and intricacy of quantum software systems.

Method: The study constructs decision models for six key design areas in quantum software by mining GitHub and Stack Exchange data and conducting a Systematic Literature Review to identify patterns, strategies, and quality attributes. These models are evaluated through semi-structured interviews with 16 practitioners.

Result: The proposed decision models help practitioners select suitable architectural patterns and strategies, making quantum software design more effective. The models were found to be familiar, understandable, complete, and useful by the practitioners, and the dataset is publicly available for further research.

Conclusion: Decision models based on empirical data and literature can address the challenge of pattern and strategy selection in quantum software architecture, supporting practitioners in building more efficient and effective quantum software systems.

Abstract: Quantum software represents disruptive technologies in terms of
quantum-specific software systems, services, and applications - leverage the
principles of quantum mechanics via programmable quantum bits (Qubits) that
manipulate quantum gates (QuGates) - to achieve quantum supremacy in computing.
Quantum software architecture enables quantum software developers to abstract
away implementation-specific details (i.e., mapping of Qubits and QuGates to
high-level architectural components and connectors). Architectural patterns and
strategies can provide reusable knowledge and best practices to engineer
quantum software systems effectively and efficiently. However, quantum software
practitioners face significant challenges in selecting and implementing
appropriate patterns and strategies due to the complexity of quantum software
systems and the lack of guidelines. To address these challenges, this study
proposes decision models for selecting patterns and strategies in six critical
design areas in quantum software systems: Communication, Decomposition, Data
Processing, Fault Tolerance, Integration and Optimization, and Algorithm
Implementation. These decision models are constructed based on data collected
from both a mining study (i.e., GitHub and Stack Exchange) and a Systematic
Literature Review, which were used to identify relevant patterns and strategies
with their involved Quality Attributes (QAs). We then conducted semi-structured
interviews with 16 quantum software practitioners to evaluate the familiarity,
understandability, completeness, and usefulness of the proposed decision
models. The results show that the proposed decision models can aid
practitioners in selecting suitable patterns and strategies to address the
challenges related to the architecture design of quantum software systems. The
dataset is available at [6], allowing the community to reproduce and build upon
our findings.

</details>


### [2] [MetaLint: Generalizable Idiomatic Code Quality Analysis through Instruction-Following and Easy-to-Hard Generalization](https://arxiv.org/abs/2507.11687)
*Atharva Naik,Lawanya Baghel,Dhakshin Govindarajan,Darsh Agrawal,Daniel Fried,Carolyn Rose*

Main category: cs.SE

TL;DR: MetaLint is a new framework that improves code quality analysis in language models by enabling adaptive reasoning on evolving code patterns, achieving strong results on benchmarks compared to larger models without needing retraining.


<details>
  <summary>Details</summary>
Motivation: Large Language Models are effective at code generation but perform poorly in code quality analysis due to their reliance on static training data, which limits their adaptability to new or evolving coding best practices.

Method: The authors introduce MetaLint, an instruction-following framework that formulates code quality analysis as a task of identifying and fixing problematic code patterns based on high-level specifications. MetaLint uses instruction tuning on synthetic, linter-generated data to help models generalize from simple to complex tasks, without requiring retraining on static rule sets.

Result: MetaLint-trained models demonstrated improved generalization to unseen coding idioms from real-world standards like Python Enhancement Proposals (PEPs), achieving a 70.37% F-score and the highest recall (70.43%) in idiom detection. For problem localization, it achieved a 26.73% score, which is competitive for its model size and comparable to larger models.

Conclusion: MetaLint enables models to adapt to novel and complex code quality issues, outperforming traditional, static approaches and showing promise for adaptable, future-proof code analysis with competitive results even at smaller model sizes.

Abstract: Large Language Models, though successful in code generation, struggle with
code quality analysis because they are limited by static training data and
can't easily adapt to evolving best practices. We introduce MetaLint, a new
instruction-following framework that formulates code quality analysis as the
task of detecting and fixing problematic semantic code fragments or code idioms
based on high-level specifications. Unlike conventional approaches that train
models on static, rule-based data, MetaLint employs instruction tuning on
synthetic linter-generated data to support easy-to-hard generalization,
enabling models to adapt to novel or complex code patterns without retraining.
To evaluate this, we construct a benchmark of challenging idioms inspired by
real-world coding standards such as Python Enhancement Proposals (PEPs) and
assess whether MetaLint-trained models reason adaptively or simply memorize.
Our results show that MetaLint improves generalization to unseen PEP idioms,
achieving a 70.37% F-score on idiom detection with the highest recall (70.43%)
among all evaluated models. It also achieves 26.73% on localization,
competitive for its 4B parameter size and comparable to larger state-of-the-art
models like o3-mini, highlighting its potential for future-proof code quality
analysis.

</details>


### [3] [REST in Pieces: RESTful Design Rule Violations in Student-Built Web Apps](https://arxiv.org/abs/2507.11689)
*Sergio Di Meglio,Valeria Pontillo,Luigi Libero Lucio Starace*

Main category: cs.SE

TL;DR: Most CS students struggle with REST API best practices, as seen in frequent rule violations in their web projects. This highlights a need for enhanced API design teaching and more automation in code assessment to improve student readiness for industry standards.


<details>
  <summary>Details</summary>
Motivation: Software quality, especially regarding REST API design, is often overlooked in undergraduate computer science programs due to constraints like limited instruction time and emphasis on basic programming skills. This can leave students underprepared for real-world software development standards and industry expectations.

Method: The study analyzed 40 full-stack web applications created by third-year computer science students in a Web Technologies course. An automated static analysis tool was used to evaluate the applications' adherence to established REST API design rules.

Result: The analysis found widespread violations of basic API design conventions among student projects: 98% had incorrect endpoint path formatting (missing hyphens), 88% exhibited improper pluralization, and 83% misused HTTP methods. This demonstrates significant gaps in students' API design skills.

Conclusion: The study concludes there is a strong need for more comprehensive and targeted instruction on API design principles in CS curricula. Additionally, integrating automated analysis tools in student projects can help elevate code quality and better align student skills with industry expectations.

Abstract: In Computer Science Bachelor's programs, software quality is often
underemphasized due to limited time and a focus on foundational skills, leaving
many students unprepared for industry expectations. To better understand the
typical quality of student code and inform both education and hiring practices,
we analyze 40 full-stack web applications developed in a third-year Web
Technologies course. Using an automated static analysis pipeline, we assess
adherence to REST API design rules. Results reveal frequent violations of
foundational conventions, such as missing hyphens in endpoint paths (98%),
incorrect pluralization (88%), and misuse of HTTP methods (83%). These findings
highlight the need for more focused instruction on API design and support the
adoption of automated tools to improve code quality in student projects.

</details>


### [4] [Extremal Testing for Network Software using LLMs](https://arxiv.org/abs/2507.11898)
*Rathin Singha,Harry Qian,Srinath Saikrishnan,Tracy Zhao,Ryan Beckett,Siva Kesava Reddy Kakarla,George Varghese*

Main category: cs.SE

TL;DR: The paper proposes and demonstrates that using LLMs to automate extremal network software testing is easy, effective, and uncovers new bugs, improving on existing boundary value approaches and pointing to further automation with agentic AI.


<details>
  <summary>Details</summary>
Motivation: Physicists, when testing theories, typically consider extreme scenarios manually. The paper seeks to automate such 'extremal testing' in network software, improving and extending traditional methods like Boundary Value Analysis.

Method: The authors use Large Language Models (LLMs) to generate input constraints for network protocols and then to create test cases that violate these constraints. This methodology is demonstrated with HTTP, BGP, and DNS implementations and is also applied to algorithms like shortest path. The paper discusses the potential for agent-based AI to further automate the process.

Result: The LLM-driven extremal tests successfully uncovered new bugs in implementations of HTTP, BGP, and DNS. The methodology extends to both protocol software and algorithmic network code, and LLMs were also able to generate filtering code to block extremal inputs.

Conclusion: LLMs can automate the creation of extremal test cases for network software, going beyond traditional Boundary Value Analysis, effectively finding new bugs and improving testing processes. Agentic AI could further enhance this automation.

Abstract: Physicists often manually consider extreme cases when testing a theory. In
this paper, we show how to automate extremal testing of network software using
LLMs in two steps: first, ask the LLM to generate input constraints (e.g., DNS
name length limits); then ask the LLM to generate tests that violate the
constraints. We demonstrate how easy this process is by generating extremal
tests for HTTP, BGP and DNS implementations, each of which uncovered new bugs.
We show how this methodology extends to centralized network software such as
shortest path algorithms, and how LLMs can generate filtering code to reject
extremal input. We propose using agentic AI to further automate extremal
testing. LLM-generated extremal testing goes beyond an old technique in
software testing called Boundary Value Analysis.

</details>


### [5] [A Task Taxonomy for Conformance Checking](https://arxiv.org/abs/2507.11976)
*Jana-Rebecca Rehse,Michael Grohs,Finn Klessascheck,Lisa-Marie Klein,Tatiana von Landesberger,Luise Pufahl*

Main category: cs.SE

TL;DR: The paper introduces a task taxonomy to classify and clarify the purposes of conformance checking visualization tools in process mining, addressing the unclear analytical goals currently present in industry solutions.


<details>
  <summary>Details</summary>
Motivation: Current conformance checking visualization tools are developed mainly by vendors, resulting in diverse visualizations whose analytical purposes are often unclear. Without a systematic understanding of these purposes, it is hard to assess the usefulness of the visualizations.

Method: The authors propose a task taxonomy for conformance checking analyses. The taxonomy categorizes tasks based on their goal, means, constraint type, data characteristics, data target, and data cardinality. This integrates concepts from process mining and visual analytics.

Result: The proposed taxonomy enables clearer specification and evaluation of the role and effectiveness of conformance checking visualizations. It helps researchers identify and define the analytical purposes of such visualizations, thereby serving as a foundation for better tool development and cross-disciplinary collaboration.

Conclusion: A systematic task taxonomy for conformance checking helps clarify and improve the analytical utility of visualizations, fostering more informed evaluation and collaboration across related disciplines.

Abstract: Conformance checking is a sub-discipline of process mining, which compares
observed process traces with a process model to analyze whether the process
execution conforms with or deviates from the process design. Organizations can
leverage this analysis, for example to check whether their processes comply
with internal or external regulations or to identify potential improvements.
Gaining these insights requires suitable visualizations, which make complex
results accessible and actionable. So far, however, the development of
conformance checking visualizations has largely been left to tool vendors. As a
result, current tools offer a wide variety of visual representations for
conformance checking, but the analytical purposes they serve often remain
unclear. However, without a systematic understanding of these purposes, it is
difficult to evaluate the visualizations' usefulness. Such an evaluation hence
requires a deeper understanding of conformance checking as an analysis domain.
To this end, we propose a task taxonomy, which categorizes the tasks that can
occur when conducting conformance checking analyses. This taxonomy supports
researchers in determining the purpose of visualizations, specifying relevant
conformance checking tasks in terms of their goal, means, constraint type, data
characteristics, data target, and data cardinality. Combining concepts from
process mining and visual analytics, we address researchers from both
disciplines to enable and support closer collaborations.

</details>


### [6] [LLAMA: Multi-Feedback Smart Contract Fuzzing Framework with LLM-Guided Seed Generation](https://arxiv.org/abs/2507.12084)
*Keke Gai,Haochen Liang,Jing Yu,Liehuang Zhu,Dusit Niyato*

Main category: cs.SE

TL;DR: LLAMA is an LLM-based smart contract fuzzing framework that advances mutation scheduling and testing effectiveness. It achieves high coverage and vulnerability detection, outperforming other state-of-the-art tools.


<details>
  <summary>Details</summary>
Motivation: Existing fuzzers for smart contracts mainly focus on seed scheduling and generation, with little attention given to mutation scheduling. Enhancing mutation scheduling has significant potential to improve the effectiveness of fuzzing for identifying vulnerabilities in smart contracts.

Method: The authors propose LLAMA, a Multi-feedback Smart Contract Fuzzing framework based on Large Language Models (LLMs). LLAMA combines: (i) hierarchical prompting to generate valid initial seeds using LLMs and a pre-fuzzing phase to select promising inputs; (ii) a multi-feedback optimization system using runtime coverage and dependency feedback to simultaneously enhance seed generation, selection, and mutation scheduling; (iii) an evolutionary fuzzing engine that adjusts mutation operator probabilities based on observed effectiveness and utilizes symbolic execution to avoid local maxima and discover deeper contract vulnerabilities.

Result: LLAMA surpasses state-of-the-art fuzzers in coverage and in vulnerability discovery for smart contracts. It achieves 91% instruction coverage, 90% branch coverage, and detects 132 out of 148 known vulnerabilities across various categories.

Conclusion: LLAMA is a highly effective, adaptive, and practical fuzzing framework for smart contract security, significantly improving both testing coverage and vulnerability detection compared to existing approaches.

Abstract: Smart contracts play a pivotal role in blockchain ecosystems, and fuzzing
remains an important approach to securing smart contracts. Even though mutation
scheduling is a key factor influencing fuzzing effectiveness, existing fuzzers
have primarily explored seed scheduling and generation, while mutation
scheduling has been rarely addressed by prior work. In this work, we propose a
Large Language Models (LLMs)-based Multi-feedback Smart Contract Fuzzing
framework (LLAMA) that integrates LLMs, evolutionary mutation strategies, and
hybrid testing techniques. Key components of the proposed LLAMA include: (i) a
hierarchical prompting strategy that guides LLMs to generate semantically valid
initial seeds, coupled with a lightweight pre-fuzzing phase to select
high-potential inputs; (ii) a multi-feedback optimization mechanism that
simultaneously improves seed generation, seed selection, and mutation
scheduling by leveraging runtime coverage and dependency feedback; and (iii) an
evolutionary fuzzing engine that dynamically adjusts mutation operator
probabilities based on effectiveness, while incorporating symbolic execution to
escape stagnation and uncover deeper vulnerabilities. Our experiments
demonstrate that LLAMA outperforms state-of-the-art fuzzers in both coverage
and vulnerability detection. Specifically, it achieves 91% instruction coverage
and 90% branch coverage, while detecting 132 out of 148 known vulnerabilities
across diverse categories. These results highlight LLAMA's effectiveness,
adaptability, and practicality in real-world smart contract security testing
scenarios.

</details>


### [7] [From Static to Intelligent: Evolving SaaS Pricing with LLMs](https://arxiv.org/abs/2507.12104)
*Francisco Javier Cavero,Juan C. Alonso,Antonio Ruiz-Cortés*

Main category: cs.SE

TL;DR: Introduces a tool (AI4Pricing2Yaml) that uses web scraping and LLMs to automatically convert SaaS pricing pages into machine-readable formats, improving management efficiency. Validated on 30 SaaS sites, it works well but still faces challenges with complex and dynamic data.


<details>
  <summary>Details</summary>
Motivation: The rapid expansion of the SaaS market has made pricing structures more complex and difficult for DevOps teams to manage manually. This complexity, along with the absence of automated tools for pricing analysis, restricts efficiency and scalability.

Method: The paper proposes an LLM-driven approach that automates the transformation of static HTML pricing information into intelligent, machine-readable pricing models (iPricing). The implementation, called AI4Pricing2Yaml, uses a combination of web scraping and large language models to extract essential pricing data from SaaS websites.

Result: The system was validated on a dataset of 30 commercial SaaS products, covering over 150 intelligent pricings. It successfully extracted the desired pricing elements (such as plans, features, usage limits, and add-ons) from all steps. However, some challenges remain, including dealing with hallucinations, complex pricing structures, and dynamic web content.

Conclusion: Automating the transformation of SaaS pricing into intelligent models can streamline management, improve efficiency, and provide greater consistency and scalability. Further research is needed to address current limitations and enhance the adaptability of this approach to a wider range of SaaS websites.

Abstract: The SaaS paradigm has revolutionized software distribution by offering
flexible pricing options to meet diverse customer needs. However, the rapid
expansion of the SaaS market has introduced significant complexity for DevOps
teams, who must manually manage and evolve pricing structures, an approach that
is both time-consuming and prone to errors. The absence of automated tools for
pricing analysis restricts the ability to efficiently evaluate, optimize, and
scale these models. This paper proposes leveraging intelligent pricing
(iPricing), dynamic, machine-readable pricing models, as a solution to these
challenges. Intelligent pricing enables competitive analysis, streamlines
operational decision-making, and supports continuous pricing evolution in
response to market dynamics, leading to improved efficiency and accuracy. We
present an LLM-driven approach that automates the transformation of static HTML
pricing into iPricing, significantly improving efficiency and consistency while
minimizing human error. Our implementation, AI4Pricing2Yaml, features a basic
Information Extractor that uses web scraping and LLMs technologies to extract
essential pricing components, plans, features, usage limits, and add-ons, from
SaaS websites. Validation against a dataset of 30 distinct commercial SaaS,
encompassing over 150 intelligent pricings, demonstrates the system's
effectiveness in extracting the desired elements across all steps. However,
challenges remain in addressing hallucinations, complex structures, and dynamic
content. This work highlights the potential of automating intelligent pricing
transformation to streamline SaaS pricing management, offering implications for
improved consistency and scalability in an increasingly intricate pricing
landscape. Future research will focus on refining extraction capabilities and
enhancing the system's adaptability to a wider range of SaaS websites.

</details>


### [8] [An Online A/B Testing Decision Support System for Web Usability Assessment Based on a Linguistic Decision-making Methodology: Case of Study a Virtual Learning Environment](https://arxiv.org/abs/2507.12118)
*Noe Zermeño,Cristina Zuheros,Lucas Daniel Del Rosso Calache,Francisco Herrera,Rosana Montes*

Main category: cs.SE

TL;DR: The paper introduces a new, user-centered methodology for web usability evaluation that uses design thinking, linguistic decision-making, and A/B testing. Tested on Moodle platforms with real users, the method supports richer usability insights by including both real and fictional user perspectives.


<details>
  <summary>Details</summary>
Motivation: Enhancing user satisfaction with interfaces is increasingly important. Assessing web usability with traditional methods has limitations, especially when evaluating multiple designs and involving both real and hypothetical users. There is a need for better methodologies and tools to support this broader testing.

Method: The authors propose a methodology named Linguistic Decision-Making for Web Usability Evaluation, which incorporates user-centered design, design thinking, and linguistic decision-making. This approach includes role-playing scenarios and established usability tests (like the System Usability Scale), integrated into an A/B testing decision support system. They demonstrate the method with real users evaluating three Moodle platforms at a university.

Result: The methodology was applied successfully in a case study involving real users comparing three Moodle platforms at the University of Guadalajara, providing a structured approach to web usability testing that handles both real and fictional user scenarios.

Conclusion: A new methodology for web usability evaluation allows for comprehensive assessment using both real and hypothetical user input, supported by a decision system, and proves effective in real-world educational technology scenarios.

Abstract: In recent years, attention has increasingly focused on enhancing user
satisfaction with user interfaces, spanning both mobile applications and
websites. One fundamental aspect of human-machine interaction is the concept of
web usability. In order to assess web usability, the A/B testing technique
enables the comparison of data between two designs. Expanding the scope of
tests to include the designs being evaluated, in conjunction with the
involvement of both real and fictional users, presents a challenge for which
few online tools offer support. We propose a methodology for web usability
evaluation based on user-centered approaches such as design thinking and
linguistic decision-making, named Linguistic Decision-Making for Web Usability
Evaluation. This engages people in role-playing scenarios and conducts a number
of usability tests, including the widely recognized System Usability Scale. We
incorporate the methodology into a decision support system based on A/B
testing. We use real users in a case study to assess three Moodle platforms at
the University of Guadalajara, Mexico.

</details>


### [9] [GitChameleon: Evaluating AI Code Generation Against Python Library Version Incompatibilities](https://arxiv.org/abs/2507.12367)
*Diganta Misra,Nizar Islah,Victor May,Brice Rauby,Zihan Wang,Justine Gehring,Antonio Orvieto,Muawiz Chaudhary,Eilif B. Muller,Irina Rish,Samira Ebrahimi Kahou,Massimo Caccia*

Main category: cs.SE

TL;DR: The paper introduces GitChameleon, a benchmark dataset for evaluating how well AI systems can generate Python code compatible with specific library versions. Current large language models and code assistants struggle with this, achieving only around 50% success, which highlights the difficulty of the problem. The dataset is publicly available to support future research.


<details>
  <summary>Details</summary>
Motivation: Software libraries change rapidly, which makes it hard for code generation systems to stay up-to-date and generate code that works with the latest library versions while remaining compatible with older ones. Existing benchmarks do not thoroughly test model abilities to generate version-specific, executable code.

Method: The paper presents GitChameleon, a curated dataset of 328 Python code problems, each linked to specific library versions and equipped with unit tests. The dataset is used to systematically evaluate LLMs, code assistants, agents, and RAG systems for their ability to generate code conditioned on library versions and verify functional accuracy by executing generated code.

Result: Evaluations reveal that current state-of-the-art models perform poorly on this task, with success rates only between 48-51%. This highlights the complexity of version-conditioned code generation and the challenge it poses to existing solutions.

Conclusion: GitChameleon, as an execution-based benchmark, exposes significant challenges for AI code generation systems regarding library version compatibility. Its release aims to drive progress in developing more robust and flexible code generation techniques.

Abstract: The rapid evolution of software libraries poses a considerable hurdle for
code generation, necessitating continuous adaptation to frequent version
updates while preserving backward compatibility. While existing code evolution
benchmarks provide valuable insights, they typically lack execution-based
evaluation for generating code compliant with specific library versions. To
address this, we introduce GitChameleon, a novel, meticulously curated dataset
comprising 328 Python code completion problems, each conditioned on specific
library versions and accompanied by executable unit tests. GitChameleon
rigorously evaluates the capacity of contemporary large language models (LLMs),
LLM-powered agents, code assistants, and RAG systems to perform
version-conditioned code generation that demonstrates functional accuracy
through execution. Our extensive evaluations indicate that state-of-the-art
systems encounter significant challenges with this task; enterprise models
achieving baseline success rates in the 48-51\% range, underscoring the
intricacy of the problem. By offering an execution-based benchmark emphasizing
the dynamic nature of code libraries, GitChameleon enables a clearer
understanding of this challenge and helps guide the development of more
adaptable and dependable AI code generation methods. We make the dataset and
evaluation code publicly available at
https://github.com/mrcabbage972/GitChameleonBenchmark.

</details>


### [10] [MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks](https://arxiv.org/abs/2507.12284)
*Artem Chervyakov,Alexander Kharitonov,Pavel Zadorozhny,Adamenko Pavel,Rodion Levichev,Dmitrii Vorobev,Dmitrii Salikhov,Aidar Valeev,Alena Pestova,Maria Dziuba,Ilseyar Alimova,Artem Zavgorodnev,Aleksandr Medvedev,Stanislav Moiseev,Elena Bruches,Daniil Grebenkin,Roman Derunets,Vikulov Vladimir,Anton Emelyanov,Dmitrii Babaev,Vladimir V. Ivanov,Valentin Malykh,Alena Fenogenova*

Main category: cs.SE

TL;DR: MERA Code is a new benchmark that evaluates LLMs' code generation in Russian, focusing on practical coding skills and code quality. It addresses previous gaps in LLM benchmarking and is openly available to improve and standardize evaluations.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluation methods in software engineering focus on natural language abilities, neglecting code quality and real-world performance. Existing benchmarks lack assessments for executable code and real production risks, especially for non-English contexts.

Method: The authors introduce MERA Code, a new benchmark designed to evaluate code generation abilities of LLMs in Russian. The benchmark features 11 tasks across 8 programming languages, a taxonomy of coding skills, an open-source codebase, a scoring system, and a public platform with a leaderboard and submission system. They evaluate open and frontier API models using this setup.

Result: MERA Code highlights limitations of current LLMs in handling practical coding tasks in non-English environments. Findings emphasize gaps in model capabilities beyond high-level reasoning, showing a need for better, standardized evaluation frameworks.

Conclusion: MERA Code extends current benchmarks to cover code quality, providing tools and procedures to better evaluate LLMs for software engineering tasks in Russian. It is released publicly to drive research, inform model development, and introduce standardized evaluations.

Abstract: Advancements in LLMs have enhanced task automation in software engineering;
however, current evaluations primarily focus on natural language tasks,
overlooking code quality. Most benchmarks prioritize high-level reasoning over
executable code and real-world performance, leaving gaps in understanding true
capabilities and risks associated with these models in production. To address
this issue, we propose MERA Code, a new addition to the MERA benchmark family,
specifically focused on evaluating code for the latest code generation LLMs in
Russian. This benchmark includes 11 evaluation tasks that span 8 programming
languages. Our proposed evaluation methodology features a taxonomy that
outlines the practical coding skills necessary for models to complete these
tasks. The benchmark comprises an open-source codebase for users to conduct
MERA assessments, a scoring system compatible with various programming
environments, and a platform featuring a leaderboard and submission system. We
evaluate open LLMs and frontier API models, analyzing their limitations in
terms of practical coding tasks in non-English languages. We are publicly
releasing MERA to guide future research, anticipate groundbreaking features in
model development, and standardize evaluation procedures.

</details>


### [11] [SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?](https://arxiv.org/abs/2507.12415)
*Xinyi He,Qian Liu,Mingzhe Du,Lin Yan,Zhijie Fan,Yiming Huang,Zejian Yuan,Zejun Ma*

Main category: cs.SE

TL;DR: The authors introduce SWE-Perf, a new benchmark for evaluating LLMs on code performance optimization in real-world repositories. Current LLM methods perform poorly compared to experts, indicating considerable room for improvement and future research in this area.


<details>
  <summary>Details</summary>
Motivation: Current Large Language Models (LLMs) excel at code generation and bug fixing, but their ability to optimize code performance, especially at the repository level, has not been systematically evaluated. Code performance optimization is essential for real-world and production-level software, creating a significant need for benchmark tools in this area.

Method: The authors introduce SWE-Perf, a benchmark specifically created to systematically assess LLMs on code performance optimization tasks in realistic, repository-level contexts. It consists of 140 instances drawn from real performance-improving pull requests in widely-used GitHub repositories. Each instance is complete with source code, target functions, tests, expert patches, and an executable environment. They use SWE-Perf to evaluate various LLM-powered methods, covering both file-level and repo-level optimization approaches.

Result: Evaluation using SWE-Perf demonstrates that there is a large gap between the code performance optimization ability of current LLM solutions and that of human experts. Both file-level and repo-level LLM-based approaches fall short of expert-authored performance patches.

Conclusion: There is a significant opportunity and need for research to advance LLMs in code performance optimization at the repository level. SWE-Perf provides the first benchmark framework for this, and current LLM approaches are notably less effective than expert solutions.

Abstract: Code performance optimization is paramount in real-world software engineering
and critical for production-level systems. While Large Language Models (LLMs)
have demonstrated impressive capabilities in code generation and bug fixing,
their proficiency in enhancing code performance at the repository level remains
largely unexplored. To address this gap, we introduce SWE-Perf, the first
benchmark specifically designed to systematically evaluate LLMs on code
performance optimization tasks within authentic repository contexts. SWE-Perf
comprises 140 carefully curated instances, each derived from
performance-improving pull requests from popular GitHub repositories. Each
benchmark instance includes the relevant codebase, target functions,
performance-related tests, expert-authored patches, and executable
environments. Through a comprehensive evaluation of representative methods that
span file-level and repo-level approaches (e.g., Agentless and OpenHands), we
reveal a substantial capability gap between existing LLMs and expert-level
optimization performance, highlighting critical research opportunities in this
emerging field.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [12] [Quantum circuits are just a phase](https://arxiv.org/abs/2507.11676)
*Chris Heunen,Louis Lemonnier,Christopher McNally,Alex Rice*

Main category: cs.PL

TL;DR: Current quantum programming languages are too low-level, making quantum code hard to scale and reason about. This paper introduces a new, higher-level language based on phase operations and pattern-matching constructs, making quantum programs more abstract and expressive. The approach supports fundamental quantum algorithms, has strong formal underpinnings, and translates efficiently into quantum circuits, marking a significant step forward for quantum programming.


<details>
  <summary>Details</summary>
Motivation: Quantum programming languages currently operate at a low level of abstraction, focusing primarily on circuit-level descriptions. This restricts scalability, code clarity, and the ability to support higher-level reasoning. There is a need for more abstract and expressive programming constructs for quantum computing.

Method: The authors introduce a novel quantum programming language that operates at a higher level of abstraction. The language provides a global phase operation for phase shifts and a quantum version of the 'if let' construct for subspace selection via pattern matching. It uses concepts like eigendecomposition, conjugation, and controlled unitaries. The language's expressive power is demonstrated by constructing a universal quantum gate set, expressing important quantum algorithms, offering clear denotational semantics, and implementing a prototype compiler that translates language terms to quantum circuits.

Result: The new quantum programming language enables concise and natural expression of core quantum algorithms such as Grover's search, Hamiltonian simulation, Quantum Fourier Transform, Quantum Signal Processing, and Quantum Eigenvalue Transformation. The language is universal, and its semantics are rigorously grounded in categorical quantum mechanics. The authors also provide a sound and efficient compiler from the language to quantum circuits.

Conclusion: This work provides a principled and practical advancement in quantum programming by introducing a higher-level language construct that enhances abstraction, expressiveness, and structure. The approach improves the scalability and clarity of quantum programs while allowing efficient compilation to current quantum circuits.

Abstract: Quantum programs today are written at a low level of abstraction - quantum
circuits akin to assembly languages - and even advanced quantum programming
languages essentially function as circuit description languages. This state of
affairs impedes scalability, clarity, and support for higher-level reasoning.
More abstract and expressive quantum programming constructs are needed.
  To this end, we introduce a novel yet simple quantum programming language for
generating unitaries from "just a phase"; we combine a (global) phase operation
that captures phase shifts with a quantum analogue of the "if let" construct
that captures subspace selection via pattern matching. This minimal language
lifts the focus from quantum gates to eigendecomposition, conjugation, and
controlled unitaries; common building blocks in quantum algorithm design.
  We demonstrate several aspects of the expressive power of our language in
several ways. Firstly, we establish that our representation is universal by
deriving a universal quantum gate set. Secondly, we show that important quantum
algorithms can be expressed naturally and concisely, including Grover's search
algorithm, Hamiltonian simulation, Quantum Fourier Transform, Quantum Signal
Processing, and the Quantum Eigenvalue Transformation. Furthermore, we give
clean denotational semantics grounded in categorical quantum mechanics.
Finally, we implement a prototype compiler that efficiently translates terms of
our language to quantum circuits, and prove that it is sound with respect to
these semantics. Collectively, these contributions show that this construct
offers a principled and practical step toward more abstract and structured
quantum programming.

</details>


### [13] [Picat Through the Lens of Advent of Code](https://arxiv.org/abs/2507.11731)
*Neng-Fa Zhou,Cristian Grozea,Håkan Kjellerstrand,Oisín Mac Fhearaí*

Main category: cs.PL

TL;DR: This paper demonstrates that Picat, a multi-paradigm language, allows for concise and efficient solutions to Advent of Code 2024 problems by utilizing its powerful built-in features, making it advantageous over imperative languages for these tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to illustrate how Picat's unique blend of programming paradigms and built-in features make it particularly suitable for solving complex algorithmic problems, such as those featured in the Advent of Code 2024.

Method: The authors present and analyze solutions to several Advent of Code 2024 problems using Picat, leveraging the language's constraint solving, tabling, pattern matching, and backtracking capabilities.

Result: Picat enables concise, declarative, and efficient implementations for problems like reverse engineering and path finding, demonstrating a significant effort reduction compared to imperative languages.

Conclusion: Picat's features, particularly SAT-based constraint solving and tabling, make it highly effective for certain problem types and superior in efficiency and conciseness over many imperative programming solutions.

Abstract: Picat is a logic-based, multi-paradigm programming language that integrates
features from logic, functional, constraint, and imperative programming
paradigms. This paper presents solutions to several problems from the 2024
Advent of Code (AoC). While AoC problems are not designed for any specific
programming language, certain problem types, such as reverse engineering and
path-finding, are particularly well-suited to Picat due to its built-in
constraint solving, pattern matching, backtracking, and dynamic programming
with tabling. This paper demonstrates that Picat's features, especially its
SAT-based constraint solving and tabling, enable concise, declarative, and
highly efficient implementations of problems that would require significantly
more effort in imperative languages.

</details>


### [14] [Universal Synthesis of Differentiably Tunable Numerical Abstract Transformers](https://arxiv.org/abs/2507.11827)
*Shaurya Gomber,Debangshu Banerjee,Gagandeep Singh*

Main category: cs.PL

TL;DR: This paper presents a universal, tunable solution for synthesizing abstract transformers in numerical static analysis, using a differentiable search strategy and delivering improved results across several domains compared to previous fixed, hand-crafted methods.


<details>
  <summary>Details</summary>
Motivation: Numerical abstract interpretation is powerful for static analysis, but current tools use hand-crafted, instruction-specific transformers that are not extensible and lack flexibility. This inhibits precise, compositional reasoning and forces downstream tasks to use a fixed analysis strategy.

Method: The paper introduces a universal transformer synthesis algorithm for polyhedral numerical domains, able to automatically generate a family of sound abstract transformers for any Quadratic-Bounded Guarded Operator (QGO). To efficiently navigate the complex, differentiable space of possible transformers, the Adaptive Gradient Guidance (AGG) search procedure is proposed, optimizing transformer selection for downstream objectives and constraints. These ideas are implemented in the USTAD framework.

Result: The universal synthesis algorithm can generate sound transformer families for various domains. Experiments in the Zones, Octagons, and Polyhedra domains show that USTAD can provide significant and tunable precision improvements, outperforming existing baselines by enabling compositional reasoning and efficient search over transformer space.

Conclusion: USTAD, using a universal transformer synthesis algorithm and AGG search strategy, extends flexibility, tunability, and precision in numerical abstract program analysis across domains, addressing limitations of current hand-crafted approaches.

Abstract: Numerical abstract interpretation is a widely used framework for the static
analysis of numerical programs. However, existing numerical abstract
interpreters rely on hand-crafted, instruction-specific transformers tailored
to each domain, with no general algorithm for handling common operations across
domains. This limits extensibility, prevents precise compositional reasoning
over instruction sequences, and forces all downstream tasks to use the same
fixed transformer regardless of their precision, efficiency, or task-specific
requirements. To address these limitations, we propose a universal transformer
synthesis algorithm that constructs a parametric family of sound abstract
transformers for any given polyhedral numerical domain and a concrete operator
from the class of Quadratic-Bounded Guarded Operators (QGO), which includes
both individual instructions and structured sequences. Each instantiation in
this family is sound by construction, enabling downstream analyses to adapt the
transformer to their particular needs. The space of transformers is
differentiable but complex. To efficiently explore this space of transformers,
we introduce the Adaptive Gradient Guidance (AGG) procedure, a gradient-guided
search strategy that steers the search process based on downstream analysis
objectives and runtime constraints. We implement these ideas in the USTAD
framework and evaluate their effectiveness across three numerical abstract
domains: Zones, Octagons, and Polyhedra. Our results demonstrate that the
universal synthesis algorithm successfully constructs sound families of
transformers across domains, and that USTAD achieves significant, tunable
precision gains over baselines by leveraging compositional reasoning and
efficient gradient-guided traversal of the transformer space.

</details>


### [15] [Towards Relational Contextual Equality Saturation](https://arxiv.org/abs/2507.11897)
*Tyler Hou,Shadaj Laddad,Joseph M. Hellerstein*

Main category: cs.PL

TL;DR: The paper discusses preliminary work to support contextual equality saturation—rewrite rules based on context—in the relational framework egglog, summarizing current techniques, applications, and integration challenges.


<details>
  <summary>Details</summary>
Motivation: Equality saturation is highly effective for program optimization, but conventional approaches are limited when rewrite rules need to depend on the context in which terms appear within expressions.

Method: The paper explores extending the existing technique of contextual equality saturation, which allows for context-sensitive rewrite rules, into the relational equality saturation framework provided by egglog. The authors summarize existing contextual techniques, describe their applications, and discuss challenges in integrating them with relational models.

Result: The work-in-progress expands the capability of egglog by bringing in contextual equality saturation, highlighting potential applications and key challenges in adapting this technique to relational settings.

Conclusion: The ongoing work aims to unify contextual and relational equality saturation in the egglog framework, with the goal of enabling more expressive and context-sensitive program analysis and optimization.

Abstract: Equality saturation is a powerful technique for program optimization.
Contextual equality saturation extends this to support rewrite rules that are
conditioned on where a term appears in an expression. Existing work has brought
contextual reasoning to egg; in this paper, we share our ongoing work to extend
this to relational equality saturation in egglog. We summarize the existing
approaches to contextual equality saturation, outline its main applications,
and identify key challenges in combining this approach with relational models.

</details>
