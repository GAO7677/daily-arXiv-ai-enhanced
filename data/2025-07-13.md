<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 6]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A German Gold-Standard Dataset for Sentiment Analysis in Software Engineering](https://arxiv.org/abs/2507.07325)
*Martin Obaidi,Marc Herrmann,Elisa Schmid,Raymond Ochsner,Kurt Schneider,Jil Klünder*

Main category: cs.SE

TL;DR: A new German dataset of developer statements for sentiment analysis was created and validated, filling a gap for the software engineering field. Existing tools perform poorly in this domain, underlining the importance of this contribution.


<details>
  <summary>Details</summary>
Motivation: Existing sentiment analysis tools in software engineering are primarily based on English or non-German datasets, leaving a gap for robust German-language resources. There is a need for domain-specific sentiment analysis datasets for the German-speaking software engineering community.

Method: The authors created a dataset of 5,949 unique German developer statements from the Android-Hilfe.de forum. Each statement was annotated with one of six emotions (per Shaver et al.’s model) by four German-speaking computer science students. The annotation process was evaluated for interrater agreement and reliability. The dataset was then tested with existing German sentiment analysis tools.

Result: The annotation process showed high interrater agreement and reliability, indicating the dataset is valid and robust. Evaluation with current sentiment analysis tools exposed the lack of suitable German domain-specific solutions for software engineering. Potential optimizations for annotation and additional use cases for the dataset are presented.

Conclusion: The paper introduces a new, well-annotated German sentiment analysis dataset tailored for software engineering, which is valid, reliable, and addresses a critical resource gap in the community. Existing tools fail to capture domain-specific sentiment well, highlighting the need for such data. Further work is suggested on annotation methods and dataset applications.

Abstract: Sentiment analysis is an essential technique for investigating the emotional
climate within developer teams, contributing to both team productivity and
project success. Existing sentiment analysis tools in software engineering
primarily rely on English or non-German gold-standard datasets. To address this
gap, our work introduces a German dataset of 5,949 unique developer statements,
extracted from the German developer forum Android-Hilfe.de. Each statement was
annotated with one of six basic emotions, based on the emotion model by Shaver
et al., by four German-speaking computer science students. Evaluation of the
annotation process showed high interrater agreement and reliability. These
results indicate that the dataset is sufficiently valid and robust to support
sentiment analysis in the German-speaking software engineering community.
Evaluation with existing German sentiment analysis tools confirms the lack of
domain-specific solutions for software engineering. We also discuss approaches
to optimize annotation and present further use cases for the dataset.

</details>


### [2] [Automatic Generation of Explainability Requirements and Software Explanations From User Reviews](https://arxiv.org/abs/2507.07344)
*Martin Obaidi,Jannik Fischbach,Jakob Droste,Hannah Deters,Marc Herrmann,Jil Klünder,Steffen Krätzig,Hugo Villamizar,Kurt Schneider*

Main category: cs.SE

TL;DR: The paper proposes and evaluates an automated tool for turning user feedback into software explainability requirements and explanations. While AI can generate clear explanations, human validation is still required to ensure the correctness of both requirements and explanations.


<details>
  <summary>Details</summary>
Motivation: There is a growing need to provide explainability in software systems for transparency, trust, and compliance, but it is difficult to systematically transform user feedback into structured explainability requirements and explanations.

Method: A tool-supported, automated approach was developed to extract explainability requirements from user reviews and generate aligned explanations. The approach was evaluated using a dataset of 58 annotated user reviews from collaboration with an industrial manufacturer.

Result: AI-generated requirements were less relevant and correct compared to manually created ones, but the AI-generated explanations were often favored for their clarity and style. However, correctness issues persist, highlighting the need for human validation.

Conclusion: The paper introduces an automated method to derive explainability requirements and generate explanations from user reviews but notes that human oversight is necessary to ensure correctness. The work also provides empirical insights and a curated dataset to advance future research.

Abstract: Explainability has become a crucial non-functional requirement to enhance
transparency, build user trust, and ensure regulatory compliance. However,
translating explanation needs expressed in user feedback into structured
requirements and corresponding explanations remains challenging. While existing
methods can identify explanation-related concerns in user reviews, there is no
established approach for systematically deriving requirements and generating
aligned explanations. To contribute toward addressing this gap, we introduce a
tool-supported approach that automates this process. To evaluate its
effectiveness, we collaborated with an industrial automation manufacturer to
create a dataset of 58 user reviews, each annotated with manually crafted
explainability requirements and explanations. Our evaluation shows that while
AI-generated requirements often lack relevance and correctness compared to
human-created ones, the AI-generated explanations are frequently preferred for
their clarity and style. Nonetheless, correctness remains an issue,
highlighting the importance of human validation. This work contributes to the
advancement of explainability requirements in software systems by (1)
introducing an automated approach to derive requirements from user reviews and
generate corresponding explanations, (2) providing empirical insights into the
strengths and limitations of automatically generated artifacts, and (3)
releasing a curated dataset to support future research on the automatic
generation of explainability requirements.

</details>


### [3] [Towards an Engineering Workflow Management System for Asset Administration Shells using BPMN](https://arxiv.org/abs/2507.07468)
*Sten Grüner,Nafise Eskandani*

Main category: cs.SE

TL;DR: The paper presents a secure, scalable, and automated approach for integrating Industry 4.0 technologies into engineering workflows using AAS, BPMN, and a novel infrastructure, leading to improved efficiency and collaboration.


<details>
  <summary>Details</summary>
Motivation: Industry 4.0 aims for automation and optimization in plant and process engineering, requiring technologies that enable interoperability, automation, and secure data exchange across organizations. The Asset Administration Shell (AAS) is positioned as a key element to address these needs.

Method: The paper explores integrating the Asset Administration Shell (AAS) into engineering workflows, using Business Process Model and Notation (BPMN) for structured process definition. It proposes a distributed copy-on-write infrastructure for the AAS and implements a workflow management prototype to automate AAS operations and engineering workflows.

Result: A distributed AAS copy-on-write infrastructure was developed, improving both security and scalability, thus enabling better cross-organizational collaboration. A workflow management prototype was also introduced, automating AAS operations and engineering workflows, thereby increasing efficiency and traceability.

Conclusion: Combining AAS with BPMN and introducing a distributed infrastructure provides significant improvements in workflow automation, security, scalability, and cross-organization engineering collaboration.

Abstract: The integration of Industry 4.0 technologies into engineering workflows is an
essential step toward automating and optimizing plant and process engineering
processes. The Asset Administration Shell (AAS) serves as a key enabler for
creating interoperable Digital Twins that facilitate engineering data exchange
and automation. This paper explores the use of AAS within engineering
workflows, particularly in combination with Business Process Model and Notation
(BPMN) to define structured and automated processes. We propose a distributed
AAS copy-on-write infrastructure that enhances security and scalability while
enabling seamless cross organizational collaboration. We also introduce a
workflow management prototype automating AAS operations and engineering
workflows, improving efficiency and traceability.

</details>


### [4] [From Requirements to Code: Understanding Developer Practices in LLM-Assisted Software Engineering](https://arxiv.org/abs/2507.07548)
*Jonathan Ullrich,Matthias Koch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: LLMs can't yet turn standard requirements documents directly into working code; developers must manually translate and enrich requirements before LLMs are useful, so traditional software engineering skills are still needed.


<details>
  <summary>Details</summary>
Motivation: With the rise of powerful generative Large Language Models (LLMs) capable of advanced code generation, there is speculation about these tools replacing traditional software engineering by generating code directly from requirements. However, the real-world process of feeding requirements into LLMs and how developers adapt is not well understood.

Method: The authors conducted interviews with 18 practitioners from 14 different companies. These interviews explored how developers use information from requirements and design artifacts to prompt LLMs for code generation.

Result: Their findings show that requirements, as they are typically written, are too abstract for LLMs to use directly. Instead, developers manually break down requirements into more concrete programming tasks and infuse prompts with specific design decisions and architectural constraints.

Conclusion: Despite the abilities of LLMs, classic requirements engineering (RE) activities remain critical. Developers must still interpret and restructure requirements before LLMs can effectively assist with code generation. The study's theory informs future research on automating requirements-oriented software engineering tasks.

Abstract: With the advent of generative LLMs and their advanced code generation
capabilities, some people already envision the end of traditional software
engineering, as LLMs may be able to produce high-quality code based solely on
the requirements a domain expert feeds into the system. The feasibility of this
vision can be assessed by understanding how developers currently incorporate
requirements when using LLMs for code generation-a topic that remains largely
unexplored. We interviewed 18 practitioners from 14 companies to understand how
they (re)use information from requirements and other design artifacts to feed
LLMs when generating code. Based on our findings, we propose a theory that
explains the processes developers employ and the artifacts they rely on. Our
theory suggests that requirements, as typically documented, are too abstract
for direct input into LLMs. Instead, they must first be manually decomposed
into programming tasks, which are then enriched with design decisions and
architectural constraints before being used in prompts. Our study highlights
that fundamental RE work is still necessary when LLMs are used to generate
code. Our theory is important for contextualizing scientific approaches to
automating requirements-centric SE tasks.

</details>


### [5] [Prompt Engineering for Requirements Engineering: A Literature Review and Roadmap](https://arxiv.org/abs/2507.07682)
*Kaicheng Huang,Fanyu Wang,Yutan Huang,Chetan Arora*

Main category: cs.SE

TL;DR: This paper systematically reviews Prompt Engineering techniques for Requirements Engineering, offering a taxonomy and a practical roadmap to guide future research and application, with the goal of making LLMs more reliable and accessible in RE tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper is to address the significant uncertainty and lack of controllability in the use of large language models (LLMs) for requirements engineering (RE) tasks. The absence of clear, standardized guidance for prompt engineering (PE) hinders the trustworthy and effective use of LLMs in this field.

Method: The authors perform a systematic literature review based on Kitchenham's and Petersen's secondary-study protocol. They searched six digital libraries, screened 867 records, and analyzed 35 primary studies to map current PE techniques and their applications to RE.

Result: The study proposes a hybrid taxonomy linking technique-oriented prompt engineering patterns to task-oriented RE roles. It maps out what RE tasks have been addressed, which LLM families and prompt types have been used, and identifies existing limitations and research gaps in the field.

Conclusion: The paper provides the first systematic roadmap for PE in RE, presenting a step-by-step guide for evolving current ad-hoc prompt engineering prototypes into reproducible, practitioner-friendly workflows.

Abstract: Advancements in large language models (LLMs) have led to a surge of prompt
engineering (PE) techniques that can enhance various requirements engineering
(RE) tasks. However, current LLMs are often characterized by significant
uncertainty and a lack of controllability. This absence of clear guidance on
how to effectively prompt LLMs acts as a barrier to their trustworthy
implementation in the RE field. We present the first roadmap-oriented
systematic literature review of Prompt Engineering for RE (PE4RE). Following
Kitchenham's and Petersen's secondary-study protocol, we searched six digital
libraries, screened 867 records, and analyzed 35 primary studies. To bring
order to a fragmented landscape, we propose a hybrid taxonomy that links
technique-oriented patterns (e.g., few-shot, Chain-of-Thought) to task-oriented
RE roles (elicitation, validation, traceability). Two research questions, with
five sub-questions, map the tasks addressed, LLM families used, and prompt
types adopted, and expose current limitations and research gaps. Finally, we
outline a step-by-step roadmap showing how today's ad-hoc PE prototypes can
evolve into reproducible, practitioner-friendly workflows.

</details>


### [6] [From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry](https://arxiv.org/abs/2507.07689)
*Chetan Arora,Fanyu Wang,Chakkrit Tantithamthavorn,Aldeida Aleti,Shaun Kenyon*

Main category: cs.SE

TL;DR: The paper explores using AI, specifically Retrieval-Augmented Generation models, to automate and improve requirements engineering in the space industry, showing early success in reducing effort and improving requirement coverage for smaller organizations.


<details>
  <summary>Details</summary>
Motivation: Requirements engineering in the space industry is complex and demanding, particularly for smaller organizations, due to the need to extract actionable requirements from large, unstructured documents and adhere to stringent standards.

Method: The paper proposes a modular, AI-driven approach using Retrieval-Augmented Generation (RAG) models. The system preprocesses raw mission documents, classifies their contents, retrieves relevant information from domain standards, and uses large language models (LLMs) to synthesize draft requirements. The approach is demonstrated on a real-world mission document in partnership with an industry collaborator.

Result: Preliminary results suggest the method reduces manual effort, enhances coverage of relevant requirements, and helps achieve lightweight compliance alignment.

Conclusion: AI-driven approaches like RAG can significantly support and (semi-)automate requirements generation in space missions, lowering entry barriers for smaller organizations and enhancing the requirements engineering process.

Abstract: Requirements engineering (RE) in the space industry is inherently complex,
demanding high precision, alignment with rigorous standards, and adaptability
to mission-specific constraints. Smaller space organisations and new entrants
often struggle to derive actionable requirements from extensive, unstructured
documents such as mission briefs, interface specifications, and regulatory
standards. In this innovation opportunity paper, we explore the potential of
Retrieval-Augmented Generation (RAG) models to support and (semi-)automate
requirements generation in the space domain. We present a modular, AI-driven
approach that preprocesses raw space mission documents, classifies them into
semantically meaningful categories, retrieves contextually relevant content
from domain standards, and synthesises draft requirements using large language
models (LLMs). We apply the approach to a real-world mission document from the
space domain to demonstrate feasibility and assess early outcomes in
collaboration with our industry partner, Starbound Space Solutions. Our
preliminary results indicate that the approach can reduce manual effort,
improve coverage of relevant requirements, and support lightweight compliance
alignment. We outline a roadmap toward broader integration of AI in RE
workflows, intending to lower barriers for smaller organisations to participate
in large-scale, safety-critical missions.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [7] [On Propositional Program Equivalence (extended abstract)](https://arxiv.org/abs/2507.07480)
*Tobias Kappé*

Main category: cs.PL

TL;DR: General program equivalence is undecidable, but by using (G)KAT to abstract semantics, propositional equivalence becomes both decidable and practical to analyze.


<details>
  <summary>Details</summary>
Motivation: General program equivalence is undecidable, making direct analysis infeasible. The authors address whether abstraction over statement semantics can make equivalence checking feasible, focusing on propositional equivalence.

Method: The paper discusses recent developments in propositional program equivalence through the framework of (Guarded) Kleene Algebra with Tests ((G)KAT), which abstracts program semantics for analysis.

Result: It is demonstrated that, by abstracting away from the underlying statement semantics, propositional program equivalence is both decidable and practically feasible using (G)KAT.

Conclusion: Abstraction using (G)KAT leads to practical and decidable methods for propositional program equivalence, opening ways to effectively analyze program structure without dealing with undecidability.

Abstract: General program equivalence is undecidable. However, if we abstract away the
semantics of statements, then this problem becomes not just decidable, but
practically feasible. For instance, a program of the form "if $b$ then $e$ else
$f$" should be equivalent to "if not $b$ then $f$ else $e$" - no matter what
$b$, $e$ and $f$ are. This kind of equivalence is known as propositional
equivalence. In this extended abstract, we discuss recent developments in
propositional program equivalence from the perspective of (Guarded) Kleene
Algebra with Tests, or (G)KAT.

</details>
