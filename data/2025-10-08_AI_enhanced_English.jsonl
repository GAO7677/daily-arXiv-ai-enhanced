{"id": "2510.05147", "categories": ["cs.SE", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.05147", "abs": "https://arxiv.org/abs/2510.05147", "authors": ["Yu Zhu"], "title": "Adaptive Reinforcement Learning for Dynamic Configuration Allocation in Pre-Production Testing", "comment": null, "summary": "Ensuring reliability in modern software systems requires rigorous\npre-production testing across highly heterogeneous and evolving environments.\nBecause exhaustive evaluation is infeasible, practitioners must decide how to\nallocate limited testing resources across configurations where failure\nprobabilities may drift over time. Existing combinatorial optimization\napproaches are static, ad hoc, and poorly suited to such non-stationary\nsettings. We introduce a novel reinforcement learning (RL) framework that\nrecasts configuration allocation as a sequential decision-making problem. Our\nmethod is the first to integrate Q-learning with a hybrid reward design that\nfuses simulated outcomes and real-time feedback, enabling both sample\nefficiency and robustness. In addition, we develop an adaptive online-offline\ntraining scheme that allows the agent to quickly track abrupt probability\nshifts while maintaining long-run stability. Extensive simulation studies\ndemonstrate that our approach consistently outperforms static and\noptimization-based baselines, approaching oracle performance. This work\nestablishes RL as a powerful new paradigm for adaptive configuration\nallocation, advancing beyond traditional methods and offering broad\napplicability to dynamic testing and resource scheduling domains.", "AI": {"tldr": "The paper proposes an RL-based strategy for dynamic software testing resource allocation, integrating Q-learning and hybrid rewards for adaptability. It outperforms standard methods in changing environments and is applicable beyond software testing.", "motivation": "Exhaustive pre-production software testing is infeasible due to environment heterogeneity and shifting failure probabilities. Current optimization methods lack adaptability for non-stationary settings, necessitating a more dynamic approach.", "method": "The authors introduce a RL framework using Q-learning integrated with a hybrid reward design (simulated and real-time feedback). It includes an adaptive online-offline training scheme to respond to changing failure probabilities and maintain stability.", "result": "Extensive simulations show that the RL framework surpasses static and optimization-based methods, approaching oracle-level performance, and is broadly applicable to dynamic resource scheduling.", "conclusion": "Reinforcement learning (RL), with Q-learning and hybrid rewards, offers superior adaptive configuration allocation for software testing, consistently outperforming traditional static and optimization-based strategies."}}
{"id": "2510.05156", "categories": ["cs.SE", "cs.AI", "cs.CR", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.05156", "abs": "https://arxiv.org/abs/2510.05156", "authors": ["Lesly Miculicich", "Mihir Parmar", "Hamid Palangi", "Krishnamurthy Dj Dvijotham", "Mirko Montanari", "Tomas Pfister", "Long T. Le"], "title": "VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation", "comment": "22 pages", "summary": "The deployment of autonomous AI agents in sensitive domains, such as\nhealthcare, introduces critical risks to safety, security, and privacy. These\nagents may deviate from user objectives, violate data handling policies, or be\ncompromised by adversarial attacks. Mitigating these dangers necessitates a\nmechanism to formally guarantee that an agent's actions adhere to predefined\nsafety constraints, a challenge that existing systems do not fully address. We\nintroduce VeriGuard, a novel framework that provides formal safety guarantees\nfor LLM-based agents through a dual-stage architecture designed for robust and\nverifiable correctness. The initial offline stage involves a comprehensive\nvalidation process. It begins by clarifying user intent to establish precise\nsafety specifications. VeriGuard then synthesizes a behavioral policy and\nsubjects it to both testing and formal verification to prove its compliance\nwith these specifications. This iterative process refines the policy until it\nis deemed correct. Subsequently, the second stage provides online action\nmonitoring, where VeriGuard operates as a runtime monitor to validate each\nproposed agent action against the pre-verified policy before execution. This\nseparation of the exhaustive offline validation from the lightweight online\nmonitoring allows formal guarantees to be practically applied, providing a\nrobust safeguard that substantially improves the trustworthiness of LLM agents.", "AI": {"tldr": "VeriGuard is a framework for formally guaranteeing the safety of LLM agents. It uses a thorough offline process to specify and verify safety policies, and monitors agent actions at runtime to ensure compliance, achieving robust trust and risk reduction.", "motivation": "Autonomous AI agents in sensitive fields like healthcare can fail, violate privacy, or be compromised, and current systems lack mechanisms for guaranteeing safe behavior. The need to formalize and assure agent adherence to strict safety constraints motivated the development of VeriGuard.", "method": "VeriGuard consists of a dual-stage architecture: (1) an offline stage that validates user intents, synthesizes behavior policies, and subjects them to both testing and formal verification; and (2) an online stage that monitors and validates agent actions in real time against pre-verified policies.", "result": "VeriGuard improves agent trustworthiness by formally guaranteeing adherence to user objectives and safety specifications, using policy synthesis, verification, and runtime monitoring to mitigate various risks.", "conclusion": "VeriGuard successfully provides formal safety guarantees for LLM-based agents by separating comprehensive offline policy verification from lightweight online action monitoring. This dual-stage architecture enables robust and practical enforcement of safety constraints in sensitive domains."}}
{"id": "2510.05365", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05365", "abs": "https://arxiv.org/abs/2510.05365", "authors": ["Irtaza Sajid Qureshi", "Zhen Ming", "Jiang"], "title": "Test Case Generation from Bug Reports via Large Language Models: A Cognitive Layered Evaluation Framework", "comment": null, "summary": "Large Language Models (LLMs) are increasingly applied to automated software\ntesting, yet their ability to generalize beyond memorized patterns and reason\nabout natural language bug reports remains unclear. We present a systematic\nevaluation of LLM reasoning in test case generation, structured around the\ncognitive layers of Bloom's taxonomy: \\textit{Remember}, \\textit{Understand},\n\\textit{Apply}, \\textit{Analyze}, \\textit{Evaluate}, and \\textit{Create}, which\nprogressively assess higher levels of cognitive and reasoning capabilities.\nBuilding on the LIBRO framework, we evaluate StarCoder and GPT-4o on Defects4J,\nGHRB, and mutated variants that introduce linguistic and semantic challenges.\nOur findings show that both models largely reproduce prior results with minor\ndeviations (\\textit{Remember}), exhibit partial robustness to linguistic\nrephrasings and translations while uncovering unique reproducible bugs\n(\\textit{Understand}), but suffer severe performance drops exceeding 60\\% under\nidentifier mutations (\\textit{Apply}). Conversely, providing near-identical\nfew-shot examples in an open-book setting improves success rates by up to three\ntimes, and component-level analysis reveals that structured technical elements,\nsuch as test code and method names, are far more impactful than narrative\ndescriptions for successful test generation (\\textit{Analyze}). These insights\nilluminate the cognitive processes underlying LLM-generated tests, suggest\nconcrete directions for improving performance, and establish a robust and\nrealistic evaluation paradigm for this task.", "AI": {"tldr": "LLMs are decent at basic pattern reproduction and some understanding in test generation, but fail badly when code identifiers change. However, giving models few-shot examples boosts performance. Focusing on technical, structured input helps much more than narratives. This suggests how to better use and assess LLMs for software test automation.", "motivation": "LLMs are increasingly being used for automated software testing, but it is unclear how well they generalize beyond memorized patterns and reason about natural language bug reports.", "method": "They systematically evaluate LLM reasoning in test case generation across Bloom\u2019s taxonomy layers using the LIBRO framework. They apply this on StarCoder and GPT-4o, testing on Defects4J and GHRB datasets, as well as mutated versions that introduce linguistic and semantic challenges.", "result": "Both models mainly replicate previous results at the 'Remember' and partially 'Understand' levels. They uncover unique bugs but see large (>60%) performance drops with identifier mutations ('Apply'). Few-shot, open-book setups improve success rates up to three times. Technical elements like test code and method names are more useful than narrative descriptions for successful test generation.", "conclusion": "The research sheds light on the cognitive reasoning abilities of LLMs in test generation and provides guidance for improving performance and evaluation paradigms."}}
{"id": "2510.05390", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05390", "abs": "https://arxiv.org/abs/2510.05390", "authors": ["Felicity Anderson", "Julien Sindt", "Neil Chue Hong"], "title": "Who Do You Think You Are? Creating RSE Personas from GitHub Interactions", "comment": "36 pages. Invited extended paper of original poster at deRSE2025. To\n  be published in ECEASST", "summary": "We describe data-driven RSE personas: an approach combining software\nrepository mining and data-driven personas applied to research software (RS),\nan attempt to describe and identify common and rare patterns of Research\nSoftware Engineering (RSE) development. This allows individuals and RS project\nteams to understand their contributions, impact and repository dynamics - an\nimportant foundation for improving RSE. We evaluate the method on different\npatterns of collaborative interaction behaviours by contributors to mid-sized\npublic RS repositories (those with 10-300 committers) on GitHub. We demonstrate\nhow the RSE personas method successfully characterises a sample of 115,174\nrepository contributors across 1,284 RS repositories on GitHub, sampled from\n42,284 candidate software repository records queried from Zenodo. We identify,\nname and summarise seven distinct personas from low to high interactivity:\nEphemeral Contributor; Occasional Contributor; Project Organiser; Moderate\nContributor; Low-Process Closer; Low-Coding Closer; and Active Contributor.\nThis demonstrates that large datasets can be analysed despite difficulties of\ncomparing software projects with different project management factors, research\ndomains and contributor backgrounds.", "AI": {"tldr": "The paper presents a data-driven personas approach to analyze and classify contributor behaviors in research software repositories, identifying seven typical personas from large-scale GitHub mining, thus helping teams understand and improve Research Software Engineering dynamics.", "motivation": "The paper aims to understand patterns and dynamics of Research Software Engineering (RSE) contributions by analyzing collaborative behavior in software repositories. The motivation is to help individuals and RS project teams better comprehend their roles, impacts, and repository interactions, which are crucial for improving RSE practices.", "method": "The authors combine software repository mining with data-driven persona generation, applying this methodology to research software repositories. They evaluate the method by assessing contributor patterns in mid-sized public RS repositories on GitHub, analyzing data from over 1,284 repositories and more than 115,000 contributors.", "result": "They successfully identify and classify seven distinct contributor personas representing various levels of interactivity, such as Ephemeral Contributor, Occasional Contributor, Project Organiser, Moderate Contributor, Low-Process Closer, Low-Coding Closer, and Active Contributor. The study demonstrates the feasibility of large-scale analysis across heterogeneous projects and contributor backgrounds.", "conclusion": "The RSE personas method provides actionable insights into contributor behaviors and patterns in research software, supporting teams and individuals in understanding and improving their software engineering practices. It enables meaningful analysis despite differences in project management styles and domains."}}
{"id": "2510.05441", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05441", "abs": "https://arxiv.org/abs/2510.05441", "authors": ["Yiannis Charalambous", "Claudionor N. Coelho Jr", "Luis Lamb", "Lucas C. Cordeiro"], "title": "UnitTenX: Generating Tests for Legacy Packages with AI Agents Powered by Formal Verification", "comment": null, "summary": "This paper introduces UnitTenX, a state-of-the-art open-source AI multi-agent\nsystem designed to generate unit tests for legacy code, enhancing test coverage\nand critical value testing. UnitTenX leverages a combination of AI agents,\nformal methods, and Large Language Models (LLMs) to automate test generation,\naddressing the challenges posed by complex and legacy codebases. Despite the\nlimitations of LLMs in bug detection, UnitTenX offers a robust framework for\nimproving software reliability and maintainability. Our results demonstrate the\neffectiveness of this approach in generating high-quality tests and identifying\npotential issues. Additionally, our approach enhances the readability and\ndocumentation of legacy code.", "AI": {"tldr": "UnitTenX is an AI-powered multi-agent system that automates generation of unit tests for legacy code, increasing test coverage and reliability. The approach outperforms traditional methods, improves documentation, and addresses limitations of current LLM-based bug detection.", "motivation": "Legacy code often lacks adequate unit tests, making software unreliable and difficult to maintain. Traditional methods struggle with complex, legacy codebases. There is a need for automated, effective solutions that can increase test coverage and improve code quality.", "method": "UnitTenX combines AI agents, formal methods, and Large Language Models to automate the generation of unit tests. The system tackles legacy and complex codebases, aiming to improve test coverage and code reliability despite known limitations of LLMs in bug detection.", "result": "The results show that UnitTenX can generate high-quality unit tests effectively, identify potential issues, and improve documentation and readability of legacy code.", "conclusion": "UnitTenX is a robust, open-source solution that addresses unit test generation for legacy code. It enhances software reliability and maintainability by automating critical value testing and improving documentation, even with LLM limitations."}}
{"id": "2510.05450", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05450", "abs": "https://arxiv.org/abs/2510.05450", "authors": ["Saul Goldman", "Hong Yi Lin", "Jirat Pasuksmit", "Patanamon Thongtanunam", "Kla Tantithamthavorn", "Zhe Wang", "Ray Zhang", "Ali Behnaz", "Fan Jiang", "Michael Siers", "Ryan Jiang", "Mike Buller", "Minwoo Jeong", "Ming Wu"], "title": "What Types of Code Review Comments Do Developers Most Frequently Resolve?", "comment": "The paper has been accepted the 40th IEEE/ACM International\n  Conference on Automated Software Engineering, ASE 2025", "summary": "Large language model (LLM)-powered code review automation tools have been\nintroduced to generate code review comments. However, not all generated\ncomments will drive code changes. Understanding what types of generated review\ncomments are likely to trigger code changes is crucial for identifying those\nthat are actionable. In this paper, we set out to investigate (1) the types of\nreview comments written by humans and LLMs, and (2) the types of generated\ncomments that are most frequently resolved by developers. To do so, we\ndeveloped an LLM-as-a-Judge to automatically classify review comments based on\nour own taxonomy of five categories. Our empirical study confirms that (1) the\nLLM reviewer and human reviewers exhibit distinct strengths and weaknesses\ndepending on the project context, and (2) readability, bugs, and\nmaintainability-related comments had higher resolution rates than those focused\non code design. These results suggest that a substantial proportion of\nLLM-generated comments are actionable and can be resolved by developers. Our\nwork highlights the complementarity between LLM and human reviewers and offers\nsuggestions to improve the practical effectiveness of LLM-powered code review\ntools.", "AI": {"tldr": "The paper analyzes differences between human and LLM-generated code review comments, finding that readability, bug, and maintainability issues lead to more code changes than design-related comments. LLMs and humans have complementary strengths, and LLM-generated comments can be practically useful for developers.", "motivation": "LLM-powered code review tools generate comments, but it's unclear which comments truly trigger code changes. The paper aims to understand the types of comments most likely to drive resolution, to improve the actionable value of automated reviews.", "method": "The authors create a taxonomy of five comment categories and develop an LLM-as-a-Judge classification system. They empirically analyze and compare human and LLM-generated comments, and study which types are resolved most often by developers.", "result": "Readability, bug-related, and maintainability-focused comments\u2014regardless of author\u2014have higher resolution rates than those about code design. LLM and human reviewers show different strengths depending on context.", "conclusion": "Significant portions of LLM-generated review comments are actionable and can be resolved by developers. Human and LLM reviewers can complement each other. Improvement suggestions are provided for code review automation tools."}}
{"id": "2510.05604", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05604", "abs": "https://arxiv.org/abs/2510.05604", "authors": ["Rintaro Kanaji", "Brittany Reid", "Yutaro Kashiwa", "Raula Gaikovina Kula", "Hajimu Iida"], "title": "An Empirical Study of Security-Policy Related Issues in Open Source Projects", "comment": "Accepted in PROFES 2025", "summary": "GitHub recommends that projects adopt a SECURITY.md file that outlines\nvulnerability reporting procedures. However, the effectiveness and operational\nchallenges of such files are not yet fully understood. This study aims to\nclarify the challenges that SECURITY.md files face in the vulnerability\nreporting process within open-source communities. Specifically, we classified\nand analyzed the content of 711 randomly sampled issues related to SECURITY.md.\nWe also conducted a quantitative comparative analysis of the close time and\nnumber of responses for issues concerning six community health files, including\nSECURITY.md. Our analysis revealed that 79.5% of SECURITY.md-related issues\nwere requests to add the file, and reports that included links were closed,\nwith a median time that was 2 days shorter. These findings offer practical\ninsights for improving security reporting policies and community management,\nultimately contributing to a more secure open-source ecosystem.", "AI": {"tldr": "The paper analyzes how GitHub's SECURITY.md files impact vulnerability reporting, finding most issues are requests to add the file and that issues with resource links are resolved faster, offering suggestions to improve open-source security management.", "motivation": "The motivation is to evaluate the real-world effectiveness and challenges of SECURITY.md files in facilitating vulnerability reporting in open-source projects, which is currently not well understood despite GitHub\u2019s recommendations.", "method": "Content classification and quantitative analysis of 711 randomly sampled SECURITY.md-related GitHub issues. Comparative analysis of issue close times and responses for six community health files was conducted.", "result": "79.5% of SECURITY.md-related issues were requests to add the file. Issues that included links were resolved about 2 days faster (median time), providing actionable insights for better security practices in open-source communities.", "conclusion": "The study concludes that SECURITY.md files in GitHub projects face operational challenges, especially in their adoption and utility for vulnerability reporting. Many issues are requests to add the file, and including resource links helps resolve these issues faster."}}
{"id": "2510.05705", "categories": ["cs.SE", "cs.DL", "q-bio.OT"], "pdf": "https://arxiv.org/pdf/2510.05705", "abs": "https://arxiv.org/abs/2510.05705", "authors": ["Eva Mart\u00edn del Pico", "Josep Llu\u00eds Gelp\u00ed", "Salvador Capella-Guti\u00e9rrez"], "title": "The Software Observatory: aggregating and analysing software metadata for trend computation and FAIR assessment", "comment": null, "summary": "In the ever-changing realm of research software development, it is crucial\nfor the scientific community to grasp current trends to identify gaps that can\npotentially hinder scientific progress. The adherence to the FAIR (Findable,\nAccessible, Interoperable, Reusable) principles can serve as a proxy to\nunderstand those trends and provide a mechanism to propose specific actions.\n  The Software Observatory at OpenEBench\n(https://openebench.bsc.es/observatory) is a novel web portal that consolidates\nsoftware metadata from various sources, offering comprehensive insights into\ncritical research software aspects. Our platform enables users to analyse\ntrends, identify patterns and advancements within the Life Sciences research\nsoftware ecosystem, and understand its evolution over time. It also evaluates\nresearch software according to FAIR principles for research software, providing\nscores for different indicators.\n  Users have the ability to visualise this metadata at different levels of\ngranularity, ranging from the entire software landscape to specific communities\nto individual software entries through the FAIRsoft Evaluator. Indeed, the\nFAIRsoft Evaluator component streamlines the assessment process, helping\ndevelopers efficiently evaluate and obtain guidance to improve their software's\nFAIRness.\n  The Software Observatory represents a valuable resource for researchers and\nsoftware developers, as well as stakeholders, promoting better software\ndevelopment practices and adherence to FAIR principles for research software.", "AI": {"tldr": "The Software Observatory is a new platform that helps researchers and developers analyze and improve research software by consolidating metadata, visualizing trends, and assessing adherence to FAIR principles.", "motivation": "The motivation is to help the scientific community identify current trends and gaps in research software development, which may hinder scientific progress, and to enable actionable insights using the FAIR principles as a framework.", "method": "The study introduces the Software Observatory at OpenEBench, which consolidates metadata from various sources and provides comprehensive analytics, including visualization and FAIRness evaluation, using the FAIRsoft Evaluator component.", "result": "The Software Observatory aggregates and visualizes research software metadata, enables trend analysis, and provides FAIRness scores, supporting the improvement of research software practices across the life sciences.", "conclusion": "The Software Observatory is a valuable tool for researchers, software developers, and stakeholders, promoting improved software development and adherence to FAIR principles in research software."}}
{"id": "2510.05768", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05768", "abs": "https://arxiv.org/abs/2510.05768", "authors": ["Robin Kimmel", "Judith Michael", "Andreas Wortmann", "Jingxi Zhang"], "title": "Digital Twins for Software Engineering Processes", "comment": null, "summary": "Digital twins promise a better understanding and use of complex systems. To\nthis end, they represent these systems at their runtime and may interact with\nthem to control their processes. Software engineering is a wicked challenge in\nwhich stakeholders from many domains collaborate to produce software artifacts\ntogether. In the presence of skilled software engineer shortage, our vision is\nto leverage DTs as means for better rep- resenting, understanding, and\noptimizing software engineering processes to (i) enable software experts making\nthe best use of their time and (ii) support domain experts in producing\nhigh-quality software. This paper outlines why this would be beneficial, what\nsuch a digital twin could look like, and what is missing for realizing and\ndeploying software engineering digital twins.", "AI": {"tldr": "This paper discusses how digital twins could improve software engineering by representing and optimizing processes, helping both software and domain experts, but highlights that substantial work is needed for practical adoption.", "motivation": "A shortage of skilled software engineers and the complex nature of collaborative software development drive the need for advanced representations and tools to optimize processes.", "method": "The study outlines the conceptual framework for digital twins in software engineering, discusses potential benefits, proposes a vision, and analyzes gaps for implementation.", "result": "Digital twins can help software experts manage their time efficiently and assist domain experts in producing higher-quality software. The paper identifies benefits, potential structures, and existing gaps for deployment.", "conclusion": "Digital twins have significant potential for improving software engineering processes, but further research and development are necessary to realize and deploy them effectively."}}
{"id": "2510.05788", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05788", "abs": "https://arxiv.org/abs/2510.05788", "authors": ["Nikita Pavlichenko", "Iurii Nazarov", "Ivan Dolgov", "Ekaterina Garanina", "Dmitry Ustalov", "Ivan Bondyrev", "Kseniia Lysaniuk", "Evgeniia Vu", "Kirill Chekmenev", "Joseph Shtok", "Yaroslav Golubev", "Anton Semenkin", "Uladzislau Sazanovich"], "title": "Mellum: Production-Grade in-IDE Contextual Code Completion with Multi-File Project Understanding", "comment": "11 pages, 4 figures, 3 tables", "summary": "We present the Mellum models family, open-weight code completion models\ndesigned for interactive use in JetBrains IDEs. Mellums have 4B parameters,\nadopt a Llama-style architecture, and are pre-trained on ~4T tokens of\npermissively licensed, multi-language code. Our studies show that (i) careful\ndata curation and staged training significantly improve the model's quality,\n(ii) editor-critical capabilities such as context packing are necessary for\nhigh-quality suggestions, and (iii) a compact, task-focused model can meet the\ncost and latency constraints of interactive completion.\n  In the paper, we describe an end-to-end industrial pipeline for producing\ncontextualized in-editor completion: disciplined data governance, multi-stage\ntraining that includes fill-in-the-middle and project context via supervised\nfine-tuning, and alignment via direct preference optimization using feedback\nfrom real-world scenarios. Our quality evaluations include both large-scale\noffline benchmarks and online telemetry from production deployments in\nJetBrains IDEs. Mellums are released under the Apache-2.0 license on\nHuggingFace, with a public model card providing a reproducible reference for\npractitioners. Our experience offers a pragmatic blueprint for taking a\nfocused, open model from a research prototype to at scale production for\nhundreds of thousands of users.", "AI": {"tldr": "Mellum is an open-source, 4B parameter code completion model optimized for JetBrains IDEs. Through careful data governance and training methods, it achieves high-quality suggestions while being cost- and latency-efficient. Mellum is production-ready, performant at scale, and openly available for use and research.", "motivation": "There is a growing demand for high-quality, efficient code completion features in Integrated Development Environments (IDEs), especially for interactive use. Current models may not balance quality, efficiency, governance, and user needs, particularly in production scenarios. This paper aims to address these challenges by developing an open-source, production-ready code completion model.", "method": "The paper introduces the Mellum model family, which follows a Llama-style architecture with 4 billion parameters. The models are pre-trained on approximately 4 trillion tokens of multi-language, permissively licensed code. The authors describe an industrial pipeline: careful data governance, staged training including fill-in-the-middle and project context supervised fine-tuning, and alignment using direct preference optimization based on real user feedback. Multiple large-scale offline and online evaluation methods are used.", "result": "The authors demonstrate that careful data curation and staged training significantly enhance model quality. Features such as context packing are shown to be crucial for robust code suggestions. The compact model size allows meeting cost and latency requirements for interactive code completion. Mellum models perform well in both benchmarks and live deployments, serving hundreds of thousands of users.", "conclusion": "The Mellum family provides a practical, open-source solution for in-editor code completion, with proven methods for quality, efficiency, and scalable deployment. It serves as a reproducible and pragmatic blueprint for transitioning research models to production-ready tools in real-world IDE environments. The models and pipeline are openly released for the community."}}
{"id": "2510.05878", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05878", "abs": "https://arxiv.org/abs/2510.05878", "authors": ["Darja Smite", "Franz Zieris", "Lars-Ola Damm"], "title": "A Wave of Resignations in the Aftermath of Remote Onboarding", "comment": "9 pages, submitted to the Journal of Systems and Software, In\n  Practice track", "summary": "The COVID-19 pandemic has permanently altered workplace structures,\nnormalizing remote work. However, critical evidence highlights challenges with\nfully remote arrangements, particularly for software teams. This study\ninvestigates employee resignation patterns at Ericsson, a global developer of\nsoftware-intensive systems, before, during, and after the pandemic. Using HR\ndata from 2016-2025 in Ericsson Sweden, we analyze how different work\nmodalities (onsite, remote, and hybrid) influence employee retention. Our\nfindings show a marked increase in resignations from summer 2021 to summer\n2023, especially among employees with less than five years of tenure. Employees\nonboarded remotely during the pandemic were significantly more likely to resign\nwithin their first three years, even after returning to the office. Exit\nsurveys suggest that remote onboarding may fail to establish the necessary\norganizational attachment, the feeling of belonging and long-term retention. By\ncontrast, the company's eventual successful return to pre-pandemic retention\nrates illustrates the value of differentiated work policies and supports\nreconsidering selective return-to-office (RTO) mandates. Our study demonstrates\nthe importance of employee integration practices in hybrid environments where\nthe requirement for in-office presence for recent hires shall be accompanied by\nin-office presence from their team members and more senior staff whose\nmentoring and social interactions contribute to integration into the corporate\nwork environment. We hope these actionable insights will inform HR leaders and\npolicymakers in shaping post-pandemic work practices, demonstrating that\ncarefully crafted hybrid models anchored in organizational attachment and\nmentorship can sustain retention in knowledge-intensive companies.", "AI": {"tldr": "Remote onboarding during the pandemic led to higher resignation rates among new hires at Ericsson, reflecting a lack of organizational attachment. Differentiated hybrid work policies and stronger in-person integration for new employees improved retention. Effective hybrid models with mentorship should guide post-pandemic HR strategies.", "motivation": "The study was motivated by the need to understand the effects of remote, onsite, and hybrid work modalities on employee retention\u2014especially in the context of the COVID-19 pandemic, which forced rapid adoption of remote work structures. It seeks to address challenges faced by software teams surrounding employee resignation and organizational attachment.", "method": "The researchers analyzed HR data from Ericsson Sweden between 2016 and 2025. They examined resignation patterns before, during, and after the pandemic, categorizing employees based on their work modality (onsite, remote, hybrid). They also reviewed exit surveys to assess the reasons behind resignations and organizational attachment issues.", "result": "The study found a significant increase in resignations from summer 2021 to summer 2023, especially among employees with less than five years at the company. Those who were onboarded remotely during the pandemic were much more likely to resign within their first three years. The successful return to pre-pandemic retention rates was achieved through differentiated work policies, particularly emphasizing mentorship and team integration during hybrid and in-person onboarding.", "conclusion": "Hybrid work models, when properly designed to include mentorship and opportunities for organizational attachment, can maintain employee retention in knowledge-intensive industries. Selective return-to-office policies, focused on integrating new hires with their teams and senior staff, demonstrate clear benefits in fostering belonging and long-term retention."}}
{"id": "2510.05968", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05968", "abs": "https://arxiv.org/abs/2510.05968", "authors": ["Scott Frees"], "title": "Extending ResourceLink: Patterns for Large Dataset Processing in MCP Applications", "comment": null, "summary": "Large language models translate natural language into database queries, yet\ncontext window limitations prevent direct deployment in reporting systems where\ncomplete datasets exhaust available tokens. The Model Context Protocol\nspecification defines ResourceLink for referencing external resources, but\npractical patterns for implementing scalable reporting architectures remain\nundocumented. This paper presents patterns for building LLM-powered reporting\nsystems that decouple query generation from data retrieval. We introduce a\ndual-response pattern extending ResourceLink to support both iterative query\nrefinement and out-of-band data access, accompanied by patterns for\nmulti-tenant security and resource lifecycle management. These patterns address\nfundamental challenges in LLM-driven reporting applications and provide\npractical guidance for developers building them.", "AI": {"tldr": "The paper introduces practical architectural patterns\u2014including a dual-response model using ResourceLink\u2014to enable scalable, secure LLM-powered reporting systems by decoupling query generation from data retrieval and addressing context window limitations.", "motivation": "Large language models can translate natural language into database queries but are hindered by context window limitations, especially with large datasets common in reporting systems. Practical implementation patterns for overcoming these limits and building scalable architectures are lacking.", "method": "The paper presents architectural patterns, specifically a dual-response pattern that extends the Model Context Protocol's ResourceLink. This supports iterative query refinement and external data access, alongside methods to handle multi-tenant security and resource lifecycle.", "result": "The proposed patterns successfully decouple query generation from data retrieval, enable scalable usage of LLMs in reporting systems, and address security and resource management challenges.", "conclusion": "The research provides pragmatic solutions and patterns that overcome key limitations in LLM-driven reporting systems, empowering developers to build scalable, secure applications."}}
{"id": "2510.06000", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06000", "abs": "https://arxiv.org/abs/2510.06000", "authors": ["Daniel Otten", "Trevor Stalnaker", "Nathan Wintersgill", "Oscar Chaparro", "Denys Poshyvanyk"], "title": "Prompting in Practice: Investigating Software Developers' Use of Generative AI Tools", "comment": null, "summary": "The integration of generative artificial intelligence (GenAI) tools has\nfundamentally transformed software development. Although prompt engineering has\nemerged as a critical skill, existing research focuses primarily on individual\ntechniques rather than software developers' broader workflows. This study\npresents a systematic investigation of how software engineers integrate GenAI\ntools into their professional practice through a large-scale survey examining\nprompting strategies, conversation patterns, and reliability assessments across\nvarious software engineering tasks.\n  We surveyed 91 software engineers, including 72 active GenAI users, to\nunderstand AI usage patterns throughout the development process. Our 14 key\nfindings show that while code generation is nearly universal, proficiency\nstrongly correlates with using AI for more nuanced tasks such as debugging and\ncode review, and that developers prefer iterative multi-turn conversations to\nsingle-shot prompting. Documentation tasks are perceived as most reliable,\nwhile complex code generation and debugging present sizable challenges. Our\ninsights provide an empirical baseline of current developer practices, from\nsimple code generation to deeper workflow integration, with actionable insights\nfor future improvements.", "AI": {"tldr": "Surveying 91 software engineers, this work finds GenAI heavily used for code generation, with experienced users expanding its use to debugging and code review via iterative conversations. Documentation tasks are most reliable; complex coding and debugging remain challenging. The findings offer a baseline for future GenAI tool improvements in development workflows.", "motivation": "While prompt engineering as a skill is well-studied, there is a knowledge gap regarding how developers integrate GenAI into their broader workflows beyond individual techniques.", "method": "A large-scale survey of 91 software engineers (72 active GenAI users) was used, examining their prompting strategies, conversation patterns, and reliability assessments.", "result": "Code generation is a nearly universal application of GenAI. More proficient users apply GenAI to complex tasks\u2014debugging and code review\u2014often using iterative multi-turn conversations. Documentation leveraging GenAI is perceived as the most reliable, but challenges remain for complex code generation and debugging.", "conclusion": "The study provides foundational empirical insights into how software developers currently use GenAI tools, highlighting areas for future improvement."}}
{"id": "2510.06104", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06104", "abs": "https://arxiv.org/abs/2510.06104", "authors": ["Elijah Kayode Adejumo", "Brittany Johnson"], "title": "Explaining Code Risk in OSS: Towards LLM-Generated Fault Prediction Interpretations", "comment": null, "summary": "Open Source Software (OSS) has become a very important and crucial\ninfrastructure worldwide because of the value it provides. OSS typically\ndepends on contributions from developers across diverse backgrounds and levels\nof experience. Making safe changes, such as fixing a bug or implementing a new\nfeature, can be challenging, especially in object-oriented systems where\ncomponents are interdependent. Static analysis and defect-prediction tools\nproduce metrics (e.g., complexity,coupling) that flag potentially fault-prone\ncomponents, but these signals are often hard for contributors new or unfamiliar\nwith the codebase to interpret. Large Language Models (LLMs) have shown strong\nperformance on software engineering tasks such as code summarization and\ndocumentation generation. Building on this progress, we investigate whether\nLLMs can translate fault-prediction metrics into clear, human-readable risk\nexplanations and actionable guidance to help OSS contributors plan and review\ncode modifications. We outline explanation types that an LLM-generated\nassistant could provide (descriptive, contextual, and actionable explanations).\nWe also outline our next steps to assess usefulness through a task-based study\nwith OSS contributors, comparing metric-only baselines to LLM-generated\nexplanations on decision quality, time-to-completion, and error rates", "AI": {"tldr": "The paper explores using LLMs to generate clear risk explanations from fault-prediction metrics to help OSS contributors. They propose several explanation styles and plan to test usefulness through direct comparison with conventional metric cues.", "motivation": "OSS projects rely on contributors with varying expertise, and interpreting metric-based warnings about code risks is challenging for less experienced contributors. Improving their understanding and decision quality is vital for project health and safety.", "method": "The authors propose to use LLMs for generating descriptive, contextual, and actionable risk explanations based on software fault-prediction metrics, and plan to evaluate this approach via a task-based study comparing LLM-based guidance to traditional metric-only information.", "result": "The outlined approach has created explanation types for LLM-enabled code review assistance. The next step is to empirically assess the impact on decision quality, completion time, and error rates with OSS contributors.", "conclusion": "The paper concludes that leveraging LLMs to translate defect-prediction metrics into human-readable explanations and actionable advice has the potential to improve the ability of OSS contributors to interpret complex signals and safely plan or review code modifications."}}
{"id": "2510.06187", "categories": ["cs.SE", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.06187", "abs": "https://arxiv.org/abs/2510.06187", "authors": ["Griffin Pitts", "Aum Pandya", "Darsh Rank", "Tirth Bhatt", "Muntasir Hoq", "Bita Akram"], "title": "Automated Program Repair of Uncompilable Student Code", "comment": null, "summary": "A significant portion of student programming submissions in CS1 learning\nenvironments are uncompilable, limiting their use in student modeling and\ndownstream knowledge tracing. Traditional modeling pipelines often exclude\nthese cases, discarding observations of student learning. This study\ninvestigates automated program repair as a strategy to recover uncompilable\ncode while preserving students' structural intent for use in student modeling.\nWithin this framework, we assess large language models (LLMs) as repair agents,\nincluding GPT-5 (OpenAI), Claude 3.5 Haiku (Anthropic), and Gemini 2.5 Flash\n(Google), under high- and low-context prompting conditions. Repairs were\nevaluated for compilability, edit distance, and preservation of students'\noriginal structure and logic. We find that while all three LLMs are capable of\nproducing compilable repairs, their behavior diverges in how well they preserve\nstudents' control flow and code structure, which affects their pedagogical\nutility. By recovering uncompilable submissions, this work enables richer and\nmore comprehensive analyses of learners' coding processes and development over\ntime.", "AI": {"tldr": "The paper shows that LLMs can repair uncompilable student code, enabling richer learning analysis, though the fidelity of repairs to students' original logic and structure varies by model.", "motivation": "Many student programming submissions in CS1 are uncompilable, reducing their value for analyzing learning progress and knowledge tracing. Previous methods exclude these cases, losing potentially valuable learning data.", "method": "This study explores automated program repair using large language models (LLMs) such as GPT-5, Claude 3.5 Haiku, and Gemini 2.5 Flash. It evaluates these LLMs under high- and low-context prompting, measuring the compilability, edit distance, and preservation of students' code structure and logic in their repairs.", "result": "All three LLMs can produce compilable code from uncompilable submissions, but they differ in how well they preserve student intent, control flow, and code structure. These differences impact their usefulness for educational analysis.", "conclusion": "Applying LLMs for repairing uncompilable student code allows more comprehensive analysis of student learning and coding development, as previously excluded data can now be recovered with fidelity to the original intent."}}
