{"id": "2507.07480", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.07480", "abs": "https://arxiv.org/abs/2507.07480", "authors": ["Tobias Kapp\u00e9"], "title": "On Propositional Program Equivalence (extended abstract)", "comment": null, "summary": "General program equivalence is undecidable. However, if we abstract away the\nsemantics of statements, then this problem becomes not just decidable, but\npractically feasible. For instance, a program of the form \"if $b$ then $e$ else\n$f$\" should be equivalent to \"if not $b$ then $f$ else $e$\" - no matter what\n$b$, $e$ and $f$ are. This kind of equivalence is known as propositional\nequivalence. In this extended abstract, we discuss recent developments in\npropositional program equivalence from the perspective of (Guarded) Kleene\nAlgebra with Tests, or (G)KAT.", "AI": {"tldr": "General program equivalence is undecidable, but by using abstractions like propositional equivalence and mathematical frameworks such as (G)KAT, equivalence checking becomes both practical and feasible.", "motivation": "Program equivalence is generally undecidable, meaning it's impossible to always determine whether two arbitrary programs are equivalent. However, by abstracting away the semantics of statements, the problem can be transformed into one that is both theoretically and practically solvable. The motivation is to find practical equivalence notions and methods for programs where full equivalence is too difficult.", "method": "The paper focuses on studying a form of program equivalence known as propositional equivalence, specifically using the algebraic framework called (Guarded) Kleene Algebra with Tests (G(K)AT). This involves formalizing and reasoning about program equivalence at the propositional level, allowing for effective checking of certain equivalences.", "result": "Recent developments in using (G)KAT for propositional program equivalence are summarized, illustrating that this framework offers powerful tools for deciding equivalence at an abstract level. It is shown that many intuitive program transformations can be justified formally using (G)KAT.", "conclusion": "By working at the level of propositional equivalence and leveraging (G)KAT, it is possible to efficiently decide equivalence between many programs that would otherwise be too difficult to compare due to the undecidability of general program equivalence."}}
{"id": "2507.07325", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07325", "abs": "https://arxiv.org/abs/2507.07325", "authors": ["Martin Obaidi", "Marc Herrmann", "Elisa Schmid", "Raymond Ochsner", "Kurt Schneider", "Jil Kl\u00fcnder"], "title": "A German Gold-Standard Dataset for Sentiment Analysis in Software Engineering", "comment": "This paper has been accepted at the 33rd IEEE International\n  Requirements Engineering Workshop (REW 2025)", "summary": "Sentiment analysis is an essential technique for investigating the emotional\nclimate within developer teams, contributing to both team productivity and\nproject success. Existing sentiment analysis tools in software engineering\nprimarily rely on English or non-German gold-standard datasets. To address this\ngap, our work introduces a German dataset of 5,949 unique developer statements,\nextracted from the German developer forum Android-Hilfe.de. Each statement was\nannotated with one of six basic emotions, based on the emotion model by Shaver\net al., by four German-speaking computer science students. Evaluation of the\nannotation process showed high interrater agreement and reliability. These\nresults indicate that the dataset is sufficiently valid and robust to support\nsentiment analysis in the German-speaking software engineering community.\nEvaluation with existing German sentiment analysis tools confirms the lack of\ndomain-specific solutions for software engineering. We also discuss approaches\nto optimize annotation and present further use cases for the dataset.", "AI": {"tldr": "This paper presents a high-quality, emotion-annotated German sentiment dataset for software engineering, addressing a key resource gap. The dataset shows strong annotation reliability and reveals the need for more domain-specific sentiment analysis tools in German.", "motivation": "Current sentiment analysis tools for software engineering are mainly based on English or non-German data, lacking resources for the German-speaking community. There is a need for domain-specific datasets to strengthen sentiment analysis capabilities within German developer teams.", "method": "The authors created a dataset of 5,949 unique developer statements from the German forum Android-Hilfe.de. Each statement was annotated with one of six basic emotions by four German-speaking computer science students, following Shaver et al.'s emotion model. The annotation process was quantitatively evaluated for interrater agreement and reliability.", "result": "The annotation process delivered high interrater agreement and reliability, demonstrating that the dataset is valid and robust. When existing German sentiment analysis tools were evaluated on the dataset, they confirmed the lack of domain-specific solutions for software engineering. The study also provides insights on optimizing annotation and proposes additional use cases for the dataset.", "conclusion": "The newly introduced German sentiment dataset fills a significant gap for the German-speaking software engineering community, offering a robust resource for sentiment analysis. The dataset's reliability supports its use for research and practical applications, and future work can further optimize and extend its use cases."}}
{"id": "2507.07344", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07344", "abs": "https://arxiv.org/abs/2507.07344", "authors": ["Martin Obaidi", "Jannik Fischbach", "Jakob Droste", "Hannah Deters", "Marc Herrmann", "Jil Kl\u00fcnder", "Steffen Kr\u00e4tzig", "Hugo Villamizar", "Kurt Schneider"], "title": "Automatic Generation of Explainability Requirements and Software Explanations From User Reviews", "comment": "This paper has been accepted at the 33rd IEEE International\n  Requirements Engineering Workshop (REW 2025)", "summary": "Explainability has become a crucial non-functional requirement to enhance\ntransparency, build user trust, and ensure regulatory compliance. However,\ntranslating explanation needs expressed in user feedback into structured\nrequirements and corresponding explanations remains challenging. While existing\nmethods can identify explanation-related concerns in user reviews, there is no\nestablished approach for systematically deriving requirements and generating\naligned explanations. To contribute toward addressing this gap, we introduce a\ntool-supported approach that automates this process. To evaluate its\neffectiveness, we collaborated with an industrial automation manufacturer to\ncreate a dataset of 58 user reviews, each annotated with manually crafted\nexplainability requirements and explanations. Our evaluation shows that while\nAI-generated requirements often lack relevance and correctness compared to\nhuman-created ones, the AI-generated explanations are frequently preferred for\ntheir clarity and style. Nonetheless, correctness remains an issue,\nhighlighting the importance of human validation. This work contributes to the\nadvancement of explainability requirements in software systems by (1)\nintroducing an automated approach to derive requirements from user reviews and\ngenerate corresponding explanations, (2) providing empirical insights into the\nstrengths and limitations of automatically generated artifacts, and (3)\nreleasing a curated dataset to support future research on the automatic\ngeneration of explainability requirements.", "AI": {"tldr": "An automated tool was developed to extract explainability requirements and explanations from user reviews. While AI-generated explanations are often clear and stylistically preferred, their relevance and correctness need improvement, making human validation important. A curated dataset is released for further research.", "motivation": "There is an increasing demand for explainability in software to ensure transparency, build trust, and meet regulatory requirements. However, there is a lack of systematic methods to derive explainability requirements and corresponding explanations from user feedback.", "method": "The authors developed a tool-supported, automated approach that extracts explainability requirements and generates explanations from user reviews. They evaluated their approach by collaborating with an industrial automation company, using a dataset of 58 annotated user reviews with corresponding hand-crafted requirements and explanations.", "result": "The AI-generated requirements were often less relevant and accurate compared to human-generated ones, but the AI-generated explanations were frequently preferred for their clarity and style. However, correctness issues still exist in AI-generated outputs, emphasizing the need for human validation.", "conclusion": "This work presents an automated method to derive explainability requirements and explanations from user reviews, empirically highlights the current strengths (clarity, style) and weaknesses (correctness) of AI-generated artifacts, and provides a curated dataset for future research."}}
{"id": "2507.07468", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07468", "abs": "https://arxiv.org/abs/2507.07468", "authors": ["Sten Gr\u00fcner", "Nafise Eskandani"], "title": "Towards an Engineering Workflow Management System for Asset Administration Shells using BPMN", "comment": "7 pages, 7 figures, Accepted at IFAC EAAS 2025\n  (https://j3c.org/eaas.php)", "summary": "The integration of Industry 4.0 technologies into engineering workflows is an\nessential step toward automating and optimizing plant and process engineering\nprocesses. The Asset Administration Shell (AAS) serves as a key enabler for\ncreating interoperable Digital Twins that facilitate engineering data exchange\nand automation. This paper explores the use of AAS within engineering\nworkflows, particularly in combination with Business Process Model and Notation\n(BPMN) to define structured and automated processes. We propose a distributed\nAAS copy-on-write infrastructure that enhances security and scalability while\nenabling seamless cross organizational collaboration. We also introduce a\nworkflow management prototype automating AAS operations and engineering\nworkflows, improving efficiency and traceability.", "AI": {"tldr": "The paper proposes integrating AAS with BPMN in a distributed copy-on-write infrastructure, improving automation, security, collaboration, efficiency, and traceability in engineering workflows.", "motivation": "To address the need for automation and optimization in plant and process engineering by leveraging Industry 4.0 technologies, particularly the Asset Administration Shell (AAS) and interoperable Digital Twins.", "method": "The paper explores the integration of AAS in engineering workflows, combines it with BPMN for defining structured automated processes, proposes a distributed AAS copy-on-write infrastructure, and introduces a workflow management prototype that automates AAS operations.", "result": "A distributed AAS infrastructure is implemented that enhances security, scalability, and cross-organizational collaboration. A workflow management prototype is provided to automate operations, improving efficiency and traceability.", "conclusion": "Integrating AAS with BPMN and distributed infrastructure can significantly enhance automation, collaboration, and efficiency in engineering workflows."}}
{"id": "2507.07548", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07548", "abs": "https://arxiv.org/abs/2507.07548", "authors": ["Jonathan Ullrich", "Matthias Koch", "Andreas Vogelsang"], "title": "From Requirements to Code: Understanding Developer Practices in LLM-Assisted Software Engineering", "comment": "This paper has been accepted for publication at the 33rd IEEE\n  International Requirements Engineering (RE) conference", "summary": "With the advent of generative LLMs and their advanced code generation\ncapabilities, some people already envision the end of traditional software\nengineering, as LLMs may be able to produce high-quality code based solely on\nthe requirements a domain expert feeds into the system. The feasibility of this\nvision can be assessed by understanding how developers currently incorporate\nrequirements when using LLMs for code generation-a topic that remains largely\nunexplored. We interviewed 18 practitioners from 14 companies to understand how\nthey (re)use information from requirements and other design artifacts to feed\nLLMs when generating code. Based on our findings, we propose a theory that\nexplains the processes developers employ and the artifacts they rely on. Our\ntheory suggests that requirements, as typically documented, are too abstract\nfor direct input into LLMs. Instead, they must first be manually decomposed\ninto programming tasks, which are then enriched with design decisions and\narchitectural constraints before being used in prompts. Our study highlights\nthat fundamental RE work is still necessary when LLMs are used to generate\ncode. Our theory is important for contextualizing scientific approaches to\nautomating requirements-centric SE tasks.", "AI": {"tldr": "Despite LLMs' advanced capabilities, developers must still manually convert abstract requirements into detailed programming tasks supplemented with design and architecture info, indicating software engineering fundamentals remain necessary.", "motivation": "There is growing speculation that advanced generative LLMs could replace traditional software engineering by generating quality code directly from requirements. However, it is unclear how developers currently adapt and incorporate requirements into LLM prompts for code generation.", "method": "The researchers conducted interviews with 18 practitioners from 14 different companies to investigate how they utilize requirements and related design artifacts when leveraging LLMs for coding tasks.", "result": "The study found that standard requirements are too abstract to be provided directly to LLMs. Developers need to manually break down requirements into concrete tasks and supplement them with detailed design decisions and constraints before using them with LLMs.", "conclusion": "Manual refinement of requirements into actionable programming tasks is still essential, even when using LLMs for code generation. As such, foundational requirements engineering work remains crucial, and full automation of the process is not currently feasible."}}
{"id": "2507.07682", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07682", "abs": "https://arxiv.org/abs/2507.07682", "authors": ["Kaicheng Huang", "Fanyu Wang", "Yutan Huang", "Chetan Arora"], "title": "Prompt Engineering for Requirements Engineering: A Literature Review and Roadmap", "comment": null, "summary": "Advancements in large language models (LLMs) have led to a surge of prompt\nengineering (PE) techniques that can enhance various requirements engineering\n(RE) tasks. However, current LLMs are often characterized by significant\nuncertainty and a lack of controllability. This absence of clear guidance on\nhow to effectively prompt LLMs acts as a barrier to their trustworthy\nimplementation in the RE field. We present the first roadmap-oriented\nsystematic literature review of Prompt Engineering for RE (PE4RE). Following\nKitchenham's and Petersen's secondary-study protocol, we searched six digital\nlibraries, screened 867 records, and analyzed 35 primary studies. To bring\norder to a fragmented landscape, we propose a hybrid taxonomy that links\ntechnique-oriented patterns (e.g., few-shot, Chain-of-Thought) to task-oriented\nRE roles (elicitation, validation, traceability). Two research questions, with\nfive sub-questions, map the tasks addressed, LLM families used, and prompt\ntypes adopted, and expose current limitations and research gaps. Finally, we\noutline a step-by-step roadmap showing how today's ad-hoc PE prototypes can\nevolve into reproducible, practitioner-friendly workflows.", "AI": {"tldr": "This paper systematically reviews prompt engineering for requirements engineering, creates a hybrid taxonomy linking PE techniques to RE tasks, maps research coverage and gaps, and proposes a roadmap for reliable, practitioner-friendly PE workflows in RE.", "motivation": "Prompt engineering techniques for large language models (LLMs) show promise for requirements engineering (RE) tasks, but a lack of clarity and controllability in LLMs limits their reliable application in RE. There is no systematic guidance for using LLMs in this domain.", "method": "The authors conducted a systematic literature review following Kitchenham's and Petersen's protocols, searching six digital libraries, screening 867 records, and analyzing 35 primary studies relevant to prompt engineering for requirements engineering (PE4RE).", "result": "The study introduces a hybrid taxonomy connecting prompt engineering patterns (like few-shot and chain-of-thought) to specific RE tasks (such as elicitation, validation, and traceability). Mapping existing work, it identifies task coverage, LLM families, prompt types used, and uncovers current limitations and research gaps.", "conclusion": "The paper provides a roadmap to help advance prompt engineering for requirements engineering from prototype use toward systematic, reproducible workflows suitable for practitioners, addressing fragmentation and promoting trust in LLMs for RE."}}
{"id": "2507.07689", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07689", "abs": "https://arxiv.org/abs/2507.07689", "authors": ["Chetan Arora", "Fanyu Wang", "Chakkrit Tantithamthavorn", "Aldeida Aleti", "Shaun Kenyon"], "title": "From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry", "comment": null, "summary": "Requirements engineering (RE) in the space industry is inherently complex,\ndemanding high precision, alignment with rigorous standards, and adaptability\nto mission-specific constraints. Smaller space organisations and new entrants\noften struggle to derive actionable requirements from extensive, unstructured\ndocuments such as mission briefs, interface specifications, and regulatory\nstandards. In this innovation opportunity paper, we explore the potential of\nRetrieval-Augmented Generation (RAG) models to support and (semi-)automate\nrequirements generation in the space domain. We present a modular, AI-driven\napproach that preprocesses raw space mission documents, classifies them into\nsemantically meaningful categories, retrieves contextually relevant content\nfrom domain standards, and synthesises draft requirements using large language\nmodels (LLMs). We apply the approach to a real-world mission document from the\nspace domain to demonstrate feasibility and assess early outcomes in\ncollaboration with our industry partner, Starbound Space Solutions. Our\npreliminary results indicate that the approach can reduce manual effort,\nimprove coverage of relevant requirements, and support lightweight compliance\nalignment. We outline a roadmap toward broader integration of AI in RE\nworkflows, intending to lower barriers for smaller organisations to participate\nin large-scale, safety-critical missions.", "AI": {"tldr": "The paper explores how AI, specifically Retrieval-Augmented Generation models, can help automate and improve requirements engineering in the space industry, making it more accessible for smaller players. Early tests show reduced manual work and better compliance, suggesting a promising path for broader AI integration.", "motivation": "Requirements engineering in the space industry is challenging due to the complexity, the need for precision, strict compliance standards, and adaptation to specific mission needs. Smaller organizations or newcomers particularly face difficulties extracting actionable requirements from large, unstructured documents.", "method": "The paper proposes a modular, AI-driven framework that utilizes Retrieval-Augmented Generation (RAG) models. This approach preprocesses mission documents, categorizes them semantically, retrieves relevant content from standards, and uses large language models (LLMs) to synthesize draft requirements. The method is applied to a real-world mission document for demonstration and initial evaluation.", "result": "Preliminary results demonstrate that the approach can reduce manual requirements engineering efforts, increase relevant requirements coverage, and aid in compliance alignment. The paper shows feasibility with a real case study in collaboration with an industry partner.", "conclusion": "The AI-supported and semi-automated approach to requirements engineering in the space sector can democratize participation, especially for smaller organizations, by lowering barriers and improving efficiency. The authors propose a future roadmap for integrating AI more deeply into RE workflows."}}
