{"id": "2508.19449", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19449", "abs": "https://arxiv.org/abs/2508.19449", "authors": ["Md Afif Al Mamun", "Gias Uddin", "Lan Xia", "Longyu Zhang"], "title": "Stack Trace-Based Crash Deduplication with Transformer Adaptation", "comment": "This work is currently under review at IEEE Transactions on Software\n  Engineering. The replication package will be made publicly available upon\n  acceptance", "summary": "Automated crash reporting systems generate large volumes of duplicate\nreports, overwhelming issue-tracking systems and increasing developer workload.\nTraditional stack trace-based deduplication methods, relying on string\nsimilarity, rule-based heuristics, or deep learning (DL) models, often fail to\ncapture the contextual and structural relationships within stack traces. We\npropose dedupT, a transformer-based approach that models stack traces\nholistically rather than as isolated frames. dedupT first adapts a pretrained\nlanguage model (PLM) to stack traces, then uses its embeddings to train a\nfully-connected network (FCN) to rank duplicate crashes effectively. Extensive\nexperiments on real-world datasets show that dedupT outperforms existing DL and\ntraditional methods (e.g., sequence alignment and information retrieval\ntechniques) in both duplicate ranking and unique crash detection, significantly\nreducing manual triage effort. On four public datasets, dedupT improves Mean\nReciprocal Rank (MRR) often by over 15% compared to the best DL baseline and up\nto 9% over traditional methods while achieving higher Receiver Operating\nCharacteristic Area Under the Curve (ROC-AUC) in detecting unique crash\nreports. Our work advances the integration of modern natural language\nprocessing (NLP) techniques into software engineering, providing an effective\nsolution for stack trace-based crash deduplication.", "AI": {"tldr": "dedupT, a transformer-based crash deduplication approach, outperforms current methods in ranking duplicates and detecting unique crash reports, reducing developer workload and improving issue-tracking efficiency.", "motivation": "Automated crash reporting systems produce many duplicate reports, overwhelming issue-tracking systems and increasing developer workload. Existing deduplication methods often fail to capture the full context and relationships in stack traces.", "method": "The paper introduces dedupT, a transformer-based method. It adapts a pretrained language model (PLM) to stack traces and uses the resulting embeddings with a fully-connected network (FCN) to rank duplicate crashes more effectively.", "result": "dedupT outperforms both traditional and existing deep learning methods in duplicate ranking and unique crash detection across four public datasets, showing over 15% improvement in Mean Reciprocal Rank compared to the best DL baseline and up to 9% versus traditional techniques, along with higher ROC-AUC for unique report detection.", "conclusion": "dedupT provides a more accurate and efficient solution for stack trace-based crash deduplication, lowering manual triage efforts and advancing the use of NLP in software engineering."}}
{"id": "2508.19558", "categories": ["cs.SE", "cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.19558", "abs": "https://arxiv.org/abs/2508.19558", "authors": ["Zhuohao Li", "Wenqing Chen", "Jianxing Yu", "Zhichao Lu"], "title": "Functional Consistency of LLM Code Embeddings: A Self-Evolving Data Synthesis Framework for Benchmarking", "comment": null, "summary": "Embedding models have demonstrated strong performance in tasks like\nclustering, retrieval, and feature extraction while offering computational\nadvantages over generative models and cross-encoders. Benchmarks such as MTEB\nhave shown that text embeddings from large language models (LLMs) capture rich\nsemantic information, but their ability to reflect code-level functional\nsemantics remains unclear. Existing studies largely focus on code clone\ndetection, which emphasizes syntactic similarity and overlooks functional\nunderstanding. In this paper, we focus on the functional consistency of LLM\ncode embeddings, which determines if two code snippets perform the same\nfunction regardless of syntactic differences. We propose a novel data synthesis\nframework called Functionality-Oriented Code Self-Evolution to construct\ndiverse and challenging benchmarks. Specifically, we define code examples\nacross four semantic and syntactic categories and find that existing datasets\npredominantly capture syntactic properties. Our framework generates four unique\nvariations from a single code instance, providing a broader spectrum of code\nexamples that better reflect functional differences. Extensive experiments on\nthree downstream tasks-code clone detection, code functional consistency\nidentification, and code retrieval-demonstrate that embedding models\nsignificantly improve their performance when trained on our evolved datasets.\nThese results highlight the effectiveness and generalization of our data\nsynthesis framework, advancing the functional understanding of code.", "AI": {"tldr": "The paper proposes a novel data synthesis framework that generates functionally diverse code benchmarks, enabling embedding models to better capture code functionalities (not just syntax) and improving their performance across various code-related tasks.", "motivation": "Current embedding models are strong in text-related tasks but their ability to capture code-level functional semantics is unclear. Previous work focuses mostly on syntactic similarity (like clone detection) rather than on functional equivalence in code, which is a significant gap.", "method": "The authors introduce a new data synthesis framework called Functionality-Oriented Code Self-Evolution. This framework creates benchmarks by defining code examples across four semantic and syntactic categories, and generates four unique variations from a single code instance to better reflect functional differences. Extensive experiments are conducted on tasks like code clone detection, functional consistency identification, and code retrieval.", "result": "When embedding models are trained on the evolved datasets generated by the proposed framework, their performance on downstream tasks improves significantly. The framework broadens the scope of code variations to include more functionally informative examples.", "conclusion": "The framework effectively advances the training and evaluation of embedding models towards functional understanding of code. It outperforms traditional datasets that focus too much on syntax, and promotes generalization across multiple tasks."}}
{"id": "2508.19610", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.19610", "abs": "https://arxiv.org/abs/2508.19610", "authors": ["Kathrin Figl", "Maria Kirchner", "Sebastian Baltes", "Michael Felderer"], "title": "The Influence of Code Comments on the Perceived Helpfulness of Stack Overflow Posts", "comment": "31 pages, 7 figures, 2 tables, to appear in the Empirical Software\n  Engineering journal", "summary": "Question-and-answer platforms such as Stack Overflow have become an important\nway for software developers to share and retrieve knowledge. However, reusing\npoorly understood code can lead to serious problems, such as bugs or security\nvulnerabilities. To better understand how code comments affect the perceived\nhelpfulness of Stack Overflow answers, we conducted an online experiment\nsimulating a Stack Overflow environment (n=91). The results indicate that both\nblock and inline comments are perceived as significantly more helpful than\nuncommented source code. Moreover, novices rated code snippets with block\ncomments as more helpful than those with inline comments. Interestingly, other\nsurface features, such as the position of an answer and its answer score, were\nconsidered less important. The content of Stack Overflow has been a major\nsource for training large language models. AI-based coding assistants such as\nGitHub Copilot, which are based on these models, might change the way Stack\nOverflow is used. However, our findings have implications beyond this specific\nplatform. First, they may help to improve the relevance of community-driven\nplatforms such as Stack Overflow, which provide human advice and explanations\nof code solutions, complementing AI-based support for software developers.\nSecond, since chat-based AI tools can be prompted to generate code in different\nways, knowing which properties influence perceived helpfulness might lead to\ntargeted prompting strategies to generate more readable code snippets.", "AI": {"tldr": "Code snippets with comments (especially block comments for novices) are rated as much more helpful on Stack Overflow than those without, suggesting that effective commenting is crucial for useful code sharing and can inform both platform design and AI code generation.", "motivation": "The paper aims to understand how code comments affect the perceived helpfulness of answers on Q&A platforms like Stack Overflow, given that poorly understood code can lead to problems like bugs or vulnerabilities.", "method": "The researchers conducted an online experiment simulating Stack Overflow, with 91 participants evaluating code snippets with different types of comments (block, inline, none) and other answer features (such as position and score).", "result": "Both block and inline comments made code snippets appear significantly more helpful than uncommented code. Novices in particular found block comments more helpful than inline ones. Other answer features like position and score were less important to perceived helpfulness.", "conclusion": "Code comments play a vital role in making code snippets more helpful on Stack Overflow. These insights can guide improvements for both human advice on community-driven platforms and AI-based coding assistants, by highlighting the importance of commenting style and content over superficial features."}}
{"id": "2508.19663", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.19663", "abs": "https://arxiv.org/abs/2508.19663", "authors": ["Lola Solovyeva", "Eduardo Carneiro Oliveira", "Shiyu Fan", "Alper Tuncay", "Shamil Gareev", "Andrea Capiluppi"], "title": "Leveraging LLMs for Automated Translation of Legacy Code: A Case Study on PL/SQL to Java Transformation", "comment": null, "summary": "The VT legacy system, comprising approximately 2.5 million lines of PL/SQL\ncode, lacks consistent documentation and automated tests, posing significant\nchallenges for refactoring and modernisation. This study investigates the\nfeasibility of leveraging large language models (LLMs) to assist in translating\nPL/SQL code into Java for the modernised \"VTF3\" system. By leveraging a dataset\ncomprising 10 PL/SQL-to-Java code pairs and 15 Java classes, which collectively\nestablished a domain model for the translated files, multiple LLMs were\nevaluated. Furthermore, we propose a customized prompting strategy that\nintegrates chain-of-guidance reasoning with $n$-shot prompting. Our findings\nindicate that this methodology effectively guides LLMs in generating\nsyntactically accurate translations while also achieving functional\ncorrectness. However, the findings are limited by the small sample size of\navailable code files and the restricted access to test cases used for\nvalidating the correctness of the generated code. Nevertheless, these findings\nlay the groundwork for scalable, automated solutions in modernising large\nlegacy systems.", "AI": {"tldr": "This paper explores using large language models to translate PL/SQL legacy code to Java for easier modernization, showing promising initial results but noting limitations due to a small dataset and limited testing.", "motivation": "The legacy VT system, consisting of 2.5 million PL/SQL lines, has poor documentation and lacks automated tests, making modernization and refactoring very difficult.", "method": "The study used a dataset of 10 PL/SQL-to-Java code pairs and 15 Java classes to evaluate several large language models (LLMs). A custom prompting strategy was developed that combines chain-of-guidance reasoning and n-shot prompting to improve translation quality.", "result": "The proposed methodology successfully guided LLMs to produce syntactically correct and functionally accurate PL/SQL-to-Java translations. However, the results are restricted by the small dataset and lack of comprehensive testing resources.", "conclusion": "LLMs, when guided by a tailored prompting strategy, can be effective in modernizing legacy codebases by translating PL/SQL to Java, but further research with larger datasets and better test coverage is needed for scalable adoption."}}
{"id": "2508.19797", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.19797", "abs": "https://arxiv.org/abs/2508.19797", "authors": ["Joan Giner-Miguelez", "Abel G\u00f3mez", "Jordi Cabot"], "title": "Enabling Content Management Systems as an Information Source in Model-driven Projects", "comment": null, "summary": "Content Management Systems (CMSs) are the most popular tool when it comes to\ncreate and publish content across the web. Recently, CMSs have evolved,\nbecoming \\emph{headless}. Content served by a \\emph{headless CMS} aims to be\nconsumed by other applications and services through REST APIs rather than by\nhuman users through a web browser. This evolution has enabled CMSs to become a\nnotorious source of content to be used in a variety of contexts beyond pure web\nnavigation. As such, CMS have become an important component of many information\nsystems. Unfortunately, we still lack the tools to properly discover and manage\nthe information stored in a CMS, often highly customized to the needs of a\nspecific domain. Currently, this is mostly a time-consuming and error-prone\nmanual process.\n  In this paper, we propose a model-based framework to facilitate the\nintegration of headless CMSs in software development processes. Our framework\nis able to discover and explicitly represent the information schema behind the\nCMS. This facilitates designing the interaction between the CMS model and other\ncomponents consuming that information. These interactions are then generated as\npart of a middleware library that offers platform-agnostic access to the CMS to\nall the client applications. The complete framework is open-source and\navailable online.", "AI": {"tldr": "Headless CMSs are powerful yet tricky to integrate due to their custom schemas. This paper presents an open-source, model-based framework that automates schema discovery and generates middleware, making integration easier and less error-prone.", "motivation": "Headless CMSs have become increasingly important for delivering content to a variety of applications, not just web browsers. However, discovering and managing their highly customized schemas is still a complex, manual, and error-prone process.", "method": "The authors introduce a model-based framework that automatically discovers and explicitly represents the information schema of a headless CMS. This framework also generates middleware for platform-agnostic access to the CMS for all client applications.", "result": "The framework enables easier integration of headless CMSs into software development workflows by simplifying schema discovery and providing reusable middleware. The solution is open-source and publicly available.", "conclusion": "The proposed framework provides a systematic and efficient approach for integrating headless CMSs, improving schema management, reducing manual effort, and enabling broader and easier consumption of CMS-provided content."}}
{"id": "2508.19803", "categories": ["cs.SE", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.19803", "abs": "https://arxiv.org/abs/2508.19803", "authors": ["Peter Fettke", "Wolfgang Reisig"], "title": "Towards a fundamental theory of modeling discrete systems", "comment": "6 pages, 2 figures, author prepared version of final manuscript\n  accepted at the 44th International Conference on Conceptual Modeling, 20-23\n  October 2025, Poitiers / Futuroscope, France, Workshop on Fundamentals of\n  Conceptual Modeling (FCM)", "summary": "Modeling is a central concern in both science and engineering. However, we\nneed a new fundamental theory to address the challenges of the digital age. In\nthis paper, we first explain why modeling is fundamental and which challenges\nmust be addressed in the digital world. As a main contribution, we introduce\nthe Heraklit modeling framework as a new approach to modeling. We conclude with\nsome general remarks. Future work will involve the correctness of modeling, the\nnotion of information, and the description of invariance in modeling.", "AI": {"tldr": "This paper highlights the need for a new modeling theory for digital challenges and proposes the Heraklit framework to address these issues, setting the stage for future research.", "motivation": "Traditional modeling approaches struggle to address challenges unique to the digital era, such as complexity, correctness, and dynamic information.", "method": "The paper introduces and explains a new theoretical framework called Heraklit for modeling.", "result": "Heraklit framework is presented as a novel approach, aiming to serve as a basis for further development in modeling theories for the digital age.", "conclusion": "The paper provides foundational groundwork, with plans for future research into correctness, information theory, and invariance in modeling."}}
{"id": "2508.19834", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.19834", "abs": "https://arxiv.org/abs/2508.19834", "authors": ["Antero Taivalsaari", "Tommi Mikkonen", "Cesare Pautasso"], "title": "On the Future of Software Reuse in the Era of AI Native Software Engineering", "comment": "21 pages", "summary": "Software development is currently under a paradigm shift in which artificial\nintelligence and generative software reuse are taking the center stage in\nsoftware creation. Earlier opportunistic software reuse practices and organic\nsoftware development methods are rapidly being replaced by \"AI Native\"\napproaches in which developers place their trust on code that has been\ngenerated by artificial intelligence. This is leading to a new form of software\nreuse that is conceptually not all that different from cargo cult development.\nIn this paper we discuss the implications of AI-assisted generative software\nreuse, bring forth relevant questions, and define a research agenda for\ntackling the central issues associated with this emerging approach.", "AI": {"tldr": "AI-driven generative software reuse is transforming development, creating new challenges similar to cargo cult programming. This paper discusses these issues and suggests future research directions.", "motivation": "The motivation is to address the rapid paradigm shift towards AI Native software development, where AI-generated code is increasingly trusted and reused, raising concerns similar to cargo cult development.", "method": "The authors discuss the implications of AI-assisted generative software reuse, raise relevant questions, and propose a research agenda to address the central challenges.", "result": "The paper identifies the central issues of AI-assisted generative software reuse, outlines relevant questions, and provides a research agenda for further inquiry.", "conclusion": "The paper concludes that AI-assisted generative software reuse is fundamentally changing software development practices, warranting critical examination and a dedicated research agenda to address emerging challenges."}}
{"id": "2508.19882", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19882", "abs": "https://arxiv.org/abs/2508.19882", "authors": ["Qunying Song", "He Ye", "Mark Harman", "Federica Sarro"], "title": "Generative AI for Testing of Autonomous Driving Systems: A Survey", "comment": "67 pages, 6 figures, 29 tables", "summary": "Autonomous driving systems (ADS) have been an active area of research, with\nthe potential to deliver significant benefits to society. However, before\nlarge-scale deployment on public roads, extensive testing is necessary to\nvalidate their functionality and safety under diverse driving conditions.\nTherefore, different testing approaches are required, and achieving effective\nand efficient testing of ADS remains an open challenge. Recently, generative AI\nhas emerged as a powerful tool across many domains, and it is increasingly\nbeing applied to ADS testing due to its ability to interpret context, reason\nabout complex tasks, and generate diverse outputs. To gain a deeper\nunderstanding of its role in ADS testing, we systematically analyzed 91\nrelevant studies and synthesized their findings into six major application\ncategories, primarily centered on scenario-based testing of ADS. We also\nreviewed their effectiveness and compiled a wide range of datasets, simulators,\nADS, metrics, and benchmarks used for evaluation, while identifying 27\nlimitations. This survey provides an overview and practical insights into the\nuse of generative AI for testing ADS, highlights existing challenges, and\noutlines directions for future research in this rapidly evolving field.", "AI": {"tldr": "Generative AI is increasingly used for testing autonomous driving systems, mainly for generating diverse test scenarios. This survey of 91 studies summarizes application areas, effectiveness, and evaluation tools, while also highlighting key challenges and future research opportunities.", "motivation": "Autonomous driving systems (ADS) promise significant societal benefits but require thorough testing to ensure safety and reliability before widespread deployment. Traditional testing faces challenges in covering diverse scenarios efficiently and effectively, sparking interest in new approaches like generative AI.", "method": "The authors conducted a systematic analysis of 91 relevant studies, synthesizing their findings into six major categories related to scenario-based testing of ADS. They reviewed the effectiveness of generative AI approaches, cataloged datasets, simulators, ADS, metrics, and benchmarks used, and identified key limitations in current research.", "result": "Generative AI is mainly applied in scenario-based testing for ADS, offering advantages in generating diverse and complex testing scenarios. The survey organizes the current landscape, highlights application areas, reviews evaluation tools, and identifies 27 significant limitations in the field.", "conclusion": "Generative AI holds substantial promise for enhancing the testing of ADS but faces notable challenges and limitations. The survey provides a comprehensive overview, identifies gaps, and suggests future research directions for the effective integration of generative AI in ADS testing."}}
{"id": "2508.20086", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20086", "abs": "https://arxiv.org/abs/2508.20086", "authors": ["Youwei Huang", "Jianwen Li", "Sen Fang", "Yao Li", "Peng Yang", "Bin Hu", "Tao Zhang"], "title": "Smart Contract Intent Detection with Pre-trained Programming Language Model", "comment": "10 pages, 5 figures, conference", "summary": "Malicious intent in smart contract development can lead to substantial\neconomic losses. SmartIntentNN is a deep learning model specifically designed\nto identify unsafe intents in smart contracts. This model integrates the\nUniversal Sentence Encoder, a K-means clustering-based intent highlighting\nmechanism, and a Bidirectional Long Short-Term Memory network for multi-label\nclassification, achieving an F1 of 0.8633 in distinguishing ten different\nintent categories. In this study, we present an upgraded version of this model,\nSmartIntentNN2 (Smart Contract Intent Neural Network V2). A significant\nenhancement in V2 is the incorporation of a BERT-based pre-trained language\nmodel, which has been trained on a dataset of 16,000 real smart contracts using\na Masked Language Modeling objective. SmartIntentNN2 retains the BiLSTM-based\nmulti-label classification network. With an improved F1 of 0.927, V2\ndemonstrates enhanced performance compared to its predecessor, establishing\nitself as the state-of-the-art model for smart contract intent detection.", "AI": {"tldr": "The paper introduces SmartIntentNN2, a deep learning model that improves smart contract intent detection using BERT and BiLSTM. Trained on 16,000 real contracts, it achieves an F1 score of 0.927, outperforming earlier models and establishing itself as state-of-the-art in detecting malicious or unsafe intent in smart contract code.", "motivation": "Malicious intent in smart contracts can cause severe economic damage. Detecting such unsafe intentions during smart contract development is crucial to improving blockchain security and trust.", "method": "The paper presents SmartIntentNN2, an upgraded deep learning model for smart contract intent detection. It integrates a BERT-based pre-trained language model (trained on 16,000 contracts with a Masked Language Modeling objective) alongside a Bidirectional Long Short-Term Memory (BiLSTM) network for multi-label classification. The approach builds on earlier architectures (Universal Sentence Encoder, K-means clustering for intent highlighting) while focusing on improved intent detection.", "result": "SmartIntentNN2 achieves a high F1 score of 0.927 in detecting ten different intent categories within smart contracts, outperforming its predecessor SmartIntentNN (F1 of 0.8633).", "conclusion": "SmartIntentNN2 sets a new benchmark for smart contract intent detection, offering state-of-the-art performance thanks to the integration of a BERT-based pretrained language model and a BiLSTM classifier."}}
