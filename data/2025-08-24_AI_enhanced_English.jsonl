{"id": "2508.15135", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15135", "abs": "https://arxiv.org/abs/2508.15135", "authors": ["Sumudu Liyanage", "Sherlock A. Licorish", "Markus Wagner", "Stephen G. MacDonell"], "title": "On the need to perform comprehensive evaluations of automated program repair benchmarks: Sorald case study", "comment": null, "summary": "In supporting the development of high-quality software, especially necessary\nin the era of LLMs, automated program repair (APR) tools aim to improve code\nquality by automatically addressing violations detected by static analysis\nprofilers. Previous research tends to evaluate APR tools only for their ability\nto clear violations, neglecting their potential introduction of new (sometimes\nsevere) violations, changes to code functionality and degrading of code\nstructure. There is thus a need for research to develop and assess\ncomprehensive evaluation frameworks for APR tools. This study addresses this\nresearch gap, and evaluates Sorald (a state-of-the-art APR tool) as a proof of\nconcept. Sorald's effectiveness was evaluated in repairing 3,529 SonarQube\nviolations across 30 rules within 2,393 Java code snippets extracted from Stack\nOverflow. Outcomes show that while Sorald fixes specific rule violations, it\nintroduced 2,120 new faults (32 bugs, 2088 code smells), reduced code\nfunctional correctness--as evidenced by a 24% unit test failure rate--and\ndegraded code structure, demonstrating the utility of our framework. Findings\nemphasize the need for evaluation methodologies that capture the full spectrum\nof APR tool effects, including side effects, to ensure their safe and effective\nadoption.", "AI": {"tldr": "This paper shows that automated program repair tools like Sorald can fix code issues but may also introduce new faults and degrade code quality, suggesting the need for more thorough evaluation frameworks.", "motivation": "Automated program repair (APR) tools are increasingly important for improving software quality, especially with the rise of LLMs. However, current assessments of APR tools often overlook negative side effects such as new violations and reduced code quality.", "method": "The paper develops a comprehensive evaluation framework for APR tools and applies it to Sorald, evaluating its repair of 3,529 SonarQube rule violations across 2,393 Java snippets from Stack Overflow.", "result": "Sorald successfully fixed specific rule violations but introduced 2,120 new faults (32 bugs, 2,088 code smells), led to a 24% unit test failure rate, and degraded code structure.", "conclusion": "Current APR tool evaluations are insufficient and need to include a broader range of effects, including potential harm, to ensure safe adoption. The presented framework highlights these issues and advocates for more comprehensive evaluation methodologies."}}
{"id": "2508.15411", "categories": ["cs.SE", "cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.15411", "abs": "https://arxiv.org/abs/2508.15411", "authors": ["Frederik Vandeputte"], "title": "Foundational Design Principles and Patterns for Building Robust and Adaptive GenAI-Native Systems", "comment": null, "summary": "Generative AI (GenAI) has emerged as a transformative technology,\ndemonstrating remarkable capabilities across diverse application domains.\nHowever, GenAI faces several major challenges in developing reliable and\nefficient GenAI-empowered systems due to its unpredictability and inefficiency.\nThis paper advocates for a paradigm shift: future GenAI-native systems should\nintegrate GenAI's cognitive capabilities with traditional software engineering\nprinciples to create robust, adaptive, and efficient systems.\n  We introduce foundational GenAI-native design principles centered around five\nkey pillars -- reliability, excellence, evolvability, self-reliance, and\nassurance -- and propose architectural patterns such as GenAI-native cells,\norganic substrates, and programmable routers to guide the creation of resilient\nand self-evolving systems. Additionally, we outline the key ingredients of a\nGenAI-native software stack and discuss the impact of these systems from\ntechnical, user adoption, economic, and legal perspectives, underscoring the\nneed for further validation and experimentation. Our work aims to inspire\nfuture research and encourage relevant communities to implement and refine this\nconceptual framework.", "AI": {"tldr": "This paper proposes foundational principles and new architectural patterns for building reliable, efficient, and adaptive GenAI-native systems, advocating for integration of AI with traditional software engineering to address current GenAI challenges.", "motivation": "Generative AI (GenAI) has shown significant promise, but its adoption in systems is hindered by unpredictability and inefficiency, motivating the search for more reliable and efficient ways to build GenAI-powered systems.", "method": "The paper introduces five foundational design principles (reliability, excellence, evolvability, self-reliance, and assurance) and proposes architectural patterns such as GenAI-native cells, organic substrates, and programmable routers. It also specifies key components for a GenAI-native software stack and discusses multi-perspective impacts.", "result": "A conceptual framework for building GenAI-native systems that integrates AI capabilities with classical software engineering, guiding the design of resilient, adaptive, and efficient systems, and encouraging validation and iteration by the community.", "conclusion": "Combining AI's cognitive strengths with established software engineering through new design principles and architectural patterns will foster the development of robust and self-evolving GenAI-native systems."}}
{"id": "2508.15423", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15423", "abs": "https://arxiv.org/abs/2508.15423", "authors": ["Ruiqi Wang", "Zezhou Yang", "Cuiyun Gao", "Xin Xia", "Qing Liao"], "title": "An Empirical Study of Knowledge Distillation for Code Understanding Tasks", "comment": "Accepted by ICSE 2026 (Cycle 1)", "summary": "Pre-trained language models (PLMs) have emerged as powerful tools for code\nunderstanding. However, deploying these PLMs in large-scale applications faces\npractical challenges due to their computational intensity and inference\nlatency. Knowledge distillation (KD), a promising model compression and\nacceleration technique, addresses these limitations by transferring knowledge\nfrom large teacher models to compact student models, enabling efficient\ninference while preserving most of the teacher models' capabilities. While this\ntechnique has shown remarkable success in natural language processing and\ncomputer vision domains, its potential for code understanding tasks remains\nlargely underexplored.\n  In this paper, we systematically investigate the effectiveness and usage of\nKD in code understanding tasks. Our study encompasses two popular types of KD\nmethods, i.e., logit-based and feature-based KD methods, experimenting across\neight student models and two teacher PLMs from different domains on three\ndownstream tasks. The experimental results indicate that KD consistently offers\nnotable performance boosts across student models with different sizes compared\nwith standard fine-tuning. Notably, code-specific PLM demonstrates better\neffectiveness as the teacher model. Among all KD methods, the latest\nfeature-based KD methods exhibit superior performance, enabling student models\nto retain up to 98% teacher performance with merely 5% parameters. Regarding\nstudent architecture, our experiments reveal that similarity with teacher\narchitecture does not necessarily lead to better performance. We further\ndiscuss the efficiency and behaviors in the KD process and inference, summarize\nthe implications of findings, and identify promising future directions.", "AI": {"tldr": "Knowledge distillation greatly improves small code-understanding models, with new methods allowing them to perform almost as well as much larger models using far fewer resources; using code-specialized teacher models helps, and matching architectures does not always yield better results.", "motivation": "Deploying large pre-trained language models (PLMs) for code understanding is hampered by their high computational demands and slow inference. Although model compression via knowledge distillation (KD) offers a way to address this, its potential in code understanding tasks has yet to be comprehensively explored.", "method": "The paper systematically evaluates knowledge distillation techniques for code understanding. It compares logit-based and feature-based KD methods, using eight student models and two teacher PLMs across three code understanding tasks. The study examines performance retention, parameter efficiency, and the influence of architectural similarity.", "result": "Knowledge distillation significantly enhances the performance of compact student models compared to standard fine-tuning, with feature-based methods enabling student models to retain up to 98% of the teacher models\u2019 performance while using only 5% of the parameters. Code-specific PLMs work better as teachers, and student-teacher architectural similarity does not guarantee better outcomes.", "conclusion": "Knowledge distillation, especially feature-based methods, is highly effective for compressing PLMs in code understanding tasks, enabling efficient deployment without substantial loss in performance. Architectural similarity between student and teacher is not a decisive factor for success. The paper also highlights efficiency considerations and proposes future research directions."}}
{"id": "2508.15495", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15495", "abs": "https://arxiv.org/abs/2508.15495", "authors": ["Dongjun Yu", "Xiao Yan", "Zhenrui Li", "Jipeng Xiao", "Haochuan He", "Yongda Yu", "Hao Zhang", "Guoping Rong", "Xiaobo Huang"], "title": "SynthCoder: A Synthetical Strategy to Tune LLMs for Code Completion", "comment": null, "summary": "Code completion is a prominent application of Large Language Models (LLMs) in\nsoftware engineering. Due to the near real-time response requirements of this\ntask, base models with small to medium-sized parameters are typically employed,\nsupplemented by various optimization and post-training techniques. However,\nthese optimization methods often have trade-offs, leading to a seesaw effect\nwhere performance improvements on certain datasets or metrics are accompanied\nby degradations on others -- sometimes even falling below the baseline model's\nperformance. This paper proposes SynthCoder, a model that integrates leading\nindustry practices to achieve state-of-the-art performance on the\nFill-in-the-Middle (FIM) code completion task. In specific, we first construct\na diverse dataset by combining Abstract Syntax Tree (AST) node extraction with\nheuristics that simulate developer behavior. Then we enrich our training corpus\nwith cross-file contextual information using the BM25 algorithm and call\ngraphs, enhancing the model's ability to perform code completion in both\nfile-level and repository-level scenarios. As the last step, we employ a\ntwo-stage training process using the Seed-Coder-8B-Base as the base model.\nFirst, we fine-tune the model using Curriculum Learning technology. Following\nthis, we perform alignment using Direct Preference Optimization (DPO) with\npreference pairs generated through Rejection Sampling. Experimental results\ndemonstrate that our final model excels on mainstream repository-level code\ncompletion benchmarks, including aiXcoder, ExecRepoBench, CrossCodeEval, and\nCoLT. Furthermore, our carefully curated training set effectively mitigates the\nmodel's tendency to just repeat existing code, a common issue existing in\nvarious code completion models.", "AI": {"tldr": "SynthCoder is a code completion model that uses an enriched and diverse training set combined with advanced training strategies to outperform prior models on major benchmarks, while also avoiding issues like excessive code repetition.", "motivation": "Code completion with LLMs is important for software engineering, but optimizing models for this task often results in a trade-off between performance on different datasets or metrics. Existing methods can even degrade performance below the baseline in some cases. The motivation is to address these trade-offs and build a model that consistently excels in code completion.", "method": "The authors construct a diverse dataset by combining AST node extraction with heuristics simulating developer behavior and enrich it with cross-file context using the BM25 algorithm and call graphs. Their system, SynthCoder, uses a two-stage training pipeline: first, Curriculum Learning is applied to fine-tune the base model (Seed-Coder-8B-Base), and then Direct Preference Optimization (DPO) is used, employing preference pairs generated via Rejection Sampling for alignment.", "result": "SynthCoder achieves state-of-the-art results on mainstream repository-level code completion benchmarks like aiXcoder, ExecRepoBench, CrossCodeEval, and CoLT. The novel training set also reduces the model's tendency to simply repeat existing code, addressing a key limitation of previous approaches.", "conclusion": "SynthCoder successfully integrates industry best practices and novel data enrichment strategies to overcome common trade-offs in LLM-based code completion, establishing new performance standards on multiple benchmarks and improving result diversity."}}
{"id": "2508.15109", "categories": ["cs.PL", "D.3.0; F.3.1"], "pdf": "https://arxiv.org/pdf/2508.15109", "abs": "https://arxiv.org/abs/2508.15109", "authors": ["Ziteng Wang", "Ruijie Fang", "Linus Zheng", "Dixin Tang", "Isil Dillig"], "title": "Homomorphism Calculus for User-Defined Aggregations", "comment": null, "summary": "Data processing frameworks like Apache Spark and Flink provide built-in\nsupport for user-defined aggregation functions (UDAFs), enabling the\nintegration of domain-specific logic. However, for these frameworks to support\n\\emph{efficient} UDAF execution, the function needs to satisfy a\n\\emph{homomorphism property}, which ensures that partial results from\nindependent computations can be merged correctly. Motivated by this problem,\nthis paper introduces a novel \\emph{homomorphism calculus} that can both verify\nand refute whether a UDAF is a dataframe homomorphism. If so, our calculus also\nenables the construction of a corresponding merge operator which can be used\nfor incremental computation and parallel execution. We have implemented an\nalgorithm based on our proposed calculus and evaluate it on real-world UDAFs,\ndemonstrating that our approach significantly outperforms two leading\nsynthesizers.", "AI": {"tldr": "The paper presents a homomorphism calculus that automates checking and enabling efficient parallel execution of user-defined aggregation functions in data frameworks, and demonstrates clear performance improvements over existing methods.", "motivation": "Efficient execution of user-defined aggregation functions (UDAFs) in data processing frameworks requires that these functions possess the homomorphism property, which allows for correct merging of partial results. However, verifying this property and constructing the necessary merge operators can be challenging.", "method": "The paper introduces a novel 'homomorphism calculus' capable of verifying and refuting whether a UDAF satisfies the dataframe homomorphism property. If the property is verified, the calculus also facilitates the automatic construction of an appropriate merge operator for incremental and parallel computation.", "result": "An algorithm implementing the homomorphism calculus was developed and tested on real-world UDAFs. The evaluation shows that this approach significantly outperforms two leading synthesizers in terms of efficiency.", "conclusion": "The proposed homomorphism calculus provides a practical solution for verifying homomorphism properties of UDAFs, as well as constructing merge operators, enhancing the efficiency of user-defined aggregation in data processing frameworks like Spark and Flink."}}
{"id": "2508.15496", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15496", "abs": "https://arxiv.org/abs/2508.15496", "authors": ["Elena Masserini", "Diego Clerissi", "Daniela Micucci", "Jo\u00e3o R. Campos", "Leonardo Mariani"], "title": "Towards the Assessment of Task-based Chatbots: From the TOFU-R Snapshot to the BRASATO Curated Dataset", "comment": "10 pages, 10 figure, Accepted at IEEE International Symposium on\n  Software Reliability Engineering (ISSRE) 2025", "summary": "Task-based chatbots are increasingly being used to deliver real services, yet\nassessing their reliability, security, and robustness remains underexplored,\nalso due to the lack of large-scale, high-quality datasets. The emerging\nautomated quality assessment techniques targeting chatbots often rely on\nlimited pools of subjects, such as custom-made toy examples, or outdated, no\nlonger available, or scarcely popular agents, complicating the evaluation of\nsuch techniques. In this paper, we present two datasets and the tool support\nnecessary to create and maintain these datasets. The first dataset is RASA\nTASK-BASED CHATBOTS FROM GITHUB (TOFU-R), which is a snapshot of the Rasa\nchatbots available on GitHub, representing the state of the practice in\nopen-source chatbot development with Rasa. The second dataset is BOT RASA\nCOLLECTION (BRASATO), a curated selection of the most relevant chatbots for\ndialogue complexity, functional complexity, and utility, whose goal is to ease\nreproducibility and facilitate research on chatbot reliability.", "AI": {"tldr": "This paper introduces two comprehensive datasets, TOFU-R and BRASATO, alongside supporting tools, to advance research in evaluating task-based chatbot reliability by overcoming existing barriers like poor dataset availability and reproducibility.", "motivation": "The motivation is to address the lack of high-quality, large-scale datasets for evaluating the reliability, security, and robustness of task-based chatbots. Existing automated quality assessment methods are hampered by limited, outdated, or unavailable chatbot sources.", "method": "The paper presents and maintains two new datasets: TOFU-R (a snapshot of Rasa chatbots from GitHub) and BRASATO (a curated collection of notable chatbots), along with supporting tools and documentation.", "result": "The authors provide the TOFU-R and BRASATO datasets, enabling reproducibility and more effective research on chatbot reliability, with diverse examples representing real-world and functionally complex agents.", "conclusion": "These contributions significantly facilitate research in chatbot quality assessment by offering accessible, updated, and relevant datasets which overcome previous limitations in the field."}}
{"id": "2508.15137", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15137", "abs": "https://arxiv.org/abs/2508.15137", "authors": ["Ruijie Fang", "Zachary Kincaid", "Thomas Reps"], "title": "Software Model Checking via Summary-Guided Search (Extended Version)", "comment": "Preliminary manuscript of extended version of paper that will appear\n  in OOPSLA 2025. 36 pages", "summary": "In this work, we describe a new software model-checking algorithm called GPS.\nGPS treats the task of model checking a program as a directed search of the\nprogram states, guided by a compositional, summary-based static analysis. The\nsummaries produced by static analysis are used both to prune away infeasible\npaths and to drive test generation to reach new, unexplored program states. GPS\ncan find both proofs of safety and counter-examples to safety (i.e., inputs\nthat trigger bugs), and features a novel two-layered search strategy that\nrenders it particularly efficient at finding bugs in programs featuring long,\ninput-dependent error paths. To make GPS refutationally complete (in the sense\nthat it will find an error if one exists, if it is allotted enough time), we\nintroduce an instrumentation technique and show that it helps GPS achieve\nrefutation-completeness without sacrificing overall performance. We benchmarked\nGPS on a suite of benchmarks including both programs from the Software\nVerification Competition (SV-COMP) and from prior literature, and found that\nour implementation of GPS outperforms state-of-the-art software model checkers\n(including the top performers in SV-COMP ReachSafety-Loops category), both in\nterms of the number of benchmarks solved and in terms of running time.", "AI": {"tldr": "GPS is a new software model-checking algorithm that efficiently discovers bugs and proofs, outperforming leading tools in both benchmarks solved and speed, through guided search and static analysis summaries.", "motivation": "Traditional software model checkers can struggle to efficiently find bugs, especially in programs with long, input-dependent error paths, and often balance between completeness and performance.", "method": "They propose GPS, a new model-checking algorithm that uses directed search of program states guided by compositional, summary-based static analysis. The algorithm prunes infeasible paths, generates tests to explore new program states, and uses a novel two-layered search strategy. An additional instrumentation technique is introduced for refutational completeness.", "result": "GPS is shown to be refutationally complete (capable of finding errors if enough time is given). Empirical benchmarks on SV-COMP and literature programs show GPS outperforms state-of-the-art model checkers both in number of benchmarks solved and runtime.", "conclusion": "GPS provides an efficient, refutationally complete model checker that effectively finds bugs and proofs of safety, with superior performance compared to current best tools."}}
{"id": "2508.15503", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15503", "abs": "https://arxiv.org/abs/2508.15503", "authors": ["Sebastian Baltes", "Florian Angermeir", "Chetan Arora", "Marvin Mu\u00f1oz Bar\u00f3n", "Chunyang Chen", "Lukas B\u00f6hme", "Fabio Calefato", "Neil Ernst", "Davide Falessi", "Brian Fitzgerald", "Davide Fucci", "Marcos Kalinowski", "Stefano Lambiase", "Daniel Russo", "Mircea Lungu", "Lutz Prechelt", "Paul Ralph", "Christoph Treude", "Stefan Wagner"], "title": "Evaluation Guidelines for Empirical Studies in Software Engineering involving LLMs", "comment": "Draft of evaluation guidelines for empirical studies in software\n  engineering involving LLMs (see also llm-guidelines.org)", "summary": "Large language models (LLMs) are increasingly being integrated into software\nengineering (SE) research and practice, yet their non-determinism, opaque\ntraining data, and evolving architectures complicate the reproduction and\nreplication of empirical studies. We present a community effort to scope this\nspace, introducing a taxonomy of LLM-based study types together with eight\nguidelines for designing and reporting empirical studies involving LLMs. The\nguidelines present essential (must) criteria as well as desired (should)\ncriteria and target transparency throughout the research process. Our\nrecommendations, contextualized by our study types, are: (1) to declare LLM\nusage and role; (2) to report model versions, configurations, and fine-tuning;\n(3) to document tool architectures; (4) to disclose prompts and interaction\nlogs; (5) to use human validation; (6) to employ an open LLM as a baseline; (7)\nto report suitable baselines, benchmarks, and metrics; and (8) to openly\narticulate limitations and mitigations. Our goal is to enable reproducibility\nand replicability despite LLM-specific barriers to open science. We maintain\nthe study types and guidelines online as a living resource for the community to\nuse and shape (llm-guidelines.org).", "AI": {"tldr": "The paper identifies key reproducibility issues with LLMs in software engineering and offers a detailed taxonomy and eight practical guidelines to improve study transparency, reproducibility, and replicability. These resources are available online for the community to refine and adopt.", "motivation": "The paper is motivated by the challenges of applying large language models (LLMs) in software engineering (SE) research, especially due to LLMs' non-determinism, hidden training data, and ever-changing architectures which hamper reproducibility and replication in empirical studies.", "method": "The authors conducted a community-driven effort to map the field, establishing a taxonomy for LLM-based studies and crafting eight practical guidelines for the design and reporting of SE experiments using LLMs.", "result": "The result is the introduction of a taxonomy for study types and a set of eight clear guidelines, categorized by essential and desirable transparency criteria. These guidelines aim to improve reproducibility and replicability in LLM-focused SE research and are hosted online as an evolving community resource.", "conclusion": "The study concludes that following these guidelines will help overcome LLM-specific obstacles to open science, facilitating more transparent, reproducible, and replicable empirical research in SE. The guidelines and study types are intended to be living resources that can adapt and improve with community input."}}
{"id": "2508.15157", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.15157", "abs": "https://arxiv.org/abs/2508.15157", "authors": ["David M Kahn", "Jan Hoffmann", "Runming Li"], "title": "Big-Stop Semantics: A Simple Way to Get the Benefits of Small-Step Semantics in a Big-Step Judgment", "comment": "26 pages, 27 figures", "summary": "As evident in the programming language literature, many practitioners favor\nspecifying dynamic program behavior using big-step over small-step semantics.\nUnlike small-step semantics, which must dwell on every intermediate program\nstate, big-step semantics conveniently jump directly to the ever-important\nresult of the computation. Big-step semantics also typically involve fewer\ninference rules than their small-step counterparts. However, in exchange for\nergonomics, big-step semantics give up power: Small-step semantics describes\nprogram behaviors that are outside the grasp of big-step semantics, notably\ndivergence. This work presents a little-known extension of big-step semantics\nwith inductive definitions that captures diverging computations without\nintroducing error states. This big-stop semantics is illustrated for typed,\nuntyped, and effectful variants of PCF, as well as a while-loop-based\nimperative language. Big-stop semantics extends the standard big-step inference\nrules with a few additional rules to define an evaluation judgment that is\nequivalent to the reflexive-transitive closure of small-step transitions. This\nsimple extension contrasts with other solutions in the literature which\nsacrifice ergonomics by introducing many additional inference rules, global\nstate, and/or less-commonly-understood reasoning principles like coinduction.", "AI": {"tldr": "This paper presents a straightforward extension to big-step semantics\u2014called 'big-stop semantics'\u2014using a handful of new rules, letting it describe diverging computations (like infinite loops) efficiently. Unlike previous approaches, it preserves simplicity, making it practical for language designers and theorists.", "motivation": "Big-step semantics, while more ergonomic and requiring fewer inference rules than small-step semantics, traditionally cannot describe divergence (non-terminating behavior). The motivation is to extend big-step semantics to capture such behavior without resorting to complex or less-practical techniques used in previous literature.", "method": "The authors extend traditional big-step semantics by adding a few inductive rules, allowing the semantics to describe not only terminating but also diverging computations, and illustrate this approach across several language variants, including PCF and imperative languages.", "result": "The proposed 'big-stop semantics' provides a semantics that is equivalent to the reflexive-transitive closure of small-step semantics, enables direct reasoning about divergence, and maintains the simplicity and practicality of traditional big-step style, avoiding error states or complicated transformers like coinduction.", "conclusion": "The paper introduces 'big-stop semantics', an extension to big-step semantics, which elegantly captures both normal and diverging computations without sacrificing the ergonomic advantages of big-step semantics."}}
{"id": "2508.15512", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15512", "abs": "https://arxiv.org/abs/2508.15512", "authors": ["Markus Borg", "Martin Larsson", "Philip Breid", "Nadim Hagatulah"], "title": "QUPER-MAn: Benchmark-Guided Target Setting for Maintainability Requirements", "comment": "Accepted at the 1st International Workshop on Responsible Software\n  Engineering", "summary": "Maintainable source code is essential for sustainable development in any\nsoftware organization. Unfortunately, many studies show that maintainability\noften receives less attention than its importance warrants. We argue that\nrequirements engineering can address this gap the problem by fostering\ndiscussions and setting appropriate targets in a responsible manner. In this\npreliminary work, we conducted an exploratory study of industry practices\nrelated to requirements engineering for maintainability. Our findings confirm\nprevious studies: maintainability remains a second-class quality concern.\nExplicit requirements often make sweeping references to coding conventions.\nTools providing maintainability proxies are common but typically only used in\nimplicit requirements related to engineering practices. To address this, we\npropose QUPER-MAn, a maintainability adaption of the QUPER model, which was\noriginally developed to help organizations set targets for performance\nrequirements. Developed using a design science approach, QUPER-MAn, integrates\nmaintainability benchmarks and supports target setting. We posit that it can\nshift maintainability from an overlooked development consequence to an actively\nmanaged goal driven by informed and responsible engineering decisions.", "AI": {"tldr": "The study confirms that maintainability is often neglected in software development, typically handled by generic coding guidelines and minimally used tools. The authors suggest that using their QUPER-MAn model can help organizations set clear, measurable maintainability targets, making it a central concern in engineering processes.", "motivation": "Maintainability is critical for sustainable software development, yet often underemphasized in practice. The motivation is to address this gap by using requirements engineering to promote proper attention and goal-setting for maintainable code.", "method": "An exploratory industry study examining current practices in requirements engineering concerning maintainability. The authors then propose QUPER-MAn, a maintainability-focused adaptation of the QUPER model, using a design science approach.", "result": "Findings confirm previous studies: maintainability is under-prioritized, mostly referenced in broad terms like coding conventions. Maintainability tools are present but used only implicitly. The proposed QUPER-MAn model incorporates benchmarks and supports explicit target-setting for maintainability.", "conclusion": "QUPER-MAn can transform maintainability from an afterthought to a managed, goal-oriented quality attribute in software engineering, based on informed decisions."}}
{"id": "2508.15166", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.15166", "abs": "https://arxiv.org/abs/2508.15166", "authors": ["Jingbo Wang", "Shashin Halalingaiah", "Weiyi Chen", "Chao Wang", "Isil Dillig"], "title": "Probabilistic Inference for Datalog with Correlated Inputs", "comment": "Accepted for publication at OOPSLA 2025 (R2)", "summary": "Probabilistic extensions of logic programming languages, such as ProbLog,\nintegrate logical reasoning with probabilistic inference to evaluate\nprobabilities of output relations; however, prior work does not account for\npotential statistical correlations among input facts. This paper introduces\nPraline, a new extension to Datalog designed for precise probabilistic\ninference in the presence of (partially known) input correlations. We formulate\nthe inference task as a constrained optimization problem, where the solution\nyields sound and precise probability bounds for output facts. However, due to\nthe complexity of the resulting optimization problem, this approach alone often\ndoes not scale to large programs. To address scalability, we propose a more\nefficient $\\delta$-exact inference algorithm that leverages constraint solving,\nstatic analysis, and iterative refinement. Our empirical evaluation on\nchallenging real-world benchmarks, including side-channel analysis,\ndemonstrates that our method not only scales effectively but also delivers\ntight probability bounds.", "AI": {"tldr": "This paper introduces Praline, improving probabilistic logic programming with a scalable approach to handle correlated inputs, achieving precise and scalable probabilistic inference in real-world scenarios.", "motivation": "Existing probabilistic logic programming languages like ProbLog do not handle statistical correlations among input facts, limiting the precision of probabilistic inference.", "method": "Praline, a new Datalog extension, models the inference task as a constrained optimization problem to capture input correlations and uses a scalable delta-exact inference algorithm combining constraint solving, static analysis, and iterative refinement.", "result": "Empirical results on real-world benchmarks, such as side-channel analysis, show that Praline scales well and provides precise (tight) probability bounds for output facts.", "conclusion": "Praline advances probabilistic logic programming by enabling sound and precise probabilistic inference even in the presence of correlated input facts, overcoming scalability limitations of earlier approaches."}}
{"id": "2508.15536", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15536", "abs": "https://arxiv.org/abs/2508.15536", "authors": ["Yi Zhang", "He Jiang", "Xiaochen Li", "Shikai Guo", "Peiyu Zou", "Zun Wang"], "title": "A Novel Mutation Based Method for Detecting FPGA Logic Synthesis Tool Bugs", "comment": null, "summary": "FPGA (Field-Programmable Gate Array) logic synthesis tools are key components\nin the EDA (Electronic Design Automation) toolchain. They convert hardware\ndesigns written in description languages such as Verilog into gate-level\nrepresentations for FPGAs. However, defects in these tools may lead to\nunexpected behaviors and pose security risks. Therefore, it is crucial to\nharden these tools through testing. Although several methods have been proposed\nto automatically test FPGA logic synthesis tools, the challenge remains of\ninsufficient semantic and logical complexity in test programs. In this paper,\nwe propose VERMEI, a new method for testing FPGA logic synthesis tools. VERMEI\nconsists of three modules: preprocessing, equivalent mutation, and bug\nidentification. The preprocessing module identifies zombie logic (inactive code\nwith no impact on the circuit output) in seed programs through simulation and\ncoverage analysis. The equivalent mutation module generates equivalent variants\nof seed programs by pruning or inserting logic fragments in zombie areas. It\nuses Bayesian sampling to extract logic fragments from historical Verilog\ndesigns, making the generated variants have complex control flows and\nstructures. The bug identification module, based on differential testing,\ncompares the synthesized outputs of seed and variant programs to identify bugs.\nExperiments on Yosys, Vivado, and Quartus demonstrate that VERMEI outperforms\nthe state-of-the-art methods. Within five months, VERMEI reported 15 bugs to\nvendors, 9 of which were confirmed as new.", "AI": {"tldr": "The paper proposes VERMEI, a novel automated testing method for FPGA logic synthesis tools, which identifies and exploits inactive code (zombie logic) to generate complex test cases, leading to superior bug discovery compared to state-of-the-art approaches.", "motivation": "FPGA logic synthesis tools are essential but can have defects that pose functional and security risks. Existing automated testing methods struggle to generate test programs with sufficient semantic and logical complexity to robustly evaluate these tools.", "method": "The proposed method, VERMEI, includes three modules: 1) Preprocessing, which finds zombie logic through simulation and coverage analysis; 2) Equivalent mutation, which generates complex program variants by manipulating zombie logic using fragments sampled from historical Verilog designs; 3) Bug identification via differential testing, comparing outputs to identify discrepancies.", "result": "In extensive experiments on popular synthesis tools (Yosys, Vivado, Quartus), VERMEI was able to identify more bugs than previous methods. Over a five-month period, VERMEI discovered 15 bugs, 9 of which were confirmed as new by vendors.", "conclusion": "VERMEI significantly improves the effectiveness of automated testing for FPGA logic synthesis tools by generating complex and diverse test cases that reveal more bugs compared to existing techniques."}}
{"id": "2508.15264", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.15264", "abs": "https://arxiv.org/abs/2508.15264", "authors": ["Patrick Redmond", "Jonathan Castello", "Jos\u00e9 Manuel Calder\u00f3n Trilla", "Lindsey Kuper"], "title": "Exploring the Theory and Practice of Concurrency in the Entity-Component-System Pattern", "comment": "This is an extended version (with appendices) of the OOPSLA 2025\n  paper", "summary": "The Entity-Component-System (ECS) software design pattern, long used in game\ndevelopment, encourages a clean separation of identity (entities), data\nproperties (components), and computational behaviors (systems). Programs\nwritten using the ECS pattern are naturally concurrent, and the pattern offers\nmodularity, flexibility, and performance benefits that have led to a\nproliferation of ECS frameworks. Nevertheless, the ECS pattern is little-known\nand not well understood outside of a few domains. Existing explanations of the\nECS pattern tend to be mired in the concrete details of particular ECS\nframeworks, or they explain the pattern in terms of imperfect metaphors or in\nterms of what it is not. We seek a rigorous understanding of the ECS pattern\nvia the design of a formal model, Core ECS, that abstracts away the details of\nspecific implementations to reveal the essence of software using the ECS\npattern. We identify a class of Core ECS programs that behave deterministically\nregardless of scheduling, enabling use of the ECS pattern as a\ndeterministic-by-construction concurrent programming model. With Core ECS as a\npoint of comparison, we then survey several real-world ECS frameworks and find\nthat they all leave opportunities for deterministic concurrency unexploited.\nOur findings point out a space for new ECS implementation techniques that\nbetter leverage such opportunities.", "AI": {"tldr": "This paper formalizes the ECS design pattern with Core ECS, highlights unexplored opportunities for deterministic concurrency in current frameworks, and calls for improved implementations based on this deeper understanding.", "motivation": "Although the Entity-Component-System (ECS) pattern offers modularity, flexibility, and performance, its rigorous understanding is limited, and its usage outside of game development and similar domains is minimal. Existing explanations often focus too much on specific frameworks or use metaphors, rather than providing a clear, abstract definition.", "method": "The authors design a formal model called Core ECS to abstractly define the ECS pattern, removing implementation details. They identify classes of programs within this model that are deterministic regardless of scheduling. The authors also survey several existing ECS frameworks, comparing them to their formal model.", "result": "They found that real-world ECS frameworks do not fully exploit opportunities for deterministic concurrency. Their formal Core ECS model demonstrates such capabilities, indicating possibilities for new implementations that provide better deterministic concurrent behavior.", "conclusion": "A formal, abstract model (Core ECS) clarifies the essence and power of the ECS pattern, especially with respect to deterministic concurrency, revealing limitations and untapped potential in modern ECS frameworks."}}
{"id": "2508.15570", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15570", "abs": "https://arxiv.org/abs/2508.15570", "authors": ["Marion Wiese", "Kamila Serwa", "Anastasia Besier", "Ariane S. Marion-Jetten", "Eva Bittner"], "title": "Establishing Technical Debt Management -- A Five-Step Workshop Approach and an Action Research Study", "comment": "Accepted for publication by the Journal of Systems and Software --\n  Special Issue on Managing Technical Debt in Software-Intensive Products and\n  Services", "summary": "Context. Technical debt (TD) items are constructs in a software system\nproviding short-term benefits but hindering future changes. TD management (TDM)\nis frequently researched but rarely adopted in practice. Goal. This study aimed\nto establish a TDM process in an IT company based on a predefined workshop\nconcept. We analyzed which research approaches practitioners adopted for each\nTD activity and the TDM's long-term effect on TD awareness. Method. We used\naction research (five action cycles in 16 months) with an IT team that creates\nIT solutions for signal processing. To examine TD awareness, we (1) analyzed\nquestionnaires completed during each workshop, (2) observed team meetings, (3)\nadopted a method from psychology for measuring awareness in decision-making\nsituations called TD-SAGAT, and (4) evaluated the backlog data. Results.\nPractitioners preferred TD repayment and prioritization based on the system's\nevolution and cost calculations, i.e., repayment of so-called low-hanging\nfruits. Reminders in the backlog items, such as checkboxes or text templates,\nled to a sustainable rise in TD awareness. Conclusions. We showed that a\nworkshop-based approach is feasible and leads to sustainable process changes.\nNew ideas for TDM applicable to other IT teams emerged, e.g., using a\nre-submission date, using a Talked about TD checkbox, and using visualizations\nfor TD prioritization.", "AI": {"tldr": "Implementing workshop-based technical debt management processes in an IT team led to greater and sustained TD awareness, with practical approaches such as backlog reminders and new prioritization methods proving effective.", "motivation": "Technical debt (TD) management is commonly studied in research but seldom implemented effectively in practice. The paper seeks to bridge the gap between theoretical approaches and practical adoption in software teams.", "method": "The study used action research over five action cycles spanning 16 months with an IT team specialized in signal processing. Data was collected through workshop questionnaires, meeting observations, a psychology-based awareness measurement tool (TD-SAGAT), and backlog data analysis.", "result": "Practitioners mainly focus TD repayment and prioritization on system evolution and cost, often addressing the so-called 'low-hanging fruits.' Backlog reminders such as checkboxes or templates significantly and sustainably increased TD awareness among the team.", "conclusion": "A workshop-based approach for technical debt management is feasible in real-world IT teams and results in sustainable changes. Innovative TDM practices emerged from this process, such as re-submission dates, discussion checkboxes, and visual tools for prioritization, which could be applied elsewhere."}}
{"id": "2508.15333", "categories": ["cs.PL", "F.3.3"], "pdf": "https://arxiv.org/pdf/2508.15333", "abs": "https://arxiv.org/abs/2508.15333", "authors": ["Francesco Dagnino", "Paola Giannini", "Violet Ka I Pun", "Ulises Torrella"], "title": "Fair Termination for Resource-Aware Active Objects", "comment": "18 pages, 12 pages of appendix, 12 figures, APLAS 2025", "summary": "Active object systems are a model of distributed computation that has been\nadopted for modelling distributed systems and business process workflows. This\nfield of modelling is, in essence, concurrent and resource-aware, motivating\nthe development of resource-aware formalisations on the active object model.\nThe contributions of this work are the development of a core calculus for\nresource-aware active objects together with a type system ensuring that\nwell-typed programs are fairly terminating, i.e., they can always eventually\nterminate. To achieve this, we combine techniques from graded semantics and\ntype systems, which are quite well understood for sequential programs, with\nthose for fair termination, which have been developed for synchronous~sessions.", "AI": {"tldr": "This paper introduces a core calculus and type system for resource-aware active object systems, ensuring that well-typed programs always fairly terminate by integrating techniques from graded semantics and fair termination.", "motivation": "The motivation is to provide a formal model for distributed and concurrent systems that are resource-aware, to improve modelling of distributed computations such as business workflows. Ensuring fairness and termination is a key concern in such systems.", "method": "The paper develops a core calculus for resource-aware active objects, together with a type system. It combines graded semantics and type system techniques from sequential programming with fair termination methods from synchronous sessions.", "result": "The result is a formal framework and type system where any well-typed program is guaranteed to be fairly terminating, ensuring programs will eventually terminate under fair scheduling.", "conclusion": "The proposed calculus and type system ensure fair termination in resource-aware active object systems, advancing concurrency modelling with termination guarantees."}}
{"id": "2508.15584", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15584", "abs": "https://arxiv.org/abs/2508.15584", "authors": ["Maria Teresa Rossi", "Leonardo Mariani", "Oliviero Riganelli"], "title": "From PREVENTion to REACTion: Enhancing Failure Resolution in Naval Systems", "comment": null, "summary": "Complex and large industrial systems often misbehave, for instance, due to\nwear, misuse, or faults. To cope with these incidents, it is important to\ntimely detect their occurrences, localize the sources of the problems, and\nimplement the appropriate countermeasures. This paper reports our experience\nwith a state-of-the-art failure prediction method, PREVENT, and its extension\nwith a troubleshooting module, REACT, applied to naval systems developed by\nFincantieri. Our results show how to integrate anomaly detection with\ntroubleshooting procedures. We conclude by discussing a lesson learned, which\nmay help deploy and extend these analyses to other industrial products.", "AI": {"tldr": "The paper presents an integrated approach to predicting and troubleshooting failures in industrial systems using PREVENT and REACT, showing successful application in naval systems and offering insights for wider use.", "motivation": "Industrial systems often experience failures due to factors like wear, misuse, or faults. Detecting and managing these failures quickly is critical to maintaining system integrity and reliability.", "method": "The paper applies PREVENT, a failure prediction method, and extends it with a troubleshooting module called REACT. Both techniques are evaluated in the context of naval systems to demonstrate their effectiveness in integrating anomaly detection with practical troubleshooting.", "result": "Experiments show successful integration of anomaly detection and troubleshooting procedures in naval systems. The authors share practical lessons that can aid in deploying such approaches in other industrial settings.", "conclusion": "The research demonstrates that combining state-of-the-art failure prediction (PREVENT) with a troubleshooting module (REACT) enhances detection and management of faults in industrial systems. The integrated approach is effective and holds promise for broader adoption."}}
{"id": "2508.15576", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.15576", "abs": "https://arxiv.org/abs/2508.15576", "authors": ["Andreas L\u00f6\u00f6w", "Seung Hoon Park", "Daniele Nantes-Sobrinho", "Sacha-\u00c9lie Ayoun", "Opale Sj\u00f6stedt", "Philippa Gardner"], "title": "Compositional Symbolic Execution for the Next 700 Memory Models (Extended Version)", "comment": null, "summary": "Multiple successful compositional symbolic execution (CSE) tools and\nplatforms exploit separation logic (SL) for compositional verification and/or\nincorrectness separation logic (ISL) for compositional bug-finding, including\nVeriFast, Viper, Gillian, CN, and Infer-Pulse. Previous work on the Gillian\nplatform, the only CSE platform that is parametric on the memory model, meaning\nthat it can be instantiated to different memory models, suggests that the\nability to use custom memory models allows for more flexibility in supporting\nanalysis of a wide range of programming languages, for implementing custom\nautomation, and for improving performance. However, the literature lacks a\nsatisfactory formal foundation for memory-model-parametric CSE platforms.\n  In this paper, inspired by Gillian, we provide a new formal foundation for\nmemory-model-parametric CSE platforms. Our foundation advances the state of the\nart in four ways. First, we mechanise our foundation (in the interactive\ntheorem prover Rocq). Second, we validate our foundation by instantiating it to\na broad range of memory models, including models for C and CHERI. Third,\nwhereas previous memory-model-parametric work has only covered SL analyses, we\ncover both SL and ISL analyses. Fourth, our foundation is based on standard\ndefinitions of SL and ISL (including definitions of function specification\nvalidity, to ensure sound interoperation with other tools and platforms also\nbased on standard definitions).", "AI": {"tldr": "This paper introduces a formal foundation for symbolic execution platforms that support multiple memory models, enabling broader language and analysis coverage with assured compatibility and rigor.", "motivation": "Existing compositional symbolic execution (CSE) platforms benefit from customizable memory models for verification and bug-finding. While tools like Gillian allow memory model parametricity, there is no satisfactory formal foundation for such parametric CSE platforms.", "method": "The authors introduce a novel formal foundation for memory-model-parametric CSE platforms, mechanizing it in the Rocq interactive theorem prover. They validate their approach by instantiating it with various memory models, including those for C and CHERI. Their foundation accommodates both separation logic (SL) and incorrectness separation logic (ISL) analyses, built upon standard definitions.", "result": "The proposed formal foundation is mechanized in Rocq, supports a wide array of memory models, accommodates both SL and ISL, and uses standard definitions to ensure compatibility with existing tools.", "conclusion": "The new foundation provides the necessary formal underpinnings for memory-model-parametric CSE platforms, enhancing flexibility, usability for different languages and analyses, and facilitating sound interoperability with other tools."}}
{"id": "2508.15750", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.15750", "abs": "https://arxiv.org/abs/2508.15750", "authors": ["Celeste Barnaby", "Qiaochu Chen", "Ramya Ramalingam", "Osbert Bastani", "Isil Dillig"], "title": "Active Learning for Neurosymbolic Program Synthesis", "comment": null, "summary": "The goal of active learning for program synthesis is to synthesize the\ndesired program by asking targeted questions that minimize user interaction.\nWhile prior work has explored active learning in the purely symbolic setting,\nsuch techniques are inadequate for the increasingly popular paradigm of\nneurosymbolic program synthesis, where the synthesized program incorporates\nneural components. When applied to the neurosymbolic setting, such techniques\ncan -- and, in practice, do -- return an unintended program due to\nmispredictions of neural components. This paper proposes a new active learning\ntechnique that can handle the unique challenges posed by neural network\nmispredictions. Our approach is based upon a new evaluation strategy called\nconstrained conformal evaluation (CCE), which accounts for neural\nmispredictions while taking into account user-provided feedback. Our proposed\nmethod iteratively makes CCE more precise until all remaining programs are\nguaranteed to be observationally equivalent. We have implemented this method in\na tool called SmartLabel and experimentally evaluated it on three neurosymbolic\ndomains. Our results demonstrate that SmartLabel identifies the ground truth\nprogram for 98% of the benchmarks, requiring under 5 rounds of user interaction\non average. In contrast, prior techniques for active learning are only able to\nconverge to the ground truth program for at most 65% of the benchmarks.", "AI": {"tldr": "This paper introduces a new active learning method for neurosymbolic program synthesis, addressing neural mispredictions by using constrained conformal evaluation (CCE). The implemented tool, SmartLabel, outperforms previous approaches, achieving near-perfect accuracy and fewer user interactions in experiments.", "motivation": "Active learning in program synthesis aims to minimize user interaction by asking targeted questions. Traditional methods work well in symbolic settings but struggle with neurosymbolic program synthesis due to neural component mispredictions.", "method": "The paper proposes a new active learning approach, introducing constrained conformal evaluation (CCE), which considers neural mispredictions and integrates user feedback. The method iteratively refines CCE until only observationally equivalent programs remain. Implementation is done in a tool called SmartLabel, tested in three neurosymbolic domains.", "result": "SmartLabel identifies the ground truth program in 98% of benchmarks, requiring less than 5 rounds of user interaction on average. Previous methods only achieve up to 65% accuracy in finding the correct program.", "conclusion": "The presented active learning technique, based on CCE, substantially improves the effectiveness and efficiency of program synthesis in neurosymbolic settings by better handling neural mispredictions and reducing user interaction."}}
