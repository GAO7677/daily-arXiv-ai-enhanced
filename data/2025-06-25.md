<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 10]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Black-Box Test Code Fault Localization Driven by Large Language Models and Execution Estimation](https://arxiv.org/abs/2506.19045)
*Ahmadreza Saboor Yaraghi,Golnaz Gharachorlu,Sakina Fatima,Lionel C. Briand,Ruiyuan Wan,Ruifeng Gao*

Main category: cs.SE

TL;DR: This paper introduces a novel static approach using LLMs to localize faults in test scripts based on a single failure log, showing high accuracy and efficiency without needing repeated executions or system code access.


<details>
  <summary>Details</summary>
Motivation: Traditional fault localization requires repeated executions, which is infeasible for non-deterministic failures or expensive executions. Existing LLM-based methods typically focus on the system under test, neglecting fault localization in complex test code, which often triggers failures in practice.

Method: The authors propose a fully static, LLM-driven approach for localizing faults in system test code, without running the test case. They estimate likely failure-related execution traces from a single failure log using three novel algorithms to prune irrelevant code, then use LLMs prompted by this trace and the error message to rank possible faulty code locations. This black-box method works without requiring system under test source code.

Result: Evaluations on an industrial dataset show that their estimated traces achieve an F1 score of about 90% compared to actual traces. Further, their code-pruning significantly reduces LLM inference time (up to 34%) without harming localization performance. Block-level analysis offers a strong practical trade-off, getting an 81% top-3 hit rate while keeping sufficient context.

Conclusion: A new static, LLM-based method enables effective fault localization in system test code without execution or access to SUT code, achieving high accuracy, efficiency, and practical trade-offs in real-world test case datasets.

Abstract: Fault localization (FL) is a critical step in debugging which typically
relies on repeated executions to pinpoint faulty code regions. However,
repeated executions can be impractical in the presence of non-deterministic
failures or high execution costs. While recent efforts have leveraged Large
Language Models (LLMs) to aid execution-free FL, these have primarily focused
on identifying faults in the system under test (SUT) rather than in the often
complex system test code. However, the latter is also important as, in
practice, many failures are triggered by faulty test code. To overcome these
challenges, we introduce a fully static, LLM-driven approach for system test
code fault localization (TCFL) that does not require executing the test case.
Our method uses a single failure execution log to estimate the test's execution
trace through three novel algorithms that identify only code statements likely
involved in the failure. This pruned trace, combined with the error message, is
used to prompt the LLM to rank potential faulty locations. Our black-box,
system-level approach requires no access to the SUT source code and is
applicable to large test scripts that assess full system behavior. We evaluate
our technique at function, block, and line levels using an industrial dataset
of faulty test cases not previously used in pre-training LLMs. Results show
that our best estimated trace closely match actual traces, with an F1 score of
around 90%. Additionally, pruning the complex system test code reduces the
LLM's inference time by up to 34% without any loss in FL performance. Our
results further suggest that block-level TCFL offers a practical balance,
narrowing the search space while preserving useful context, achieving an 81%
hit rate at top-3 (Hit@3).

</details>


### [2] [Dataset of Yul Contracts to Support Solidity Compiler Research](https://arxiv.org/abs/2506.19153)
*Krzysztof Fonal*

Main category: cs.SE

TL;DR: YulCode is a pioneering and extensive dataset of Yul-based smart contracts, filling a major gap for low-level Ethereum research, and enabling advancements in contract analysis, verification, and tools.


<details>
  <summary>Details</summary>
Motivation: There was a lack of publicly available datasets focused specifically on Yul, the Ethereum intermediate language, hampering research and tool development for low-level smart contract analysis.

Method: The authors compiled Solidity source files deployed on the Ethereum mainnet to generate Yul-based contract instances and constructed a large dataset (YulCode) of these instances.

Result: The YulCode dataset consists of 348,840 Yul-based smart contract instances, representing about 135,013 unique contracts derived from real-world decentralized applications deployed on Ethereum.

Conclusion: YulCode is the first public and comprehensive Yul-focused dataset, providing valuable resources for machine learning, formal verification, optimization, and software engineering research on Ethereum smart contracts.

Abstract: The YulCode dataset presents a comprehensive collection of 348,840 Yul-based
smart contract instances, comprising approximately 135,013 unique contracts.
These contracts were generated through the compilation of Solidity source files
that have been deployed on the Ethereum mainnet, making the dataset directly
representative of real-world decentralized applications. YulCode provides a
rich foundation for a variety of research and development tasks, including but
not limited to machine learning applications, formal verification, optimization
analysis, and software engineering tool evaluation in the context of low-level
smart contract code. To the best of our knowledge at the time of writing,
YulCode is the first and only publicly available dataset that focuses
specifically on Yul, an intermediate language designed for the Ethereum Virtual
Machine (EVM). As such, it fills a critical gap in the current ecosystem of
smart contract datasets and opens new avenues for research and tooling aimed at
low-level contract analysis and generation.

</details>


### [3] [Generating and Understanding Tests via Path-Aware Symbolic Execution with LLMs](https://arxiv.org/abs/2506.19287)
*Yaoxuan Wu,Xiaojie Zhou,Ahmad Humayun,Muhammad Ali Gulzar,Miryung Kim*

Main category: cs.SE

TL;DR: PALM is a test generation tool that combines symbolic path enumeration with LLM-driven test input generation, transforming program paths into variants that LLMs can readily understand and generate tests for. Its interactive coverage visualization aids users in verifying and exploring test coverage, outperforming traditional symbolic execution and LLM-only test generation.


<details>
  <summary>Details</summary>
Motivation: Symbolic execution is effective for test generation but struggles with complex code, library functions, and constraint solving limits. Large Language Models (LLMs) can generate diverse tests but lack systematic path exploration and miss edge cases. There's a need to combine the strengths of both approaches while overcoming their individual limitations.

Method: The authors propose PALM, a system that uses static AST-level analysis to enumerate program paths and transform each path into an executable program variant with embedded assertions. This bypasses the need for SMT-based constraint solving, allowing the LLM to generate tests for each path variant. PALM also includes an interactive frontend for users to visualize path coverage and understand the relationship between generated tests and code paths.

Result: PALM improves the coverage of interesting and complex code paths by combining symbolic path enumeration with LLM-based test generation. The interactive frontend enhances user understanding of which paths have been tested. A user study with 12 participants showed that the PALM interface aided in recognizing path coverage and verifying which code paths were exercised by generated tests.

Conclusion: PALM provides a novel and effective approach to automated test generation by marrying symbolic path enumeration with LLM-assistance, and offering intuitive path coverage visualization. It successfully overcomes key limitations of both symbolic execution and LLM-only approaches, helping users understand and verify which program paths are covered by generated tests.

Abstract: Symbolic execution is a widely used technique for test generation, offering
systematic exploration of program paths through constraint solving. However, it
is fundamentally constrained by the capability to model the target code
including library functions in terms of symbolic constraint and the capability
of underlying constraint solvers. As a result, many paths involving complex
features remain unanalyzed or insufficiently modeled. Recent advances in large
language models (LLMs) have shown promise in generating diverse and valid test
inputs. Yet, LLMs lack mechanisms for systematically enumerating program paths
and often fail to cover subtle corner cases. We observe that directly prompting
an LLM with the full program leads to missed coverage of interesting paths. In
this paper, we present PALM, a test generation system that combines symbolic
path enumeration with LLM-assisted test generation. PALM statically enumerates
possible paths through AST-level analysis and transforms each into an
executable variant with embedded assertions that specify the target path. This
avoids the need to translate path constraints into SMT formulae, by instead
constructing program variants that LLM can interpret. Importantly, PALM is the
first to provide an interactive frontend that visualizes path coverage
alongside generated tests, assembling tests based on the specific paths they
exercise. A user study with 12 participants demonstrates that PALM's frontend
helps users better understand path coverage and identify which paths are
actually exercised by PALM-generated tests, through verification and
visualization of their path profiles.

</details>


### [4] [What Makes the Best Decomposition? Investigating Binary Decomposition Under FCG Variance](https://arxiv.org/abs/2506.19425)
*Ang Jia,He Jiang,Zhilei Ren,Xiaochen Li,Ming Fan,Ting Liu*

Main category: cs.SE

TL;DR: The paper shows that binary decomposition methods for reuse detection struggle due to highly variable function call graphs across compiler settings. By systematically studying this variance, the authors demonstrate that current methods lack stability and coverage, and propose a way to identify optimal decompositions for improved evaluations.


<details>
  <summary>Details</summary>
Motivation: Binary decomposition is vital for binary reuse detection, but existing methods depend on function call graphs (FCGs), which may differ greatly due to various compilation settings and function inlining, potentially compromising decomposition effectiveness.

Method: The authors conducted the first systematic empirical study analyzing the variance of FCGs caused by different compilers, optimizations, and architectures. They built a dataset compiled by 17 compilers, 6 optimization settings, and 4 architectures, assessing how FCGs change. They further evaluated the performance of existing anchor-based and clustering-based binary decomposition methods under these varying FCGs, and proposed a method to identify the optimal binary decomposition.

Result: They found that FCGs change size dramatically with compilation settings, although certain mappings persist. Existing decomposition methods perform poorly in cross-compiler evaluations with diverse optimizations, showing low coverage or unstable results. The proposed optimal decomposition method enables better performance assessment by comparison.

Conclusion: FCGs are highly variable across compilation settings, challenging the effectiveness and reliability of existing binary decomposition methods for reuse detection. The study highlights weaknesses in current techniques and provides a new way to define and evaluate optimal decomposition outcomes under such variability.

Abstract: Binary decomposition, which decomposes binary files into modules, plays a
critical role in binary reuse detection. Existing binary decomposition works
either apply anchor-based methods by extending anchor functions to generate
modules, or apply clustering-based methods by using clustering algorithms to
group binary functions, which all rely on that reused code shares similar
function call relationships. However, we find that function call graphs (FCGs)
vary a lot when using different compilation settings, especially with diverse
function inlining decisions.
  In this work, we conduct the first systematic empirical study on the variance
of FCGs compiled by various compilation settings and explore its effect on
binary decomposition methods. We first construct a dataset compiled by 17
compilers, using 6 optimizations to 4 architectures and analyze the changes and
mappings of the FCGs. We find that the size of FCGs changes dramatically, while
the FCGs are still linked by three different kinds of mappings. Then we
evaluate the existing works under the FCG variance, and results show that
existing works are facing great challenges when conducting cross-compiler
evaluation with diverse optimization settings. Finally, we propose a method to
identify the optimal decomposition and compare the existing decomposition works
with the optimal decomposition. Existing works either suffer from low coverage
or cannot generate stable community similarities.

</details>


### [5] [LLM-based Multi-Agent System for Intelligent Refactoring of Haskell Code](https://arxiv.org/abs/2506.19481)
*Shahbaz Siddeeq,Muhammad Waseem,Zeeshan Rasheed,Md Mahade Hasan,Jussi Rasku,Mika Saari,Henri Terho,Kalle Makela,Kai-Kristian Kemell,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: This paper presents a multi-agent system based on large language models to automate refactoring of Haskell code, achieving measurable improvements in code complexity, quality, efficiency, and memory use. The approach supports more maintainable and automated development for functional programming.


<details>
  <summary>Details</summary>
Motivation: Refactoring is essential in software development for maintaining and scaling systems, but it remains labor-intensive due to the need to closely analyze code and avoid introducing new defects. The motivation is to reduce manual effort and the risk of errors through automation.

Method: The authors propose a large language model (LLM)-based multi-agent system for automating refactoring of Haskell code. The system uses specialized agents with different roles: code analysis, refactoring execution, verification, and debugging. They tested this approach on open-source Haskell codebases.

Result: The LLM-based multi-agent system led to an average of 11.03% decrease in code complexity, 22.46% improvement in code quality, 13.27% increased performance efficiency, and up to 14.57% optimization in memory allocation.

Conclusion: LLM-based multi-agent systems can effectively automate and improve the refactoring process for functional programming languages, enhancing maintainability and supporting development workflows.

Abstract: Refactoring is a constant activity in software development and maintenance.
Scale and maintain software systems are based on code refactoring. However,
this process is still labor intensive, as it requires programmers to analyze
the codebases in detail to avoid introducing new defects. In this research, we
put forward a large language model (LLM)-based multi-agent system to automate
the refactoring process on Haskell code. The objective of this research is to
evaluate the effect of LLM-based agents in performing structured and
semantically accurate refactoring on Haskell code. Our proposed multi-agent
system based on specialized agents with distinct roles, including code
analysis, refactoring execution, verification, and debugging. To test the
effectiveness and practical applicability of the multi-agent system, we
conducted evaluations using different open-source Haskell codebases. The
results of the experiments carried out showed that the proposed LLM-based
multi-agent system could average 11.03% decreased complexity in code, an
improvement of 22.46% in overall code quality, and increase performance
efficiency by an average of 13.27%. Furthermore, memory allocation was
optimized by up to 14.57%. These results highlight the ability of LLM-based
multi-agent in managing refactoring tasks targeted toward functional
programming paradigms. Our findings hint that LLM-based multi-agent systems
integration into the refactoring of functional programming languages can
enhance maintainability and support automated development workflows.

</details>


### [6] [Integrating Pair Programming as a Work Practice](https://arxiv.org/abs/2506.19511)
*Nina Haugland Andersen,Anastasiia Tkalich,Nils Brede Moe,Darja Smite,Asgaut Mjølne Söderbom,Ola Hast,Viktoria Stray*

Main category: cs.SE

TL;DR: The paper explores why pair programming isn't consistently adopted despite its benefits. Through interviews and analysis at an agile company, the authors identified factors influencing engagement such as team attitudes, effort, and infrastructure. They conclude that sustained use relies on direct, confirmed benefits and team-driven adaptations.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to understand why, despite the well-known advantages of pair programming (PP), its widespread and sustained adoption remains inconsistent in software development teams—especially as software complexity and the necessity for knowledge sharing have increased.

Method: The authors used an exploratory single-case study approach at a mature agile company in Norway. They collected qualitative data via two rounds of interviews with team members filling various roles and analyzed the data using thematic analysis.

Result: The study found that a range of factors influence engagement with PP. These include perceptions of PP's contributions to daily work, the effort required for effective participation, team and company attitudes, the availability of resources and infrastructure, and the specific characteristics of tasks.

Conclusion: Long-term engagement in pair programming hinges on users experiencing tangible, confirmed benefits. Customizing and adapting the practice based on collective team learning further supports sustained adoption. The results offer guidance to practitioners looking to integrate PP into their team's workflow.

Abstract: Context: Pair programming (PP) is more relevant than ever. As modern systems
grow in complexity, knowledge sharing and collaboration across teams have
become essential. However, despite well-documented benefits of PP, its adoption
remains inconsistent across software teams. Objective: This study aims to
understand the factors that facilitate or hinder team members' adoption as well
as lasting engagement in PP. Method: We have conducted an exploratory
single-case study in a mature agile company in Norway. We collected data
through two rounds of interviews with team members in different roles and
performed a thematic analysis of the interviews. Results: Our key finding is
that multiple factors, related to the perceptions of how PP contributes to
daily work, efforts associated with engaging in PP sessions, company and team
attitudes, resources, infrastructure, and task characteristics, affect PP
engagement. Conclusion: Long-term engagement in PP requires expected benefits
with the practice being confirmed in firsthand experiences. Adapting the
practice to each unique team, with insights drawn from collective learning, is
also beneficial. Our findings will be beneficial for software practitioners
seeking to make PP an integrated part of their team's workflow.

</details>


### [7] [Lost in Translation? Converting RegExes for Log Parsing into Dynatrace Pattern Language](https://arxiv.org/abs/2506.19539)
*Julian Fragner,Christian Macho,Bernhard Dieber,Martin Pinzger*

Main category: cs.SE

TL;DR: Reptile automates the conversion of regular expressions to the Dynatrace Pattern Language for log analytics, achieving strong accuracy and reducing manual migration work for companies adopting new platforms.


<details>
  <summary>Details</summary>
Motivation: Companies increasingly rely on commercial log analytics platforms with specialized pattern languages for log parsing. Migrating existing workflows requires converting numerous regular expressions (RegExes) to these new languages, a process that is labor-intensive and error-prone.

Method: The authors propose Reptile, a tool that combines rule-based conversion from RegExes to Dynatrace Pattern Language (DPL) with best-effort strategies for partial conversions. Additionally, Reptile leverages GPT-4 to further optimize the generated patterns.

Result: In evaluations using 946 real-world RegExes, Reptile achieved safe conversion for 73.7% of them. Further assessments using 23 RegExes demonstrated that Reptile's GPT-4 optimization achieved F1-scores and MCC values above 0.91, indicating strong performance.

Conclusion: Reptile effectively automates much of the RegEx-to-DPL conversion process, reducing migration effort and error rates. Its success suggests practical value for enterprises transitioning to modern log analytics platforms like Dynatrace.

Abstract: Log files provide valuable information for detecting and diagnosing problems
in enterprise software applications and data centers. Several log analytics
tools and platforms were developed to help filter and extract information from
logs, typically using regular expressions (RegExes). Recent commercial log
analytics platforms provide domain-specific languages specifically designed for
log parsing, such as Grok or the Dynatrace Pattern Language (DPL). However,
users who want to migrate to these platforms must manually convert their
RegExes into the new pattern language, which is costly and error-prone. In this
work, we present Reptile, which combines a rule-based approach for converting
RegExes into DPL patterns with a best-effort approach for cases where a full
conversion is impossible. Furthermore, it integrates GPT-4 to optimize the
obtained DPL patterns. The evaluation with 946 RegExes collected from a large
company shows that Reptile safely converted 73.7% of them. The evaluation of
Reptile's pattern optimization with 23 real-world RegExes showed an F1-score
and MCC above 0.91. These results are promising and have ample practical
implications for companies that migrate to a modern log analytics platform,
such as Dynatrace.

</details>


### [8] [Simulating the Waterfall Model: A Systematic Review](https://arxiv.org/abs/2506.19653)
*Antonios Saravanos*

Main category: cs.SE

TL;DR: This paper mapped how the Waterfall Model is simulated in academic research. It found mostly adapted (not original) versions are used, with discrete-event simulation and a shift to open-source tools. The paper suggests more accessible tools and further study into integrating the Waterfall Model with contemporary hybrid approaches.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the ongoing use of the Waterfall Model in software development, particularly in hybrid approaches, despite Agile methodologies being more prevalent. Additionally, there is a lack of research on how the Waterfall Model is actually simulated within academic contexts.

Method: The research employs a systematic mapping study. It involved a structured search across major academic databases to identify and select relevant peer-reviewed papers (published 2000-2024), followed by analysis of these studies across four dimensions: simulation methodologies, tools/platforms, trends (geographic and temporal), and fidelity to Royce's original model.

Result: Discrete-event simulation is the most commonly used methodology, aligning with the sequential nature of the Waterfall Model. Earlier studies used proprietary tools, but there is a trend toward open-source, Python-based tools. No identified study has implemented Royce's original seven-phase model fully; most use adaptations.

Conclusion: While the simulation of the Waterfall Model remains a niche research area, it is present in academic literature. The findings indicate a need for more accessible modeling tools and encourage future research to further explore integration of the Waterfall Model with modern hybrid software development practices.

Abstract: This systematic mapping study examines how the Waterfall Model has been
represented in computational simulations within peer-reviewed literature. While
Agile methodologies dominate contemporary software design practices, the
Waterfall Model persists, particularly, within hybrid approaches that fuse
structured, sequential workflows with the adaptability of agile practices.
Despite its continued presence, little attention has been given to how the
Waterfall Model is simulated in research contexts. A structured search of major
academic databases identified 68 peer-reviewed studies published between 2000
and 2024. After applying inclusion criteria, selected studies were analyzed
across four dimensions: (1) simulation methodologies (e.g., discrete-event
simulation, system dynamics), (2) platforms and tools (e.g., Simphony.NET,
SimPy), (3) geographic and temporal trends, and (4) fidelity to Royce's
original seven-phase model. Discrete-event simulation was most commonly used,
reflecting the model's sequential nature. Early work relied on proprietary
platforms, while recent studies increasingly use open-source, Python-based
tools. No studies fully implemented Royce's original formulation, most employed
adaptations. These findings suggest that although niche, simulation of the
Waterfall Model is present in academic discourse. This work highlights the need
for accessible modeling tools and calls for future research that integrates the
waterfall software process model with modern hybrid practices.

</details>


### [9] [Adaptive Request Scheduling for CodeLLM Serving with SLA Guarantees](https://arxiv.org/abs/2506.19677)
*Shi Chang,Boyuan Chen,Kishanthan Thangarajah,Hanan Lutfiyya,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: Static batching strategies for serving CodeLLMs struggle with fluctuating workloads. SABER, a new dynamic batching system, predicts per-request SLA feasibility and adjusts in real time, boosting throughput and stability without manual tuning.


<details>
  <summary>Details</summary>
Motivation: Efficiently serving Code Large Language Models (CodeLLMs) in resource-constrained, self-hosted environments is challenging due to fluctuating request rates and varying workloads. Existing systems with static batching cannot maintain stable, high-performance service, leading to frequent SLA violations.

Method: The study proposes SABER, a dynamic batching strategy that predicts whether each incoming request can meet its SLA and dynamically adjusts batching decisions in real time. This approach does not require manual tuning or restarting the service.

Result: SABER improves goodput by up to 26% compared with the best static batching configurations and reduces latency variability by up to 45%.

Conclusion: SLA-aware, adaptive scheduling like SABER is essential for robust, high-performance CodeLLM serving in practical, self-hosted scenarios.

Abstract: Code Large Language Models (CodeLLMs) are increasingly integrated into modern
software development workflows, yet efficiently serving them in
resource-constrained, self-hosted environments remains a significant challenge.
Existing LLM serving systems employs Continuous Batching for throughput
improvement. However, they rely on static batch size configurations that cannot
adapt to fluctuating request rates or heterogeneous workloads, leading to
frequent SLA (Service Level Agreement) violations and unstable performance. In
this study, We propose SABER, a dynamic batching strategy that predicts
per-request SLA feasibility and adjusts decisions in real time. SABER improves
goodput by up to 26% over the best static configurations and reduces latency
variability by up to 45%, all without manual tuning or service restarts. Our
results demonstrate that SLA-aware, adaptive scheduling is key to robust,
high-performance CodeLLM serving.

</details>


### [10] [Exploring Developer Experience Factors in Software Ecosystems](https://arxiv.org/abs/2506.19757)
*Rodrigo Oliveira Zacarias,Léo Carvalho Ramos Antunes,Márcio de Oliveira Barros,Rodrigo Pereira dos Santos,Patricia Lago*

Main category: cs.SE

TL;DR: This paper identifies and prioritizes key developer experience (DX) factors influencing third-party developer adoption and continued involvement in software ecosystem (SECO) platforms. Using a systematic mapping and Delphi study, it finds that financial concerns, technical resources, entry barriers, and financial incentives are the strongest factors in shaping developer engagement. The results offer clear guidance to improve DX and SECO sustainability.


<details>
  <summary>Details</summary>
Motivation: Developer experience (DX) significantly affects developer performance and long-term engagement in software ecosystem (SECO) platforms. However, there has been a lack of a clear, prioritized roadmap of the most influential DX factors within SECOs, despite recognized importance. Addressing this gap is critical for sustaining developer interest and contributions.

Method: The authors conducted a systematic mapping study (SMS) of 29 previous studies to compile state-of-the-art DX factors in SECOs. They further employed a Delphi study methodology with 21 third-party developers to evaluate the influence of 27 identified DX factors on developers’ decisions to adopt and continuously contribute to SECO platforms.

Result: The most influential DX factors for adoption and ongoing contribution to SECOs are: financial costs of platform usage, availability of desired technical resources, low barriers to entry in the applications marketplace, and potential for increased financial gains.

Conclusion: DX is crucial to the growth and sustainability of SECOs. The identified set of DX factors provides practical insights and actionable recommendations for both researchers and practitioners to enhance third-party developer engagement in SECOs.

Abstract: Context: Developer experience (DX) plays a key role in developers'
performance and their continued involvement in a software ecosystem (SECO)
platform. While researchers and practitioners have recognized several factors
affecting DX in SECO platforms, a clear roadmap of the most influential factors
is still missing. This is particularly important given the direct impact on
developers' interest in SECO and their ongoing engagement with the common
technological platform. Goal: This work aims to identify key DX factors and
understand how they influence third-party developers' decisions to adopt and
keep contributing to a SECO. Methods: We conducted a systematic mapping study
(SMS), analyzing 29 studies to assess the state-of-the-art of DX in SECO.
Additionally, we conducted a Delphi study to evaluate the influence of 27 DX
factors (identified in our SMS) from the perspective of 21 third-party
developers to adopt and keep contributing to a SECO. Results: The factors that
most strongly influence developers' adoption and ongoing contributions to a
SECO are: financial costs for using the platform, desired technical resources
for development, low barriers to entry into the applications market, and more
financial gains. Conclusion: DX is essential for the success and sustainability
of SECO. Our set of DX factors provides valuable insights and recommendations
for researchers and practitioners to address key DX concerns from the
perspective of third-party developers.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [11] [Mix-of-Language-Experts Architecture for Multilingual Programming](https://arxiv.org/abs/2506.18923)
*Yifan Zong,Yuntian Deng,Pengyu Nie*

Main category: cs.PL

TL;DR: MoLE is a modular LLM setup that combines shared and language-specific expertise for code tasks, achieving both efficiency and high performance across multiple programming languages.


<details>
  <summary>Details</summary>
Motivation: Existing methods to support multilingual programming with LLMs either lack specialization for individual languages or are computationally expensive due to duplicated models. There is a need for an efficient approach that doesn't compromise on performance.

Method: The authors propose MoLE (Mix-of-Language-Experts), a new LLM architecture featuring a base model, a shared LoRA module, and several language-specific LoRA modules. These are collectively finetuned to balance shared knowledge and language-specific expertise. During inference, MoLE dynamically uses the appropriate language-specific module based on the programming language.

Result: MoLE offers better parameter efficiency than having separate LoRA models for each language and achieves higher accuracy than a naively shared LLM finetuned for all languages.

Conclusion: MoLE architecture effectively balances efficiency and specialization, offering a scalable solution for multilingual programming with LLMs.

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
aiding developers with tasks like code comprehension, generation, and
translation. Supporting multilingual programming -- i.e., coding tasks across
multiple programming languages -- typically requires either (1) finetuning a
single LLM across all programming languages, which is cost-efficient but
sacrifices language-specific specialization and performance, or (2) finetuning
separate LLMs for each programming language, which allows for specialization
but is computationally expensive and storage-intensive due to the duplication
of parameters. This paper introduces MoLE (Mix-of-Language-Experts), a novel
architecture that balances efficiency and specialization for multilingual
programming. MoLE is composed of a base model, a shared LoRA (low-rank
adaptation) module, and a collection of language-specific LoRA modules. These
modules are jointly optimized during the finetuning process, enabling effective
knowledge sharing and specialization across programming languages. During
inference, MoLE automatically routes to the language-specific LoRA module
corresponding to the programming language of the code token being generated.
Our experiments demonstrate that MoLE achieves greater parameter efficiency
compared to training separate language-specific LoRAs, while outperforming a
single shared LLM finetuned for all programming languages in terms of accuracy.

</details>


### [12] [The Autonomous Data Language -- Concepts, Design and Formal Verification](https://arxiv.org/abs/2506.19457)
*Tom T. P. Franken,Thomas Neele,Jan Friso Groote*

Main category: cs.PL

TL;DR: This paper introduces a new parallel programming paradigm—data-autonomous computation—and presents the AuDaLa language, which allows data elements to autonomously drive parallel processes, resulting in more natural programming and easier verification of parallel programs.


<details>
  <summary>Details</summary>
Motivation: Current parallel programming languages prioritize processors and threads, making data and memory management difficult and creating a disconnect between implementations and original algorithms.

Method: The authors introduce a new parallel programming paradigm called the data-autonomous paradigm, where autonomous data elements drive computation. They also present AuDaLa, the first programming language based on this paradigm, including its type system and operational semantics. They illustrate its use with programming examples and demonstrate formal program verification.

Result: The data-autonomous paradigm enables highly parallel collaboration among data, resulting in a natural programming style distinct from existing approaches. AuDaLa supports this paradigm and facilitates formal verification of parallel programs.

Conclusion: The data-autonomous paradigm, realized through the AuDaLa language, offers a new, natural way to program parallel systems by focusing on autonomous data elements and enables better formal verification.

Abstract: Nowadays, the main advances in computational power are due to parallelism.
However, most parallel languages have been designed with a focus on processors
and threads. This makes dealing with data and memory in programs hard, which
distances the implementation from its original algorithm. We propose a new
paradigm for parallel programming, the data-autonomous paradigm, where
computation is performed by autonomous data elements. Programs in this paradigm
are focused on making the data collaborate in a highly parallel fashion. We
furthermore present AuDaLa, the first data autonomous programming language, and
provide a full formalisation that includes a type system and operational
semantics. Programming in AuDaLa is very natural, as illustrated by examples,
albeit in a style very different from sequential and contemporary parallel
programming. Additionally, it lends itself for the formal verification of
parallel programs, which we demonstrate.

</details>
