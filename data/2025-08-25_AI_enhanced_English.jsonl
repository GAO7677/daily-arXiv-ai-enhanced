{"id": "2508.15866", "categories": ["cs.PL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15866", "abs": "https://arxiv.org/abs/2508.15866", "authors": ["Lingxiao Li", "Salar Rahili", "Yiwei Zhao"], "title": "Correctness-Guaranteed Code Generation via Constrained Decoding", "comment": "Published at COLM 2025", "summary": "Language Models (LMs) are increasingly being used for code generation, but\nensuring the correctness of generated programs remains a significant challenge.\nAlthough imperfect code may be acceptable during software development with\nhuman oversight, domains such as video games and robotics require one-shot\ncorrectness for runtime-critical components. We present a constrained decoding\nalgorithm for generating semantically correct programs that incorporates a\ncontext-sensitive parser, which, at each step, outputs a regular expression\nthat satisfies a critical non-extensible property to guide the generation of\nthe next token sequence that can continue to a correct program. To build such a\ncontext-sensitive parser, we propose a framework of a dynamic tree of parsers\n(ToP) during parsing, where each parser corresponds to a modular context-free\ngrammar enriched with contextual information such as variable scopes and type\nconstraints, with tree branches representing ambiguity in the future code\nsegment. We demonstrate our approach through sLua, a strongly typed variant of\nLua, showing that our method can generate semantically correct programs\nconforming to any prescribed scripting API. We further show that, with careful\ndesign, our semantic guarantees extend to runtime correctness, as validated in\nthe application of generating game mechanics for a roguelike video game.", "AI": {"tldr": "The paper proposes a new constrained decoding approach using a context-sensitive parser for language models, enabling the generation of guaranteed correct code, validated in a strongly typed Lua setting for game development tasks.", "motivation": "There is a pressing need for code generation methods that guarantee correctness in domains like video games and robotics, where errors in generated code are unacceptable at runtime. Existing language models often produce imperfect code, but critical applications require one-shot correctness.", "method": "The authors introduce a constrained decoding algorithm that integrates a context-sensitive parser. This parser outputs a regular expression at each generation step, which enforces a critical property ensuring that the next token is always part of a potentially correct program. The parser is implemented as a dynamic tree of modular, context-enriched context-free grammars, enabling handling of code ambiguity and context (e.g., variable scopes, type constraints) during parsing.", "result": "They validate their approach using sLua, a strongly-typed Lua variant, demonstrating the ability to generate semantically correct programs for various scripting APIs. The semantic guarantees were tested in a roguelike video game, showing runtime correctness of generated game mechanics.", "conclusion": "The presented method successfully guides code generation towards semantically and runtime-correct programs, making it suitable for safety-critical coding domains like robotics and game engines. The approach is practical, generalizable, and improves over traditional language model generation by enforcing stronger correctness guarantees."}}
{"id": "2508.15898", "categories": ["cs.PL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.15898", "abs": "https://arxiv.org/abs/2508.15898", "authors": ["Matthew Sotoudeh", "Zachary Yedidia"], "title": "Automated Formal Verification of a Software Fault Isolation System", "comment": "Short paper to appear at FMCAD 2025, https://fmcad.org/", "summary": "Software fault isolation (SFI) is a popular way to sandbox untrusted\nsoftware. A key component of SFI is the verifier that checks the untrusted code\nis written in a subset of the machine language that guarantees it never reads\nor writes outside of a region of memory dedicated to the sandbox. Soundness\nbugs in the SFI verifier would break the SFI security model and allow the\nsupposedly sandboxed code to read protected memory. In this paper, we address\nthe concern of SFI verifier bugs by performing an automated formal verification\nof a recent SFI system called Lightweight Fault Isolation (LFI). In particular,\nwe formally verify that programs accepted by the LFI verifier never read or\nwrite to memory outside of a designated sandbox region.", "AI": {"tldr": "Formal methods are used to prove that the LFI SFI verifier does not accept programs that could access memory outside the sandbox, significantly strengthening SFI security guarantees.", "motivation": "Software Fault Isolation (SFI) is crucial for sandboxing untrusted code, but relies heavily on the verifier's correctness. If the verifier has bugs, the security promises of SFI can be void, allowing untrusted code access to protected memory. This potential vulnerability motivates the authors to formally analyze the verifier for a recent SFI system.", "method": "The authors perform automated formal verification on the Lightweight Fault Isolation (LFI) system's verifier. They use formal methods to prove that programs accepted by the LFI verifier cannot access memory outside their allocated sandbox.", "result": "They successfully verify that programs accepted by the LFI verifier are memory-safe with respect to the sandbox boundary\u2014in other words, such programs never read or write memory outside the designated region.", "conclusion": "Automated formal verification can assure the soundness of SFI verifiers such as that in LFI, thereby eliminating a critical security risk."}}
{"id": "2508.16063", "categories": ["cs.PL", "cs.FL"], "pdf": "https://arxiv.org/pdf/2508.16063", "abs": "https://arxiv.org/abs/2508.16063", "authors": ["Paul Krogmeier", "P. Madhusudan"], "title": "Synthesizing DSLs for Few-Shot Learning", "comment": null, "summary": "We study the problem of synthesizing domain-specific languages (DSLs) for\nfew-shot learning in symbolic domains. Given a base language and instances of\nfew-shot learning problems, where each instance is split into training and\ntesting samples, the DSL synthesis problem asks for a grammar over the base\nlanguage that guarantees that small expressions solving training samples also\nsolve corresponding testing samples. We prove that the problem is decidable for\na class of languages whose semantics over fixed structures can be evaluated by\ntree automata and when expression size corresponds to parse tree depth in the\ngrammar, and, furthermore, the grammars solving the problem correspond to a\nregular set of trees. We also prove decidability results for variants of the\nproblem where DSLs are only required to express solutions for input learning\nproblems and where DSLs are defined using macro grammars.", "AI": {"tldr": "This work proves that it's possible to automatically synthesize DSLs for few-shot learning in symbolic domains under certain conditions, leveraging tree automata. They also handle some relaxed problem settings.", "motivation": "The motivation is to enable automatic generation of efficient and expressive domain-specific languages tailored to few-shot learning problems in symbolic domains, ensuring small and generalizable solutions.", "method": "The authors use formal language theory, specifically tree automata and grammar analysis, to study the decidability of the DSL synthesis problem. They analyze the structure of languages and grammars conducive to decidable synthesis and establish theoretical results based on these methods.", "result": "The paper proves that the DSL synthesis problem is decidable for a class of languages evaluated by tree automata when expression size corresponds to parse tree depth. The set of suitable grammars is regular. They also demonstrate decidability for weaker variants of the problem.", "conclusion": "The paper establishes that synthesizing DSLs for few-shot learning is decidable under certain conditions, and grammars solving the problem form a regular set. Additional decidability results are provided for problem variants involving macro grammars and relaxed requirements."}}
{"id": "2508.16125", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16125", "abs": "https://arxiv.org/abs/2508.16125", "authors": ["Zhenyang Xu", "Hongxu Xu", "Yongqiang Tian", "Xintong Zhou", "Chengnian Sun"], "title": "Leveraging Large Language Models to Detect Missed Peephole Optimizations", "comment": null, "summary": "By replacing small, suboptimal instruction sequences within programs with a\nmore efficient equivalent, peephole optimization can not only directly optimize\ncode size and performance, but also potentially enables further transformations\nin the subsequent optimization pipeline. Although peephole optimization is a\ncritical class of compiler optimizations, discovering new and effective\npeephole optimizations is challenging as the instruction sets can be extremely\ncomplex and diverse. Previous methods either do not scale well or can only\ncapture a limited subset of peephole optimizations. In this work, we leverage\nLarge Language Models (LLMs) to detect missed peephole optimizations. We\npropose Lampo, a novel automated framework that synergistically combines the\ncreative but unreliable code optimization ability of LLMs with rigorous\ncorrectness verification performed by translation validation tools, integrated\nin a feedback-driven iterative process. Through a comprehensive evaluation\nwithin LLVM ecosystems, we show that Lampo can successfully detect up to 17 out\nof 25 previously reported missed optimizations in LLVM on average, and that 22\nout of 25 can potentially be found by Lampo with different LLMs. For\ncomparison, the state-of-the-art superoptimizer for LLVM, Souper, identified 15\nof them. Moreover, within seven months of development and intermittent\nexperiments, Lampo found 26 missed peephole optimizations, 15 of which have\nbeen confirmed and 6 already fixed. These results demonstrate Lampo's strong\npotential in continuously detecting missed peephole optimizations.", "AI": {"tldr": "Lampo uses LLMs and formal verification to automate detection of missed peephole optimizations in LLVM, outperforming previous tools and helping improve code quality in compilers.", "motivation": "Peephole optimization is a crucial compiler technique that improves code by replacing inefficient instruction sequences with optimized equivalents. However, discovering new, effective peephole optimizations is difficult because of the complexity and diversity of instruction sets. Existing methods scale poorly or only handle a limited range of optimizations. Thus, a better, scalable solution is needed.", "method": "The authors propose Lampo, an automated framework that uses Large Language Models (LLMs) to suggest new code optimizations and utilizes translation validation tools to rigorously check correctness. Lampo operates in a feedback-driven iterative manner to enhance the reliability of LLM-generated optimizations.", "result": "Lampo detected up to 17 out of 25 previously reported missed peephole optimizations in LLVM on average, and potentially up to 22 out of 25 depending on the LLM used. By comparison, Souper, the state-of-the-art superoptimizer for LLVM, found 15. Over a seven-month period, Lampo discovered 26 missed optimizations, 15 confirmed and 6 already fixed.", "conclusion": "Lampo significantly improves the detection of missed peephole optimizations, outperforming state-of-the-art approaches and showing promise as a continuous discovery tool for compiler optimization."}}
{"id": "2508.15941", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15941", "abs": "https://arxiv.org/abs/2508.15941", "authors": ["Imen Trabelsi", "Brahim Mahmoudi", "Jean Baptiste Minani", "Naouel Moha", "Yann-Ga\u00ebl Gu\u00e9h\u00e9neuc"], "title": "A Systematic Literature Review of Machine Learning Approaches for Migrating Monolithic Systems to Microservices", "comment": null, "summary": "Scalability and maintainability challenges in monolithic systems have led to\nthe adoption of microservices, which divide systems into smaller, independent\nservices. However, migrating existing monolithic systems to microservices is a\ncomplex and resource-intensive task, which can benefit from machine learning\n(ML) to automate some of its phases. Choosing the right ML approach for\nmigration remains challenging for practitioners. Previous works studied\nseparately the objectives, artifacts, techniques, tools, and benefits and\nchallenges of migrating monolithic systems to microservices. No work has yet\ninvestigated systematically existing ML approaches for this migration to\nunderstand the \\revised{automated migration phases}, inputs used, ML techniques\napplied, evaluation processes followed, and challenges encountered. We present\na systematic literature review (SLR) that aggregates, synthesises, and\ndiscusses the approaches and results of 81 primary studies (PSs) published\nbetween 2015 and 2024. We followed the Preferred Reporting Items for Systematic\nReview and Meta-Analysis (PRISMA) statement to report our findings and answer\nour research questions (RQs). We extract and analyse data from these PSs to\nanswer our RQs. We synthesise the findings in the form of a classification that\nshows the usage of ML techniques in migrating monolithic systems to\nmicroservices. The findings reveal that some phases of the migration process,\nsuch as monitoring and service identification, are well-studied, while others,\nlike packaging microservices, remain unexplored. Additionally, the findings\nhighlight key challenges, including limited data availability, scalability and\ncomplexity constraints, insufficient tool support, and the absence of\nstandardized benchmarking, emphasizing the need for more holistic solutions.", "AI": {"tldr": "This systematic review aggregates and analyzes research on applying ML to monolith-to-microservice migration, finding that although some steps benefit from ML, others lack solutions. Challenges include data scarcity, scaling difficulties, and insufficient tool support, pointing to a need for more comprehensive approaches.", "motivation": "Monolithic system architectures face scalability and maintainability issues, driving the need to migrate to microservices. However, the migration process is complex and labor-intensive, and there is a lack of systematic insight about how machine learning (ML) can automate or assist in this migration.", "method": "The authors conduct a Systematic Literature Review (SLR) of 81 primary studies published between 2015 and 2024, adhering to the PRISMA methodology. They extract and analyze data to classify and synthesize how ML techniques are used in various phases of migrating monolithic systems to microservices.", "result": "Some migration phases, such as monitoring and service identification, are well-explored with ML techniques, while others, like microservice packaging, are understudied. Key challenges in applying ML to migration include limited data, scalability constraints, lack of tools, and absence of standardized benchmarks.", "conclusion": "While ML techniques have advanced certain migration phases, significant gaps remain, especially in underexplored processes and tool support. Addressing these issues is vital for more comprehensive, automated migration solutions."}}
{"id": "2508.16522", "categories": ["cs.PL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.16522", "abs": "https://arxiv.org/abs/2508.16522", "authors": ["Rohan Yadav", "Joseph Guman", "Sean Treichler", "Michael Garland", "Alex Aiken", "Fredrik Kjolstad", "Michael Bauer"], "title": "On the Duality of Task and Actor Programming Models", "comment": null, "summary": "Programming models for distributed and heterogeneous machines are rapidly\ngrowing in popularity to meet the demands of modern workloads. Task and actor\nmodels are common choices that offer different trade-offs between development\nproductivity and achieved performance. Task-based models offer better\nproductivity and composition of software, whereas actor-based models routinely\ndeliver better peak performance due to lower overheads. While task-based and\nactor-based models appear to be different superficially, we demonstrate these\nprogramming models are duals of each other. Importantly, we show that this\nduality extends beyond functionality to performance, and elucidate techniques\nthat let task-based systems deliver performance competitive with actor-based\nsystems without compromising productivity. We apply these techniques to both\nRealm, an explicitly parallel task-based runtime, as well as Legion, an\nimplicitly parallel task-based runtime. We show these techniques reduce Realm's\noverheads by between 1.7-5.3x, coming within a factor of two of the overheads\nimposed by heavily optimized actor-based systems like Charm++ and MPI. We\nfurther show that our techniques enable between 1.3-5.0x improved strong\nscaling of unmodified Legion applications.", "AI": {"tldr": "The paper shows that task-based and actor-based distributed programming models are duals, and by applying specific optimizations, task-based runtimes (Realm and Legion) can achieve performance close to actor-based systems like Charm++ and MPI, while retaining higher productivity.", "motivation": "Modern workloads often run on distributed and heterogeneous machines, requiring programming models that balance productivity and performance. Task-based and actor-based models are widely used, each with distinct advantages and limitations. The motivation is to understand these differences and explore if their benefits can be combined.", "method": "The authors analyze the duality between task-based and actor-based programming models, comparing their functional and performance characteristics. They introduce techniques to make task-based runtimes more competitive in performance with actor-based systems, then implement these optimizations in Realm and Legion.", "result": "Performance overhead in the Realm runtime was reduced by 1.7\u20135.3x, achieving overheads within a factor of two compared to optimized actor-based systems like Charm++ and MPI. The techniques also improved strong scaling in Legion applications by 1.3\u20135.0x without modifying the applications.", "conclusion": "Task-based and actor-based programming models, despite superficial differences, are fundamentally duals. Through proposed techniques, task-based systems can approach or match the performance of actor-based systems without losing their advantages in productivity. This enables developers to choose models for productivity while getting close to peak performance."}}
{"id": "2508.16025", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16025", "abs": "https://arxiv.org/abs/2508.16025", "authors": ["Saba Naqvi", "Mohammad Baqar"], "title": "Breaking Barriers in Software Testing: The Power of AI-Driven Automation", "comment": "10 Pages", "summary": "Software testing remains critical for ensuring reliability, yet traditional\napproaches are slow, costly, and prone to gaps in coverage. This paper presents\nan AI-driven framework that automates test case generation and validation using\nnatural language processing (NLP), reinforcement learning (RL), and predictive\nmodels, embedded within a policy-driven trust and fairness model. The approach\ntranslates natural language requirements into executable tests, continuously\noptimizes them through learning, and validates outcomes with real-time analysis\nwhile mitigating bias. Case studies demonstrate measurable gains in defect\ndetection, reduced testing effort, and faster release cycles, showing that\nAI-enhanced testing improves both efficiency and reliability. By addressing\nintegration and scalability challenges, the framework illustrates how AI can\nshift testing from a reactive, manual process to a proactive, adaptive system\nthat strengthens software quality in increasingly complex environments.", "AI": {"tldr": "An AI-driven framework automates and optimizes software testing, resulting in higher defect detection, less manual effort, and faster releases, demonstrating that AI can make testing more reliable and scalable.", "motivation": "Traditional software testing is slow, costly, and incomplete, which motivates the need for more efficient, effective, and comprehensive testing approaches.", "method": "The paper introduces an AI-driven framework that combines natural language processing, reinforcement learning, and predictive models within a policy-based trust and fairness model to automate the generation, optimization, and validation of test cases.", "result": "Case studies show improved defect detection, reduced testing workload, and expedited software release cycles by using the proposed framework.", "conclusion": "The AI-driven framework significantly enhances software testing efficiency and reliability, transforming it from a manual, reactive process to a proactive, adaptive system that meets the demands of complex software environments."}}
{"id": "2508.16517", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.16517", "abs": "https://arxiv.org/abs/2508.16517", "authors": ["Bingkun Yao", "Ning Wang", "Xiangfeng Liu", "Yuxin Du", "Yuchen Hu", "Hong Gao", "Zhe Jiang", "Nan Guan"], "title": "ARSP: Automated Repair of Verilog Designs via Semantic Partitioning", "comment": null, "summary": "Debugging functional Verilog bugs consumes a significant portion of front-end\ndesign time. While Large Language Models (LLMs) have demonstrated great\npotential in mitigating this effort, existing LLM-based automated debugging\nmethods underperform on industrial-scale modules. A major reason for this is\nbug signal dilution in long contexts, where a few bug-relevant tokens are\noverwhelmed by hundreds of unrelated lines, diffusing the model's attention. To\naddress this issue, we introduce ARSP, a two-stage system that mitigates\ndilution via semantics-guided fragmentation. A Partition LLM splits a module\ninto semantically tight fragments; a Repair LLM patches each fragment; edits\nare merged without altering unrelated logic. A synthetic data framework\ngenerates fragment-level training pairs spanning bug types, design styles, and\nscales to supervise both models. Experiments show that ARSP achieves 77.92%\npass@1 and 83.88% pass@5, outperforming mainstream commercial LLMs including\nClaude-3.7 and SOTA automated Verilog debugging tools Strider and MEIC. Also,\nsemantic partitioning improves pass@1 by 11.6% and pass@5 by 10.2% over\nwhole-module debugging, validating the effectiveness of fragment-level scope\nreduction in LLM-based Verilog debugging.", "AI": {"tldr": "ARSP, a system using two LLMs for semantic partitioning and patching, greatly improves Verilog bug debugging on large code by focusing on smaller fragments, beating current commercial tools and closing the 'bug signal dilution' gap.", "motivation": "Debugging functional Verilog bugs is a time-consuming process for front-end design. Although LLMs have shown promise in automating debugging, their effectiveness drops for large-scale, industrial modules due to 'bug signal dilution', where relevant information is buried within large amounts of unrelated code.", "method": "The paper introduces ARSP, a two-stage system that combats bug signal dilution. In the first stage, a Partition LLM divides a Verilog module into semantically tight fragments. In the second stage, a Repair LLM patches these fragments, and then combines their edits without disturbing unrelated code. Training data is synthetically generated to cover many bug types, design styles, and module sizes.", "result": "ARSP achieves 77.92% pass@1 and 83.88% pass@5, significantly outperforming leading commercial LLMs (like Claude-3.7) and state-of-the-art automated Verilog debuggers like Strider and MEIC. The semantic partitioning approach improves debugging success rates by 11.6% (pass@1) and 10.2% (pass@5) compared to analyzing whole modules at once.", "conclusion": "By using semantics-guided fragmentation, ARSP substantially mitigates bug signal dilution, enhancing LLM-based automated debugging performance for industrial-scale Verilog modules. This demonstrates fragment-level scope reduction is highly effective for large codebase debugging tasks."}}
{"id": "2508.16053", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16053", "abs": "https://arxiv.org/abs/2508.16053", "authors": ["Shadikur Rahman", "Umme Ayman Koana", "Hasibul Karim Shanto", "Mahmuda Akter", "Chitra Roy", "Aras M. Ismael"], "title": "Measuring the effectiveness of code review comments in GitHub repositories: A machine learning approach", "comment": null, "summary": "This paper illustrates an empirical study of the working efficiency of\nmachine learning techniques in classifying code review text by semantic\nmeaning. The code review comments from the source control repository in GitHub\nwere extracted for development activity from the existing year for three\nopen-source projects. Apart from that, programmers need to be aware of their\ncode and point out their errors. In that case, it is a must to classify the\nsentiment polarity of the code review comments to avoid an error. We manually\nlabelled 13557 code review comments generated by three open source projects in\nGitHub during the existing year. In order to recognize the sentiment polarity\n(or sentiment orientation) of code reviews, we use seven machine learning\nalgorithms and compare those results to find the better ones. Among those\nLinear Support Vector Classifier(SVC) classifier technique achieves higher\naccuracy than others. This study will help programmers to make any solution\nbased on code reviews by avoiding misconceptions.", "AI": {"tldr": "The paper evaluates seven machine learning methods for classifying code review sentiments using labelled GitHub data, finding that the Linear SVC performs best. Automating sentiment classification can help developers interpret code reviews more accurately and avoid errors.", "motivation": "The motivation for this paper is to assist programmers in identifying errors and understanding code review sentiment, which can help them avoid misconceptions and improve coding quality. Classifying the sentiment polarity of code review comments can enhance awareness and decision making during software development.", "method": "The authors manually labelled 13,557 code review comments from three open-source GitHub projects for sentiment polarity. They applied and empirically compared seven machine learning algorithms, including the Linear Support Vector Classifier (SVC), to classify the semantic meaning of these comments.", "result": "Among the tested machine learning algorithms, the Linear Support Vector Classifier (SVC) demonstrated the highest accuracy in classifying the sentiment polarity of code review comments.", "conclusion": "The study concludes that the Linear SVC is the most effective machine learning technique among those tested for classifying code review sentiment. This classification can improve developers' understanding of code feedback, helping them avoid misconceptions and make more informed decisions."}}
{"id": "2508.16071", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16071", "abs": "https://arxiv.org/abs/2508.16071", "authors": ["Mahinthan Chandramohan", "Jovan Jancic", "Yuntong Zhang", "Padmanabhan Krishnan"], "title": "From Benchmark Data To Applicable Program Repair: An Experience Report", "comment": null, "summary": "This paper describes our approach to automated program repair. We combine\nvarious techniques from the literature to achieve this. Our experiments show\nthat our approach performs better than other techniques on standard benchmarks.\nHowever, on closer inspection, none of these techniques work on realistic\ndefects that we see in industry.\n  We find that augmenting code with formal specifications enables LLMs to\ngenerate higher-quality unit tests, especially for complex production code with\nimproved coverage of edge cases and exception handling. However, specifications\nadd little value for well-understood errors (e.g., null pointer, index out of\nbounds), but are beneficial for logic and string manipulation errors. Despite\nencouraging benchmark results, real-world adoption is limited since passing\ntests do not guarantee correct patches. Current challenges include insufficient\nexpressiveness of the JML specification language, necessitating advanced\nverification tools and richer predicates. Our ongoing work is exploring\ncontract automata, programming by example, and testcase repair, with a focus on\nintegrating human feedback and measuring productivity gains - highlighting the\ngap between academic benchmarks and practical industry needs", "AI": {"tldr": "The paper proposes combining automated repair techniques with formal specifications to improve unit test generation and repair effectiveness, showing better benchmark results but revealing a persistent gap for real-world defects. Existing methods and specification languages are not expressive enough, and passing more tests doesn't guarantee correctness. Future work targets advanced specifications and integrating human feedback.", "motivation": "The authors aim to improve automated program repair, especially addressing the gap between results on academic benchmarks and actual industry needs. Industry defects are not properly solved by existing techniques, motivating an exploration of enhanced methods.", "method": "They combine various literature techniques for automated repair, and augment code with formal specifications to help LLMs generate superior unit tests, particularly for complex code. They also experiment with JML and consider new avenues like contract automata, programming by example, and testcase repair, including human feedback integration.", "result": "Their combined approach outperforms other techniques on standard benchmarks. Augmenting code with specifications helps LLMs generate better-quality unit tests, especially for complex cases (logic/string errors), but is less useful for well-understood errors. In real-world scenarios, the expressiveness of JML is a bottleneck and correct test passes do not guarantee actual repair success. Practical adoption remains an issue.", "conclusion": "While their approach is effective on benchmarks and gains some benefits with specification augmentation, it is still hampered by limitations in specification languages and real-world repair guarantees. There remains a significant gap between what benchmarks measure and what industry requires. Their future work is focused on closing this gap."}}
{"id": "2508.16104", "categories": ["cs.SE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.16104", "abs": "https://arxiv.org/abs/2508.16104", "authors": ["Arturo Miguel Russell Bernal", "Maureen Petterson", "Pedro Antonio Alarcon Granadeno", "Michael Murphy", "James Mason", "Jane Cleland-Huang"], "title": "Validating Terrain Models in Digital Twins for Trustworthy sUAS Operations", "comment": "Submitted to EDTconf 2025", "summary": "With the increasing deployment of small Unmanned Aircraft Systems (sUAS) in\nunfamiliar and complex environments, Environmental Digital Twins (EDT) that\ncomprise weather, airspace, and terrain data are critical for safe flight\nplanning and for maintaining appropriate altitudes during search and\nsurveillance operations. With the expansion of sUAS capabilities through edge\nand cloud computing, accurate EDT are also vital for advanced sUAS\ncapabilities, like geolocation. However, real-world sUAS deployment introduces\nsignificant sources of uncertainty, necessitating a robust validation process\nfor EDT components. This paper focuses on the validation of terrain models, one\nof the key components of an EDT, for real-world sUAS tasks. These models are\nconstructed by fusing U.S. Geological Survey (USGS) datasets and satellite\nimagery, incorporating high-resolution environmental data to support mission\ntasks. Validating both the terrain models and their operational use by sUAS\nunder real-world conditions presents significant challenges, including limited\ndata granularity, terrain discontinuities, GPS and sensor inaccuracies, visual\ndetection uncertainties, as well as onboard resources and timing constraints.\nWe propose a 3-Dimensions validation process grounded in software engineering\nprinciples, following a workflow across granularity of tests, simulation to\nreal world, and the analysis of simple to edge conditions. We demonstrate our\napproach using a multi-sUAS platform equipped with a Terrain-Aware Digital\nShadow.", "AI": {"tldr": "The paper introduces a new, multi-dimensional process to robustly validate terrain models for drone operations, using real-world and simulated tests. The approach improves safety and reliability for systems that help drones navigate complex environments.", "motivation": "Small unmanned aircraft systems (sUAS) are increasingly used in unfamiliar and complex environments, requiring accurate Environmental Digital Twins (EDT) for safe navigation and advanced capabilities like geolocation. Deploying sUAS in the real world, however, introduces uncertainty, making robust validation of EDT components (especially terrain models) essential.", "method": "The paper proposes a three-dimensional validation process based on software engineering principles. This process evaluates terrain models by testing at varying granularities, transitioning from simulation to real-world conditions, and analyzing from basic to more extreme operational scenarios. Terrain models are created by fusing USGS datasets and satellite imagery with high-resolution environmental data, and validated in a multi-sUAS platform with a Terrain-Aware Digital Shadow.", "result": "The proposed validation approach is demonstrated successfully on a multi-sUAS platform. The process addresses key validation challenges such as data granularity, terrain discontinuities, sensor inaccuracies, and operational constraints, facilitating robust terrain model validation for real-world sUAS operations.", "conclusion": "A robust, software engineering-based three-dimension validation strategy for terrain models is practical and addresses real-world challenges encountered in sUAS missions, enhancing the reliability and operational safety of Environmental Digital Twins in fielded sUAS systems."}}
{"id": "2508.16131", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16131", "abs": "https://arxiv.org/abs/2508.16131", "authors": ["Zoe Kotti", "Konstantina Dritsa", "Diomidis Spinellis", "Panos Louridas"], "title": "The Fools are Certain; the Wise are Doubtful: Exploring LLM Confidence in Code Completion", "comment": "30 pages, 10 figures", "summary": "Code completion entails the task of providing missing tokens given a\nsurrounding context. It can boost developer productivity while providing a\npowerful code discovery tool. Following the Large Language Model (LLM) wave,\ncode completion has been approached with diverse LLMs fine-tuned on code (code\nLLMs). The performance of code LLMs can be assessed with downstream and\nintrinsic metrics. Downstream metrics are usually employed to evaluate the\npractical utility of a model, but can be unreliable and require complex\ncalculations and domain-specific knowledge. In contrast, intrinsic metrics such\nas perplexity, entropy, and mutual information, which measure model confidence\nor uncertainty, are simple, versatile, and universal across LLMs and tasks, and\ncan serve as proxies for functional correctness and hallucination risk in\nLLM-generated code. Motivated by this, we evaluate the confidence of LLMs when\ngenerating code by measuring code perplexity across programming languages,\nmodels, and datasets using various LLMs, and a sample of 1008 files from 657\nGitHub projects. We find that strongly-typed languages exhibit lower perplexity\nthan dynamically typed languages. Scripting languages also demonstrate higher\nperplexity. Perl appears universally high in perplexity, whereas Java appears\nlow. Code perplexity depends on the employed LLM, but not on the code dataset.\nAlthough code comments often increase perplexity, the language ranking based on\nperplexity is barely affected by their presence. LLM researchers, developers,\nand users can employ our findings to assess the benefits and suitability of\nLLM-based code completion in specific software projects based on how language,\nmodel choice, and code characteristics impact model confidence.", "AI": {"tldr": "This paper shows that code LLM confidence (measured by perplexity) varies most by programming language and model, with strongly-typed languages and Java being easiest for LLMs. Intrinsic metrics like perplexity reliably assess and compare model performance for code completion.", "motivation": "The motivation is to evaluate intrinsic metrics such as perplexity to assess the confidence and utility of code LLMs for code completion tasks across languages and models, addressing limitations of existing, complex, and unreliable downstream metrics.", "method": "They measured code perplexity across various programming languages, LLMs, and datasets by generating code on a sample of 1008 files from 657 GitHub projects, comparing perplexity values and their implications across languages, models, and the presence of code comments.", "result": "Strongly-typed languages generally show lower perplexity than dynamically typed ones. Scripting languages and especially Perl exhibit higher perplexity, while Java is low. Perplexity is more dependent on LLM choice than on code dataset. Although comments increase perplexity, language ranking remains consistent.", "conclusion": "Perplexity is a useful intrinsic metric for evaluating LLM confidence in code generation across languages and models. These findings help stakeholders gauge the suitability of LLM-based code completion for different projects, depending on language, model, and code characteristics."}}
{"id": "2508.16165", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.16165", "abs": "https://arxiv.org/abs/2508.16165", "authors": ["Sebastian Lubos", "Alexander Felfernig", "Gerhard Leitner", "Julian Schwazer"], "title": "Towards Recommending Usability Improvements with Multimodal Large Language Models", "comment": null, "summary": "Usability describes a set of essential quality attributes of user interfaces\n(UI) that influence human-computer interaction. Common evaluation methods, such\nas usability testing and inspection, are effective but resource-intensive and\nrequire expert involvement. This makes them less accessible for smaller\norganizations. Recent advances in multimodal LLMs offer promising opportunities\nto automate usability evaluation processes partly by analyzing textual, visual,\nand structural aspects of software interfaces. To investigate this possibility,\nwe formulate usability evaluation as a recommendation task, where multimodal\nLLMs rank usability issues by severity. We conducted an initial\nproof-of-concept study to compare LLM-generated usability improvement\nrecommendations with usability expert assessments. Our findings indicate the\npotential of LLMs to enable faster and more cost-effective usability\nevaluation, which makes it a practical alternative in contexts with limited\nexpert resources.", "AI": {"tldr": "This paper explores using multimodal LLMs to automatically evaluate UI usability, comparing their recommendations to expert assessments. Results show potential, suggesting LLMs could make usability evaluation more accessible and less resource-dependent.", "motivation": "Usability evaluation is important for improving human-computer interaction, but traditional methods are resource-intensive and require expert involvement, making them inaccessible for smaller organizations. There is a need for more efficient and widely accessible usability evaluation methods.", "method": "The authors reframe usability evaluation as a recommendation task and use multimodal large language models (LLMs) to rank usability issues by severity. They conduct a proof-of-concept study comparing LLM-generated usability recommendations to those of human experts.", "result": "The study found that LLMs can generate usability improvement recommendations that show promise when compared to expert assessments.", "conclusion": "Multimodal LLMs have the potential to provide faster and more cost-effective usability evaluation, offering a practical alternative where expert resources are limited."}}
{"id": "2508.16181", "categories": ["cs.SE", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.16181", "abs": "https://arxiv.org/abs/2508.16181", "authors": ["Zirui Li", "Stephan Husung", "Haoze Wang"], "title": "LLM-Assisted Semantic Alignment and Integration in Collaborative Model-Based Systems Engineering Using SysML v2", "comment": "Accepted by IEEE ISSE 2025, DOI pending", "summary": "Cross-organizational collaboration in Model-Based Systems Engineering (MBSE)\nfaces many challenges in achieving semantic alignment across independently\ndeveloped system models. SysML v2 introduces enhanced structural modularity and\nformal semantics, offering a stronger foundation for interoperable modeling.\nMeanwhile, GPT-based Large Language Models (LLMs) provide new capabilities for\nassisting model understanding and integration. This paper proposes a\nstructured, prompt-driven approach for LLM-assisted semantic alignment of SysML\nv2 models. The core contribution lies in the iterative development of an\nalignment approach and interaction prompts, incorporating model extraction,\nsemantic matching, and verification. The approach leverages SysML v2 constructs\nsuch as alias, import, and metadata extensions to support traceable, soft\nalignment integration. It is demonstrated with a GPT-based LLM through an\nexample of a measurement system. Benefits and limitations are discussed.", "AI": {"tldr": "This paper introduces a structured, prompt-driven method to help Large Language Models (like GPT) align the semantics of SysML v2 models for cross-organizational Model-Based Systems Engineering. The approach uses SysML v2 features and demonstrates improvements in semantic integration, although some limitations persist.", "motivation": "Model-Based Systems Engineering (MBSE) increasingly involves cross-organization collaboration, which suffers from difficulties in achieving semantic alignment due to independent model development. Traditional approaches lack effective solutions for semantic interoperability, motivating exploration of new tools and methods.", "method": "The authors propose a structured approach using Large Language Models (LLMs), specifically GPT-based, for semantic alignment of SysML v2 models. This prompt-driven method involves iteratively developing alignment strategies, using model extraction, semantic matching, and verification. Key SysML v2 features (alias, import, metadata) are leveraged to enable traceable, soft integration.", "result": "The approach is demonstrated through an example measurement system model using a GPT-based LLM, which successfully assists in aligning system semantics. The method allows for structured, interactive semantic integration, with both advantages and limitations identified through the case study.", "conclusion": "A prompt-driven, LLM-assisted semantic alignment strategy effectively supports the integration of independently developed SysML v2 models. SysML v2\u2019s modular and formalized constructs, combined with LLM capabilities, can substantially aid semantic interoperability in MBSE. Some challenges and areas for improvement remain, as discussed."}}
{"id": "2508.16273", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16273", "abs": "https://arxiv.org/abs/2508.16273", "authors": ["Maria Teresa Rossi", "Martina De Sanctis", "Ludovico Iovino", "Manuel Wimmer"], "title": "A Systematic Mapping Study on Smart Cities Modeling Approaches", "comment": null, "summary": "The Smart City concept was introduced to define an idealized city\ncharacterized by automation and connection. It then evolved rapidly by\nincluding further aspects, such as economy, environment. Since then, many\npublications have explored various aspects of Smart Cities across different\napplication domains and research communities, acknowledging the\ninterdisciplinary nature of this subject. In particular, our interest focuses\non how smart cities are designed and modeled, as a whole or as regards with\ntheir subsystems, when dealing with the accomplishment of the research goals in\nthis complex and heterogeneous domain. To this aim, we performed a systematic\nmapping study on smart cities modeling approaches identifying the relevant\ncontributions (i) to get an overview of existing research approaches, (ii) to\nidentify whether there are any publication trends, and (iii) to identify\npossible future research directions. We followed the guidelines for conducting\nsystematic mapping studies by Petersen et al. to analyze smart cities modeling\npublications. Our analysis revealed the following main findings: (i) smart\ngovernance is the most investigated and modeled smart city dimension; (ii) the\nmost used modeling approaches are business, architectural, and ontological\nmodeling approaches, spanning multiple application fields; (iii) the great\nmajority of existing technologies for modeling smart cities are not yet proven\nin operational environments; (iv) diverse research communities publish their\nresults in a multitude of different venues which further motivates the\npresented literature study. Researchers can use our results for better\nunderstanding the state-of-the-art in modeling smart cities, and as a\nfoundation for further analysis of specific approaches about smart cities\nmodeling. Lastly, we also discuss the impact of our analysis for the\nModel-Driven Engineering community.", "AI": {"tldr": "This paper systematically maps modeling approaches for smart cities, finding governance is the most studied aspect, models often remain untested in practice, and publications are highly dispersed. Results guide future research and help understand current methods.", "motivation": "The motivation behind this paper is to address the complexity and interdisciplinary nature of smart cities research, specifically by investigating how smart cities and their subsystems are designed and modeled across various domains.", "method": "The method employed is a systematic mapping study following Petersen et al.'s guidelines, analyzing published literature on smart city modeling approaches to categorize trends and directions.", "result": "The analysis revealed that smart governance is the most researched dimension, business/architectural/ontological modeling are the predominant approaches, most technologies remain untested in real-world environments, and research outputs are widely dispersed across venues.", "conclusion": "The study provides an overview of the state-of-the-art in smart city modeling, helping researchers navigate the interdisciplinary field and laying groundwork for future targeted studies, particularly for the Model-Driven Engineering community."}}
{"id": "2508.16307", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16307", "abs": "https://arxiv.org/abs/2508.16307", "authors": ["Jinsheng Ba", "Yuancheng Jiang", "Manuel Rigger"], "title": "Metamorphic Coverage", "comment": null, "summary": "Metamorphic testing is a widely used methodology that examines an expected\nrelation between pairs of executions to automatically find bugs, such as\ncorrectness bugs. We found that code coverage cannot accurately measure the\nextent to which code is validated and mutation testing is computationally\nexpensive for evaluating metamorphic testing methods. In this work, we propose\nMetamorphic Coverage (MC), a coverage metric that examines the distinct code\nexecuted by pairs of test inputs within metamorphic testing. Our intuition is\nthat, typically, a bug can be observed if the corresponding code is executed\nwhen executing either test input but not the other one, so covering more\ndifferential code covered by pairs of test inputs might be more likely to\nexpose bugs. While most metamorphic testing methods have been based on this\ngeneral intuition, our work defines and systematically evaluates MC on five\nwidely used metamorphic testing methods for testing database engines,\ncompilers, and constraint solvers. The code measured by MC overlaps with the\nbug-fix locations of 50 of 64 bugs found by metamorphic testing methods, and MC\nhas a stronger positive correlation with bug numbers than line coverage. MC is\n4x more sensitive than line coverage in distinguishing testing methods'\neffectiveness, and the average value of MC is 6x smaller than line coverage\nwhile still capturing the part of the program that is being tested. MC required\n359x less time than mutation testing. Based on a case study for an automated\ndatabase system testing approach, we demonstrate that when used for feedback\nguidance, MC significantly outperforms code coverage, by finding 41\\% more\nbugs. Consequently, this work might have broad applications for assessing\nmetamorphic testing methods and improving test-case generation.", "AI": {"tldr": "The paper introduces Metamorphic Coverage (MC), a new metric specifically designed for metamorphic testing. MC better correlates with bug detection, distinguishes between testing methods more effectively, and is far less computationally expensive than mutation testing or line coverage. Empirical results show MC both identifies more bug-prone code and enables more efficient and effective test-case generation, making it a valuable tool in software testing.", "motivation": "The motivation of this paper is to address the limitations of existing code coverage and mutation testing metrics when evaluating the effectiveness of metamorphic testing methods. Code coverage cannot precisely measure the code truly validated by tests, while mutation testing is prohibitively expensive, motivating the need for a better, more efficient metric.", "method": "The paper proposes Metamorphic Coverage (MC), a new coverage metric that assesses the distinct code exercised by pairs of test inputs in metamorphic testing. The authors systematically evaluate MC on five widely used metamorphic testing techniques across domains such as database engines, compilers, and constraint solvers. Empirical studies compare MC with traditional line coverage and mutation testing in terms of correlation with real bug detection, sensitivity, and computational cost.", "result": "MC overlapped with the bug-fix locations of 50 out of 64 bugs and showed a stronger positive correlation with bug numbers than line coverage. MC was found to be 4 times more sensitive than line coverage for distinguishing test method effectiveness, requires significantly less computational resources than mutation testing (359x less), and is able to guide testing more effectively (41% more bugs found in a case study). It captures the most relevant program areas with a much smaller metric value than line coverage.", "conclusion": "Metamorphic Coverage is a more precise, sensitive, and efficient metric than traditional code coverage and mutation testing for evaluating metamorphic testing methods. MC not only overlaps well with real bug locations, better guides test generation, and distinguishes testing method effectiveness, but also drastically reduces computational effort needed to assess test quality. This metric has significant potential to advance the practical evaluation of metamorphic testing and automated test-case generation."}}
{"id": "2508.16318", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16318", "abs": "https://arxiv.org/abs/2508.16318", "authors": ["Juan C. Alonso", "Alberto Martin-Lopez", "Sergio Segura", "Gabriele Bavota", "Antonio Ruiz-Cort\u00e9s"], "title": "SATORI: Static Test Oracle Generation for REST APIs", "comment": "Accepted for publication at 40th IEEE/ACM International Conference on\n  Automated Software Engineering, ASE 2025", "summary": "REST API test case generation tools are evolving rapidly, with growing\ncapabilities for the automated generation of complex tests. However, despite\ntheir strengths in test data generation, these tools are constrained by the\ntypes of test oracles they support, often limited to crashes, regressions, and\nnoncompliance with API specifications or design standards. This paper\nintroduces SATORI (Static API Test ORacle Inference), a black-box approach for\ngenerating test oracles for REST APIs by analyzing their OpenAPI Specification.\nSATORI uses large language models to infer the expected behavior of an API by\nanalyzing the properties of the response fields of its operations, such as\ntheir name and descriptions. To foster its adoption, we extended the\nPostmanAssertify tool to automatically convert the test oracles reported by\nSATORI into executable assertions. Evaluation results on 17 operations from 12\nindustrial APIs show that SATORI can automatically generate up to hundreds of\nvalid test oracles per operation. SATORI achieved an F1-score of 74.3%,\noutperforming the state-of-the-art dynamic approach AGORA+ (69.3%)-which\nrequires executing the API-when generating comparable oracle types. Moreover,\nour findings show that static and dynamic oracle inference methods are\ncomplementary: together, SATORI and AGORA+ found 90% of the oracles in our\nannotated ground-truth dataset. Notably, SATORI uncovered 18 bugs in popular\nAPIs (Amadeus Hotel, Deutschebahn, FDIC, GitLab, Marvel, OMDb and Vimeo)\nleading to documentation updates by the API maintainers.", "AI": {"tldr": "REST API test tools usually lack rich test oracles. SATORI uses large language models on OpenAPI specs to infer test oracles, converting them into executable assertions. SATORI outperforms dynamic approaches, finds many real bugs, and, when combined with other tools, covers most ground-truth oracles for tested APIs.", "motivation": "Current REST API test case generation tools are good at generating test data, but lack support for diverse test oracles, limiting verification to crashes, regressions, and specification violations. There is a need for approaches that can generate richer test oracles automatically.", "method": "The paper presents SATORI, a black-box tool that uses large language models to infer test oracles by analyzing the OpenAPI Specification of REST APIs, particularly the names and descriptions of response fields. SATORI integrates with PostmanAssertify to convert inferred oracles into executable assertions. SATORI's effectiveness is evaluated on multiple industrial APIs, and compared to AGORA+, a dynamic oracle inference method.", "result": "SATORI automatically generated hundreds of valid oracles per API operation, achieving an F1-score of 74.3%, outperforming AGORA+ (69.3%). Combining both static (SATORI) and dynamic (AGORA+) methods resulted in coverage of 90% of ground-truth oracles. SATORI found 18 undocumented bugs, prompting real updates to API documentation.", "conclusion": "SATORI substantially advances REST API test oracle generation by providing a scalable, automated, and effective solution leveraging static analysis and LLMs. Its generated oracles are both plentiful and high-quality, and complement dynamic methods. SATORI's real-world impact is demonstrated by bug discoveries and documentation improvements in major APIs."}}
{"id": "2508.16341", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16341", "abs": "https://arxiv.org/abs/2508.16341", "authors": ["Sebastian Copei", "Oliver Hohlfeld", "Jens Kosiol"], "title": "The (C)omprehensive (A)rchitecture (P)attern (I)ntegration method: Navigating the sea of technology", "comment": null, "summary": "The technological landscape changes daily, making it nearly impossible for a\nsingle person to be aware of all trends or available tools that may or may not\nbe suitable for their software project. This makes tool selection and\narchitectural design decisions a complex problem, especially for large-scale\nsoftware systems. To tackle this issue, we introduce CAPI, the Comprehensive\nArchitecture Pattern Integration method that uses a diagnostic decision tree to\nsuggest architectural patterns depending on user needs. By suggesting patterns\ninstead of tools, the overall complexity for further decisions is lower as\nthere are fewer architectural patterns than tools due to the abstract nature of\npatterns. Moreover, since tools implement patterns, each non-proposed pattern\nreduces the number of tools to choose from, reducing complexity. We iteratively\ndeveloped CAPI, evaluating its understandability and usability in small studies\nwith academic participants. When satisfied with the outcome, we performed a\nuser-study with industry representatives to investigate the state-of-the-art in\ntechnology selection and the effectiveness of our proposed method. We find that\ntechnology selection is largely performed via trial and error, that CAPI is\nuniformly perceived as helpful, and that CAPI is able to reproduce the\nproductive architectural environments of our participants.", "AI": {"tldr": "CAPI, a decision-tree based method, makes technology selection in software projects easier by recommending architectural patterns instead of specific tools, leading to less complexity and better decision-making, as validated by studies with academics and industry professionals.", "motivation": "Technology selection and architectural design are increasingly complex due to the rapidly changing technological landscape and the vast number of available tools. This paper aims to address the difficulty in making informed decisions for large-scale software projects.", "method": "The authors introduce CAPI, a Comprehensive Architecture Pattern Integration method. CAPI utilizes a diagnostic decision tree to suggest relevant architectural patterns based on user needs, thereby reducing complexity in technology selection. The method was developed iteratively and evaluated through small studies with academic participants, followed by a user-study involving industry representatives.", "result": "The study found that technology selection is frequently conducted by trial and error. CAPI was seen as uniformly helpful by users and was capable of recreating the productive architectural environments experienced by participants.", "conclusion": "CAPI simplifies technology selection in software projects by focusing on architectural patterns rather than individual tools, reducing decision complexity and improving outcomes for both academic and industry participants."}}
{"id": "2508.16402", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16402", "abs": "https://arxiv.org/abs/2508.16402", "authors": ["Zihan Wang", "Jiaze Chen", "Zhicheng Liu", "Markus Mak", "Yidi Du", "Geonsik Moon", "Luoqi Xu", "Aaron Tua", "Kunshuo Peng", "Jiayi Lu", "Mingfei Xia", "Boqian Zou", "Chenyang Ran", "Guang Tian", "Shoutai Zhu", "Yeheng Duan", "Zhenghui Kang", "Zhenxing Lin", "Shangshu Li", "Qiang Luo", "Qingshen Long", "Zhiyong Chen", "Yihan Xiao", "Yurong Wu", "Daoguang Zan", "Yuyi Fu", "Mingxuan Wang", "Ming Ding"], "title": "AetherCode: Evaluating LLMs' Ability to Win In Premier Programming Competitions", "comment": "15 pages", "summary": "Competitive programming has emerged as a critical benchmark for evaluating\nthe reasoning and coding capabilities of Large Language Models (LLMs). Despite\nimpressive progress on existing benchmarks, we argue that current evaluations\noverstate model proficiency, masking a substantial gap between LLMs and elite\nhuman programmers. This gap arises from two key limitations: insufficient\ndifficulty and scope of benchmark problems, and evaluation bias from\nlow-quality test cases. To address these shortcomings, we present AetherCode, a\nnew benchmark that draws problems from premier programming competitions such as\nIOI and ICPC, offering broader coverage and higher difficulty. AetherCode\nfurther incorporates comprehensive, expert-validated test suites built through\na hybrid of automated generation and human curation, ensuring rigorous and\nreliable assessment. By combining challenging problem design with robust\nevaluation, AetherCode provides a more faithful measure of LLM capabilities and\nsets a new standard for future research in code reasoning.", "AI": {"tldr": "Previous benchmarks for coding by LLMs are too easy and biased. AetherCode introduces tougher problems from real competitions and high-quality test sets, providing a truer test of LLM code reasoning.", "motivation": "Current benchmarks for evaluating LLMs in competitive programming are too easy and contain biases due to low-quality test cases. This overestimates LLM abilities compared to top human programmers, masking the real performance gap.", "method": "The authors introduce AetherCode, a new benchmark based on challenging problems from leading competitions (IOI, ICPC) and backed by thorough, expert-validated test suites created via automated and manual methods.", "result": "AetherCode delivers broader problem coverage, higher difficulty, and more rigorous evaluation compared to previous benchmarks, resulting in a more accurate assessment of LLM performance in coding tasks.", "conclusion": "AetherCode addresses key flaws in previous benchmarks and sets a new, higher standard for evaluating code reasoning skills in LLMs, better reflecting their true capabilities and limitations."}}
{"id": "2508.16419", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.16419", "abs": "https://arxiv.org/abs/2508.16419", "authors": ["Akshay Mhatre", "Noujoud Nader", "Patrick Diehl", "Deepti Gupta"], "title": "LLM-GUARD: Large Language Model-Based Detection and Repair of Bugs and Security Vulnerabilities in C++ and Python", "comment": null, "summary": "Large Language Models (LLMs) such as ChatGPT-4, Claude 3, and LLaMA 4 are\nincreasingly embedded in software/application development, supporting tasks\nfrom code generation to debugging. Yet, their real-world effectiveness in\ndetecting diverse software bugs, particularly complex, security-relevant\nvulnerabilities, remains underexplored. This study presents a systematic,\nempirical evaluation of these three leading LLMs using a benchmark of\nfoundational programming errors, classic security flaws, and advanced,\nproduction-grade bugs in C++ and Python. The dataset integrates real code from\nSEED Labs, OpenSSL (via the Suresoft GLaDOS database), and PyBugHive, validated\nthrough local compilation and testing pipelines. A novel multi-stage,\ncontext-aware prompting protocol simulates realistic debugging scenarios, while\na graded rubric measures detection accuracy, reasoning depth, and remediation\nquality. Our results show that all models excel at identifying syntactic and\nsemantic issues in well-scoped code, making them promising for educational use\nand as first-pass reviewers in automated code auditing. Performance diminishes\nin scenarios involving complex security vulnerabilities and large-scale\nproduction code, with ChatGPT-4 and Claude 3 generally providing more nuanced\ncontextual analyses than LLaMA 4. This highlights both the promise and the\npresent constraints of LLMs in serving as reliable code analysis tools.", "AI": {"tldr": "The study benchmarks ChatGPT-4, Claude 3, and LLaMA 4 for bug and vulnerability detection in C++/Python code. LLMs excel at basic error detection, performing best as educational or first-pass review tools, but struggle with advanced security flaws. ChatGPT-4 and Claude 3 are generally more context-aware than LLaMA 4, revealing current limitations in LLM-driven code analysis.", "motivation": "With the rise of LLMs like ChatGPT-4, Claude 3, and LLaMA 4 in software development, the study aims to assess how effectively these models detect a range of software bugs, especially complex, security-relevant vulnerabilities, an area that lacks systematic evaluation.", "method": "The authors systematically and empirically evaluate three leading LLMs on a curated benchmark involving foundational errors, classic security flaws, and advanced bugs in C++ and Python, sourced from SEED Labs, OpenSSL, and PyBugHive, validated through compilation/testing. A context-aware, multi-stage prompting protocol simulates realistic debugging scenarios, with a graded rubric used to measure accuracy, reasoning, and remediation.", "result": "All models perform well at identifying basic syntactic and semantic mistakes, showing promise for educational contexts and preliminary code audits. Their effectiveness drops significantly for complex security flaws and large-scale production bugs, with ChatGPT-4 and Claude 3 exhibiting stronger contextual understanding than LLaMA 4.", "conclusion": "While LLMs demonstrate potential as tools for code analysis, especially for educational purposes and simple bug detection, they are currently limited in fully addressing complex vulnerabilities and real-world production needs. Their reliability as advanced code auditing tools is constrained."}}
{"id": "2508.16445", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16445", "abs": "https://arxiv.org/abs/2508.16445", "authors": ["Sonia Nicoletti", "Paolo Ciancarini"], "title": "Using LLMs and Essence to Support Software Practice Adoption", "comment": null, "summary": "Recent advancements in natural language processing (NLP) have enabled the\ndevelopment of automated tools that support various domains, including software\nengineering. However, while NLP and artificial intelligence (AI) research has\nextensively focused on tasks such as code generation, less attention has been\ngiven to automating support for the adoption of best practices, the evolution\nof ways of working, and the monitoring of process health. This study addresses\nthis gap by exploring the integration of Essence, a standard and thinking\nframework for managing software engineering practices, with large language\nmodels (LLMs). To this end, a specialised chatbot was developed to assist\nstudents and professionals in understanding and applying Essence. The chatbot\nemploys a retrieval-augmented generation (RAG) system to retrieve relevant\ncontextual information from a curated knowledge base. Four different LLMs were\nused to create multiple chatbot configurations, each evaluated both as a base\nmodel and augmented with the RAG system. The system performance was evaluated\nthrough both the relevance of retrieved context and the quality of generated\nresponses. Comparative analysis against the general-purpose LLMs demonstrated\nthat the proposed system consistently outperforms its baseline counterpart in\ndomain-specific tasks. By facilitating access to structured software\nengineering knowledge, this work contributes to bridging the gap between\ntheoretical frameworks and practical application, potentially improving process\nmanagement and the adoption of software development practices. While further\nvalidation through user studies is required, these findings highlight the\npotential of LLM-based automation to enhance learning and decision-making in\nsoftware engineering.", "AI": {"tldr": "A chatbot combining the Essence software engineering framework with LLMs and a retrieval-augmented generation system yields better support for practical software engineering tasks than general-purpose LLMs, improving access to best practices, though more user studies are needed.", "motivation": "Although NLP and AI have achieved progress in domains like code generation, little work has facilitated the adoption of best practices or monitored process health in software engineering. The motivation is to bridge this gap by making theoretical frameworks like Essence more accessible for practical application.", "method": "The authors developed a specialized chatbot integrating the Essence framework with large language models (LLMs), using a retrieval-augmented generation (RAG) system to enhance context retrieval. They tested four different LLMs, each both as a base model and with RAG augmentation. Evaluation focused on the contextual relevance of retrieved information and the quality of generated responses.", "result": "The proposed chatbot system, especially when augmented with the RAG system, consistently outperformed baseline general-purpose LLMs in domain-specific tasks, delivering more relevant context and higher-quality responses for software engineering practice adoption.", "conclusion": "Integrating Essence with LLMs via a RAG-augmented chatbot provides effective automated support for learning and applying software engineering practices. This bridges theoretical and practical gaps, potentially improving process management. Further user-based validation is suggested."}}
{"id": "2508.16499", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16499", "abs": "https://arxiv.org/abs/2508.16499", "authors": ["Kazuki Kusama", "Honglin Shu", "Masanari Kondo", "Yasutaka Kamei"], "title": "How Small is Enough? Empirical Evidence of Quantized Small Language Models for Automated Program Repair", "comment": null, "summary": "Background: Large language models (LLMs) have greatly improved the accuracy\nof automated program repair (APR) methods. However, LLMs are constrained by\nhigh computational resource requirements. Aims: We focus on small language\nmodels (SLMs), which perform well even with limited computational resources\ncompared to LLMs. We aim to evaluate whether SLMs can achieve competitive\nperformance in APR tasks. Method: We conducted experiments on the QuixBugs\nbenchmark to compare the bug-fixing accuracy of SLMs and LLMs. We also analyzed\nthe impact of int8 quantization on APR performance. Results: The latest SLMs\ncan fix bugs as accurately as--or even more accurately than--LLMs. Also, int8\nquantization had minimal effect on APR accuracy while significantly reducing\nmemory requirements. Conclusions: SLMs present a viable alternative to LLMs for\nAPR, offering competitive accuracy with lower computational costs, and\nquantization can further enhance their efficiency without compromising\neffectiveness.", "AI": {"tldr": "Small language models are as accurate as large ones in automated program repair tasks and require less computational power. Memory can be reduced further with quantization, without affecting performance.", "motivation": "LLMs have improved APR but require high computational resources. The authors are motivated to investigate whether smaller, more resource-efficient models (SLMs) can perform competitively in automated program repair tasks.", "method": "Experiments were conducted using the QuixBugs benchmark to compare the bug-fixing accuracy of SLMs and LLMs. Additionally, the impact of int8 quantization on APR performance and memory usage was analyzed.", "result": "Latest SLMs can fix bugs with accuracy comparable to or exceeding LLMs. Int8 quantization substantially reduces memory requirements but minimally affects APR accuracy.", "conclusion": "SLMs are a viable alternative to LLMs for automated program repair, providing similar or better accuracy with lower resource requirements. Quantization further increases efficiency without sacrificing effectiveness."}}
