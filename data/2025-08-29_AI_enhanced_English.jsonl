{"id": "2508.20365", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.20365", "abs": "https://arxiv.org/abs/2508.20365", "authors": ["Naoki Kobayashi", "Ryosuke Sato", "Ayumi Shinohara", "Ryo Yoshinaka"], "title": "Solvable Tuple Patterns and Their Applications to Program Verification", "comment": null, "summary": "Despite the recent progress of automated program verification techniques,\nfully automated verification of programs manipulating recursive data structures\nremains a challenge. We introduce the notion of solvable tuple patterns (STPs)\nto express invariants between list-like recursive data structures. A\ndistinguishing feature of STPs is that they can be efficiently inferred from\nonly a small number of positive samples; no negative samples are required. An\nSMT solver that supports the sequence theory can be used to check that an\ninferred STP is indeed an inductive invariant. After presenting basic\nproperties of STPs and an STP inference algorithm, we show how to incorporate\nthe STP inference into a CHC (Constrained Horn Clauses) solver supporting\nlist-like data structures, which serves as a uniform backend for automated\nprogram verification tools. A CHC solver incorporating the STP inference has\nwon the ADT-LIN category of CHC-COMP 2025 by a big margin.", "AI": {"tldr": "This paper introduces STPs, a new method for efficiently inferring program invariants for list-like data structures using only positive samples. Integrating STP inference into a CHC solver greatly boosts verification performance, achieving state-of-the-art results in competition.", "motivation": "Automated verification of programs that manipulate recursive data structures, such as lists, is still difficult, especially when trying to infer invariants efficiently and without requiring negative samples.", "method": "The paper introduces 'solvable tuple patterns' (STPs) to express invariants of recursive data structures. It presents their basic properties, an algorithm to infer STPs from positive samples, and a method to validate STPs using SMT solvers supporting sequence theory. The approach is integrated into a CHC solver.", "result": "The approach enables efficient inference of invariants for list-like data structures; only positive samples are needed. The enhanced CHC solver with STP inference achieved superior performance, winning the ADT-LIN category of CHC-COMP 2025 by a large margin.", "conclusion": "STPs provide an effective way to infer and utilize inductive invariants for recursive data structures, improving the automation and efficacy of program verification tools."}}
{"id": "2508.20922", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.20922", "abs": "https://arxiv.org/abs/2508.20922", "authors": ["Markus B\u00f6ck", "J\u00fcrgen Cito"], "title": "Static Factorisation of Probabilistic Programs With User-Labelled Sample Statements and While Loops", "comment": null, "summary": "It is commonly known that any Bayesian network can be implemented as a\nprobabilistic program, but the reverse direction is not so clear. In this work,\nwe address the open question to what extent a probabilistic program with\nuser-labelled sample statements and while loops - features found in languages\nlike Gen, Turing, and Pyro - can be represented graphically. To this end, we\nextend existing operational semantics to support these language features. By\ntranslating a program to its control-flow graph, we define a sound static\nanalysis that approximates the dependency structure of the random variables in\nthe program. As a result, we obtain a static factorisation of the implicitly\ndefined program density, which is equivalent to the known Bayesian network\nfactorisation for programs without loops and constant labels, but constitutes a\nnovel graphical representation for programs that define an unbounded number of\nrandom variables via loops or dynamic labels. We further develop a sound\nprogram slicing technique to leverage this structure to statically enable three\nwell-known optimisations for the considered program class: we reduce the\nvariance of gradient estimates in variational inference and we speed up both\nsingle-site Metropolis Hastings and sequential Monte Carlo. These optimisations\nare proven correct and empirically shown to match or outperform existing\ntechniques.", "AI": {"tldr": "This paper extends graphical modeling to probabilistic programs with loops and dynamic sample statements, allowing static analysis and new optimizations, which are theoretically sound and shown to improve performance over existing methods.", "motivation": "While any Bayesian network can be implemented as a probabilistic program, the reverse is not clear\u2014especially for programs with loops and dynamic features. The motivation is to bridge this gap by providing a way to represent such probabilistic programs graphically, thus allowing static analysis and optimization.", "method": "They extend operational semantics to handle user-labelled sample statements and while loops in probabilistic programming languages. Then, they translate programs into control-flow graphs, enabling static analysis of random variable dependencies. They also develop a static program slicing technique for optimization purposes.", "result": "A novel static graphical representation for probabilistic programs with loops or dynamic features is introduced, generalizing Bayesian networks. The proposed optimizations are shown to be both sound and practically beneficial, matching or surpassing current methods in empirical tests.", "conclusion": "The paper shows that probabilistic programs with loops and dynamic labels can be statically analyzed to obtain a graphical representation that extends classic Bayesian network structures. This allows for the application of three static optimizations\u2014variance reduction in variational inference, faster single-site Metropolis Hastings, and more efficient sequential Monte Carlo\u2014which are proven and empirically validated."}}
{"id": "2508.20340", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.20340", "abs": "https://arxiv.org/abs/2508.20340", "authors": ["Maolin Sun", "Yibiao Yang", "Yuming Zhou"], "title": "Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce Formula Generators", "comment": null, "summary": "Satisfiability Modulo Theory (SMT) solvers are foundational to modern systems\nand programming languages research, providing the foundation for tasks like\nsymbolic execution and automated verification. Because these solvers sit on the\ncritical path, their correctness is essential, and high-quality test formulas\nare key to uncovering bugs. However, while prior testing techniques performed\nwell on earlier solver versions, they struggle to keep pace with rapidly\nevolving features. Recent approaches based on Large Language Models (LLMs) show\npromise in exploring advanced solver capabilities, but two obstacles remain:\nnearly half of the generated formulas are syntactically invalid, and iterative\ninteractions with the LLMs introduce substantial computational overhead. In\nthis study, we present Chimera, a novel LLM-assisted fuzzing framework that\naddresses both issues by shifting from direct formula generation to the\nsynthesis of reusable term (i.e., logical expression) generators. Particularly,\nChimera uses LLMs to (1) automatically extract context-free grammars (CFGs) for\nSMT theories, including solver-specific extensions, from documentation, and (2)\nsynthesize composable Boolean term generators that adhere to these grammars.\nDuring fuzzing, Chimera populates structural skeletons derived from existing\nformulas with the terms iteratively produced by the LLM-synthesized generators.\nThis design ensures syntactic validity while promoting semantic diversity.\nNotably, Chimera requires only one-time LLM interaction investment,\ndramatically reducing runtime cost. We evaluated Chimera on two leading SMT\nsolvers: Z3 and cvc5. Our experiments show that Chimera has identified 43\nconfirmed bugs, 40 of which have already been fixed by developers.", "AI": {"tldr": "Chimera uses LLMs to extract grammars and generate reusable formula generators for SMT solver testing, ensuring valid, diverse tests with minimal computational cost; it discovered 43 real bugs in leading solvers.", "motivation": "SMT solvers are crucial in modern programming and verification, demanding correctness and robustness. Prior testing approaches struggle with the pace of solver advancement and robust test generation.", "method": "Chimera, an LLM-assisted fuzzing framework, synthesizes CFGs from documentation and generates reusable Boolean term generators that ensure syntactically valid and semantically diverse test formulas. The LLM is only used once to minimize runtime overhead. Chimera populates skeletons from existing formulas with terms from these generators.", "result": "Chimera identified 43 confirmed bugs in Z3 and cvc5 SMT solvers, 40 of which have been fixed.", "conclusion": "Chimera is an effective, scalable approach to generating high-quality, valid formulas for SMT solver testing, combining the strengths of LLMs and grammar synthesis, and significantly reducing computational cost while uncovering real bugs."}}
{"id": "2508.20119", "categories": ["cs.SE", "cs.LG", "68T42", "I.2.6; I.2.2; D.2.2"], "pdf": "https://arxiv.org/pdf/2508.20119", "abs": "https://arxiv.org/abs/2508.20119", "authors": ["Daniel M. Yellin"], "title": "Evaluating LLMs on microservice-based applications: how complex is your specification?", "comment": "20 pages + 7 pages appendices. 7 Figures. 8 Tables", "summary": "In this paper we evaluate how far LLMs have advanced in generating code for\nreal-world problems. Specifically, we explore code synthesis for\nmicroservice-based applications, a widely used architecture pattern. We define\na standard template for specifying these applications, and we propose a metric\nfor judging the difficulty level of a specification. The higher the score, the\nmore difficult it is to generate code for the specification. We develop a\nframework to automate the process of testing LLM-synthesized code for a\nmicroservice using unit tests. Our experimental results show that strong LLMs\n(like GPT-3o-mini) do fairly well on medium difficulty specifications but do\nvery poorly on those of higher difficulty levels. This is due to more intricate\nbusiness logic, a greater use of external services, database integration and\ninclusion of non-functional capabilities such as authentication. We analyzed\nthe errors in LLM-synthesized code and report on the key challenges LLMs face\nin generating code for these specifications thereby suggesting future research\ndirections to improve code synthesis for real-world problems.", "AI": {"tldr": "LLMs can generate code for moderately complex microservices but fail on more difficult, real-world scenarios involving complex logic and integrations. The study identifies main challenges and suggests research directions to enhance LLM-based code synthesis.", "motivation": "With the growing use of LLMs for code generation, it is crucial to assess their actual performance on complex, real-world programming tasks, especially in modern architectures like microservices. The motivation is to understand current limitations and drive future improvement.", "method": "The authors defined a standard template for specifying microservice-based applications and proposed a metric to assess specification difficulty. They also developed an automated framework for testing LLM-generated code using unit tests, then ran experiments using various specification difficulties.", "result": "Experimental results indicated that strong LLMs perform adequately on medium-difficulty tasks but struggle significantly with higher-difficulty specifications, which involve complex business logic, integration with external services, databases, and non-functional requirements like authentication.", "conclusion": "LLMs currently face key challenges in synthesizing code for highly complex, real-world microservice specifications. Error analysis reveals the need for further research focused on overcoming obstacles in LLM code synthesis, particularly for applications with intricate requirements."}}
{"id": "2508.20124", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20124", "abs": "https://arxiv.org/abs/2508.20124", "authors": ["Yunlong Feng", "Yang Xu", "Xiao Xu", "Binyuan Hui", "Junyang Lin"], "title": "Towards Better Correctness and Efficiency in Code Generation", "comment": null, "summary": "While code large language models have demonstrated remarkable progress in\ncode generation, the generated code often exhibits poor runtime efficiency,\nlimiting its practical application in performance-sensitive scenarios. To\naddress this limitation, we propose an efficiency-oriented reinforcement\nlearning framework guided by a novel performance reward. Based on this\nframework, we take a deeper dive into the code efficiency problem, identifying\nthen proposing methods to overcome key bottlenecks: (1) Dynamic exploration\novercomes the static data constraints of offline fine-tuning, enabling the\ndiscovery of more efficient code implementations. (2) The error-insensitive\nreinforcement learning method and high-contrast efficiency signals are crucial\nfor mitigating systematic errors and achieving effective optimization. (3)\nOnline exploration is most effective when starting from a high-correctness\nbaseline, as this allows for efficiency improvements without sacrificing\naccuracy. With these discoveries, we finally propose a two-stage tuning method,\nwhich achieves high and balanced performance across correctness and efficiency.\nThe results of experiments show the effectiveness of the method, which improves\ncode correctness by 10.18\\% and runtime efficiency by 7.75\\% on a 7B model,\nachieving performance comparable to much larger model.", "AI": {"tldr": "They introduce a new reinforcement learning framework focused on code efficiency, identify key optimization bottlenecks, and propose solutions (dynamic exploration, error-insensitive RL, and a two-stage tuning method) that boost both correctness and speed of generated code.", "motivation": "Current code-generating large language models produce code with poor runtime efficiency, making them less useful for performance-critical scenarios.", "method": "They design a reinforcement learning framework with a novel reward for performance, utilizing dynamic exploration, robustness to errors, and a two-stage tuning approach for balanced optimization of correctness and efficiency.", "result": "The proposed method improves code correctness by 10.18% and runtime efficiency by 7.75% on a 7B model, achieving performance comparable to much larger models.", "conclusion": "The framework and tuning method effectively enhance code efficiency and correctness, making smaller models competitive with significantly larger ones for code generation tasks."}}
{"id": "2508.20370", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20370", "abs": "https://arxiv.org/abs/2508.20370", "authors": ["Lingzhe Zhang", "Tong Jia", "Kangjin Wang", "Weijie Hong", "Chiming Duan", "Minghua He", "Ying Li"], "title": "Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought", "comment": null, "summary": "As contemporary microservice systems become increasingly popular and\ncomplex-often comprising hundreds or even thousands of fine-grained,\ninterdependent subsystems-they are facing more frequent failures. Ensuring\nsystem reliability thus demands accurate root cause localization. While traces\nand metrics have proven to be effective data sources for this task, existing\nmethods either heavily rely on pre-defined schemas, which struggle to adapt to\nevolving operational contexts, or lack interpretability in their reasoning\nprocess, thereby leaving Site Reliability Engineers (SREs) confused. In this\npaper, we conduct a comprehensive study on how SREs localize the root cause of\nfailures, drawing insights from multiple professional SREs across different\norganizations. Our investigation reveals that human root cause analysis\nexhibits three key characteristics: recursiveness, multi-dimensional expansion,\nand cross-modal reasoning. Motivated by these findings, we introduce RCLAgent,\nan adaptive root cause localization method for microservice systems that\nleverages a multi-agent recursion-of-thought framework. RCLAgent employs a\nnovel recursion-of-thought strategy to guide the LLM's reasoning process,\neffectively integrating data from multiple agents and tool-assisted analysis to\naccurately pinpoint the root cause. Experimental evaluations on various public\ndatasets demonstrate that RCLAgent achieves superior performance by localizing\nthe root cause using only a single request-outperforming state-of-the-art\nmethods that depend on aggregating multiple requests. These results underscore\nthe effectiveness of RCLAgent in enhancing the efficiency and precision of root\ncause localization in complex microservice environments.", "AI": {"tldr": "Microservice failures are hard to diagnose with current methods. This paper presents RCLAgent, an LLM-driven, multi-agent system inspired by human SRE reasoning. It identifies root causes more accurately and efficiently, even from single requests, setting a new standard for reliability in microservices.", "motivation": "Microservice systems are increasingly complex and prone to failures. Traditional root cause localization methods are either inflexible (relying on fixed schemas) or non-interpretable, leaving SREs confused. There is a need for a more adaptive, interpretable, and effective root cause localization technique.", "method": "The paper proposes RCLAgent, an adaptive root cause localization method utilizing a multi-agent recursion-of-thought framework. It models the human SRE approach by employing recursive, multi-dimensional, and cross-modal reasoning, guiding a large language model (LLM) to synthesize information from different agents and automated tools.", "result": "RCLAgent outperforms state-of-the-art methods on public datasets by localizing the root cause of failures using only a single request, rather than requiring aggregation over multiple requests. It improves both efficiency and accuracy of root cause localization.", "conclusion": "RCLAgent offers an effective, interpretable, and adaptive approach to root cause localization in complex microservice systems by drawing on human reasoning strategies and leveraging advanced multi-agent LLM frameworks. It enables SREs to identify problems faster and more accurately."}}
{"id": "2508.20563", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20563", "abs": "https://arxiv.org/abs/2508.20563", "authors": ["Zheying Zhang", "Tomas Herda", "Victoria Pichler", "Pekka Abrahamsson", "Geir K. Hanssen", "Joshua Kerievsky", "Alex Polyakov", "Mohit Chandna", "Marius Irgens", "Kai-Kristian Kemell", "Ayman Asad Khan", "Crystal Kwok", "Evan Leybourn", "Munish Malik", "Dorota Mleczko", "Morteza Moalagh", "Christopher Morales", "Yuliia Pieskova", "Daniel Plan\u00f6tscher", "Mika Saari", "Anastasiia Tkalich", "Karl Josef Gstettner", "Xiaofeng Wang"], "title": "AI and Agile Software Development: A Research Roadmap from the XP2025 Workshop", "comment": null, "summary": "This paper synthesizes the key findings from a full-day XP2025 workshop on\n\"AI and Agile: From Frustration to Success\", held in Brugg-Windisch,\nSwitzerland. The workshop brought together over 30 interdisciplinary academic\nresearchers and industry practitioners to tackle the concrete challenges and\nemerging opportunities at the intersection of Generative Artificial\nIntelligence (GenAI) and agile software development. Through structured,\ninteractive breakout sessions, participants identified shared pain points like\ntool fragmentation, governance, data quality, and critical skills gaps in AI\nliteracy and prompt engineering. These issues were further analyzed, revealing\nunderlying causes and cross-cutting concerns. The workshop concluded by\ncollaboratively co-creating a multi-thematic research roadmap, articulating\nboth short-term, implementable actions and visionary, long-term research\ndirections. This cohesive agenda aims to guide future investigation and drive\nthe responsible, human-centered integration of GenAI into agile practices.", "AI": {"tldr": "A workshop brought together experts from academia and industry to identify and analyze the challenges of incorporating Generative AI into agile software development. Key pain points were discussed, and a research roadmap was created to drive responsible, human-centered integration of GenAI in agile practices.", "motivation": "There is a growing intersection between Generative Artificial Intelligence (GenAI) and agile software development, accompanied by numerous practical challenges, such as tool fragmentation, governance, data quality issues, and skills gaps related to AI literacy and prompt engineering.", "method": "The researchers conducted a full-day workshop with over 30 academic researchers and industry practitioners, using structured, interactive breakout sessions to identify, analyze, and collaboratively address pain points and opportunities at the intersection of GenAI and agile development.", "result": "The workshop resulted in the identification of key challenges (e.g., tool fragmentation, governance, data quality, skills gaps), analysis of underlying causes, and the co-creation of a multi-thematic research roadmap with both short-term and long-term actions for integrating GenAI into agile practices.", "conclusion": "A collaboratively developed research agenda was established, guiding responsible and human-centered integration of GenAI into agile software development, addressing both immediate and future needs in the field."}}
{"id": "2508.20737", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20737", "abs": "https://arxiv.org/abs/2508.20737", "authors": ["Wei Ma", "Yixiao Yang", "Qiang Hu", "Shi Ying", "Zhi Jin", "Bo Du", "Zhenchang Xing", "Tianlin Li", "Junjie Shi", "Yang Liu", "Linxiao Jiang"], "title": "Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol", "comment": null, "summary": "Applications of Large Language Models~(LLMs) have evolved from simple text\ngenerators into complex software systems that integrate retrieval augmentation,\ntool invocation, and multi-turn interactions. Their inherent non-determinism,\ndynamism, and context dependence pose fundamental challenges for quality\nassurance. This paper decomposes LLM applications into a three-layer\narchitecture: \\textbf{\\textit{System Shell Layer}}, \\textbf{\\textit{Prompt\nOrchestration Layer}}, and \\textbf{\\textit{LLM Inference Core}}. We then assess\nthe applicability of traditional software testing methods in each layer:\ndirectly applicable at the shell layer, requiring semantic reinterpretation at\nthe orchestration layer, and necessitating paradigm shifts at the inference\ncore. A comparative analysis of Testing AI methods from the software\nengineering community and safety analysis techniques from the AI community\nreveals structural disconnects in testing unit abstraction, evaluation metrics,\nand lifecycle management. We identify four fundamental differences that\nunderlie 6 core challenges. To address these, we propose four types of\ncollaborative strategies (\\emph{Retain}, \\emph{Translate}, \\emph{Integrate},\nand \\emph{Runtime}) and explore a closed-loop, trustworthy quality assurance\nframework that combines pre-deployment validation with runtime monitoring.\nBased on these strategies, we offer practical guidance and a protocol proposal\nto support the standardization and tooling of LLM application testing. We\npropose a protocol \\textbf{\\textit{Agent Interaction Communication Language}}\n(AICL) that is used to communicate between AI agents. AICL has the\ntest-oriented features and is easily integrated in the current agent framework.", "AI": {"tldr": "LLM applications require new approaches to quality assurance due to their complexity and dynamic behavior. This paper analyzes traditional vs. AI testing methods, presents a layered architecture, introduces practical strategies, and proposes a standardized protocol (AICL) to facilitate robust and trustworthy testing.", "motivation": "As large language model (LLM) applications grow more complex, integrating retrieval, tool use, and multi-turn interactions, they present unique challenges for quality assurance that traditional software testing methods may not address. This motivates a systematic investigation of how to ensure the reliability and safety of such systems.", "method": "The paper decomposes LLM applications into a three-layer architecture and analyzes the applicability of traditional software testing methods at each layer. It conducts a comparative analysis between software engineering testing and AI safety techniques, identifies structural disconnects and core challenges, and proposes collaborative strategies and a new protocol (AICL) to standardize testing for LLM applications.", "result": "The analysis identifies four fundamental differences that lead to six core testing challenges in LLM systems. Four collaborative strategies\u2014Retain, Translate, Integrate, and Runtime\u2014are proposed to bridge the identified gaps. Additionally, the paper introduces the Agent Interaction Communication Language (AICL) protocol to facilitate test-oriented, agent-based communication and streamline standardization efforts.", "conclusion": "A systematic approach is required to ensure the quality assurance of LLM-based applications, involving both adaptation and innovation in testing strategies. The proposed architecture analysis, protocol, and strategies aim to provide practical guidance and support the development of robust tooling and standards for LLM application testing."}}
{"id": "2508.20744", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20744", "abs": "https://arxiv.org/abs/2508.20744", "authors": ["Shabnam Hassani", "Mehrdad Sabetzadeh", "Daniel Amyot"], "title": "From Law to Gherkin: A Human-Centred Quasi-Experiment on the Quality of LLM-Generated Behavioural Specifications from Food-Safety Regulations", "comment": null, "summary": "Context: Laws and regulations increasingly affect software design and quality\nassurance, but legal texts are written in technology-neutral language. This\ncreates challenges for engineers who must develop compliance artifacts such as\nrequirements and acceptance criteria. Manual creation is labor-intensive,\nerror-prone, and requires domain expertise. Advances in Generative AI (GenAI),\nespecially Large Language Models (LLMs), offer a way to automate deriving such\nartifacts.\n  Objective: We present the first systematic human-subject study of LLMs'\nability to derive behavioral specifications from legal texts using a\nquasi-experimental design. These specifications translate legal requirements\ninto a developer-friendly form.\n  Methods: Ten participants evaluated specifications generated from food-safety\nregulations by Claude and Llama. Using Gherkin, a structured BDD language, 60\nspecifications were produced. Each participant assessed 12 across five\ncriteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Each\nspecification was reviewed by two participants, yielding 120 assessments.\n  Results: For Relevance, 75% of ratings were highest and 20% second-highest.\nClarity reached 90% highest. Completeness: 75% highest, 19% second.\nSingularity: 82% highest, 12% second. Time Savings: 68% highest, 24% second. No\nlowest ratings occurred. Mann-Whitney U tests showed no significant differences\nacross participants or models. Llama slightly outperformed Claude in Clarity,\nCompleteness, and Time Savings, while Claude was stronger in Singularity.\nFeedback noted hallucinations and omissions but confirmed the utility of the\nspecifications.\n  Conclusion: LLMs can generate high-quality Gherkin specifications from legal\ntexts, reducing manual effort and providing structured artifacts useful for\nimplementation, assurance, and test generation.", "AI": {"tldr": "The paper shows that large language models can effectively automate the conversion of legal requirements into developer-friendly software specifications, reducing manual effort and maintaining high quality as judged by human evaluators.", "motivation": "Legal requirements affect software design, but translating legal language into actionable software specifications is difficult, error-prone, and requires expertise. Manual methods are costly and slow. Advances in Generative AI offer new automation opportunities.", "method": "The authors conducted a systematic human-subject study using a quasi-experimental design. Ten participants evaluated 60 Gherkin-style behavioral specifications generated by two LLMs (Claude and Llama) from food-safety legal texts. Each participant assessed 12 specifications against five criteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Mann-Whitney U tests were used for statistical analysis.", "result": "Most specification ratings were in the top two categories for all criteria. Mann-Whitney U tests indicated no statistically significant differences across participants or models, though Llama slightly surpassed Claude in most criteria except Singularity. Feedback mentioned some hallucinations and omissions, but overall, the generated specifications were considered valuable.", "conclusion": "LLMs are capable of producing high-quality, structured Gherkin specifications from legal texts. This can reduce manual work and assist in requirements engineering, software assurance, and test planning."}}
{"id": "2508.20774", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20774", "abs": "https://arxiv.org/abs/2508.20774", "authors": ["Markus Funke", "Patricia Lago"], "title": "Towards an Architectural Perspective for Sustainability: Bundle the Needs from Industry", "comment": null, "summary": "Sustainability is increasingly recognized as an emerging quality property in\nsoftware-intensive systems, yet architects lack structured guidance to address\nit effectively throughout the software design phase. Architectural\nperspectives-an architectural knowledge artifact composed of concerns,\nactivities, tactics, pitfalls, and checklists-offer a promising approach to\ntackle such emerging quality properties across architectural views and are also\nindependent of architecture frameworks and industry contexts. In this paper, we\npresent a sustainability perspective vision, i.e., a revised notion of\narchitectural perspective meant to be filled with its own elements to target\nsustainability concerns. We formulate our sustainability perspective vision\nthrough evidence from applying snowballing to seminal literature and from\nconducting a focus group with experts in the field. Our findings confirm the\nrelevance of the different perspective elements in practice and highlight\nimplications for shaping a sustainability perspective that meets industrial\nneeds.", "AI": {"tldr": "The paper introduces a framework to help software architects address sustainability during design by using tailored architectural perspectives. Through literature review and expert feedback, the approach is validated and shown to meet real-world industry needs.", "motivation": "Sustainability is becoming an important quality in software systems, but software architects lack structured methods to address it during design. The authors aim to provide a framework to support sustainability considerations independently of existing architecture frameworks and across different contexts.", "method": "The authors develop a 'sustainability perspective vision'\u2014a framework based on architectural perspectives comprised of concerns, activities, tactics, pitfalls, and checklists. They use literature snowballing and a focus group with domain experts to gather evidence and refine the approach.", "result": "The study confirms that the elements of the proposed architectural perspective are relevant and applicable in practice. Empirical findings highlight important considerations for adapting the perspective to industrial needs.", "conclusion": "A specialized sustainability perspective can help systematically address sustainability in software architecture design. The approach is validated by expert input and literature, showing both its relevance and the practical implications of adopting such a perspective."}}
{"id": "2508.20902", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20902", "abs": "https://arxiv.org/abs/2508.20902", "authors": ["Baharin A. Jodat", "Khouloud Gaaloul", "Mehrdad Sabetzadeh", "Shiva Nejati"], "title": "Automated Test Oracles for Flaky Cyber-Physical System Simulators: Approach and Evaluation", "comment": null, "summary": "Simulation-based testing of cyber-physical systems (CPS) is costly due to the\ntime-consuming execution of CPS simulators. In addition, CPS simulators may be\nflaky, leading to inconsistent test outcomes and requiring repeated test\nre-execution for reliable test verdicts. Automated test oracles that do not\nrequire system execution are therefore crucial for reducing testing costs.\nIdeally, such test oracles should be interpretable to facilitate human\nunderstanding of test verdicts, and they must be robust against the potential\nflakiness of CPS simulators. In this article, we propose assertion-based test\noracles for CPS as sets of logical and arithmetic predicates defined over the\ninputs of the system under test. Given a test input, our assertion-based test\noracle determines, without requiring test execution, whether the test passes,\nfails, or if the oracle is inconclusive in predicting a verdict. We describe\ntwo methods for generating assertion-based test oracles: one using genetic\nprogramming~(GP) that employs well-known spectrum-based fault localization\n(SBFL) ranking formulas, namely Ochiai, Tarantula, and Naish, as fitness\nfunctions; and the other using decision trees (DT) and decision rules (DR). We\nevaluate our assertion-based test oracles through case studies in the domains\nof aerospace, networking and autonomous driving. We show that test oracles\ngenerated using GP with Ochiai are significantly more accurate than those\nobtained using GP with Tarantula and Naish or using DT or DR. Moreover, this\naccuracy advantage remains even when accounting for the flakiness of the system\nunder test. We further show that the assertion-based test oracles generated by\nGP with Ochiai are robust against flakiness with only 4% average variation in\ntheir accuracy results across four different network and autonomous driving\nsystems with flaky behaviours.", "AI": {"tldr": "Assertion-based test oracles, generated via genetic programming with Ochiai fitness, offer robust and accurate verdicts for CPS testing without needing simulator execution, thus reducing costs and overcoming simulator flakiness.", "motivation": "Simulation-based testing of cyber-physical systems (CPS) is resource-intensive due to the slow execution and unreliability of simulators, which leads to high costs and inconsistent test outcomes. There is a need for automated test oracles that do not require actual system execution, are interpretable, and robust to simulator flakiness.", "method": "The authors propose assertion-based test oracles, which are logical and arithmetic predicates over system inputs to predict test results without running the system. They introduce two methods for generating these oracles: (1) using genetic programming (GP) with spectrum-based fault localization (SBFL) formulas (Ochiai, Tarantula, Naish) as fitness functions; (2) using decision trees (DT) and decision rules (DR).", "result": "GP with the Ochiai formula produced assertion-based test oracles that are significantly more accurate than those generated with Tarantula, Naish, DT, or DR. This accuracy advantage was consistent even when the CPS simulators exhibited flaky behavior. The accuracy varied by only 4% on average across different systems, showing robustness to simulator flakiness.", "conclusion": "Assertion-based test oracles generated using GP and the Ochiai ranking formula are both accurate and robust for CPS testing, reducing the need for costly and unreliable simulation runs while being interpretable and resilient to simulator flakiness."}}
{"id": "2508.20911", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20911", "abs": "https://arxiv.org/abs/2508.20911", "authors": ["Zuocheng Feng", "Kaiwen Zhang", "Miaomiao Wang", "Yiming Cheng", "Yuandao Cai", "Xiaofeng Li", "Guanjun Liu"], "title": "Deep Learning Based Concurrency Bug Detection and Localization", "comment": null, "summary": "Concurrency bugs, caused by improper synchronization of shared resources in\nmulti-threaded or distributed systems, are notoriously hard to detect and thus\ncompromise software reliability and security. The existing deep learning\nmethods face three main limitations. First, there is an absence of large and\ndedicated datasets of diverse concurrency bugs for them. Second, they lack\nsufficient representation of concurrency semantics. Third, binary\nclassification results fail to provide finer-grained debug information such as\nprecise bug lines. To address these problems, we propose a novel method for\neffective concurrency bug detection as well as localization. We construct a\ndedicated concurrency bug dataset to facilitate model training and evaluation.\nWe then integrate a pre-trained model with a heterogeneous graph neural network\n(GNN), by incorporating a new Concurrency-Aware Code Property Graph (CCPG) that\nconcisely and effectively characterizes concurrency semantics. To further\nfacilitate debugging, we employ SubgraphX, a GNN-based interpretability method,\nwhich explores the graphs to precisely localize concurrency bugs, mapping them\nto specific lines of source code. On average, our method demonstrates an\nimprovement of 10\\% in accuracy and precision and 26\\% in recall compared to\nstate-of-the-art methods across diverse evaluation settings.", "AI": {"tldr": "The paper introduces a new deep learning approach for concurrency bug detection and localization using a special code graph and interpretability tools. It provides a new dataset, improved bug detection accuracy, and precise localization\u2014outperforming current methods.", "motivation": "Concurrency bugs in multi-threaded or distributed systems are hard to detect and undermine reliability and security. Existing deep learning methods suffer from the lack of large, dedicated datasets, insufficient representation of concurrency semantics, and fail to provide fine-grained localization of bugs.", "method": "The authors create a dedicated concurrency bug dataset and propose a new method integrating a pre-trained model with a heterogeneous graph neural network using a Concurrency-Aware Code Property Graph (CCPG) for richer semantic representation. SubgraphX, a GNN-based interpretability technique, is used for precise bug localization at the code line level.", "result": "The proposed approach outperforms state-of-the-art methods with a 10% improvement in accuracy and precision, and a 26% increase in recall across various evaluations.", "conclusion": "The novel method effectively detects and localizes concurrency bugs by addressing dataset, representation, and interpretability limitations, thereby enhancing debugging capabilities and outperforming prior work."}}
{"id": "2508.20977", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20977", "abs": "https://arxiv.org/abs/2508.20977", "authors": ["Shiwen Shan", "Yintong Huo", "Yuxin Su", "Zhining Wang", "Dan Li", "Zibin Zheng"], "title": "ConfLogger: Enhance Systems' Configuration Diagnosability through Configuration Logging", "comment": "13 pages, 6 figures, accpeted by ICSE '26 (The 48th IEEE/ACM\n  International Conference on Software Engineering)", "summary": "Modern configurable systems offer customization via intricate configuration\nspaces, yet such flexibility introduces pervasive configuration-related issues\nsuch as misconfigurations and latent softwarebugs. Existing diagnosability\nsupports focus on post-failure analysis of software behavior to identify\nconfiguration issues, but none of these approaches look into whether the\nsoftware clue sufficient failure information for diagnosis. To fill in the\nblank, we propose the idea of configuration logging to enhance existing logging\npractices at the source code level. We develop ConfLogger, the first tool that\nunifies configuration-aware static taint analysis with LLM-based log generation\nto enhance software configuration diagnosability. Specifically, our method 1)\nidentifies configuration-sensitive code segments by tracing\nconfiguration-related data flow in the whole project, and 2) generates\ndiagnostic log statements by analyzing configuration code contexts. Evaluation\nresults on eight popular software systems demonstrate the effectiveness of\nConfLogger to enhance configuration diagnosability. Specifically,\nConfLogger-enhanced logs successfully aid a log-based misconfiguration\ndiagnosis tool to achieve 100% accuracy on error localization in 30 silent\nmisconfiguration scenarios, with 80% directly resolvable through explicit\nconfiguration information exposed. In addition, ConfLogger achieves 74%\ncoverage of existing logging points, outperforming baseline LLM-based loggers\nby 12% and 30%. It also gains 8.6% higher in precision, 79.3% higher in recall,\nand 26.2% higher in F1 compared to the state-of-the-art baseline in terms of\nvariable logging while also augmenting diagnostic value. A controlled user\nstudy on 22 cases further validated its utility, speeding up diagnostic time by\n1.25x and improving troubleshooting accuracy by 251.4%.", "AI": {"tldr": "The paper presents ConfLogger, a tool combining static analysis and LLM-powered log generation to improve diagnosing configuration errors. Testing on multiple systems shows dramatic gains in error localization accuracy, coverage, and user troubleshooting speed compared to previous methods.", "motivation": "Modern configurable systems enable extensive customization but come with increased risks of misconfigurations and software bugs. Existing diagnosability approaches primarily focus on analyzing software after failures occur, and do not consider whether the system logs provide enough information for diagnosing configuration problems.", "method": "The paper introduces ConfLogger, a new tool that integrates configuration-aware static taint analysis with LLM-based log generation. The approach first identifies configuration-sensitive code through project-wide data flow tracing, then generates targeted diagnostic log statements by examining the context of configuration-relevant code.", "result": "ConfLogger was evaluated on eight popular software systems. Results show that logs produced by ConfLogger enable a log-based misconfiguration diagnosis tool to localize errors with 100% accuracy in 30 silent misconfiguration scenarios, 80% of which were directly resolvable via the explicit configuration information exposed. ConfLogger covers 74% of existing logging points, outperforming baseline LLM-based loggers by 12% and 30%, and surpasses the current best method by 8.6% in precision, 79.3% in recall, and 26.2% in F1 for variable logging. A controlled user study demonstrated 1.25x faster diagnosis and a 251.4% improvement in troubleshooting accuracy.", "conclusion": "ConfLogger significantly enhances the diagnosability of configuration-related issues in configurable software systems. By automatically exposing configuration-relevant information in logs, it considerably improves the effectiveness and accuracy of both automated and manual misconfiguration diagnosis compared to existing solutions."}}
{"id": "2508.21050", "categories": ["cs.SE", "cs.CY", "K.2; K.6.3; K.4; K.7"], "pdf": "https://arxiv.org/pdf/2508.21050", "abs": "https://arxiv.org/abs/2508.21050", "authors": ["Thomas J. Misa"], "title": "Dynamics of Gender Bias in Software Engineering", "comment": "26 pages, 3 figures", "summary": "The field of software engineering is embedded in both engineering and\ncomputer science, and may embody gender biases endemic to both. This paper\nsurveys software engineering's origins and its long-running attention to\nengineering professionalism, profiling five leaders; it then examines the\nfield's recent attention to gender issues and gender bias. It next\nquantitatively analyzes women's participation as research authors in the\nfield's leading International Conference of Software Engineering (1976-2010),\nfinding a dozen years with statistically significant gender exclusion. Policy\ndimensions of research on gender bias in computing are suggested.", "AI": {"tldr": "This paper reviews software engineering's history and gender bias, finds substantial evidence of women's exclusion in conference authorship, and calls for policy responses to gender inequity.", "motivation": "The motivation is to investigate the presence of gender biases within software engineering, especially given its roots in both engineering and computer science, where such biases may already exist.", "method": "The paper surveys the historical development of software engineering, examines literature addressing professionalism and gender bias, profiles five leaders, and quantitatively analyzes the participation of women as research authors at the International Conference of Software Engineering from 1976 to 2010.", "result": "The analysis found a dozen years with statistically significant gender exclusion among research authors at the conference, indicating persistent issues of gender bias in the field.", "conclusion": "Gender bias has been a recurring issue in software engineering, as shown by historical and quantitative evidence; the paper suggests considering policy interventions to address these biases."}}
