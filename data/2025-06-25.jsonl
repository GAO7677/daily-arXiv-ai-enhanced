{"id": "2506.19045", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.19045", "abs": "https://arxiv.org/abs/2506.19045", "authors": ["Ahmadreza Saboor Yaraghi", "Golnaz Gharachorlu", "Sakina Fatima", "Lionel C. Briand", "Ruiyuan Wan", "Ruifeng Gao"], "title": "Black-Box Test Code Fault Localization Driven by Large Language Models and Execution Estimation", "comment": null, "summary": "Fault localization (FL) is a critical step in debugging which typically\nrelies on repeated executions to pinpoint faulty code regions. However,\nrepeated executions can be impractical in the presence of non-deterministic\nfailures or high execution costs. While recent efforts have leveraged Large\nLanguage Models (LLMs) to aid execution-free FL, these have primarily focused\non identifying faults in the system under test (SUT) rather than in the often\ncomplex system test code. However, the latter is also important as, in\npractice, many failures are triggered by faulty test code. To overcome these\nchallenges, we introduce a fully static, LLM-driven approach for system test\ncode fault localization (TCFL) that does not require executing the test case.\nOur method uses a single failure execution log to estimate the test's execution\ntrace through three novel algorithms that identify only code statements likely\ninvolved in the failure. This pruned trace, combined with the error message, is\nused to prompt the LLM to rank potential faulty locations. Our black-box,\nsystem-level approach requires no access to the SUT source code and is\napplicable to large test scripts that assess full system behavior. We evaluate\nour technique at function, block, and line levels using an industrial dataset\nof faulty test cases not previously used in pre-training LLMs. Results show\nthat our best estimated trace closely match actual traces, with an F1 score of\naround 90%. Additionally, pruning the complex system test code reduces the\nLLM's inference time by up to 34% without any loss in FL performance. Our\nresults further suggest that block-level TCFL offers a practical balance,\nnarrowing the search space while preserving useful context, achieving an 81%\nhit rate at top-3 (Hit@3)."}
{"id": "2506.19153", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.19153", "abs": "https://arxiv.org/abs/2506.19153", "authors": ["Krzysztof Fonal"], "title": "Dataset of Yul Contracts to Support Solidity Compiler Research", "comment": "4 pages", "summary": "The YulCode dataset presents a comprehensive collection of 348,840 Yul-based\nsmart contract instances, comprising approximately 135,013 unique contracts.\nThese contracts were generated through the compilation of Solidity source files\nthat have been deployed on the Ethereum mainnet, making the dataset directly\nrepresentative of real-world decentralized applications. YulCode provides a\nrich foundation for a variety of research and development tasks, including but\nnot limited to machine learning applications, formal verification, optimization\nanalysis, and software engineering tool evaluation in the context of low-level\nsmart contract code. To the best of our knowledge at the time of writing,\nYulCode is the first and only publicly available dataset that focuses\nspecifically on Yul, an intermediate language designed for the Ethereum Virtual\nMachine (EVM). As such, it fills a critical gap in the current ecosystem of\nsmart contract datasets and opens new avenues for research and tooling aimed at\nlow-level contract analysis and generation."}
{"id": "2506.19287", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.19287", "abs": "https://arxiv.org/abs/2506.19287", "authors": ["Yaoxuan Wu", "Xiaojie Zhou", "Ahmad Humayun", "Muhammad Ali Gulzar", "Miryung Kim"], "title": "Generating and Understanding Tests via Path-Aware Symbolic Execution with LLMs", "comment": null, "summary": "Symbolic execution is a widely used technique for test generation, offering\nsystematic exploration of program paths through constraint solving. However, it\nis fundamentally constrained by the capability to model the target code\nincluding library functions in terms of symbolic constraint and the capability\nof underlying constraint solvers. As a result, many paths involving complex\nfeatures remain unanalyzed or insufficiently modeled. Recent advances in large\nlanguage models (LLMs) have shown promise in generating diverse and valid test\ninputs. Yet, LLMs lack mechanisms for systematically enumerating program paths\nand often fail to cover subtle corner cases. We observe that directly prompting\nan LLM with the full program leads to missed coverage of interesting paths. In\nthis paper, we present PALM, a test generation system that combines symbolic\npath enumeration with LLM-assisted test generation. PALM statically enumerates\npossible paths through AST-level analysis and transforms each into an\nexecutable variant with embedded assertions that specify the target path. This\navoids the need to translate path constraints into SMT formulae, by instead\nconstructing program variants that LLM can interpret. Importantly, PALM is the\nfirst to provide an interactive frontend that visualizes path coverage\nalongside generated tests, assembling tests based on the specific paths they\nexercise. A user study with 12 participants demonstrates that PALM's frontend\nhelps users better understand path coverage and identify which paths are\nactually exercised by PALM-generated tests, through verification and\nvisualization of their path profiles."}
{"id": "2506.19425", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.19425", "abs": "https://arxiv.org/abs/2506.19425", "authors": ["Ang Jia", "He Jiang", "Zhilei Ren", "Xiaochen Li", "Ming Fan", "Ting Liu"], "title": "What Makes the Best Decomposition? Investigating Binary Decomposition Under FCG Variance", "comment": null, "summary": "Binary decomposition, which decomposes binary files into modules, plays a\ncritical role in binary reuse detection. Existing binary decomposition works\neither apply anchor-based methods by extending anchor functions to generate\nmodules, or apply clustering-based methods by using clustering algorithms to\ngroup binary functions, which all rely on that reused code shares similar\nfunction call relationships. However, we find that function call graphs (FCGs)\nvary a lot when using different compilation settings, especially with diverse\nfunction inlining decisions.\n  In this work, we conduct the first systematic empirical study on the variance\nof FCGs compiled by various compilation settings and explore its effect on\nbinary decomposition methods. We first construct a dataset compiled by 17\ncompilers, using 6 optimizations to 4 architectures and analyze the changes and\nmappings of the FCGs. We find that the size of FCGs changes dramatically, while\nthe FCGs are still linked by three different kinds of mappings. Then we\nevaluate the existing works under the FCG variance, and results show that\nexisting works are facing great challenges when conducting cross-compiler\nevaluation with diverse optimization settings. Finally, we propose a method to\nidentify the optimal decomposition and compare the existing decomposition works\nwith the optimal decomposition. Existing works either suffer from low coverage\nor cannot generate stable community similarities."}
{"id": "2506.18923", "categories": ["cs.PL", "cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18923", "abs": "https://arxiv.org/abs/2506.18923", "authors": ["Yifan Zong", "Yuntian Deng", "Pengyu Nie"], "title": "Mix-of-Language-Experts Architecture for Multilingual Programming", "comment": "Accepted at LLM4Code @ ICSE 2025", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\naiding developers with tasks like code comprehension, generation, and\ntranslation. Supporting multilingual programming -- i.e., coding tasks across\nmultiple programming languages -- typically requires either (1) finetuning a\nsingle LLM across all programming languages, which is cost-efficient but\nsacrifices language-specific specialization and performance, or (2) finetuning\nseparate LLMs for each programming language, which allows for specialization\nbut is computationally expensive and storage-intensive due to the duplication\nof parameters. This paper introduces MoLE (Mix-of-Language-Experts), a novel\narchitecture that balances efficiency and specialization for multilingual\nprogramming. MoLE is composed of a base model, a shared LoRA (low-rank\nadaptation) module, and a collection of language-specific LoRA modules. These\nmodules are jointly optimized during the finetuning process, enabling effective\nknowledge sharing and specialization across programming languages. During\ninference, MoLE automatically routes to the language-specific LoRA module\ncorresponding to the programming language of the code token being generated.\nOur experiments demonstrate that MoLE achieves greater parameter efficiency\ncompared to training separate language-specific LoRAs, while outperforming a\nsingle shared LLM finetuned for all programming languages in terms of accuracy."}
{"id": "2506.19481", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.19481", "abs": "https://arxiv.org/abs/2506.19481", "authors": ["Shahbaz Siddeeq", "Muhammad Waseem", "Zeeshan Rasheed", "Md Mahade Hasan", "Jussi Rasku", "Mika Saari", "Henri Terho", "Kalle Makela", "Kai-Kristian Kemell", "Pekka Abrahamsson"], "title": "LLM-based Multi-Agent System for Intelligent Refactoring of Haskell Code", "comment": "arXiv admin note: text overlap with arXiv:2502.07928", "summary": "Refactoring is a constant activity in software development and maintenance.\nScale and maintain software systems are based on code refactoring. However,\nthis process is still labor intensive, as it requires programmers to analyze\nthe codebases in detail to avoid introducing new defects. In this research, we\nput forward a large language model (LLM)-based multi-agent system to automate\nthe refactoring process on Haskell code. The objective of this research is to\nevaluate the effect of LLM-based agents in performing structured and\nsemantically accurate refactoring on Haskell code. Our proposed multi-agent\nsystem based on specialized agents with distinct roles, including code\nanalysis, refactoring execution, verification, and debugging. To test the\neffectiveness and practical applicability of the multi-agent system, we\nconducted evaluations using different open-source Haskell codebases. The\nresults of the experiments carried out showed that the proposed LLM-based\nmulti-agent system could average 11.03% decreased complexity in code, an\nimprovement of 22.46% in overall code quality, and increase performance\nefficiency by an average of 13.27%. Furthermore, memory allocation was\noptimized by up to 14.57%. These results highlight the ability of LLM-based\nmulti-agent in managing refactoring tasks targeted toward functional\nprogramming paradigms. Our findings hint that LLM-based multi-agent systems\nintegration into the refactoring of functional programming languages can\nenhance maintainability and support automated development workflows."}
{"id": "2506.19457", "categories": ["cs.PL", "cs.DC", "D.3.1; F.3.1; F.3.2"], "pdf": "https://arxiv.org/pdf/2506.19457", "abs": "https://arxiv.org/abs/2506.19457", "authors": ["Tom T. P. Franken", "Thomas Neele", "Jan Friso Groote"], "title": "The Autonomous Data Language -- Concepts, Design and Formal Verification", "comment": "48 pages, preprint submitted to Elsevier", "summary": "Nowadays, the main advances in computational power are due to parallelism.\nHowever, most parallel languages have been designed with a focus on processors\nand threads. This makes dealing with data and memory in programs hard, which\ndistances the implementation from its original algorithm. We propose a new\nparadigm for parallel programming, the data-autonomous paradigm, where\ncomputation is performed by autonomous data elements. Programs in this paradigm\nare focused on making the data collaborate in a highly parallel fashion. We\nfurthermore present AuDaLa, the first data autonomous programming language, and\nprovide a full formalisation that includes a type system and operational\nsemantics. Programming in AuDaLa is very natural, as illustrated by examples,\nalbeit in a style very different from sequential and contemporary parallel\nprogramming. Additionally, it lends itself for the formal verification of\nparallel programs, which we demonstrate."}
{"id": "2506.19511", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.19511", "abs": "https://arxiv.org/abs/2506.19511", "authors": ["Nina Haugland Andersen", "Anastasiia Tkalich", "Nils Brede Moe", "Darja Smite", "Asgaut Mjølne Söderbom", "Ola Hast", "Viktoria Stray"], "title": "Integrating Pair Programming as a Work Practice", "comment": "The pre-print is submitted to the Journal of Systems and Software", "summary": "Context: Pair programming (PP) is more relevant than ever. As modern systems\ngrow in complexity, knowledge sharing and collaboration across teams have\nbecome essential. However, despite well-documented benefits of PP, its adoption\nremains inconsistent across software teams. Objective: This study aims to\nunderstand the factors that facilitate or hinder team members' adoption as well\nas lasting engagement in PP. Method: We have conducted an exploratory\nsingle-case study in a mature agile company in Norway. We collected data\nthrough two rounds of interviews with team members in different roles and\nperformed a thematic analysis of the interviews. Results: Our key finding is\nthat multiple factors, related to the perceptions of how PP contributes to\ndaily work, efforts associated with engaging in PP sessions, company and team\nattitudes, resources, infrastructure, and task characteristics, affect PP\nengagement. Conclusion: Long-term engagement in PP requires expected benefits\nwith the practice being confirmed in firsthand experiences. Adapting the\npractice to each unique team, with insights drawn from collective learning, is\nalso beneficial. Our findings will be beneficial for software practitioners\nseeking to make PP an integrated part of their team's workflow."}
{"id": "2506.19539", "categories": ["cs.SE", "cs.AI", "D.2.7"], "pdf": "https://arxiv.org/pdf/2506.19539", "abs": "https://arxiv.org/abs/2506.19539", "authors": ["Julian Fragner", "Christian Macho", "Bernhard Dieber", "Martin Pinzger"], "title": "Lost in Translation? Converting RegExes for Log Parsing into Dynatrace Pattern Language", "comment": "18 pages, 7 tables, 18 figures", "summary": "Log files provide valuable information for detecting and diagnosing problems\nin enterprise software applications and data centers. Several log analytics\ntools and platforms were developed to help filter and extract information from\nlogs, typically using regular expressions (RegExes). Recent commercial log\nanalytics platforms provide domain-specific languages specifically designed for\nlog parsing, such as Grok or the Dynatrace Pattern Language (DPL). However,\nusers who want to migrate to these platforms must manually convert their\nRegExes into the new pattern language, which is costly and error-prone. In this\nwork, we present Reptile, which combines a rule-based approach for converting\nRegExes into DPL patterns with a best-effort approach for cases where a full\nconversion is impossible. Furthermore, it integrates GPT-4 to optimize the\nobtained DPL patterns. The evaluation with 946 RegExes collected from a large\ncompany shows that Reptile safely converted 73.7% of them. The evaluation of\nReptile's pattern optimization with 23 real-world RegExes showed an F1-score\nand MCC above 0.91. These results are promising and have ample practical\nimplications for companies that migrate to a modern log analytics platform,\nsuch as Dynatrace."}
{"id": "2506.19653", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.19653", "abs": "https://arxiv.org/abs/2506.19653", "authors": ["Antonios Saravanos"], "title": "Simulating the Waterfall Model: A Systematic Review", "comment": null, "summary": "This systematic mapping study examines how the Waterfall Model has been\nrepresented in computational simulations within peer-reviewed literature. While\nAgile methodologies dominate contemporary software design practices, the\nWaterfall Model persists, particularly, within hybrid approaches that fuse\nstructured, sequential workflows with the adaptability of agile practices.\nDespite its continued presence, little attention has been given to how the\nWaterfall Model is simulated in research contexts. A structured search of major\nacademic databases identified 68 peer-reviewed studies published between 2000\nand 2024. After applying inclusion criteria, selected studies were analyzed\nacross four dimensions: (1) simulation methodologies (e.g., discrete-event\nsimulation, system dynamics), (2) platforms and tools (e.g., Simphony.NET,\nSimPy), (3) geographic and temporal trends, and (4) fidelity to Royce's\noriginal seven-phase model. Discrete-event simulation was most commonly used,\nreflecting the model's sequential nature. Early work relied on proprietary\nplatforms, while recent studies increasingly use open-source, Python-based\ntools. No studies fully implemented Royce's original formulation, most employed\nadaptations. These findings suggest that although niche, simulation of the\nWaterfall Model is present in academic discourse. This work highlights the need\nfor accessible modeling tools and calls for future research that integrates the\nwaterfall software process model with modern hybrid practices."}
{"id": "2506.19677", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.19677", "abs": "https://arxiv.org/abs/2506.19677", "authors": ["Shi Chang", "Boyuan Chen", "Kishanthan Thangarajah", "Hanan Lutfiyya", "Ahmed E. Hassan"], "title": "Adaptive Request Scheduling for CodeLLM Serving with SLA Guarantees", "comment": null, "summary": "Code Large Language Models (CodeLLMs) are increasingly integrated into modern\nsoftware development workflows, yet efficiently serving them in\nresource-constrained, self-hosted environments remains a significant challenge.\nExisting LLM serving systems employs Continuous Batching for throughput\nimprovement. However, they rely on static batch size configurations that cannot\nadapt to fluctuating request rates or heterogeneous workloads, leading to\nfrequent SLA (Service Level Agreement) violations and unstable performance. In\nthis study, We propose SABER, a dynamic batching strategy that predicts\nper-request SLA feasibility and adjusts decisions in real time. SABER improves\ngoodput by up to 26% over the best static configurations and reduces latency\nvariability by up to 45%, all without manual tuning or service restarts. Our\nresults demonstrate that SLA-aware, adaptive scheduling is key to robust,\nhigh-performance CodeLLM serving."}
{"id": "2506.19757", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.19757", "abs": "https://arxiv.org/abs/2506.19757", "authors": ["Rodrigo Oliveira Zacarias", "Léo Carvalho Ramos Antunes", "Márcio de Oliveira Barros", "Rodrigo Pereira dos Santos", "Patricia Lago"], "title": "Exploring Developer Experience Factors in Software Ecosystems", "comment": "58 pages", "summary": "Context: Developer experience (DX) plays a key role in developers'\nperformance and their continued involvement in a software ecosystem (SECO)\nplatform. While researchers and practitioners have recognized several factors\naffecting DX in SECO platforms, a clear roadmap of the most influential factors\nis still missing. This is particularly important given the direct impact on\ndevelopers' interest in SECO and their ongoing engagement with the common\ntechnological platform. Goal: This work aims to identify key DX factors and\nunderstand how they influence third-party developers' decisions to adopt and\nkeep contributing to a SECO. Methods: We conducted a systematic mapping study\n(SMS), analyzing 29 studies to assess the state-of-the-art of DX in SECO.\nAdditionally, we conducted a Delphi study to evaluate the influence of 27 DX\nfactors (identified in our SMS) from the perspective of 21 third-party\ndevelopers to adopt and keep contributing to a SECO. Results: The factors that\nmost strongly influence developers' adoption and ongoing contributions to a\nSECO are: financial costs for using the platform, desired technical resources\nfor development, low barriers to entry into the applications market, and more\nfinancial gains. Conclusion: DX is essential for the success and sustainability\nof SECO. Our set of DX factors provides valuable insights and recommendations\nfor researchers and practitioners to address key DX concerns from the\nperspective of third-party developers."}
{"id": "2506.18923", "categories": ["cs.PL", "cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18923", "abs": "https://arxiv.org/abs/2506.18923", "authors": ["Yifan Zong", "Yuntian Deng", "Pengyu Nie"], "title": "Mix-of-Language-Experts Architecture for Multilingual Programming", "comment": "Accepted at LLM4Code @ ICSE 2025", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\naiding developers with tasks like code comprehension, generation, and\ntranslation. Supporting multilingual programming -- i.e., coding tasks across\nmultiple programming languages -- typically requires either (1) finetuning a\nsingle LLM across all programming languages, which is cost-efficient but\nsacrifices language-specific specialization and performance, or (2) finetuning\nseparate LLMs for each programming language, which allows for specialization\nbut is computationally expensive and storage-intensive due to the duplication\nof parameters. This paper introduces MoLE (Mix-of-Language-Experts), a novel\narchitecture that balances efficiency and specialization for multilingual\nprogramming. MoLE is composed of a base model, a shared LoRA (low-rank\nadaptation) module, and a collection of language-specific LoRA modules. These\nmodules are jointly optimized during the finetuning process, enabling effective\nknowledge sharing and specialization across programming languages. During\ninference, MoLE automatically routes to the language-specific LoRA module\ncorresponding to the programming language of the code token being generated.\nOur experiments demonstrate that MoLE achieves greater parameter efficiency\ncompared to training separate language-specific LoRAs, while outperforming a\nsingle shared LLM finetuned for all programming languages in terms of accuracy."}
