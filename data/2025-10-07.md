<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 28]
- [cs.PL](#cs.PL) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Repairing Leaks in Resource Wrappers](https://arxiv.org/abs/2510.03461)
*Sanjay Malakar,Michael D. Ernst,Martin Kellogg,Manu Sridharan*

Main category: cs.SE

TL;DR: This paper introduces a robust method for automatically repairing resource leaks, even in complex cases involving resource wrappers. By advancing program analysis and repair strategies, their tool (Arodnap) fixes 68% of leaks compared to 41% by prior methods.


<details>
  <summary>Details</summary>
Motivation: Automatically repairing resource leaks is challenging, especially for resource types not found in hard-coded lists, such as resource wrappers. Existing approaches have a limited scope due to their inability to handle wrappers that require custom closing mechanisms.

Method: This paper presents several improvements: (1) integrating resource management specification inference into the repair process, (2) transforming code to aid analysis, (3) introducing field containment analysis to track resource lifetimes, and (4) proposing a new repair pattern for non-final field resources.

Result: The proposed implementation, Arodnap, increases the percentage of automatically fixed resource leak warnings from 41% to 68% on the NJR benchmark suite, significantly improving over prior state-of-the-art.

Conclusion: By enhancing the repair pipeline to reason about resource wrappers and fields, and by providing more sophisticated program analysis and repair patterns, the approach substantially increases the range and effectiveness of automatic resource leak repairs.

Abstract: A resource leak occurs when a program fails to release a finite resource like
a socket, file descriptor or database connection. While sound static analysis
tools can detect all leaks, automatically repairing them remains challenging.
Prior work took the output of a detection tool and attempted to repair only
leaks from a hard-coded list of library resource types. That approach limits
the scope of repairable leaks: real-world code uses resource wrappers that
store a resource in a field and must themselves be closed. This paper makes
four key contributions to improve resource leak repair in the presence of
wrappers. (1) It integrates inference of resource management specifications
into the repair pipeline, enabling extant fixing approaches to reason about
wrappers. (2) It transforms programs into variants that are easier to analyze,
making inference, detection, and fixing tools more effective; for instance, it
makes detection tools report problems closer to the root cause, often in a
client of a resource wrapper rather than within the wrapper class itself. (3) A
novel field containment analysis reasons about resource lifetimes, enabling
repair of more leaks involving resources stored in fields. (4) It introduces a
new repair pattern and more precise reasoning to better handle resources stored
in non-final fields. Prior work fixed 41% of resource leak warnings in the NJR
benchmark suite; our implementation Arodnap fixes 68%.

</details>


### [2] [ALMAS: an Autonomous LLM-based Multi-Agent Software Engineering Framework](https://arxiv.org/abs/2510.03463)
*Vali Tawosi,Keshav Ramani,Salwa Alamir,Xiaomo Liu*

Main category: cs.SE

TL;DR: The paper introduces ALMAS, a multi-agent LLM framework for comprehensive, agile-aligned automation of the software development life-cycle, demonstrating its ability to generate and update applications autonomously in collaboration with human teams.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) have shown promise in automating various aspects of software development, but most existing systems focus on code-related tasks rather than the full software development life-cycle (SDLC). The motivation is to create a more comprehensive and integrated framework that supports all SDLC stages and agile methodologies, enhancing the productivity and collaboration of human developers.

Method: The paper proposes ALMAS, an Autonomous LLM-based Multi-Agent Software Engineering framework. This framework aligns LLM agents with specific agile roles and is designed to be modular, allowing it to integrate easily with human teams and existing development environments. The paper also demonstrates ALMAS through published works and a detailed use case in which it generates an application and adds a new feature autonomously.

Result: ALMAS is shown to be capable of working end-to-end within the software development life-cycle, not only generating an application but also autonomously adding a new feature. The use case and published works demonstrate its progress and potential for seamless integration with agile software development teams.

Conclusion: ALMAS represents a significant step toward comprehensive automation in software engineering, addressing both coding and non-coding tasks throughout the SDLC. Its modular and agile-aligned approach demonstrates promise for real-world integration and collaboration with human developers.

Abstract: Multi-agent Large Language Model (LLM) systems have been leading the way in
applied LLM research across a number of fields. One notable area is software
development, where researchers have advanced the automation of code
implementation, code testing, code maintenance, inter alia, using LLM agents.
However, software development is a multifaceted environment that extends beyond
just code. As such, a successful LLM system must factor in multiple stages of
the software development life-cycle (SDLC). In this paper, we propose a vision
for ALMAS, an Autonomous LLM-based Multi-Agent Software Engineering framework,
which follows the above SDLC philosophy such that it may work within an agile
software development team to perform several tasks end-to-end. ALMAS aligns its
agents with agile roles, and can be used in a modular fashion to seamlessly
integrate with human developers and their development environment. We showcase
the progress towards ALMAS through our published works and a use case
demonstrating the framework, where ALMAS is able to seamlessly generate an
application and add a new feature.

</details>


### [3] [Relative Code Comprehensibility Prediction](https://arxiv.org/abs/2510.03474)
*Nadeeshan De Silva,Martin Kellogg,Oscar Chaparro*

Main category: cs.SE

TL;DR: Directly predicting relative code comprehensibility between code snippets using machine learning is significantly more effective than predicting absolute comprehensibility, overcoming data noise issues and better supporting software engineering tasks like refactoring assessment.


<details>
  <summary>Details</summary>
Motivation: Existing code comprehensibility metrics and direct machine learning models for predicting code comprehensibility have shown poor correlation with human understanding, largely due to noise in human comprehension data. There is a need for more reliable prediction methods to aid developers in tasks like code refactoring.

Method: The authors propose and empirically test a machine learning approach that predicts the relative comprehensibility between pairs of code snippets (i.e., which of two code snippets is easier for humans to understand), instead of predicting the absolute comprehensibility of individual snippets. They used a dataset containing 150 Java code snippets and 12.5k human comprehension scores from prior studies, comparing model performance to naive baselines.

Result: Models predicting absolute comprehensibility performed only marginally better than baselines (maximum 33.4% improvement and often underperformed). In contrast, relative comprehensibility models showed large improvements over baselines, achieving 137.8% and 74.7% average improvements for snippet-wise and developer-wise scenarios, respectively.

Conclusion: Predicting relative comprehensibility between code snippets not only mitigates the noise present in human comprehensibility data but is also more effective than predicting absolute comprehensibility. This approach has practical applicability for software engineering tasks, such as evaluating the impact of refactoring on code comprehensibility.

Abstract: Automatically predicting how difficult it is for humans to understand a code
snippet can assist developers in tasks like deciding when and where to
refactor. Despite many proposed code comprehensibility metrics, studies have
shown they often correlate poorly with actual measurements of human
comprehensibility. This has motivated the use of machine learning models to
predict human comprehensibility directly from code, but these models have also
shown limited accuracy.
  We argue that model inaccuracy stems from inherent noise in human
comprehensibility data, which confuses models trained to predict it directly.
To address this, we propose training models to predict the relative
comprehensibility of two code snippets - that is, predicting which snippet a
human would find easier to understand without predicting each snippet's
comprehensibility in isolation. This mitigates noise in predicting 'absolute'
comprehensibility measurements, but is still useful for downstream
software-engineering tasks like assessing whether refactoring improves or
hinders comprehensibility.
  We conducted a study to assess and compare the effectiveness of absolute and
relative code comprehensibility prediction via machine learning. We used a
dataset of 150 Java code snippets and 12.5k human comprehensibility
measurements from prior user studies, comparing the models' performance with
naive baselines (eg 'always predict the majority class'). Our findings indicate
that absolute comprehensibility models improve over the baselines by at most
33.4% and frequently underperform. In contrast, relative comprehensibility
models are substantially better, with average improvements of 137.8% and 74.7%
for snippet-wise and developer-wise prediction, respectively. These results
suggest that relative comprehensibility models learn more effectively from the
data, supporting their practical applicability for downstream SE tasks.

</details>


### [4] [LLM Agents for Automated Dependency Upgrades](https://arxiv.org/abs/2510.03480)
*Vali Tawosi,Salwa Alamir,Xiaomo Liu,Manuela Veloso*

Main category: cs.SE

TL;DR: This paper introduces an automated LLM agent framework for upgrading library dependencies in Java codebases, showing strong efficiency and a precision rate of 71.4%, outperforming current state-of-the-art approaches.


<details>
  <summary>Details</summary>
Motivation: Maintaining up-to-date library dependencies is essential for innovation and security in growing codebases, but manual updates are time-consuming and risk introducing breaking changes.

Method: The authors propose a framework utilizing LLM (Large Language Model) agents combined with migration documentation to automatically locate usages of outdated libraries in Java codebases and apply recommended updates. The system includes a Summary Agent, Control Agent, and Code Agent to handle different aspects of the upgrade process.

Result: The framework was validated on three synthetic repositories exhibiting significant library upgrades and benchmarked against state-of-the-art solutions. The approach performed upgrades with reduced token usage and achieved a precision of 71.4%.

Conclusion: The proposed LLM agent-based system efficiently automates library upgrades in codebases, performing with higher precision and efficiency compared to existing methods.

Abstract: As a codebase expands over time, its library dependencies can become outdated
and require updates to maintain innovation and security. However, updating a
library can introduce breaking changes in the code, necessitating significant
developer time for maintenance. To address this, we introduce a framework of
LLM agents to be used in combination with migration documentation to
automatically recommend and apply code updates and ensure compatibility with
new versions. Our solution can automatically localize updated library usages in
live Java codebases and implement recommended fixes in a user-friendly manner.
The system architecture consists of multiple key components: a Summary Agent,
Control Agent, and Code Agent. To validate our approach, we apply the framework
on an industrial use case by which we create three synthetic code repositories
with major Upgrade changes and benchmark our approach against state-of-the-art
methods. Results show that our approach not only performs upgrades using fewer
tokens across all cases but also achieves a precision of 71.4%, highlighting
its efficiency and effectiveness compared to state-of-the-art methods.

</details>


### [5] [AgentHub: A Research Agenda for Agent Sharing Infrastructure](https://arxiv.org/abs/2510.03495)
*Erik Pautsch,Tanmay Singla,Wenxin Jiang,Huiyun Peng,Behnaz Hassanshahi,Konstantin Läufer,George K. Thiruvathukal,James C. Davis*

Main category: cs.SE

TL;DR: This paper proposes AgentHub, a research agenda identifying key challenges to create a unified infrastructure for discovering and governing LLM-based agents, aiming for ecosystems as robust as those for software libraries.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents are multiplying, but unlike mature software registries and model hubs, there is no unified infrastructure for their discovery, evaluation, and governance, limiting reuse and scalability.

Method: This study reviews current fragmented approaches and proposes a broad framework by identifying key engineering challenges including capability clarity, lifecycle transparency, interoperability, governance, security, and workflow integration.

Result: AgentHub provides a community-wide vision and set of challenges to encourage development of reliable platforms enabling composable and trustworthy agent sharing.

Conclusion: AgentHub defines a comprehensive research agenda aimed at addressing the infrastructure gaps for sharing and managing LLM-based agents, envisioning seamless, trusted, and scalable agent ecosystems.

Abstract: LLM-based agents are rapidly proliferating, yet the infrastructure for
discovering, evaluating, and governing them remains fragmented compared to
mature ecosystems like software package registries (e.g., npm) and model hubs
(e.g., Hugging Face). Recent research and engineering works have begun to
consider the requisite infrastructure, but so far they focus narrowly -- on
distribution, naming, or protocol negotiation. However, considering broader
software engineering requirements would improve open-source distribution and
ease reuse. We therefore propose AgentHub, a research agenda for agent sharing.
By framing the key challenges of capability clarity, lifecycle transparency,
interoperability, governance, security, and workflow integration, AgentHub
charts a community-wide agenda for building reliable and scalable agent
ecosystems. Our vision is a future where agents can be shared, trusted, and
composed as seamlessly as today's software libraries.

</details>


### [6] [REFINE: Enhancing Program Repair Agents through Context-Aware Patch Refinement](https://arxiv.org/abs/2510.03588)
*Anvith Pabba,Simin Chen,Alex Mathai,Anindya Chakraborty,Baishakhi Ray*

Main category: cs.SE

TL;DR: Refine is a novel framework that refines partially correct patches produced by LLM-based program repair tools. By resolving ambiguities, enhancing candidate diversity, and using LLM-powered review, Refine significantly improves APR performance, offering state-of-the-art results and broad applicability.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based APR techniques struggle to produce fully correct fixes due to limited understanding of code context and often generate only partially correct patches. There is a need for a systematic approach that can transform these near-correct solutions into fully correct program repairs.

Method: The paper introduces Refine, a novel patch refinement framework for LLM-based automatic program repair. Refine operates by disambiguating issue and code contexts, diversifying patch candidates through test-time scaling, and aggregating partial fixes using LLM-powered code review. It is implemented as a general module that can be integrated into various APR systems.

Result: Refine achieves state-of-the-art results for workflow-based APR systems and approaches the best-known results for all APR systems. It improves AutoCodeRover's performance by 14.67%, achieves a 51.67% score on SWE-Bench Lite, and increases the resolution rate by 12.2% on SWE-Bench Verified. Across different APR systems, it yields an average improvement of 14%.

Conclusion: Patch refinement is a crucial and effective missing component in current LLM-based APR systems, significantly improving their ability to produce correct program fixes. The proposed framework, Refine, demonstrates strong generalizability and boosts the effectiveness of multiple APR workflows.

Abstract: Large Language Models (LLMs) have recently shown strong potential in
automatic program repair (APR), especially in repository-level settings where
the goal is to generate patches based on natural language issue descriptions,
large codebases, and regression tests. However, despite their promise, current
LLM-based APR techniques often struggle to produce correct fixes due to limited
understanding of code context and over-reliance on incomplete test suites. As a
result, they frequently generate Draft Patches-partially correct patches that
either incompletely address the bug or overfit to the test cases. In this work,
we propose a novel patch refinement framework, Refine, that systematically
transforms Draft Patches into correct ones. Refine addresses three key
challenges: disambiguating vague issue and code context, diversifying patch
candidates through test-time scaling, and aggregating partial fixes via an
LLM-powered code review process. We implement Refine as a general refinement
module that can be integrated into both open-agent-based and workflow-based APR
systems. Our evaluation on the SWE-Bench Lite benchmark shows that Refine
achieves state-of-the-art results among workflow-based approaches and
approaches the best-known performance across all APR categories. Specifically,
Refine boosts AutoCodeRover's performance by 14.67%, achieving a score of
51.67% and surpassing all prior baselines. On SWE-Bench Verified, Refine
improves the resolution rate by 12.2%, and when integrated across multiple APR
systems, it yields an average improvement of 14%-demonstrating its broad
effectiveness and generalizability. These results highlight the effectiveness
of refinement as a missing component in current APR pipelines and the potential
of agentic collaboration in closing the gap between near-correct and correct
patches. We also open source our code.

</details>


### [7] [Generating High-Level Test Cases from Requirements using LLM: An Industry Study](https://arxiv.org/abs/2510.03641)
*Satoshi Masuda,Satoshi Kouzawa,Kyousuke Sezai,Hidetoshi Suhara,Yasuaki Hiruta,Kunihiro Kudou*

Main category: cs.SE

TL;DR: Manual high-level test case generation from requirement documents is laborious. The authors present an LLM-based prompt-only method, bypassing costly RAG setup. Tests on Bluetooth and Mozilla data show decent recall, suggesting the approach is viable for automating test case generation in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Currently, the automatic generation of high-level test cases from natural language requirement documents is a labor-intensive manual process. While LLMs and retrieval-augmented generation (RAG) techniques are being considered, building application-specific RAGs for different knowledge domains is costly and inefficient. There is a strong need for a generalized approach that can generate high-level test cases without requiring tailored RAGs.

Method: The authors propose a prompt-based approach using LLMs for generating high-level test cases directly from requirement documents, without the need for RAG. The process involves inputting requirement documents into an LLM to first produce relevant test design techniques, then generating high-level test cases for each technique. The quality of generated test cases is assessed using semantic similarity metrics.

Result: Experiments using Bluetooth and Mozilla datasets (each with available requirement documents and high-level test cases) showed macro-recall measurements of 0.81 and 0.37, respectively. This indicates reasonable effectiveness, especially compared to the manual and RAG-based methods, although performance varies across domains.

Conclusion: The proposed prompt-only LLM-based method can effectively and feasibly generate high-level test cases from requirement documents, without the need for labor-intensive RAG creation. This approach demonstrates practical promise, as indicated by experimental recall values, and could be adopted for broader industrial usage.

Abstract: Currently, generating high-level test cases described in natural language
from requirement documents is performed manually. In the industry, including
companies specializing in software testing, there is a significant demand for
the automatic generation of high-level test cases from requirement documents
using Large Language Models (LLMs). Efforts to utilize LLMs for requirement
analysis are underway. In some cases, retrieval-augmented generation (RAG) is
employed for generating high-level test cases using LLMs. However, in practical
applications, it is necessary to create a RAG tailored to the knowledge system
of each specific application, which is labor-intensive. Moreover, when applying
high-level test case generation as a prompt, there is no established method for
instructing the generation of high-level test cases at a level applicable to
other specifications without using RAG. It is required to establish a method
for the automatic generation of high-level test cases that can be generalized
across a wider range of requirement documents. In this paper, we propose a
method for generating high-level (GHL) test cases from requirement documents
using only prompts, without creating RAGs. In the proposed method, first, the
requirement document is input into the LLM to generate test design techniques
corresponding to the requirement document. Then, high-level test cases are
generated for each of the generated test design techniques. Furthermore, we
verify an evaluation method based on semantic similarity of the generated
high-level test cases. In the experiments, we confirmed the method using
datasets from Bluetooth and Mozilla, where requirement documents and high-level
test cases are available, achieving macro-recall measurement of 0.81 and 0.37,
respectively. We believe that the method is feasible for practical application
in generating high-level test cases without using RAG.

</details>


### [8] [Detecting and Preventing Latent Risk Accumulation in High-Performance Software Systems](https://arxiv.org/abs/2510.03712)
*Jahidul Arafat,Kh. M. Moniruzzaman,Shamim Hossain,Fariha Tasmin,Kamrujjaman,Ahsan Habib Tareq*

Main category: cs.SE

TL;DR: The paper presents a proactive framework for detecting and reducing hidden risks in optimized distributed systems, using mathematical models, intelligent testing, and monitoring. It provides strong empirical results, improving reliability, reducing incident impact, and delivering quick financial returns.


<details>
  <summary>Details</summary>
Motivation: Modern distributed systems are highly optimized for performance, but these optimizations can mask hidden vulnerabilities (latent risks) that can cause catastrophic failures when systems deviate from the optimized norm (e.g., cache failures unleashing underlying database bottlenecks). Existing reliability engineering is mostly reactive, not preemptively identifying such risks.

Method: The paper introduces a comprehensive framework based on mathematical modeling, perturbation testing, and risk-aware optimization. It defines a Latent Risk Index (LRI), implements three systems: HYDRA (for perturbation/risk discovery), RAVEN (for ongoing monitoring), and APEX (for risk-aware, performance-preserving optimization). The approach combines empirical analysis, real-world deployment, and rigorous statistical validation.

Result: The framework achieves high rates of risk detection (HYDRA: 89.7%), precise monitoring (RAVEN: 92.9% precision, 93.8% recall), significant reduction in latent risks (APEX: 59.2%), and substantial performance retention (96.6% baseline performance). Real-world deployment reduced mean time to recovery by 69.1%, incident severity by 78.6%, and prevented 81 incidents, yielding significant financial benefits and fast ROI.

Conclusion: This approach shifts the paradigm in distributed systems reliability from reactive management to proactive risk detection and optimization, offering measurable improvements in reliability and cost savings.

Abstract: Modern distributed systems employ aggressive optimization strategies that
create latent risks - hidden vulnerabilities where exceptional performance
masks catastrophic fragility when optimizations fail. Cache layers achieving
99% hit rates can obscure database bottlenecks until cache failures trigger
100x load amplification and cascading collapse. Current reliability engineering
focuses on reactive incident response rather than proactive detection of
optimization-induced vulnerabilities. This paper presents the first
comprehensive framework for systematic latent risk detection, prevention, and
optimization through integrated mathematical modeling, intelligent perturbation
testing, and risk-aware performance optimization. We introduce the Latent Risk
Index (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),
enabling predictive risk assessment. Our framework integrates three systems:
HYDRA employing six optimization-aware perturbation strategies achieving 89.7%
risk discovery rates, RAVEN providing continuous production monitoring with
92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling
risk-aware optimization maintaining 96.6% baseline performance while reducing
latent risks by 59.2%. Evaluation across three testbed environments
demonstrates strong statistical validation with large effect sizes (Cohen
d>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24
weeks shows 69.1% mean time to recovery reduction, 78.6% incident severity
reduction, and 81 prevented incidents generating 1.44M USD average annual
benefits with 3.2-month ROI. Our approach transforms reliability engineering
from reactive incident management to proactive risk-aware optimization.

</details>


### [9] [APIDA-Chat: Structured Synthesis of API Search Dialogues to Bootstrap Conversational Agents](https://arxiv.org/abs/2510.03743)
*Zachary Eberhart,Collin McMillan*

Main category: cs.SE

TL;DR: APIDA-Chat is an open-source pipeline for generating realistic API search dialogues for niche libraries, enabling effective model fine-tuning where data is scarce. Using a planner and student-teacher setup, it improves benchmarks while being cost-effective, modular, and privacy-preserving.


<details>
  <summary>Details</summary>
Motivation: Large language models perform well in explaining popular APIs, but struggle with niche or proprietary ones due to scarce dialogue data for fine-tuning.

Method: The authors developed APIDA-Chat, an open-source pipeline that uses symbolic dialogue-act scripts to generate realistic, domain-grounded API search conversations. Phase I uses a dialogue planner and a teacher LLM to synthesize quality dialogue data, which is then used to fine-tune a smaller student model. In Phase II, the student model generates new dialogues without external LLMs, improving cost and privacy.

Result: The fine-tuned student model improved BLEU score from 0.38 to 0.50 and BERTScore from 0.88 to 0.91 compared to the base model. It runs on a single consumer GPU, and all components are publicly released.

Conclusion: APIDA-Chat offers a practical, modular, open-source solution for generating training data and fine-tuning dialogue models for niche API domains, addressing data scarcity, cost, and privacy problems.

Abstract: Large-language-model assistants are suitable for explaining popular APIs, yet
they falter on niche or proprietary libraries because the multi-turn dialogue
data needed for fine-tuning are scarce. We present APIDA-Chat, an open-source
pipeline that converts symbolic dialogue-act "scripts" into realistic,
domain-grounded API Search conversations using a lightweight model for
inexpensive training data generation. Phase I pairs a legacy dialogue planner
with a high-capability teacher LLM (o4-mini) to synthesize a "gold set" of
realized dialogues; then, a smaller Llama 3.2 3B student model is fine-tuned on
this corpus. Phase II drops the teacher and reuses the same planner with the
fine-tuned model, allowing rapid, low-cost synthesis of new dialogues without
exposing source code to external services. The fine-tuned student improves BLEU
from 0.38 to 0.50 and BERTScore from 0.88 to 0.91 versus the base model while
running entirely on a single consumer GPU. All components are modular and
publicly released to serve as a conservative baseline for future work.
APIDA-Chat is open-sourced at https://github.com/Zeberhart/apida-chat and a
video demo is available at https://youtu.be/YqmZBHyGbPs .

</details>


### [10] [Code4MeV2: a Research-oriented Code-completion Platform](https://arxiv.org/abs/2510.03755)
*Roham Koohestani,Parham Bateni,Aydin Ebrahimi,Behdad Etezadi,Kiarash Karimi,Maliheh Izadi*

Main category: cs.SE

TL;DR: The paper presents Code4MeV2, an open-source code completion tool designed for research, overcoming proprietary barriers by enabling transparent data collection and achieving industry-level performance according to expert and user feedback.


<details>
  <summary>Details</summary>
Motivation: Most AI-powered code completion tools generate valuable user interaction data, but this data is kept proprietary by large corporations. This restricts academic research in human-AI interaction, as researchers lack access to reproducible, large-scale data and must build bespoke platforms for studies.

Method: The authors developed Code4MeV2, an open-source, research-oriented code completion plugin for JetBrains IDEs. It uses a client-server architecture, offers inline code completion and a chat assistant, and focuses on modular, transparent data collection. Its efficacy was assessed through expert evaluations and a user study involving eight participants.

Result: Code4MeV2 delivers industry-level code completion performance with about 200ms latency. Feedback from both researchers and users highlighted its informativeness and usefulness for research purposes.

Conclusion: Code4MeV2 provides the academic community with an effective, open-source platform for research on code completion and human-AI interaction, enabling fine-grained data collection and greater reproducibility.

Abstract: The adoption of AI-powered code completion tools in software development has
increased substantially, yet the user interaction data produced by these
systems remain proprietary within large corporations. This creates a barrier
for the academic community, as researchers must often develop dedicated
platforms to conduct studies on human--AI interaction, making reproducible
research and large-scale data analysis impractical. In this work, we introduce
Code4MeV2, a research-oriented, open-source code completion plugin for
JetBrains IDEs, as a solution to this limitation. Code4MeV2 is designed using a
client--server architecture and features inline code completion and a
context-aware chat assistant. Its core contribution is a modular and
transparent data collection framework that gives researchers fine-grained
control over telemetry and context gathering. Code4MeV2 achieves
industry-comparable performance in terms of code completion, with an average
latency of 200~ms. We assess our tool through a combination of an expert
evaluation and a user study with eight participants. Feedback from both
researchers and daily users highlights its informativeness and usefulness. We
invite the community to adopt and contribute to this tool. More information
about the tool can be found at https://app.code4me.me.

</details>


### [11] [A First Look at the Lifecycle of DL-Specific Self-Admitted Technical Debt](https://arxiv.org/abs/2510.03802)
*Gilberto Recupito,Vincenzo De Martino,Dario Di Nucci,Fabio Palomba*

Main category: cs.SE

TL;DR: This paper analyzes how DL-specific technical debt is introduced, persists, and resolved in ML projects. SATD is most common during early/middle development, persisting longest in training and hardware phases. Targeted management strategies are needed to improve maintainability and quality in DL-enabled systems.


<details>
  <summary>Details</summary>
Motivation: Deep Learning (DL)-enabled systems are rapidly adopted and transform software development, but they bring unique challenges such as maintaining software quality and performance. Self-Admitted Technical Debt (SATD) specifically related to DL is a growing concern, yet its lifecycle—how it's introduced, acknowledged, and resolved—remains poorly understood.

Method: The study utilizes mining software repository techniques to analyze 40 machine learning (ML) projects. It focuses on 185 instances of DL-specific SATD by tracking their introduction and persistence through project commit histories.

Result: DL-specific SATD is most frequently introduced in early and middle development stages, especially during feature implementation and bug fixes. Training and Hardware phases exhibit the longest SATD persistence, signifying where technical debt accumulates and remains unresolved.

Conclusion: There is a pressing need for DL-specific SATD management strategies within DL-enabled systems. By understanding the patterns and durations associated with DL-specific SATD, developers can better target interventions to improve maintainability and software quality.

Abstract: The rapid adoption of Deep Learning (DL)-enabled systems has revolutionized
software development, driving innovation across various domains. However, these
systems also introduce unique challenges, particularly in maintaining software
quality and performance. Among these challenges, Self-Admitted Technical Debt
(SATD) has emerged as a growing concern, significantly impacting the
maintainability and overall quality of ML and DL-enabled systems. Despite its
critical implications, the lifecycle of DL-specific SATD, how developers
introduce, acknowledge, and address it over time-remains underexplored. This
study presents a preliminary analysis of the persistence and lifecycle of
DL-specific SATD in DL-enabled systems. The purpose of this project is to
uncover the patterns of SATD introduction, recognition, and durability during
the development life cycle, providing information on how to manage these
issues. Using mining software repository techniques, we examined 40 ML
projects, focusing on 185 DL-specific SATD instances. The analysis tracked the
introduction and persistence of SATD instances through project commit histories
to assess their lifecycle and developer actions. The findings indicate that
DL-specific SATD is predominantly introduced during the early and middle stages
of project development. Training and Hardware phases showed the longest SATD
durations, highlighting critical areas where debt accumulates and persists.
Additionally, developers introduce DL-specific SATD more frequently during
feature implementation and bug fixes. This study emphasizes the need for
targeted DL-specific SATD management strategies in DL-enabled systems to
mitigate its impact. By understanding the temporal characteristics and
evolution of DL-specific SATD, developers can prioritize interventions at
critical stages to improve the maintainability and quality of the system.

</details>


### [12] [Smart Paste: Automatically Fixing Copy/Paste for Google Developers](https://arxiv.org/abs/2510.03843)
*Vincent Nguyen,Guilherme Herzog,José Cambronero,Marcus Revaj,Aditya Kini,Alexander Frömmgen,Maxim Tabachnyk*

Main category: cs.SE

TL;DR: Manually editing pasted code is common and tedious among Google developers. By using AI to suggest post-paste edits, Smart Paste streamlines this process within the IDE, achieving high user acceptance and meaningful company-wide impact. The approach serves as a strong reference for deploying practical AI features in large environments.


<details>
  <summary>Details</summary>
Motivation: Developers frequently paste code rather than manually typing, which leads to repetitive and time-consuming post-paste edits. Improving and automating these follow-up edits would significantly enhance developer productivity and satisfaction.

Method: The authors iteratively developed and scaled the Smart Paste feature for IDEs at Google, integrating it within Google's environment, focusing on user experience, system integration, and enhancing model capabilities using AI to suggest relevant post-paste edits.

Result: After deployment, Smart Paste received strongly positive feedback, achieving a 45% acceptance rate for its suggestions. The accepted edits constitute more than 1% of all code written across Google, demonstrating substantial impact at enterprise scale.

Conclusion: Smart Paste effectively reduces developer pain associated with editing pasted code, proving valuable and influential within Google's software development ecosystem. The methodology and deployment strategies outlined can guide the development of similar AI-powered productivity features.

Abstract: Manually editing pasted code is a long-standing developer pain point. In
internal software development at Google, we observe that code is pasted 4 times
more often than it is manually typed. These paste actions frequently require
follow-up edits, ranging from simple reformatting and renaming to more complex
style adjustments and cross-language translations. Prior work has shown deep
learning can be used to predict these edits. In this work, we show how to
iteratively develop and scale Smart Paste, an IDE feature for post-paste edit
suggestions, to Google's development environment. This experience can serve as
a guide for AI practitioners on a holistic approach to feature development,
covering user experience, system integration, and model capabilities. Since
deployment, Smart Paste has had overwhelmingly positive feedback with a 45%
acceptance rate. At Google's enterprise scale, these accepted suggestions
account substantially for over 1% of all code written company-wide.

</details>


### [13] [Designing Empirical Studies on LLM-Based Code Generation: Towards a Reference Framework](https://arxiv.org/abs/2510.03862)
*Nathalia Nascimento,Everton Guimaraes,Paulo Alencar*

Main category: cs.SE

TL;DR: A new framework is proposed to standardize how studies evaluate large language model-based code generation, aiming to make future experiments more consistent, comparable, and reproducible.


<details>
  <summary>Details</summary>
Motivation: There is a lack of standardization in the empirical evaluation of large language model (LLM)-based code generation, leading to inconsistent study designs and limited comparability and reproducibility of results.

Method: The authors propose and elaborate a theoretical framework for designing and reporting empirical studies on LLM-based code generation. The framework is informed by their own research experience and a comparative analysis of existing studies. It structures evaluation around core components such as problem sources, quality attributes, and metrics, and is validated through case mappings.

Result: The framework offers a structured method to evaluate LLM-based code generation, enhancing the rigor and comparability of empirical studies. Its applicability is demonstrated through case mappings, and opportunities for further refinement are identified.

Conclusion: A theoretical framework for standardizing the empirical evaluation of LLM-based code generation is proposed and preliminarily validated. The framework supports structured experimentation and reporting, with the aim of improving comparability and reproducibility. Future work will focus on maturing the framework for broader adoption.

Abstract: The rise of large language models (LLMs) has introduced transformative
potential in automated code generation, addressing a wide range of software
engineering challenges. However, empirical evaluation of LLM-based code
generation lacks standardization, with studies varying widely in goals, tasks,
and metrics, which limits comparability and reproducibility. In this paper, we
propose a theoretical framework for designing and reporting empirical studies
on LLM-based code generation. The framework is grounded in both our prior
experience conducting such experiments and a comparative analysis of key
similarities and differences among recent studies. It organizes evaluation
around core components such as problem sources, quality attributes, and
metrics, supporting structured and systematic experimentation. We demonstrate
its applicability through representative case mappings and identify
opportunities for refinement. Looking forward, we plan to evolve the framework
into a more robust and mature tool for standardizing LLM evaluation across
software engineering contexts.

</details>


### [14] [Adversarial Agent Collaboration for C to Rust Translation](https://arxiv.org/abs/2510.03879)
*Tianyu Li,Ruishi Li,Bo Wang,Brandon Paulsen,Umang Mathur,Prateek Saxena*

Main category: cs.SE

TL;DR: ACToR introduces an adversarial, LLM-based approach for translating C code to Rust, outperforming previous methods in reliability, scale, and correctness on real-world codebases, thus offering a feasible path to safer software.


<details>
  <summary>Details</summary>
Motivation: Translating legacy C code to memory-safe languages like Rust helps prevent critical memory safety vulnerabilities that are common in C software, but current automated translation efforts struggle with larger codebases and frequently break due to complex analyses.

Method: The authors introduce ACToR, an LLM agent-based translation approach inspired by GANs. It involves a translator agent that generates and refines Rust code to pass existing tests, and a discriminator agent that identifies failing tests, allowing iterative improvement.

Result: ACToR was able to translate all 63 real-world command line utilities tested (average size: 485 LoC) with more than a 90% test pass rate and no human intervention. It also improved translation correctness by up to 18.9% over non-adversarial baselines.

Conclusion: ACToR is the first system to reliably translate C programs of this size to Rust with such high test pass rates and correctness, demonstrating significant practical advances in automated, scalable, and memory-safe code translation.

Abstract: Translating C to memory-safe languages, like Rust, prevents critical memory
safety vulnerabilities that are prevalent in legacy C software. Existing
approaches for C to safe Rust translation, including LLM-assisted ones, do not
generalize on larger (> 500 LoC) C codebases because they depend on complex
program analyses that frequently break. In this work, we present ACToR
(Adversarial C To Rust translator), a simple LLM agent-based approach. Inspired
by GANs, ACToR pits a generator agent against a discriminator agent, which
collaborate to iteratively generate a Rust translation. On each iteration, the
translator agent synthesizes and refines a Rust translation to pass an existing
suite of tests, and then the discriminator agent finds new failing tests. We
demonstrate that ACToR translates all of the 63 real-world command line
utilities considered in our benchmarks, which have an average size of 485 lines
of code, and it achieves over 90% test pass rate with zero human intervention.
To our knowledge, it is the first such system that reliably translates C
programs of this scale. Furthermore, ACToR improves translation correctness by
up to 18.9% compared to baseline, non-adversarial approaches.

</details>


### [15] [Rethinking Services in the Quantum Age: The SOQ Paradigm](https://arxiv.org/abs/2510.03890)
*Jose Garcia-Alonso,Enrique Moguel,Jaime Alvarado-Valiente,Javier Romero-Alvarez,Álvaro M. Aparicio-Morales,Juan M. Murillo,Francisco Javier Cavero,Adrián Romero-Flores,Alfonso E. Marquez-Chamorro,José Antonio Parejo,Antonio Ruiz-Cortés,Giuseppe Bisicchia,Alessandro Bocci,Antonio Brogi*

Main category: cs.SE

TL;DR: This paper proposes Service-Oriented Quantum (SOQ), a new paradigm that establishes quantum services as standalone, interoperable building blocks for modern software systems, offering practical solutions for scalability and integration challenges in quantum computing.


<details>
  <summary>Details</summary>
Motivation: Quantum computing, despite its impressive theoretical and practical promise, faces barriers in real-world integration due to hardware limitations and the absence of sound software engineering principles.

Method: The paper introduces Service-Oriented Quantum (SOQ), building on but diverging from Quantum Service-Oriented Computing (QSOC), and articulates core principles and a layered technology stack for SOQ realization. It also outlines challenges such as interoperability, pricing models, and workforce development.

Result: SOQ redefines quantum services as autonomous, composable, and interoperable entities and provides a framework for their integration into software systems. The paper presents SOQ’s technology stack and clarifies essential research and engineering barriers to be addressed.

Conclusion: By positioning quantum services as independently scalable and modular software entities, SOQ enables the robust and flexible integration of quantum computing into real-world systems, advancing both technological adoption and engineering practice.

Abstract: Quantum computing is rapidly progressing from theoretical promise to
practical implementation, offering significant computational advantages for
tasks in optimization, simulation, cryptography, and machine learning. However,
its integration into real-world software systems remains constrained by
hardware fragility, platform heterogeneity, and the absence of robust software
engineering practices. This paper introduces Service-Oriented Quantum (SOQ), a
novel paradigm that reimagines quantum software systems through the lens of
classical service-oriented computing. Unlike prior approaches such as Quantum
Service-Oriented Computing (QSOC), which treat quantum capabilities as
auxiliary components within classical systems, SOQ positions quantum services
as autonomous, composable, and interoperable entities. We define the
foundational principles of SOQ, propose a layered technology stack to support
its realization, and identify the key research and engineering challenges that
must be addressed, including interoperability, hybridity, pricing models,
service abstractions, and workforce development. This approach is of vital
importance for the advancement of quantum technology because it enables the
scalable, modular, and interoperable integration of quantum computing into
real-world software systems independently and without relying on a dedicated
classical environment to manage quantum processing.

</details>


### [16] [A Brief History of the Waterfall Model: Past, Present, and Future](https://arxiv.org/abs/2510.03894)
*Antonios Saravanos*

Main category: cs.SE

TL;DR: This paper critically reviews the history and evolution of the waterfall software development model, arguing that its adaptability and integration into hybrid approaches ensure its ongoing relevance in software engineering.


<details>
  <summary>Details</summary>
Motivation: To provide a historical and critical overview of the waterfall software development model, tracing its origins, evolution, and continued influence on contemporary software engineering practices.

Method: A literature review and synthesis of scholarly studies are used to analyze the history, criticism, adaptation, and evolving role of the waterfall model in software engineering.

Result: The waterfall model, despite criticism for its rigidity and high failure rates, persists in certain domains and its principles influence modern hybrid development frameworks. The model has evolved from a standalone framework to a component within agile-traditional hybrid methodologies.

Conclusion: The waterfall model remains relevant by being adaptable and integrated into contemporary context-aware development strategies. Recognizing its strengths and limitations allows for informed methodology choices and better process design.

Abstract: The waterfall model, one of the earliest software development methodologies,
has played a foundational role in shaping contemporary software engineering
practices. This paper provides a historical and critical overview of the model,
tracing its conceptual origins in software engineering, its formalization by
Royce, and its evolution through decades of industry adoption and critique.
Although often criticized for its rigidity, shortcomings, and high failure
rates, the waterfall model persists in specific domains. Its principles
continue to influence contemporary hybrid development frameworks that combine
traditional and agile methods. Drawing on a range of scholarly sources, this
study synthesizes key developments in the perception and application of the
waterfall model. The analysis highlights how the model has shifted from a
standalone framework to a component within modern hybrid methodologies. By
revisiting its origins, assessing its present utility, and examining its role
in contemporary development practices, this paper argues that the waterfall
model remains relevant, not as a relic of the past but as part of context-aware
development strategies. The paper contends that the model's enduring relevance
lies in its adaptability. By recognizing both its limitations and its
strengths, and by understanding its integration within hybrid approaches,
practitioners can make more informed decisions about methodology selection and
process design in diverse development environments.

</details>


### [17] [Multi-Agent Code-Orchestrated Generation for Reliable Infrastructure-as-Code](https://arxiv.org/abs/2510.03902)
*Rana Nameer Hussain Khan,Dawood Wasif,Jin-Hee Cho,Ali Butt*

Main category: cs.SE

TL;DR: MACOG uses a team of specialized AI agents (not just one big model) to generate cloud infrastructure code that's more accurate and policy-compliant. This architecture outperforms existing methods on benchmarks, and key features like feedback and policy checks are crucial to its success.


<details>
  <summary>Details</summary>
Motivation: Cloud-native infrastructures have become highly complex, necessitating Infrastructure-as-Code (IaC) for reliable, reproducible, and scalable deployment. However, existing large language models (LLMs) that generate IaC code from natural language typically make mistakes: their monolithic, single-step approach often yields syntactic errors, non-compliant code, and poor scalability. There is a need to improve the accuracy and policy-compliance of automated IaC code generation.

Method: The paper introduces MACOG, a Multi-Agent LLM-based architecture that breaks down IaC code generation into modular subtasks managed by specialized agents (such as Architect, Security Prover, DevOps, etc.). These agents communicate via a shared-blackboard system driven by a finite-state orchestrator. The system ensures code correctness with Terraform Plan (for validation) and Open Policy Agent (OPA) for policy enforcement.

Result: MACOG outperforms traditional LLM-based approaches on the IaC-Eval benchmark. For example, integrating MACOG with GPT-5 raises the score from 54.90 to 74.02, and for Gemini-2.5 Pro from 43.56 to 60.13. It also demonstrates improvements in BLEU, CodeBERTScore, and LLM-judge metrics. Further analysis reveals that essential features like constrained decoding and deployment feedback are critical components, as removing them causes significant performance drops.

Conclusion: MACOG’s multi-agent, orchestrated approach produces IaC code that is more accurate, compliant, and robust than previous monolithic LLM methods. The system represents a step forward in automated, reliable infrastructure code generation for complex cloud-native environments.

Abstract: The increasing complexity of cloud-native infrastructure has made
Infrastructure-as-Code (IaC) essential for reproducible and scalable
deployments. While large language models (LLMs) have shown promise in
generating IaC snippets from natural language prompts, their monolithic,
single-pass generation approach often results in syntactic errors, policy
violations, and unscalable designs. In this paper, we propose MACOG
(Multi-Agent Code-Orchestrated Generation), a novel multi-agent LLM-based
architecture for IaC generation that decomposes the task into modular subtasks
handled by specialized agents: Architect, Provider Harmonizer, Engineer,
Reviewer, Security Prover, Cost and Capacity Planner, DevOps, and Memory
Curator. The agents interact via a shared-blackboard, finite-state orchestrator
layer, and collectively produce Terraform configurations that are not only
syntactically valid but also policy-compliant and semantically coherent. To
ensure infrastructure correctness and governance, we incorporate Terraform Plan
for execution validation and Open Policy Agent (OPA) for customizable policy
enforcement. We evaluate MACOG using the IaC-Eval benchmark, where MACOG is the
top enhancement across models, e.g., GPT-5 improves from 54.90 (RAG) to 74.02
and Gemini-2.5 Pro from 43.56 to 60.13, with concurrent gains on BLEU,
CodeBERTScore, and an LLM-judge metric. Ablations show constrained decoding and
deploy feedback are critical: removing them drops IaC-Eval to 64.89 and 56.93,
respectively.

</details>


### [18] [Refactoring with LLMs: Bridging Human Expertise and Machine Understanding](https://arxiv.org/abs/2510.03914)
*Yonnel Chen Kuang Piao,Jean Carlors Paul,Leuson Da Silva,Arghavan Moradi Dakhel,Mohammad Hamdaqa,Foutse Khomh*

Main category: cs.SE

TL;DR: Integrating human best-practice refactoring guidelines as instructions for LLMs greatly improves their automated refactoring performance and code quality, especially when using rule-based or goal-focused strategies.


<details>
  <summary>Details</summary>
Motivation: Code refactoring improves code quality and maintainability but is often neglected by developers due to its high cost and lack of immediate benefits. Existing automated refactoring tools are limited in the variety of refactoring types they can handle.

Method: The study leverages instruction-following and code comprehension capabilities of advanced LLMs (e.g., GPT-mini, DeepSeek-V3), designing multiple instruction strategies based on Martin Fowler's refactoring guidelines. These strategies encode motivations, procedural steps, and objectives for 61 refactoring types and are evaluated on benchmarks and real-world GitHub code snippets.

Result: Instruction designs based on Fowler's guidelines allowed LLMs to successfully perform all benchmarked refactoring types while preserving semantics. Rule-based instructions often yielded better performance than descriptive instructions in some scenarios, and focusing models on the overall refactoring goal improved code quality further.

Conclusion: Human-inspired instruction strategies significantly enhance the ability of LLMs to automatically perform diverse refactoring tasks, preserve program semantics, and improve code quality in practice.

Abstract: Code refactoring is a fundamental software engineering practice aimed at
improving code quality and maintainability. Despite its importance, developers
often neglect refactoring due to the significant time, effort, and resources it
requires, as well as the lack of immediate functional rewards. Although several
automated refactoring tools have been proposed, they remain limited in
supporting a broad spectrum of refactoring types. In this study, we explore
whether instruction strategies inspired by human best-practice guidelines can
enhance the ability of Large Language Models (LLMs) to perform diverse
refactoring tasks automatically. Leveraging the instruction-following and code
comprehension capabilities of state-of-the-art LLMs (e.g., GPT-mini and
DeepSeek-V3), we draw on Martin Fowler's refactoring guidelines to design
multiple instruction strategies that encode motivations, procedural steps, and
transformation objectives for 61 well-known refactoring types. We evaluate
these strategies on benchmark examples and real-world code snippets from GitHub
projects. Our results show that instruction designs grounded in Fowler's
guidelines enable LLMs to successfully perform all benchmark refactoring types
and preserve program semantics in real-world settings, an essential criterion
for effective refactoring. Moreover, while descriptive instructions are more
interpretable to humans, our results show that rule-based instructions often
lead to better performance in specific scenarios. Interestingly, allowing
models to focus on the overall goal of refactoring, rather than prescribing a
fixed transformation type, can yield even greater improvements in code quality.

</details>


### [19] [Why Does the Engineering Manager Still Exist in Agile Software Development?](https://arxiv.org/abs/2510.03920)
*Ravi Kalluri*

Main category: cs.SE

TL;DR: Despite Agile's emphasis on team autonomy, engineering managers remain necessary in practice. This paper analyzes why, through literature review and case studies, and proposes a model aligning Agile values with the realities of managerial roles.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the paradox that, while Agile methods advocate for decentralized authority and team autonomy, engineering managers remain prevalent in Agile software organizations. This challenges the assumption that Agile should eliminate traditional managerial roles.

Method: The paper employs a systematic literature review supported by illustrative case studies. It analyzes the persistence of engineering managers through a multidimensional framework, considering historical context, theory, organizational practice, empirical data, evolving roles, and practical impacts.

Result: The analysis reveals that engineering managers continue to play important roles in Agile organizations, even as those roles evolve to align with Agile principles. The paper proposes a conceptual model that reconciles Agile philosophy with managerial necessity.

Conclusion: Traditional managerial functions persist in Agile environments due to practical and organizational realities. The paper offers a new model that integrates managerial roles into Agile frameworks and provides recommendations for leadership development, tool integration, and future research.

Abstract: Although Agile methodologies emphasize decentralized decision-making and team
autonomy, engineering managers continue to be employed in Agile software
organizations. This apparent paradox suggests that traditional managerial
functions persist despite the theoretical displacement of managerial hierarchy
in Agile. This paper explores the persistence of engineering managers through a
multidimensional framework encompassing historical context, theoretical
tensions, organizational realities, empirical evidence, evolving managerial
roles, and practical implications. A systematic literature review underpins our
multifaceted analysis, supplemented by illustrative case studies. We conclude
by proposing a conceptual model that reconciles Agile principles with
managerial necessity, offering guidance for practitioners, researchers, and
tool designers. Implications for leadership development, tool integration, and
future research are discussed.

</details>


### [20] [Bamboo: LLM-Driven Discovery of API-Permission Mappings in the Android Framework](https://arxiv.org/abs/2510.04078)
*Han Hu,Wei Minn,Yonghui Liu,Jiakun Liu,Ferdian Thung,Terry Yue Zhuo,Lwin Khin Shar,Debin Gao,David Lo*

Main category: cs.SE

TL;DR: This paper proposes a new LLM-based tool for discovering Android API-permission mappings, greatly improving over existing methods in coverage and effectiveness, helping developers avoid mistakes and strengthen app security.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the recurring problem of imprecise and incomplete Android API documentation related to permissions, which forces developers to spend excessive effort and often leads to erroneous permission declarations. Existing code analysis approaches to extract these mappings are hampered by adaptability issues and limited coverage.

Method: The paper introduces a novel methodology relying on large language models (LLMs) and incorporates a dual-role prompting strategy and an API-driven code generation approach. These are integrated into a tool (referred to as \tool{}), designed to systematically discover API-permission mappings.

Result: The new tool, \tool{}, uncovers significantly more API-permission mappings than previous approaches: 2,234 for Android 6, 3,552 for Android 7, and 4,576 for Android 10, thus outperforming existing baselines.

Conclusion: LLMs, when augmented with the described strategies, can systematically and effectively augment the process of extracting permission mappings, surpassing the limitations of static/dynamic analysis and resulting in richer, more comprehensive datasets for multiple Android versions.

Abstract: The permission mechanism in the Android Framework is integral to safeguarding
the privacy of users by managing users' and processes' access to sensitive
resources and operations. As such, developers need to be equipped with an
in-depth understanding of API permissions to build robust Android apps.
Unfortunately, the official API documentation by Android chronically suffers
from imprecision and incompleteness, causing developers to spend significant
effort to accurately discern necessary permissions. This potentially leads to
incorrect permission declarations in Android app development, potentially
resulting in security violations and app failures. Recent efforts in improving
permission specification primarily leverage static and dynamic code analyses to
uncover API-permission mappings within the Android framework. Yet, these
methodologies encounter substantial shortcomings, including poor adaptability
to Android SDK and Framework updates, restricted code coverage, and a
propensity to overlook essential API-permission mappings in intricate
codebases. This paper introduces a pioneering approach utilizing large language
models (LLMs) for a systematic examination of API-permission mappings. In
addition to employing LLMs, we integrate a dual-role prompting strategy and an
API-driven code generation approach into our mapping discovery pipeline,
resulting in the development of the corresponding tool, \tool{}. We formulate
three research questions to evaluate the efficacy of \tool{} against
state-of-the-art baselines, assess the completeness of official SDK
documentation, and analyze the evolution of permission-required APIs across
different SDK releases. Our experimental results reveal that \tool{} identifies
2,234, 3,552, and 4,576 API-permission mappings in Android versions 6, 7, and
10 respectively, substantially outprforming existing baselines.

</details>


### [21] [GA4GC: Greener Agent for Greener Code via Multi-Objective Configuration Optimization](https://arxiv.org/abs/2510.04135)
*Jingzhi Gong,Yixin Bian,Luis de la Cal,Giovanni Pinna,Anisha Uteem,David Williams,Mar Zamorano,Karine Even-Mendoza,W. B. Langdon,Hector Menendez,Federica Sarro*

Main category: cs.SE

TL;DR: GA4GC framework helps coding agents powered by LLMs run much more efficiently and accurately by optimizing key settings, especially temperature, leading to big runtime savings and better code correctness.


<details>
  <summary>Details</summary>
Motivation: Coding agents powered by LLMs have unsustainable resource and environmental costs, with large token usage that can negate the benefits of code optimization, posing barriers to industrial-scale deployment.

Method: The paper introduces GA4GC—a framework for systematically balancing coding agent runtime costs and code performance, using Pareto optimization of agent hyperparameters and prompt templates.

Result: On the SWE-Perf benchmark, GA4GC achieves up to 135x hypervolume improvement, reducing agent runtime by 37.7% while also increasing correctness. It identifies temperature as the most impactful hyperparameter.

Conclusion: The findings provide practical strategies to optimize both agent sustainability and effectiveness, supporting more eco-friendly and scalable deployment of coding agents.

Abstract: Coding agents powered by LLMs face critical sustainability and scalability
challenges in industrial deployment, with single runs consuming over 100k
tokens and incurring environmental costs that may exceed optimization benefits.
This paper introduces GA4GC, the first framework to systematically optimize
coding agent runtime (greener agent) and code performance (greener code)
trade-offs by discovering Pareto-optimal agent hyperparameters and prompt
templates. Evaluation on the SWE-Perf benchmark demonstrates up to 135x
hypervolume improvement, reducing agent runtime by 37.7% while improving
correctness. Our findings establish temperature as the most critical
hyperparameter, and provide actionable strategies to balance agent
sustainability with code optimization effectiveness in industrial deployment.

</details>


### [22] [Detecting Semantic Clones of Unseen Functionality](https://arxiv.org/abs/2510.04143)
*Konstantinos Kitsios,Francesco Sovrano,Earl T. Barr,Alberto Bacchelli*

Main category: cs.SE

TL;DR: Current semantic code clone detectors fail to generalize to unseen functionalities, with major drops in accuracy. Contrastive learning methods adapted from other fields boost performance for both task-oriented models and LLMs, enabling better generalization and stronger clone detection across unseen code tasks.


<details>
  <summary>Details</summary>
Motivation: Semantic code clone detection is important for developers to identify code snippets with similar functionality. However, current neural models struggle to detect clones of functionalities not seen during training, which limits practical utility.

Method: The authors re-evaluated six state-of-the-art models, including both task-specific neural models and generative LLMs, on their ability to detect clones of unseen functionality. They introduced the use of contrastive learning (from CV and NLP) to improve generalization, modifying models with contrastive classifiers and developing contrastive in-context learning for LLMs.

Result: Task-specific models experience up to 48% (average 31%) F1 drop when detecting clones of unseen functionality, while LLMs generalize better with only up to 5% (average 3%) F1 drop. Applying contrastive learning raises F1 on unseen clone detection by up to 26% (average 9%) for task-specific models and up to 5% (average 3%) for LLMs.

Conclusion: Semantic clone detection models struggle with unseen functionalities. Contrastive learning significantly improves performance, especially narrowing the generalization gap for task-specific models.

Abstract: Semantic code clone detection is the task of detecting whether two snippets
of code implement the same functionality (e.g., Sort Array). Recently, many
neural models achieved near-perfect performance on this task. These models seek
to make inferences based on their training data. Consequently, they better
detect clones similar to those they have seen during training and may struggle
to detect those they have not. Developers seeking clones are, of course,
interested in both types of clones. We confirm this claim through a literature
review, identifying three practical clone detection tasks in which the model's
goal is to detect clones of a functionality even if it was trained on clones of
different functionalities. In light of this finding, we re-evaluate six
state-of-the-art models, including both task-specific models and generative
LLMs, on the task of detecting clones of unseen functionality. Our experiments
reveal a drop in F1 of up to 48% (average 31%) for task-specific models. LLMs
perform on par with task-specific models without explicit training for clone
detection, but generalize better to unseen functionalities, where F1 drops up
to 5% (average 3%) instead. We propose and evaluate the use of contrastive
learning to improve the performance of existing models on clones of unseen
functionality. We draw inspiration from the computer vision and natural
language processing fields where contrastive learning excels at measuring
similarity between two objects, even if they come from classes unseen during
training. We replace the final classifier of the task-specific models with a
contrastive classifier, while for the generative LLMs we propose contrastive
in-context learning, guiding the LLMs to focus on the differences between
clones and non-clones. The F1 on clones of unseen functionality is improved by
up to 26% (average 9%) for task-specific models and up to 5% (average 3%) for
LLMs.

</details>


### [23] [Multi Language Models for On-the-Fly Syntax Highlighting](https://arxiv.org/abs/2510.04166)
*Marco Edoardo Palma,Pooja Rani,Harald C. Gall*

Main category: cs.SE

TL;DR: This paper presents a scalable deep learning approach for syntax highlighting that supports multiple languages in one model, lowers operational cost, improves generalization, and reduces training data needs, making online code editors faster and easier to maintain.


<details>
  <summary>Details</summary>
Motivation: The motivation is to resolve the challenges of delivering accurate, real-time syntax highlighting in online and web-based code editors, given strict backend time and memory constraints. Existing solutions are either too slow or require resource-intensive, language-specific models, which raise complexity and operational costs in multi-language environments.

Method: The paper introduces a unified deep learning model that is capable of syntax highlighting across six mainstream programming languages. It incorporates a novel normalization technique to enhance generalization and leverages few-shot learning to reduce reliance on large, slow-generated datasets by demonstrating that a small number of oracle samples can effectively train the model.

Result: The proposed unified model reduces deployment complexity by a factor of six and improves performance on unseen languages. The normalization technique substantially boosts generalization, and few-shot learning shows large datasets are not required, minimizing dependence on brute-force highlight generators.

Conclusion: Efficient, scalable, and cost-effective syntax highlighting for multiple programming languages is achievable through a unified model, normalization-driven generalization, and minimal training data requirements, addressing core limitations of previous approaches.

Abstract: Syntax highlighting is a critical feature in modern software development
environments, enhancing code readability and developer productivity. However,
delivering accurate highlighting in real time remains challenging for online
and web-based development tools due to strict time and memory constraints on
backend services. These systems must serve highlights rapidly and frequently,
even when code is partially valid or invalid. This has led to on-the-fly syntax
highlighting, where visual annotations are generated just before content is
served, often at high request rates and under incomplete input conditions. To
meet these demands efficiently, state-of-the-art models use deep learning to
learn the behavior of brute-force syntax highlighting resolvers, tools that are
easy to implement but too slow for production. Through the Deep Abstraction
process, brute-force strategies are encoded into fast statistical models that
achieve both high accuracy and low-latency inference. Despite their success,
such models face key challenges: they support only one programming language per
model, require large datasets from slow brute-force generators, and involve
resource-intensive training. In multi-language environments, this means
maintaining multiple independent models, increasing system complexity and
operational cost. This work addresses these issues by introducing a unified
model capable of highlighting up to six mainstream programming languages,
reducing deployment complexity by a factor of six and improving performance on
unseen languages. A novel normalization technique significantly enhances model
generalization, while few-shot learning experiments show that a small number of
oracle samples can replace large datasets, minimizing dependence on brute-force
generators. Combined, these innovations enable efficient, scalable, and
cost-effective syntax highlighting across diverse programming languages.

</details>


### [24] [Selecting Cybersecurity Requirements: Effects of LLM Use and Professional Software Development Experience](https://arxiv.org/abs/2510.04274)
*Damjan Fujs,Damjan Vavpotič,Tomaž Hovelja,Marko Poženel*

Main category: cs.SE

TL;DR: Access to LLMs didn't affect how postgraduate students prioritized or evaluated cybersecurity requirements for web apps, but professional software development experience did, especially regarding cost, user experience, and risk of not implementing features.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to understand the effects of using Large Language Models (LLMs) and varying software development experience levels on how individuals prioritize cybersecurity requirements for web applications.

Method: Twenty-three postgraduate students were divided into two groups: one with LLM access, one without. They prioritized security requirements using the MoSCoW method and rated their solutions on several evaluation criteria.

Result: No significant difference was found between groups with or without LLM access in the prioritization or evaluation of cybersecurity solutions. However, differences based on professional experience were significant for criteria like estimated cost, perceived user experience impact, and risk assessment.

Conclusion: LLM access did not impact the prioritization or evaluation of cybersecurity requirements, but professional experience did, affecting considerations such as development cost, user experience, and risk assessment.

Abstract: This study investigates how access to Large Language Models (LLMs) and
varying levels of professional software development experience affect the
prioritization of cybersecurity requirements for web applications. Twenty-three
postgraduate students participated in a research study to prioritize security
requirements (SRs) using the MoSCoW method and subsequently rated their
proposed solutions against multiple evaluation criteria. We divided
participants into two groups (one with and the other without access to LLM
support during the task). Results showed no significant differences related to
LLM use, suggesting that access to LLMs did not noticeably influence how
participants evaluated cybersecurity solutions. However, statistically
significant differences emerged between experience groups for certain criteria,
such as estimated cost to develop a feature, perceived impact on user
experience, and risk assessment related to non-implementation of the proposed
feature. Participants with more professional experience tended to provide
higher ratings for user experience impact and lower risk estimates.

</details>


### [25] [Challenge on Optimization of Context Collection for Code Completion](https://arxiv.org/abs/2510.04349)
*Dmitry Ustalov,Egor Bogomolov,Alexander Bezzubov,Yaroslav Golubev,Evgeniy Glukhov,Georgii Levtsov,Vladimir Kovalenko*

Main category: cs.SE

TL;DR: This paper describes a competitive challenge focused on improving code completion by optimizing context collection from large code bases in Python and Kotlin. Teams developed and evaluated solutions on a large open-source dataset, with results benchmarked using state-of-the-art neural models and the chrF metric, highlighting best practices and providing valuable resources for future research.


<details>
  <summary>Details</summary>
Motivation: The increasing use of AI in software engineering highlights the need to evaluate how well these systems can utilize information from large code bases for tasks like code completion.

Method: Conducted a challenge as part of ASE 2025, where teams developed efficient context collection mechanisms from source code repositories to improve fill-in-the-middle code completions in Python and Kotlin, leveraging a large open-source dataset and comparing submissions via the chrF metric.

Result: Submissions were assessed on completion quality by multiple neural models; nineteen teams participated in Python, eight in Kotlin, with six teams moving to the private phase and five submitting papers for further analysis.

Conclusion: Systematic evaluation shows active and diverse approaches to optimizing code completion via better context collection, contributing datasets and benchmarks for future AI-assisted software engineering research.

Abstract: The rapid advancement of workflows and methods for software engineering using
AI emphasizes the need for a systematic evaluation and analysis of their
ability to leverage information from entire projects, particularly in large
code bases. In this challenge on optimization of context collection for code
completion, organized by JetBrains in collaboration with Mistral AI as part of
the ASE 2025 conference, participants developed efficient mechanisms for
collecting context from source code repositories to improve fill-in-the-middle
code completions for Python and Kotlin. We constructed a large dataset of
real-world code in these two programming languages using permissively licensed
open-source projects. The submissions were evaluated based on their ability to
maximize completion quality for multiple state-of-the-art neural models using
the chrF metric. During the public phase of the competition, nineteen teams
submitted solutions to the Python track and eight teams submitted solutions to
the Kotlin track. In the private phase, six teams competed, of which five
submitted papers to the workshop.

</details>


### [26] [MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models](https://arxiv.org/abs/2510.04363)
*Hyunjun Kim,Sejong Kim*

Main category: cs.SE

TL;DR: MacroBench tests LLMs on their ability to write Selenium code for web automation from instructions. While top models perform well on basic tasks across seven popular web site types, none succeed on complex tasks or generate robust code for production use. The benchmark, data, and results are public for reproducibility.


<details>
  <summary>Details</summary>
Motivation: Evaluate the real-world capability of LLMs to automatically synthesize reusable browser automation scripts from natural language instructions.

Method: Introduced MacroBench—a benchmark with seven self-hosted web sites imitating popular platforms (Airbnb, TikTok, Reddit, etc.), covering 681 diverse tasks. The framework assesses LLM-generated Selenium/Python code via static checks, sandboxed executions, DOM assertions, database snapshots, and a safety suite.

Result: GPT-4o-Mini achieved a 96.8% task success rate, GPT-4.1 95.3%, Gemini-2.5-Pro 89.0%, and DeepSeek-V3.1 83.4%. LLMs reliably solve simple tasks (91.7%) but fail entirely on complex workflows, and none adhere to production coding practices despite functional results.

Conclusion: MacroBench enables reproducible, realistic assessments of LLM synthesized web automation programs. Current LLMs perform well on simple browser tasks but cannot handle complex workflows or produce production-quality code.

Abstract: We introduce MacroBench, a code-first benchmark that evaluates whether LLMs
can synthesize reusable browser automation programs from natural language goals
by reading HTML/DOM and emitting Python with Selenium. MacroBench instantiates
seven self-hosted sites: Airbnb-like, TikTok-like, Reddit-like, Instagram-like,
Facebook-like, Discord-like, and Threads-like, covering 681 tasks across
interaction complexity and targeting difficulty. Our end-to-end protocol
validates generated code via static checks, sandboxed execution, and outcome
verification including DOM assertions and database snapshots, and includes a
safety suite for scraping, spam/abuse, and credential/privacy prompts. Across
2636 model-task runs, we observe stratified success: GPT-4o-Mini achieves 96.8
percent, GPT-4.1 achieves 95.3 percent, Gemini-2.5-Pro achieves 89.0 percent,
and DeepSeek-V3.1 achieves 83.4 percent. Models handle simple tasks reliably at
91.7 percent but fail on complex workflows at 0.0 percent, and none meet
production-quality coding practices despite functional completion. We release
our complete benchmark pipeline, evaluation framework, and experimental results
to enable reproducible assessment of macro synthesis for web automation.

</details>


### [27] [Reconsidering Requirements Engineering: Human-AI Collaboration in AI-Native Software Development](https://arxiv.org/abs/2510.04380)
*Mateen Ahmed Abbasi,Petri Ihantola,Tommi Mikkonen,Niko Mäkitalo*

Main category: cs.SE

TL;DR: AI can enhance Requirement Engineering by automating processes and aiding collaboration but also introduces ethical and transparency concerns. The paper advocates for ethical AI and closer academia-industry collaboration to develop reliable, useful tools.


<details>
  <summary>Details</summary>
Motivation: Requirement Engineering (RE) is foundational for software development, but faces challenges like ambiguity, conflicting needs, and managing changing requirements. There is a need to address these issues for successful system implementation.

Method: The paper explores how AI can enhance RE by automating tasks, improving prioritization, and facilitating collaboration. It also discusses opportunities, challenges, and ethical considerations of integrating AI into RE.

Result: AI can improve efficiency, accuracy, and management in RE, but brings new concerns such as ethical issues, biases, and lack of transparency. Enhanced collaboration and ethical practices are needed.

Conclusion: AI has the potential to transform RE processes if implemented with ethical safeguards and strong academia-industry collaboration, resulting in trustworthy and practical tools for software development.

Abstract: Requirement Engineering (RE) is the foundation of successful software
development. In RE, the goal is to ensure that implemented systems satisfy
stakeholder needs through rigorous requirements elicitation, validation, and
evaluation processes. Despite its critical role, RE continues to face
persistent challenges, such as ambiguity, conflicting stakeholder needs, and
the complexity of managing evolving requirements. A common view is that
Artificial Intelligence (AI) has the potential to streamline the RE process,
resulting in improved efficiency, accuracy, and management actions. However,
using AI also introduces new concerns, such as ethical issues, biases, and lack
of transparency. This paper explores how AI can enhance traditional RE
practices by automating labor-intensive tasks, supporting requirement
prioritization, and facilitating collaboration between stakeholders and AI
systems. The paper also describes the opportunities and challenges that AI
brings to RE. In particular, the vision calls for ethical practices in AI,
along with a much-enhanced collaboration between academia and industry
professionals. The focus should be on creating not only powerful but also
trustworthy and practical AI solutions ready to adapt to the fast-paced world
of software development.

</details>


### [28] [Smart Hiring Redefined: An Intelligent Recruitment Management Platform](https://arxiv.org/abs/2510.04437)
*Fangzhe Wu,Dongyang Lyu,Xiaoqi Li*

Main category: cs.SE

TL;DR: Traditional recruitment is insufficient for modern demands. Intelligent recruitment systems use automation and data to make hiring faster, more efficient, and accurate, becoming essential for organizational success.


<details>
  <summary>Details</summary>
Motivation: Traditional recruitment methods are inefficient, costly, and suffer from information asymmetry, making it difficult for enterprises to meet their talent acquisition needs.

Method: The paper discusses the implementation and advantages of intelligent recruitment management systems, which use automation and data-driven methods to improve recruitment efficiency.

Result: Intelligent recruitment systems vastly improve processes like resume screening, candidate matching, and interview scheduling, delivering higher efficiency and accuracy than manual methods.

Conclusion: Intelligent recruitment management systems are vital for modern organizations, optimizing recruitment processes and strengthening talent strategies amid digital and intelligent transformation.

Abstract: Against the backdrop of deepening digital and intelligent transformation in
human resource management, traditional recruitment models struggle to fully
meet enterprises' growing demand for precise talent acquisition due to limited
efficiency, high costs, and information asymmetry. As a vital tool for
optimizing recruitment processes, reducing labor and time costs, and enhancing
core competitiveness, intelligent recruitment management systems become an
indispensable component of modern organizational talent strategies.Compared
with the labor intensive tasks of resume screening, candidate position
matching, and interview coordination in traditional manual recruitment,
intelligent recruitment systems significantly enhance the efficiency and
accuracy of the hiring process through automation and data driven approaches.
These systems enable rapid parsing of massive resume volumes, intelligent
matching of candidates to positions, and automated scheduling of interview
processes.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [29] [PLSEMANTICSBENCH: Large Language Models As Programming Language Interpreters](https://arxiv.org/abs/2510.03415)
*Aditya Thimmaiah,Jiyang Zhang,Jayanth Srinivasa,Junyi Jessy Li,Milos Gligoric*

Main category: cs.PL

TL;DR: This paper investigates whether LLMs can execute code purely from formal semantics, using the IMP language and various complexity benchmarks. LLMs perform well on standard semantics tasks, struggle with nonstandard ones, and face difficulties with complex semantic reasoning, highlighting opportunities and current limitations for using LLMs as language interpreters.


<details>
  <summary>Details</summary>
Motivation: Large language models excel at code reasoning, raising the question of whether they can act as interpreters for programs based solely on formal programming language semantics, aiding rapid prototyping of new languages.

Method: The authors use the imperative language IMP, formalized via small-step operational semantics (SOS) and rewriting-based operational semantics (K-semantics). They create three evaluation sets of varying code complexity and assess model performance on tasks including final-state prediction, semantic rule prediction, and execution trace prediction. Nonstandard semantics are introduced via systematic rule mutations to separate memorization from true semantic competence.

Result: Strong LLMs show high performance on standard semantics but their performance drops on nonstandard semantics, indicating limited robust semantic understanding. Patterns in failures are identified, and providing formal semantics aids simple programs but hinders complex ones.

Conclusion: LLMs show promise as programming language interpreters with formal semantics, but currently lack robust, generalizable understanding of programming language semantics. The authors provide a benchmark suite for further research.

Abstract: As large language models (LLMs) excel at code reasoning, a natural question
arises: can an LLM execute programs (i.e., act as an interpreter) purely based
on a programming language's formal semantics? If so, it will enable rapid
prototyping of new programming languages and language features. We study this
question using the imperative language IMP (a subset of C), formalized via
small-step operational semantics (SOS) and rewriting-based operational
semantics (K-semantics). We introduce three evaluation sets-Human-Written,
LLM-Translated, and Fuzzer- Generated-whose difficulty is controlled by
code-complexity metrics spanning the size, control-flow, and data-flow axes.
Given a program and its semantics formalized with SOS/K-semantics, models are
evaluated on three tasks ranging from coarse to fine: (1) final-state
prediction, (2) semantic rule prediction, and (3) execution trace prediction.
To distinguish pretraining memorization from semantic competence, we define two
nonstandard semantics obtained through systematic mutations of the standard
rules. Across strong code/reasoning LLMs, performance drops under nonstandard
semantics despite high performance under the standard one. We further find that
(i) there are patterns to different model failures, (ii) most reasoning models
perform exceptionally well on coarse grained tasks involving reasoning about
highly complex programs often containing nested loop depths beyond five, and
surprisingly, (iii) providing formal semantics helps on simple programs but
often hurts on more complex ones. Overall, the results show a promise that LLMs
could serve as programming language interpreters, but points to the lack of
their robust semantics understanding. We release the benchmark and the
supporting code at https://github.com/EngineeringSoftware/PLSemanticsBench.

</details>


### [30] [Encoding Numeric Computations and Infusing Heuristic Knowledge Using Integrity Constraints in stableKanren](https://arxiv.org/abs/2510.04049)
*Xiangyu Guo,Ajay Bansal*

Main category: cs.PL

TL;DR: This paper shows that stableKanren supports efficient and simple numeric computations in logic programming through integrity constraints. By adjusting program/query design and adding heuristics, solver performance is greatly improved in combinatorial tasks like SEND+MORE=MONEY.


<details>
  <summary>Details</summary>
Motivation: Integrating numeric computation in logic programming languages such as miniKanren is challenging and often leads to complex representations or inefficient computations. There is a need for a logic programming system that supports both symbolic and numeric computation efficiently.

Method: The paper extends stableKanren (an extension of miniKanren supporting stable model semantics) to encode numeric computations using integrity constraints. It compares three approaches for supporting numeric computation—relational number representation, grounding numbers to symbols, and constraint store construction—and demonstrates their impact. The authors use combinatorial search problems (like SEND+MORE=MONEY) to show various ways to infuse heuristic knowledge and optimize solver performance. External hybrid solutions are also briefly discussed.

Result: stableKanren allows for a more straightforward numerical representation than standard relational representations and avoids full grounding of numbers as symbols, balancing symbolic and numeric computation. Its syntax for expressing constraints is simpler compared to other methods. Experiments show that infusing heuristic knowledge into declarative queries and programs significantly improves the performance of combinatorial problem solvers. Using external functions for hybrid solutions is also feasible.

Conclusion: stableKanren enhances relational programming by providing efficient, straightforward support for numeric computations with simple syntax, balancing symbolic and numeric approaches, and allowing performance improvements through heuristics infusion and hybridization.

Abstract: This paper presents examples of using integrity constraints in stableKanren
to encode numeric computations for problem solving. Then, we use one of the
examples to introduce multiple ways to infuse heuristic knowledge and reduce
solving time. stableKanren is an extension of miniKanren that supports normal
logic programs under stable model semantics. stableKanren further supports
numeric computation by constructing a constraint store for integrity
constraints. There are three ways to extend a relational programming language
with numeric computations: relational number representation, grounding numbers
to symbols, and constraint store construction. We demonstrate that the numeric
computations in stableKanren have a straightforward numerical representation
compared to relational number representations. More importantly, stableKanren
balances symbolic and numeric computation in relational programming by avoiding
the grounding of all numbers to symbols. Lastly, it also has simpler syntax
compared to other constraint store construction approaches. stableKanren
supports combinatorial search problem solving under a declarative generate and
test paradigm. Such a paradigm generates all possible combinations of solutions
to the problem, then applies a set of constraints to prune out the unwanted
solutions. We demonstrate that different approaches to writing programs or
queries affect the solver's performance in the SEND+MORE=MONEY puzzle. The
performance gradually improves as more heuristic knowledge is infused through
the programs or queries. Additionally, we show how to use an external function
to achieve a hybrid solution.

</details>


### [31] [Retrofitting Control Flow Graphs in LLVM IR for Auto Vectorization](https://arxiv.org/abs/2510.04890)
*Shihan Fang,Wenxin Zheng*

Main category: cs.PL

TL;DR: This paper presents a new vectorization framework with specialized IR extensions that boost SIMD utilization, making compiler vectorization more effective and delivering large performance gains over existing solutions like LLVM and GCC.


<details>
  <summary>Details</summary>
Motivation: Current compilers like LLVM and GCC are limited in their ability to fully leverage SIMD instruction sets for parallelism, due to fragmented vectorization passes and insufficient extensibility. Simplifying control flow analysis and precisely finding vectorization opportunities remain challenging, motivating the need for a better approach.

Method: The paper introduces a novel vectorization pipeline with two specialized intermediate representation (IR) extensions: SIR (for encoding high-level structural information) and VIR (for representing instruction dependencies via data dependency analysis). These extensions are used to create a flexible, extensible framework that integrates improved vectorization passes and enables accurate identification of vectorization opportunities.

Result: The proposed pipeline demonstrates significant performance improvements, achieving speedups of up to 53% and 58% over LLVM and GCC, respectively, according to experimental evaluations.

Conclusion: By extending compiler infrastructure with SIR and VIR IR formats, the proposed pipeline enhances analysis and prediction of vectorization opportunities, improves pass interoperability, and achieves considerable computational speedups. This work demonstrates the potential for significant impact on compiler design and high-performance software execution.

Abstract: Modern processors increasingly rely on SIMD instruction sets, such as AVX and
RVV, to significantly enhance parallelism and computational performance.
However, production-ready compilers like LLVM and GCC often fail to fully
exploit available vectorization opportunities due to disjoint vectorization
passes and limited extensibility. Although recent attempts in heuristics and
intermediate representation (IR) designs have attempted to address these
problems, efficiently simplifying control flow analysis and accurately
identifying vectorization opportunities remain challenging tasks.
  To address these issues, we introduce a novel vectorization pipeline
featuring two specialized IR extensions: SIR, which encodes high-level
structural information, and VIR, which explicitly represents instruction
dependencies through data dependency analysis. Leveraging the detailed
dependency information provided by VIR, we develop a flexible and extensible
vectorization framework. This approach substantially improves interoperability
across vectorization passes and expands the search space for identifying
isomorphic instructions, ultimately enhancing both the scope and efficiency of
automatic vectorization. Experimental evaluations demonstrate that our proposed
vectorization pipeline achieves significant performance improvements,
delivering speedups of up to 53% and 58% compared to LLVM and GCC,
respectively.

</details>


### [32] [concurrentKanren: miniKanren for parallel execution](https://arxiv.org/abs/2510.04994)
*Sjoerd Dost*

Main category: cs.PL

TL;DR: This paper demonstrates a parallel version of miniKanren in Go, showing it's feasible, can speed up execution via implicit parallelism, and paves the way for future work in language-independent parallel logic programming models.


<details>
  <summary>Details</summary>
Motivation: Concurrent logic programming is established, but concurrent miniKanren implementations are rare. The motivation is to explore parallel miniKanren to improve performance and utilize parallelism, especially for legacy programs.

Method: The authors present a parallel implementation of miniKanren using the Go programming language. They utilize implicit parallelism to run logic programs concurrently, and discuss various implementation strategies through evaluation.

Result: They demonstrate that parallel execution of miniKanren is feasible and shows potential for improved performance. The exploration sets the stage for more generic, language-independent models of parallelism in logic programming.

Conclusion: The study successfully presents and validates a parallel miniKanren in Go, highlighting its feasibility and performance benefits, and suggesting that this approach can inform more general models for parallel logic programming.

Abstract: Concurrent logic programming predates miniKanren, but concurrent
implementations of miniKanren have remained largely unexplored. In this work we
present a parallel implementation of miniKanren in Go, demonstrating its
feasibility and potential for performance improvements. Our approach leverages
implicit parallelism allowing legacy programs to benefit from parallel
execution. We discuss implementation strategies and evaluate the impact of
parallelism, laying groundwork for future language-agnostic models.

</details>
