<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 41]
- [cs.PL](#cs.PL) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Choosing the Right Git Workflow: A Comparative Analysis of Trunk-based vs. Branch-based Approaches](https://arxiv.org/abs/2507.08943)
*Pedro Lopes,Paola Accioly,Paulo Borba,Vitor Menezes*

Main category: cs.SE

TL;DR: This paper studies how Brazilian developers choose between branch-based and trunk-based Git workflows. Through interviews and a survey, it finds that trunk-based is best for small, experienced, fast-moving teams, while branch-based fits larger, less experienced groups despite more management overhead.


<details>
  <summary>Details</summary>
Motivation: Despite the widespread adoption of Git, there is limited scientific research on how different Git workflows (branch-based vs. trunk-based) affect software teams, especially regarding which contexts favor each workflow type.

Method: The authors conducted semi-structured interviews and a survey with Brazilian software developers to gather evidence on their workflow preferences and the factors influencing the adoption of each model.

Result: Trunk-based workflows are better suited for fast-paced projects with experienced, smaller teams. Branch-based workflows are more appropriate for larger teams with less experience, even though they introduce additional management complexity.

Conclusion: Choice of Git workflow strongly depends on team size, experience, and pace of development. There is no one-size-fits-all solution; workflow should be selected to match team characteristics and project needs.

Abstract: Git has become one of the most widely used version control systems today.
Among its distinguishing features, its ability to easily and quickly create
branches stands out, allowing teams to customize their workflows. In this
context, various formats of collaborative development workflows using Git have
emerged and gained popularity among software engineers. We can categorize such
workflows into two main types: branch-based workflows and trunk-based
workflows. Branch-based workflows typically define a set of remote branches
with well-defined objectives, such as feature branches, a branch for feature
integration, and a main branch. The goal is to migrate changes from the most
isolated branch to the main one shared by all as the code matures. In this
category, GitFlow stands out as the most popular example. In contrast,
trunk-based workflows have a single remote branch where developers integrate
their changes directly. In this range of options, choosing a workflow that
maximizes team productivity while promoting software quality becomes a
non-trivial task. Despite discussions on forums, social networks, and blogs,
few scientific articles have explored this topic. In this work, we provide
evidence on how Brazilian developers work with Git workflows and what factors
favor or hinder the use of each model. To this end, we conducted
semi-structured interviews and a survey with software developers. Our results
indicate that trunk-based development favors fast-paced projects with
experienced and smaller teams, while branch-based development suits less
experienced and larger teams better, despite posing management challenges.

</details>


### [2] [Semantic Source Code Segmentation using Small and Large Language Models](https://arxiv.org/abs/2507.08992)
*Abdelhalim Dahou,Ansgar Scherp,Sebastian Kurten,Brigitte Mathiak,Madhu Chauhan*

Main category: cs.SE

TL;DR: The paper presents automated R code segmentation methods using language models, showing that fine-tuned smaller models with context-based line-by-line analysis are most effective, outperforming larger models even without R-specific pre-training.


<details>
  <summary>Details</summary>
Motivation: Source code segmentation is essential for efficient knowledge retrieval and maintenance in large codebases, especially in low-resource programming languages like R, where existing manual and syntactic methods have become impractical.

Method: The paper introduces two novel automated methods for domain-specific R code segmentation using language models: (1) context-based line-by-line analysis and (2) range-based segment determination. Experiments are conducted with both large language models (LLMs) and smaller language models (SLMs), leveraging a newly constructed human-annotated dataset, StatCodeSeg. Experiments are also extended to Python code to test generalizability.

Result: Context-based line-by-line analysis outperforms range-based segmentation. Smaller models such as CodeBERT and an encoder-only version of CodeT5+ perform better than larger LLMs, despite being fine-tuned exclusively on a modest amount (4,130 lines) of manually annotated code and not being pre-trained on R code.

Conclusion: Automated code segmentation for R is feasible and effective using fine-tuned SLMs and context-based methods, with smaller models outperforming larger LLMs even without prior R-specific pre-training.

Abstract: Source code segmentation, dividing code into functionally coherent segments,
is crucial for knowledge retrieval and maintenance in software development.
While enabling efficient navigation and comprehension of large codebases,
manual and syntactic analysis approaches have become impractical as
repositories grow, especially for low-resource languages like R and their
research domains (e.g., social sciences, psychology).This paper introduces an
automated, domain-specific approach for research R code segmentation using
Large and Small Language Models (LLMs/SLMs). It presents two novel approaches
and a human-annotated dataset, StatCodeSeg. We explore two distinct approaches:
line-by-line analysis with context and range-based segment determination. We
experiment with LLMs and fine-tuned SLMs. To support the generalizability of
our approaches, we also include experiments on Python code from the computer
science domain.Our results show that context-based line-by-line analysis is
superior over range-based segmentation.Using smaller language models like
CodeBERT and an encoder-only version of CodeT5+ are better than their LLM
counterparts. Most notably, these two best-performing models did not see R code
during pre-training versus the LLMs but were only fine-tuned on 4,130 lines of
manually annotated code.

</details>


### [3] [Accelerating Drug Discovery Through Agentic AI: A Multi-Agent Approach to Laboratory Automation in the DMTA Cycle](https://arxiv.org/abs/2507.09023)
*Yao Fehlis,Charles Crain,Aidan Jensen,Michael Watson,James Juhasz,Paul Mandel,Betty Liu,Shawn Mahon,Daren Wilson,Nick Lynch-Jonely,Ben Leedom,David Fuller*

Main category: cs.SE

TL;DR: Tippy introduces AI agent automation to drug discovery, dramatically improving workflow speed and efficiency by using a multi-agent system throughout the DMTA cycle.


<details>
  <summary>Details</summary>
Motivation: Traditional drug discovery approaches are increasingly unable to meet the accelerating demands of modern therapeutic development, creating an urgent need for more efficient, automated solutions.

Method: The paper proposes Tippy, a novel multi-agent AI framework comprised of five specialized agents (Supervisor, Molecule, Lab, Analysis, Report) with an additional Safety Guardrail agent. These agents are designed to autonomously handle different stages of the Design-Make-Test-Analyze (DMTA) cycle in drug discovery, facilitating reasoning, planning, and collaboration.

Result: The implementation of Tippy leads to notable improvements in workflow efficiency, faster decision-making, and better coordination across disciplines within the DMTA cycle.

Conclusion: Tippy demonstrates that specialized AI agents can significantly accelerate and streamline the laboratory automation process in pharmaceutical research, setting a new standard for AI-driven drug discovery.

Abstract: The pharmaceutical industry faces unprecedented challenges in drug discovery,
with traditional approaches struggling to meet modern therapeutic development
demands. This paper introduces a novel AI framework, Tippy, that transforms
laboratory automation through specialized AI agents operating within the
Design-Make-Test-Analyze (DMTA) cycle. Our multi-agent system employs five
specialized agents - Supervisor, Molecule, Lab, Analysis, and Report, with
Safety Guardrail oversight - each designed to excel in specific phases of the
drug discovery pipeline. Tippy represents the first production-ready
implementation of specialized AI agents for automating the DMTA cycle,
providing a concrete example of how AI can transform laboratory workflows. By
leveraging autonomous AI agents that reason, plan, and collaborate, we
demonstrate how Tippy accelerates DMTA cycles while maintaining scientific
rigor essential for pharmaceutical research. The system shows significant
improvements in workflow efficiency, decision-making speed, and
cross-disciplinary coordination, offering a new paradigm for AI-assisted drug
discovery.

</details>


### [4] [Towards Extracting Software Requirements from App Reviews using Seq2seq Framework](https://arxiv.org/abs/2507.09039)
*Aakash Sorathiya,Gouri Ginde*

Main category: cs.SE

TL;DR: The paper introduces a deep learning-based sequence approach for extracting requirements from noisy app reviews, outperforming previous methods and helping developers better understand user needs.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to effectively extract software requirements from mobile app reviews due to their informal language, errors, and irrelevant content, making it difficult for developers to understand user needs and improve software.

Method: The paper reformulates requirements extraction from app reviews as a Named Entity Recognition (NER) task using a sequence-to-sequence (Seq2seq) framework. The proposed model consists of a BiLSTM encoder, an LSTM decoder, self-attention mechanism, GloVe embeddings, and a CRF model. This framework was evaluated on two annotated datasets of app reviews.

Result: The proposed Seq2seq NER framework outperformed current state-of-the-art methods, with an F1 score of 0.96 on a large dataset of 23,816 app reviews, and achieved comparable performance (F1 score of 0.47) on a smaller, manually annotated dataset of 1,000 reviews.

Conclusion: Reformulating requirements extraction as an NER task and utilizing an enhanced Seq2seq framework significantly improves the extraction of meaningful requirements from noisy and informal mobile app reviews.

Abstract: Mobile app reviews are a large-scale data source for software improvements. A
key task in this context is effectively extracting requirements from app
reviews to analyze the users' needs and support the software's evolution.
Recent studies show that existing methods fail at this task since app reviews
usually contain informal language, grammatical and spelling errors, and a large
amount of irrelevant information that might not have direct practical value for
developers. To address this, we propose a novel reformulation of requirements
extraction as a Named Entity Recognition (NER) task based on the
sequence-to-sequence (Seq2seq) generation approach. With this aim, we propose a
Seq2seq framework, incorporating a BiLSTM encoder and an LSTM decoder, enhanced
with a self-attention mechanism, GloVe embeddings, and a CRF model. We
evaluated our framework on two datasets: a manually annotated set of 1,000
reviews (Dataset 1) and a crowdsourced set of 23,816 reviews (Dataset 2). The
quantitative evaluation of our framework showed that it outperformed existing
state-of-the-art methods with an F1 score of 0.96 on Dataset 2, and achieved
comparable performance on Dataset 1 with an F1 score of 0.47.

</details>


### [5] [CMER: A Context-Aware Approach for Mining Ethical Concern-related App Reviews](https://arxiv.org/abs/2507.09049)
*Aakash Sorathiya,Gouri Ginde*

Main category: cs.SE

TL;DR: The paper proposes CMER, a novel framework that combines natural language inference and a large language model to efficiently extract privacy and security concerns from mobile app reviews, outperforming keyword-based methods and enabling better handling of ethical issues in app development.


<details>
  <summary>Details</summary>
Motivation: As mobile apps become more widespread, users increasingly raise ethical concerns such as privacy and security in their app reviews. However, identifying these concerns is difficult due to domain-specific language and the prevalence of more generic feedback, making automated extraction laborious.

Method: The authors introduce CMER, a context-aware method leveraging Natural Language Inference (NLI) with domain-specific hypotheses and a decoder-only Large Language Model (LLM, LLaMA-like) to identify ethical concern-related app reviews without requiring labeled data. The approach is evaluated on a large dataset of investment app reviews, comparing different NLI and LLM models, then combining the best-performing ones.

Result: CMER successfully extracted 2,178 additional privacy and security-related reviews that a traditional keyword-based approach missed, illustrating its effectiveness in surfacing overlooked ethical concerns.

Conclusion: CMER provides an improved way to mine ethical concern-related app reviews, outperforming existing keyword-based methods and offering valuable input for further software engineering processes.

Abstract: With the increasing proliferation of mobile applications in our daily lives,
the concerns surrounding ethics have surged significantly. Users communicate
their feedback in app reviews, frequently emphasizing ethical concerns, such as
privacy and security. Incorporating these reviews has proved to be useful for
many areas of software engineering (e.g., requirement engineering, testing,
etc.). However, app reviews related to ethical concerns generally use
domain-specific language and are typically overshadowed by more generic
categories of user feedback, such as app reliability and usability. Thus,
making automated extraction a challenging and time-consuming effort.
  This study proposes CMER (A \underline{C}ontext-Aware Approach for
\underline{M}ining \underline{E}thical Concern-related App
\underline{R}eviews), a novel approach that combines Natural Language Inference
(NLI) and a decoder-only (LLaMA-like) Large Language Model (LLM) to extract
ethical concern-related app reviews at scale. In CMER, NLI provides
domain-specific context awareness by using domain-specific hypotheses, and the
Llama-like LLM eliminates the need for labeled data in the classification task.
We evaluated the validity of CMER by mining privacy and security-related
reviews (PSRs) from the dataset of more than 382K app reviews of mobile
investment apps. First, we evaluated four NLI models and compared the results
of domain-specific hypotheses with generic hypotheses. Next, we evaluated three
LLMs for the classification task. Finally, we combined the best NLI and LLM
models (CMER) and extracted 2,178 additional PSRs overlooked by the previous
study using a keyword-based approach, thus demonstrating the effectiveness of
CMER. These reviews can be further refined into actionable requirement
artifacts.

</details>


### [6] [SAGE: A Context-Aware Approach for Mining Privacy Requirements Relevant Reviews from Mental Health Apps](https://arxiv.org/abs/2507.09051)
*Aakash Sorathiya,Gouri Ginde*

Main category: cs.SE

TL;DR: SAGE, leveraging domain-aware NLI and a GPT model, effectively identifies privacy-relevant reviews in mental health apps without fine-tuning, outperforming other classifiers and uncovering previously missed user privacy concerns.


<details>
  <summary>Details</summary>
Motivation: Mental health apps require sensitive personal data, leading to privacy concerns among users. However, privacy-related user feedback is often overshadowed by issues like reliability and usability, making it difficult to automatically identify reviews relevant to privacy requirements.

Method: The study introduces SAGE, an automated, context-aware approach that uses Natural Language Inference (NLI) with mental health domain-specific privacy hypotheses and a GPT model to mine privacy-relevant reviews from app feedback, without requiring model fine-tuning.

Result: SAGE achieved an F1 score of 0.85 on a dataset of 204K app reviews, surpassing fine-tuned classifiers like BERT and T5. It also identified 748 privacy-related reviews that previous keyword-based methods missed, highlighting its improved effectiveness.

Conclusion: SAGE is a superior, fine-tuning-free method for mining privacy requirements from user reviews in mental health apps, allowing for more accurate extraction of actionable privacy requirement artifacts.

Abstract: Mental health (MH) apps often require sensitive user data to customize
services for mental wellness needs. However, such data collection practices in
some MH apps raise significant privacy concerns for users. These concerns are
often mentioned in app reviews, but other feedback categories, such as
reliability and usability, tend to take precedence. This poses a significant
challenge in automatically identifying privacy requirements-relevant reviews
(privacy reviews) that can be utilized to extract privacy requirements and
address users' privacy concerns. Thus, this study introduces SAGE, a
context-aware approach to automatically mining privacy reviews from MH apps
using Natural Language Inference (NLI) with MH domain-specific privacy
hypotheses (provides domain-specific context awareness) and a GPT model
(eliminates the need for fine-tuning). The quantitative evaluation of SAGE on a
dataset of 204K app reviews achieved an F1 score of 0.85 without any
fine-tuning, outperforming the fine-tuned baseline classifiers BERT and T5.
Furthermore, SAGE extracted 748 privacy reviews previously overlooked by
keyword-based methods, demonstrating its effectiveness through qualitative
evaluation. These reviews can later be refined into actionable privacy
requirement artifacts.

</details>


### [7] [SetupBench: Assessing Software Engineering Agents' Ability to Bootstrap Development Environments](https://arxiv.org/abs/2507.09063)
*Avi Arora,Jinu Jang,Roshanak Zilouchian Moghaddam*

Main category: cs.SE

TL;DR: SetupBench is a new benchmark evaluating LLM agents' abilities to set up real software environments from scratch. Current state-of-the-art agents perform poorly, struggling with tasks like package installation and database setup, and acting inefficiently. The benchmark highlights major gaps in LLM agents’ real-world software task capabilities.


<details>
  <summary>Details</summary>
Motivation: Large Language Model (LLM) agents promise to assist with end-to-end real-world software tasks. However, current benchmarks only test them in controlled environments with all dependencies pre-installed, failing to reflect real-world scenarios where software setup and configuration are required. There is a need for benchmarks that assess agents' ability to bootstrap environments from scratch.

Method: The authors introduce SetupBench, a benchmark with 93 instances requiring agents to bootstrap environments on bare Linux sandboxes. Tasks involve installing packages, resolving dependency conflicts, setting up databases, and configuring services across multiple programming languages and database engines. Each task has a natural language description and a success criterion. The benchmark is used to evaluate OpenHands, a leading coding agent.

Result: OpenHands exhibited low success rates, particularly struggling with repository setup (38.9-57.4%) and local database configuration (20.0-53.3%). Common failure modes included incomplete installation of development tools, hallucinated task constraints, and changes that do not persist, which disrupt human-agent workflows. Furthermore, agents took 38-89% more actions than optimal human solutions, indicating inefficient exploration strategies.

Conclusion: Current LLM agents lack robust environment-bootstrap capabilities, a critical skill for real-world software development tasks. SetupBench exposes these weaknesses and provides a challenging, realistic benchmark for improving agent performance in practical, end-to-end development scenarios.

Abstract: Modern Large Language Model (LLM) agents promise end to end assistance with
real-world software tasks, yet existing benchmarks evaluate LLM agents almost
exclusively in pre-baked environments where every dependency is pre-installed.
To fill this gap, we introduce SetupBench, a 93 instance benchmark that
isolates the environment-bootstrap skill: starting from a bare Linux sandbox,
an agent must install packages, resolve dependency conflicts, initialize
databases, and configure background services. Our tasks span seven language
ecosystems, five database engines, and multi-service orchestration scenarios,
each accompanies by a natural language problem statement and a deterministic
success command. Through evaluation of OpenHands, a state-of-the-art coding
agent, we find low success rates across task categories, with particular
challenges in repository setup (38.9-57.4%) and local database configuration
(20.0-53.3%). Our analysis reveals systematic failure modes including
incomplete development tooling installation, hallucinated task constraints, and
non-persistent environment modifications that break agent-human collaboration
workflows. We identify substantial inefficiencies in agent exploration
strategies, with 38-89% of actions being unnecessary compared to optimal human
behavior. These findings highlight gaps in current agents' practical
environment-bootstrap capabilities. By targeting this critical yet
under-evaluated capability, SetupBench provides a rigorous yard-stick for the
next generation of software developer agents aiming to solve end to end
real-wold tasks.

</details>


### [8] [SPICE: An Automated SWE-Bench Labeling Pipeline for Issue Clarity, Test Coverage, and Effort Estimation](https://arxiv.org/abs/2507.09108)
*Aaditya Bhatia,Gustavo A. Oliva,Gopi Krishnan Rajbahadur,Haoxiang Zhang,Yihao Chen,Zhilong Chen,Arthur Leung,Dayi Lin,Boyuan Chen,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: SPICE is an automated pipeline that enables accurate, low-cost labeling of software engineering datasets, reducing costs over 19,000x while maintaining expert-level quality; it facilitates large-scale dataset creation for research and practice.


<details>
  <summary>Details</summary>
Motivation: High-quality labeled datasets are essential for training and evaluating foundation models in software engineering, but manual annotation is extremely costly and labor-intensive.

Method: The paper introduces SPICE, an automated and scalable pipeline that labels SWE-bench-style datasets for issue clarity, test coverage, and effort estimation using context-aware code navigation, rationale-driven prompting, and multi-pass consensus methods.

Result: SPICE achieves strong agreement with expert annotations, decreases the cost of labeling 1,000 instances from around $100,000 to $5.10, and enables large-scale dataset generation. The authors have also released the SPICE tool and a large labeled dataset (SPICE Bench) for the community.

Conclusion: SPICE provides an effective, scalable, and low-cost solution for creating high-quality labeled datasets in software engineering, making broad dataset creation practical and enabling advances in SE-focused foundation models.

Abstract: High-quality labeled datasets are crucial for training and evaluating
foundation models in software engineering, but creating them is often
prohibitively expensive and labor-intensive. We introduce SPICE, a scalable,
automated pipeline for labeling SWE-bench-style datasets with annotations for
issue clarity, test coverage, and effort estimation. SPICE combines
context-aware code navigation, rationale-driven prompting, and multi-pass
consensus to produce labels that closely approximate expert annotations.
SPICE's design was informed by our own experience and frustration in labeling
more than 800 instances from SWE-Gym. SPICE achieves strong agreement with
human-labeled SWE-bench Verified data while reducing the cost of labeling 1,000
instances from around $100,000 (manual annotation) to just $5.10. These results
demonstrate SPICE's potential to enable cost-effective, large-scale dataset
creation for SE-focused FMs. To support the community, we release both SPICE
tool and SPICE Bench, a new dataset of 6,802 SPICE-labeled instances curated
from 291 open-source projects in SWE-Gym (over 13x larger than SWE-bench
Verified).

</details>


### [9] [Position Paper: Programming Language Techniques for Bridging LLM Code Generation Semantic Gaps](https://arxiv.org/abs/2507.09135)
*Yalong Du,Chaozheng Wang,Huaijin Wang*

Main category: cs.SE

TL;DR: Current LLMs for code generation have reliability issues due to their statistical nature. This paper argues that integrating Programming Language (PL) techniques—like structure, verification, and correctness guarantees—can significantly improve the trustworthiness and reliability of AI-generated code.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) excel at generating code, but due to their black-box and statistical nature, their output may suffer from syntax errors, unreliable behavior, and lack of trustworthiness. This creates a need to address the semantic reliability of code produced by LLMs.

Method: The paper advocates for integrating Programming Language (PL) techniques with LLMs. This includes leveraging structured program representations, providing formal correctness guarantees, and implementing robust verification mechanisms to improve the reliability of generated code.

Result: The integration of PL techniques with LLMs can elevate code generation from simple statistical pattern matching to producing code that is more reliable, interpretable, and verifiable. This approach would address the shortcomings of current LLM-based code generation models, enhancing trustworthiness and practical utility.

Conclusion: To produce code that is not only functionally correct but also interpretable, verifiable, and trustworthy, it is essential to combine LLMs with principled PL techniques. This integration is fundamental in moving beyond the current limitations of black-box generative models in code generation.

Abstract: Large Language Models have demonstrated remarkable capabilities in automated
code generation, yet their statistical nature and black-box characteristics
create significant semantic gaps manifested through syntax errors, semantic
hallucinations, and reliability concerns. This position paper argues that
principled integration of Programming Language (PL) techniques is essential for
bridging these gaps. Through structured program representations, formal
correctness guarantees, and robust verification mechanisms, PL techniques can
elevate LLM-generated code from statistical pattern matching to truly reliable
and trustworthy levels. This integration is crucial for developing systems that
generate code that is not only functionally correct but also interpretable,
verifiable, and ultimately trustworthy.

</details>


### [10] [OpenCAMS: An Open-Source Connected and Automated Mobility Co-Simulation Platform for Advanced Transportation Research](https://arxiv.org/abs/2507.09186)
*Minhaj Uddin Ahmad,Akid Abrar,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.SE

TL;DR: OpenCAMS is a flexible open-source co-simulation platform that unites SUMO, CARLA, and OMNeT++ for synchronized study of traffic, vehicle perception, and communication, offering a comprehensive, expandable tool for intelligent transportation system research.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of intelligent transportation systems (ITS) requires advanced simulation platforms that can accurately model interactions between traffic, perception, and communication domains for research in safety, mobility, and cybersecurity.

Method: OpenCAMS employs a tightly coupled, time-synchronized, bidirectional co-simulation framework that integrates three established simulators: SUMO for traffic modeling, CARLA for high-fidelity vehicle perception and control, and OMNeT++ for event-driven network communication. The architecture maintains modularity and allows future expansions without significant redesign.

Result: OpenCAMS successfully synchronizes and combines SUMO, CARLA, and OMNeT++ to offer a comprehensive environment for ITS research. It supports advanced simulations such as detailed sensor emulation, network-wide traffic management, and C-V2X communication, while remaining fully open-source and extensible for future needs.

Conclusion: OpenCAMS provides the research community with an accessible, modular, and synchronized open-source platform to advance intelligent transportation systems, enabling collaborative, future-proof research in transportation safety, mobility, and cybersecurity.

Abstract: We introduce OpenCAMS (Open-Source Connected and Automated Mobility
Co-Simulation Platform), an open-source, synchronized, and extensible
co-simulation framework that tightly couples three best-in-class simulation
tools: (i) SUMO, (ii) CARLA, and (iii) OMNeT++. OpenCAMS is designed to support
advanced research in transportation safety, mobility, and cybersecurity by
combining the strengths of each simulation domain. Specifically, SUMO provides
large-scale, microscopic traffic modeling; CARLA offers high-fidelity 3D
perception, vehicle dynamics, and control simulation; and OMNeT++ enables
modular, event-driven network communication, such as cellular
vehicle-to-everything (C-V2X). OpenCAMS employs a time-synchronized,
bidirectional coupling architecture that ensures coherent simulation
progression across traffic, perception, and communication domains while
preserving modularity and reproducibility. For example, CARLA can simulate and
render a subset of vehicles that require detailed sensor emulation and control
logic; SUMO orchestrates network-wide traffic flow, vehicle routing, and
traffic signal management; and OMNeT++ dynamically maps communication nodes to
both mobile entities (e.g., vehicles) and static entities (e.g., roadside
units) to enable C-V2X communication. While these three simulators form the
foundational core of OpenCAMS, the platform is designed to be expandable and
future-proof, allowing additional simulators to be integrated on top of this
core without requiring fundamental changes to the system architecture. The
OpenCAMS platform is fully open-source and publicly available through its
GitHub repository https://github.com/minhaj6/carla-sumo-omnetpp-cosim,
providing the research community with an accessible, flexible, and
collaborative environment for advancing next-generation intelligent
transportation systems.

</details>


### [11] [Back to the Basics: Rethinking Issue-Commit Linking with LLM-Assisted Retrieval](https://arxiv.org/abs/2507.09199)
*Huihui Huang,Ratnadira Widyasari,Ting Zhang,Ivana Clairine Irsan,Jieke Shi,Han Wei Ang,Frank Liauw,Eng Lieh Ouh,Lwin Khin Shar,Hong Jin Kang,David Lo*

Main category: cs.SE

TL;DR: This paper shows that current issue-commit linking tools perform poorly in realistic settings with many unrelated commits. The authors propose a new evaluation benchmark and introduce EasyLink, a tool using a vector database and a language model, which greatly outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of issue-commit linking tools do not consider the more complex, realistic scenarios where a repository contains many unrelated plausible commits, making it harder for tools to identify the correct links. There is a need for more realistic benchmarks and improved techniques for this task.

Method: The authors introduce the Realistic Distribution Setting (RDS) to create a more challenging evaluation dataset from 20 open-source projects. They evaluate current tools, propose EasyLink—an approach that uses a vector database for information retrieval and a large language model for semantic reranking—and compare its performance to existing methods.

Result: When tools are evaluated under the RDS, deep learning-based approaches perform much worse than previously reported, with their performance dropping by more than half, and a traditional IR method (VSM) outperforming deep learning models. EasyLink, combining modern IR with a language model for reranking, significantly outperforms both, achieving a Precision@1 of 75.91%, over four times the current state-of-the-art.

Conclusion: Realistic evaluation settings reveal substantial weaknesses in current state-of-the-art deep learning models for issue-commit linking. The EasyLink approach, which leverages information retrieval and language model reranking, offers a robust and effective solution. The paper also presents guidelines to drive further research in this field.

Abstract: Issue-commit linking, which connects issues with commits that fix them, is
crucial for software maintenance. Existing approaches have shown promise in
automatically recovering these links. Evaluations of these techniques assess
their ability to identify genuine links from plausible but false links.
However, these evaluations overlook the fact that, in reality, when a
repository has more commits, the presence of more plausible yet unrelated
commits may interfere with the tool in differentiating the correct fix commits.
To address this, we propose the Realistic Distribution Setting (RDS) and use it
to construct a more realistic evaluation dataset that includes 20 open-source
projects. By evaluating tools on this dataset, we observe that the performance
of the state-of-the-art deep learning-based approach drops by more than half,
while the traditional Information Retrieval method, VSM, outperforms it.
  Inspired by these observations, we propose EasyLink, which utilizes a vector
database as a modern Information Retrieval technique. To address the
long-standing problem of the semantic gap between issues and commits, EasyLink
leverages a large language model to rerank the commits retrieved from the
database. Under our evaluation, EasyLink achieves an average Precision@1 of
75.91%, improving over the state-of-the-art by over four times. Additionally,
this paper provides practical guidelines for advancing research in issue-commit
link recovery.

</details>


### [12] [Explainability as a Compliance Requirement: What Regulated Industries Need from AI Tools for Design Artifact Generation](https://arxiv.org/abs/2507.09220)
*Syed Tauhid Ullah Shah,Mohammad Hussein,Ann Barcomb,Mohammad Moshirpour*

Main category: cs.SE

TL;DR: AI tools for generating design artifacts in requirements engineering struggle with transparency and explainability, limiting their use in regulated industries. Interviews with practitioners reveal significant challenges and the need for specific improvements to enhance trust, collaboration, and compliance. The paper provides a roadmap to make these tools more practical and adoptable in safety-critical domains.


<details>
  <summary>Details</summary>
Motivation: AI tools are increasingly used to automate the generation of design artifacts from textual requirements in Requirements Engineering (RE). While promising increased efficiency, such tools see limited adoption in regulated industries due to the need for transparency and traceability.

Method: The study uses semi-structured interviews with ten practitioners from safety-critical industries to investigate the explainability gap in AI-driven design artifact generation.

Result: The lack of explainability in current AI tools causes extensive manual validation, reduced stakeholder trust, difficulties with domain-specific terminology, disrupted team collaboration, and poses regulatory compliance risks, undermining their efficiency benefits.

Conclusion: The paper identifies critical improvements needed for AI tools in RE: source tracing, transparent decision-making justifications, support for domain-specific adaptation, and compliance validation features. These improvements are essential for increasing the adoption and reliability of AI tools in safety-critical and regulated environments.

Abstract: Artificial Intelligence (AI) tools for automating design artifact generation
are increasingly used in Requirements Engineering (RE) to transform textual
requirements into structured diagrams and models. While these AI tools,
particularly those based on Natural Language Processing (NLP), promise to
improve efficiency, their adoption remains limited in regulated industries
where transparency and traceability are essential. In this paper, we
investigate the explainability gap in AI-driven design artifact generation
through semi-structured interviews with ten practitioners from safety-critical
industries. We examine how current AI-based tools are integrated into workflows
and the challenges arising from their lack of explainability. We also explore
mitigation strategies, their impact on project outcomes, and features needed to
improve usability. Our findings reveal that non-explainable AI outputs
necessitate extensive manual validation, reduce stakeholder trust, struggle to
handle domain-specific terminology, disrupt team collaboration, and introduce
regulatory compliance risks, often negating the anticipated efficiency
benefits. To address these issues, we identify key improvements, including
source tracing, providing clear justifications for tool-generated decisions,
supporting domain-specific adaptation, and enabling compliance validation. This
study outlines a practical roadmap for improving the transparency, reliability,
and applicability of AI tools in requirements engineering workflows,
particularly in regulated and safety-critical environments where explainability
is crucial for adoption and certification.

</details>


### [13] [Enhancing Interpretability in Software Change Management with Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.09315)
*Yongqian Sun,Weihua Kuang,Chao Shen,Xidao Wen,Tinghua Zheng,Heng Liu,Shenglin Zhang,Bo Wu,Dan Pei*

Main category: cs.SE

TL;DR: SCELM is an automated framework designed to efficiently manage software changes in online services, leading to fewer failures and lower costs.


<details>
  <summary>Details</summary>
Motivation: Frequent software changes in modern online services introduce substantial risks. Traditional change management methods struggle to efficiently and accurately handle these continuous updates, leading to increased service failures and economic losses.

Method: The proposed approach is SCELM, an end-to-end automated framework specifically designed for software change evaluation and lifecycle management. It automates and streamlines the management of software changes.

Result: Applying SCELM results in more efficient and precise handling of software changes. This leads to a significant reduction in both service failures and economic losses.

Conclusion: SCELM effectively addresses the risks associated with frequent software changes in online services, improving reliability and reducing negative impacts.

Abstract: In modern online services, frequent software changes introduce significant
risks. To tackle this challenge, we propose SCELM (Software Change Evaluation
and Lifecycle Management), an end-to-end automated framework for software
change management. SCELM aims to manage software changes efficiently and
precisely, significantly reducing service failures and economic losses.

</details>


### [14] [Enhancing NeuroEvolution-Based Game Testing: A Branch Coverage Approach for Scratch Programs](https://arxiv.org/abs/2507.09414)
*Khizra Sohail,Atif Aftab Ahmed Jilani,Nigar Azhar Butt*

Main category: cs.SE

TL;DR: This paper improves automated testing for Scratch games by replacing statement coverage with branch coverage in the NEATEST framework. The new approach finds more faults and covers more logical branches, especially in complex games.


<details>
  <summary>Details</summary>
Motivation: Automated test generation for game-like programs is difficult due to non-determinism and complex control logic. Traditional statement coverage is insufficient for detecting all faults, as it may miss certain logical branches.

Method: The authors extend the NEATEST framework—originally used for automated Scratch game testing—by integrating a branch coverage-based fitness function. The method prioritizes control-dependent branches to improve neuroevolution-driven test exploration. Comparative experiments on 25 Scratch games, as well as mutation analysis, assess the new approach (NBC) against the previous statement coverage method (NSC).

Result: NBC (the new branch coverage approach) achieves higher branch coverage in 13 out of 25 games, especially in those with complex conditional logic, and produces a lower false positive rate in mutation testing than NSC.

Conclusion: Branch coverage-based automated test generation improves the effectiveness and reliability of testing in Scratch game programs, out-performing statement coverage by covering more logical branches and reducing false positives in mutation testing.

Abstract: Automated test generation for game-like programs presents unique challenges
due to their non-deterministic behavior and complex control structures. The
NEATEST framework has been used for automated testing in Scratch games,
employing neuroevolution-based test generation optimized for statement
coverage. However, statement coverage alone is often insufficient for fault
detection, as it does not guarantee execution of all logical branches. This
paper introduces a branch coverage-based fitness function to enhance test
effectiveness in automated game testing. We extend NEATEST by integrating a
branch fitness function that prioritizes control-dependent branches, guiding
the neuroevolution process to maximize branch exploration. To evaluate the
effectiveness of this approach, empirical experiments were conducted on 25
Scratch games, comparing Neatest with Statement Coverage (NSC) against Neatest
with Branch Coverage (NBC). A mutation analysis was also performed to assess
the fault detection capabilities of both techniques. The results demonstrate
that NBC achieves higher branch coverage than NSC in 13 out of 25 games,
particularly in programs with complex conditional structures. Moreover, NBC
achieves a lower false positive rate in mutation testing, making it a more
reliable approach for identifying faulty behavior in game programs. These
findings confirm that branch coverage-based test generation improves test
coverage and fault detection in Scratch programs.

</details>


### [15] [Evaluating LLMs on Sequential API Call Through Automated Test Generation](https://arxiv.org/abs/2507.09481)
*Yuheng Huang,Da Song,Zhenlan Ji,Shuai Wang,Lei Ma*

Main category: cs.SE

TL;DR: The paper introduces StateGen, an automated system that creates and evaluates challenging coding tasks centered on sequential API interactions. This approach addresses limitations in current benchmarks and enables more thorough testing of LLMs with external tool use.


<details>
  <summary>Details</summary>
Motivation: Although LLMs can integrate with external APIs to perform complex real-world tasks, the evaluation of their API tool use is still preliminary. Current benchmarks rely heavily on manual test cases and simplistic evaluation methods, often ignoring the nuanced interactions between sequential API calls common in actual applications.

Method: The paper introduces StateGen, an automated framework that generates diverse coding tasks involving sequential API interactions. StateGen leverages state-machine-based API constraint solving, validation, energy-based sampling, and control-flow injection to create executable programs, which are then turned into natural language task descriptions by collaborating LLM agents.

Result: Using StateGen, the authors build StateEval, a benchmark containing 120 verified test cases across scenarios like Session Service, Tensor Operation, and ElevenLabs MCP. Experiments show that StateGen can produce realistic, challenging API-oriented tasks, and reveal improvement opportunities for LLMs using APIs.

Conclusion: StateGen provides a robust and automated method to generate and evaluate API-related tasks for LLMs, filling a significant gap in current benchmark coverage by focusing on complex, sequential API interactions and automatic semantic evaluation.

Abstract: By integrating tools from external APIs, Large Language Models (LLMs) have
expanded their promising capabilities in a diverse spectrum of complex
real-world tasks. However, testing, evaluation, and analysis of LLM tool use
remain in their early stages. Most existing benchmarks rely on manually
collected test cases, many of which cannot be automatically checked for
semantic correctness and instead depend on static methods such as string
matching. Additionally, these benchmarks often overlook the complex
interactions that occur between sequential API calls, which are common in
real-world applications. To fill the gap, in this paper, we introduce StateGen,
an automated framework designed to generate diverse coding tasks involving
sequential API interactions. StateGen combines state-machine-based API
constraint solving and validation, energy-based sampling, and control-flow
injection to generate executable programs. These programs are then translated
into human-like natural language task descriptions through a collaboration of
two LLM agents. Utilizing StateGen, we construct StateEval, a benchmark
encompassing 120 verified test cases spanning across three representative
scenarios: Session Service, Tensor Operation, and ElevenLabs MCP. Experimental
results confirm that StateGen can effectively generate challenging and
realistic API-oriented tasks, highlighting areas for improvement in current
LLMs incorporating APIs.

</details>


### [16] [Towards LLM-Based Automatic Playtest](https://arxiv.org/abs/2507.09490)
*Yan Zhao,Chiwei Tang*

Main category: cs.SE

TL;DR: The paper presents Lap, a method that uses ChatGPT for automatic playtesting of match-3 games without APIs. By converting game boards to numeric matrices and prompting the LLM, Lap surpasses existing tools in code coverage and crash detection, showing that LLMs can effectively automate game testing processes even for non-text games.


<details>
  <summary>Details</summary>
Motivation: Manual playtesting of video games is resource-intensive and current automated tools lack the necessary domain knowledge and problem-solving skills. There is a need to enhance automatic playtesting, particularly for non-text-based games that lack APIs or direct textual representations of game states.

Method: The paper introduces 'Lap', an approach that uses Large Language Models (specifically ChatGPT) for automatic playtesting of match-3 games. Lap processes game environments by converting game board snapshots into numeric matrices, uses LLM prompting for move generation, and iteratively performs actions within the game.

Result: Lap was evaluated on the open-source match-3 game CasseBonbons. It was compared with three existing tools, showing that Lap achieved higher code coverage and triggered more program crashes, indicating more thorough and effective testing.

Conclusion: Lap demonstrates the feasibility and effectiveness of employing LLMs (ChatGPT) for automated playtesting in non-text-based games without requiring robust APIs. This approach can enhance quality assurance in games and highlights new opportunities for LLM applications in software testing.

Abstract: Playtesting is the process in which people play a video game for testing. It
is critical for the quality assurance of gaming software. Manual playtesting is
time-consuming and expensive. However, automating this process is challenging,
as playtesting typically requires domain knowledge and problem-solving skills
that most conventional testing tools lack. Recent advancements in artificial
intelligence (AI) have opened up new possibilities for applying Large Language
Models (LLMs) to playtesting. However, significant challenges remain: current
LLMs cannot visually perceive game environments, and most existing research
focuses on text-based games or games with robust APIs. Many non-text games lack
APIs to provide textual descriptions of game states, making it almost
impossible to naively apply LLMs for playtesting. This paper introduces Lap,
our novel approach to LLM-based Automatic Playtesting, which uses ChatGPT to
test match-3 games, a category of games where players match three or more
identical tiles in a row or column to earn points. Lap encompasses three key
phases: processing of game environments, prompting-based action generation, and
action execution. Given a match-3 game, Lap takes a snapshot of the game board
and converts it to a numeric matrix. It then prompts the ChatGPT-O1-mini API to
suggest moves based on that matrix and tentatively applies the suggested moves
to earn points and trigger changes in the game board. It repeats the
above-mentioned three steps iteratively until timeout. For evaluation, we
conducted a case study using Lap on an open-source match-3 game, CasseBonbons,
and empirically compared it with three existing tools. Our results are
promising: Lap outperformed existing tools by achieving higher code coverage
and triggering more program crashes. This research sheds light on the future of
automatic testing and LLM applications.

</details>


### [17] [It Only Gets Worse: Revisiting DL-Based Vulnerability Detectors from a Practical Perspective](https://arxiv.org/abs/2507.09529)
*Yunqian Wang,Xiaohong Li,Yao Zhang,Yuekang Li,Zhiping Zhou,Ruitao Feng*

Main category: cs.SE

TL;DR: DL-based vulnerability detectors are not as reliable or adaptable as assumed. Performance and consistency issues remain, and pre-trained models are not always better. VulTegra framework reveals that minor adjustments can boost detection performance, and effective vulnerability detection requires consideration beyond CWE labels.


<details>
  <summary>Details</summary>
Motivation: Deep learning-based detectors for software vulnerabilities are popular, but there are doubts about their reliability, consistency, and real-world applicability. Current models often produce high false positive/negative rates and may not adapt well to new vulnerabilities, necessitating a rigorous evaluation to identify improvement areas.

Method: The authors present VulTegra, a multidimensional evaluation framework that systematically compares both scratch-trained and pre-trained deep learning models for vulnerability detection. The analysis is done across various scenarios, with a focus on factors influencing detection performance and model consistency.

Result: VulTegra reveals that even state-of-the-art detectors have low consistency, are limited in real-world scenarios, and face scalability challenges. Pre-trained models are not always superior to scratch-trained ones; each has unique strengths in specific situations. The study identifies critical factors affecting performance, demonstrating that tuning even a single factor leads to recall improvements across all tested models, and better F1 scores for most.

Conclusion: The paper underscores that existing DL-based vulnerability detectors have notable limitations, especially regarding consistency and adaptability. Improvements depend on understanding the effect of code features and vulnerability types, rather than reliance on broad CWE classifications. There is no universally superior model, and tailored improvements based on real-world use cases are essential.

Abstract: With the growing threat of software vulnerabilities, deep learning (DL)-based
detectors have gained popularity for vulnerability detection. However, doubts
remain regarding their consistency within declared CWE ranges, real-world
effectiveness, and applicability across scenarios. These issues may lead to
unreliable detection, high false positives/negatives, and poor adaptability to
emerging vulnerabilities. A comprehensive analysis is needed to uncover
critical factors affecting detection and guide improvements in model design and
deployment. In this paper, we present VulTegra, a novel evaluation framework
that conducts a multidimensional comparison of scratch-trained and
pre-trained-based DL models for vulnerability detection. VulTegra reveals that
state-of-the-art (SOTA) detectors still suffer from low consistency, limited
real-world capabilities, and scalability challenges. Contrary to common belief,
pre-trained models are not consistently better than scratch-trained models but
exhibit distinct strengths in specific contexts.Importantly, our study exposes
the limitations of relying solely on CWE-based classification and identifies
key factors that significantly affect model performance. Experimental results
show that adjusting just one such factor consistently improves recall across
all seven evaluated detectors, with six also achieving better F1 scores. Our
findings provide deeper insights into model behavior and emphasize the need to
consider both vulnerability types and inherent code features for effective
detection.

</details>


### [18] [A Serverless Architecture for Real-Time Stock Analysis using Large Language Models: An Iterative Development and Debugging Case Study](https://arxiv.org/abs/2507.09583)
*Taniv Ashraf*

Main category: cs.SE

TL;DR: This paper presents a nearly zero-cost, AI-driven serverless system for real-time stock analysis using Google's Gemini LLM. It covers the system's design, debugging, and deployment, demonstrating how powerful and accessible tools can be developed for financial analysis by individuals.


<details>
  <summary>Details</summary>
Motivation: The motivation is to make financial data analysis more accessible and democratized by leveraging the power of new Large Language Models (LLMs) such as Google's Gemini. The authors aim to show that individuals can build advanced, low-cost, AI-powered financial tools.

Method: The authors designed and implemented a novel, serverless, real-time stock analysis system. This system uses Gemini's API for qualitative financial analysis, automates data ingestion and processing through GitHub Actions, and presents the results via a decoupled static frontend. The paper includes a detailed case study of the debugging process, addressing both common and rare software and platform issues.

Result: The resulting system is robust, event-driven, serverless, and operates at nearly zero cost. It is publicly available for use, and the complete source code is accessible. The architecture supports real-time AI-powered financial analysis.

Conclusion: The paper concludes that LLMs can play a significant role in democratizing financial analysis by enabling the creation of sophisticated, low-cost tools by individuals. Importantly, it highlights the value of robust debugging methodologies and human-AI collaboration in software development.

Abstract: The advent of powerful, accessible Large Language Models (LLMs) like Google's
Gemini presents new opportunities for democratizing financial data analysis.
This paper documents the design, implementation, and iterative debugging of a
novel, serverless system for real-time stock analysis. The system leverages the
Gemini API for qualitative assessment, automates data ingestion and processing
via GitHub Actions, and presents the findings through a decoupled, static
frontend. We detail the architectural evolution of the system, from initial
concepts to a robust, event-driven pipeline, highlighting the practical
challenges encountered during deployment. A significant portion of this paper
is dedicated to a case study on the debugging process, covering common software
errors, platform-specific permission issues, and rare, environment-level
platform bugs. The final architecture operates at a near-zero cost,
demonstrating a viable model for individuals to build sophisticated AI-powered
financial tools. The operational application is publicly accessible, and the
complete source code is available for review. We conclude by discussing the
role of LLMs in financial analysis, the importance of robust debugging
methodologies, and the emerging paradigm of human-AI collaboration in software
development.

</details>


### [19] [How to Define Design in Industrial Control and Automation Software](https://arxiv.org/abs/2507.09594)
*Aydin Homay*

Main category: cs.SE

TL;DR: The paper critiques current, often subjective, design practices in software and automation. By applying design theory, it offers clearer definitions, distinctions, and approaches for more scientific, systematic, and innovative design.


<details>
  <summary>Details</summary>
Motivation: The motivation comes from the lack of a scientific foundation in design, which leads to subjective decisions, hampering efficiency and innovation—especially notable in the software and industrial control/automation sectors.

Method: The paper conducts a review of existing design definitions in the software industry, critiques misconceptions, explores scientific definitions inspired by design theory, and addresses key questions like the onset, nature, and quality of design. It also discusses the distinction between ad-hoc and systematic approaches and how to balance operational and evolutionary concerns.

Result: The study clarifies when design begins, defines design scientifically, distinguishes between good and poor design, differentiates design from design language, and evaluates the merits of ad-hoc vs. systematic design in balancing operational and evolutionary aspects.

Conclusion: A more scientific and systematic approach to design, grounded in advances from design theory, can help overcome subjectivity, improve efficiency and innovation, and provide better balance in addressing operational and evolutionary requirements in software and automation systems.

Abstract: Design is a fundamental aspect of engineering, enabling the creation of
products, systems, and organizations to meet societal and/or business needs.
However, the absence of a scientific foundation in design often results in
subjective decision-making, reducing both efficiency and innovation. This
challenge is particularly evident in the software industry and, by extension,
in the domain of industrial control and automation systems (iCAS).
  In this study, first we review the existing design definitions within the
software industry, challenge prevailing misconceptions about design, review
design definition in the field of design theory and address key questions such
as: When does design begin? How can design be defined scientifically? What
constitutes good design? and the difference between design and design language
by relying on advancements in the field of design theory. We also evaluate the
distinction between ad-hoc and systematic design approaches, and present
arguments on how to balance complementary operational concerns while resolving
conflicting evolutionary concerns.

</details>


### [20] [The Mythical Good Software](https://arxiv.org/abs/2507.09596)
*Aydin Homay*

Main category: cs.SE

TL;DR: The paper challenges the traditional view of cohesion and coupling as distinct software design goals, arguing that they are fundamentally related and that an uncritical pursuit of higher cohesion may be counterproductive.


<details>
  <summary>Details</summary>
Motivation: To reevaluate the traditional software engineering principles of cohesion and coupling, questioning their distinctness and practical application.

Method: The authors provide a theoretical analysis and philosophical discussion on the relationship between cohesion and coupling in software design.

Result: The study suggests that considering cohesion and coupling as entirely separate principles is misleading, as they are fundamentally related. Additionally, the pursuit of higher cohesion should not be done blindly, as it may come with trade-offs or costs.

Conclusion: Designers should not strictly pursue high cohesion and low coupling as separate goals, but rather understand the nuanced, interconnected relationship between them and make balanced design decisions.

Abstract: Good software has high cohesion and low coupling is clumsy, obscure, and in
some certain cases could be actually a harmful state of being. It is clumsy
because there is no perfect correlation between higher cohesiveness and optimum
design, and it is obscure because it conveys the message that coupling and
cohesion are two distinct design principles, while there are in principle the
same design approaches, and only the time and space differ between them, and it
could also be a harmful state of being because we should not always aim for
higher cohesiveness without considering its cost.
  In the course of this study, we aim to elucidate for the readers the meaning
and underlying philosophy of the aforementioned paragraph.

</details>


### [21] [Complexity and Coupling: A Functional Domain Approach](https://arxiv.org/abs/2507.09599)
*Aydin Homay*

Main category: cs.SE

TL;DR: Traditional definitions of complexity and coupling in engineering are unclear and inconsistent. This paper proposes clear, scientifically grounded definitions focused on the functional domain, showing that complexity and coupling are functionally, not physically, based. Effective design depends on addressing these issues functionally.


<details>
  <summary>Details</summary>
Motivation: There is widespread ambiguity and inconsistency in the definitions of complexity and coupling, especially in industrial control and automation systems (iCAS). Existing definitions, often based on physical attributes, contribute to confusion. The paper seeks to provide clarity and scientific grounding for these concepts.

Method: The authors offer a new precise definition of complexity and coupling, specifically tailored to the functional domain. They illustrate their arguments using examples from various fields such as software engineering, industrial automation, and mechanical design. The analysis highlights the disconnect between commonly held beliefs and scientific reality.

Result: The paper shows that complexity is not directly related to system size or the number of components, and that coupling should be understood within the functional, not physical, domain. Coupled design increases complexity, but this complexity can potentially be managed and reduced by focusing on functional aspects.

Conclusion: Defining and addressing complexity and coupling in the functional domain is essential for effective design, especially in industrial control and automation systems.

Abstract: This paper provides a precise and scientific definition of complexity and
coupling, grounded in the functional domain, particularly within industrial
control and automation systems (iCAS). We highlight the widespread ambiguity in
defining complexity and coupling, emphasizing that many existing definitions
rooted in physical attributes lead to confusion and inconsistencies.
Furthermore, we re-exhibit why coupled design inherently increases complexity
and how potentially this complexity could be reduced. Drawing on examples from
various disciplines, such as software engineering, industrial automation, and
mechanical design, we demonstrate that complexity does not necessarily
correlate with system size or the number of components, and coupling, unlike
common belief in software engineering, actually does not occur in the physical
domain but in the functional domain. We conclude that effective design
necessitates addressing coupling and complexity within the functional domain.

</details>


### [22] [Code Review as Decision-Making -- Building a Cognitive Model from the Questions Asked During Code Review](https://arxiv.org/abs/2507.09637)
*Lo Gullstrand Heander,Emma Söderberg,Christofer Rydenfält*

Main category: cs.SE

TL;DR: This paper presents a cognitive model of code review constructed from an ethnographic study. It shows that code review involves decision-making similar to recognition-primed decision-making. Understanding this can inspire better tool support to improve efficiency and enjoyment while preserving essential social benefits.


<details>
  <summary>Details</summary>
Motivation: Although code review is crucial for code quality and team dynamics, existing tools and processes present challenges that cause inefficiencies and frustrations. Full automation of code review risks losing significant interpersonal benefits, such as knowledge sharing and collective code ownership. The authors aim to better understand the underlying cognitive processes to guide improved tool development and preserve these benefits.

Method: The study uses an ethnographic think-aloud approach, involving 10 participants and 34 code reviews. The researchers conduct thematic, statistical, temporal, and sequential analyses of transcribed review sessions to construct a cognitive model of code review.

Result: The authors create the Code Review as Decision-Making (CRDM) model. It describes code review as a process with two main phases: orientation (establishing context and rationale) and analytical (deep analysis and planning). Throughout, reviewers make various decisions, such as when to comment, seek more information, or verify results. The model highlights similarities with recognition-primed decision-making processes.

Conclusion: By understanding code review as a decision-making process, tool and process improvements can be better targeted to support the actual reviewer workflows and cognitive needs, potentially increasing efficiency, enjoyment, and preservation of interpersonal benefits. Improved tools—whether AI-driven or not—should respect and facilitate these phases of cognitive work rather than automate away vital human interactions.

Abstract: Code review is a well-established and valued practice in the software
engineering community contributing to both code quality and interpersonal
benefits. However, there are challenges in both tools and processes that give
rise to misalignments and frustrations. Recent research seeks to address this
by automating code review entirely, but we believe that this risks losing the
majority of the interpersonal benefits such as knowledge transfer and shared
ownership.
  We believe that by better understanding the cognitive processes involved in
code review, it would be possible to improve tool support, with out without AI,
and make code review both more efficient, more enjoyable, while increasing or
maintaining all of its benefits. In this paper, we conduct an ethnographic
think-aloud study involving 10 participants and 34 code reviews. We build a
cognitive model of code review bottom up through thematic, statistical,
temporal, and sequential analysis of the transcribed material. Through the
data, the similarities between the cognitive process in code review and
decision-making processes, especially recognition-primed decision-making,
become apparent.
  The result is the Code Review as Decision-Making (CRDM) model that shows how
the developers move through two phases during the code review; first an
orientation phase to establish context and rationale and then an analytical
phase to understand, assess, and plan the rest of the review. Throughout the
process several decisions must be taken, on writing comments, finding more
information, voting, running the code locally, verifying continuous integration
results, etc.
  Analysis software and process-coded data publicly available at:
https://doi.org/10.5281/zenodo.15758266

</details>


### [23] [Is Quantization a Deal-breaker? Empirical Insights from Large Code Models](https://arxiv.org/abs/2507.09665)
*Saima Afrin,Bowen Xu,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: Applying activation-aware quantization to code LLMs reduces their resource footprint without harming their ability to generate functionally correct and high-quality code in terms of maintainability and simplicity.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) require extensive computational resources and contribute to significant environmental impact due to their large carbon footprint. Model quantization lowers the precision of model parameters, reducing resource needs. Previous work mainly addressed functional correctness of quantized models, neglecting critical code quality aspects.

Method: The authors applied Activation-aware Weight Quantization (AWQ) to two code-specific LLMs, CodeLlama and DeepSeekCoder. They used these quantized models to generate Java and Python code, and assessed the output using static analysis tools to evaluate quality metrics like cyclomatic complexity, cognitive complexity, and lines of code.

Result: Quantization did not only preserve functional correctness but also maintained important code quality attributes, such as maintainability and structural simplicity, in the generated outputs.

Conclusion: Model quantization, specifically AWQ, proves effective for code LLMs by reducing resource demands and carbon footprint while retaining both functional and qualitative aspects of code quality.

Abstract: The growing scale of large language models (LLMs) not only demands extensive
computational resources but also raises environmental concerns due to their
increasing carbon footprint. Model quantization emerges as an effective
approach that can reduce the resource demands of LLMs by decreasing parameter
precision without substantially affecting performance (e.g., 16 bit to 4 bit).
While recent studies have established quantization as a promising approach for
optimizing large code models (LCMs), a specialized subset of LLMs tailored for
automated software engineering, their findings offer only limited insights into
its practical implications. Specifically, current investigations focus only on
the functional correctness of the code generated by quantized models,
neglecting how quantization impacts critical aspects of code quality such as
reliability, maintainability, and security. To bridge this gap, our study
investigates the effects of quantization on the qualitative aspects of
automatically generated code. We apply Activation-aware Weight Quantization
(AWQ) to two widely used code models, CodeLlama and DeepSeekCoder, to generate
Java and Python code. Using state-of-the-art static analysis tools, we evaluate
software quality metrics and static features including cyclomatic complexity,
cognitive complexity, and lines of code. Our findings reveal that quantization
is a robust technique that not only preserves functional correctness, but also
retains key qualitative code attributes sought after by developers, such as
maintainability and structural simplicity.

</details>


### [24] [OrQstrator: An AI-Powered Framework for Advanced Quantum Circuit Optimization](https://arxiv.org/abs/2507.09682)
*Laura Baird,Armin Moin*

Main category: cs.SE

TL;DR: OrQstrator is a new Deep RL-powered system that smartly combines multiple quantum circuit optimization techniques to produce hardware-aware, efficient quantum circuits for today's constrained quantum computers.


<details>
  <summary>Details</summary>
Motivation: Quantum circuit optimization is crucial in the NISQ era, where hardware is limited, noisy, and resources are constrained. There is a need for smarter, adaptive, and hardware-aware optimization strategies to maximize circuit performance on current quantum devices.

Method: The authors introduce OrQstrator, a modular quantum circuit optimization framework powered by Deep Reinforcement Learning (DRL). It uses an orchestration engine to coordinate three optimization modules: a DRL-based circuit rewriter, a domain-specific local optimizer, and a parameterized template circuit instantiator. The orchestration engine learns which optimizer to apply based on circuit and hardware features.

Result: OrQstrator produces quantum circuits that are optimized for hardware-aware criteria such as reduced gate count, lower depth, and improved expected fidelity. The framework adapts to backend constraints using insights from the existing NISQ Analyzer tool.

Conclusion: OrQstrator demonstrates a promising and modular approach to quantum circuit optimization in the NISQ era by combining machine learning and domain-specific strategies, efficiently adapting to hardware limitations for practical implementation.

Abstract: We propose a novel approach, OrQstrator, which is a modular framework for
conducting quantum circuit optimization in the Noisy Intermediate-Scale Quantum
(NISQ) era. Our framework is powered by Deep Reinforcement Learning (DRL). Our
orchestration engine intelligently selects among three complementary circuit
optimizers: A DRL-based circuit rewriter trained to reduce depth and gate count
via learned rewrite sequences; a domain-specific optimizer that performs
efficient local gate resynthesis and numeric optimization; a parameterized
circuit instantiator that improves compilation by optimizing template circuits
during gate set translation. These modules are coordinated by a central
orchestration engine that learns coordination policies based on circuit
structure, hardware constraints, and backend-aware performance features such as
gate count, depth, and expected fidelity. The system outputs an optimized
circuit for hardware-aware transpilation and execution, leveraging techniques
from an existing state-of-the-art approach, called the NISQ Analyzer, to adapt
to backend constraints.

</details>


### [25] [Prompting for Performance: Exploring LLMs for Configuring Software](https://arxiv.org/abs/2507.09790)
*Helge Spieker,Théo Matricon,Nassim Belmecheri,Jørn Eirik Betten,Gauthier Le Bartz Lyan,Heraldo Borges,Quentin Mazouni,Dennis Gross,Arnaud Gotlieb,Mathieu Acher*

Main category: cs.SE

TL;DR: The paper explores using large language models (LLMs) to help configure software for better performance. LLMs show potential in some cases but also have significant limitations, such as making mistakes or using superficial logic. This work is an initial step toward harnessing LLMs for reliable software configuration advice.


<details>
  <summary>Details</summary>
Motivation: Configuring software to achieve optimal performance is difficult due to a large number of configuration options and their interactions; traditional approaches require substantial domain expertise or high computational effort, making automation desirable.

Method: The authors conducted an exploratory study using prompts to evaluate the ability of several large language models (LLMs) to aid in performance-oriented software configuration tasks, such as identifying, ranking, and recommending configuration options across different systems.

Result: Preliminary results showed that LLMs sometimes align well with expert assessments but may also produce errors, such as hallucinations or shallow reasoning, highlighting both their utility and limitations for these tasks.

Conclusion: LLMs offer promising but currently imperfect assistance for configuring software systems to optimize performance; further systematic evaluation and tailored designs are needed to make LLM-based support reliable.

Abstract: Software systems usually provide numerous configuration options that can
affect performance metrics such as execution time, memory usage, binary size,
or bitrate. On the one hand, making informed decisions is challenging and
requires domain expertise in options and their combinations. On the other hand,
machine learning techniques can search vast configuration spaces, but with a
high computational cost, since concrete executions of numerous configurations
are required. In this exploratory study, we investigate whether large language
models (LLMs) can assist in performance-oriented software configuration through
prompts. We evaluate several LLMs on tasks including identifying relevant
options, ranking configurations, and recommending performant configurations
across various configurable systems, such as compilers, video encoders, and SAT
solvers. Our preliminary results reveal both positive abilities and notable
limitations: depending on the task and systems, LLMs can well align with expert
knowledge, whereas hallucinations or superficial reasoning can emerge in other
cases. These findings represent a first step toward systematic evaluations and
the design of LLM-based solutions to assist with software configuration.

</details>


### [26] [Measuring What Matters: A Framework for Evaluating Safety Risks in Real-World LLM Applications](https://arxiv.org/abs/2507.09820)
*Jia Yi Goh,Shaun Khoo,Nyx Iskandar,Gabriel Chua,Leanne Tan,Jessica Foo*

Main category: cs.SE

TL;DR: This paper presents a practical, tested framework for evaluating and managing application-level safety in LLM systems, moving beyond model-level safety to provide actionable guidance for organizations deploying LLM applications.


<details>
  <summary>Details</summary>
Motivation: Current safety evaluations of large language models mostly focus on the models themselves, ignoring additional risks introduced at the application level (e.g., through system prompts, retrieval mechanisms, or guardrails). There is a need for frameworks that address application-specific safety concerns as these components can significantly impact overall LLM safety.

Method: The authors propose a practical evaluation framework for LLM application-level safety, which includes: (1) a method for developing customized safety risk taxonomies tailored to particular applications, and (2) concrete practices for evaluating safety risks in deployed LLM systems. The framework's effectiveness is demonstrated via real-world deployment across several organizational use cases.

Result: The framework was successfully validated through internal pilots within the organization, demonstrating its utility in helping organizations systematically evaluate and manage application-level safety risks in LLM-based systems.

Conclusion: The proposed framework addresses the gap between theoretical AI safety concepts and real-world deployment needs, offering actionable procedures and guiding principles for safe and scalable LLM application deployment.

Abstract: Most safety testing efforts for large language models (LLMs) today focus on
evaluating foundation models. However, there is a growing need to evaluate
safety at the application level, as components such as system prompts,
retrieval pipelines, and guardrails introduce additional factors that
significantly influence the overall safety of LLM applications. In this paper,
we introduce a practical framework for evaluating application-level safety in
LLM systems, validated through real-world deployment across multiple use cases
within our organization. The framework consists of two parts: (1) principles
for developing customized safety risk taxonomies, and (2) practices for
evaluating safety risks in LLM applications. We illustrate how the proposed
framework was applied in our internal pilot, providing a reference point for
organizations seeking to scale their safety testing efforts. This work aims to
bridge the gap between theoretical concepts in AI safety and the operational
realities of safeguarding LLM applications in practice, offering actionable
guidance for safe and scalable deployment.

</details>


### [27] [Turning the Tide: Repository-based Code Reflection](https://arxiv.org/abs/2507.09866)
*Wei Zhang,Jian Yang,Jiaxi Yang,Ya Wang,Zhoujun Li,Zeyu Cui,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: This paper introduces a new, realistic benchmark and dataset for evaluating code LLMs in real-world repository modification contexts, advancing the measurement of models' code reflection and generation abilities, with extensive experimental comparison across more than 40 models.


<details>
  <summary>Details</summary>
Motivation: Current code LLM benchmarks often overlook the practical scenario of modifying code within repositories, focusing instead on code generation and limited practical application. There are also concerns regarding reflection capabilities and data leaks in dynamic benchmarks.

Method: The authors introduce 'LiveRepoReflection,' a new benchmark containing 1,888 rigorously selected test cases across six programming languages, specifically targeting code understanding and generation within multi-file repository settings. They also create 'RepoReflection-Instruct,' a large-scale, instruction-tuning dataset for model training, and train 'RepoReflectionCoder' using a two-turn dialogue for code generation and error repair. Benchmarking involves over 40 LLMs.

Result: LiveRepoReflection provides a challenging, diverse, and high-quality benchmark for testing code LLMs in repository modification tasks. RepoReflectionCoder, trained on the new instruction set, participates in evaluations, presenting comparative results among 40+ LLMs. The approach highlights the varied capabilities of different models in repository-based code reflection.

Conclusion: The study addresses a significant gap in current LLM benchmarking by proposing a realistic, repository-based evaluation for code reflection tasks, offering a new benchmark and dataset for more relevant assessment and LLM development.

Abstract: Code large language models (LLMs) enhance programming by understanding and
generating code across languages, offering intelligent feedback, bug detection,
and code updates through reflection, improving development efficiency and
accessibility. While benchmarks (e.g. HumanEval/LiveCodeBench) evaluate code
generation and real-world relevance, previous works ignore the scenario of
modifying code in repositories. Considering challenges remaining in improving
reflection capabilities and avoiding data contamination in dynamic benchmarks,
we introduce LiveRepoReflection, a challenging benchmark for evaluating code
understanding and generation in multi-file repository contexts, featuring 1,888
rigorously filtered test cases across $6$ programming languages to ensure
diversity, correctness, and high difficulty. Further, we create
RepoReflection-Instruct, a large-scale, quality-filtered instruction-tuning
dataset derived from diverse sources, used to train RepoReflectionCoder through
a two-turn dialogue process involving code generation and error-driven repair.
The leaderboard evaluates over 40 LLMs to reflect the model performance of
repository-based code reflection.

</details>


### [28] [PathFuzzing: Worst Case Analysis by Fuzzing Symbolic-Execution Paths](https://arxiv.org/abs/2507.09892)
*Zimu Chen,Di Wang*

Main category: cs.SE

TL;DR: PathFuzzing is a novel method for worst-case analysis in software, leveraging both fuzzing and symbolic execution. It shows better performance over traditional approaches in experimental benchmarks.


<details>
  <summary>Details</summary>
Motivation: Worst-case resource consumption estimation is essential in software development to ensure reliability and efficiency. Existing approaches like fuzzing and symbolic execution have limitations in terms of code coverage and path explosion, respectively, making worst-case analysis (WCA) challenging.

Method: The authors introduce 'PathFuzzing,' which transforms the original program into a symbolic version that interprets execution paths as binary strings. It applies evolutionary fuzzing methods to search for binary strings (representing branch decisions) that correspond to satisfiable paths with high resource consumption.

Result: Experimental results on a benchmark suite indicate that PathFuzzing generally outperforms standard fuzzing and symbolic execution baselines in identifying worst-case resource consuming paths.

Conclusion: PathFuzzing effectively addresses the WCA problem by combining the strengths of fuzzing and symbolic execution, resulting in superior identification of worst-case resource consumption scenarios.

Abstract: Estimating worst-case resource consumption is a critical task in software
development. The worst-case analysis (WCA) problem is an optimization-based
abstraction of this task. Fuzzing and symbolic execution are widely used
techniques for addressing the WCA problem. However, improving code coverage in
fuzzing or managing path explosion in symbolic execution within the context of
WCA poses significant challenges. In this paper, we propose PathFuzzing, aiming
to combine the strengths of both techniques to design a WCA method. The key
idea is to transform a program into a symbolic one that takes an execution path
(encoded as a binary string) and interprets the bits as branch decisions.
PathFuzzing then applies evolutionary fuzzing techniques to the transformed
program to search for binary strings that represent satisfiable path conditions
and lead to high resource consumption. We evaluate the performance of
PathFuzzing experimentally on a benchmark suite that consists of prior work's
benchmarks and some added by us. Results show that PathFuzzing generally
outperforms a fuzzing and a symbolic-execution baseline.

</details>


### [29] [Modelling Interrelations Between Agile Practices: The Agile Map](https://arxiv.org/abs/2507.09907)
*Thomas Hansper,Kevin Phong Pham,Michael Neumann*

Main category: cs.SE

TL;DR: The paper addresses the complexity of combining agile practices by creating the Agile Map, a model that shows how these practices are interrelated, helping teams select and combine them more effectively.


<details>
  <summary>Details</summary>
Motivation: There is a wide variety in how agile practices are adopted and tailored in practice, making it challenging to understand how different agile practices are interrelated. This can limit the successful combination and use of such practices, as some depend on or support others.

Method: The study systematically investigates interrelations between agile practices. The authors develop a model—called the Agile Map—that describes these interrelations based on a systematic approach.

Result: The result is the development of the Agile Map: a theoretical model that systematically describes relationships between various agile practices.

Conclusion: The Agile Map provides an overview of how agile practices are related, supporting practitioners in meaningful selection and combination of agile practices.

Abstract: Agile methods are defined through guidelines comprising various practices
intended to enable agile ways of working. These guidelines further comprise a
specific set of agile practices aiming to enable teams for an agile way of
working. However, due to its wide-spread use in practice we know that agile
practices are adopted and tailored intensively, which lead to a high variety of
agile practices in terms of their level of detail. Problem: A high variety of
agile practices can be challenging as we do not know how different agile
practices are interrelated with each other. To be more precise, tailoring and
adopting agile practices may lead to the challenge, that the combinatorial use
of several agile practices can only be successful to a limited extent, as
practices support or even require each other for a effective use in practice.
Objective: Our study aims to provide an enabler for this problem. We want to
identify interrelations between agile practices and describe them in a
systematic manner. Contribution: The core contribution of this paper is the
Agile Map, a theoretical model describing relations between agile practices
following a systematic approach aiming to provide an overview of coherences
between agile practices. The model aims to support practitioners in selecting
and combining agile practices in a meaningful way.

</details>


### [30] [When Less is More: A systematic review of four-day workweek conceptualizations and their effects on organizational performance](https://arxiv.org/abs/2507.09911)
*Marvin Auf der Landwehr,Julia Topp,Michael Neumann*

Main category: cs.SE

TL;DR: This paper systematically reviews literature on four-day workweeks in IT, develops a meta-framework relating various compressed schedule models to their organizational/social effects, and offers practical guidance for adoption tailored to managerial needs.


<details>
  <summary>Details</summary>
Motivation: Agile IT organizations need work environments that are motivating, efficient, and flexible to optimize value creation. There is growing interest in compressed work schedules (like the four-day workweek), which are believed to offer benefits for both organizations and employees.

Method: A systematic review was conducted of both academic and practice-oriented literature. The study combined a systematic literature review with a web content analysis to examine four-day workweek schedules and their effects on organizations and employees.

Result: The study generated a meta-framework that links different types of compressed work schedules with their organizational and social effects. This framework can be used to guide managers in adopting compressed work schedules according to their specific needs and circumstances.

Conclusion: Compressed work schedules, such as the four-day workweek, have meaningful impacts on IT organizations. The meta-framework developed in this study can help decision-makers evaluate and implement these schedules effectively, considering both organizational goals and employee well-being.

Abstract: Context: Agile IT organizations, which are characterized by self-organization
and collaborative social interactions, require motivating, efficient and
flexible work environments to maximize value creation. Compressed work
schedules such as the four-day workweek have evolved into multiple facets over
the last decades and are associated with various benefits for organizations and
their employees. Objective: Our objective in this study is to deepen our
comprehension of the impact of compressed work schedules on the operational
efficacy of IT enterprises, while concurrently developing a comprehensive
framework delineating the intricacies of compressed work schedules.Method: We
conducted a systematic review of available conceptualizations related to
four-day workweek schedules and elaborate on their organizational and social
effects. To cover scientific and practice-oriented literature, our review
combined a systematic literature review and a web content analysis. Results:
Based on the generated insights, we derive a meta-framework that matches
conceptualizations and effects, finally guiding the adoption of compressed work
schedules based on individual managerial prerequisites and circumstances.

</details>


### [31] [Explicit Vulnerability Generation with LLMs: An Investigation Beyond Adversarial Attacks](https://arxiv.org/abs/2507.10054)
*Emir Bosnak,Sahand Moslemi,Mayasah Lami,Anil Koyuncu*

Main category: cs.SE

TL;DR: Open-source LLMs often generate insecure code when asked, especially for student-like users and moderate-code complexity. Their safety systems are insufficient, highlighting real risks for code assistance applications.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand how open-source large language models (LLMs) generate insecure or vulnerable code when explicitly prompted, a behavior that is not fully understood, especially in direct threat scenarios beyond unintended vulnerabilities or complex adversarial prompts.

Method: The study employs a dual experimental design: (1) Dynamic Prompting, which creates prompts by varying vulnerability types, user personas (like students versus professionals), and prompt directness using structured templates; and (2) Reverse Prompting, which generates prompts from real-world vulnerable code samples to analyze how accurately models reproduce those vulnerabilities. Three 7B-parameter open-source LLMs (Qwen2, Mistral, Gemma) are evaluated using static analysis (ESBMC) to detect vulnerabilities and assess type correctness.

Result: All tested models frequently generated vulnerable code. Qwen2 produced the most correct vulnerability types. Student-like personas in prompts led to higher rates of vulnerabilities than professional personas, and direct prompts were only slightly more effective than indirect ones. Vulnerability reproduction rates followed an inverted-U relationship with cyclomatic complexity, peaking at moderate complexity.

Conclusion: Open-source LLMs' safety mechanisms are inadequate, especially in responding to benign-sounding educational prompts, as they still readily generate vulnerable code under various prompting scenarios, indicating a significant security risk.

Abstract: Large Language Models (LLMs) are increasingly used as code assistants, yet
their behavior when explicitly asked to generate insecure code remains poorly
understood. While prior research has focused on unintended vulnerabilities or
adversarial prompting techniques, this study examines a more direct threat
scenario: open-source LLMs generating vulnerable code when prompted either
directly or indirectly. We propose a dual experimental design: (1) Dynamic
Prompting, which systematically varies vulnerability type, user persona, and
directness across structured templates; and (2) Reverse Prompting, which
derives prompts from real vulnerable code samples to assess vulnerability
reproduction accuracy. We evaluate three open-source 7B-parameter models
(Qwen2, Mistral, and Gemma) using ESBMC static analysis to assess both the
presence of vulnerabilities and the correctness of the generated vulnerability
type. Results show all models frequently produce vulnerable outputs, with Qwen2
achieving highest correctness rates. User persona significantly affects
success, where student personas achieved higher vulnerability rates than
professional roles, while direct prompts were marginally more effective.
Vulnerability reproduction followed an inverted-U pattern with cyclomatic
complexity, peaking at moderate ranges. Our findings expose limitations of
safety mechanisms in open-source models, particularly for seemingly benign
educational requests.

</details>


### [32] [LLMShot: Reducing snapshot testing maintenance via LLMs](https://arxiv.org/abs/2507.10062)
*Ergün Batuhan Kaynak,Mayasah Lami,Sahand Moslemi,Anil Koyuncu*

Main category: cs.SE

TL;DR: LLMShot is a new AI framework that automates the analysis of UI snapshot test failures using visual language models, greatly reducing manual effort with high accuracy, though challenges remain for controllable visual reasoning.


<details>
  <summary>Details</summary>
Motivation: Snapshot testing is important for UI validation, but frequent UI changes lead to high maintenance overhead. Manual inspection is needed to distinguish between regressions and design changes, which is burdensome as applications evolve. There is a clear need for automated solutions.

Method: The paper introduces LLMShot, a framework that uses vision-based Large Language Models to classify UI changes in snapshot test failures. The authors built a dataset from a complex iOS app with feature flags to create realistic test scenarios. They evaluated LLMShot using different Gemma3 model variants and also explored selective ignore mechanisms for controllable visual reasoning.

Result: The 12B Gemma3 model in LLMShot achieved over 84% recall in identifying root causes of failures. The 4B model showed practical performance for continuous integration environments. The study also exposed limitations in current prompting-based selective ignore approaches.

Conclusion: LLMShot is the first automated, semantics-aware approach for analyzing snapshot test failures. It provides developers with structured, actionable insights to reduce manual triage workload and points the way toward more intelligent UI testing.

Abstract: Snapshot testing has emerged as a critical technique for UI validation in
modern software development, yet it suffers from substantial maintenance
overhead due to frequent UI changes causing test failures that require manual
inspection to distinguish between genuine regressions and intentional design
changes. This manual triage process becomes increasingly burdensome as
applications evolve, creating a need for automated analysis solutions. This
paper introduces LLMShot, a novel framework that leverages vision-based Large
Language Models to automatically analyze snapshot test failures through
hierarchical classification of UI changes. To evaluate LLMShot's effectiveness,
we developed a comprehensive dataset using a feature-rich iOS application with
configurable feature flags, creating realistic scenarios that produce authentic
snapshot differences representative of real development workflows. Our
evaluation using Gemma3 models demonstrates strong classification performance,
with the 12B variant achieving over 84% recall in identifying failure root
causes while the 4B model offers practical deployment advantages with
acceptable performance for continuous integration environments. However, our
exploration of selective ignore mechanisms revealed significant limitations in
current prompting-based approaches for controllable visual reasoning. LLMShot
represents the first automated approach to semantic snapshot test analysis,
offering developers structured insights that can substantially reduce manual
triage effort and advance toward more intelligent UI testing paradigms.

</details>


### [33] [Accelerating Automatic Program Repair with Dual Retrieval-Augmented Fine-Tuning and Patch Generation on Large Language Models](https://arxiv.org/abs/2507.10103)
*Hanyang Guo,Xiaoheng Xie,Hong-Ning Dai,Peng Di,Yu Zhang,Bishenghui Tao,Zibin Zheng*

Main category: cs.SE

TL;DR: This paper introduces SelRepair, an APR method leveraging a fine-tuned LLM and a dual RAG module to efficiently incorporate code-specific semantic and syntactic features, resulting in better repair accuracy and faster inference than previous methods.


<details>
  <summary>Details</summary>
Motivation: Automated Program Repair (APR) methods are crucial for software quality and developer efficiency, but existing approaches are limited by repair type, training data quality, and model size. Recent advances, including LLMs combined with RAG, have not fully addressed code repair tasks or incorporated code-specific features.

Method: The authors propose SelRepair, which integrates a fine-tuned LLM with a dual Retrieval-Augmented-Generation module. The approach utilizes a bug-fix pair dataset for fine-tuning and a selection gate that retrieves relevant information based on semantic and syntactic/structural similarity, optimizing token usage and inference time.

Result: On Java datasets, SelRepair achieves higher exact match rates (26.29% and 17.64% on different datasets) and reduces inference time by at least 6.42% by controlling input lengths, outperforming previous APR methods.

Conclusion: SelRepair, with its fine-tuned LLM and code-aware dual RAG design, addresses limitations of prior APR methods and shows improved accuracy and efficiency in automated code repair.

Abstract: Automated Program Repair (APR) is essential for ensuring software reliability
and quality while enhancing efficiency and reducing developers' workload.
Although rule-based and learning-based APR methods have demonstrated their
effectiveness, their performance was constrained by the defect type of repair,
the quality of training data, and the size of model parameters. Recently, Large
Language Models (LLMs) combined with Retrieval-Augmented-Generation (RAG) have
been increasingly adopted in APR tasks. However, current code LLMs and RAG
designs neither fully address code repair tasks nor consider code-specific
features. To overcome these limitations, we propose SelRepair, a novel APR
approach with integration of a fine-tuned LLM with a newly-designed dual RAG
module. This approach uses a bug-fix pair dataset for fine-tuning and
incorporates semantic and syntactic/structural similarity information through
an RAG selection gate. This design ensures relevant information is retrieved
efficiently, thereby reducing token length and inference time. Evaluations on
Java datasets show SelRepair outperforms other APR methods, achieving 26.29%
and 17.64% in terms of exact match (EM) on different datasets while reducing
inference time by at least 6.42% with controlled input lengths.

</details>


### [34] [Breaking the Myth: Can Small Models Infer Postconditions Too?](https://arxiv.org/abs/2507.10182)
*Gehao Zhang,Zhenting Wang,Juan Zhai*

Main category: cs.SE

TL;DR: A small, fine-tuned code model can match or outperform much larger LLMs in generating correct and expressive software specifications, making automated specification generation more practical and efficient for real-world use.


<details>
  <summary>Details</summary>
Motivation: Generating formal specifications is crucial for software correctness, but manual efforts are tedious and error-prone. While LLMs can automate this task, their large size and high computational requirements pose challenges.

Method: The authors create a custom dataset of prompts, reasoning logs, and postconditions and supervise the fine-tuning of a 7B-parameter code model. The approach addresses real-world dependencies and ensures pre-state information is preserved for expressive and accurate specifications. The model is evaluated on a Java bugs benchmark (Defects4J) and compared to larger models including GPT-4o.

Result: The fine-tuned small model matches or exceeds the performance of much larger models in syntax and semantic correctness and bug-distinguishing ability, demonstrating strong performance with lower resource demands.

Conclusion: Small, fine-tuned language models are capable of high-quality automated specification generation, offering a practical, efficient, and resource-conscious alternative to using large LLMs.

Abstract: Formal specifications are essential for ensuring software correctness, yet
manually writing them is tedious and error-prone. Large Language Models (LLMs)
have shown promise in generating such specifications from natural language
intents, but the giant model size and high computational demands raise a
fundamental question: Do we really need large models for this task? In this
paper, we show that a small, fine-tuned language model can achieve high-quality
postcondition generation with much lower computational costs. We construct a
specialized dataset of prompts, reasoning logs, and postconditions, then
supervise the fine-tuning of a $7$B-parameter code model. Our approach tackles
real-world repository dependencies and preserves pre-state information,
allowing for expressive and accurate specifications. We evaluate the model on a
benchmark of real-world Java bugs (Defects4J) and compare against both
proprietary giants (e.g., GPT-4o) and open-source large models. Empirical
results demonstrate that our compact model matches or outperforms significantly
larger counterparts in syntax correctness, semantic correctness, and
bug-distinguishing capability. These findings highlight that targeted
fine-tuning on a modest dataset can enable small models to achieve results
formerly seen only in massive, resource-heavy LLMs, offering a practical and
efficient path for the real-world adoption of automated specification
generation.

</details>


### [35] [Towards a Framework for Operationalizing the Specification of Trustworthy AI Requirements](https://arxiv.org/abs/2507.10228)
*Hugo Villamizar,Daniel Mendez,Marcos Kalinowski*

Main category: cs.SE

TL;DR: This vision paper proposes combining AMDiRE and PerSpecML to improve requirements engineering for trustworthy AI systems by balancing structure and multi-perspective analysis, and outlines future research topics.


<details>
  <summary>Details</summary>
Motivation: There are increased concerns about the trustworthiness of AI systems, which are often complex, context-dependent, and have properties that are hard to specify. Traditional requirements engineering practices may not suffice for these new challenges.

Method: The paper proposes integrating two approaches: AMDiRE (an artifact-based requirements engineering methodology focused on deterministic systems) and PerSpecML (a perspective-based method for dealing with concerns in machine learning systems, such as data-driven and non-deterministic behaviors).

Result: The integration of AMDiRE and PerSpecML is described as a potential way to operationalize trustworthiness requirements in AI-enabled systems, making requirements more consistent and traceable. The authors also identify key research directions and open challenges for further investigation.

Conclusion: Combining structured artefact-centric approaches with perspective-based guidance can better address the unique requirements and trust issues in ML-enabled systems. This integration provides a new pathway to link stakeholder concerns with systematic requirements models.

Abstract: Growing concerns around the trustworthiness of AI-enabled systems highlight
the role of requirements engineering (RE) in addressing emergent,
context-dependent properties that are difficult to specify without structured
approaches. In this short vision paper, we propose the integration of two
complementary approaches: AMDiRE, an artefact-based approach for RE, and
PerSpecML, a perspective-based method designed to support the elicitation,
analysis, and specification of machine learning (ML)-enabled systems. AMDiRE
provides a structured, artefact-centric, process-agnostic methodology and
templates that promote consistency and traceability in the results; however, it
is primarily oriented toward deterministic systems. PerSpecML, in turn,
introduces multi-perspective guidance to uncover concerns arising from the
data-driven and non-deterministic behavior of ML-enabled systems. We envision a
pathway to operationalize trustworthiness-related requirements, bridging
stakeholder-driven concerns and structured artefact models. We conclude by
outlining key research directions and open challenges to be discussed with the
RE community.

</details>


### [36] [An Empirical Study of Interaction Bugs in ROS-based Software](https://arxiv.org/abs/2507.10235)
*Zhixiang Chen,Zhuangbin Chen,Xingjie Cai,Wei Li,Zibin Zheng*

Main category: cs.SE

TL;DR: This paper analyzes 121 interaction bugs in ROS-based robotics systems, classifies them, and draws actionable insights to help build more reliable robotic systems by mitigating failures at component interaction points.


<details>
  <summary>Details</summary>
Motivation: Modern robots rely on multiple interacting software and hardware components, and failures often occur at their interaction points. However, reliability issues caused by component interactions—called interaction bugs (iBugs)—are poorly understood and studied.

Method: The paper presents an empirical study by analyzing 121 interaction bugs (iBugs) across ten representative, actively maintained ROS (Robot Operating System) projects. The study categorizes the iBugs, examines their root causes, fixing approaches, and resulting impact.

Result: iBugs were classified into three main types: intra-system, hardware, and environmental. The study identified common root causes and effective strategies for resolving these bugs, offering insights into how such interaction bugs arise and propagate.

Conclusion: The findings highlight the importance of addressing interaction-level reliability in robotics. Improved awareness and systematic handling of iBugs can lead to the design of more robust and safer robotic systems.

Abstract: Modern robotic systems integrate multiple independent software and hardware
components, each responsible for distinct functionalities such as perception,
decision-making, and execution. These components interact extensively to
accomplish complex end-to-end tasks. As a result, the overall system
reliability depends not only on the correctness of individual components, but
also on the correctness of their interactions. Failures often manifest at the
boundaries between components, yet interaction-related reliability issues in
robotics--referred to here as interaction bugs (iBugs)--remain underexplored.
  This work presents an empirical study of iBugs within robotic systems built
using the Robot Operating System (ROS), a widely adopted open-source robotics
framework. A total of 121 iBugs were analyzed across ten actively maintained
and representative ROS projects. The identified iBugs are categorized into
three major types: intra-system iBugs, hardware iBugs, and environmental iBugs,
covering a broad range of interaction scenarios in robotics. The analysis
includes an examination of root causes, fixing strategies, and the impact of
these bugs. Several findingsa are derived that shed light on the nature of
iBugs and suggest directions for improving their prevention and detection.
These insights aim to inform the design of more robust and safer robotic
systems.

</details>


### [37] [Helveg: Diagrams for Software Documentation](https://arxiv.org/abs/2507.10244)
*Adam Štěpánek,David Kuťák,Barbora Kozlíková,Jan Byška*

Main category: cs.SE

TL;DR: The paper presents Helveg, a tool that turns API documentation into interactive, diagram-based visualizations, making codebase exploration easier for developers. After initial user testing exposed usability problems, major improvements were made and validated with further user evaluation.


<details>
  <summary>Details</summary>
Motivation: Software developers often struggle to understand unfamiliar codebases, especially when onboarding or exploring external libraries. Traditional documentation is static and does not support interactive or high-level exploration, making it less effective for this purpose.

Method: The paper introduces an approach that integrates software architecture visualization into API reference documentation. The approach uses interactive node-link diagrams with expressive node glyphs and filtering capabilities, implemented in a prototype tool called Helveg that automatically visualizes C# codebases. The paper reports iterative development, with user testing guiding improvements to glyph design, interaction, and the UI.

Result: User testing of the initial Helveg prototype revealed issues with readability, intuitiveness, and user experience. In response, the tool was significantly improved, and the new version was evaluated again with the same user group to assess these enhancements.

Conclusion: The improved Helveg tool brings significant benefits to codebase exploration by making documentation more interactive and visually informative. However, iterative user testing remains crucial as initial designs may have usability issues that need to be addressed.

Abstract: Software developers often have to gain an understanding of a codebase. Be it
programmers getting onboarded onto a team project or, for example, developers
striving to grasp an external open-source library. In either case, they
frequently turn to the project's documentation. However, documentation in its
traditional textual form is ill-suited for this kind of high-level exploratory
analysis, since it is immutable from the readers' perspective and thus forces
them to follow a predefined path. We have designed an approach bringing aspects
of software architecture visualization to API reference documentation. It
utilizes a highly interactive node-link diagram with expressive node glyphs and
flexible filtering capabilities, providing a high-level overview of the
codebase as well as details on demand. To test our design, we have implemented
a prototype named Helveg, capable of automatically generating diagrams of C\#
codebases. User testing of Helveg confirmed its potential, but it also revealed
problems with the readability, intuitiveness, and user experience of our tool.
Therefore, in this paper, which is an extended version of our VISSOFT paper
with DOI 10.1109/VISSOFT64034.2024.00012, we address many of these problems
through major changes to the glyph design, means of interaction, and user
interface of the tool. To assess the improvements, this new version of Helveg
was evaluated again with the same group of participants as the previous
version.

</details>


### [38] [A Grounded Theory on the Teacher and Student Roles in Pair Programming](https://arxiv.org/abs/2507.10305)
*Linus Ververs,Trang Linh Lam,Janina Berger,Lutz Prechelt*

Main category: cs.SE

TL;DR: Pair programming can suffer when one developer dominates; ignoring power gaps may trigger defensive behaviors and reduce knowledge sharing, teamwork, and code quality.


<details>
  <summary>Details</summary>
Motivation: Pair programming is widely adopted, but the dynamics of knowledge transfer within pairs—especially when there is an imbalance—are not fully understood. The study aims to explore under what circumstances knowledge transfer may negatively impact pair programming.

Method: The study uses Grounded Theory Methodology involving 17 recorded pair programming sessions with 18 developers from 5 German software companies, complemented by 6 interviews from developers in 4 additional companies.

Result: The researchers introduce 'student' and 'teacher' roles to address one-sided knowledge gaps and highlight specific pitfalls. They develop a theory focused on the 'Power Gap', showing that unchecked power imbalances lead to defensive behaviors, which harm knowledge transfer, team cohesion, and code quality.

Conclusion: Knowledge transfer in pair programming can be harmful if power imbalances are ignored and developers do not consider their partners' needs, leading to defensive attitudes and negative cycles affecting collaboration and output quality.

Abstract: Context: Pair programming is an established (agile) practice and is practiced
throughout the industry. Objective: Understand under what circumstances
knowledge transfer can harm a pair programming session. Method: Grounded Theory
Methodology based on 17 recorded pair programming sessions with 18 developers
from 5 German software companies accompanied, by 6 interviews with different
developers from 4 other German companies. Results: We define the student and
teacher roles to help developers deal with a one-sided knowledge gap. We
describe pitfalls to avoid and develop a grounded theory centered around the
Power Gap in pair programming. Conclusions: Knowledge transfer can be harmful
when developers don't pay attention to their partners needs and desires. If
developers don't pay attention to the Power Gap and keep it in check, Defensive
Behavior may arise that leads to a vicious cycle impacting the knowledge
transfer, the Togetherness and the code quality in a negative way.

</details>


### [39] [Streamlined Airborne Software Development for Large UAVs: From Unified Data Collection to Automated Code Generation](https://arxiv.org/abs/2507.10321)
*Viktor Sinitsyn,Nils Schlautmann,Florian Schwaiger,Florian Holzapfel*

Main category: cs.SE

TL;DR: This paper introduces an automated and flexible toolchain for digital interface and onboard software development in aerospace, which has proven effective in real projects and aims to help start-ups handle integration and assurance challenges efficiently.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper lies in addressing challenges faced by start-ups in the aerospace industry, specifically the need for efficient, automated, and consistent processing of digital intra-device communication interfaces for onboard equipment.

Method: The authors present a novel process and toolchain focusing on automation, flexibility, and compliance with design assurance requirements for the development of digital interfaces and onboard software.

Result: The new process and toolchain were successfully applied in several completed projects, demonstrating their effectiveness in streamlining development.

Conclusion: The proposed solution improves efficiency, clarity, and consistency in digital interface development for aerospace onboard equipment, providing start-ups with modern tools to overcome traditional industry challenges.

Abstract: The aerospace industry has experienced significant transformations over the
last decade, driven by technological advancements and innovative solutions in
goods and personal transportation. This evolution has spurred the emergence of
numerous start-ups that now face challenges traditionally encountered by
established aerospace companies. Among these challenges is the efficient
processing of digital intra-device communication interfaces for onboard
equipment - a critical component for ensuring seamless system integration and
functionality. Addressing this challenge requires solutions that emphasize
clear and consistent interface descriptions, automation of processes, and
reduced labor-intensive efforts.
  This paper presents a novel process and toolchain designed to streamline the
development of digital interfaces and onboard software, which our team has
successfully applied in several completed projects. The proposed approach
focuses on automation and flexibility while maintaining compliance with design
assurance requirements.

</details>


### [40] [AssertCoder: LLM-Based Assertion Generation via Multimodal Specification Extraction](https://arxiv.org/abs/2507.10338)
*Enyuan Tian,Yiwei Ci,Qiusong Yang,Yufeng Li,Zhichao Lyu*

Main category: cs.SE

TL;DR: AssertCoder is an automated framework that transforms various hardware specification formats into high-quality verification assertions, significantly boosting verification accuracy and efficiency over existing tools.


<details>
  <summary>Details</summary>
Motivation: Assertion-Based Verification (ABV) is essential for confirming the functional correctness of modern hardware, but writing SystemVerilog Assertions (SVAs) by hand is time-consuming and prone to errors.

Method: The paper introduces AssertCoder, a unified framework that generates high-quality SVAs automatically from multimodal hardware design specifications. The process includes modality-sensitive preprocessing to handle different specification formats, semantic analyzers for extracting structured signal-level representations, and multi-step chain-of-thought prompting for assertion synthesis. Generated assertions are evaluated and refined using a mutation-based model checking approach.

Result: AssertCoder outperforms state-of-the-art methods in experiments across three real-world RTL designs, achieving an average of 8.4% improvement in functional correctness and 5.8% better mutation detection.

Conclusion: Automating assertion generation with AssertCoder significantly improves both the quality and efficiency of assertion-based verification in hardware design, surpassing existing manual and automated solutions.

Abstract: Assertion-Based Verification (ABV) is critical for ensuring functional
correctness in modern hardware systems. However, manually writing high-quality
SVAs remains labor-intensive and error-prone. To bridge this gap, we propose
AssertCoder, a novel unified framework that automatically generates
high-quality SVAs directly from multimodal hardware design specifications.
AssertCoder employs a modality-sensitive preprocessing to parse heterogeneous
specification formats (text, tables, diagrams, and formulas), followed by a set
of dedicated semantic analyzers that extract structured representations aligned
with signal-level semantics. These representations are utilized to drive
assertion synthesis via multi-step chain-of-thought (CoT) prompting. The
framework incorporates a mutation-based evaluation approach to assess assertion
quality via model checking and further refine the generated assertions.
Experimental evaluation across three real-world Register-Transfer Level (RTL)
designs demonstrates AssertCoder's superior performance, achieving an average
increase of 8.4% in functional correctness and 5.8% in mutation detection
compared to existing state-of-the-art approaches.

</details>


### [41] [Self-Admitted GenAI Usage in Open-Source Software](https://arxiv.org/abs/2507.10422)
*Tao Xiao,Youmei Fan,Fabio Calefato,Christoph Treude,Raula Gaikovina Kula,Hideaki Hata,Sebastian Baltes*

Main category: cs.SE

TL;DR: The paper finds that although generative AI tools are widely acknowledged by developers in open-source projects, their integration is being responsibly managed without causing the code instability or increased churn that is often assumed.


<details>
  <summary>Details</summary>
Motivation: With the rise of generative AI tools like GitHub Copilot and ChatGPT, it's becoming difficult to distinguish between AI-generated and manually written source code. This raises questions about how GenAI tools are actually being used in open-source software projects and the implications for software development.

Method: The authors introduce the idea of 'self-admitted GenAI usage'—instances where developers explicitly acknowledge using GenAI tools in software artifacts. They analyze over 250,000 GitHub repositories to identify such admissions, conduct qualitative coding to derive a taxonomy, review policy documents, and survey developers to understand concerns and practices related to GenAI. They also examine repository code churn to measure the impact of GenAI usage.

Result: They found 1,292 self-admitted GenAI usage instances in 156 repositories. The researchers created a taxonomy of tasks, content types, and purposes for GenAI use. Analysis of policies and survey responses revealed significant ethical, legal, and practical concerns, and showed developers are proactively managing GenAI adoption with guidelines and quality control. Importantly, code churn did not generally increase in repositories using GenAI, challenging prevailing assumptions.

Conclusion: Developers are actively managing the integration of GenAI tools in open-source projects, emphasizing transparency, attribution, and quality control. The anticipated negative impact of GenAI adoption on code churn is not supported by their findings, suggesting that GenAI's integration does not degrade codebase stability as commonly feared.

Abstract: The widespread adoption of generative AI (GenAI) tools such as GitHub Copilot
and ChatGPT is transforming software development. Since generated source code
is virtually impossible to distinguish from manually written code, their
real-world usage and impact on open-source software development remain poorly
understood. In this paper, we introduce the concept of self-admitted GenAI
usage, that is, developers explicitly referring to the use of GenAI tools for
content creation in software artifacts. Using this concept as a lens to study
how GenAI tools are integrated into open-source software projects, we analyze a
curated sample of more than 250,000 GitHub repositories, identifying 1,292 such
self-admissions across 156 repositories in commit messages, code comments, and
project documentation. Using a mixed methods approach, we derive a taxonomy of
32 tasks, 10 content types, and 11 purposes associated with GenAI usage based
on 284 qualitatively coded mentions. We then analyze 13 documents with policies
and usage guidelines for GenAI tools and conduct a developer survey to uncover
the ethical, legal, and practical concerns behind them. Our findings reveal
that developers actively manage how GenAI is used in their projects,
highlighting the need for project-level transparency, attribution, and quality
control practices in the new era of AI-assisted software development. Finally,
we examine the longitudinal impact of GenAI adoption on code churn in 151
repositories with self-admitted GenAI usage and find no general increase,
contradicting popular narratives on the impact of GenAI on software
development.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [42] [Bounded Model Checking of RISC-V Machine Code with Context-Free-Language Ordered Binary Decision Diagrams](https://arxiv.org/abs/2507.09539)
*Anna Bolotina,Christoph M. Kirsch,Stefanie Muroya Lei,Matthias Pleschinger*

Main category: cs.PL

TL;DR: The paper proposes shifting symbolic execution reasoning fully to the machine code level using BDDs before invoking SMT solvers. The tools rotor and bitme, especially leveraging CFLOBDDs, show potential for scalable, bit-precise software analysis, alleviating state explosion and outperforming standard SMT solver approaches.


<details>
  <summary>Details</summary>
Motivation: Symbolic execution is powerful for software analysis, but suffers from scalability issues due to state explosion. Existing approaches manage control flow internally and use SMT solvers for data flow, typically at source or IR level, making machine code generation part of the trust base. The paper seeks to shift reasoning directly to machine code and fully offload it to solvers, questioning whether bit-precise reasoning can scale for software as it does for hardware.

Method: They developed two tools, rotor and bitme, targeting RISC-V machine code. Rotor is used for model generation, and bitme for bounded model checking. They implemented two kinds of binary decision diagrams (BDDs) in bitme: algebraic decision diagrams (ADDs) and context-free-language ordered binary decision diagrams (CFLOBDDs). Bitme uses these BDDs for domain propagation, only invoking SMT solvers when necessary.

Result: Experiments show that current SMT solvers struggle on their tasks, but using BDDs (especially CFLOBDDs) in bitme significantly improves symbolic execution by speeding up SMT solving and addressing state explosion. CFLOBDDs may offer better scalability compared to ADDs.

Conclusion: Offloading reasoning to BDDs at the machine code level can improve symbolic execution scalability, reducing the reliance on SMT solvers and potentially offering more efficient analysis. This opens up new avenues for scalable, bit-precise software analysis directly on machine code.

Abstract: Symbolic execution is a powerful technique for analyzing the behavior of
software yet scalability remains a challenge due to state explosion in control
and data flow. Existing tools typically aim at managing control flow
internally, often at the expense of completeness, while offloading reasoning
over data flow to SMT solvers. Moreover, reasoning typically happens on source
code or intermediate representation level to leverage structural information,
making machine code generation part of the trust base. We are interested in
changing the equation in two non-trivial ways: pushing reasoning down to
machine code level, and then offloading reasoning entirely into SMT solvers and
other, possibly more efficient solver technology. In more abstract terms, we
are asking if bit-precise reasoning technology can be made scalable on
software, and not just hardware. For this purpose, we developed two tools
called rotor and bitme for model generation and bounded model checking,
respectively. We chose RISC-V restricted to integer arithmetic as modeling
target for rotor since RISC-V integer semantics is essentially equivalent to
established SMT semantics over bitvectors and arrays of bitvectors. While
state-of-the-art SMT solvers struggle in our experiments, we have evidence that
there is potential for improvement. To show the potential, we have slightly
generalized and then implemented in bitme two types of binary decision diagrams
(BDDs): algebraic decision diagrams (ADDs) and context-free-language ordered
binary decision diagrams (CFLOBDDs). Bitme uses BDDs to propagate program input
through models, essentially generalizing constant propagation to domain
propagation. SMT solvers only get involved when model input cannot be
propagated, significanly speeding up SMT solving. We then study the impact on
state explosion of CFLOBDDs, which are potentially more scalable than ADDs.

</details>


### [43] [BeePL: Correct-by-compilation kernel extensions](https://arxiv.org/abs/2507.09883)
*Swarn Priya,Frédéric Besson,Connor Sughrue,Tim Steenvoorden,Jamie Fulford,Freek Verbeek,Binoy Ravindran*

Main category: cs.PL

TL;DR: BeePL is a new, formally verified language and toolchain for eBPF programs, guaranteeing key safety properties through static analysis and runtime checks, and addressing flaws in the current eBPF verifier.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the shortcomings of the existing eBPF verifier, which is either too conservative (rejecting valid programs) or unsound (allowing unsafe programs). Since eBPF is used to extend kernel functionality, higher standards for safety and correctness are essential.

Method: The authors introduce BeePL, a domain-specific language for eBPF. BeePL has a formally verified type system that statically enforces important safety properties. The authors also provide formal type soundness proofs and a verified compilation strategy extending CompCert for generating BPF bytecode from BeePL programs. Runtime checks are inserted by the compiler to handle properties that can't be ensured statically.

Result: BeePL ensures type-correct memory access, safe pointer usage, structured control flow, and absence of unbounded loops. Formal proofs guarantee safety properties are upheld for well-typed programs. The verified compiler completes an end-to-end verifiable toolchain for safely extending kernel functionality.

Conclusion: BeePL provides a principled, formally verified approach to safe eBPF programming and toolchain construction, addressing both soundness and completeness issues in existing verifier implementations.

Abstract: eBPF is a technology that allows developers to safely extend kernel
functionality without modifying kernel source code or developing loadable
kernel modules. Since the kernel governs critical system operations and
enforces isolation boundaries between user space and privileged data, any
mechanism that modifies its behavior must meet the highest standards of safety
and correctness. To this end, the eBPF toolchain includes a verifier, which
statically checks safety properties such as memory access validity, bounded
loops, and type correctness before loading the program into the kernel.
However, the existing verifier is both overly conservative in some
cases-rejecting valid programs-and unsound in others, permitting unsafe
behavior that violates the intended semantics of the kernel interface.
  To address these challenges, we introduce BeePL, a domain-specific language
for eBPF with a formally verified type system. The BeePL type system, along
with the language design, statically enforces key safety properties such as
type-correct memory access, safe pointer usage, absence of unbounded loops, and
structured control flow. These guarantees are backed by formal type soundness
proofs, ensuring that well-typed programs satisfy the safety invariants
required by the eBPF execution environment. BeePL also proves that well-typed
source programs meet critical eBPF-specific properties related to memory
safety, termination, and control flow, enabling high-level reasoning prior to
compilation. For properties not fully enforceable statically-such as dynamic
bounds and undefined behavior-BeePL inserts semantics-preserving runtime checks
during compilation. We develop a verified compilation strategy that extends
CompCert to generate BPF bytecode from BeePL programs, establishing a
principled foundation for an end-to-end verifiable toolchain for safe kernel
extensions.

</details>


### [44] [Rows and Capabilities as Modal Effects](https://arxiv.org/abs/2507.10301)
*Wenhao Tang,Sam Lindley*

Main category: cs.PL

TL;DR: The authors introduce a unified framework based on modal effect types to compare and analyze row-polymorphic and capability-based effect systems. Their translations are type- and semantics-preserving, clarifying the relationships between different effect tracking approaches and informing future language design.


<details>
  <summary>Details</summary>
Motivation: There is a lack of understanding regarding the precise relationship between effect systems based on row polymorphism and those based on capabilities, mainly because effect tracking is often coupled with other language features. This complicates comparative analysis.

Method: The authors propose a uniform framework that leverages modal effect types to decouple effect tracking from other features, like functions. They use modal types to encode, analyze, and compare different effect systems by providing macro translations from both row-based and capability-based effect systems into their unified framework. They further show that these translations preserve both types and semantics.

Result: The proposed encodings successfully translate existing row-based and capability-based effect systems into the unified modal effect type framework while preserving their types and semantics. This enables clear comparison and analysis of the core mechanisms of effect tracking in these systems.

Conclusion: The modal effect type framework captures the essence of effect tracking strategies in various effect systems, allowing detailed comparison and analysis. It leads to valuable insights for designing programming languages with effect systems and clarifies the relationships between previously disparate approaches.

Abstract: Effect handlers allow programmers to model and compose computational effects
modularly. Effect systems statically guarantee that all effects are handled.
Several recent practical effect systems are based on either row polymorphism or
capabilities. However, there remains a gap in understanding the precise
relationship between effect systems with such disparate foundations. The main
difficulty is that in both row-based and capability-based systems, effect
tracking is typically entangled with other features such as functions.
  We propose a uniform framework for encoding, analysing, and comparing effect
systems. Our framework exploits and generalises modal effect types, a recent
novel effect system which decouples effect tracking from functions via
modalities. Modalities offer fine-grained control over when and how effects are
tracked, enabling us to express different strategies for effect tracking. We
give encodings as macro translations from existing row-based and
capability-based effect systems into our framework and show that these
encodings preserve types and semantics. Our encodings reveal the essence of
effect tracking mechanisms in different effect systems, enable a direct
analysis on their differences, and provide valuable insights on language
design.

</details>


### [45] [Orthologic Type Systems](https://arxiv.org/abs/2507.10482)
*Simon Guilloud,Viktor Kunčak*

Main category: cs.PL

TL;DR: The paper introduces orthologic as a logical foundation for type systems with advanced type operations, providing efficient algorithms for subtyping and type normalization. This allows type systems to support intersection, union, and negation types under subtyping assumptions in a practical and canonical way.


<details>
  <summary>Details</summary>
Motivation: Current type systems that incorporate intersection, union, and negation types struggle with efficient reasoning when subtyping and type constructors are involved. There is a need for a logical foundation that naturally supports these features and allows practical decision and normalization procedures.

Method: The paper uses orthologic, an extension of logic, as the foundation for type system design. It extends orthologic to support monotonic and antimonotonic functions for robust handling of type constructors. The authors develop a proof system with function symbols, analyze its properties (such as partial cut elimination), and devise algorithms for subtyping decision and type normalization.

Result: The research yields an O(n^2(1+m)) algorithm for deciding subtyping under m assumptions and an O(n^2) polynomial-time normalization algorithm for simplifying types to minimal canonical form. The approach allows efficient and canonical handling of intersection, union, and negation types.

Conclusion: Orthologic can effectively underpin type systems with complex type operations and subtyping, providing both theoretical soundness and efficient algorithms for core operations like subtyping decision and type normalization.

Abstract: We propose to use orthologic as the basis for designing type systems
supporting intersection, union, and negation types in the presence of subtyping
assumptions. We show how to extend orthologic to support monotonic and
antimonotonic functions, supporting the use of type constructors in such type
systems. We present a proof system for orthologic with function symbols,
showing that it admits partial cut elimination. Using these insights, we
present an $\mathcal O(n^2(1+m))$ algorithm for deciding the subtyping relation
under $m$ assumptions. We also show $O(n^2)$ polynomial-time normalization
algorithm, allowing simplification of types to their minimal canonical form.

</details>
