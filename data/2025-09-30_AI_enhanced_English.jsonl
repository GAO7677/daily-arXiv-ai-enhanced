{"id": "2509.22982", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.22982", "abs": "https://arxiv.org/abs/2509.22982", "authors": ["David M Kahn", "Jan Hoffmann", "Thomas Reps", "Jessie Grosen"], "title": "Efficient Cost Bounds with Linear Maps", "comment": null, "summary": "The Automatic Amortized Resource Analysis (AARA) derives program-execution\ncost bounds using types. To do so, AARA often makes use of cost-free types,\nwhich are critical for the composition of types and cost bounds. However,\ninferring cost-free types using the current state-of-the-art algorithm is\nexpensive due to recursive dependence on additional cost-free types.\nFurthermore, that algorithm uses a heuristic only applicable to polynomial cost\nbounds, and not, e.g., exponential bounds. This paper presents a new approach\nto these problems by representing the cost-free types of a function in a new\nway: with a linear map, which can stand for infinitely many cost-free types.\nSuch maps enable an algebraic flavor of reasoning about cost bounds (including\nnon-polynomial bounds) via matrix inequalities. These inequalities can be\nsolved with off-the-shelf linear-programming tools for many programs, so that\ntypes can always be efficiently checked and often be efficiently inferred. An\nexperimental evaluation with a prototype implementation shows that-when it is\napplicable-the inference of linear maps is exponentially more efficient than\nthe state-of-the-art algorithm.", "AI": {"tldr": "This paper presents a new way to infer cost-free types in AARA using linear maps and matrix inequalities, enabling efficient inference (even for non-polynomial bounds) with linear programming tools and achieving much greater efficiency over previous algorithms.", "motivation": "Current AARA algorithms for resource bound inference rely on cost-free types, which are necessary for composing types but costly to infer due to recursive dependencies. Existing heuristics only work for polynomial cost bounds, limiting applicability to more complex cases such as exponential bounds.", "method": "The paper introduces a new approach representing cost-free types using linear maps. This representation supports reasoning with matrix inequalities and leverages linear-programming tools for efficient solution and inference, applicable to non-polynomial bounds.", "result": "Experimental evaluation using a prototype implementation demonstrates that linear map inference is exponentially more efficient than the current state-of-the-art algorithm for cases where it applies.", "conclusion": "The proposed linear map method enables more efficient inference of cost-free types in AARA, broadens the method\u2019s applicability beyond polynomial bounds, and achieves significantly improved performance compared to existing approaches."}}
{"id": "2509.23061", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23061", "abs": "https://arxiv.org/abs/2509.23061", "authors": ["Xu Xu", "Xin Li", "Xingwei Qu", "Jie Fu", "Binhang Yuan"], "title": "Local Success Does Not Compose: Benchmarking Large Language Models for Compositional Formal Verification", "comment": null, "summary": "We introduce DafnyCOMP, a benchmark for evaluating large language models\n(LLMs) on compositional specification generation in Dafny. Unlike prior\nbenchmarks that focus on single-function tasks, DafnyCOMP targets programs\ncomposed of multiple interacting functions with data dependencies, requiring\nreasoning across component boundaries. The benchmark consists of 300\nautomatically synthesized multi-function programs. We evaluate several\nstate-of-the-art LLM families and find that, while they perform well on\nsingle-function verification, their performance drops sharply on compositional\ntasks. Analysis reveals systematic failures in cross-functional reasoning,\nincluding fragile specifications, misalignment between implementations and\nproofs, and unstable reasoning. DafnyCOMP thus provides a diagnostic tool for\nmeasuring progress toward reliable, verifiable, and compositional code\ngeneration with LLMs.", "AI": {"tldr": "The paper presents DafnyCOMP, a benchmark for testing LLMs on complex, multi-function programming tasks in Dafny. LLMs struggle significantly more with compositional code generation than with single-function tasks, highlighting a key area for future improvement.", "motivation": "Previous benchmarks for language models mainly evaluated single-function tasks, which do not reflect the complexity of real-world programming involving multiple interacting functions. The paper is motivated by the need for a benchmark that tests LLMs on compositional reasoning.", "method": "The authors introduce DafnyCOMP, a benchmark comprising 300 automatically synthesized multi-function Dafny programs. LLMs are evaluated on their ability to generate correct and verifiable specifications for these compositional tasks. Performance is compared to single-function benchmarks to assess the drop in effectiveness.", "result": "State-of-the-art LLMs perform comparatively well on single-function verification, but their performance significantly decreases on compositional specification tasks. The analysis shows systematic problems in cross-functional reasoning, such as fragile specifications and misalignments between code and proofs.", "conclusion": "DafnyCOMP exposes current limitations of LLMs in compositional code generation and serves as a diagnostic tool to track improvements in reliable and verifiable code synthesis across multiple components."}}
{"id": "2509.23229", "categories": ["cs.PL", "D.2.4; F.3.1"], "pdf": "https://arxiv.org/pdf/2509.23229", "abs": "https://arxiv.org/abs/2509.23229", "authors": ["Yawen Guan", "Cl\u00e9ment Pit-Claudel"], "title": "Fine-Grained Reasoning About Container-Internal Pointers with Logical Pinning", "comment": null, "summary": "Most separation logics hide container-internal pointers for modularity. This\nmakes it difficult to specify container APIs that temporarily expose those\npointers to the outside, and to verify programs that use these APIs. We present\nlogical pinning, a lightweight borrowing model for sequential programs that\nallows users to selectively track container-internal pointers at the logical\nlevel. Our model generalizes the magic-wand operator, making it easy to write\nand prove precise specifications, including pointer-stability properties.\nBecause it only changes how representation predicates and specifications are\nwritten, our approach is compatible with most separation logic variants. We\ndemonstrate the practicality of logical pinning by verifying small but\nrepresentative pointer-manipulating programs, and deriving more precise\nversions of common container specifications. In doing so, we show that our\napproach subsumes some well-known proof patterns, simplifies some complex\nproofs, and enables reasoning about program patterns not supported by\ntraditional specifications. All of our results are mechanized in the Rocq proof\nassistant, using the CFML library.", "AI": {"tldr": "Logical pinning is a lightweight borrowing model for separation logic, allowing fine-grained pointer tracking for more precise program verification. It's practical, compatible with existing logics, proven via mechanization, and improves or subsumes existing specification and proof methods.", "motivation": "Traditional separation logics obscure container-internal pointers for modularity, which complicates API specifications that temporarily expose those pointers and hinders verification of programs that use such APIs.", "method": "The authors introduce 'logical pinning', a borrowing model generalizing the magic-wand operator, and mechanize their approach in the Rocq proof assistant using CFML. They verify small pointer-manipulating programs as case studies.", "result": "Logical pinning subsumes standard proof patterns, simplifies complex proofs, and supports reasoning about program behaviors previously unsupported by traditional methods. It yields more precise container specifications and has been successfully mechanized.", "conclusion": "Logical pinning enables more precise and flexible specification and verification of pointer-manipulating programs by allowing selective tracking of container-internal pointers, and is compatible with most separation logic variants."}}
{"id": "2509.25114", "categories": ["cs.PL", "cs.SC", "math.AG"], "pdf": "https://arxiv.org/pdf/2509.25114", "abs": "https://arxiv.org/abs/2509.25114", "authors": ["Erdenebayar Bayarmagnai", "Fatemeh Mohammadi", "R\u00e9mi Pr\u00e9bet"], "title": "From Affine to Polynomial: Synthesizing Loops with Branches via Algebraic Geometry", "comment": null, "summary": "Ensuring software correctness remains a fundamental challenge in formal\nprogram verification. One promising approach relies on finding polynomial\ninvariants for loops. Polynomial invariants are properties of a program loop\nthat hold before and after each iteration. Generating such invariants is a\ncrucial task in loop analysis, but it is undecidable in the general case.\nRecently, an alternative approach to this problem has emerged, focusing on\nsynthesizing loops from invariants. However, existing methods only synthesize\naffine loops without guard conditions from polynomial invariants. In this\npaper, we address a more general problem, allowing loops to have polynomial\nupdate maps with a given structure, inequations in the guard condition, and\npolynomial invariants of arbitrary form.\n  We use algebraic geometry tools to design and implement an algorithm that\ncomputes a finite set of polynomial equations whose solutions correspond to all\nnondeterministic branching loops satisfying the given invariants. Furthermore,\nwe introduce a new class of invariants for which we present a significantly\nmore efficient algorithm. In other words, we reduce the problem of synthesizing\nloops to find solutions of multivariate polynomial systems with rational\nentries. This final step is handled in our software using an SMT solver.", "AI": {"tldr": "This paper tackles the challenge of generating polynomial invariants for software loops by instead synthesizing loops from those invariants, using tools from algebraic geometry and SMT solving. Their approach generalizes past work to allow complex guard conditions and update maps, offers a more efficient method for a new invariant class, and enables automated synthesis of nondeterministic branching loops relevant to program verification.", "motivation": "The motivation behind this paper is the fundamental challenge in ensuring software correctness through formal program verification. Specifically, the difficulty lies in generating polynomial invariants for loops, a crucial but generally undecidable task in loop analysis. Existing methods have limitations as they only address affine loops without guard conditions, motivating the need for a more general and robust approach.", "method": "The paper introduces an approach that uses algebraic geometry tools to design and implement an algorithm for synthesizing loops based on given polynomial invariants. It allows loops to have polynomial update maps, inequations in the guard condition, and invariants of arbitrary polynomial form. The algorithm computes a finite set of polynomial equations to characterize all nondeterministic branching loops that satisfy the invariants. Additionally, a new class of invariants is defined, for which a more efficient algorithm is presented. The synthesis problem is ultimately reduced to solving multivariate polynomial systems with rational entries, which is handled using an SMT solver in their software.", "result": "The authors successfully designed and implemented an algorithm capable of characterizing and synthesizing a broad class of nondeterministic branching loops from arbitrary polynomial invariants and guard conditions. They introduced a new class of invariants enabling more efficient synthesis, and demonstrated their method can solve the problem by reducing it to multivariate polynomial systems solvable by SMT solvers.", "conclusion": "The paper extends invariant-based loop synthesis from restricted cases to general polynomial invariants and guards, enabling automated synthesis of complex loops. The use of algebraic geometry and SMT solvers makes the approach both general and computationally feasible, contributing significantly to formal program verification."}}
{"id": "2509.22908", "categories": ["cs.SE", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.22908", "abs": "https://arxiv.org/abs/2509.22908", "authors": ["Sergiu Bursuc", "Theodore Ehrenborg", "Shaowei Lin", "Lacramioara Astefanoaei", "Ionel Emilian Chiosa", "Jure Kukovec", "Alok Singh", "Oliver Butterley", "Adem Bizid", "Quinn Dougherty", "Miranda Zhao", "Max Tan", "Max Tegmark"], "title": "A benchmark for vericoding: formally verified program synthesis", "comment": "25 pages, 1 figure; data available at\n  https://github.com/Beneficial-AI-Foundation/vericoding-benchmark", "summary": "We present and test the largest benchmark for vericoding, LLM-generation of\nformally verified code from formal specifications - in contrast to vibe coding,\nwhich generates potentially buggy code from a natural language description. Our\nbenchmark contains 12,504 formal specifications, with 3,029 in Dafny, 2,334 in\nVerus/Rust and 7,141 in Lean. Of these, 6,174 are new unseen problems. We find\nvericoding success rates of 27% in Lean, 44% in Verus/Rust and 82% in Dafny\nusing off-the-shelf LLMs. Adding natural-language descriptions does not\nsignificantly improve performance. We also find that LLM progress has improved\nprogress on pure Dafny verification from 68% to 96% over the past year. The\nbenchmark and vericoding results are shared at\nhttps://github.com/Beneficial-AI-Foundation/vericoding-benchmark", "AI": {"tldr": "A new large benchmark for LLM-generated, formally verified code shows high verification rates in Dafny, moderate in Verus/Rust, and low in Lean. Performance has improved notably for Dafny. Natural language prompts offer little added value.", "motivation": "The aim is to shift from generating potentially buggy code via natural language prompts to generating provably correct code directly from formal specifications using LLMs, assessing the current capability and progress of LLMs in this area.", "method": "The authors created and tested a large benchmark consisting of 12,504 formal specifications in Dafny, Verus/Rust, and Lean. They evaluated off-the-shelf LLMs for their ability to generate formally verified code and measured success rates. They also analyzed performance improvements over time and the effects of adding natural-language descriptions.", "result": "Success rates for vericoding with LLMs were 82% for Dafny, 44% for Verus/Rust, and 27% for Lean. Adding natural language had negligible effect. LLM performance in Dafny verification has risen from 68% to 96% in the past year. The dataset and results have been made publicly available.", "conclusion": "LLMs can generate formally verified code from formal specifications, but success rates vary significantly across languages and benchmarks. Recent progress in LLMs has greatly improved verification rates for Dafny, but natural-language descriptions offer little performance boost."}}
{"id": "2509.22978", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22978", "abs": "https://arxiv.org/abs/2509.22978", "authors": ["Teeradaj Racharak", "Chaiyong Ragkhitwetsagul", "Chayanee Junplong", "Akara Supratak"], "title": "Towards Human-interpretable Explanation in Code Clone Detection using LLM-based Post Hoc Explainer", "comment": null, "summary": "Recent studies highlight various machine learning (ML)-based techniques for\ncode clone detection, which can be integrated into developer tools such as\nstatic code analysis. With the advancements brought by ML in code\nunderstanding, ML-based code clone detectors could accurately identify and\nclassify cloned pairs, especially semantic clones, but often operate as black\nboxes, providing little insight into the decision-making process. Post hoc\nexplainers, on the other hand, aim to interpret and explain the predictions of\nthese ML models after they are made, offering a way to understand the\nunderlying mechanisms driving the model's decisions. However, current post hoc\ntechniques require white-box access to the ML model or are computationally\nexpensive, indicating a need for advanced post hoc explainers. In this paper,\nwe propose a novel approach that leverages the in-context learning capabilities\nof large language models to elucidate the predictions made by the ML-based code\nclone detectors. We perform a study using ChatGPT-4 to explain the code clone\nresults inferred by GraphCodeBERT. We found that our approach is promising as a\npost hoc explainer by giving the correct explanations up to 98% and offering\ngood explanations 95% of the time. However, the explanations and the code line\nexamples given by the LLM are useful in some cases. We also found that lowering\nthe temperature to zero helps increase the accuracy of the explanation. Lastly,\nwe list the insights that can lead to further improvements in future work. This\nstudy paves the way for future studies in using LLMs as a post hoc explainer\nfor various software engineering tasks.", "AI": {"tldr": "This paper proposes using ChatGPT-4 to explain GraphCodeBERT code clone detection results, achieving high accuracy and usefulness. The study suggests LLMs are effective post hoc explainers and highlights directions for refinement and broader application in software engineering.", "motivation": "ML-based code clone detectors accurately identify and classify code clones, particularly semantic clones, but are often opaque and difficult to interpret. Current post hoc explainers are limited by the need for white-box access or heavy computational demands, leaving a gap for interpretable, practical solutions.", "method": "The authors use large language models, specifically ChatGPT-4, to explain the predictions of the GraphCodeBERT code clone detector, leveraging in-context learning to act as a post hoc explainer. They conduct a study to evaluate the effectiveness of this approach in providing understandable and correct explanations.", "result": "The approach achieved up to 98% accuracy in providing correct explanations and 95% in delivering good explanations. Setting the LLM temperature to zero further improved explanation accuracy. The usefulness of the explanations and code line examples varied depending on the case.", "conclusion": "Integrating large language models as post hoc explainers is promising for interpreting ML-based code clone detectors, addressing challenges of interpretability, and paving the way for their use in other software engineering tasks. Additional insights were identified for future improvement."}}
{"id": "2509.24515", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.24515", "abs": "https://arxiv.org/abs/2509.24515", "authors": ["Yu-Fu Fu", "Meng Xu", "Taesoo Kim"], "title": "Agentic Specification Generator for Move Programs", "comment": "18 pages; Extended version of ASE'25 paper with extra appendices", "summary": "While LLM-based specification generation is gaining traction, existing tools\nprimarily focus on mainstream programming languages like C, Java, and even\nSolidity, leaving emerging and yet verification-oriented languages like Move\nunderexplored. In this paper, we introduce MSG, an automated specification\ngeneration tool designed for Move smart contracts. MSG aims to highlight key\ninsights that uniquely present when applying LLM-based specification generation\nto a new ecosystem. Specifically, MSG demonstrates that LLMs exhibit robust\ncode comprehension and generation capabilities even for non-mainstream\nlanguages. MSG successfully generates verifiable specifications for 84% of\ntested Move functions and even identifies clauses previously overlooked by\nexperts. Additionally, MSG shows that explicitly leveraging specification\nlanguage features through an agentic, modular design improves specification\nquality substantially (generating 57% more verifiable clauses than conventional\ndesigns). Incorporating feedback from the verification toolchain further\nenhances the effectiveness of MSG, leading to a 30% increase in generated\nverifiable specifications.", "AI": {"tldr": "MSG is a new LLM-based tool for generating specifications for Move smart contracts. It shows that LLMs work well with less-common languages, achieves high coverage, creates more useful clauses than existing approaches, and its design and feedback mechanisms significantly boost specification quality.", "motivation": "While LLM-based specification generation is becoming popular, such tools mainly target widely used programming languages, neglecting emerging, verification-focused languages like Move used in smart contracts. Addressing this gap could extend these tools\u2019 benefits to new ecosystems.", "method": "The authors introduce MSG, an automated tool that applies LLMs for specification generation specifically for the Move programming language. MSG leverages an agentic, modular architecture, incorporates explicit use of specification language features, and integrates feedback from the verification toolchain.", "result": "MSG generated verifiable specifications for 84% of tested Move functions, exceeded expert-written clauses in some cases, produced 57% more verifiable clauses through its modular design, and achieved a 30% increase in verifiable specifications by using verification toolchain feedback.", "conclusion": "LLMs can be successfully leveraged for specification generation in non-mainstream, verification-oriented languages like Move. MSG\u2019s agentic, modular approach and feedback incorporation not only attains strong coverage but also enhances the quality and usefulness of generated specifications."}}
{"id": "2509.23261", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23261", "abs": "https://arxiv.org/abs/2509.23261", "authors": ["Fei Gu", "Zi Liang", "Hongzong LI", "Jiahao MA"], "title": "The Matthew Effect of AI Programming Assistants: A Hidden Bias in Software Evolution", "comment": null, "summary": "AI-assisted programming is rapidly reshaping software development, with large\nlanguage models (LLMs) enabling new paradigms such as vibe coding and agentic\ncoding. While prior works have focused on prompt design and code generation\nquality, the broader impact of LLM-driven development on the iterative dynamics\nof software engineering remains underexplored. In this paper, we conduct\nlarge-scale experiments on thousands of algorithmic programming tasks and\nhundreds of framework selection tasks to systematically investigate how\nAI-assisted programming interacts with the software ecosystem. Our analysis\nreveals \\textbf{a striking Matthew effect: the more popular a programming\nlanguage or framework, the higher the success rate of LLM-generated code}. The\nphenomenon suggests that AI systems may reinforce existing popularity\nhierarchies, accelerating convergence around dominant tools while hindering\ndiversity and innovation. We provide a quantitative characterization of this\neffect and discuss its implications for the future evolution of programming\necosystems.", "AI": {"tldr": "LLMs in programming tend to succeed more with popular languages and frameworks, risking further entrenchment of dominant tools and reducing ecosystem diversity.", "motivation": "The motivation is to understand the broader impact of AI-assisted programming, specifically by large language models (LLMs), on the iterative dynamics and ecosystem of software engineering, beyond just code generation quality.", "method": "The authors conducted large-scale experiments involving thousands of algorithmic programming tasks and hundreds of framework selection tasks to systematically analyze LLM interactions with the software ecosystem.", "result": "The results quantitatively show a Matthew effect: LLMs generate code with higher success rates for more popular programming languages and frameworks, potentially reinforcing existing popularity hierarchies.", "conclusion": "AI systems powered by LLMs may accelerate convergence around dominant languages and frameworks, which can hinder diversity and innovation in programming ecosystems."}}
{"id": "2509.23297", "categories": ["cs.SE", "cs.GR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.23297", "abs": "https://arxiv.org/abs/2509.23297", "authors": ["Anthony Savidis", "Christos Vasilopoulos"], "title": "Code Arcades: 3d Visualization of Classes, Dependencies and Software Metrics", "comment": null, "summary": "Software visualization seeks to represent software artifacts graphical-ly in\ntwo or three dimensions, with the goal of enhancing comprehension, anal-ysis,\nmaintenance, and evolution of the source code. In this context, visualiza-tions\nemploy graphical forms such as dependency structures, treemaps, or time-lines\nthat incorporate repository histories. These visualizations allow software\nengineers to identify structural patterns, detect complexity hotspots, and\ninfer system behaviors that are difficult to perceive directly from source\ntext. By adopting metaphor-based approaches, visualization tools provide\nmacroscopic overviews while enabling focused inspection of specific program\nelements, thus offering an accessible means of understanding large-scale\nsystems. The contri-bution of our work lies in three areas. First, we introduce\na configurable group-ing mechanism that supports flexible organization of code\nelements based on arbitrary relationships. Second, we combine fine-grained and\ncoarse-grained software metrics to provide a multi-level perspective on system\nproperties. Third, we present an interactive visualization engine that allows\ndevelopers to dynamically adjust rendering attributes. Collectively, these\nadvances provide a more adaptable and insightful approach to source code\ncomprehension.", "AI": {"tldr": "This paper presents a novel software visualization approach featuring configurable grouping, multi-level metrics, and interactive viewing, collectively advancing flexible and deep source code understanding.", "motivation": "Software visualization aims to aid in the comprehension, analysis, maintenance, and evolution of source code by representing software artifacts in graphical form, addressing the challenges of understanding large and complex software systems.", "method": "The paper introduces three main contributions: (1) a configurable grouping mechanism allowing flexible organization of code elements according to arbitrary relationships; (2) integration of both fine-grained and coarse-grained software metrics to achieve a multi-level perspective; and (3) an interactive visualization engine enabling real-time adjustment of visualization attributes.", "result": "The proposed system enables software engineers to identify structural patterns, complexity hotspots, and infer system behaviors, significantly improving the accessibility and depth of source code comprehension.", "conclusion": "By providing more adaptable and insightful software visualizations, the authors' approach helps developers better understand large-scale systems, facilitating improvements in analysis, maintenance, and evolution tasks."}}
{"id": "2509.23469", "categories": ["cs.SE", "68N30", "D.2.8"], "pdf": "https://arxiv.org/pdf/2509.23469", "abs": "https://arxiv.org/abs/2509.23469", "authors": ["Mykola Kuz", "Ivan Yaremiy", "Hanna Yaremii", "Mykola Pikuliak", "Ihor Lazarovych", "Mykola Kozlenko", "Denys Vekeryk"], "title": "Methods for evaluating software accessibility", "comment": "10 pages, 4 figures, 1 table", "summary": "The development and enhancement of methods for evaluating software\naccessibility is a relevant challenge in modern software engineering, as\nensuring equal access to digital services is a key factor in improving their\nefficiency and inclusivity. The increasing digitalization of society\nnecessitates the creation of software that complies with international\naccessibility standards such as ISO/IEC 25023 and WCAG. Adhering to these\nstandards helps eliminate barriers to software use for individuals with diverse\nphysical, sensory, and cognitive needs. Despite advancements in regulatory\nframeworks, existing accessibility evaluation methodologies are often\ngeneralized and fail to account for the specific needs of different user\ncategories or the unique ways they interact with digital systems. This\nhighlights the need for the development of new, more detailed methods for\ndefining metrics that influence the quality of user interaction with software\nproducts. Building a classification and mathematical model and developing\naccessibility assessment methods for software based on it. A method for\nassessing the quality subcharacteristic \"Accessibility\", which is part of the\n\"Usability\" quality characteristic, has been developed. This enabled the\nanalysis of a website's inclusivity for individuals with visual impairments,\nand the formulation of specific recommendations for further improvements, which\nis a crucial step toward creating an inclusive digital environment. Comparing\nto standardized approaches, a more detailed and practically oriented\naccessibility assessment methodology has been proposed. Using this methodology,\nan analysis of the accessibility of the main pages of Vasyl Stefanyk\nPrecarpathian National University's website was conducted, and improvements\nwere suggested to enhance its inclusivity.", "AI": {"tldr": "The paper proposes a new, detailed method for evaluating software accessibility, applies it to a university website, and recommends improvements for visually impaired users, advancing inclusivity beyond standard methods.", "motivation": "The paper is motivated by the challenge of evaluating software accessibility effectively to ensure equal access for users with diverse needs. Existing methods are often too generalized and do not cater to specific user categories or interaction modes.", "method": "The authors propose developing a classification and mathematical model for accessibility. They create a method for assessing the 'Accessibility' subcharacteristic within the broader 'Usability' quality metric, focusing on metrics tailored to users with visual impairments.", "result": "A more detailed and practical methodology for software accessibility assessment is developed and applied to analyze the inclusivity of Vasyl Stefanyk Precarpathian National University's website, with specific improvements recommended.", "conclusion": "The developed assessment method is more thorough and practical than standardized approaches, enabling more effective evaluation and improvement of web accessibility, especially for visually impaired users."}}
{"id": "2509.23586", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23586", "abs": "https://arxiv.org/abs/2509.23586", "authors": ["Yuan-An Xiao", "Pengfei Gao", "Chao Peng", "Yingfei Xiong"], "title": "Improving the Efficiency of LLM Agent Systems through Trajectory Reduction", "comment": "20 pages, 4 figures", "summary": "Multi-turn agent systems based on Large Language Models (LLMs) have been\nincreasingly popular for software engineering tasks. While LLM agents show\ndecent effectiveness, the high computational cost of input tokens due to the\never-growing trajectory remains an efficiency concern for their applications.\nEfficiency is largely neglected in existing studies and agent products, and\nthis paper fills the gap by introducing an inference-time trajectory reduction\napproach to reduce the cost of agents.\n  Through analyzing existing agent trajectories, we demonstrate that useless,\nredundant, and expired information is widespread in all trajectories, which can\nbe identified and reduced without harming the agent's performance. We then\ndesign a simple yet effective trajectory reduction approach, AgentDiet, which\nautomatically removes such waste information. We implement AgentDiet on a\ntop-performing coding agent, and the evaluation on two LLMs and two benchmarks\nshows that AgentDiet can reduce input tokens by 39.9% ~ 59.7%, or the final\ncomputational cost by 21.1% ~ 35.9%, while maintaining the same agent\nperformance. This indicates that trajectory reduction is a promising direction\nfor agent systems.", "AI": {"tldr": "The paper introduces AgentDiet, a method that reduces computational cost for multi-turn LLM agents by automatically removing redundant and expired information from their input trajectories, achieving significant efficiency gains without losing performance.", "motivation": "Multi-turn LLM agent systems are efficient for software engineering but suffer from high computational costs due to large input token trajectories. Previous research has largely overlooked efficiency concerns.", "method": "The authors analyzed agent trajectories to identify redundant, useless, and expired information. They developed AgentDiet, an inference-time trajectory reduction technique that automatically prunes wasteful information from agent inputs. AgentDiet was implemented on a top coding agent and tested on two LLMs and two benchmarks.", "result": "AgentDiet reduced input tokens by 39.9% to 59.7%, and overall computational cost by 21.1% to 35.9%, without affecting agent performance.", "conclusion": "Trajectory reduction, as implemented by AgentDiet, is a promising approach to improving computational efficiency in multi-turn LLM agent systems for software engineering without sacrificing effectiveness."}}
{"id": "2509.23645", "categories": ["cs.SE", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.23645", "abs": "https://arxiv.org/abs/2509.23645", "authors": ["A S M Shahadat Hossain", "Colin Brown", "David Koop", "Tanu Malik"], "title": "Similarity-Based Assessment of Computational Reproducibility in Jupyter Notebooks", "comment": "10 pages", "summary": "Computational reproducibility refers to obtaining consistent results when\nrerunning an experiment. Jupyter Notebook, a web-based computational notebook\napplication, facilitates running, publishing, and sharing computational\nexperiments along with their results. However, rerunning a Jupyter Notebook may\nnot always generate identical results due to various factors, such as\nrandomness, changes in library versions, or variations in the computational\nenvironment. This paper introduces the Similarity-based Reproducibility Index\n(SRI) -- a metric for assessing the reproducibility of results in Jupyter\nNotebooks. SRI employs novel methods developed based on similarity metrics\nspecific to different types of Python objects to compare rerun outputs against\noriginal outputs. For every cell generating an output in a rerun notebook, SRI\nreports a quantitative score in the range [0, 1] as well as some qualitative\ninsights to assess reproducibility. The paper also includes a case study in\nwhich the proposed metric is applied to a set of Jupyter Notebooks,\ndemonstrating how various similarity metrics can be leveraged to quantify\ncomputational reproducibility.", "AI": {"tldr": "This paper introduces the Similarity-based Reproducibility Index (SRI), a metric and tool for quantifying how reproducible Jupyter Notebook outputs are using similarity metrics. SRI delivers detailed scores and insights per output cell, and a case study demonstrates its utility in assessing reproducibility.", "motivation": "Although Jupyter Notebooks are widely used for sharing computational experiments, rerunning them does not always yield the same results due to factors such as randomness or changes in the computational environment, making reproducibility challenging to gauge.", "method": "The paper introduces the Similarity-based Reproducibility Index (SRI), which uses novel similarity metrics tailored to different Python object types to quantitatively and qualitatively evaluate the reproducibility of Jupyter Notebook outputs. Each output cell is scored between 0 and 1 based on how similar the rerun result is to the original. The paper also presents a case study applying SRI to multiple notebooks.", "result": "SRI provides both quantitative (a score between 0 and 1 for each output cell) and qualitative insights on reproducibility in Jupyter Notebooks. The case study shows that SRI can effectively use different similarity metrics to measure and compare computational reproducibility across notebooks.", "conclusion": "The Similarity-based Reproducibility Index is an effective tool for assessing and quantifying the reproducibility of computational experiments in Jupyter Notebooks, helping users understand and improve reproducibility with precise, output-specific feedback."}}
{"id": "2509.23675", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23675", "abs": "https://arxiv.org/abs/2509.23675", "authors": ["Xinyue Zuo", "Yifan Zhang", "Hongshu Wang", "Yufan Cai", "Zhe Hou", "Jing Sun", "Jin Song Dong"], "title": "PAT-Agent: Autoformalization for Model Checking", "comment": "Accepted in ASE 2025 (International Conference on Automated Software\n  Engineering)", "summary": "Recent advances in large language models (LLMs) offer promising potential for\nautomating formal methods. However, applying them to formal verification\nremains challenging due to the complexity of specification languages, the risk\nof hallucinated output, and the semantic gap between natural language and\nformal logic. We introduce PAT-Agent, an end-to-end framework for natural\nlanguage autoformalization and formal model repair that combines the generative\ncapabilities of LLMs with the rigor of formal verification to automate the\nconstruction of verifiable formal models. In PAT-Agent, a Planning LLM first\nextracts key modeling elements and generates a detailed plan using semantic\nprompts, which then guides a Code Generation LLM to synthesize syntactically\ncorrect and semantically faithful formal models. The resulting code is verified\nusing the Process Analysis Toolkit (PAT) model checker against user-specified\nproperties, and when discrepancies occur, a Repair Loop is triggered to\niteratively correct the model using counterexamples. To improve flexibility, we\nbuilt a web-based interface that enables users, particularly non-FM-experts, to\ndescribe, customize, and verify system behaviors through user-LLM interactions.\nExperimental results on 40 systems show that PAT-Agent consistently outperforms\nbaselines, achieving high verification success with superior efficiency. The\nablation studies confirm the importance of both planning and repair components,\nand the user study demonstrates that our interface is accessible and supports\neffective formal modeling, even for users with limited formal methods\nexperience.", "AI": {"tldr": "PAT-Agent uses LLMs to automatically create and repair formal models from natural language, verifies them, and helps users (including non-experts) build correct models more efficiently than baseline approaches.", "motivation": "Large language models (LLMs) have the potential to automate formal methods. However, using LLMs for formal verification is difficult due to complex specification languages, possible hallucinations, and a gap between natural language and formal logic.", "method": "The authors introduce PAT-Agent, an end-to-end framework that leverages LLMs for natural language autoformalization and formal model repair. PAT-Agent uses a Planning LLM to extract modeling elements and generate a plan, guiding a Code Generation LLM to produce formal models. PAT-Agent integrates model checking via Process Analysis Toolkit and includes a Repair Loop that iteratively corrects models using counterexamples, all accessible through a user-friendly web interface.", "result": "PAT-Agent outperforms existing baselines across 40 systems, achieving higher verification success and efficiency. Ablation studies confirm that planning and repair are essential components, while user studies show that the system enables non-experts to effectively perform formal modeling and verification.", "conclusion": "PAT-Agent successfully automates formal model construction and repair, making formal verification more accessible and efficient, especially for users without expertise in formal methods."}}
{"id": "2509.23679", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23679", "abs": "https://arxiv.org/abs/2509.23679", "authors": ["Zeqin Liao", "Yuhong Nan", "Zixu Gao", "Henglong Liang", "Sicheng Hao", "Jiajing Wu", "Zibin Zheng"], "title": "Satellite: Detecting and Analyzing Smart Contract Vulnerabilities caused by Subcontract Misuse", "comment": "This is the author version of the article accepted for publication in\n  IEEE Transactions on Software Engineering. The final version is available at\n  10.1109/TSE.2025.3613470", "summary": "Developers of smart contracts pervasively reuse subcontracts to improve\ndevelopment efficiency. Like any program language, such subcontract reuse may\nunexpectedly include, or introduce vulnerabilities to the end-point smart\ncontract. Unfortunately, automatically detecting such issues poses several\nunique challenges. Particularly, in most cases, smart contracts are compiled as\nbytecode, whose class-level information (e.g., inheritance, virtual function\ntable), and even semantics (e.g., control flow and data flow) are fully\nobscured as a single smart contract after compilation.\n  In this paper, we propose Satellite, a new bytecode-level static analysis\nframework for subcontract misuse vulnerability (SMV) detection in smart\ncontracts. Satellite incorporates a series of novel designs to enhance its\noverall effectiveness.. Particularly, Satellite utilizes a transfer learning\nmethod to recover the inherited methods, which are critical for identifying\nsubcontract reuse in smart contracts. Further, Satellite extracts a set of\nfine-grained method-level features and performs a method-level comparison, for\nidentifying the reuse part of subcontract in smart contracts. Finally,\nSatellite summarizes a set of SMV indicators according to their types, and\nhence effectively identifies SMVs. To evaluate Satellite, we construct a\ndataset consisting of 58 SMVs derived from real-world attacks and collect\nadditional 56 SMV patterns from SOTA studies. Experiment results indicate that\nSatellite exhibits good performance in identifying SMV, with a precision rate\nof 84.68% and a recall rate of 92.11%. In addition, Satellite successfully\nidentifies 14 new/unknown SMV over 10,011 real-world smart contracts, affecting\na total amount of digital assets worth 201,358 USD.", "AI": {"tldr": "Satellite is a new static analysis tool for detecting vulnerabilities introduced by subcontract reuse in smart contracts at the bytecode level. It uses transfer learning and method-level feature analysis, exhibiting high precision and recall, and has uncovered new security issues in real-world contracts.", "motivation": "Smart contract developers often reuse subcontracts to boost efficiency, but this practice can unintentionally introduce or spread vulnerabilities. Most smart contracts are distributed in bytecode form, making it hard to analyze issues due to obscured class-level and semantic information after compilation.", "method": "The paper proposes Satellite, a bytecode-level static analysis framework designed to detect subcontract misuse vulnerabilities (SMVs) in smart contracts. Satellite leverages transfer learning to recover inherited methods, extracts method-level features, compares them to identify reused parts, and summarizes SMV indicators by type. The framework is evaluated using a dataset of real-world SMVs and patterns.", "result": "Satellite demonstrated strong performance, achieving a precision rate of 84.68% and recall rate of 92.11% for SMV detection. It also identified 14 previously unknown SMVs in over 10,000 smart contracts, impacting digital assets amounting to more than $200,000.", "conclusion": "Satellite is an effective static analysis tool for detecting subcontract misuse vulnerabilities in smart contracts at the bytecode level, even uncovering previously unknown security issues in real-world deployments."}}
{"id": "2509.23806", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23806", "abs": "https://arxiv.org/abs/2509.23806", "authors": ["Chih-Duo Hong", "Yu Wang", "Yao-Chen Chang", "Fang Yu"], "title": "Influence-Guided Concolic Testing of Transformer Robustness", "comment": null, "summary": "Concolic testing for deep neural networks alternates concrete execution with\nconstraint solving to search for inputs that flip decisions. We present an\n{influence-guided} concolic tester for Transformer classifiers that ranks path\npredicates by SHAP-based estimates of their impact on the model output. To\nenable SMT solving on modern architectures, we prototype a solver-compatible,\npure-Python semantics for multi-head self-attention and introduce practical\nscheduling heuristics that temper constraint growth on deeper models. In a\nwhite-box study on compact Transformers under small $L_0$ budgets, influence\nguidance finds label-flip inputs more efficiently than a FIFO baseline and\nmaintains steady progress on deeper networks. Aggregating successful attack\ninstances with a SHAP-based critical decision path analysis reveals recurring,\ncompact decision logic shared across attacks. These observations suggest that\n(i) influence signals provide a useful search bias for symbolic exploration,\nand (ii) solver-friendly attention semantics paired with lightweight scheduling\nmake concolic testing feasible for contemporary Transformer models, offering\npotential utility for debugging and model auditing.", "AI": {"tldr": "This paper introduces SHAP-guided concolic testing tailored for Transformers. By ranking path predicates by their influence, and redesigning attention mechanisms for SMT solvers, they efficiently find adversarial examples and demonstrate scalability to deeper models. The method could help in debugging and auditing neural classifiers.", "motivation": "Concolic testing for neural networks is promising but struggles with deeper architectures like Transformers due to constraint growth and inefficient search for adversarial inputs.", "method": "They developed an 'influence-guided' concolic tester for Transformer classifiers that ranks path predicates using SHAP estimates to bias the search. They also designed solver-compatible Python semantics for multi-head self-attention and scheduling heuristics to control constraint growth.", "result": "Influence-guided concolic testing finds adversarial inputs (label flips) more efficiently than baseline FIFO search, maintains steady search progress for deeper models, and SHAP-based critical path analysis shows shared decision logic among attack instances.", "conclusion": "Influence signals (via SHAP) effectively guide symbolic exploration in concolic testing. Solver-friendly attention semantics and lightweight scheduling make concolic testing viable for real Transformer models, which can aid debugging and auditing. This approach is more efficient and scalable for deep architectures like Transformers."}}
{"id": "2509.23812", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23812", "abs": "https://arxiv.org/abs/2509.23812", "authors": ["Dianshu Liao", "Xin Yin", "Shidong Pan", "Chao Ni", "Zhenchang Xing", "Xiaoyu Sun"], "title": "Navigating the Labyrinth: Path-Sensitive Unit Test Generation with Large Language Models", "comment": null, "summary": "Unit testing is essential for software quality assurance, yet writing and\nmaintaining tests remains time-consuming and error-prone. To address this\nchallenge, researchers have proposed various techniques for automating unit\ntest generation, including traditional heuristic-based methods and more recent\napproaches that leverage large language models (LLMs). However, these existing\napproaches are inherently path-insensitive because they rely on fixed\nheuristics or limited contextual information and fail to reason about deep\ncontrol-flow structures. As a result, they often struggle to achieve adequate\ncoverage, particularly for deep or complex execution paths. In this work, we\npresent a path-sensitive framework, JUnitGenie, to fill this gap by combining\ncode knowledge with the semantic capabilities of LLMs in guiding context-aware\nunit test generation. After extracting code knowledge from Java projects,\nJUnitGenie distills this knowledge into structured prompts to guide the\ngeneration of high-coverage unit tests. We evaluate JUnitGenie on 2,258 complex\nfocal methods from ten real-world Java projects. The results show that\nJUnitGenie generates valid tests and improves branch and line coverage by\n29.60% and 31.00% on average over both heuristic and LLM-based baselines. We\nfurther demonstrate that the generated test cases can uncover real-world bugs,\nwhich were later confirmed and fixed by developers.", "AI": {"tldr": "JUnitGenie is a new framework for automating unit test generation in Java. It combines code understanding and LLMs to produce better tests, achieves much higher coverage, and helps find real bugs, outperforming previous approaches.", "motivation": "Unit testing is crucial for ensuring software quality, but manual creation and upkeep are costly and error-prone. Existing automated methods, relying on heuristics or LLMs, do not adequately cover complex control-flow paths in code.", "method": "The authors propose JUnitGenie, a path-sensitive framework that combines code knowledge extraction from Java projects with semantic capabilities of large language models. This framework uses structured prompts to guide context-aware, high-coverage unit test generation.", "result": "JUnitGenie is evaluated on 2,258 complex Java methods across ten real-world projects. It produces valid tests and increases branch coverage by 29.60% and line coverage by 31.00% compared to heuristic and LLM-only baselines. The tests generated also help identify actual bugs that developers subsequently fix.", "conclusion": "JUnitGenie effectively improves automated unit test generation by leveraging both code knowledge and LLM semantics, leading to superior coverage and the discovery of real-world bugs compared to prior methods."}}
{"id": "2509.23824", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23824", "abs": "https://arxiv.org/abs/2509.23824", "authors": ["Zhifan Ye", "Jiachi Chen", "Zhenzhe Shao", "Lingfeng Bao", "Xiaohu Yang", "Zhongxin Liu"], "title": "SolContractEval: A Benchmark for Evaluating Contract-Level Solidity Code Generation", "comment": null, "summary": "The rise of blockchain has brought smart contracts into mainstream use,\ncreating a demand for smart contract generation tools. While large language\nmodels (LLMs) excel at generating code in general-purpose languages, their\neffectiveness on Solidity, the primary language for smart contracts, remains\nunderexplored. Solidity constitutes only a small portion of typical LLM\ntraining data and differs from general-purpose languages in its\nversion-sensitive syntax and limited flexibility. These factors raise concerns\nabout the reliability of existing LLMs for Solidity code generation.\nCritically, existing evaluations, focused on isolated functions and synthetic\ninputs, fall short of assessing models' capabilities in real-world contract\ndevelopment.\n  To bridge this gap, we introduce SolContractEval, the first contract-level\nbenchmark for Solidity code generation. It comprises 124 tasks drawn from real\non-chain contracts across nine major domains. Each task input, consisting of\ncomplete context dependencies, a structured contract framework, and a concise\ntask prompt, is independently annotated and cross-validated by experienced\ndevelopers. To enable precise and automated evaluation of functional\ncorrectness, we also develop a dynamic evaluation framework based on historical\ntransaction replay. Building on SolContractEval, we perform a systematic\nevaluation of six mainstream LLMs. We find that Claude-3.7-Sonnet achieves the\nhighest overall performance, though evaluated models underperform relative to\ntheir capabilities on class-level generation tasks in general-purpose\nprogramming languages. Second, current models perform better on tasks that\nfollow standard patterns but struggle with complex logic and inter-contract\ndependencies. Finally, they exhibit limited understanding of Solidity-specific\nfeatures and contextual dependencies.", "AI": {"tldr": "The paper introduces SolContractEval, a benchmark for evaluating LLMs on real-world Solidity smart contract generation. Results show LLMs lag behind their performance with general-purpose languages, particularly with complex logic and Solidity-specific features, highlighting the need for further improvement.", "motivation": "The popularity of blockchain has increased the need for effective smart contract generation tools, but current large language models (LLMs) are underexplored in their ability to generate Solidity code, which is critical for smart contracts. Existing evaluations are insufficient as they focus on isolated functions and synthetic tasks, not real-world contract development.", "method": "The authors introduce SolContractEval, a contract-level benchmark for Solidity code generation, consisting of 124 tasks from actual on-chain contracts across various domains. Tasks include complete context, structured frameworks, and annotated guidance. They also create an automated evaluation framework based on historical transaction replay for functional correctness. Six mainstream LLMs are systematically assessed with this new benchmark.", "result": "Claude-3.7-Sonnet achieved the best overall performance among the tested LLMs, but all evaluated models performed worse on Solidity compared to general-purpose language tasks. Models handled standard patterns better than complex logic and inter-contract dependencies, and demonstrated limited understanding of Solidity-specific features and contextual requirements.", "conclusion": "While LLMs show promise for Solidity code generation, they are not yet as effective as they are for general-purpose programming languages, particularly in handling complex smart contract logic and unique Solidity features. SolContractEval provides a foundation for more accurate model evaluations and future advancements."}}
{"id": "2509.23835", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23835", "abs": "https://arxiv.org/abs/2509.23835", "authors": ["Yukai Zhao", "Menghan Wu", "Xing Hu", "Xin Xia"], "title": "HFuzzer: Testing Large Language Models for Package Hallucinations via Phrase-based Fuzzing", "comment": null, "summary": "Large Language Models (LLMs) are widely used for code generation, but they\nface critical security risks when applied to practical production due to\npackage hallucinations, in which LLMs recommend non-existent packages. These\nhallucinations can be exploited in software supply chain attacks, where\nmalicious attackers exploit them to register harmful packages. It is critical\nto test LLMs for package hallucinations to mitigate package hallucinations and\ndefend against potential attacks. Although researchers have proposed testing\nframeworks for fact-conflicting hallucinations in natural language generation,\nthere is a lack of research on package hallucinations. To fill this gap, we\npropose HFUZZER, a novel phrase-based fuzzing framework to test LLMs for\npackage hallucinations. HFUZZER adopts fuzzing technology and guides the model\nto infer a wider range of reasonable information based on phrases, thereby\ngenerating enough and diverse coding tasks. Furthermore, HFUZZER extracts\nphrases from package information or coding tasks to ensure the relevance of\nphrases and code, thereby improving the relevance of generated tasks and code.\nWe evaluate HFUZZER on multiple LLMs and find that it triggers package\nhallucinations across all selected models. Compared to the mutational fuzzing\nframework, HFUZZER identifies 2.60x more unique hallucinated packages and\ngenerates more diverse tasks. Additionally, when testing the model GPT-4o,\nHFUZZER finds 46 unique hallucinated packages. Further analysis reveals that\nfor GPT-4o, LLMs exhibit package hallucinations not only during code generation\nbut also when assisting with environment configuration.", "AI": {"tldr": "HFUZZER is a new fuzzing framework for detecting package hallucinations in code-generating LLMs. It finds more hallucinated packages and task diversity than previous methods, showing widespread vulnerabilities in popular models, especially during coding and configuration.", "motivation": "Large Language Models (LLMs) are extensively used for code generation, but they suffer from package hallucinations, where non-existent packages are recommended, posing severe security risks in production. These hallucinations can be exploited for supply chain attacks.", "method": "The authors propose HFUZZER, a novel phrase-based fuzzing framework to systematically test LLMs for package hallucinations. HFUZZER uses fuzzing technology driven by phrases extracted from package information or coding tasks, increasing the diversity and relevance of generated code tasks.", "result": "HFUZZER triggers package hallucinations in all evaluated LLMs. It outperforms mutational fuzzing frameworks, identifying 2.60x more unique hallucinated packages and producing more diverse tasks. For GPT-4o, it discovers 46 unique hallucinated packages and finds hallucinations occur during both code generation and environment configuration.", "conclusion": "HFUZZER effectively tests for package hallucinations, uncovering significant vulnerabilities across mainstream LLMs. Its phrase-based fuzzing approach is more effective than traditional mutational fuzzing, highlighting the need for robust testing and mitigation strategies before LLMs are used in production."}}
{"id": "2509.23961", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23961", "abs": "https://arxiv.org/abs/2509.23961", "authors": ["Sheikh Md Mushfiqur Rahman", "Nasir Eisty"], "title": "Learning-Based Testing for Deep Learning: Enhancing Model Robustness with Adversarial Input Prioritization", "comment": null, "summary": "Context: Deep Neural Networks (DNNs) are increasingly deployed in critical\napplications, where resilience against adversarial inputs is paramount.\nHowever, whether coverage-based or confidence-based, existing test\nprioritization methods often fail to efficiently identify the most\nfault-revealing inputs, limiting their practical effectiveness. Aims: This\nproject aims to enhance fault detection and model robustness in DNNs by\nintegrating Learning-Based Testing (LBT) with hypothesis and mutation testing\nto efficiently prioritize adversarial test cases. Methods: Our method selects a\nsubset of adversarial inputs with a high likelihood of exposing model faults,\nwithout relying on architecture-specific characteristics or formal\nverification, making it adaptable across diverse DNNs. Results: Our results\ndemonstrate that the proposed LBT method consistently surpasses baseline\napproaches in prioritizing fault-revealing inputs and accelerating fault\ndetection. By efficiently organizing test permutations, it uncovers all\npotential faults significantly faster across various datasets, model\narchitectures, and adversarial attack techniques. Conclusion: Beyond improving\nfault detection, our method preserves input diversity and provides effective\nguidance for model retraining, further enhancing robustness. These advantages\nestablish our approach as a powerful and practical solution for adversarial\ntest prioritization in real-world DNN applications.", "AI": {"tldr": "The paper introduces a Learning-Based Testing approach that, augmented with hypothesis and mutation testing, prioritizes adversarial test cases for DNNs. The approach is architecture-agnostic, improves fault detection speed and robustness, and is effective across various datasets and attack types.", "motivation": "Deep Neural Networks (DNNs) are used in critical applications that require robustness against adversarial inputs. Existing test prioritization methods are inefficient at quickly identifying inputs that reveal model faults, which reduces their effectiveness in practical scenarios.", "method": "The paper proposes integrating Learning-Based Testing (LBT) with hypothesis and mutation testing to prioritize adversarial test cases. The method selects a subset of adversarial inputs likely to reveal faults, without relying on specific model architectures or formal verification, making it broadly applicable.", "result": "The proposed LBT method outperforms baseline approaches by consistently prioritizing fault-revealing inputs and speeding up fault detection. It organizes test cases efficiently, uncovering faults faster across different datasets, models, and adversarial attacks.", "conclusion": "The method not only improves fault detection but also preserves input diversity and aids in model retraining, boosting DNN robustness. This makes it a practical and powerful solution for adversarial test prioritization in real-world applications."}}
{"id": "2509.24032", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24032", "abs": "https://arxiv.org/abs/2509.24032", "authors": ["Jialun Zhang", "Merve G\u00fclmez", "Thomas Nyman", "Gang Tan"], "title": "SandCell: Sandboxing Rust Beyond Unsafe Code", "comment": null, "summary": "Rust is a modern systems programming language that ensures memory safety by\nenforcing ownership and borrowing rules at compile time. While the unsafe\nkeyword allows programmers to bypass these restrictions, it introduces\nsignificant risks. Various approaches for isolating unsafe code to protect safe\nRust from vulnerabilities have been proposed, yet these methods provide only\nfixed isolation boundaries and do not accommodate expressive policies that\nrequire sandboxing both safe and unsafe code. This paper presents SandCell for\nflexible and lightweight isolation in Rust by leveraging existing syntactic\nboundaries. SandCell allows programmers to specify which components to sandbox\nwith minimal annotation effort, enabling fine-grained control over isolation.\nThe system also introduces novel techniques to minimize overhead when\ntransferring data between sandboxes. Our evaluation demonstrates SandCell's\neffectiveness in preventing vulnerabilities across various Rust applications\nwhile maintaining reasonable performance overheads.", "AI": {"tldr": "SandCell is a Rust isolation system that gives programmers flexible control to sandbox code, including both safe and unsafe portions. It uses simple annotations and novel data transfer methods to maintain low overhead and is proven effective at preventing vulnerabilities, offering a practical improvement over existing fixed-boundary Rust isolation tools.", "motivation": "Rust provides memory safety via ownership and borrowing, but 'unsafe' code can introduce vulnerabilities. Existing isolation methods in Rust only support fixed boundaries and cannot flexibly sandbox both safe and unsafe code, limiting security policies.", "method": "The paper develops SandCell, a system for flexible, lightweight isolation in Rust. It uses syntactic boundaries to allow developers to specify and sandbox components with minimal annotations, supporting fine-grained control. SandCell also introduces techniques to efficiently transfer data between sandboxes, minimizing overhead.", "result": "SandCell is shown to effectively prevent vulnerabilities in a range of Rust applications. The evaluation indicates it can do so while keeping performance overheads within reasonable levels.", "conclusion": "SandCell enables fine-grained and expressive isolation of both safe and unsafe Rust code via minimal programmer effort, providing better security without substantially impacting performance."}}
{"id": "2509.24091", "categories": ["cs.SE", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.24091", "abs": "https://arxiv.org/abs/2509.24091", "authors": ["Spandan Garg", "Roshanak Zilouchian Moghaddam"], "title": "PerfBench: Can Agents Resolve Real-World Performance Bugs?", "comment": null, "summary": "Performance bugs are inefficiencies in software that waste computational\nresources without causing functional failures, making them particularly\nchallenging to detect and fix. While recent advances in Software Engineering\nagents have shown promise in automated bug fixing, existing benchmarks\nprimarily focus on functional correctness and fail to evaluate agents'\nabilities to identify and resolve non-functional issues like performance bugs.\nWe introduce PerfBench, a benchmark comprising 81 real-world performance\nbug-fixing tasks from popular .NET repositories on GitHub. Unlike existing\nbenchmarks that rely on pre-existing test suites, PerfBench features a novel\nevaluation harness that allows agents to generate their own performance\nbenchmarks and validates fixes by comparing execution metrics collected for\ndeveloper fix and agent fix. Each task in PerfBench is derived from actual\ndeveloper fixes linked to performance-related issues, which are then verified\nby human experts, ensuring real-world relevance. Our evaluation reveals that\ncurrent state-of-the-art coding agents struggle with performance optimization\ntasks, with baseline OpenHands agent achieving only a ~3% success rate on our\nbenchmark. We develop OpenHands-Perf-Agent, which incorporates\nperformance-aware tooling and instructions and achieves a ~20% success rate on\nthe benchmark. We show that by ensuring the agent has proper instructions to\nbenchmark its changes and tooling for benchmark output processing, we can\nimprove the agent performance significantly, but room for improvement still\nremains. PerfBench provides a challenging test set for furthering the\ncapabilities of agents in fixing performance issues.", "AI": {"tldr": "PerfBench introduces a benchmark for performance bug-fixing tasks and shows current coding agents struggle to optimize code (~3\u201320% success rate). Adding performance-aware tools improves results, but much progress remains possible.", "motivation": "Performance bugs, although non-functional, waste resources and are difficult to detect/fix. Existing benchmarks focus only on functional correctness and overlook performance bugs, limiting the evaluation of automated bug-fixing agents.", "method": "Authors introduce PerfBench, a benchmark with 81 real-world performance bug-fixing tasks from .NET GitHub repositories. PerfBench uses a new evaluation system where agents generate and validate their own benchmarks by comparing execution metrics for both developer and agent fixes. Tasks are derived from verified developer fixes, ensuring real-world relevance.", "result": "State-of-the-art coding agents perform poorly on performance bug tasks: the baseline OpenHands achieves ~3% success, but a tailored agent, OpenHands-Perf-Agent, with performance-aware tooling/instructions, reaches ~20% success. Enhanced benchmarking and output processing significantly improve agent outcomes.", "conclusion": "PerfBench is a rigorous test set that exposes gaps in current agents\u2019 abilities to fix performance bugs and lays the foundation for further advancements by highlighting the importance of dedicated evaluation and performance-aware agent design."}}
{"id": "2509.24148", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24148", "abs": "https://arxiv.org/abs/2509.24148", "authors": ["Yiran Hu", "Nan Jiang", "Shanchao Liang", "Yi Wu", "Lin Tan"], "title": "TENET: Leveraging Tests Beyond Validation for Code Generation", "comment": null, "summary": "Test-Driven Development (TDD) is a widely adopted software engineering\npractice that requires developers to create and execute tests alongside code\nimplementation, ensuring that software behavior is continuously validated and\nrefined. In the era of vibe coding, where developers increasingly delegate code\nwriting to large language models (LLMs) by specifying high-level intentions,\nTDD becomes even more crucial, as test cases serve as executable specifications\nthat explicitly define and verify intended functionality beyond what\nnatural-language descriptions and code context can convey. While vibe coding\nunder TDD is promising, there are three main challenges: (1) selecting a small\nyet effective test suite to improve the generation accuracy and control the\nexecution workload, (2) retrieving context such as relevant code effectively,\nand (3) systematically using test feedback for effective code refinement. To\naddress these challenges, we introduce TENET, an LLM agent for generating\nfunctions in complex real-world repositories under the TDD setting. TENET\nfeatures three components: (1) a novel test harness mechanism that selects a\nconcise test suite to maximize diversity of target usage scenarios; (2) a\ntailored agent toolset that performs efficient retrieval of relevant code with\ninteractive debugging; and (3) a reflection-based refinement workflow that\niteratively analyzes failures, replenishes context, and applies code\nrefinement. TENET achieves 69.08% and 81.77% Pass@1 on RepoCod and RepoEval\nbenchmarks, outperforming the best agentic baselines by 9.49 and 2.17\npercentage points, respectively. In addition, this is the first study of\ntest-driven code generation with repository-level context, examining how\ndifferent aspects of test suites affect the performance of LLM agents under the\nTDD setting.", "AI": {"tldr": "TENET is a new agent for LLM-based, TDD-driven code generation that addresses key challenges in selecting test suites, retrieving context, and refining code. It outperforms previous approaches and provides the first in-depth analysis of test-driven code generation with repository-level context.", "motivation": "With the rise of 'vibe coding', where developers use LLMs to generate code from high-level intentions, TDD becomes more essential to define and verify desired functionality, going beyond vague natural-language instructions. However, key challenges exist such as selecting effective test suites, retrieving relevant context, and refining code effectively via test feedback.", "method": "The authors introduce TENET, an LLM agent for TDD-based code generation in large repositories. TENET has three main components: (1) a test harness for concise, diverse test suite selection, (2) a specialized retrieval and debugging agent toolset, and (3) a reflection-based workflow that iteratively refines code using test feedback.", "result": "TENET achieves state-of-the-art results with 69.08% Pass@1 on RepoCod and 81.77% on RepoEval benchmarks, outperforming best agentic baselines by 9.49 and 2.17 percentage points, respectively. The study also systematically analyzes the impact of test suite design on code generation performance in repository-level context.", "conclusion": "TENET significantly enhances the effectiveness of LLM-based code generation under TDD in real-world repositories, surpassing existing agentic methods and providing new insights into the relationship between test suite features and agent performance."}}
{"id": "2509.24215", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.24215", "abs": "https://arxiv.org/abs/2509.24215", "authors": ["Wenxuan Wang", "Yongjiang Wu", "Junyuan Zhang", "Shuqing Li", "Yun Peng", "Wenting Chen", "Shuai Wang", "Michael R. Lyu"], "title": "Metamorphic Testing for Audio Content Moderation Software", "comment": "Accepted by ASE 2025", "summary": "The rapid growth of audio-centric platforms and applications such as WhatsApp\nand Twitter has transformed the way people communicate and share audio content\nin modern society. However, these platforms are increasingly misused to\ndisseminate harmful audio content, such as hate speech, deceptive\nadvertisements, and explicit material, which can have significant negative\nconsequences (e.g., detrimental effects on mental health). In response,\nresearchers and practitioners have been actively developing and deploying audio\ncontent moderation tools to tackle this issue. Despite these efforts, malicious\nactors can bypass moderation systems by making subtle alterations to audio\ncontent, such as modifying pitch or inserting noise. Moreover, the\neffectiveness of modern audio moderation tools against such adversarial inputs\nremains insufficiently studied. To address these challenges, we propose MTAM, a\nMetamorphic Testing framework for Audio content Moderation software.\nSpecifically, we conduct a pilot study on 2000 audio clips and define 14\nmetamorphic relations across two perturbation categories: Audio Features-Based\nand Heuristic perturbations. MTAM applies these metamorphic relations to toxic\naudio content to generate test cases that remain harmful while being more\nlikely to evade detection. In our evaluation, we employ MTAM to test five\ncommercial textual content moderation software and an academic model against\nthree kinds of toxic content. The results show that MTAM achieves up to 38.6%,\n18.3%, 35.1%, 16.7%, and 51.1% error finding rates (EFR) when testing\ncommercial moderation software provided by Gladia, Assembly AI, Baidu,\nNextdata, and Tencent, respectively, and it obtains up to 45.7% EFR when\ntesting the state-of-the-art algorithms from the academy.", "AI": {"tldr": "Audio moderation systems on popular platforms are vulnerable to adversarially modified toxic content. The proposed MTAM framework generates such adversarial examples, revealing that commercial and academic moderation tools frequently fail to detect them, with error finding rates up to 51.1%. Robustness of current moderation solutions must be improved.", "motivation": "The motivation behind this paper lies in the increasing misuse of audio-centric platforms (like WhatsApp and Twitter) to spread harmful audio content, such as hate speech and deceptive ads. Current moderation tools can often be evaded via subtle audio modifications, and their robustness against such tactics is not well-studied. This paper aims to address this gap.", "method": "The paper proposes MTAM, a Metamorphic Testing framework for Audio moderation software, defining 14 metamorphic relations (across audio features-based and heuristic perturbations). MTAM systematically generates adversarial test cases from toxic audio by introducing modifications that preserve harmfulness but are more likely to bypass moderation. The framework is then used to test commercial and academic moderation tools against three toxic content types.", "result": "MTAM was applied to five commercial moderation systems (Gladia, Assembly AI, Baidu, Nextdata, Tencent) and an academic model, finding high error rates (up to 51.1% EFR for Tencent, and up to 45.7% for the academic model), demonstrating that contemporary audio moderation tools are vulnerable to adversarial attacks generated by MTAM.", "conclusion": "The study concludes that current commercial and academic audio moderation tools are insufficiently robust against adversarially modified toxic audio. MTAM effectively reveals these weaknesses, highlighting an urgent need for developing more resilient moderation systems."}}
{"id": "2509.24344", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24344", "abs": "https://arxiv.org/abs/2509.24344", "authors": ["Theo Koraag", "Niklas Wagner", "Felix Dobslaw", "Lucas Gren"], "title": "Comparing Open-Source and Commercial LLMs for Domain-Specific Analysis and Reporting: Software Engineering Challenges and Design Trade-offs", "comment": null, "summary": "Context: Large Language Models (LLMs) enable automation of complex natural\nlanguage processing across domains, but research on domain-specific\napplications like Finance remains limited. Objectives: This study explored\nopen-source and commercial LLMs for financial report analysis and commentary\ngeneration, focusing on software engineering challenges in implementation.\nMethods: Using Design Science Research methodology, an exploratory case study\niteratively designed and evaluated two LLM-based systems: one with local\nopen-source models in a multi-agent workflow, another using commercial GPT-4o.\nBoth were assessed through expert evaluation of real-world financial reporting\nuse cases. Results: LLMs demonstrated strong potential for automating financial\nreporting tasks, but integration presented significant challenges. Iterative\ndevelopment revealed issues including prompt design, contextual dependency, and\nimplementation trade-offs. Cloud-based models offered superior fluency and\nusability but raised data privacy and external dependency concerns. Local\nopen-source models provided better data control and compliance but required\nsubstantially more engineering effort for reliability and usability.\nConclusion: LLMs show strong potential for financial reporting automation, but\nsuccessful integration requires careful attention to architecture, prompt\ndesign, and system reliability. Implementation success depends on addressing\ndomain-specific challenges through tailored validation mechanisms and\nengineering strategies that balance accuracy, control, and compliance.", "AI": {"tldr": "The paper explores the use of both open-source and commercial LLMs for automating financial report analysis. While LLMs show strong promise, especially cloud-based models for fluency, practical deployment faces integration, privacy, and reliability challenges. Tailored engineering and validation are crucial for success in financial contexts.", "motivation": "Research in applying LLMs to domain-specific areas such as Finance is limited; automating financial report analysis is important for efficiency and accuracy, but presents technical engineering challenges.", "method": "Design Science Research methodology was used in an exploratory case study, designing and evaluating two LLM-based systems\u2014one with open-source local models in a multi-agent workflow and another with commercial GPT-4o. Expert evaluation was conducted on real-world financial reporting use cases.", "result": "LLMs have high potential for automating financial reporting tasks. Cloud-based models (GPT-4o) are more fluent and usable but pose data privacy concerns, while open-source local models offer better data control and compliance at the cost of higher engineering effort. Key challenges include prompt design, contextual dependency, and integration trade-offs.", "conclusion": "While LLMs are promising for financial reporting automation, successful integration requires architectural care, robust prompt design, and reliable systems. Domain-specific validation and tailored engineering strategies are necessary to balance accuracy, control, and compliance."}}
{"id": "2509.24347", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24347", "abs": "https://arxiv.org/abs/2509.24347", "authors": ["Junjie Meng", "Jie An", "Yong Li", "Andrea Turrini", "Fanjiang Xu", "Naijun Zhan", "Miaomiao Zhang"], "title": "Efficient Decomposition Identification of Deterministic Finite Automata from Examples", "comment": null, "summary": "The identification of deterministic finite automata (DFAs) from labeled\nexamples is a cornerstone of automata learning, yet traditional methods focus\non learning monolithic DFAs, which often yield a large DFA lacking simplicity\nand interoperability. Recent work addresses these limitations by exploring DFA\ndecomposition identification problems (DFA-DIPs), which model system behavior\nas intersections of multiple DFAs, offering modularity for complex tasks.\nHowever, existing DFA-DIP approaches depend on SAT encodings derived from\nAugmented Prefix Tree Acceptors (APTAs), incurring scalability limitations due\nto their inherent redundancy.\n  In this work, we advance DFA-DIP research through studying two variants: the\ntraditional Pareto-optimal DIP and the novel states-optimal DIP, which\nprioritizes a minimal number of states. We propose a novel framework that\nbridges DFA decomposition with recent advancements in automata representation.\nOne of our key innovations replaces APTA with 3-valued DFA (3DFA) derived\ndirectly from labeled examples. This compact representation eliminates\nredundancies of APTA, thus drastically reducing variables in the improved SAT\nencoding. Experimental results demonstrate that our 3DFA-based approach\nachieves significant efficiency gains for the Pareto-optimal DIP while enabling\na scalable solution for the states-optimal DIP.", "AI": {"tldr": "By switching from APTA to compact 3DFA representations for SAT encoding, the authors dramatically enhance the scalability and efficiency of automata decomposition learning, enabling practical solutions to complex DFA identification tasks.", "motivation": "Traditional deterministic finite automata (DFA) learning often produces large, monolithic automata that lack simplicity and modularity, which are important for understanding and managing complex systems. Previous attempts to decompose DFA models (DFA-DIPs) were hampered by unscalable SAT encodings due to redundancy in Augmented Prefix Tree Acceptors (APTAs).", "method": "The paper proposes a new framework that replaces the use of APTA with a more compact 3-valued DFA (3DFA) representation, derived directly from labeled examples, to serve as the basis for SAT encodings. This approach is applied to both the classic Pareto-optimal DFA decomposition identification problem (DIP) and a new variant, the states-optimal DIP prioritizing minimal state numbers.", "result": "The 3DFA-based encoding significantly reduces the number of variables in the SAT-based approach, making the method more scalable and efficient than previous APTA-based techniques.", "conclusion": "The new 3DFA-based framework improves scalability and efficiency in solving DFA decomposition identification problems, particularly for the Pareto-optimal and states-optimal variants. It outperforms previous methods relying on APTA-based encoding."}}
{"id": "2509.24352", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24352", "abs": "https://arxiv.org/abs/2509.24352", "authors": ["Minghua He", "Tong Jia", "Chiming Duan", "Pei Xiao", "Lingzhe Zhang", "Kangjin Wang", "Yifan Wu", "Ying Li", "Gang Huang"], "title": "Walk the Talk: Is Your Log-based Software Reliability Maintenance System Really Reliable?", "comment": "Accepted by ASE 2025 (NIER Track)", "summary": "Log-based software reliability maintenance systems are crucial for sustaining\nstable customer experience. However, existing deep learning-based methods\nrepresent a black box for service providers, making it impossible for providers\nto understand how these methods detect anomalies, thereby hindering trust and\ndeployment in real production environments. To address this issue, this paper\ndefines a trustworthiness metric, diagnostic faithfulness, for models to gain\nservice providers' trust, based on surveys of SREs at a major cloud provider.\nWe design two evaluation tasks: attention-based root cause localization and\nevent perturbation. Empirical studies demonstrate that existing methods perform\npoorly in diagnostic faithfulness. Consequently, we propose FaithLog, a\nfaithful log-based anomaly detection system, which achieves faithfulness\nthrough a carefully designed causality-guided attention mechanism and\nadversarial consistency learning. Evaluation results on two public datasets and\none industrial dataset demonstrate that the proposed method achieves\nstate-of-the-art performance in diagnostic faithfulness.", "AI": {"tldr": "Current deep learning log anomaly detectors are black boxes that providers can't trust. This paper introduces a new trust metric and method (FaithLog) with mechanisms for better explainability, achieving top performance in faithfulness on benchmark datasets.", "motivation": "Deep learning-based log anomaly detection is effective but lacks interpretability, preventing service providers from trusting and deploying these systems in critical environments. There is a need for more trustworthy, explainable anomaly detection methods.", "method": "The paper introduces a new metric, 'diagnostic faithfulness', to assess trustworthiness of anomaly detection models. They design two evaluation tasks: attention-based root cause localization and event perturbation. FaithLog, a new system, leverages a causality-guided attention mechanism and adversarial consistency learning to improve faithfulness.", "result": "Empirical studies show current methods perform poorly in diagnostic faithfulness. FaithLog, tested on two public and one industrial dataset, achieves state-of-the-art results for diagnostic faithfulness.", "conclusion": "FaithLog establishes a new benchmark for trustworthy, explainable log-based anomaly detection, addressing a key barrier to real-world deployment and operator trust."}}
{"id": "2509.24364", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24364", "abs": "https://arxiv.org/abs/2509.24364", "authors": ["Minghua He", "Chiming Duan", "Pei Xiao", "Tong Jia", "Siyu Yu", "Lingzhe Zhang", "Weijie Hong", "Jin Han", "Yifan Wu", "Ying Li", "Gang Huang"], "title": "United We Stand: Towards End-to-End Log-based Fault Diagnosis via Interactive Multi-Task Learning", "comment": "ASE 2025 (Research Track)", "summary": "Log-based fault diagnosis is essential for maintaining software system\navailability. However, existing fault diagnosis methods are built using a\ntask-independent manner, which fails to bridge the gap between anomaly\ndetection and root cause localization in terms of data form and diagnostic\nobjectives, resulting in three major issues: 1) Diagnostic bias accumulates in\nthe system; 2) System deployment relies on expensive monitoring data; 3) The\ncollaborative relationship between diagnostic tasks is overlooked. Facing this\nproblems, we propose a novel end-to-end log-based fault diagnosis method,\nChimera, whose key idea is to achieve end-to-end fault diagnosis through\nbidirectional interaction and knowledge transfer between anomaly detection and\nroot cause localization. Chimera is based on interactive multi-task learning,\ncarefully designing interaction strategies between anomaly detection and root\ncause localization at the data, feature, and diagnostic result levels, thereby\nachieving both sub-tasks interactively within a unified end-to-end framework.\nEvaluation on two public datasets and one industrial dataset shows that Chimera\noutperforms existing methods in both anomaly detection and root cause\nlocalization, achieving improvements of over 2.92% - 5.00% and 19.01% - 37.09%,\nrespectively. It has been successfully deployed in production, serving an\nindustrial cloud platform.", "AI": {"tldr": "Chimera is an end-to-end log-based fault diagnosis method that bridges anomaly detection and root cause localization using interactive multi-task learning, yielding notable accuracy improvements and practical deployment success.", "motivation": "Log-based fault diagnosis is crucial for software system reliability, but current approaches do not connect the tasks of anomaly detection and root cause localization. This disconnection causes diagnostic bias, reliance on costly data, and ignores collaboration between tasks.", "method": "The paper proposes Chimera, a novel end-to-end log-based fault diagnosis method using interactive multi-task learning. Chimera enables bidirectional interaction and knowledge transfer between anomaly detection and root cause localization at data, feature, and result levels within a unified framework.", "result": "On two public and one industrial dataset, Chimera shows significant performance improvements: 2.92%-5.00% higher in anomaly detection, and 19.01%-37.09% higher in root cause localization compared to existing techniques. It has also been deployed on an industrial cloud platform.", "conclusion": "Chimera effectively closes the gap between anomaly detection and root cause localization, leading to better diagnostic performance and successful real-world deployment."}}
{"id": "2509.24380", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24380", "abs": "https://arxiv.org/abs/2509.24380", "authors": ["Shuiguang Deng", "Hailiang Zhao", "Ziqi Wang", "Guanjie Cheng", "Peng Chen", "Wenzhuo Qian", "Zhiwei Ling", "Jianwei Yin", "Albert Y. Zomaya", "Schahram Dustdar"], "title": "Agentic Services Computing", "comment": null, "summary": "The rise of LLM-powered agents is driving a fundamental transformation in\nservices computing: from static, request-response functions to dynamic,\ngoal-oriented, and autonomous multi-agent ecosystems. In response to this\nshift, we introduce Agentic Service Computing (ASC), a new paradigm that\nreimagines services as intelligent, self-adaptive, and socially embedded\nentities. This comprehensive survey presents a lifecycle-driven framework for\nASC, structured around four core phases: Design, Deployment, Operation, and\nEvolution. We systematically analyze ASC through four foundational research\ndimensions: (1) Perception, Context, and Environment Modeling, (2) Autonomous\nDecision-Making and Task Execution, (3) Multi-Agent Collaboration and\nOrganization, and (4) Evaluation, Value Alignment, and Trustworthiness. We\nexamine how these dimensions are instantiated, integrated, and continuously\nadapted across the service lifecycle. Our synthesis reveals that agentic\nservices are not merely assembled but orchestrated: contextual awareness\nenables robust deployment; autonomous reasoning supports real-time operation;\ncollaborative structures emerge and evolve through interaction; and\ntrustworthiness must be upheld as a cross-cutting, lifelong imperative. We\nfurther identify and discuss emerging trends shaping the future of ASC. By\nintegrating classical principles of services computing with advances in\nLLM-based multi-agent systems, this work establishes a holistic and\nforward-looking foundation for ASC. It provides a unified reference for\nresearchers and practitioners aiming to develop adaptive, accountable, and\nhuman-centered intelligent services.", "AI": {"tldr": "This survey redefines services computing for the era of LLM-powered agents, introducing Agentic Service Computing (ASC), a lifecycle-based framework merging classical and modern approaches. It systematically reviews perception, autonomy, collaboration, and trust across the service lifecycle, and offers a reference for building future intelligent, adaptive, and trustworthy agent-based services.", "motivation": "The motivation behind this paper is the fundamental transformation occurring in services computing, driven by the rise of large language model (LLM)-powered agents. Traditional request-response architectures are shifting toward dynamic, goal-oriented, and autonomous multi-agent ecosystems, necessitating new paradigms and frameworks to develop and manage these intelligent services.", "method": "The method adopted in this paper is a comprehensive survey and the proposal of a lifecycle-driven framework for Agentic Service Computing (ASC). The framework is organized around four phases\u2014Design, Deployment, Operation, and Evolution\u2014and analyzes ASC through four research dimensions: perception and environment modeling, autonomous decision-making, multi-agent collaboration, and evaluation/trustworthiness.", "result": "The paper synthesizes current research and identifies that agentic services are orchestrated rather than simply assembled. Key findings include the importance of contextual awareness in deployment, autonomous reasoning for operations, emergence and evolution of collaborative structures, and the need for ongoing trust and value alignment. The review also highlights emerging trends and integrates classical service principles with modern multi-agent system advances.", "conclusion": "Agentic Service Computing (ASC) is a promising, holistic paradigm that combines classic services computing with LLM-based multi-agent systems. The paper provides a foundational reference for designing adaptive, trustworthy, and human-centric intelligent services, guiding researchers and practitioners in the development and management of such systems."}}
{"id": "2509.24419", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24419", "abs": "https://arxiv.org/abs/2509.24419", "authors": ["Yuanhe Zhang", "Zhiquan Yang", "Shengyi Pan", "Zhongxin Liu"], "title": "Unit Test Update through LLM-Driven Context Collection and Error-Type-Aware Refinement", "comment": null, "summary": "Unit testing is critical for ensuring software quality and software system\nstability. The current practice of manually maintaining unit tests suffers from\nlow efficiency and the risk of delayed or overlooked fixes. Therefore, an\nautomated approach is required to instantly update unit tests, with the\ncapability to both repair and enhance unit tests. However, existing automated\ntest maintenance methods primarily focus on repairing broken tests, neglecting\nthe scenario of enhancing existing tests to verify new functionality.\nMeanwhile, due to their reliance on rule-based context collection and the lack\nof verification mechanisms, existing approaches struggle to handle complex code\nchanges and often produce test cases with low correctness. To address these\nchallenges, we propose TESTUPDATER, a novel LLM based approach that enables\nautomated just-in-time test updates in response to production code changes.\nTESTUPDATER first leverages the LLM to analyze code changes and identify\nrelevant context, which it then extracts and filters. Then, through carefully\ndesigned prompts, TESTUPDATER guides the LLM step by step to handle various\ntypes of code changes and introduce new dependencies, enabling both test repair\nand enhancement. Finally, we introduce an error-type-aware iterative refinement\nmechanism that executes the LLM-updated tests and repairs failures, which\nsignificantly improves the overall correctness of test updates. Since existing\ntest repair datasets lack scenarios of test enhancement, we further construct a\nnew benchmark, UPDATES4J, with 195 real-world samples from 7 projects.\nExperimental results show that TESTUPDATER achieves a compilation pass rate of\n94.4% and a test pass rate of 86.7%, outperforming the state-of-the-art method\nSYNTER by 15.9% and 20.0%, respectively. Furthermore, TESTUPDATER exhibits\n12.9% higher branch coverage and 15.2% greater line coverage than SYNTER.", "AI": {"tldr": "Maintaining unit tests manually is slow and error-prone. TESTUPDATER, an LLM-based system, automates both repair and enhancement of tests for updated code, outperforming existing solutions in correctness and test coverage, as shown on a new real-world benchmark.", "motivation": "Unit testing is important for software quality, but manual maintenance is inefficient and can miss necessary fixes. Existing automated approaches mostly focus on repairing broken tests and rarely address enhancing tests to cover new features, while also struggling with accuracy due to rigid context collection methods.", "method": "The authors propose TESTUPDATER, a large language model (LLM) based system that analyzes code changes, extracts and filters relevant context, uses step-by-step prompting for various code change types, and employs an error-type-aware iterative refinement process to repair and enhance unit tests just-in-time. The method also introduces a new benchmark, UPDATES4J.", "result": "TESTUPDATER achieves a compilation pass rate of 94.4%, test pass rate of 86.7%, and outperforms SYNTER (the state-of-the-art) by 15.9% and 20.0% respectively. It also delivers 12.9% higher branch coverage and 15.2% greater line coverage than SYNTER.", "conclusion": "TESTUPDATER is an effective automated tool for just-in-time updating, repairing, and enhancing unit tests in response to production code changes. It improves both the correctness and coverage of test suites over current methods."}}
{"id": "2509.24485", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24485", "abs": "https://arxiv.org/abs/2509.24485", "authors": ["Vlad Stirbu", "Mateen Ahmed Abbasi", "Teerath Das", "Jesse Haimi", "Niko Iljin", "Pyry Kotilainen", "Petrus Lipsanen", "Niko M\u00e4kitalo", "Maiju Sipil\u00e4", "Venla Veijalainen", "Tommi Mikkonen"], "title": "Towards Shift-Up: A Framework and a Prestudy on High-Value Activities in GenAI Native Software Development", "comment": null, "summary": "Generative AI (GenAI) has significantly influenced software engineering.\nAssociated tools have created a shift in software engineering, where\nspecialized agents, based on user-provided prompts, are replacing human\ndevelopers. In this paper, we propose a framework for GenAI native development\nthat we call \\textit{shift-up}, which helps software teams focus on high-value\nwork while being supported by GenAI. Furthermore, we also present a preliminary\nstudy testing these ideas with current GenAI tools. Towards the end of the\npaper, we propose future research goals to study shift-up in more detail.", "AI": {"tldr": "The paper introduces 'shift-up,' a GenAI-focused framework to help software teams optimize work by leveraging AI agents, supported by a preliminary study and setting the stage for future research.", "motivation": "The motivation is to address the evolving role of GenAI in software engineering, exploring how AI-powered specialized agents are changing traditional development practices and how teams can adapt to focus on high-value tasks.", "method": "The authors propose a new framework called 'shift-up' for GenAI native development and conduct a preliminary study using current GenAI tools to test its feasibility and impact.", "result": "A framework called 'shift-up' is presented, aiming to help software teams leverage GenAI for increased focus on valuable work, supported by preliminary findings from experimental use of GenAI tools.", "conclusion": "GenAI is fundamentally transforming software engineering, and the 'shift-up' framework offers a structured approach for teams to maximize productivity by reallocating focus to higher-value tasks. The paper encourages further research to validate and refine this approach."}}
{"id": "2509.24498", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24498", "abs": "https://arxiv.org/abs/2509.24498", "authors": ["Zhihao Li", "Chaozheng Wang", "Zongjie Li", "Xinyong Peng", "Zelin Su", "Qun Xia", "Haochuan Lu", "Ting Xiong", "Man Ho Lam", "Shuzheng Gao", "Yuchong Xie", "Cuiyun Gao", "Shuai Wang", "Yuetang Deng", "Huafeng Ma"], "title": "JSProtect: A Scalable Obfuscation Framework for Mini-Games in WeChat", "comment": "10 pages", "summary": "The WeChat mini-game ecosystem faces rampant intellectual property theft to\nother platforms via secondary development, yet existing JavaScript obfuscation\ntools are ill-equipped for large-scale applications, suffering from prohibitive\nprocessing times, severe runtime performance degradation, and unsustainable\ncode size inflation. This paper introduces JSProtect, a high-throughput\nparallelized obfuscation framework designed to overcome these fundamental\nlimitations. At the core of our framework is the Parallel-Aware Scope Analysis\n(PASA) algorithm, which enables two key optimizations: independent code\npartitioning for multi-core processing and independent namespace management\nthat aggressively reuses short identifiers to combat code bloat. Our evaluation\ndemonstrates that JSProtectprocesses 20MB codebases in minutes, maintaining\n100\\% semantic equivalence while controlling code size inflation to as low as\n20\\% compared to over 1,000\\% with baseline tools. Furthermore, it preserves\nnear-native runtime performance and provides superior security effectiveness\nagainst both static analysis tools and large language models. This work\npresents a new paradigm for industrial-scale JavaScript protection that\neffectively balances robust security with high performance and scalability.", "AI": {"tldr": "JSProtect is a new, fast, and scalable JavaScript obfuscation framework for WeChat mini-games that avoids huge code inflation and performance loss; it delivers strong security with minimal impact on runtime and code size, outperforming existing tools by a wide margin.", "motivation": "The WeChat mini-game ecosystem experiences intellectual property theft through secondary development on other platforms. Existing JavaScript obfuscation tools cannot efficiently protect large-scale applications due to slow processing speeds, runtime performance issues, and excessive code size growth.", "method": "This paper introduces JSProtect, a parallelized obfuscation framework using the Parallel-Aware Scope Analysis (PASA) algorithm. This algorithm supports independent code partitioning for multi-core processing and efficient namespace management that reuses short identifiers to minimize code bloat.", "result": "JSProtect can obfuscate 20MB JavaScript codebases in minutes, maintains 100% semantic equivalence, limits code size inflation to around 20% (far better than over 1,000% with traditional tools), and keeps runtime performance near-native. It also effectively defends against static analysis and large language models.", "conclusion": "JSProtect provides a new paradigm for large-scale JavaScript protection, achieving a balance between enhanced security, scalability, and high performance."}}
{"id": "2509.24507", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24507", "abs": "https://arxiv.org/abs/2509.24507", "authors": ["Qinglin Wang", "Zhihong Sun", "Ruyun Wang", "Tao Huang", "Zhi Jin", "Ge Li", "Chen Lyu"], "title": "SemGuard: Real-Time Semantic Evaluator for Correcting LLM-Generated Code", "comment": "Accepted by the 40th IEEE/ACM Automated Software Engineering\n  Conference (ASE 2025)", "summary": "Large Language Models (LLMs) can translate natural language requirements into\ncode, yet empirical analyses of representative models reveal that semantic\nerrors-programs that compile but behave incorrectly-constitute the majority of\nobserved faults (e.g., >60% on DeepSeek-Coder-6.7B and QwenCoder-7B). Post-hoc\nrepair pipelines detect such faults only after execution, incurring latency,\nrelying on incomplete test suites, and often mis-localizing the defect. Since\nsemantic drift originates in the autoregressive decoding process, intervening\nwhile the code is being generated is a direct way to stop error propagation.\nConstrained-decoding approaches such as ROCODE attempt this, but still wait\nuntil the entire program runs to obtain feedback and use entropy heuristics\nthat do not truly capture semantics. A more effective solution must inject\nsemantic signals-early and precisely-into the decoding process.We present\nSemGuard, a semantic-evaluator-driven framework that performs real-time,\nline-level semantic supervision. To train the evaluator, we build SemDiff, the\nfirst dataset with fine-grained annotations that mark the exact line where a\ncorrect and an incorrect implementation diverge. The evaluator, once embedded\nin the LLM's decoder, flags deviations on partial code, rolls back to the\nfaulty line, and guides regeneration-without executing the program or requiring\ntest cases. Across four benchmarks, SemGuard consistently outperforms\nstate-of-the-art baselines. It lowers the semantic error rate by 19.86% on\nSemDiff relative to ROCODE, and lifts Pass@1 by 48.92% on the real-world\nLiveCodeBench with CodeLlama-7B. Similar gains hold for StarCoder2-7B on MBPP\nand for DeepSeekCoder-6.7B on the Java benchmark SemDiff-Java, demonstrating\nmodel- and language-agnostic effectiveness.", "AI": {"tldr": "SemGuard introduces real-time, line-level semantic supervision for LLM code generation, catching errors early without running code or needing test cases. It achieves substantial accuracy improvements over prior methods across multiple models and datasets.", "motivation": "Existing LLMs often produce semantic errors, which are difficult to catch and fix during code generation. Post-hoc repair approaches are inefficient and can miss faults due to incomplete test suites. Improving error detection during generation, rather than after execution, is crucial for reducing semantic defects in LLM-generated code.", "method": "The paper introduces SemGuard, a framework that embeds a semantic evaluator into the LLM's decoding process for real-time, line-level semantic supervision. It uses SemDiff, a new dataset with fine-grained annotations marking lines where correct and incorrect implementations diverge, to train the evaluator. The evaluator flags and rolls back partial-code deviations during generation, guiding regeneration without executing code or relying on test cases.", "result": "SemGuard substantially outperforms existing methods. It lowers semantic error rates by 19.86% compared to ROCODE and improves Pass@1 by 48.92% on LiveCodeBench using CodeLlama-7B. Significant gains are also shown for StarCoder2-7B on MBPP and DeepSeekCoder-6.7B on SemDiff-Java, indicating cross-model and cross-language effectiveness.", "conclusion": "Early, semantic-level supervision during code generation is highly effective at reducing semantic errors from LLMs. SemGuard achieves state-of-the-art results on multiple benchmarks and is both model- and language-agnostic, showing promise for robust, error-minimized code generation."}}
{"id": "2509.24637", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24637", "abs": "https://arxiv.org/abs/2509.24637", "authors": ["Zhensu Sun", "Chengran Yang", "Chao Peng", "Pengfei Gao", "Xiaoning Du", "Li Li", "David Lo"], "title": "Bridging Developer Instructions and Code Completion Through Instruction-Aware Fill-in-the-Middle Paradigm", "comment": "10 pages", "summary": "Large Language Models (LLMs) have significantly advanced code completion, yet\nthey often fail when the developer's intent is underspecified in the code\ncontext. To address this, developers usually add natural language instructions\n(e.g., comments) into the code context to clarify their intent. However,\nexisting code LLMs applied for code completion systems merely undergo a\nfill-in-the-middle (FIM) pre-training, which struggles to leverage this\ninformation effectively due to the lack of instruction-like training data.\nExisting instruction-tuning techniques, which improve instruction-following in\ngeneral code generation, paradoxically degrade FIM performance, forcing a\ntrade-off between instruction-following and infilling capabilities. To address\nthis gap, we introduce Instruction-aware Fill-in-the-Middle (IFIM), an\ninstruction-tuning method specifically designed to enhance FIM code completion\nmodels. IFIM extends the conventional FIM training objective by incorporating\nan explicit instruction section into the input, enabling the model to learn\nfrom (prefix, instruction, suffix) triplets. This approach allows the model to\neffectively leverage developer-provided directives while preserving its core\ncompletion abilities when no instructions are present. To facilitate this, we\nconstructed a large-scale dataset by using GPT-4o to generate concise,\nintent-focused instructions for code infilling examples. We evaluated IFIM by\napplying it to two popular base models, Deepseek-Coder and Qwen2.5-Coder, on\nthe benchmarks derived from HumanEval-infilling and RepoMasterEval. The results\ndemonstrate that IFIM significantly improves instruction-following\ncapabilities, boosting the Pass@1 score from 84.6% to 93.6% on\nHumanEval-infilling. Moreover, this enhancement does not compromise the models'\noriginal performance on FIM code completion tasks with no instructions\nprovided.", "AI": {"tldr": "IFIM is a new training method for code completion LLMs that makes them better at following developer instructions without hurting their performance on normal fill-in-the-middle tasks. Applied to two popular models, it vastly improves instruction adherence, solving the usual trade-off with minimal downsides.", "motivation": "Large Language Models (LLMs) improve code completion but often falter when a developer's intent is unclear. Developers typically add natural language instructions (comments) to clarify intent, but current code LLMs are not optimized to utilize these effectively, as standard fill-in-the-middle (FIM) training lacks instruction-like data. Moreover, conventional instruction-tuning improves instruction following but degrades FIM, causing a trade-off. This motivates the need for a method that enhances both instruction-following and FIM performance.", "method": "The paper introduces Instruction-aware Fill-in-the-Middle (IFIM), a training method that incorporates explicit instruction sections into FIM objectives. IFIM trains models on (prefix, instruction, suffix) triplets, enabling them to leverage developer directives while maintaining FIM capabilities. A large-scale dataset with intent-focused instructions was created using GPT-4o for code infilling examples. The approach was applied to Deepseek-Coder and Qwen2.5-Coder and evaluated on HumanEval-infilling and RepoMasterEval benchmarks.", "result": "IFIM significantly improves instruction-following capabilities, increasing the Pass@1 score from 84.6% to 93.6% on HumanEval-infilling. Importantly, the enhancement does not degrade the base models' performance on standard FIM code completion when no instructions are given.", "conclusion": "Instruction-aware Fill-in-the-Middle (IFIM) bridges the gap between instruction-following and infilling abilities in code LLMs. By integrating explicit instruction sections in the training objective, IFIM enables models to utilize developer intent instructions in code completion without sacrificing performance on standard FIM tasks."}}
{"id": "2509.24694", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24694", "abs": "https://arxiv.org/abs/2509.24694", "authors": ["Gangda Xiong", "Tao Chen"], "title": "CoTune: Co-evolutionary Configuration Tuning", "comment": "Accepted by ASE 2025", "summary": "To automatically tune configurations for the best possible system performance\n(e.g., runtime or throughput), much work has been focused on designing\nintelligent heuristics in a tuner. However, existing tuner designs have mostly\nignored the presence of complex performance requirements (e.g., the latency\nshall ideally be 2 seconds), but simply assume that better performance is\nalways more preferred. This would not only waste valuable information in a\nrequirement but might also consume extensive resources to tune for a goal with\nlittle gain. Yet, prior studies have shown that simply incorporating the\nrequirement as a tuning objective is problematic since the requirement might be\ntoo strict, harming convergence; or its highly diverse satisfactions might lead\nto premature convergence. In this paper, we propose CoTune, a tool that takes\nthe information of a given target performance requirement into account through\nco-evolution. CoTune is unique in the sense that it creates an auxiliary\nperformance requirement to be co-evolved with the configurations, which assists\nthe target performance requirement when it becomes ineffective or even\nmisleading, hence allowing the tuning to be guided by the requirement while\nbeing robust to its harm. Experiment results on 162 cases (nine systems and 18\nrequirements) reveal that CoTune considerably outperforms existing tuners,\nranking as the best for 90% cases (against the 0%--35% for other tuners) with\nup to 2.9x overall improvements, while doing so under a much better efficiency.", "AI": {"tldr": "CoTune is a novel configuration tuner that leverages co-evolution of explicit and auxiliary performance requirements, significantly exceeding the performance and efficiency of current tuning methods by robustly meeting target requirements.", "motivation": "Existing system configuration tuners often overlook the explicit presence and influence of complex performance requirements, such as target latency, which can lead to inefficient resource usage and sub-optimal tuning strategies.", "method": "The paper introduces CoTune, a co-evolutionary tuning tool that leverages both the explicit target performance requirement and an auxiliary requirement evolved in parallel. This dual-guided evolution helps avoid pitfalls like strict convergence or premature stagnation by offering robustness and adaptability during tuning.", "result": "CoTune was evaluated on 162 test cases across nine systems and 18 different requirements, where it achieved the best results in 90% of cases, compared to 0%-35% for competing tuners, and delivered up to 2.9x overall improvements with higher efficiency.", "conclusion": "Incorporating both primary and auxiliary requirements through co-evolution, as accomplished in CoTune, makes automatic tuning for explicit performance requirements substantially more effective and efficient, outperforming state-of-the-art solutions."}}
{"id": "2509.24782", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24782", "abs": "https://arxiv.org/abs/2509.24782", "authors": ["Muhammad Laiq"], "title": "Large language models for behavioral modeling: A literature survey", "comment": "9 pages", "summary": "In recent years, large language models (LLMs) have been extensively utilized\nfor behavioral modeling, for example, to automatically generate sequence\ndiagrams. However, no overview of this work has been published yet. Such an\noverview will help identify future research directions and inform practitioners\nand educators about the effectiveness of LLMs in assisting behavioral modeling.\nThis study aims to provide an overview of the existing research on the use of\nLLMs for behavioral modeling, particularly focusing on use case and sequence\ndiagrams. Through a term-based search, we filtered and identified 14 relevant\nprimary studies. Our analysis of the selected primary studies reveals that LLMs\nhave demonstrated promising results in automatically generating use case and\nsequence diagrams. In addition, we found that most of the current literature\nlacks expert-based evaluations and has mainly used GPT-based models. Therefore,\nfuture work should evaluate a broader range of LLMs for behavioral modeling and\ninvolve domain experts to evaluate the output of LLMs.", "AI": {"tldr": "This paper reviews 14 studies on LLM-assisted behavioral modeling, finding promising results for use case and sequence diagram generation. Most studies use GPT-based models and lack expert validation. Future work should broaden model variety and involve domain experts for evaluation.", "motivation": "Large language models (LLMs) are increasingly applied to behavioral modeling tasks such as automatic generation of sequence diagrams, but there is no published overview of this work. Such a review can guide future research and inform practitioners and educators about LLM effectiveness in this domain.", "method": "The study conducts a term-based search to filter and identify 14 relevant primary studies focused on using LLMs for behavioral modeling, specifically use case and sequence diagrams. The analysis synthesizes findings from these studies.", "result": "LLMs have shown promising results in automatically generating use case and sequence diagrams. However, most studies rely on GPT-based models and typically do not involve expert-based evaluations of model outputs.", "conclusion": "There is potential for LLMs in behavioral modeling, but future research should diversify the types of LLMs assessed and incorporate expert-based evaluations to provide a more robust assessment of their effectiveness."}}
{"id": "2509.24828", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24828", "abs": "https://arxiv.org/abs/2509.24828", "authors": ["Joshua Heisler", "Johannes Reisinger", "Andreas Fischer"], "title": "Evaluating SAP Joule for Code Generation", "comment": null, "summary": "SAP has released its own proprietary generative model SAP Joule, intended for\nvarious generative tasks, including serving as a code assistant for software\nengineers. While Joule is yet not focused on SAP-specific ABAP code generation,\nit can be used for other common languages, including Javascript. This paper\ncompares SAP Joules Javascript coding capabilities against a total of 29 other\nmodels using the HumanEval-X Javascript benchmark. SAP Joule achieves a strict\naccuracy of 80.49% as the fifth best model in our evaluation. To the best of\nour knowledge, this is the first comparative evaluation of SAP Joule code\ngeneration capabilities.", "AI": {"tldr": "This paper presents the first comparative evaluation of SAP Joule's code generation abilities, showing it is a top performer (5th/30) for JavaScript tasks, with an accuracy of 80.49% on the HumanEval-X benchmark.", "motivation": "To investigate and assess the code generation capabilities of SAP Joule, particularly in JavaScript, and to provide the first comprehensive comparative study of its performance.", "method": "Comparative evaluation using the HumanEval-X JavaScript benchmark against 29 other generative models.", "result": "SAP Joule achieves a strict accuracy of 80.49%, ranking fifth overall, indicating high competence in code generation tasks for JavaScript.", "conclusion": "SAP Joule demonstrates strong performance in JavaScript code generation, ranking fifth among 30 models evaluated, with strict accuracy of 80.49%."}}
{"id": "2509.24975", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24975", "abs": "https://arxiv.org/abs/2509.24975", "authors": ["Lekang Yang", "Yuetong Liu", "Yitong Zhang", "Jia Li"], "title": "DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern", "comment": null, "summary": "Software development relies heavily on extensive unit testing, which makes\nthe efficiency of automated Unit Test Generation (UTG) particularly important.\nHowever, most existing LLMs generate test cases one token at a time in each\nforward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs)\nhave emerged, offering promising parallel generation capabilities and showing\nstrong potential for efficient UTG. Despite this advantage, their application\nto UTG is still constrained by a clear trade-off between efficiency and test\nquality, since increasing the number of tokens generated in each step often\ncauses a sharp decline in the quality of test cases. To overcome this\nlimitation, we present DiffTester, an acceleration framework specifically\ntailored for dLLMs in UTG. The key idea of DiffTester is that unit tests\ntargeting the same focal method often share repetitive structural patterns. By\ndynamically identifying these common patterns through abstract syntax tree\nanalysis during generation, DiffTester adaptively increases the number of\ntokens produced at each step without compromising the quality of the output. To\nenable comprehensive evaluation, we extend the original TestEval benchmark,\nwhich was limited to Python, by introducing additional programming languages\nincluding Java and C++. Extensive experiments on three benchmarks with two\nrepresentative models show that DiffTester delivers significant acceleration\nwhile preserving test coverage. Moreover, DiffTester generalizes well across\ndifferent dLLMs and programming languages, providing a practical and scalable\nsolution for efficient UTG in software development. Code and data are publicly\navailable at https://github.com/wellbeingyang/DLM4UTG-open .", "AI": {"tldr": "DiffTester is a framework that speeds up unit test generation in software development by enabling diffusion LLMs to produce more tokens per step without sacrificing test quality. It works by detecting repetitive patterns in unit tests and adaptively increasing generation efficiency, and it generalizes well across languages and models.", "motivation": "Unit Test Generation (UTG) is crucial for software development. The inefficiency of current LLMs in generating unit tests (as they produce one token per step) limits UTG's scalability and practicality.", "method": "DiffTester is an acceleration framework tailored for diffusion LLMs (dLLMs) in UTG. It uses dynamic analysis of abstract syntax trees to identify repetitive structural patterns among unit tests targeting the same method. This enables adaptive token generation to improve efficiency without reducing test quality.", "result": "DiffTester significantly accelerates unit test generation, while maintaining high test coverage and quality. Experiments across multiple languages (Python, Java, C++) and models show strong generalization and scalability.", "conclusion": "DiffTester offers a practical, scalable solution for efficient UTG using dLLMs, overcoming the trade-off between generation speed and test quality through adaptive generation techniques. It generalizes well to various languages and models."}}
{"id": "2509.25043", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25043", "abs": "https://arxiv.org/abs/2509.25043", "authors": ["Cristian Augusto", "Antonia Bertolino", "Guglielmo De Angelis", "Francesca Lonetti", "Jes\u00fas Mor\u00e1n"], "title": "Large Language Models for Software Testing: A Research Roadmap", "comment": "40 pages & 10 figures Submitted on 29th September 2025", "summary": "Large Language Models (LLMs) are starting to be profiled as one of the most\nsignificant disruptions in the Software Testing field.\n  Specifically, they have been successfully applied in software testing tasks\nsuch as generating test code, or summarizing documentation.\n  This potential has attracted hundreds of researchers, resulting in dozens of\nnew contributions every month, hardening researchers to\n  stay at the forefront of the wave. Still, to the best of our knowledge, no\nprior work has provided a structured vision of the progress\n  and most relevant research trends in LLM-based testing. In this article, we\naim to provide a roadmap that illustrates its current state,\n  grouping the contributions into different categories, and also sketching the\nmost promising and active research directions for the field.\n  To achieve this objective, we have conducted a semi-systematic literature\nreview, collecting articles and mapping them into the most\n  prominent categories, reviewing the current and ongoing status, and analyzing\nthe open challenges of LLM-based software testing.\n  Lastly, we have outlined several expected long-term impacts of LLMs over the\nwhole software testing field.", "AI": {"tldr": "This paper reviews and categorizes research on applying Large Language Models to software testing, providing a roadmap of current progress, trends, challenges, and future impacts.", "motivation": "The rapid proliferation of research applying Large Language Models (LLMs) in software testing has made it challenging for researchers to stay updated and to understand the overall landscape and research trends.", "method": "The authors conducted a semi-systematic literature review, collecting and categorizing relevant articles, analyzing the current and ongoing status, and identifying open challenges in LLM-based software testing.", "result": "The contributions in LLM-based software testing were grouped into categories, with an overview of research trends and open challenges provided. Several expected long-term impacts of LLMs on the software testing field were outlined.", "conclusion": "The paper provides a structured vision and roadmap of LLM-based software testing research, organizing existing work, identifying key research directions, and outlining future impacts and challenges."}}
{"id": "2509.25117", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.25117", "abs": "https://arxiv.org/abs/2509.25117", "authors": ["Sogol Masoumzadeh", "Keheliya Gallaba", "Dayi Lin", "Ahmed E. Hassan"], "title": "Towards Reliable Generation of Executable Workflows by Foundation Models", "comment": null, "summary": "Recent advancements in Foundation Models (FMs) have demonstrated significant\nprogress in comprehending complex natural language to perform intricate tasks.\nSuccessfully executing these tasks often requires orchestrating calls to FMs\nalongside other software components. However, manually decomposing a task into\na coherent sequence of smaller, logically aggregated steps, commonly referred\nto as workflows, demands considerable effort and specialized domain knowledge.\nWhile FMs can assist in generating such workflows specified in domain-specific\nlanguages (DSLs), achieving accuracy and reliability in this process remains a\nchallenge.\n  This work introduces a framework that leverages static analysis feedback to\nenable FMs to detect and repair defects in the DSL-based workflows they\ngenerate. We begin by presenting the first-ever taxonomy of incidences of\ndefects in FM-generated DSL workflows, categorizing them into 18 distinct\ntypes. Furthermore, we observe a high prevalence of defects across FM-generated\nDSL workflows, with 87.27% of the studied instances containing at least one\ndefect. This, in turn, emphasizes the magnitude of the problem in practice and\nunderscores the necessity for implementing mitigation strategies. Following\nthis, we demonstrate that nine types of these defects can be effectively\nidentified through static analysis of the workflows. For this purpose, we\ndevelop Timon, the first-of-its-kind static analyzer specifically designed for\nFM-generated DSL workflows. Finally, we show that by incorporating feedback\nfrom Timon, we can guide Pumbaa, an FM-based tool, to repair the detected\ndefect incidences. By systematically detecting and repairing defects, our work\nprovides a crucial step towards the reliable and automated generation of\nexecutable workflows from natural language requirements.", "AI": {"tldr": "The paper presents an automated framework using static analysis to detect, classify, and fix errors in workflows generated by Foundation Models from natural language. With a new taxonomy of defects, a custom static analyzer (Timon), and integration with an FM tool (Pumbaa), the framework substantially improves the reliability of automated workflow generation.", "motivation": "Foundation Models (FMs) can perform complex natural language tasks, but breaking down tasks into reliable executable workflows using domain-specific languages (DSLs) remains difficult, error-prone, and requires domain expertise. Manual workflow creation is laborious and FM-generated workflows often have defects.", "method": "The authors introduce a framework that employs static analysis to detect and repair defects in FM-generated DSL workflows. They create a taxonomy categorizing 18 types of defects in such workflows. They develop a static analyzer tool called Timon for defect detection and couple it with Pumbaa, an FM-based tool, to repair the detected defects using static analysis feedback.", "result": "The study finds that 87.27% of FM-generated DSL workflows contain at least one defect, highlighting the severity of the problem. Timon can effectively identify nine types of defects through static analysis, and providing feedback from Timon enables the FM tool Pumbaa to repair detected defects, systematically improving workflow reliability.", "conclusion": "This paper demonstrates a significant advance toward automating the reliable generation of executable workflows from natural language. By systematically detecting and repairing common workflow defects through static analysis and FM feedback loops, the approach increases the practicality and accuracy of FM-generated software workflows."}}
