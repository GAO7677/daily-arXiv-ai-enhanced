{"id": "2509.09853", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09853", "abs": "https://arxiv.org/abs/2509.09853", "authors": ["Zhiyu Fan", "Kirill Vasilevski", "Dayi Lin", "Boyuan Chen", "Yihao Chen", "Zhiqing Zhong", "Jie M. Zhang", "Pinjia He", "Ahmed E. Hassan"], "title": "SWE-Effi: Re-Evaluating Software AI Agent System Effectiveness Under Resource Constraints", "comment": null, "summary": "The advancement of large language models (LLMs) and code agents has\ndemonstrated significant potential to assist software engineering (SWE) tasks,\nsuch as autonomous issue resolution and feature addition. Existing AI for\nsoftware engineering leaderboards (e.g., SWE-bench) focus solely on solution\naccuracy, ignoring the crucial factor of effectiveness in a\nresource-constrained world. This is a universal problem that also exists beyond\nsoftware engineering tasks: any AI system should be more than correct - it must\nalso be cost-effective. To address this gap, we introduce SWE-Effi, a set of\nnew metrics to re-evaluate AI systems in terms of holistic effectiveness\nscores. We define effectiveness as the balance between the accuracy of outcome\n(e.g., issue resolve rate) and the resources consumed (e.g., token and time).\nIn this paper, we specifically focus on the software engineering scenario by\nre-ranking popular AI systems for issue resolution on a subset of the SWE-bench\nbenchmark using our new multi-dimensional metrics. We found that AI system's\neffectiveness depends not just on the scaffold itself, but on how well it\nintegrates with the base model, which is key to achieving strong performance in\na resource-efficient manner. We also identified systematic challenges such as\nthe \"token snowball\" effect and, more significantly, a pattern of \"expensive\nfailures\". In these cases, agents consume excessive resources while stuck on\nunsolvable tasks - an issue that not only limits practical deployment but also\ndrives up the cost of failed rollouts during RL training. Lastly, we observed a\nclear trade-off between effectiveness under the token budget and effectiveness\nunder the time budget, which plays a crucial role in managing project budgets\nand enabling scalable reinforcement learning, where fast responses are\nessential.", "AI": {"tldr": "This paper introduces new metrics (SWE-Effi) to evaluate AI systems for software engineering tasks by considering both accuracy and resource consumption, revealing key trade-offs and system integration challenges that are overlooked by traditional accuracy-centered benchmarks.", "motivation": "Existing AI benchmarks for software engineering, such as SWE-bench, evaluate systems mainly on solution accuracy, neglecting considerations of resource efficiency and overall effectiveness. In a resource-limited real-world context, it is crucial for AI systems to be not only accurate but also cost-effective, balancing output quality with resources consumed.", "method": "The paper introduces SWE-Effi, a set of new metrics designed to assess AI systems holistically by evaluating both the accuracy of their outcomes and the resources required (such as computational tokens and time). The authors re-rank popular AI systems using these metrics on a subset of SWE-bench, providing a multidimensional analysis of performance.", "result": "The study finds that AI effectiveness hinges not only on the system's underlying scaffold but also on its integration with the base model. Major challenges identified include the 'token snowball' effect, where systems consume ever-increasing resources, and so-called 'expensive failures,' where agents waste substantial resources on unsolvable problems. Moreover, the paper reveals a trade-off between efficiency judged by tokens used and efficiency judged by time, both critical for managing budgets and scalable RL training.", "conclusion": "SWE-Effi fills a significant gap in AI evaluation for software engineering by incorporating effectiveness as a balance of accuracy and resource consumption. The new metrics expose challenges and trade-offs in current AI designs, informing better system integration and practical deployment strategies. Managing the trade-off between resource types is essential for real-world application and scalable reinforcement learning environments."}}
{"id": "2509.09873", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09873", "abs": "https://arxiv.org/abs/2509.09873", "authors": ["James Jewitt", "Hao Li", "Bram Adams", "Gopi Krishnan Rajbahadur", "Ahmed E. Hassan"], "title": "From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI Ecosystem", "comment": "9 pages, 4 figures, 5 tables, pre-print", "summary": "Hidden license conflicts in the open-source AI ecosystem pose serious legal\nand ethical risks, exposing organizations to potential litigation and users to\nundisclosed risk. However, the field lacks a data-driven understanding of how\nfrequently these conflicts occur, where they originate, and which communities\nare most affected. We present the first end-to-end audit of licenses for\ndatasets and models on Hugging Face, as well as their downstream integration\ninto open-source software applications, covering 364 thousand datasets, 1.6\nmillion models, and 140 thousand GitHub projects. Our empirical analysis\nreveals systemic non-compliance in which 35.5% of model-to-application\ntransitions eliminate restrictive license clauses by relicensing under\npermissive terms. In addition, we prototype an extensible rule engine that\nencodes almost 200 SPDX and model-specific clauses for detecting license\nconflicts, which can solve 86.4% of license conflicts in software applications.\nTo support future research, we release our dataset and the prototype engine.\nOur study highlights license compliance as a critical governance challenge in\nopen-source AI and provides both the data and tools necessary to enable\nautomated, AI-aware compliance at scale.", "AI": {"tldr": "This paper reveals widespread license conflicts in open-source AI models and datasets, especially when integrated into software applications. The authors audited millions of resources on Hugging Face and GitHub, finding over a third of model-to-application transitions improperly relax license terms. They introduce a rule engine that can solve most detected conflicts and release both their dataset and the engine to support improved license compliance.", "motivation": "Hidden license conflicts in the open-source AI ecosystem present significant legal and ethical risks which have not been systematically studied. There is a need for a data-driven understanding of the frequency, origins, and community impacts of these conflicts.", "method": "Conducted a comprehensive audit of licenses for datasets and models on Hugging Face and their downstream integration into open-source software on GitHub. Analyzed 364k datasets, 1.6M models, 140k GitHub projects, and developed a rule engine encoding 200+ license clauses for automated conflict detection.", "result": "Found systemic non-compliance: 35.5% of model-to-application transitions remove restrictive license clauses via relicensing under permissive terms. The prototype rule engine can detect and solve 86.4% of license conflicts in software applications.", "conclusion": "License compliance is a major governance issue in open-source AI. The study offers data, a dataset, and tools (rule engine) for enabling automated, scalable, AI-aware license compliance."}}
{"id": "2509.09917", "categories": ["cs.SE", "D.2.4"], "pdf": "https://arxiv.org/pdf/2509.09917", "abs": "https://arxiv.org/abs/2509.09917", "authors": ["Zehan Chen", "Long Zhang", "Zhiwei Zhang", "JingJing Zhang", "Ruoyu Zhou", "Yulong Shen", "JianFeng Ma", "Lin Yang"], "title": "SLD-Spec: Enhancement LLM-assisted Specification Generation for Complex Loop Functions via Program Slicing and Logical Deletion", "comment": "22 pages, 2 figures, conference", "summary": "Automatically generating formal specifications from program code can greatly\nenhance the efficiency of program verification and enable end-to-end automation\nfrom requirements to reliable software. However, existing LLM-based approaches\noften struggle with programs that include complex loop structures, leading to\nirrelevant specifications. Moreover, the rigorous proof obligations and design\nconstraints imposed by verification tools can further result in incomplete and\nambiguous specifications. To address these challenges, we propose SLD-Spec, an\nLLM-assisted specification generation method tailored for programs with complex\nloop constructs. SLD-Spec introduces two novel phases into the traditional\nspecification generation framework: (1) A slicing phase, which decomposes each\nfunction into code fragments containing independent loop structures, thereby\nreducing the complexity of specification generation; and (2) A logical deletion\nphase, which applies LLM-based reasoning to filter out incorrect candidate\nspecifications--especially those not easily identified by verification\ntool--while retaining valid ones. Experimental results show that on the simple\ndataset, SLD-Spec successfully verifies five more programs than the\nstate-of-the-art AutoSpec and reduces runtime by 23.73%. To address the\nlimitations of existing research, we manually construct a dataset comprising\nfour categories of complex loop programs. On this dataset, SLD-Spec\nsignificantly improves the correctness, relevance, and completeness of\ngenerated specifications compared to baseline methods, enabling 95.1% of\nassertions and 90.91% of programs to pass verification. Ablation studies\nfurther reveal that logical deletion is critical for enhancing specification\ncorrectness and relevance, while program slicing contributes significantly to\nspecification completeness. Our code and data are publicly available.", "AI": {"tldr": "SLD-Spec is an LLM-driven method for generating formal specifications from code with complex loops. By combining program slicing and LLM-based logical deletion, SLD-Spec outperforms existing approaches in correctness, relevance, completeness, and efficiency of specification generation, enabling more reliable automated program verification.", "motivation": "Existing LLM-based approaches to generating formal specifications from program code struggle with complex loop structures, resulting in irrelevant, incomplete, or ambiguous specifications. Verification tools' rigorous constraints further exacerbate these issues.", "method": "The paper proposes SLD-Spec, an LLM-assisted specification generation method specifically designed for programs with complex loops. SLD-Spec introduces two key phases: program slicing to isolate independent loop structures, and logical deletion, using LLMs to filter out incorrect specifications.", "result": "Experiments show that SLD-Spec verifies five more programs and reduces runtime by 23.73% compared to AutoSpec on simple datasets. On a manually constructed complex loop program dataset, SLD-Spec enables 95.1% of assertions and 90.91% of programs to pass verification. Ablation studies indicate both logical deletion and program slicing are crucial for correctness, relevance, and completeness of specifications.", "conclusion": "SLD-Spec substantially improves the correctness, relevance, and completeness of formal specification generation for complex loop programs over existing methods, making end-to-end program verification more efficient and reliable."}}
{"id": "2509.09918", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09918", "abs": "https://arxiv.org/abs/2509.09918", "authors": ["Seyed Moein Abtahi", "Akramul Azim"], "title": "WALL: A Web Application for Automated Quality Assurance using Large Language Models", "comment": null, "summary": "As software projects become increasingly complex, the volume and variety of\nissues in code files have grown substantially. Addressing this challenge\nrequires efficient issue detection, resolution, and evaluation tools. This\npaper presents WALL, a web application that integrates SonarQube and large\nlanguage models (LLMs) such as GPT-3.5 Turbo and GPT-4o to automate these\ntasks. WALL comprises three modules: an issue extraction tool, code issues\nreviser, and code comparison tool. Together, they enable a seamless pipeline\nfor detecting software issues, generating automated code revisions, and\nevaluating the accuracy of revisions. Our experiments, conducted on 563 files\nwith over 7,599 issues, demonstrate WALL's effectiveness in reducing human\neffort while maintaining high-quality revisions. Results show that employing a\nhybrid approach of cost-effective and advanced LLMs can significantly lower\ncosts and improve revision rates. Future work aims to enhance WALL's\ncapabilities by integrating open-source LLMs and eliminating human\nintervention, paving the way for fully automated code quality management.", "AI": {"tldr": "WALL is a web app that streamlines code issue management using AI and automation. It detects issues, revises code, and evaluates changes automatically, reducing manual work while maintaining quality. Experiments show it is cost-efficient and effective, with future plans to improve automation and cut human involvement entirely.", "motivation": "With the rising complexity of software projects, managing the increasing number and variety of code issues is challenging. There is a need for efficient, automated tools to detect, resolve, and evaluate these issues, minimizing manual effort.", "method": "WALL, a web application, integrates SonarQube for issue detection and large language models (GPT-3.5 Turbo, GPT-4o) for automated issue resolution and revision evaluation. It is composed of three modules: issue extraction, code revision, and code comparison. Experiments were conducted on 563 files comprising 7,599 issues.", "result": "WALL demonstrated its effectiveness in reducing human effort and maintaining high-quality code revisions. Using a mix of cost-effective and advanced LLMs led to lower costs and higher revision rates.", "conclusion": "WALL provides an efficient, automated pipeline for code issue management. Future enhancements will focus on integrating open-source LLMs and achieving fully automated, human-free code quality management."}}
{"id": "2509.10236", "categories": ["cs.SE", "cs.PF", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.10236", "abs": "https://arxiv.org/abs/2509.10236", "authors": ["Mingyi Li", "Junmin Xiao", "Siyan Chen", "Hui Ma", "Xi Chen", "Peihua Bao", "Liang Yuan", "Guangming Tan"], "title": "Stencil-Lifting: Hierarchical Recursive Lifting System for Extracting Summary of Stencil Kernel in Legacy Codes", "comment": "33 pages, 12 figures. Submitted to OOPSLA2'25", "summary": "We introduce Stencil-Lifting, a novel system for automatically converting\nstencil kernels written in low-level languages in legacy code into semantically\nequivalent Domain-Specific Language (DSL) implementations. Targeting the\nefficiency bottlenecks of existing verified lifting systems, Stencil-Lifting\nachieves scalable stencil kernel abstraction through two key innovations.\nFirst, we propose a hierarchical recursive lifting theory that represents\nstencil kernels, structured as nested loops, using invariant subgraphs, which\nare customized data dependency graphs that capture loop-carried computation and\nstructural invariants. Each vertex in the invariant subgraph is associated with\na predicate-based summary, encoding its computational semantics. By enforcing\nself-consistency across these summaries, Stencil-Lifting ensures the derivation\nof correct loop invariants and postconditions for nested loops, eliminating the\nneed for external verification. Second, we develop a hierarchical recursive\nlifting algorithm that guarantees termination through a convergent recursive\nprocess, avoiding the inefficiencies of search-based synthesis. The algorithm\nefficiently derives the valid summaries of stencil kernels, and its\ncompleteness is formally proven. We evaluate Stencil-Lifting on diverse stencil\nbenchmarks from two different suites and on four real-world applications.\nExperimental results demonstrate that Stencil-Lifting achieves 31.62$\\times$\nand 5.8$\\times$ speedups compared to the state-of-the-art verified lifting\nsystems STNG and Dexter, respectively, while maintaining full semantic\nequivalence. Our work significantly enhances the translation efficiency of\nlow-level stencil kernels to DSL implementations, effectively bridging the gap\nbetween legacy optimization techniques and modern DSL-based paradigms.", "AI": {"tldr": "Stencil-Lifting automates efficient and correct conversion of legacy stencil kernels to DSLs, using novel recursive abstraction and synthesis methods. It greatly outperforms prior systems in speed and scalability, with provable correctness, making legacy code modernization more practical.", "motivation": "Existing verified lifting systems for converting legacy stencil kernels to DSL implementations suffer from efficiency bottlenecks, particularly in scalability and performance. Bridging legacy stencil optimization techniques with modern DSL paradigms is challenging.", "method": "Stencil-Lifting introduces two innovations: (1) a hierarchical recursive lifting theory based on invariant subgraphs and predicate-based summaries for nested loop semantics, ensuring correct loop invariants and eliminating external verification; (2) a hierarchical recursive lifting algorithm that converges efficiently and is formally proven to be complete, avoiding search-based synthesis inefficiencies.", "result": "Stencil-Lifting outperforms state-of-the-art systems STNG and Dexter with up to 31.62\u00d7 and 5.8\u00d7 speedups respectively, while maintaining semantic equivalence. It demonstrates scalable abstraction and efficient translation for diverse benchmarks and real-world applications.", "conclusion": "Stencil-Lifting significantly improves the efficiency and scalability of automated stencil kernel translation from low-level languages to DSLs, providing a theoretically sound and experimentally validated solution that maintains correctness and boosts performance."}}
{"id": "2509.09947", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.09947", "abs": "https://arxiv.org/abs/2509.09947", "authors": ["Humza Ashraf", "Syed Muhammad Danish", "Zeeshan Sattar"], "title": "Toward Green Code: Prompting Small Language Models for Energy-Efficient Code Generation", "comment": null, "summary": "There is a growing concern about the environmental impact of large language\nmodels (LLMs) in software development, particularly due to their high energy\nuse and carbon footprint. Small Language Models (SLMs) offer a more sustainable\nalternative, requiring fewer computational resources while remaining effective\nfor fundamental programming tasks. In this study, we investigate whether prompt\nengineering can improve the energy efficiency of SLMs in code generation. We\nevaluate four open-source SLMs, StableCode-Instruct-3B,\nQwen2.5-Coder-3B-Instruct, CodeLlama-7B-Instruct, and Phi-3-Mini-4K-Instruct,\nacross 150 Python problems from LeetCode, evenly distributed into easy, medium,\nand hard categories. Each model is tested under four prompting strategies: role\nprompting, zero-shot, few-shot, and chain-of-thought (CoT). For every generated\nsolution, we measure runtime, memory usage, and energy consumption, comparing\nthe results with a human-written baseline. Our findings show that CoT prompting\nprovides consistent energy savings for Qwen2.5-Coder and StableCode-3B, while\nCodeLlama-7B and Phi-3-Mini-4K fail to outperform the baseline under any\nprompting strategy. These results highlight that the benefits of prompting are\nmodel-dependent and that carefully designed prompts can guide SLMs toward\ngreener software development.", "AI": {"tldr": "Prompt engineering, especially Chain-of-Thought, can make some small code LLMs greener, but results depend on the specific model; not all SLMs benefit equally in energy savings.", "motivation": "There is increasing concern about the environmental impact of large language models (LLMs), especially due to their high energy consumption and carbon emissions in software development. Small Language Models (SLMs) represent a more sustainable alternative, but their effectiveness and energy efficiency, especially aided by prompt engineering, need to be evaluated.", "method": "The study evaluates four open-source SLMs (StableCode-Instruct-3B, Qwen2.5-Coder-3B-Instruct, CodeLlama-7B-Instruct, Phi-3-Mini-4K-Instruct) on 150 LeetCode Python problems (spread across three difficulty categories). Each model is tested with four prompting strategies: role prompting, zero-shot, few-shot, and chain-of-thought (CoT), and the energy use, memory, and runtime for the generated solutions are measured and compared to human-written baselines.", "result": "CoT prompting reduced energy consumption for Qwen2.5-Coder and StableCode-3B, while CodeLlama-7B and Phi-3-Mini-4K did not outperform human baselines with any prompting strategy. The result indicates model-specific variation in the impact of prompting strategies on energy efficiency.", "conclusion": "Prompt engineering, especially with Chain-of-Thought prompting, can improve the energy efficiency of certain SLMs, but the effectiveness is highly model-dependent. Well-chosen prompting strategies have the potential to make SLM-assisted code generation greener, but results will vary across different models."}}
{"id": "2509.09975", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.09975", "abs": "https://arxiv.org/abs/2509.09975", "authors": ["Takasaburo Fukuda", "Takao Nakagawa", "Keisuke Miyazaki", "Susumu Tokumoto"], "title": "Development of Automated Software Design Document Review Methods Using Large Language Models", "comment": "SANER 2025", "summary": "In this study, we explored an approach to automate the review process of\nsoftware design documents by using LLM. We first analyzed the review methods of\ndesign documents and organized 11 review perspectives. Additionally, we\nanalyzed the issues of utilizing LLMs for these 11 review perspectives and\ndetermined which perspectives can be reviewed by current general-purpose LLMs\ninstead of humans. For the reviewable perspectives, we specifically developed\nnew techniques to enable LLMs to comprehend complex design documents that\ninclude table data. For evaluation, we conducted experiments using GPT to\nassess the consistency of design items and descriptions across different design\ndocuments in the design process used in actual business operations. Our results\nconfirmed that LLMs can be utilized to identify inconsistencies in software\ndesign documents during the review process.", "AI": {"tldr": "This paper investigates using LLMs to automate software design document reviews. It identifies which review perspectives LLMs can handle, develops techniques for complex document comprehension, and demonstrates via experiments that LLMs (such as GPT) can detect inconsistencies, suggesting that LLMs can assist or partially automate the review process.", "motivation": "Reviewing software design documents is a time-consuming and error-prone task when done manually. The study aims to explore whether large language models (LLMs) can automate and improve this process.", "method": "The authors analyzed common review perspectives for design documents and identified 11 key perspectives. They evaluated the suitability of LLMs for each perspective and developed new techniques for LLMs to process complex design documents, including those with table data. Experiments were performed using GPT to evaluate its ability to detect inconsistencies in real business design documents.", "result": "Experiments showed that LLMs can effectively review certain perspectives of design documents and are capable of identifying inconsistencies between design items and their descriptions.", "conclusion": "The study concludes that general-purpose LLMs can automate parts of the software design document review process, particularly for detecting inconsistencies, and can supplement human reviewers."}}
{"id": "2509.10085", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.10085", "abs": "https://arxiv.org/abs/2509.10085", "authors": ["Philipp Zech", "Irdin Pekaric"], "title": "Sustaining Research Software: A Fitness Function Approach", "comment": null, "summary": "The long-term sustainability of research software is a critical challenge, as\nit usually suffers from poor maintainability, lack of adaptability, and\neventual obsolescence. This paper proposes a novel approach to addressing this\nissue by leveraging the concept of fitness functions from evolutionary\narchitecture. Fitness functions are automated, continuously evaluated metrics\ndesigned to ensure that software systems meet desired non-functional,\narchitectural qualities over time. We define a set of fitness functions\ntailored to the unique requirements of research software, focusing on\nfindability, accessibility, interoperability and reusability (FAIR). These\nfitness functions act as proactive safeguards, promoting practices such as\nmodular design, comprehensive documentation, version control, and compatibility\nwith evolving technological ecosystems. By integrating these metrics into the\ndevelopment life cycle, we aim to foster a culture of sustainability within the\nresearch community. Case studies and experimental results demonstrate the\npotential of this approach to enhance the long-term FAIR of research software,\nbridging the gap between ephemeral project-based development and enduring\nscientific impact.", "AI": {"tldr": "This paper suggests using automated fitness functions to ensure research software stays maintainable, adaptable, and FAIR over time, helping to transform short-term projects into long-lasting scientific assets.", "motivation": "Research software often becomes unsustainable due to issues like poor maintainability and obsolescence, limiting its long-term usefulness and scientific impact.", "method": "The paper introduces the use of fitness functions, inspired by evolutionary architecture, as automated metrics continually evaluated during the software development lifecycle. These functions are specifically tailored to monitor and encourage FAIR (Findability, Accessibility, Interoperability, and Reusability) principles in research software.", "result": "Through case studies and experiments, the paper shows that integrating fitness functions improves the sustainability and FAIR qualities of research software.", "conclusion": "Applying fitness functions encourages better software practices and supports the long-term sustainability and relevance of research software within the scientific community."}}
{"id": "2509.10099", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10099", "abs": "https://arxiv.org/abs/2509.10099", "authors": ["Radu Apsan", "Vincenzo Stoico", "Michel Albonico", "Rudra Dhar", "Karthik Vaidhyanathan", "Ivano Malavolta"], "title": "Generating Energy-Efficient Code via Large-Language Models -- Where are we now?", "comment": null, "summary": "Context. The rise of Large Language Models (LLMs) has led to their widespread\nadoption in development pipelines. Goal. We empirically assess the energy\nefficiency of Python code generated by LLMs against human-written code and code\ndeveloped by a Green software expert. Method. We test 363 solutions to 9 coding\nproblems from the EvoEval benchmark using 6 widespread LLMs with 4 prompting\ntechniques, and comparing them to human-developed solutions. Energy consumption\nis measured on three different hardware platforms: a server, a PC, and a\nRaspberry Pi for a total of ~881h (36.7 days). Results. Human solutions are 16%\nmore energy-efficient on the server and 3% on the Raspberry Pi, while LLMs\noutperform human developers by 25% on the PC. Prompting does not consistently\nlead to energy savings, where the most energy-efficient prompts vary by\nhardware platform. The code developed by a Green software expert is\nconsistently more energy-efficient by at least 17% to 30% against all LLMs on\nall hardware platforms. Conclusions. Even though LLMs exhibit relatively good\ncode generation capabilities, no LLM-generated code was more energy-efficient\nthan that of an experienced Green software developer, suggesting that as of\ntoday there is still a great need of human expertise for developing\nenergy-efficient Python code.", "AI": {"tldr": "Code from Green software experts is always more energy-efficient than LLM-generated code; human-written code generally beats LLMs except on PCs. Prompts and hardware type affect results, but LLMs still can't fully replace humans for energy-efficient Python yet.", "motivation": "With the increasing use of LLMs in code generation, there is a need to understand their impact on software energy efficiency, especially compared to human developers and experts in Green software.", "method": "Empirical evaluation using 363 solutions to 9 coding problems (EvoEval benchmark) generated by 6 LLMs with 4 prompting techniques, compared to human and Green software expert code. Energy use measured on server, PC, and Raspberry Pi over ~881 hours.", "result": "Human-written code is generally more energy-efficient than LLM-generated code on most hardware, except for the PC. Green software expert consistently produces the most energy-efficient code on all platforms. Prompting technique does not guarantee energy savings or consistency.", "conclusion": "LLM-generated code, while capable, does not surpass the energy efficiency of code written by experienced Green software developers, indicating the continued need for human expertise for energy-efficient Python coding."}}
{"id": "2509.10279", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10279", "abs": "https://arxiv.org/abs/2509.10279", "authors": ["Pavel Plyusnin", "Aleksey Antonov", "Vasilii Ermakov", "Aleksandr Khaybriev", "Margarita Kikot", "Ilseyar Alimova", "Stanislav Moiseev"], "title": "Targeted Test Selection Approach in Continuous Integration", "comment": "Accepted at ICSME 2025", "summary": "In modern software development change-based testing plays a crucial role.\nHowever, as codebases expand and test suites grow, efficiently managing the\ntesting process becomes increasingly challenging, especially given the high\nfrequency of daily code commits. We propose Targeted Test Selection (T-TS), a\nmachine learning approach for industrial test selection. Our key innovation is\na data representation that represent commits as Bags-of-Words of changed files,\nincorporates cross-file and additional predictive features, and notably avoids\nthe use of coverage maps. Deployed in production, T-TS was comprehensively\nevaluated against industry standards and recent methods using both internal and\npublic datasets, measuring time efficiency and fault detection. On live\nindustrial data, T-TS selects only 15% of tests, reduces execution time by\n$5.9\\times$, accelerates the pipeline by $5.6\\times$, and detects over 95% of\ntest failures. The implementation is publicly available to support further\nresearch and practical adoption.", "AI": {"tldr": "The paper presents Targeted Test Selection (T-TS), a machine learning method for selecting relevant tests in large software projects without relying on coverage maps. T-TS achieves major reductions in test execution time and pipeline duration while maintaining high fault detection rates, and is publicly released for use and further research.", "motivation": "Change-based testing is vital in modern software development, but managing testing processes is increasingly difficult as codebases and test suites grow, especially with frequent code commits.", "method": "The paper introduces Targeted Test Selection (T-TS), a machine learning approach that represents code commits as Bags-of-Words of changed files and uses additional cross-file and predictive features, deliberately excluding coverage maps.", "result": "T-TS, when deployed in production, selected only 15% of tests, reduced execution time by 5.9 times, sped up the pipeline by 5.6 times, and detected over 95% of test failures, outperforming industry standards and other methods.", "conclusion": "T-TS is an efficient and effective approach for test selection in industrial software development, providing significant speed-ups with high fault detection rates, and is available for public adoption and research."}}
{"id": "2509.10402", "categories": ["cs.SE", "D.2.0; D.2.7"], "pdf": "https://arxiv.org/pdf/2509.10402", "abs": "https://arxiv.org/abs/2509.10402", "authors": ["Suzhen Zhong", "Ying Zou", "Bram Adams"], "title": "Developer-LLM Conversations: An Empirical Study of Interactions and Generated Code Quality", "comment": null, "summary": "Large Language Models (LLMs) are becoming integral to modern software\ndevelopment workflows, assisting developers with code generation, API\nexplanation, and iterative problem-solving through natural language\nconversations. Despite widespread adoption, there is limited understanding of\nhow developers interact with LLMs in practice and how these conversational\ndynamics influence task outcomes, code quality, and software engineering\nworkflows. To address this, we leverage CodeChat, a large dataset comprising\n82,845 real-world developer-LLM conversations, containing 368,506 code snippets\ngenerated across over 20 programming languages, derived from the WildChat\ndataset. We find that LLM responses are substantially longer than developer\nprompts, with a median token-length ratio of 14:1. Multi-turn conversations\naccount for 68% of the dataset and often evolve due to shifting requirements,\nincomplete prompts, or clarification requests. Topic analysis identifies web\ndesign (9.6% of conversations) and neural network training (8.7% of\nconversations) as the most frequent LLM-assisted tasks. Evaluation across five\nlanguages (i.e., Python, JavaScript, C++, Java, and C#) reveals prevalent and\nlanguage-specific issues in LLM-generated code: generated Python and JavaScript\ncode often include undefined variables (83.4% and 75.3% of code snippets,\nrespectively); Java code lacks required comments (75.9%); C++ code frequently\nomits headers (41.1%) and C# code shows unresolved namespaces (49.2%). During a\nconversation, syntax and import errors persist across turns; however,\ndocumentation quality in Java improves by up to 14.7%, and import handling in\nPython improves by 3.7% over 5 turns. Prompts that point out mistakes in code\ngenerated in prior turns and explicitly request a fix are most effective for\nresolving errors.", "AI": {"tldr": "The study analyzed over 80,000 real conversations between developers and LLMs, showing that LLM-generated code often has errors that persist unless directly addressed. Explicitly pointing out mistakes helps improve code quality, with certain improvements noticed over multiple turns, especially in documentation and imports.", "motivation": "Large Language Models (LLMs) are widely used in software engineering to assist developers, but there is limited empirical understanding of actual developer-LLM interactions and how these influence code quality and development workflow outcomes.", "method": "The authors analyze CodeChat, a large dataset with 82,845 real developer-LLM conversations involving code generation across 20+ programming languages, to examine interaction patterns, conversation evolution, common topics, and the effectiveness of problem resolution during multi-turn dialogue.", "result": "LLM responses are much longer than prompts. Most conversations are multi-turn, evolving due to changing requirements or clarification. The most common tasks are web design and neural network training. LLM-generated code contains recurring and language-specific issues. However, in some languages, code quality (e.g., documentation in Java, imports in Python) improves slightly across conversation turns if error corrections are explicitly requested.", "conclusion": "LLM-machine conversations are iterative and error-prone, but explicit error signaling in prompts improves code quality. Understanding these dynamics helps improve LLM integration in programming workflows and can guide better tool and prompt design."}}
