<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 8]
- [cs.PL](#cs.PL) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Aspect-Oriented Programming in Secure Software Development: A Case Study of Security Aspects in Web Applications](https://arxiv.org/abs/2509.07449)
*Mterorga Ukor*

Main category: cs.SE

TL;DR: Aspect-Oriented Programming (AOP) modularizes security in web apps better than traditional OOP, improving maintainability and reusability with minimal performance cost.


<details>
  <summary>Details</summary>
Motivation: Modern web applications face persistent security threats such as unauthorized access, data breaches, and injection attacks. Traditional OOP mixes security logic with business code, causing maintenance and reliability challenges. The motivation is to find better ways to modularize and manage security concerns in software development.

Method: The study uses a case study approach to compare security feature implementations (like authentication, authorization, input validation, encryption, logging, and session management) using Aspect-Oriented Programming (AOP) versus traditional OOP or middleware-based techniques. It measures code quality metrics (lines of code, coupling, cohesion, modularity, reusability), performance (response time, throughput, memory use), maintainability, and developer feedback, following ISO/IEC 25010 standards. Statistical analysis is used to compare results.

Result: AOP implementations improve modularity, reusability, and maintainability of security features compared to traditional OOP, while adding only minimal performance overhead.

Conclusion: AOP provides significant benefits for structuring security logic in web applications, making it easier to maintain and reuse security code with little impact on performance. This helps developers strike a better balance between security and software quality.

Abstract: Security remains a critical challenge in modern web applications, where
threats such as unauthorized access, data breaches, and injection attacks
continue to undermine trust and reliability. Traditional Object-Oriented
Programming (OOP) often intertwines security logic with business functionality,
leading to code tangling, scattering, and reduced maintainability. This study
investigates the role of Aspect-Oriented Programming (AOP) in enhancing secure
software development by modularizing cross-cutting security concerns. Using a
case study approach, we compare AOP-based implementations of security features
including authentication, authorization, input validation, encryption, logging,
and session management with conventional OOP or middleware-based approaches.
Data collection involves analyzing code quality metrics (e.g., lines of code,
coupling, cohesion, modularity index, reusability), performance metrics
(response time, throughput, memory usage), and maintainability indicators.
Developer feedback is also incorporated to assess integration and debugging
experiences. Statistical methods, guided by the ISO/IEC 25010 software quality
model, are applied to evaluate differences across implementations. The findings
demonstrate that AOP enhances modularity, reusability, and maintainability of
security mechanisms, while introducing only minimal performance overhead. The
study contributes practical insights for software engineers and researchers
seeking to balance security with software quality in web application
development.

</details>


### [2] [CRACI: A Cloud-Native Reference Architecture for the Industrial Compute Continuum](https://arxiv.org/abs/2509.07498)
*Hai Dinh-Tuan*

Main category: cs.SE

TL;DR: Traditional industrial architectures are too rigid for Industry 4.0. This paper proposes CRACI, a flexible, cloud-native architecture validated via theory and real-world data, showing improved scalability and suitability for modern industrial systems.


<details>
  <summary>Details</summary>
Motivation: Traditional hierarchical architectures such as ISA-95 and RAMI 4.0 are insufficient for Industry 4.0 due to their rigidity, presence of data silos, and inability to leverage cloud-native technologies. These limitations hinder the scalability and interoperability required by modern industrial systems.

Method: The paper introduces CRACI, a Cloud-native Reference Architecture for the Industrial Compute Continuum. The architecture embraces decoupled, event-driven data flows and embeds key concerns—Trust, Governance & Policy, Observability, and Lifecycle Management—as fundamental design aspects. The validation uses a two-fold approach: (1) comparative theoretical analysis with standards and proposals, and (2) quantitative evaluation using existing smart manufacturing performance data.

Result: CRACI successfully addresses legacy model limitations, offering scalable, interoperable, and modern industrial system support. Validation shows that it outperforms traditional architectures and is viable for enabling Industry 4.0 solutions.

Conclusion: CRACI is a state-of-the-art, cloud-native architecture that fulfills the requirements of today’s industrial compute continuum, overcoming the structural and technological limitations of hierarchical legacy models.

Abstract: The convergence of Information Technology (IT) and Operational Technology
(OT) in Industry 4.0 exposes the limitations of traditional, hierarchical
architectures like ISA-95 and RAMI 4.0. Their inherent rigidity, data silos,
and lack of support for cloud-native technologies impair the development of
scalable and interoperable industrial systems. This paper addresses this issue
by introducing CRACI, a Cloud-native Reference Architecture for the Industrial
Compute Continuum. Among other features, CRACI promotes a decoupled and
event-driven model to enable flexible, non-hierarchical data flows across the
continuum. It embeds cross-cutting concerns as foundational pillars: Trust,
Governance & Policy, Observability, and Lifecycle Management, ensuring quality
attributes are core to the design. The proposed architecture is validated
through a two-fold approach: (1) a comparative theoretical analysis against
established standards, operational models, and academic proposals; and (2) a
quantitative evaluation based on performance data from previously published
real-world smart manufacturing implementations. The results demonstrate that
CRACI provides a viable, state-of-the-art architecture that utilizes the
compute continuum to overcome the structural limitations of legacy models and
enable scalable, modern industrial systems.

</details>


### [3] [PatchSeeker: Mapping NVD Records to their Vulnerability-fixing Commits with LLM Generated Commits and Embeddings](https://arxiv.org/abs/2509.07540)
*Huu Hung Nguyen,Anh Tuan Nguyen,Thanh Le-Cong,Yikun Li,Han Wei Ang,Yide Yin,Frank Liauw,Shar Lwin Khin,Ouh Eng Lieh,Ting Zhang,David Lo*

Main category: cs.SE

TL;DR: PatchSeeker leverages large language models to improve mapping between NVD vulnerabilities and fixing commits by generating semantic representations and enriched commit messages, substantially outperforming previous systems and enabling better vulnerability tracking and patch analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper comes from the gap in current methods for mapping vulnerabilities from the National Vulnerability Database (NVD) to their corresponding vulnerability-fixing commits (VFCs). Existing methods are limited by relying on incomplete or unclear commit messages and fail to capture the deep semantics of vulnerability descriptions. Closing this gap is important for improving vulnerability localization, patch analysis, and constructing precise datasets.

Method: The paper introduces PatchSeeker, a method utilizing large language models (LLMs) to create semantic links between NVD records and their VFCs. PatchSeeker generates semantic embeddings from NVD vulnerability descriptions and synthesizes more informative summaries for uninformative or brief commit messages using LLMs. These enriched messages act as a bridge, connecting the descriptive language of vulnerabilities with detailed code changes.

Result: PatchSeeker significantly outperforms the prior state-of-the-art (Prospector), achieving 59.3% higher Mean Reciprocal Rank (MRR) and 27.9% higher Recall@10 on benchmark datasets. Additional evaluation on recent CVEs and ablation studies verify the effectiveness of both the commit message generation process and the choice of LLM backbones in PatchSeeker.

Conclusion: PatchSeeker effectively bridges the gap between natural language vulnerability descriptions and code-level fixes, outperforming previous methods and confirming the value of enhanced semantic modeling and commit message generation. The study also highlights remaining challenges and opportunities for further improvement.

Abstract: Software vulnerabilities pose serious risks to modern software ecosystems.
While the National Vulnerability Database (NVD) is the authoritative source for
cataloging these vulnerabilities, it often lacks explicit links to the
corresponding Vulnerability-Fixing Commits (VFCs). VFCs encode precise code
changes, enabling vulnerability localization, patch analysis, and dataset
construction. Automatically mapping NVD records to their true VFCs is therefore
critical. Existing approaches have limitations as they rely on sparse, often
noisy commit messages and fail to capture the deep semantics in the
vulnerability descriptions. To address this gap, we introduce PatchSeeker, a
novel method that leverages large language models to create rich semantic links
between vulnerability descriptions and their VFCs. PatchSeeker generates
embeddings from NVD descriptions and enhances commit messages by synthesizing
detailed summaries for those that are short or uninformative. These generated
messages act as a semantic bridge, effectively closing the information gap
between natural language reports and low-level code changes. Our approach
PatchSeeker achieves 59.3% higher MRR and 27.9% higher Recall@10 than the
best-performing baseline, Prospector, on the benchmark dataset. The extended
evaluation on recent CVEs further confirms PatchSeeker's effectiveness.
Ablation study shows that both the commit message generation method and the
selection of backbone LLMs make a positive contribution to PatchSeeker. We also
discuss limitations and open challenges to guide future work.

</details>


### [4] [Bridging the Gap Between Binary and Source Based Package Management in Spack](https://arxiv.org/abs/2509.07728)
*John Gouwar,Gregory Becker,Tamara Dahlgren,Nathan Hanford,Arjun Guha,Todd Gamblin*

Main category: cs.SE

TL;DR: Splicing for Spack lets users combine source and binary packages flexibly and quickly, greatly reducing rebuilds and installation time for complex software.


<details>
  <summary>Details</summary>
Motivation: Binary package managers are fast but limited in configurability due to ABI constraints, while source package managers are configurable but slow. Spack, a popular HPC package manager, cannot mix binaries not built together due to lack of a binary compatibility model.

Method: The paper introduces 'splicing,' an extension for Spack that models binary compatibility between packages, augments Spack’s packaging language, and improves the dependency resolution engine to enable mixing of source and binary packages.

Result: Splicing makes it possible to reuse compatible binaries and mix source and binary distributions, maintaining configurability while speeding up installation. It works for complex dependencies like MPI with minimal overhead.

Conclusion: The proposed splicing extension enables quick and flexible software installation in Spack by supporting seamless mixing of binary and source packages without frequent rebuilds, even for ABI-sensitive applications.

Abstract: Binary package managers install software quickly but they limit
configurability due to rigid ABI requirements that ensure compatibility between
binaries. Source package managers provide flexibility in building software, but
compilation can be slow. For example, installing an HPC code with a new MPI
implementation may result in a full rebuild. Spack, a widely deployed,
HPC-focused package manager, can use source and pre-compiled binaries, but
lacks a binary compatibility model, so it cannot mix binaries not built
together. We present splicing, an extension to Spack that models binary
compatibility between packages and allows seamless mixing of source and binary
distributions. Splicing augments Spack's packaging language and dependency
resolution engine to reuse compatible binaries but maintains the flexibility of
source builds. It incurs minimal installation-time overhead and allows rapid
installation from binaries, even for ABI-sensitive dependencies like MPI that
would otherwise require many rebuilds.

</details>


### [5] [What's Coming Next? Short-Term Simulation of Business Processes from Current State](https://arxiv.org/abs/2509.07747)
*Maksym Avramenko,David Chapela-Campa,Marlon Dumas,Fredrik Milani*

Main category: cs.SE

TL;DR: The paper presents a new method for short-term business process simulation that starts from the current state derived from event logs, resulting in more accurate short-term forecasts than traditional warm-up-based approaches, particularly under changing or bursty process conditions.


<details>
  <summary>Details</summary>
Motivation: Existing business process simulation methods primarily support tactical, long-term decision-making and do not effectively support operational, short-term forecasting that takes into account the real-time state of ongoing cases and resources.

Method: This paper proposes and develops a short-term simulation approach that initializes the simulation from the current operational state, as derived from event logs of ongoing cases, instead of starting from an empty state. It specifically addresses the challenges of determining the necessary state information and extracting it from event logs. The approach is implemented in a simulation engine that accepts both a simulation model and an event log, and simulates for a specified time horizon.

Result: Experimental evaluations demonstrate that this approach produces more accurate short-term forecasts, especially in conditions with concept drift or bursty patterns, compared to traditional long-term simulations that use a warm-up period.

Conclusion: Initializing simulations directly from the real-time operational state enables more precise short-term performance predictions for business processes, outperforming conventional methods in dynamic environments.

Abstract: Business process simulation is an approach to evaluate business process
changes prior to implementation. Existing methods in this field primarily
support tactical decision-making, where simulations start from an empty state
and aim to estimate the long-term effects of process changes. A complementary
use-case is operational decision-making, where the goal is to forecast
short-term performance based on ongoing cases and to analyze the impact of
temporary disruptions, such as demand spikes and shortfalls in available
resources. An approach to tackle this use-case is to run a long-term simulation
up to a point where the workload is similar to the current one (warm-up), and
measure performance thereon. However, this approach does not consider the
current state of ongoing cases and resources in the process. This paper studies
an alternative approach that initializes the simulation from a representation
of the current state derived from an event log of ongoing cases. The paper
addresses two challenges in operationalizing this approach: (1) Given a
simulation model, what information is needed so that a simulation run can start
from the current state of cases and resources? (2) How can the current state of
a process be derived from an event log? The resulting short-term simulation
approach is embodied in a simulation engine that takes as input a simulation
model and a log of ongoing cases, and simulates cases for a given time horizon.
An experimental evaluation shows that this approach yields more accurate
short-term performance forecasts than long-term simulations with warm-up
period, particularly in the presence of concept drift or bursty performance
patterns.

</details>


### [6] [What Were You Thinking? An LLM-Driven Large-Scale Study of Refactoring Motivations in Open-Source Projects](https://arxiv.org/abs/2509.07763)
*Mikel Robredo,Matteo Esposito,Fabio Palomba,Rafael Peñaloza,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: LLMs can help explain why developers refactor by matching human and literature-based motivations to an extent, especially for readability and maintainability. However, their understanding of deeper architectural motivations is limited. Combining LLM insights with traditional metrics could lead to better and more systematic refactoring strategies.


<details>
  <summary>Details</summary>
Motivation: Understanding why developers refactor code and which software metrics are reflective of these motivations, with the ultimate aim of promoting more effective and widespread adoption of refactoring in practice.

Method: A large-scale empirical study was conducted, using Large Language Models (LLMs) to analyze version control data to uncover developers’ refactoring motivations. The motivations identified were compared with those reported in prior literature.

Result: LLMs matched human judgment in 80% of cases, but only aligned with motivations from literature in 47% of cases. LLMs provided richer explanations in 22% of cases, mostly regarding readability, clarity, and structure. Most refactoring motivations were found to be pragmatic, focusing on simplification and maintainability. Metrics such as developer experience and readability were top indicators but showed weak correlation with motivation categories.

Conclusion: LLMs can effectively detect surface motivations for refactoring, but are less capable with architectural reasoning. Their main value is in generating localized explanations. Combining LLM-generated insights with software metrics could lead to hybrid approaches, potentially improving systematic refactoring prioritization and balancing immediate improvements with long-term software architecture.

Abstract: Context. Code refactoring improves software quality without changing external
behavior. Despite its advantages, its benefits are hindered by the considerable
cost of time, resources, and continuous effort it demands. Aim. Understanding
why developers refactor, and which metrics capture these motivations, may
support wider and more effective use of refactoring in practice. Method. We
performed a large-scale empirical study to analyze developers refactoring
activity, leveraging Large Language Models (LLMs) to identify underlying
motivations from version control data, comparing our findings with previous
motivations reported in the literature. Results. LLMs matched human judgment in
80% of cases, but aligned with literature-based motivations in only 47%. They
enriched 22% of motivations with more detailed rationale, often highlighting
readability, clarity, and structural improvements. Most motivations were
pragmatic, focused on simplification and maintainability. While metrics related
to developer experience and code readability ranked highest, their correlation
with motivation categories was weak. Conclusions. We conclude that LLMs
effectively capture surface-level motivations but struggle with architectural
reasoning. Their value lies in providing localized explanations, which, when
combined with software metrics, can form hybrid approaches. Such integration
offers a promising path toward prioritizing refactoring more systematically and
balancing short-term improvements with long-term architectural goals.

</details>


### [7] ["We provide our resources in a dedicated repository": Surveying the Transparency of HICSS publications](https://arxiv.org/abs/2509.07851)
*Irdin Pekaric,Giovanni Apruzzese*

Main category: cs.SE

TL;DR: Most recent HICSS papers do not share usable supplementary material in public repositories, hindering research transparency and reproducibility; only 3% of relevant papers do so, and the authors release their audit tools.


<details>
  <summary>Details</summary>
Motivation: Research transparency and reproducibility are essential for scientific progress. Many research papers involve data, artifacts, or software necessary to verify and build upon their findings, but these resources often are not shared or accessible.

Method: The study collected all 5579 papers from 2017-2024 in the HICSS proceedings, identified those with human subject research or technical implementations, reviewed 2028 relevant papers, and checked whether they included functional, publicly accessible repositories of supplementary material.

Result: Of the 2028 relevant papers analyzed, only 3% provided a functional, publicly available repository that could be used by other researchers.

Conclusion: There is a significant gap in sharing usable supplementary research material via external repositories among HICSS papers, limiting transparency and reproducibility. The authors also share their analysis tools to facilitate future research.

Abstract: Every day, new discoveries are made by researchers from all across the globe
and fields. HICSS is a flagship venue to present and discuss such scientific
advances. Yet, the activities carried out for any given research can hardly be
fully contained in a single document of a few pages-the "paper." Indeed, any
given study entails data, artifacts, or other material that is crucial to truly
appreciate the contributions claimed in the corresponding paper. External
repositories (e.g., GitHub) are a convenient tool to store all such resources
so that future work can freely observe and build upon them -- thereby improving
transparency and promoting reproducibility of research as a whole. In this
work, we scrutinize the extent to which papers recently accepted to HICSS
leverage such repositories to provide supplementary material. To this end, we
collect all the 5579 papers included in HICSS proceedings from 2017-2024. Then,
we identify those entailing either human subject research (850) or technical
implementations (737), or both (147). Finally, we review their text, examining
how many include a link to an external repository-and, inspect its contents.
Overall, out of 2028 papers, only 3\% have a functional and publicly available
repository that is usable by downstream research. We release all our tools.

</details>


### [8] [Breaking Android with AI: A Deep Dive into LLM-Powered Exploitation](https://arxiv.org/abs/2509.07933)
*Wanni Vidulige Ishan Perera,Xing Liu,Fan liang,Junyi Zhang*

Main category: cs.SE

TL;DR: AI and LLMs, such as PentestGPT, can automate and streamline Android penetration testing to efficiently achieve rooting and privilege escalation, but human oversight remains essential to ensure ethical and accurate application.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to improve and automate Android device penetration testing using AI and Large Language Models (LLMs), specifically exploring how tools like PentestGPT could streamline the process of identifying and executing rooting techniques. The study seeks to address the limitations of manual testing and evaluate whether AI can enhance efficacy, reliability, and scalability in cybersecurity tasks.

Method: The authors used an Android emulator (Genymotion) as the test environment, carrying out both traditional manual rooting and AI-driven exploitation techniques produced using PentestGPT. The process was automated using AI-generated scripts, and a web application was developed to integrate OpenAI's API for automated script generation. Comparative analysis was then conducted between the results of automated and manual penetration tests.

Result: The results indicate that LLMs and AI tools can significantly streamline exploitation workflows and automate penetration testing protocols. However, AI-driven methods still require human oversight to ensure accuracy and ethical use. Both strengths and weaknesses of LLMs in this context were identified.

Conclusion: AI-powered tools like LLMs can enhance efficiency and scalability in Android penetration testing, but human control is necessary to manage accuracy and address ethical considerations. The study contributes useful insights to the field of AI-enabled cybersecurity, suggesting both practical benefits and the importance of guiding responsible use.

Abstract: The rapid evolution of Artificial Intelligence (AI) and Large Language Models
(LLMs) has opened up new opportunities in the area of cybersecurity, especially
in the exploitation automation landscape and penetration testing. This study
explores Android penetration testing automation using LLM-based tools,
especially PentestGPT, to identify and execute rooting techniques. Through a
comparison of the traditional manual rooting process and exploitation methods
produced using AI, this study evaluates the efficacy, reliability, and
scalability of automated penetration testing in achieving high-level privilege
access on Android devices. With the use of an Android emulator (Genymotion) as
the testbed, we fully execute both traditional and exploit-based rooting
methods, automating the process using AI-generated scripts. Secondly, we create
a web application by integrating OpenAI's API to facilitate automated script
generation from LLM-processed responses. The research focuses on the
effectiveness of AI-enabled exploitation by comparing automated and manual
penetration testing protocols, by determining LLM weaknesses and strengths
along the way. We also provide security suggestions of AI-enabled exploitation,
including ethical factors and potential misuse. The findings exhibit that while
LLMs can significantly streamline the workflow of exploitation, they need to be
controlled by humans to ensure accuracy and ethical application. This study
adds to the increasing body of literature on AI-powered cybersecurity and its
effect on ethical hacking, security research, and mobile device security.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [9] [veScale: Consistent and Efficient Tensor Programming with Eager-Mode SPMD](https://arxiv.org/abs/2509.07003)
*Youjie Li,Cheng Wan,Zhiqi Lin,Hongyu Zhu,Jiacheng Yang,Ziang Song,Xinyi Di,Jiawei Wu,Huiyao Shu,Wenlei Bao,Yanghua Peng,Haibin Lin,Li-Wen Chang*

Main category: cs.PL

TL;DR: veScale is a new training system for large models that simplifies distributed programming with SPMD, improves consistency and performance, and outperforms current solutions in both speed and code simplicity.


<details>
  <summary>Details</summary>
Motivation: The rapid scaling of Large Language Models (LLMs) increases the complexity of distributed training. Existing parallelism approaches (like 3D parallelism) make systems more complicated and harder to debug, motivating a shift towards simpler, more manageable paradigms like SPMD.

Method: The paper introduces veScale, an eager-mode training system designed around the SPMD paradigm. veScale tackles inconsistency issues in distributed settings (such as those found in PyTorch) by using a new distributed Random Number Generation (RNG) algorithm compatible with various sharded operators. It also optimizes training by lowering PyTorch primitive overhead and boosting communication efficiency.

Result: veScale achieves up to 2.2x speedup compared to state-of-the-art systems such as TorchTitan. It also reduces code complexity by 78.4%, while ensuring results consistent with single-device execution.

Conclusion: veScale makes distributed large-scale tensor programming more accessible by fully embracing SPMD in eager execution. It solves key challenges of consistency and performance, presenting a significant advance over previous training frameworks.

Abstract: Large Language Models (LLMs) have scaled rapidly in size and complexity,
requiring increasingly intricate parallelism for distributed training, such as
3D parallelism. This sophistication motivates a shift toward simpler, more
debuggable programming paradigm like Single Program Multiple Data (SPMD).
However, SPMD in eager execution introduces two key challenges: ensuring
consistency with single-device execution and achieving high performance at
scale. In this paper, we introduce veScale, an eager-mode training system that
fully embraces SPMD paradigm to democratize distributed tensor programming.
veScale addresses the prevalent issue of inconsistent results in systems like
PyTorch by introducing a novel algorithm of distributed Random Number
Generation (RNG) compatible with arbitrary sharded operators. veScale also
significantly boosts training performance by reducing PyTorch primitive's
overhead and improving communication efficiency. Evaluations show that veScale
delivers up to 2.2x speedup over the state-of-the-art training systems, like
TorchTitan, and cuts code complexity by 78.4%, while preserving
single-device-equivalent results.

</details>


### [10] [Fast and Extensible Hybrid Embeddings with Micros](https://arxiv.org/abs/2509.07551)
*Sean Bocirnea,William J. Bowman*

Main category: cs.PL

TL;DR: Micro embedding (syntax-to-IR) offers a faster, extensible alternative to macro embedding for language embedding in Scheme-like systems. This approach enables efficient extensibility and faster compile-time through the use of new design patterns and abstractions.


<details>
  <summary>Details</summary>
Motivation: Macro embedding is widely used for extensible shallow embeddings in Scheme-like languages, but suffers from poor compile-time performance. The paper seeks to offer a better alternative.

Method: The paper proposes 'micro embedding,' which uses syntax-to-IR transformers instead of traditional macro-based source-to-source transformation. The authors use design patterns and new abstractions to make the IR and related functions extensible.

Result: The approach achieved extensible and hybrid embedding of statically typed languages with much better compile-time performance than macro embedding.

Conclusion: Combining micros (syntax-to-IR) with specific design patterns enables a flexible, extensible embedding methodology that outperforms macro embedding in compile-time efficiency. The paper also introduces new abstractions for packaging these patterns.

Abstract: Macro embedding is a popular approach to defining extensible shallow
embeddings of object languages in Scheme like host languages. While macro
embedding has even been shown to enable implementing extensible typed languages
in systems like Racket, it comes at a cost: compile-time performance. In this
paper, we revisit micros - syntax to intermediate representation (IR)
transformers, rather than source syntax to source syntax transformers (macros).
Micro embedding enables stopping at an IR, producing a deep embedding and
enabling high performance compile-time functions over an efficient IR, before
shallowly embedding the IR back into source syntax. Combining micros with
several design patterns to enable the IR and functions over it to be
extensible, we achieve extensible hybrid embedding of statically typed
languages with significantly improved compile-time compared to macro-embedding
approaches. We describe our design patterns and propose new abstractions
packaging these patterns.

</details>


### [11] [What's in the Box: Ergonomic and Expressive Capture Tracking over Generic Data Structures (Extended Version)](https://arxiv.org/abs/2509.07609)
*Yichen Xu,Oliver Bračevac,Cao Nguyen Pham,Martin Odersky*

Main category: cs.PL

TL;DR: System Capless introduces "reach capabilities" to improve effect and resource tracking in Scala, making it possible for capturing types to work with generic data structures like collections. This enables practical and lightweight capture checking with minimal changes, paving the way for broader adoption in production Scala code.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of tracking object capabilities—particularly those embedded in generic data structures—using capturing types in Scala, which previously lacked the expressiveness to scale to standard libraries, hindering widespread adoption.

Method: The authors develop 'System Capless', a new theoretical foundation that introduces reach capabilities (rcaps) for naming and tracking capabilities within generic types. The system refines the universal capability concept using existential and universal capture set quantification and is formalized in Lean, complete with metatheory proofs. The practical aspect includes re-implementing capture checking in Scala 3 and migrating major libraries to use the new system.

Result: System Capless successfully supports type-safe, expressive capture and effect tracking even in large codebases like Scala's collections and asynchronous libraries. The migration demonstrates that the approach requires minimal changes and retains lightweight notation, validating the system's practicality and ergonomics.

Conclusion: Reach capabilities (rcaps) in System Capless overcome prior limitations and enable efficient effect and resource tracking in generic data structures, making capture checking practical for production use in Scala with little notational or code overhead.

Abstract: Capturing types in Scala unify static effect and resource tracking with
object capabilities, enabling lightweight effect polymorphism with minimal
notational overhead. However, their expressiveness has been insufficient for
tracking capabilities embedded in generic data structures, preventing them from
scaling to the standard collections library -- an essential prerequisite for
broader adoption. This limitation stems from the inability to name capabilities
within the system's notion of box types.
  This paper develops System Capless, a new foundation for capturing types that
provides the theoretical basis for reach capabilities (rcaps), a novel
mechanism for naming "what's in the box." The calculus refines the universal
capability notion into a new scheme with existential and universal capture set
quantification. Intuitively, rcaps witness existentially quantified capture
sets inside the boxes of generic types in a way that does not require exposing
existential capture types in the surface language. We have fully mechanized the
formal metatheory of System Capless in Lean, including proofs of type soundness
and scope safety. System Capless supports the same lightweight notation of
capturing types plus rcaps, as certified by a type-preserving translation, and
also enables fully optional explicit capture-set quantification to increase
expressiveness.
  Finally, we present a full reimplementation of capture checking in Scala 3
based on System Capless and migrate the entire Scala collections library and an
asynchronous programming library to evaluate its practicality and ergonomics.
Our results demonstrate that reach capabilities enable the adoption of capture
checking in production code with minimal changes and minimal-to-zero notational
overhead in a vast majority of cases.

</details>
