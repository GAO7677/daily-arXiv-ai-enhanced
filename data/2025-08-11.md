<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Empirical Evaluation of AI-Assisted Software Package Selection: A Knowledge Graph Approach](https://arxiv.org/abs/2508.05693)
*Siamak Farshidi,Amir Saberhabibi,Behbod Eskafi,Niloofar Nikfarjam,Sadegh Eskandari,Slinger Jansen,Michel Chaudron,Bedir Tekinerdogan*

Main category: cs.SE

TL;DR: The paper presents PySelect, a decision support system that leverages empirical data, multi-criteria analysis, and AI-assisted user intent modeling to improve the selection of Python packages. It outperforms existing AI tools in transparency, recommendation quality, and user satisfaction.


<details>
  <summary>Details</summary>
Motivation: Developers face challenges in selecting third-party Python packages due to an overwhelming number of choices, insufficient transparent comparison data, and shortcomings in generative AI tool recommendations, which often prioritize popularity over suitability and lack reproducibility. There is a need for a more transparent, reliable, and evidence-based approach to support informed package selection.

Method: The authors formulate package selection as a Multi-Criteria Decision-Making (MCDM) problem. They build a data-driven framework using automated pipelines to gather and integrate software metadata, usage trends, vulnerabilities, and developer sentiment from GitHub, PyPI, and Stack Overflow. The data forms a decision model connecting packages, domain features, and quality attributes. PySelect, the proposed system, uses large language models to interpret user queries and select appropriate packages. The framework is evaluated with large-scale script analysis and a user study.

Result: The framework shows high precision in data extraction, delivers better package recommendations than generative AI baselines, and receives positive user feedback regarding usefulness and ease of use. The system is demonstrated on 798,669 Python scripts from 16,887 GitHub repositories.

Conclusion: This work provides a scalable, interpretable, and reproducible system for evidence-based software package selection using MCDM, empirical data, and AI-assisted decision support, enabling better transparency and reliability in development workflows.

Abstract: Selecting third-party software packages in open-source ecosystems like Python
is challenging due to the large number of alternatives and limited transparent
evidence for comparison. Generative AI tools are increasingly used in
development workflows, but their suggestions often overlook dependency
evaluation, emphasize popularity over suitability, and lack reproducibility.
This creates risks for projects that require transparency, long-term
reliability, maintainability, and informed architectural decisions. This study
formulates software package selection as a Multi-Criteria Decision-Making
(MCDM) problem and proposes a data-driven framework for technology evaluation.
Automated data pipelines continuously collect and integrate software metadata,
usage trends, vulnerability information, and developer sentiment from GitHub,
PyPI, and Stack Overflow. These data are structured into a decision model
representing relationships among packages, domain features, and quality
attributes. The framework is implemented in PySelect, a decision support system
that uses large language models to interpret user intent and query the model to
identify contextually appropriate packages. The approach is evaluated using
798,669 Python scripts from 16,887 GitHub repositories and a user study based
on the Technology Acceptance Model. Results show high data extraction
precision, improved recommendation quality over generative AI baselines, and
positive user evaluations of usefulness and ease of use. This work introduces a
scalable, interpretable, and reproducible framework that supports
evidence-based software selection using MCDM principles, empirical data, and
AI-assisted intent modeling.

</details>


### [2] [Klear-CodeTest: Scalable Test Case Generation for Code Reinforcement Learning](https://arxiv.org/abs/2508.05710)
*Jia Fu,Xinyu Yang,Hongzhi Zhang,Yahui Liu,Jingyuan Zhang,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.SE

TL;DR: Klear-CodeTest is a new framework that generates and validates high-quality test cases for training code language models, resulting in better and more stable model performance. The approach covers more problem scenarios and ensures safe code execution, with all resources publicly available.


<details>
  <summary>Details</summary>
Motivation: Obtaining accurate and diverse feedback is a major bottleneck in training large language models (LLMs) for code via reinforcement learning. High-quality test case generation is especially challenging yet critical for evaluating code solutions effectively.

Method: The paper introduces Klear-CodeTest, a test case synthesis framework based on a novel Generator-Validation (G-V) method. This approach generates extensive test cases, including regular and edge cases, and verifies outputs using a consistency validation mechanism. Additionally, a multi-layered security sandbox is developed for safe code execution during validation.

Result: Experiments show that the Klear-CodeTest dataset and framework lead to significant improvements in model performance and training stability in code reinforcement learning tasks.

Conclusion: Klear-CodeTest provides a reliable and comprehensive test case generation and validation system, advancing the state-of-the-art in code RL by enabling more precise feedback and robust evaluation. The proposed solution improves both the performance and stability of models trained with it.

Abstract: Precise, correct feedback is crucial for effectively training large language
models (LLMs) in code reinforcement learning. However, synthesizing
high-quality test cases remains a profoundly challenging and unsolved problem.
In this work, we present Klear-CodeTest, a comprehensive test case synthesis
framework featuring rigorous verification to ensure quality and reliability of
test cases. Our approach achieves broad coverage of programming problems via a
novel Generator-Validation (G-V) framework, ensuring correctness through a
consistency validation mechanism that verifies outputs against gold solutions.
The proposed G-V framework generates comprehensive test cases including both
regular and corner cases, enhancing test coverage and discriminative power for
solution correctness assessment in code reinforcement learning. In addition, we
design a multi-layered security sandbox system optimized for online
verification platforms, guaranteeing safe and reliable code execution. Through
comprehensive experiments, we demonstrate the effectiveness of our curated
dataset, showing significant improvements in model performance and training
stability. The source codes, curated dataset and sandbox system are available
at: https://github.com/Kwai-Klear/CodeTest.

</details>


### [3] [Utilizing Composer Packages to Accelerate Laravel-Based Project Development Among Students: A Pedagogical and Practical Framework](https://arxiv.org/abs/2508.05747)
*Rohaizah Abdul Wahid,Muhamad Said Nizamuddin Nadim,Suliana Sulaiman,Syahmi Akmal Shaharudin,Muhammad Danial Jupikil,Iqqwan Jasman Su Azlan Su*

Main category: cs.SE

TL;DR: Introducing Composer packages into Laravel curricula can help students finish projects faster and learn professional software practices, but requires careful teaching to avoid superficial use and ensure deep understanding.


<details>
  <summary>Details</summary>
Motivation: Students learning Laravel in universities often struggle to complete web development projects within tight academic timelines, even with built-in scaffolding features. There is a need to both accelerate development and reinforce professional practices.

Method: This is a conceptual paper that catalogs and categorizes Composer, PHP's dependency manager, and a curated set of Composer packages. It provides practical and pedagogical guidelines, and best-practice recommendations for integrating these tools into academic settings.

Result: Composer packages, when strategically integrated and taught with deliberate instructional design, can reduce students' workload, support professional development, and better prepare them for industry needs. However, risks such as package conflicts and over-reliance require mitigation through critical evaluation and guided learning.

Conclusion: The effective use of Composer and selected packages can both speed up development and deepen professional practices in web development education. Teachers must emphasize not just tool usage but also critical thinking about when and why to use them, ensuring that practical efficiency does not undermine deep conceptual learning.

Abstract: Laravel has emerged as a foundational framework in university web development
curricula. However, despite its scaffolding capabilities, students often
struggle to complete projects within limited academic timelines. This
conceptual paper introduces Composer, PHP's standard dependency manager, and
categorizes a curated selection of Composer packages that significantly reduce
development effort while fostering professional software practices. Grounded in
practical and pedagogical considerations, the paper illustrates how educators
and learners can strategically leverage these tools to build typical academic
or personal Laravel-based systems. Central to this approach is maintaining code
quality and reinforcing conceptual understanding. The paper also addresses
potential risks such as package conflicts and over-reliance on tools, providing
best-practice recommendations to mitigate them. While the goal is to accelerate
development, the deeper objective is to reinforce professional workflows and
industry readiness. Exposure to Composer packages enhances curriculum relevance
and smooths the transition from academia to the workplace. However, effective
integration requires deliberate instructional design aligned with learning
objectives. Without guidance, students may treat packages as black boxes. Thus,
educators must teach not only how to use these tools, but also when and why,
encouraging critical evaluation of their utility and limitations. This ensures
that practical convenience supports rather than supplants deep learning.

</details>


### [4] [AI-Guided Exploration of Large-Scale Codebases](https://arxiv.org/abs/2508.05799)
*Yoseph Berhanu Alebachew*

Main category: cs.SE

TL;DR: The paper proposes a system combining traditional code analysis and large language models to create interactive, context-aware software visualization tools. This hybrid approach helps developers understand complex software more easily and sets the stage for smarter, collaborative programming environments.


<details>
  <summary>Details</summary>
Motivation: Developers face challenges understanding large, complex software systems using traditional tools, which lack interactivity, context, and adaptability. There is a need for more effective program comprehension tools that align with how developers work and think.

Method: The authors introduce a hybrid system that integrates deterministic reverse engineering with LLM-guided, intent-aware visual exploration. This system combines UML visualizations, adaptive user interfaces, historical context tracking, and collaborative features. It employs large language models to interpret developer queries, guide exploration, and adapt to interaction patterns.

Result: They developed a prototype for Java codebases showing the feasibility of their approach. The system enables more effective navigation and comprehension of complex code, but empirical evaluation and extension to multi-language support are proposed for future work.

Conclusion: Integrating LLMs with structured reverse engineering techniques can create intelligent, adaptive code comprehension environments that better support developer cognition and collaboration. This lays a foundation for future interactive tools that address limitations of traditional software visualization.

Abstract: Understanding large-scale, complex software systems is a major challenge for
developers, who spend a significant portion of their time on program
comprehension. Traditional tools such as static visualizations and reverse
engineering techniques provide structural insights but often lack
interactivity, adaptability, and integration with contextual information.
Recent advancements in large language models (LLMs) offer new opportunities to
enhance code exploration workflows, yet their lack of grounding and integration
with structured views limits their effectiveness. This work introduces a hybrid
approach that integrates deterministic reverse engineering with LLM-guided,
intent-aware visual exploration. The proposed system combines UML-based
visualization, dynamic user interfaces, historical context, and collaborative
features into an adaptive tool for code comprehension. By interpreting user
queries and interaction patterns, the LLM helps developers navigate and
understand complex codebases more effectively. A prototype implementation for
Java demonstrates the feasibility of this approach. Future work includes
empirical evaluation, scaling to polyglot systems, and exploring GUI-driven LLM
interaction models. This research lays the groundwork for intelligent,
interactive environments that align with developer cognition and collaborative
workflows.

</details>


### [5] [Enhancing Software Vulnerability Detection Through Adaptive Test Input Generation Using Genetic Algorithm](https://arxiv.org/abs/2508.05923)
*Yanusha Mehendran,Maolin Tang,Yi Lu*

Main category: cs.SE

TL;DR: A novel genetic algorithm-based method with adaptive feedback significantly boosts vulnerability detection and code coverage, outperforming traditional fuzzing approaches on real-world software.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for detecting software vulnerabilities struggle to keep up with increasing software complexity. More effective and adaptive testing techniques are needed to find deeper and more complex vulnerabilities.

Method: The study proposes a genetic algorithm-based approach for generating test inputs, using genetic operators (especially crossover) and an adaptive learning feedback mechanism. The technique evolves a population of valid test cases by learning from system execution and adjusting input generation dynamically. This enables a balance of exploration (broad search) and exploitation (targeted search) during software testing.

Result: When evaluated on nine open-source JSON-processing libraries, the proposed method significantly outperformed a benchmark evolutionary fuzzing method. It achieved average improvements of 39.8% in class coverage, 62.4% in method coverage, 105.0% in line coverage, 114.0% in instruction coverage, and 166.0% in branch coverage.

Conclusion: The new approach notably improves software vulnerability detection by enabling deeper and more complex code exploration. The results show that adaptive, feedback-driven genetic algorithms can be a scalable and effective solution for software security testing.

Abstract: Software vulnerabilities continue to undermine the reliability and security
of modern systems, particularly as software complexity outpaces the
capabilities of traditional detection methods. This study introduces a genetic
algorithm-based method for test input generation that innovatively integrates
genetic operators and adaptive learning to enhance software vulnerability
detection. A key contribution is the application of the crossover operator,
which facilitates exploration by searching across a broader space of potential
test inputs. Complementing this, an adaptive feedback mechanism continuously
learns from the system's execution behavior and dynamically guides input
generation toward promising areas of the input space. Rather than relying on
fixed or randomly selected inputs, the approach evolves a population of
structurally valid test cases using feedback-driven selection, enabling deeper
and more effective code traversal. This strategic integration of exploration
and exploitation ensures that both diverse and targeted test inputs are
developed over time. Evaluation was conducted across nine open-source
JSON-processing libraries. The proposed method achieved substantial
improvements in coverage compared to a benchmark evolutionary fuzzing method,
with average gains of 39.8% in class coverage, 62.4% in method coverage, 105.0%
in line coverage, 114.0% in instruction coverage, and 166.0% in branch
coverage. These results highlight the method's capacity to detect deeper and
more complex vulnerabilities, offering a scalable and adaptive solution to
software security testing.

</details>


### [6] [A Survey on Task Scheduling in Carbon-Aware Container Orchestration](https://arxiv.org/abs/2508.05949)
*Jialin Yang,Zainab Saad,Jiajun Wu,Xiaoguang Niu,Henry Leung,Steve Drew*

Main category: cs.SE

TL;DR: This paper systematically reviews Kubernetes scheduling methods and provides a taxonomy focused on environmental sustainability, aiming to guide the design of greener cloud computing systems in face of rising energy demands and carbon emissions.


<details>
  <summary>Details</summary>
Motivation: Due to the rapid growth in energy demands by software ecosystems and data centers, especially from large language models, there is an urgent need to address the associated environmental impact, particularly carbon emissions. Industry and research are increasingly focused on making cloud computing more sustainable through smarter task scheduling and resource management.

Method: The paper conducts a systematic review of Kubernetes scheduling strategies, classifying them as hardware-centric or software-centric, describing their sustainability goals, and organizing them by their underlying algorithms. It also develops a taxonomy focused on environmental sustainability for cloud task scheduling studies.

Result: The study offers a comprehensive taxonomy of scheduling strategies and highlights current research trends, sustainability objectives, and open challenges in Kubernetes-based cloud scheduling for environmental sustainability.

Conclusion: The insights provided can inform the design of more sustainable scheduling solutions for future cloud systems, helping reduce energy consumption and carbon emissions.

Abstract: The soaring energy demands of large-scale software ecosystems and cloud data
centers, accelerated by the intensive training and deployment of large language
models, have driven energy consumption and carbon footprint to unprecedented
levels. In response, both industry and academia are increasing efforts to
reduce the carbon emissions associated with cloud computing through more
efficient task scheduling and infrastructure orchestration. In this work, we
present a systematic review of various Kubernetes scheduling strategies,
categorizing them into hardware-centric and software-centric, annotating each
with its sustainability objectives, and grouping them according to the
algorithms they use. We propose a comprehensive taxonomy for cloud task
scheduling studies, with a particular focus on the environmental sustainability
aspect. We analyze emerging research trends and open challenges, and our
findings provide critical insight into the design of sustainable scheduling
solutions for next-generation cloud computing systems.

</details>


### [7] [Impact-driven Context Filtering For Cross-file Code Completion](https://arxiv.org/abs/2508.05970)
*Yanzhou Li,Shangqing Liu,Kangjie Chen,Tianwei Zhang,Yang Liu*

Main category: cs.SE

TL;DR: Not all retrieved code contexts help code completion. The authors introduce CODEFILTER, a filtering system that uses a likelihood-based metric to assess code chunk value, leading to higher accuracy and efficiency in repository-level code generation.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge in retrieval-augmented code generation systems: not all retrieved cross-file code contexts are helpful, and some may even harm completion performance. This motivates the need for mechanisms to filter or assess the quality of retrieved contexts.

Method: The authors introduce a likelihood-based metric to evaluate each retrieved code chunk's impact on code completion. Using this metric, they label the retrieved chunks as positive, neutral, or negative. They build a repository-level dataset with these labels and develop CODEFILTER, an adaptive filtering framework trained on this labeled dataset to select relevant contexts and filter out harmful ones.

Result: CODEFILTER is shown to consistently improve code completion accuracy across multiple benchmarks (RepoEval and CrossCodeLongEval) compared to methods without filtering. It also reduces input prompt length, improving efficiency, and demonstrates generalizability across models.

Conclusion: The CODEFILTER framework enhances repository-level code completion by leveraging a filtering mechanism to select beneficial retrieved contexts. This leads to improved accuracy, efficiency, and better attribution in code generation systems.

Abstract: Retrieval-augmented generation (RAG) has recently demonstrated considerable
potential for repository-level code completion, as it integrates cross-file
knowledge with in-file preceding code to provide comprehensive contexts for
generation. To better understand the contribution of the retrieved cross-file
contexts, we introduce a likelihood-based metric to evaluate the impact of each
retrieved code chunk on the completion. Our analysis reveals that, despite
retrieving numerous chunks, only a small subset positively contributes to the
completion, while some chunks even degrade performance. To address this issue,
we leverage this metric to construct a repository-level dataset where each
retrieved chunk is labeled as positive, neutral, or negative based on its
relevance to the target completion. We then propose an adaptive retrieval
context filtering framework, CODEFILTER, trained on this dataset to mitigate
the harmful effects of negative retrieved contexts in code completion.
Extensive evaluation on the RepoEval and CrossCodeLongEval benchmarks
demonstrates that CODEFILTER consistently improves completion accuracy compared
to approaches without filtering operations across various tasks. Additionally,
CODEFILTER significantly reduces the length of the input prompt, enhancing
computational efficiency while exhibiting strong generalizability across
different models. These results underscore the potential of CODEFILTER to
enhance the accuracy, efficiency, and attributability of repository-level code
completion.

</details>


### [8] [Position: Intelligent Coding Systems Should Write Programs with Justifications](https://arxiv.org/abs/2508.06017)
*Xiangzhe Xu,Shiwei Feng,Zian Su,Chengpeng Wang,Xiangyu Zhang*

Main category: cs.SE

TL;DR: AI coding tools need to justify their outputs in ways users can understand. The paper identifies essential properties of good justifications and suggests neuro-symbolic methods as the path forward, since current techniques fall short.


<details>
  <summary>Details</summary>
Motivation: The motivation comes from the growing adoption of AI-driven coding tools that are often opaque in their reasoning, creating trust and usability problems for users, especially for non-experts.

Method: The authors conduct a conceptual analysis, identifying justification properties (cognitive alignment and semantic faithfulness), critically evaluating current approaches like formal verification, static analysis, and explainability, and propose neuro-symbolic methods as a promising direction.

Result: The key results are the identification of critical properties for trustworthy AI-generated code justifications and an argument that neuro-symbolic methods can bridge the gap between model reasoning and user understanding more effectively than current practices.

Conclusion: The paper concludes that intelligent coding systems should not only generate code but also provide justifications that users can understand, emphasizing the need for cognitive alignment and semantic faithfulness in these justifications. Existing methods are insufficient, and neuro-symbolic approaches are recommended for future work.

Abstract: Intelligent coding systems are transforming software development by enabling
users to specify code behavior in natural language. However, the opaque
decision-making of AI-driven coders raises trust and usability concerns,
particularly for non-expert users who cannot inspect low-level implementations.
We argue that these systems should not only generate code but also produce
clear, consistent justifications that bridge model reasoning and user
understanding. To this end, we identify two critical justification
properties-cognitive alignment and semantic faithfulness-and highlight the
limitations of existing methods, including formal verification, static
analysis, and post-hoc explainability. We advocate exploring neuro-symbolic
approaches for justification generation, where symbolic constraints guide model
behavior during training and program semantics are enriched through neural
representations, enabling automated consistency checks at inference time.

</details>


### [9] [Understanding Inconsistent State Update Vulnerabilities in Smart Contracts](https://arxiv.org/abs/2508.06192)
*Lantian Li,Yuyu Chen,Jingwen Wu,Yue Pan,Zhongxing Yu*

Main category: cs.SE

TL;DR: This paper presents the first large-scale study of inconsistent state update vulnerabilities in smart contracts, analyzing 116 cases across 352 projects, revealing root causes and solutions, and demonstrating a checker that effectively detects real-world issues.


<details>
  <summary>Details</summary>
Motivation: Inconsistent state updates in smart contracts can lead to critical vulnerabilities exploited by attackers. Existing detection tools struggle to identify these flaws, motivating the need for a comprehensive investigation and better solutions.

Method: A large-scale empirical study was conducted, systematically analyzing 116 vulnerabilities in 352 smart contract projects. The study identified root causes, fix strategies, exploitation methods, and developed a proof-of-concept checker to detect issues.

Result: The study summarized root causes and exploitation methods of vulnerabilities, revealed 11 key findings, and developed a checker that detected issues in 64 popular projects, with 19 owners confirming the findings. This demonstrates the effectiveness of the approach.

Conclusion: The research provides crucial findings and practical solutions to help avoid inconsistent state update vulnerabilities in smart contracts, demonstrating that their approach can effectively detect such issues in real-world projects.

Abstract: Smart contracts enable contract terms to be automatically executed and
verified on the blockchain, and recent years have witnessed numerous
applications of them in areas such as financial institutions and supply chains.
The execution logic of a smart contract is closely related to the contract
state, and thus the correct and safe execution of the contract depends heavily
on the precise control and update of the contract state. However, the contract
state update process can have issues. In particular, inconsistent state update
issues can arise for reasons such as unsynchronized modifications. Inconsistent
state update bugs have been exploited by attackers many times, but existing
detection tools still have difficulty in effectively identifying them. This
paper conducts the first large-scale empirical study about inconsistent state
update vulnerabilities (that is, inconsistent state update bugs that are
exploitable) in smart contracts, aiming to shed light for developers,
researchers, tool builders, and language or library designers in order to avoid
inconsistent state update vulnerabilities. We systematically investigate 116
inconsistent state update vulnerabilities in 352 real-world smart contract
projects, summarizing their root causes, fix strategies, and exploitation
methods. Our study provides 11 original and important findings, and we also
give the implications of our findings. To illustrate the potential benefits of
our research, we also develop a proof-of-concept checker based on one of our
findings. The checker effectively detects issues in 64 popular GitHub projects,
and 19 project owners have confirmed the detected issues at the time of
writing. The result demonstrates the usefulness and importance of our findings
for avoiding inconsistent state update vulnerabilities in smart contracts.

</details>


### [10] [Improving the Developer Experience with a Low-Code Process Modelling Language](https://arxiv.org/abs/2508.06299)
*Henrique Henriques,Hugo Lourenço,Vasco Amaral,Miguel Goulão*

Main category: cs.SE

TL;DR: Low usability of OutSystems’ process modelling DSL (BPT) was tackled by combining usability research and redesign methods. The new version significantly improved usability and adoption indicators, suggesting enhanced developer experience and reduced maintenance costs.


<details>
  <summary>Details</summary>
Motivation: BPT, the process modelling DSL in OutSystems, had low adoption and usability issues, posing problems due to ongoing maintenance costs. Improving usability was necessary to foster higher adoption and reduce maintenance overhead.

Method: The study used interviews, a physics-of-notation review, System Usability Scale (SUS) assessments, and NASA Task Load Index (TLX) evaluations to analyze and redesign the BPT DSL. Empirical evaluations were conducted with 25 software engineers.

Result: After redesigning BPT, semantic transparency increased from 31% to 69%, response correctness from 51% to 89%, SUS scores from 42.25 to 64.78, and TLX scores decreased from 36.50 to 20.78. All improvements were statistically significant.

Conclusion: The new version of Business Process Technology (BPT) considerably improved the developer experience over the previous version, with significant increases in usability scores and semantic transparency. OutSystems users' background influenced the final design and usability outcomes.

Abstract: Context: The OutSystems Platform is a development environment composed of
several DSLs, used to specify, quickly build, and validate web and mobile
applications. The DSLs allow users to model different perspectives such as
interfaces and data models, define custom business logic and construct process
models. Problem: The DSL for process modelling (Business Process Technology
(BPT)), has a low adoption rate and is perceived as having usability problems
hampering its adoption. This is problematic given the language maintenance
costs. Method: We used a combination of interviews, a critical review of BPT
using the "Physics of Notation" and empirical evaluations of BPT using the
System Usability Scale (SUS) and the NASA Task Load indeX (TLX), to develop a
new version of BPT, taking these inputs and Outsystems' engineers' culture into
account. Results: Evaluations conducted with 25 professional software engineers
showed an increase of the semantic transparency on the new version, from 31% to
69%, an increase in the correctness of responses, from 51% to 89%, an increase
in the SUS score, from 42.25 to 64.78, and a decrease of the TLX score, from
36.50 to 20.78. These differences were statistically significant. Conclusions:
These results suggest that the new version of BPT significantly improved the
developer experience of the previous version. The end users' background with
OutSystems had a relevant impact on the final concrete syntax choices and
achieved usability indicators.

</details>


### [11] [Execution-Feedback Driven Test Generation from SWE Issues](https://arxiv.org/abs/2508.06365)
*Toufique Ahmed,Jatin Ganhotra,Avraham Shinnar,Martin Hirzel*

Main category: cs.SE

TL;DR: The paper presents e-Otter++, a tool that uses execution feedback to generate reproduction tests even when code is missing or wrong. It achieves a 63% fail-to-pass rate on a standard benchmark, representing a significant step forward for automated test generation in difficult scenarios.


<details>
  <summary>Details</summary>
Motivation: Most software engineering issues are hard to resolve because they lack functional reproduction tests, and generating such tests is challenging when the target code is missing or incorrect.

Method: The paper proposes novel techniques that exploit execution feedback to generate reproduction tests, even when the code under test is incomplete or buggy. These techniques are implemented in a new test generator tool called e-Otter++.

Result: Experiments with e-Otter++ on the TDD-Bench Verified benchmark demonstrated that it can produce reproduction tests with an average fail-to-pass rate of 63%, which is a significant improvement over existing approaches.

Conclusion: The approach and implementation of e-Otter++ advance the generation of reproduction tests for software issues where code is missing or broken, showing strong performance and overcoming key challenges in the field.

Abstract: A software engineering issue (SWE issue) is easier to resolve when
accompanied by a reproduction test. Unfortunately, most issues do not come with
functioning reproduction tests, so this paper explores how to generate them
automatically. The primary challenge in this setting is that the code to be
tested is either missing or wrong, as evidenced by the existence of the issue
in the first place. This has held back test generation for this setting:
without the correct code to execute, it is difficult to leverage execution
feedback to generate good tests. This paper introduces novel techniques for
leveraging execution feedback to get around this problem, implemented in a new
reproduction test generator called e-Otter++. Experiments show that e-Otter++
represents a leap ahead in the state-of-the-art for this problem, generating
tests with an average fail-to-pass rate of 63% on the TDD-Bench Verified
benchmark.

</details>


### [12] [What Builds Effective In-Context Examples for Code Generation?](https://arxiv.org/abs/2508.06414)
*Dongze Li,Songqiang Chen,Jialun Cao,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: The paper finds that good variable/function naming in code examples is critical for in-context learning in code generation with LLMs, while formatting matters less. LLMs still struggle with extracting generalizable insights for problem-solving, highlighting areas for future improvement.


<details>
  <summary>Details</summary>
Motivation: There is growing interest in using in-context learning (ICL) with code examples to enhance the code generation abilities of large language models (LLMs). However, it remains unclear which specific code features in these examples most affect the success of ICL.

Method: The paper conducts controlled ablation studies, systematically varying code features (e.g., naming styles, formatting, solution insights) in the code examples used for ICL, to assess their impact on code generation performance.

Result: Appropriate naming of variables and functions is crucial; their removal reduces performance by up to 30 percentage points. LLMs prioritize semantically meaningful identifier names over formatting. There are language-specific preferences for identifier verbosity. Current LLMs can use direct information from code, but struggle to infer generalizable problem-solving insights from similar solutions.

Conclusion: Optimizing naming in code examples is key for maximizing ICL effectiveness in code generation. Improving reflection-based learning remains a significant challenge. The findings provide guidance for designing ICL code prompts and reveal limitations in current LLMs' ability to generalize problem-solving knowledge.

Abstract: In-Context Learning (ICL) has emerged as a promising solution to enhance the
code generation capabilities of Large Language Models (LLMs), which
incorporates code examples inside the prompt to let LLMs learn from
demonstrations. However, despite the substantial effectiveness of the code
example-based ICL approach, the specific features (e.g., identifier naming
styles, code formatting, solution insight) within the ICL-provided code
examples that significantly contribute to the ICL's effectiveness remain
unclear. This paper systematically investigates the impact of various code
features on ICL with code examples through controlled ablation studies. Our
findings reveal that the appropriate naming of variables and functions is
crucial for effective code generation, with their elimination leading to
performance decreases of up to 30 percentage points. We further demonstrate
that LLMs prioritize semantically meaningful identifier names over formatting
conventions, with language-specific preferences regarding identifier verbosity.
Additionally, our investigation into ICL's potential for enhancing reflection
and inference capabilities reveals that current LLMs struggle to extract
generalizable problem-solving insights from similar code solutions, despite
being capable of utilizing direct information effectively. These findings are
expected to provide valuable insights for optimizing ICL systems in code
generation applications and highlight fundamental challenges in
reflection-based learning for code generation tasks.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [13] [Hybrid Game Control Envelope Synthesis](https://arxiv.org/abs/2508.05997)
*Aditi Kabra,Jonathan Laurent,Stefan Mitsch,André Platzer*

Main category: cs.PL

TL;DR: The paper introduces a logical and compositional method to automatically synthesize the most permissive safe control strategies for hybrid games in embedded systems, using subvalue maps and differential game logic, and validates the approach with practical examples.


<details>
  <summary>Details</summary>
Motivation: Embedded systems such as cars and trains involve complex control problems, which can be challenging to model and verify for safety. Using two-player hybrid games for modeling, there is a need for systematic approaches to synthesize control envelopes (collections of safe control solutions) that are maximally permissive without compromising safety.

Method: The paper introduces subvalue maps, a compositional and inductive representation of nondeterministic winning policies for hybrid games. The method employs differential game logic (dGL) to logically characterize and check whether a subvalue map corresponds to a sound (safe and winning) control envelope. Algorithms for synthesizing these policies are derived from the inductive soundness characterization.

Result: The paper proves the existence of the maximal subvalue map, which provides the most permissive set of control options that still guarantee winning outcomes. This logical framework allows for systematic synthesis of nondeterministic policies. The approach is implemented and demonstrated on diverse examples modeled with dGL, showing its applicability to various embedded control problems.

Conclusion: The proposed approach enables the synthesis of maximally permissive, nondeterministic control policies for hybrid games in embedded systems, ensuring safety and correctness by leveraging differential game logic. The resulting methods are general, compositional, and supported by implementation and experimental validation.

Abstract: Control problems for embedded systems like cars and trains can be modeled by
two-player hybrid games. Control envelopes, which are families of safe control
solutions, correspond to nondeterministic winning policies of hybrid games,
where each deterministic specialization of the policy is a control solution.
This paper synthesizes nondeterministic winning policies for hybrid games that
are as permissive as possible. It introduces subvalue maps, a compositional
representation of such policies that enables verification and synthesis along
the structure of the game. An inductive logical characterization in
differential game logic (dGL) checks whether a subvalue map induces a sound
control envelope which always induces a winning play. A policy is said to win
if it always achieves the desirable outcome when the player follows it, no
matter what actions the opponent plays. The maximal subvalue map, which allows
the most action options while still winning, is shown to exist and satisfy a
logical characterization. A family of algorithms for nondeterministic policy
synthesis can be obtained from the inductive subvalue map soundness
characterization. An implementation of these findings is evaluated on examples
that use the expressivity of dGL to model a range of diverse control
challenges.

</details>
