<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 48]
- [cs.PL](#cs.PL) [Total: 8]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [TrioXpert: An automated incident management framework for microservice system](https://arxiv.org/abs/2506.10043)
*Yongqian Sun,Yu Luo,Xidao Wen,Yuan Yuan,Xiaohui Nie,Shenglin Zhang,Tong Liu,Xi Luo*

Main category: cs.SE

TL;DR: TrioXpert is a new incident management framework for microservice systems that outperforms traditional methods by using multimodal data and large language models, improving accuracy and interpretability in multiple incident analysis tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for automated incident management in microservice systems are limited because they use only single-modal data (such as metrics, logs, or traces) and cannot effectively perform multiple related tasks at the same time. These methods also generally lack interpretability, providing little clear reasoning evidence for their decisions.

Method: The paper proposes TrioXpert, an end-to-end framework that integrates multimodal data (from both numerical and textual sources) by designing three separate data processing pipelines tailored to each modality. It uses a collaborative reasoning method powered by large language models (LLMs) to improve simultaneous handling of anomaly detection, failure triage, and root cause localization tasks, while also providing interpretable reasoning evidence.

Result: TrioXpert was extensively evaluated on two well-known microservice datasets. It showed significant performance improvements across all three tasks: Anomaly Detection (improved by 4.7% to 57.7%), Failure Triage (improved by 2.1% to 40.6%), and Root Cause Localization (improved by 1.6% to 163.1%).

Conclusion: TrioXpert effectively leverages multimodal data and LLM-based reasoning to outperform existing single-modal approaches in incident management for microservice systems, excelling in anomaly detection, failure triage, and root cause localization, with enhanced interpretability.

Abstract: Automated incident management plays a pivotal role in large-scale
microservice systems. However, many existing methods rely solely on
single-modal data (e.g., metrics, logs, and traces) and struggle to
simultaneously address multiple downstream tasks, including anomaly detection
(AD), failure triage (FT), and root cause localization (RCL). Moreover, the
lack of clear reasoning evidence in current techniques often leads to
insufficient interpretability. To address these limitations, we propose
TrioXpert, an end-to-end incident management framework capable of fully
leveraging multimodal data. TrioXpert designs three independent data processing
pipelines based on the inherent characteristics of different modalities,
comprehensively characterizing the operational status of microservice systems
from both numerical and textual dimensions. It employs a collaborative
reasoning mechanism using large language models (LLMs) to simultaneously handle
multiple tasks while providing clear reasoning evidence to ensure strong
interpretability. We conducted extensive evaluations on two popular
microservice system datasets, and the experimental results demonstrate that
TrioXpert achieves outstanding performance in AD (improving by 4.7% to 57.7%),
FT (improving by 2.1% to 40.6%), and RCL (improving by 1.6% to 163.1%) tasks.

</details>


### [2] [Reward Models Enable Scalable Code Verification by Trading Accuracy for Throughput](https://arxiv.org/abs/2506.10056)
*Gabriel Orlanski,Nicholas Roberts,Aws Albarghouthi,Frederic Sala*

Main category: cs.SE

TL;DR: Using outcome reward models (ORMs) to quickly prune incorrect solutions before full verification dramatically speeds up LLM code evaluation, with only a small loss in accuracy, challenging assumptions that always favor comprehensive test suites.


<details>
  <summary>Details</summary>
Motivation: The paper aims to challenge the common assumption that a comprehensive verifier is always preferable to an outcome reward model (ORM) when solving coding tasks with large language models, and to explore the trade-offs between accuracy and speed in such verification systems.

Method: The authors systematically analyze various program verification approaches for LLM-generated code, specifically comparing comprehensive verifiers (full test suites) to ORMs. They introduce and evaluate a generate-prune-then-rank pipeline, where an ORM is used as a fast, less accurate filter before applying a comprehensive verifier for ranking.

Result: They find that using an ORM in a generate-prune-then-rank setup dramatically increases speed (11.65x faster) while only modestly reducing accuracy (8.33% less accurate) compared to using a full test suite alone. The approach is effective because the ORM-based pruning removes incorrect but highly ranked solutions before final ranking.

Conclusion: Outcome reward models (ORMs) have significant value even when comprehensive verifiers are available, allowing for scalable and efficient program ranking by trading some accuracy for substantial speed improvements. This insight can inform the design of better LLM-based code generation and ranking systems.

Abstract: The standard paradigm for solving coding tasks via large language models
(LLMs) is to generate-then-rank programs, where the latter step uses a verifier
in the ranking process. The growing consensus is that a comprehensive verifier
(e.g., a full test suite) should be prioritized over an outcome reward model
(ORM) whenever possible, with little consideration given to the trade-offs
involved. We aim to challenge this assumption by systematically exploring the
tradeoff between speed and accuracy. We find that ORMs play a crucial role in
scaling verification through trading accuracy for speed, even when a
comprehensive verifier is available. Their value becomes especially apparent
when used in a generate-prune-then-rank approach, where a faster but less
accurate verifier removes incorrect solutions prior to ranking -- leading to a
system that is 11.65x faster while only being 8.33% less accurate than the full
test suite. We analyze the generate-prune-then-rank approach and show that it
works by filtering out incorrect but highly ranked solutions. These findings
enable the design of scalable and accurate program ranking systems.

</details>


### [3] [Online Discovery of Simulation Models for Evolving Business Processes (Extended Version)](https://arxiv.org/abs/2506.10049)
*Francesco Vinci,Gyunam Park,Wil van der Aalst,Massimiliano de Leoni*

Main category: cs.SE

TL;DR: The paper introduces an adaptive streaming-based process simulation model discovery technique that balances recent and historical data, showing superior stability and adaptability in dynamic business environments.


<details>
  <summary>Details</summary>
Motivation: Traditional simulation model discovery methods for business processes are not adaptive to real-time changes in dynamic business environments. As organizations frequently update their processes, existing static discovery approaches become insufficient.

Method: The paper presents a streaming process simulation discovery approach that combines Incremental Process Discovery with Online Machine Learning. The method prioritizes the use of recent data but also retains historical information to maintain adaptability and stability.

Result: Experiments on four different event logs show that the proposed technique delivers more stable simulation outcomes and better adapts to recent changes, especially in the presence of concept drift. The importance of giving more weight to recent data while preserving historical context is demonstrated.

Conclusion: The proposed streaming discovery technique effectively adapts business process simulation models to evolving organizational dynamics, offering improved stability and robustness to concept drift compared to traditional approaches.

Abstract: Business Process Simulation (BPS) refers to techniques designed to replicate
the dynamic behavior of a business process. Many approaches have been proposed
to automatically discover simulation models from historical event logs,
reducing the cost and time to manually design them. However, in dynamic
business environments, organizations continuously refine their processes to
enhance efficiency, reduce costs, and improve customer satisfaction. Existing
techniques to process simulation discovery lack adaptability to real-time
operational changes. In this paper, we propose a streaming process simulation
discovery technique that integrates Incremental Process Discovery with Online
Machine Learning methods. This technique prioritizes recent data while
preserving historical information, ensuring adaptation to evolving process
dynamics. Experiments conducted on four different event logs demonstrate the
importance in simulation of giving more weight to recent data while retaining
historical knowledge. Our technique not only produces more stable simulations
but also exhibits robustness in handling concept drift, as highlighted in one
of the use cases.

</details>


### [4] [Solving Package Management via Hypergraph Dependency Resolution](https://arxiv.org/abs/2506.10803)
*Ryan Gibb,Patrick Ferris,David Allsopp,Michael Winston Dales,Mark Elvers,Thomas Gazagnaire,Sadiq Jaffer,Thomas Leonard,Jon Ludlam,Anil Madhavapeddy*

Main category: cs.SE

TL;DR: HyperRes is a formal system that uses hypergraphs to unify versioned dependency resolution across many package manager ecosystems, enabling true cross-language and cross-platform dependency management without changing users' current tools.


<details>
  <summary>Details</summary>
Motivation: The motivation is the current lack of interoperability between different package managers, which hampers multi-lingual projects from expressing precise, cross-language dependencies and makes many system and hardware dependencies implicit or unversioned.

Method: The paper defines HyperRes, a formal system based on a hypergraph model, to describe versioned dependency resolution. It includes translations from existing package managers to HyperRes and models dependency constraints across ecosystems.

Result: HyperRes is demonstrated to work across currently distinct package manager ecosystems, allowing cross-ecosystem dependency resolution without requiring users to change their choice of package manager.

Conclusion: HyperRes enables translation of packaging metadata and precise, environment-specialized dependency resolution across multiple ecosystems, enhancing interoperability and reducing fragmentation between package managers.

Abstract: Package managers are everywhere, with seemingly every language and operating
system implementing their own solution. The lack of interoperability between
these systems means that multi-lingual projects are unable to express precise
dependencies across language ecosystems, and external system and hardware
dependencies are typically implicit and unversioned. We define HyperRes, a
formal system for describing versioned dependency resolution using a hypergraph
that is expressive enough to model many ecosystems and solve dependency
constraints across them. We define translations from dozens of existing package
managers to HyperRes and comprehensively demonstrate that dependency resolution
can work across ecosystems that are currently distinct. This does not require
users to shift their choice of package managers; instead, HyperRes allows for
the translation of packaging metadata between ecosystems, and for solving to be
precisely specialised to a particular deployment environment.

</details>


### [5] [The Effects of GitHub Copilot on Computing Students' Programming Effectiveness, Efficiency, and Processes in Brownfield Programming Tasks](https://arxiv.org/abs/2506.10051)
*Md Istiak Hossain Shihab,Christopher Hundhausen,Ahsun Tariq,Summit Haque,Yunhan Qiao,Brian Mulanda*

Main category: cs.SE

TL;DR: GitHub Copilot helps undergraduates complete unfamiliar coding tasks faster and with less manual effort, but students are worried they don’t fully grasp the AI’s solutions. Educators need to adapt teaching so students get the most from AI while still understanding the code.


<details>
  <summary>Details</summary>
Motivation: The increasing integration of generative AI coding assistants like GitHub Copilot in software development is changing industry practices. However, the impact of such tools on undergraduate students' effectiveness in working on legacy (brownfield) code has not been thoroughly studied.

Method: A controlled experiment was conducted involving 10 undergraduate computer science students. Each student completed similar brownfield development tasks, both with and without the aid of GitHub Copilot, in an unfamiliar legacy web application. A mixed-methods approach was used, combining quantitative performance and behavioral analysis with qualitative exit interviews.

Result: With Copilot, students completed programming tasks 35% faster and made 50% more progress on solutions. Students also spent 11% less time manually writing code and 12% less time conducting web searches. However, interviews revealed students were concerned about not fully understanding Copilot’s suggestions.

Conclusion: GenAI tools like Copilot significantly increase undergraduate students' efficiency and progress when working on brownfield programming tasks. There are, however, valid concerns about superficial understanding of generated code. The findings highlight the need for educators to update teaching methods to both utilize GenAI benefits and promote critical reflection about the code such tools generate.

Abstract: When graduates of computing degree programs enter the software industry, they
will most likely join teams working on legacy code bases developed by people
other than themselves. In these so-called brownfield software development
settings, generative artificial intelligence (GenAI) coding assistants like
GitHub Copilot are rapidly transforming software development practices, yet the
impact of GenAI on student programmers performing brownfield development tasks
remains underexplored. This paper investigates how GitHub Copilot influences
undergraduate students' programming performance, behaviors, and understanding
when completing brownfield programming tasks in which they add new code to an
unfamiliar code base. We conducted a controlled experiment in which 10
undergraduate computer science students completed highly similar brownfield
development tasks with and without Copilot in a legacy web application. Using a
mixed-methods approach combining performance analysis, behavioral analysis, and
exit interviews, we found that students completed tasks 35% faster (p < 0.05)
and made 50% more solution progress p (< 0.05) when using Copilot. Moreover,
our analysis revealed that, when using Copilot, students spent 11% less time
manually writing code (p < 0.05), and 12% less time conducting web searches (p
< 0.05), providing evidence of a fundamental shift in how they engaged in
programming. In exit interviews, students reported concerns about not
understanding how or why Copilot suggestions work. This research suggests the
need for computing educators to develop new pedagogical approaches that
leverage GenAI assistants' benefits while fostering reflection on how and why
GenAI suggestions address brownfield programming tasks. Complete study results
and analysis are presented at https://ghcopilot-icer.github.io/.

</details>


### [6] [Prompt Variability Effects On LLM Code Generation](https://arxiv.org/abs/2506.10204)
*Andrei Paleyes,Radzim Sendyka,Diana Robinson,Christian Cabrera,Neil D. Lawrence*

Main category: cs.SE

TL;DR: The authors present two general evaluation methods for testing code generation by LLMs, focusing on how prompt quality and user background affect output. Their approaches are model- and task-independent, experimentally validated, and made available for community use.


<details>
  <summary>Details</summary>
Motivation: Code generation with Large Language Models (LLMs) is greatly influenced by the quality of prompts, which in turn can be affected by the user's background and programming familiarity. There is a need to assess how sensitive LLMs are to variations in user input and to user personas.

Method: The authors propose a synthetic evaluation pipeline specifically for code generation with LLMs, alongside a systematic persona-based evaluation approach. These methods aim to identify qualitative differences in LLM outputs depending on user backgrounds, and are designed to be task- and model-agnostic.

Result: The authors provide experimental evidence supporting the effectiveness and utility of their evaluation methods, and share their codebase for community use.

Conclusion: The paper demonstrates that their synthetic and persona-based evaluation methods can reveal important differences in LLM code generation quality based on user input variations and backgrounds, offering broadly applicable tools for assessing LLM performance in code generation.

Abstract: Code generation is one of the most active areas of application of Large
Language Models (LLMs). While LLMs lower barriers to writing code and
accelerate development process, the overall quality of generated programs
depends on the quality of given prompts. Specifically, functionality and
quality of generated code can be sensitive to user's background and familiarity
with software development. It is therefore important to quantify LLM's
sensitivity to variations in the input. To this end we propose a synthetic
evaluation pipeline for code generation with LLMs, as well as a systematic
persona-based evaluation approach to expose qualitative differences of LLM
responses dependent on prospective user background. Both proposed methods are
completely independent from specific programming tasks and LLMs, and thus are
widely applicable. We provide experimental evidence illustrating utility of our
methods and share our code for the benefit of the community.

</details>


### [7] [AI-Based Software Vulnerability Detection: A Systematic Literature Review](https://arxiv.org/abs/2506.10280)
*Samiha Shimmi,Hamed Okhravi,Mona Rahimi*

Main category: cs.SE

TL;DR: A systematic review shows AI-based, especially graph-based, models leading software vulnerability detection from 2018 to 2023, but points to persistent issues with data, reproducibility, and interpretability, and indicates opportunities in emerging techniques.


<details>
  <summary>Details</summary>
Motivation: Software vulnerabilities are a major cybersecurity threat, and traditional detection methods are being surpassed by AI-driven approaches. There is a need for a systematic review summarizing advances in this field and outlining current techniques, trends, and gaps, to guide future research.

Method: This paper conducts a systematic literature review of software vulnerability detection research published between 2018 and 2023. It categorizes techniques, feature representations, and embedding methods, analyzes trends, and identifies limitations and future directions.

Result: A comprehensive taxonomy of SVD research is provided. 91% of studies use AI-based methods, with graph-based models as the most common. Major limitations in the field include dataset quality, lack of reproducibility, and challenges with interpretability. The review also identifies opportunities for future work in areas such as federated learning and quantum neural networks.

Conclusion: AI-driven methods, especially graph-based models, dominate recent SVD research. However, the field faces challenges in data quality, reproducibility, and model interpretability. Exploring new approaches like federated learning and quantum neural networks represents a promising research direction.

Abstract: Software vulnerabilities in source code pose serious cybersecurity risks,
prompting a shift from traditional detection methods (e.g., static analysis,
rule-based matching) to AI-driven approaches. This study presents a systematic
review of software vulnerability detection (SVD) research from 2018 to 2023,
offering a comprehensive taxonomy of techniques, feature representations, and
embedding methods. Our analysis reveals that 91% of studies use AI-based
methods, with graph-based models being the most prevalent. We identify key
limitations, including dataset quality, reproducibility, and interpretability,
and highlight emerging opportunities in underexplored techniques such as
federated learning and quantum neural networks, providing a roadmap for future
research.

</details>


### [8] [Minimizing False Positives in Static Bug Detection via LLM-Enhanced Path Feasibility Analysis](https://arxiv.org/abs/2506.10322)
*Xueying Du,Kai Yu,Chong Wang,Yi Zou,Wentai Deng,Zuoyu Ou,Xin Peng,Lingming Zhang,Yiling Lou*

Main category: cs.SE

TL;DR: LLM4PFA, a novel LLM-based framework, dramatically lowers false positives in static bug detection within large projects by performing advanced, agent-driven path feasibility analysis, and sets a new performance standard over existing techniques.


<details>
  <summary>Details</summary>
Motivation: Existing static bug analyzers have high false positive rates, mainly due to challenges in path feasibility validation amidst conditional branches and complex data dependencies. Current LLM-based approaches also fall short due to weak constraint cascade analysis and scalability issues in large projects.

Method: The paper proposes LLM4PFA, an iterative path feasibility analysis framework. It leverages LLM agent-based targeted constraint reasoning combined with context-aware analysis, driven by agent planning, to improve complex inter-procedural path feasibility and reduce false positives in static bug detection.

Result: LLM4PFA was able to filter out 72% to 96% of false positives in static bug detection, outperforming baselines by 41.1% to 105.7%. It missed only 3 real bugs out of 45 true positives, demonstrating high precision and effectiveness.

Conclusion: LLM4PFA significantly reduces false positives in static bug analysis of large codebases, offering scalable, precise path feasibility validation, and surpasses current state-of-the-art methods.

Abstract: Static bug analyzers play a crucial role in ensuring software quality.
However, existing analyzers for bug detection in large codebases often suffer
from high false positive rates. This is primarily due to the limited
capabilities of analyzers in path feasibility validation with multiple
conditional branches and complex data dependencies. While current LLM-based
approaches attempt to address this issue, their effectiveness remains limited
due to insufficient constraint cascade analysis and scalability challenges in
large projects. To address this challenge, we propose an iterative path
feasibility analysis framework LLM4PFA. By leveraging LLM agent based targeted
constraint reasoning, and key context-aware analysis driven by agent planning,
LLM4PFA effectively enhances complex inter-procedural path feasibility analysis
for minimizing false positives in static bug detection. Evaluation results show
that LLM4PFA precisely filters out 72% to 96% false positives reported during
static bug detection, significantly outperforming all the baselines by 41.1% -
105.7% improvements; meanwhile LLM4PFA only misses 3 real bugs of 45 true
positives.

</details>


### [9] [Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements](https://arxiv.org/abs/2506.10330)
*Seyed Moein Abtahi,Akramul Azim*

Main category: cs.SE

TL;DR: Integrating LLMs and retrieval-augmented techniques with static code analysis automates code issue fixing, significantly improves software quality, and reduces workload, while a custom tool manages LLM inaccuracies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to automate the detection and correction of code issues (bugs, vulnerabilities, code smells) in large-scale software projects to improve code quality and reduce the resources and time required for manual revision.

Method: The method involves integrating large language models (GPT-3.5 Turbo, GPT-4o) with a static code analysis framework. Detected issues are fed into an LLM-powered revision system, with iterative prompt engineering for improved output quality and RAG (retrieval-augmented generation) for enhanced contextual accuracy. A 'Code Comparison App' is used to filter and correct hallucinated LLM outputs before changes are merged.

Result: The approach led to a significant reduction in code issues, as observed by repeated runs of the static code analysis tool after automated revisions.

Conclusion: Combining large language models, static code analysis, and retrieval-augmented generation can effectively automate code issue detection and correction, thereby improving code quality and development efficiency.

Abstract: This study examined code issue detection and revision automation by
integrating Large Language Models (LLMs) such as OpenAI's GPT-3.5 Turbo and
GPT-4o into software development workflows. A static code analysis framework
detects issues such as bugs, vulnerabilities, and code smells within a
large-scale software project. Detailed information on each issue was extracted
and organized to facilitate automated code revision using LLMs. An iterative
prompt engineering process is applied to ensure that prompts are structured to
produce accurate and organized outputs aligned with the project requirements.
Retrieval-augmented generation (RAG) is implemented to enhance the relevance
and precision of the revisions, enabling LLM to access and integrate real-time
external knowledge. The issue of LLM hallucinations - where the model generates
plausible but incorrect outputs - is addressed by a custom-built "Code
Comparison App," which identifies and corrects erroneous changes before
applying them to the codebase. Subsequent scans using the static code analysis
framework revealed a significant reduction in code issues, demonstrating the
effectiveness of combining LLMs, static analysis, and RAG to improve code
quality, streamline the software development process, and reduce time and
resource expenditure.

</details>


### [10] [AutoGEEval++: A Multi-Level and Multi-Geospatial-Modality Automated Evaluation Framework for Large Language Models in Geospatial Code Generation on Google Earth Engine](https://arxiv.org/abs/2506.10365)
*Shuyang Hou,Zhangxiao Shen,Huayi Wu,Haoyue Jiao,Ziqi Liu,Lutong Xie,Chang Liu,Jianyuan Liang,Yaxian Qing,Xiaopu Zhang,Dehua Peng,Zhipeng Gui,Xuefeng Guan*

Main category: cs.SE

TL;DR: AutoGEEval++ is a novel, comprehensive automated system and benchmark for evaluating large language models generating geospatial code in Google Earth Engine, providing standardized, detailed, and scalable performance assessment across 24 models, and setting the standard for domain-specific code evaluation.


<details>
  <summary>Details</summary>
Motivation: With the rise of artificial intelligence in geoscientific code generation, current methods lack standardized, automated evaluation tools—specifically for geospatial code written for platforms like Google Earth Engine (GEE). This creates a need for reliable benchmarks and protocols to assess the performance of large language models (LLMs) in this specialized domain.

Method: The authors introduce AutoGEEval++, an enhanced evaluation framework and benchmark dataset for assessing LLMs that generate geospatial code on GEE. The system includes a comprehensive suite of 6,365 test cases, spanning multiple data types and complexity levels, and evaluates code across various metrics—accuracy, resource usage, run-time, and error analysis. AutoGEEval++ includes submission and judge modules, enabling full automation from code generation to execution-based validation.

Result: AutoGEEval++ was used to systematically evaluate 24 state-of-the-art LLMs from different categories, demonstrating differences in performance, stability, and errors across models and tasks. The results validated both the practical utility and scalability of AutoGEEval++ for evaluating domain-specific code generation by LLMs.

Conclusion: This research provides the first standardized evaluation protocol and large-scale benchmark for GEE-based LLM code generation. AutoGEEval++ supports more rigorous and unified model comparison and establishes a methodological foundation for future developments in automated, domain-specific geospatial code assessment.

Abstract: Geospatial code generation is becoming a key frontier in integrating
artificial intelligence with geo-scientific analysis, yet standardised
automated evaluation tools for this task remain absent. This study presents
AutoGEEval++, an enhanced framework building on AutoGEEval, and the first
automated assessment system for large language models (LLMs) generating
geospatial code on Google Earth Engine (GEE). It supports diverse data
modalities and varying task complexities. Built on the GEE Python API,
AutoGEEval++ features a benchmark dataset-AutoGEEval++-Bench-with 6,365 test
cases across 26 data types and three task categories: unit, combo, and theme
tests. It includes a submission programme and a judge module to realise an
end-to-end automated evaluation pipeline from code generation to
execution-based validation. The framework adopts multi-dimensional
metrics-accuracy, resource usage, run-time efficiency, and error
types-balancing hallucination control and efficiency, and enabling boundary
testing and error pattern analysis. Using AutoGEEval++, we evaluate 24
state-of-the-art LLMs (as of June 2025), including general-purpose,
reasoning-enhanced, code-centric, and geoscience-specific models. Results
reveal clear performance, stability, and error differences across task types,
model designs, and deployment settings, confirming AutoGEEval++'s practical
value and scalability in vertical-domain code generation. This work establishes
the first standardised evaluation protocol and foundational benchmark for
GEE-based LLM code generation, providing a unified basis for performance
comparison and a methodological framework for systematic, domain-specific code
evaluation.

</details>


### [11] [MLLM-Based UI2Code Automation Guided by UI Layout Information](https://arxiv.org/abs/2506.10376)
*Fan Wu,Cuiyun Gao,Shuqing Li,Xin-Cheng Wen,Qing Liao*

Main category: cs.SE

TL;DR: The paper introduces LayoutCoder, a novel framework using multimodal large language models to convert webpage images into accurate UI code while preserving layout. It significantly outperforms previous methods, aided by new techniques for layout understanding and a robust new benchmark dataset.


<details>
  <summary>Details</summary>
Motivation: Converting user interfaces to code (UI2Code) is a time-consuming, labor-intensive step in website development. Automating this process would significantly improve development efficiency, but existing deep learning methods require extensive labeled data and fail to generalize to new real-world designs. Multimodal Large Language Models (MLLMs) could help but struggle with complex layouts and accurate code generation.

Method: The paper proposes LayoutCoder, a novel MLLM-based framework for generating UI code from real-world webpage images. LayoutCoder has three main modules: (1) Element Relation Construction that groups UI components by similar structures, (2) UI Layout Parsing to generate layout trees, and (3) Layout-Guided Code Fusion for accurate code with preserved layouts. They also introduce a new dataset, Snap2Code, of 350 real-world websites split into seen and unseen sets.

Result: LayoutCoder demonstrates superior performance to state-of-the-art methods. Empirical results show a 10.14% improvement in BLEU score and a 3.95% improvement in CLIP score, averaged across datasets, compared with the best-performing baseline. Extensive evaluations were conducted on both the new Snap2Code dataset and the established Design2Code dataset.

Conclusion: LayoutCoder outperforms existing UI2Code methods by effectively capturing and preserving complex UI layouts in generated code, thanks to its layout-aware MLLM framework. Its superior generalization and code accuracy are validated across multiple datasets, demonstrating practical benefits for real-world website development.

Abstract: Converting user interfaces into code (UI2Code) is a crucial step in website
development, which is time-consuming and labor-intensive. The automation of
UI2Code is essential to streamline this task, beneficial for improving the
development efficiency. There exist deep learning-based methods for the task;
however, they heavily rely on a large amount of labeled training data and
struggle with generalizing to real-world, unseen web page designs. The advent
of Multimodal Large Language Models (MLLMs) presents potential for alleviating
the issue, but they are difficult to comprehend the complex layouts in UIs and
generate the accurate code with layout preserved. To address these issues, we
propose LayoutCoder, a novel MLLM-based framework generating UI code from
real-world webpage images, which includes three key modules: (1) Element
Relation Construction, which aims at capturing UI layout by identifying and
grouping components with similar structures; (2) UI Layout Parsing, which aims
at generating UI layout trees for guiding the subsequent code generation
process; and (3) Layout-Guided Code Fusion, which aims at producing the
accurate code with layout preserved. For evaluation, we build a new benchmark
dataset which involves 350 real-world websites named Snap2Code, divided into
seen and unseen parts for mitigating the data leakage issue, besides the
popular dataset Design2Code. Extensive evaluation shows the superior
performance of LayoutCoder over the state-of-the-art approaches. Compared with
the best-performing baseline, LayoutCoder improves 10.14% in the BLEU score and
3.95% in the CLIP score on average across all datasets.

</details>


### [12] [Bug Classification in Quantum Software: A Rule-Based Framework and Its Evaluation](https://arxiv.org/abs/2506.10397)
*Mir Mohammad Yousuf,Shabir Ahmad Sofi*

Main category: cs.SE

TL;DR: A specialized, automated framework accurately classifies bugs in quantum software repositories, revealing that classical bugs predominate and pointing to areas needing further improvement, especially severity detection.


<details>
  <summary>Details</summary>
Motivation: The increasing importance of quantum software demands higher quality and reliability, making accurate classification of software bugs critical to guide improvements and maintenance efforts. Existing approaches lack robust methods tuned to the unique characteristics and challenges of quantum software.

Method: The paper develops a rule-based automated framework that uses keyword and heuristic-based strategies tailored specifically for quantum computing. It classifies issues in quantum software repositories by various dimensions (bug type, category, severity, impacted quality attributes, and quantum-specific bug types). The framework's reliability is evaluated by manually classifying a stratified sample of 4,984 issues and comparing automated results to this ground truth using accuracy metrics and statistical validation.

Result: The framework achieved up to 85.21% accuracy, with F1-scores between 0.7075 (severity) and 0.8393 (quality attribute). Cohen's Kappa showed strong agreement for most dimensions, except severity, which had only slight agreement. Large-scale analysis revealed that classical bugs are more common than quantum-specific ones, and most bugs are low severity. The most frequent bug categories were compatibility, functional, and quantum-specific defects, and the main impacted quality attributes were usability, maintainability, and interoperability.

Conclusion: The automated classification framework is effective for distinguishing between various bug types and categories in quantum software, with strong agreement to manual classification except for severity. Classical bugs dominate quantum software repositories, but quantum-specific bugs constitute a significant portion. Severity classification requires further enhancement. The analysis provides actionable insights for improving quantum software quality.

Abstract: Accurate classification of software bugs is essential for improving software
quality. This paper presents a rule-based automated framework for classifying
issues in quantum software repositories by bug type, category, severity, and
impacted quality attributes, with additional focus on quantum-specific bug
types. The framework applies keyword and heuristic-based techniques tailored to
quantum computing. To assess its reliability, we manually classified a
stratified sample of 4,984 issues from a dataset of 12,910 issues across 36
Qiskit repositories. Automated classifications were compared with ground truth
using accuracy, precision, recall, and F1-score. The framework achieved up to
85.21% accuracy, with F1-scores ranging from 0.7075 (severity) to 0.8393
(quality attribute). Statistical validation via paired t-tests and Cohen's
Kappa showed substantial to almost perfect agreement for bug type (k = 0.696),
category (k = 0.826), quality attribute (k = 0.818), and quantum-specific bug
type (k = 0.712). Severity classification showed slight agreement (k = 0.162),
suggesting room for improvement. Large-scale analysis revealed that classical
bugs dominate (67.2%), with quantum-specific bugs at 27.3%. Frequent bug
categories included compatibility, functional, and quantum-specific defects,
while usability, maintainability, and interoperability were the most impacted
quality attributes. Most issues (93.7%) were low severity; only 4.3% were
critical. A detailed review of 1,550 quantum-specific bugs showed that over
half involved quantum circuit-level problems, followed by gate errors and
hardware-related issues.

</details>


### [13] [Towards Understanding Bugs in Distributed Training and Inference Frameworks for Large Language Models](https://arxiv.org/abs/2506.10426)
*Xiao Yu,Haoxuan Chen,Feifei Niu,Xing Hu,Jacky Wai Keung,Xin Xia*

Main category: cs.SE

TL;DR: This paper empirically analyzed 308 fixed bugs in distributed LLM frameworks, finding that while many bugs are challenging due to system complexity, nearly half require only simple fixes. There is significant potential to automate debugging and repairs using LLM-based tools.


<details>
  <summary>Details</summary>
Motivation: The complexity of distributed training and inference frameworks for large language models leads to more software bugs, impacting performance and reliability. There is a need to systematically understand these bugs to improve quality and inform more effective debugging and repair methods.

Method: A large-scale empirical analysis of 308 fixed bugs was conducted across three major distributed frameworks: DeepSpeed, Megatron-LM, and Colossal-AI. The study examined aspects such as bug symptoms, root causes, how bugs were identified and fixed, and common strategies for low-effort fixes.

Result: The analysis revealed unique root causes tied to distributed systems (like allocation strategy and communication errors). Diagnosing and fixing these bugs is challenging due to symptom-root cause disconnects, high reproduction costs, and complex interactions. However, 48% of bug fixes required minimal code changes (<=10 LOC) and used straightforward strategies like logic or parameter tweaks, suggesting automation is feasible.

Conclusion: The findings highlight both the difficulties of debugging distributed frameworks and the significant potential for automation, especially via LLM-based tools. The insights will help improve the reliability of such frameworks and offer guidance for future research on automated debugging and repair.

Abstract: With the rapid development of large language models (LLMs), distributed
training and inference frameworks like DeepSpeed have become essential for
scaling model training and inference across multiple GPUs or nodes. However,
the increasing complexity of these frameworks brings non-trivial software bugs,
which may degrade training performance, cause unexpected failures, and result
in significant resource waste. Understanding framework bugs' characteristics is
fundamental for quality assurance, allowing the design of more effective
debugging and repair methods. Thus, our paper conducts the first large-scale
empirical analysis of 308 fixed bugs across three popular distributed
training/inference frameworks: DeepSpeed, Megatron-LM, and Colossal-AI. We
examine bug symptoms, root causes, bug identification and fixing efforts, and
common low-effort fixing strategies. Additionally, the distributed nature of
these frameworks introduces unique bug root causes, such as allocation strategy
error and distributed communication error. Diagnosing and fixing complex bugs
remains challenging due to factors like the disconnect between symptoms and
root causes, high bug reproduction costs, and low-level or cross-component
interactions. Interestingly, we observe that 48% of bug fixes require minimal
code changes (<=10 LOC) and follow simple strategies such as conditional logic
optimization, parameter handling enhancement, or version compatibility
handling, indicating potential for automation. Based on these insights, we
offer several implications for improving the reliability of both distributed
training and inference frameworks and their dependent LLM projects, while also
identifying opportunities to leverage LLM-based tools for automated debugging
and repair.

</details>


### [14] [EXPEREPAIR: Dual-Memory Enhanced LLM-based Repository-Level Program Repair](https://arxiv.org/abs/2506.10484)
*Fangwen Mu,Junjie Wang,Lin Shi,Song Wang,Shoubin Li,Qing Wang*

Main category: cs.SE

TL;DR: ExpeRepair enhances automated software repairs by learning from past experiences using both episodic and semantic memories, and outperforms existing methods with adaptive, context-aware prompts.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based software repair techniques often treat issues in isolation and use static prompting, limiting their adaptability and generalization. There is a need to leverage past repair experiences to improve automated issue resolution.

Method: ExpeRepair, a novel approach inspired by the dual memory model of human cognition, uses large language models and organizes repair history into episodic (concrete examples) and semantic (abstract insights) memories. During inference, it dynamically retrieves and integrates relevant information from both memory types to construct adaptive, context-aware prompts.

Result: Experiments on the SWE-bench Lite benchmark show that ExpeRepair, using Claude 3.7 Sonnet, achieves a pass@1 score of 49.3%, surpassing all current state-of-the-art open-source solutions.

Conclusion: ExpeRepair's dual-memory, experience-driven prompting substantially boosts automated software repair performance, highlighting the advantages of marrying episodic and semantic memory with dynamic prompt construction for LLM-based repair systems.

Abstract: Automatically repairing software issues remains a fundamental challenge at
the intersection of software engineering and AI. Although recent advancements
in Large Language Models (LLMs) have demonstrated potential for
repository-level repair tasks, current methodologies exhibit two notable
limitations: (1) they often address issues in isolation, neglecting to
incorporate insights from previously resolved issues, and (2) they rely on
static and rigid prompting strategies, which constrain their ability to
generalize across diverse and evolving issue scenarios. Inspired by the dual
memory systems of human cognition, where episodic and semantic memories work
synergistically to support human reasoning and decision-making, we propose
ExpeRepair, a novel LLM-based approach that continuously learns from historical
repair experiences through dual-channel knowledge accumulation. ExpeRepair
organizes historical repair experiences into two complementary memories: an
episodic memory that stores concrete repair demonstrations, and a semantic
memory that encodes abstract reflective insights. At inference time, ExpeRepair
activates both memory systems by retrieving relevant demonstrations from
episodic memory and recalling high-level repair insights from semantic memory.
It further enhances adaptability through dynamic prompt composition,
synergistically integrating both memory types to replace static prompts with
context-aware, experience-driven prompts. Experiments on the SWE-bench Lite
benchmark demonstrate that ExpeRepair achieves a pass@1 score of 49.3% with
Claude 3.7 Sonnet, outperforming all state-of-the-art open-source methods.

</details>


### [15] [BugGen: A Self-Correcting Multi-Agent LLM Pipeline for Realistic RTL Bug Synthesis](https://arxiv.org/abs/2506.10501)
*Surya Jasper,Minh Luu,Evan Pan,Aakash Tyagi,Michael Quinn,Jiang Hu,David Kebo Houngninou*

Main category: cs.SE

TL;DR: BugGen is an automated system that uses AI to generate and validate realistic hardware bugs, producing accurate bug datasets much faster than humans and existing tools. It helps reveal gaps in verification and boosts the performance of machine learning models for debugging, making it a valuable new resource for hardware verification teams.


<details>
  <summary>Details</summary>
Motivation: As hardware systems grow in complexity, verifying their correctness becomes increasingly difficult and resource-intensive. There is a clear need for better, scalable methods to generate realistic bug datasets, which are essential for improving the effectiveness of machine learning (ML)-assisted debugging and verification. Existing approaches to bug insertion—whether manual or automated—often fail to produce diverse and reliable bugs at scale, limiting the potential of ML-based solutions.

Method: The authors introduce BugGen, an autonomous multi-agent pipeline that uses Large Language Models (LLMs) to generate, insert, and validate realistic functional bugs in RTL (Register Transfer Level) designs. BugGen automatically partitions RTL modules, selects mutation targets with a closed-loop agent-based approach, and applies iterative refinement and rollback mechanisms to ensure each bug is both syntactically correct and functionally detectable. Evaluation is performed on five OpenTitan IP blocks.

Result: BugGen produced 500 unique bugs with a 94% functional accuracy rate and a throughput of 17.7 validated bugs per hour, which is more than five times faster than manual expert insertion. It also revealed 104 previously undetected bugs in OpenTitan regressions, demonstrating its effectiveness in uncovering verification gaps. Compared to Certitude, BugGen achieved over twice the syntactic accuracy and generated more complex and functionally relevant bug scenarios. When datasets generated by BugGen were used to train ML-based failure triage models, these models achieved classification accuracy rates of 88.1% to 93.2% across various IP blocks.

Conclusion: BugGen offers a scalable, autonomous solution for generating high-quality bug datasets, which greatly improves the efficiency of hardware verification and the performance of ML-assisted debugging. Its ability to rapidly create diverse and realistic bugs not only enhances ML training but also exposes critical verification blind spots that are often missed by traditional approaches.

Abstract: Hardware complexity continues to strain verification resources, motivating
the adoption of machine learning (ML) methods to improve debug efficiency.
However, ML-assisted debugging critically depends on diverse and scalable bug
datasets, which existing manual or automated bug insertion methods fail to
reliably produce. We introduce BugGen, a first of its kind, fully autonomous,
multi-agent pipeline leveraging Large Language Models (LLMs) to systematically
generate, insert, and validate realistic functional bugs in RTL. BugGen
partitions modules, selects mutation targets via a closed-loop agentic
architecture, and employs iterative refinement and rollback mechanisms to
ensure syntactic correctness and functional detectability. Evaluated across
five OpenTitan IP blocks, BugGen produced 500 unique bugs with 94% functional
accuracy and achieved a throughput of 17.7 validated bugs per hour-over five
times faster than typical manual expert insertion. Additionally, BugGen
identified 104 previously undetected bugs in OpenTitan regressions,
highlighting its utility in exposing verification coverage gaps. Compared
against Certitude, BugGen demonstrated over twice the syntactic accuracy,
deeper exposure of testbench blind spots, and more functionally meaningful and
complex bug scenarios. Furthermore, when these BugGen-generated datasets were
employed to train ML-based failure triage models, we achieved high
classification accuracy (88.1%-93.2%) across different IP blocks, confirming
the practical utility and realism of generated bugs. BugGen thus provides a
scalable solution for generating high-quality bug datasets, significantly
enhancing verification efficiency and ML-assisted debugging.

</details>


### [16] [AdaptiveLLM: A Framework for Selecting Optimal Cost-Efficient LLM for Code-Generation Based on CoT Length](https://arxiv.org/abs/2506.10525)
*Junhang Cheng,Fang Liu,Chengru Wu,Li Zhang*

Main category: cs.SE

TL;DR: AdaptiveLLM dynamically selects the best LLM for code tasks by automatically estimating task difficulty, boosting accuracy by up to 15% and cutting computing costs nearly 90%, all without needing human-labeled data.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) improve code generation, but struggle to balance high performance with low inference costs for varied programming tasks. Existing solutions for dynamic model selection are resource-intensive and depend on human-labeled difficulty data, which may not match the LLM's understanding or be easily available in real-world applications.

Method: This paper proposes AdaptiveLLM, which uses automatically measured Chain-of-Thought (CoT) lengths from a reasoning model to estimate code task difficulty. Using k-means clustering, tasks are divided into three difficulty levels. CodeBERT is fine-tuned to recognize difficulty-aware features, and an XGBoost classifier selects the most efficient LLM for each task based on this information, optimizing efficiency and performance trade-offs.

Result: AdaptiveLLM improves code generation accuracy (7.86% better pass@1 score) and slashes computational resource usage by 88.9% compared to the previous method (ComplexityNet). It also achieves about 15% higher accuracy than using a single LLM model, with comparable cost. The automatic difficulty assessment using CoT lengths is shown to be more reliable than human evaluations.

Conclusion: AdaptiveLLM addresses both the cost and performance challenges of dynamic LLM selection for code generation by auto-assessing task difficulty, leading to both higher accuracy and greatly reduced computational costs. The proposed approach enables practical deployment of adaptive model selection in real-world settings without relying on human-labeled data.

Abstract: While Large Language Models (LLMs) have significantly advanced code
generation efficiency, they face inherent challenges in balancing performance
and inference costs across diverse programming tasks. Dynamically selecting the
optimal LLM based on task difficulty and resource constraints offers a
promising approach to achieve an optimal balance between efficiency and
performance. However, existing model selection methods are resource-intensive
and often neglect cost efficiency. Moreover, these approaches rely on
human-annotated difficulty labels that are frequently inaccessible in
real-world settings and may not align with the LLM's own assessment of task
difficulty. In this paper, we introduce AdaptiveLLM, a framework that
dynamically selects optimal LLMs for a given coding task by automatically
assessing task difficulty. Our framework first estimates task difficulty using
Chain-of-Thought lengths generated by reasoning model, clusters these into
three difficulty levels via k-means, and fine-tunes CodeBERT to embed
difficulty-aware features. A trained XGBoost classifier then selects the best
model for each problem, optimizing the performance-cost trade-off. Experimental
results show that AdaptiveLLM achieves a 7.86% improvement in pass@1 score
while reducing resource consumption by 88.9% compared to baseline method
ComplexityNet. When compared to a single model, AdaptiveLLM demonstrates an
approximately 15% accuracy improvement, while maintaining the same level of
cost consumption. Apart from that, the difficulty assessment using CoT provides
more reliable selection criteria than human evaluation. Our replication package
is available at https://github.com/cjhCoder7/AdaptiveLLM.

</details>


### [17] [Scalable Software Testing in Fast Virtual Platforms: Leveraging SystemC, QEMU and Containerization](https://arxiv.org/abs/2506.10624)
*Lukas Jünger,Jan Henrik Weinstock,Tim Kraus*

Main category: cs.SE

TL;DR: The paper proposes using containerized, open-source Virtual Platforms for cloud-based, parallel software testing before hardware is available, demonstrated in an AI accelerator case study, to accelerate HW/SW system development, especially in safety-critical fields.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of hardware/software (HW/SW) systems, especially in safety-critical industries like automotive, creates a need for extensive testing. However, hardware is often unavailable early on, which restricts timely software development and testing.

Method: The authors propose encapsulating Virtual Platforms (VPs) based on the SystemC TLM-2.0 standard using containerization technologies. This reduces environment dependencies and enables deployment in the cloud for faster and parallelized test execution. Open-source tools like QEMU and VCML are leveraged to eliminate licensing costs. The proposed method is demonstrated via a case study of an AI accelerator VP.

Result: The approach enables efficient pre-silicon execution and testing of unmodified target software, supporting fast, parallelized test execution in the cloud and eliminating the need for expensive licenses. The AI accelerator VP case study demonstrates practical applicability and robustness of the approach.

Conclusion: Containerized, open-source, cloud-deployable VPs provide a robust and practical solution for addressing HW/SW co-development challenges, specifically enabling early-stage, scalable software development and testing. This can significantly accelerate the development cycle for complex HW/SW systems.

Abstract: The ever-increasing complexity of HW/SW systems presents a persistent
challenge, particularly in safety-critical domains like automotive, where
extensive testing is imperative. However, the availability of hardware often
lags behind, hindering early-stage software development. To address this,
Virtual Platforms (VPs) based on the SystemC TLM-2.0 standard have emerged as a
pivotal solution, enabling pre-silicon execution and testing of unmodified
target software. In this study, we propose an approach leveraging
containerization to encapsulate VPs in order to reduce environment dependencies
and enable cloud deployment for fast, parallelized test execution, as well as
open-source VP technologies such as QEMU and VCML to obviate the need for seat
licenses. To demonstrate the efficacy of our approach, we present an Artificial
Intelligence (AI) accelerator VP case study. Through our research, we offer a
robust solution to address the challenges posed by the complexity of HW/SW
systems, with practical implications for accelerating HW/SW co-development.

</details>


### [18] [Not One to Rule Them All: Mining Meaningful Code Review Orders From GitHub](https://arxiv.org/abs/2506.10654)
*Abir Bouraffa,Carolin Brandt,Andy Zaidmann,Walid Maalej*

Main category: cs.SE

TL;DR: Most code reviewers don't comment on files in alphabetical order, preferring more meaningful approaches like focusing on larger changes or file relevance. These complex strategies are common in larger pull requests, suggesting review tools should offer better navigation flexibility aligned with actual reviewer preferences.


<details>
  <summary>Details</summary>
Motivation: Traditional tools display changed files in pull requests alphabetically, but this order may not suit reviewers' preferred navigation and review strategies. The study aims to understand actual navigation behaviors during code review, potentially informing tool design improvements.

Method: The researchers mined code review comments from 23,241 pull requests spanning 100 popular Java and Python repositories on GitHub. They analyzed the sequence in which reviewers commented on files to identify patterns beyond simple alphabetical ordering.

Result: 44.6% of pull requests had comments in a non-alphabetical order. Among these, 20.6% followed a largest-diff-first sequence, 17.6% were guided by file similarity to the pull request title/description, and 29% (where both production and test files changed) used a test-first order. Non-alphabetical reviews tended to have a higher proportion of reviewed files but received slightly fewer approvals.

Conclusion: Many reviewers do not follow alphabetical order; instead, they adopt meaningful, complex navigation strategies, particularly for larger pull requests. Code review tools should support flexible navigation to align with reviewer behavior.

Abstract: Developers use tools such as GitHub pull requests to review code, discuss
proposed changes, and request modifications. While changed files are commonly
presented in alphabetical order, this does not necessarily coincide with the
reviewer's preferred navigation sequence. This study investigates the different
navigation orders developers follow while commenting on changes submitted in
pull requests. We mined code review comments from 23,241 pull requests in 100
popular Java and Python repositories on GitHub to analyze the order in which
the reviewers commented on the submitted changes. Our analysis shows that for
44.6% of pull requests, the reviewers comment in a non-alphabetical order.
Among these pull requests, we identified traces of alternative meaningful
orders: 20.6% (2,134) followed a largest-diff-first order, 17.6% (1,827) were
commented in the order of the files' similarity to the pull request's title and
description, and 29% (1,188) of pull requests containing changes to both
production and test files adhered to a test-first order. We also observed that
the proportion of reviewed files to total submitted files was significantly
higher in non-alphabetically ordered reviews, which also received slightly
fewer approvals from reviewers, on average. Our findings highlight the need for
additional support during code reviews, particularly for larger pull requests,
where reviewers are more likely to adopt complex strategies rather than
following a single predefined order.

</details>


### [19] [Formalising Software Requirements using Large Language Models](https://arxiv.org/abs/2506.10704)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: VERIFAI is a project to automatically generate and trace formal specifications from natural language requirements using NLP, ontologies, LLMs, and AI, addressing key challenges in software requirements verification.


<details>
  <summary>Details</summary>
Motivation: Natural language requirements are difficult to trace and verify throughout the software development lifecycle. Manual translation to formal specifications is error-prone and time-consuming, and ensuring traceability from requirements to implementation remains a key challenge.

Method: The project explores automatic generation of formal specifications from natural language requirements. It leverages Natural Language Processing (NLP), ontologies to define system domains, similarity-based reuse of artefacts from similar systems, large language models to extract specifications, and artificial intelligence techniques to guide verification and traceability.

Result: The project has initiated research into integrating NLP, ontologies, similarity-based reuse, LLMs, and AI to automatically generate formal specifications and maintain traceability from design to verification.

Conclusion: VERIFAI aims to improve traceability and automated verification, promising more reliable software systems by bridging the gap between natural language requirements and formal software specifications using state-of-the-art AI methods.

Abstract: This paper is a brief introduction to our recently initiated project named
VERIFAI: Traceability and verification of natural language requirements. The
project addresses the challenges in the traceability and verification of formal
specifications through providing support for the automatic generation of the
formal specifications and the traceability of the requirements from the initial
software design stage through the systems implementation and verification.
Approaches explored in this project include Natural Language Processing, use of
ontologies to describe the software system domain, reuse of existing software
artefacts from similar systems (i.e. through similarity based reuse) and large
language models to identify and declare the specifications as well as use of
artificial intelligence to guide the process.

</details>


### [20] [From Tea Leaves to System Maps: Context-awareness in Monitoring Operational Machine Learning Models](https://arxiv.org/abs/2506.10770)
*Joran Leest,Claudia Raibulet,Patricia Lago,Ilias Gerostathopoulos*

Main category: cs.SE

TL;DR: ML model monitoring often fails due to lack of context awareness. This paper reviews the field and introduces the C-SAR framework, offering new ways to use contextual information for reliable, systematic ML monitoring, moving beyond simple anomaly detection.


<details>
  <summary>Details</summary>
Motivation: Traditional ML model monitoring focuses on detecting statistical anomalies but often misses failures caused by contextual misalignment, where real-world use deviates from original training assumptions. There is a lack of a shared framework for leveraging contextual information in ML monitoring.

Method: The authors conduct a systematic review of 94 primary studies from fields such as machine learning, data mining, databases, and software engineering. They synthesize their findings into a conceptual framework (C-SAR), which classifies types of contextual information relevant to ML monitoring.

Result: The study proposes the Contextual System--Aspect--Representation (C-SAR) framework to structure how contextual information is used in ML monitoring. They also identify 20 recurring, reusable patterns of system, aspect, and representation combinations, mapping these to specific monitoring activities.

Conclusion: The C-SAR framework offers a new approach for ML monitoring, shifting from basic anomaly detection to a more meaningful interpretation of context, which enables systematic root-cause analysis and more reliable monitoring. This perspective enables practitioners to construct "system maps" for comprehensive ML monitoring beyond simple statistical alerts.

Abstract: Machine learning (ML) models in production do not fail due to statistical
anomalies in their input data; they fail due to contextual misalignment -- when
their environment deviates from training assumptions, leading to unreliable
predictions. Effective ML monitoring requires rich contextual information to
move beyond detecting statistical shifts toward meaningful alerts and
systematic root-cause analysis. Yet, surprisingly, despite extensive research
in ML monitoring and related disciplines (drift detection, data validation,
out-of-distribution detection), there is no shared understanding of how to use
contextual information -- striking, given that monitoring involves
interpretation of information in context. In response, this paper presents a
systematic review to characterize and structure the various types of contextual
information in this domain. Our analysis examines 94 primary studies across
data mining, databases, software engineering, and ML. We introduce the
Contextual System--Aspect--Representation (C-SAR) framework, a conceptual model
that synthesizes our findings. We also identify 20 recurring and potentially
reusable patterns of specific system, aspect, and representation combinations,
and map them to the monitoring activities they support. This study provides a
new perspective on ML monitoring: from interpreting "tea leaves" of
observational statistics into constructing and managing "system maps" that
enable systematic and reliable ML monitoring practices.

</details>


### [21] [What Users Value and Critique: Large-Scale Analysis of User Feedback on AI-Powered Mobile Apps](https://arxiv.org/abs/2506.10785)
*Vinaik Chhetri,Krishna Upadhyay,A. B. Siddique,Umar Farooq*

Main category: cs.SE

TL;DR: The paper introduces a validated, large-scale analysis pipeline for AI mobile app reviews, enabling nuanced insights by extracting and clustering over a million sentiment-aspect pairs. It uncovers common themes in user satisfaction (productivity, reliability) and frustration (technical issues, pricing) that are often missed by traditional review analysis approaches.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to understand how users perceive, evaluate, and critique AI-powered features in mobile applications, which has not been thoroughly explored due to the massive quantity of user feedback.

Method: The paper develops and validates a multi-stage analysis pipeline that leverages a large, curated dataset of 292 AI-driven apps with 894K AI-specific reviews from Google Play. The pipeline includes human-labeled benchmarking and systematic assessment of large language models (LLMs) and various prompting strategies, with stages for review classification, aspect-sentiment extraction, and clustering. Each step is validated for accuracy and consistency.

Result: The pipeline enables high-precision, scalable analysis of user feedback, extracting over a million aspect-sentiment pairs and clustering them into 18 positive and 15 negative user topics. Key findings show that user feedback centers around a narrow set of themes: positive feedback highlights productivity, reliability, and personalization, while negative feedback identifies technical failures, pricing, and language support issues. Fine-grained analysis surfaces both satisfaction and frustration within individual reviews and provides a nuanced understanding missed by traditional analyses.

Conclusion: The proposed analysis pipeline offers a more nuanced and faithful reflection of real-world user experiences in AI-driven mobile apps, identifies both universal and category-specific drivers of satisfaction and frustration, and demonstrates the value of fine-grained, aspect-based sentiment analysis over traditional methods.

Abstract: Artificial Intelligence (AI)-powered features have rapidly proliferated
across mobile apps in various domains, including productivity, education,
entertainment, and creativity. However, how users perceive, evaluate, and
critique these AI features remains largely unexplored, primarily due to the
overwhelming volume of user feedback. In this work, we present the first
comprehensive, large-scale study of user feedback on AI-powered mobile apps,
leveraging a curated dataset of 292 AI-driven apps across 14 categories with
894K AI-specific reviews from Google Play. We develop and validate a
multi-stage analysis pipeline that begins with a human-labeled benchmark and
systematically evaluates large language models (LLMs) and prompting strategies.
Each stage, including review classification, aspect-sentiment extraction, and
clustering, is validated for accuracy and consistency. Our pipeline enables
scalable, high-precision analysis of user feedback, extracting over one million
aspect-sentiment pairs clustered into 18 positive and 15 negative user topics.
Our analysis reveals that users consistently focus on a narrow set of themes:
positive comments emphasize productivity, reliability, and personalized
assistance, while negative feedback highlights technical failures (e.g.,
scanning and recognition), pricing concerns, and limitations in language
support. Our pipeline surfaces both satisfaction with one feature and
frustration with another within the same review. These fine-grained,
co-occurring sentiments are often missed by traditional approaches that treat
positive and negative feedback in isolation or rely on coarse-grained analysis.
To this end, our approach provides a more faithful reflection of the real-world
user experiences with AI-powered apps. Category-aware analysis further uncovers
both universal drivers of satisfaction and domain-specific frustrations.

</details>


### [22] [Evaluating Large Language Models on Non-Code Software Engineering Tasks](https://arxiv.org/abs/2506.10833)
*Fabian C. Peña,Steffen Herbold*

Main category: cs.SE

TL;DR: SELU is a new benchmark for assessing LLMs on non-code SE tasks. Moderate decoder-only models excel, and code-focused pre-training adds minor benefits. The results advise model choices for real-world SE tasks and set groundwork for future benchmarks.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are already proven in code understanding and generation, but their effectiveness for non-code Software Engineering (SE) tasks is not well studied. There is a need to systematically evaluate LLMs on such non-code SE tasks.

Method: The authors introduce SELU, a comprehensive benchmark suite with 17 non-code SE tasks covering classification, regression, NER, and MLM. Data is collected from various software artifacts. They fine-tune 22 open-source LLMs, prompt 2 proprietary models, train 2 baselines, and use multiple evaluation metrics. They compare performances with statistical tests.

Result: Moderate-scale decoder-only models consistently perform best across non-code SE tasks, showing both high average results and little variation across tasks. Domain adaptation (code-centric pre-training) yields only modest gains.

Conclusion: The findings recommend moderate-scale decoder-only models for non-code SE workflows, suggest that code pre-training isn't always crucial, and propose expanding the SELU benchmark for more generative and design-oriented future tasks.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
code understanding and generation; however, their effectiveness on non-code
Software Engineering (SE) tasks remains underexplored. We present the first
comprehensive benchmark, which we name `Software Engineering Language
Understanding' (SELU), for evaluating LLMs on 17 non-code tasks, spanning from
identifying whether a requirement is functional or non-functional to estimating
the effort and complexity of backlog items. SELU covers classification,
regression, Named Entity Recognition (NER), and Masked Language Modeling (MLM)
targets, with data drawn from diverse sources such as code repositories, issue
tracking systems, and developer forums. We fine-tune 22 open-source LLMs,
prompt two proprietary alternatives, and train two baselines. Performance is
measured using metrics such as F1-macro, SMAPE, F1-micro, and accuracy, and
compared via the Bayesian signed-rank test. Our results show that
moderate-scale decoder-only models consistently form a top-tier, exhibiting
high mean performance and low across-task variance, while domain adaptation via
code-focused pre-training might yield only modest improvements. These insights
guide model selection for non-code SE workflows and highlight directions for
expanding SELU to generative and design-oriented scenarios.

</details>


### [23] [MultiCoSim: A Python-based Multi-Fidelity Co-Simulation Framework](https://arxiv.org/abs/2506.10869)
*Quinn Thibeault,Giulia Pedrielli*

Main category: cs.SE

TL;DR: MultiCoSim is a new Python-based framework that makes cyber-physical system co-simulation more flexible, automated, and modular, addressing key limitations of prior tools and enabling easier integration and evaluation of diverse CPS components.


<details>
  <summary>Details</summary>
Motivation: As cyber-physical systems (CPS) become more complex and incorporate diverse hardware, software, and physical processes, existing simulation tools struggle with co-simulation, modularity, and automation. This limits the development of reusable benchmarks and impedes comprehensive evaluation, especially in safety-critical and learning-enabled domains.

Method: The authors present MultiCoSim, a Python-based simulation framework that enables programmatic definition, composition, and configuration of simulation components. MultiCoSim supports distributed, component-based co-simulation and allows easy substitution and reconfiguration of components. Case studies showcase its flexibility through integration with custom controllers and established platforms like PX4 for aerial robotics.

Result: MultiCoSim simplifies the process of composing and managing simulations involving heterogeneous CPS components. The authors successfully demonstrate, via real-world case studies, that MultiCoSim streamlines co-simulation tasks and supports integration with both custom components and off-the-shelf solutions.

Conclusion: MultiCoSim overcomes key limitations of prior simulation tools, enabling greater automation, flexibility, and modularity in CPS co-simulation. This supports more efficient and reusable simulation pipelines for research, development, and evaluation in increasingly complex CPS environments.

Abstract: Simulation is a foundational tool for the analysis and testing of
cyber-physical systems (CPS), underpinning activities such as algorithm
development, runtime monitoring, and system verification. As CPS grow in
complexity and scale, particularly in safety-critical and learning-enabled
settings, accurate analysis and synthesis increasingly rely on the rapid use of
simulation experiments. Because CPS inherently integrate hardware, software,
and physical processes, simulation platforms must support co-simulation of
heterogeneous components at varying levels of fidelity. Despite recent advances
in high-fidelity modeling of hardware, firmware, and physics, co-simulation in
diverse environments remains challenging. These limitations hinder the
development of reusable benchmarks and impede the use of simulation for
automated and comparative evaluation.
  Existing simulation tools often rely on rigid configurations, lack automation
support, and present obstacles to portability and modularity. Many are
configured through static text files or impose constraints on how simulation
components are represented and connected, making it difficult to flexibly
compose systems or integrate components across platforms.
  To address these challenges, we introduce MultiCoSim, a Python-based
simulation framework that enables users to define, compose, and configure
simulation components programmatically. MultiCoSim supports distributed,
component-based co-simulation and allows seamless substitution and
reconfiguration of components. We demonstrate the flexibility of MultiCoSim
through case studies that include co-simulations involving custom
automaton-based controllers, as well as integration with off-the-shelf
platforms like the PX4 autopilot for aerial robotics. These examples highlight
MultiCoSim's capability to streamline CPS simulation pipelines for research and
development.

</details>


### [24] [SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks](https://arxiv.org/abs/2506.10954)
*Lianghong Guo,Yanlin Wang,Caihua Li,Pengyu Yang,Jiachi Chen,Wei Tao,Yingtian Zou,Duyu Tang,Zibin Zheng*

Main category: cs.SE

TL;DR: SWE-Factory is an automated pipeline that efficiently builds large-scale, accurate datasets for GitHub issue resolution, drastically reducing manual effort and cost. Its grading and validation are highly reliable, enabling rapid dataset creation for software engineering LLMs.


<details>
  <summary>Details</summary>
Motivation: Constructing large-scale, high-quality datasets for the GitHub issue resolution task is essential for training and evaluating the software engineering capabilities of LLMs. However, traditional methods are labor-intensive and challenging, especially in setting up environments, grading, and task validation.

Method: The paper introduces SWE-Factory, an automated pipeline composed of three core components: (1) SWE-Builder, a multi-agent system for automating the evaluation environment setup, (2) a standard exit-code-based grading method instead of manual custom parsers, and (3) automated fail2pass validation using exit code signals. The pipeline was tested on 671 issues across four programming languages, comparing LLMs like GPT-4.1-mini and Gemini-2.5-flash.

Result: SWE-Factory constructed 269 valid task instances with GPT-4.1-mini at $0.045 each and achieved similar results with Gemini-2.5-flash at $0.024 each. The exit-code-based grading method achieved 100% accuracy compared to manual inspection, and the automated fail2pass validation showed a precision of 0.92 and a recall of 1.00.

Conclusion: The proposed automated pipeline significantly streamlines the creation of large-scale GitHub issue resolution datasets with high accuracy and reduced cost, facilitating robust training and evaluation for LLMs in software engineering. The code and data are publicly available.

Abstract: Constructing large-scale datasets for the GitHub issue resolution task is
crucial for both training and evaluating the software engineering capabilities
of Large Language Models (LLMs). However, the traditional process for creating
such benchmarks is notoriously challenging and labor-intensive, particularly in
the stages of setting up evaluation environments, grading test outcomes, and
validating task instances. In this paper, we propose SWE-Factory, an automated
pipeline designed to address these challenges. To tackle these issues, our
pipeline integrates three core automated components. First, we introduce
SWE-Builder, a multi-agent system that automates evaluation environment
construction, which employs four specialized agents that work in a
collaborative, iterative loop and leverages an environment memory pool to
enhance efficiency. Second, we introduce a standardized, exit-code-based
grading method that eliminates the need for manually writing custom parsers.
Finally, we automate the fail2pass validation process using these reliable exit
code signals. Experiments on 671 issues across four programming languages show
that our pipeline can effectively construct valid task instances; for example,
with GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at $0.045 per
instance, while with Gemini-2.5-flash, it achieves comparable performance at
the lowest cost of $0.024 per instance. We also demonstrate that our
exit-code-based grading achieves 100% accuracy compared to manual inspection,
and our automated fail2pass validation reaches a precision of 0.92 and a recall
of 1.00. We hope our automated pipeline will accelerate the collection of
large-scale, high-quality GitHub issue resolution datasets for both training
and evaluation. Our code and datasets are released at
https://github.com/DeepSoftwareAnalytics/swe-factory.

</details>


### [25] [TrioXpert: An automated incident management framework for microservice system](https://arxiv.org/abs/2506.10043)
*Yongqian Sun,Yu Luo,Xidao Wen,Yuan Yuan,Xiaohui Nie,Shenglin Zhang,Tong Liu,Xi Luo*

Main category: cs.SE

TL;DR: TrioXpert is a new framework for incident management in microservices that uses multimodal data and large language models. It outperforms existing methods in anomaly detection, failure triage, and root cause localization, while also offering better interpretability.


<details>
  <summary>Details</summary>
Motivation: Current automated incident management methods in microservice systems usually use only single-modal data (like metrics, logs, or traces), making it hard to handle multiple tasks (anomaly detection, failure triage, and root cause localization) effectively. Additionally, these approaches often lack interpretability due to unclear reasoning evidence.

Method: TrioXpert, the proposed framework, is end-to-end and utilizes multimodal data. It features three separate data pipelines, each tailored to the characteristics of different data modalities. The approach integrates a collaborative reasoning mechanism powered by large language models, allowing it to manage multiple tasks concurrently while delivering clear and interpretable reasoning evidence.

Result: Extensive experiments on two popular microservice datasets show that TrioXpert significantly outperforms existing approaches in all evaluated tasks, with improvements ranging from 4.7% to 57.7% for anomaly detection, 2.1% to 40.6% for failure triage, and 1.6% to 163.1% for root cause localization.

Conclusion: TrioXpert is effective for automated incident management in large-scale microservice systems, achieving superior performance across multiple tasks and improving interpretability by leveraging multimodal data and collaborative reasoning.

Abstract: Automated incident management plays a pivotal role in large-scale
microservice systems. However, many existing methods rely solely on
single-modal data (e.g., metrics, logs, and traces) and struggle to
simultaneously address multiple downstream tasks, including anomaly detection
(AD), failure triage (FT), and root cause localization (RCL). Moreover, the
lack of clear reasoning evidence in current techniques often leads to
insufficient interpretability. To address these limitations, we propose
TrioXpert, an end-to-end incident management framework capable of fully
leveraging multimodal data. TrioXpert designs three independent data processing
pipelines based on the inherent characteristics of different modalities,
comprehensively characterizing the operational status of microservice systems
from both numerical and textual dimensions. It employs a collaborative
reasoning mechanism using large language models (LLMs) to simultaneously handle
multiple tasks while providing clear reasoning evidence to ensure strong
interpretability. We conducted extensive evaluations on two popular
microservice system datasets, and the experimental results demonstrate that
TrioXpert achieves outstanding performance in AD (improving by 4.7% to 57.7%),
FT (improving by 2.1% to 40.6%), and RCL (improving by 1.6% to 163.1%) tasks.

</details>


### [26] [Reward Models Enable Scalable Code Verification by Trading Accuracy for Throughput](https://arxiv.org/abs/2506.10056)
*Gabriel Orlanski,Nicholas Roberts,Aws Albarghouthi,Frederic Sala*

Main category: cs.SE

TL;DR: The paper demonstrates that using outcome reward models in combination with traditional verifiers—specifically, through a generate-prune-then-rank approach—substantially increases speed while keeping accuracy loss minimal, suggesting a re-evaluation of current program ranking paradigms for LLM-based coding tasks.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the current practice of using comprehensive verifiers (such as full test suites) over outcome reward models (ORMs) in program synthesis via LLMs, with little attention to the trade-offs involving speed and accuracy.

Method: The authors systematically explore the trade-off between speed and accuracy by comparing different ranking approaches, specifically focusing on the generate-prune-then-rank method, which employs a faster but less accurate verifier in the pruning step before final ranking.

Result: The generate-prune-then-rank approach achieves a significant speedup (11.65 times faster) while only sacrificing a modest amount of accuracy (8.33% less than the full verifier), demonstrating that ORMs are valuable even when comprehensive verifiers are available.

Conclusion: ORMs can effectively scale the verification process by offering a compelling trade-off between speed and accuracy, thus enabling the design of more scalable and still accurate program ranking systems.

Abstract: The standard paradigm for solving coding tasks via large language models
(LLMs) is to generate-then-rank programs, where the latter step uses a verifier
in the ranking process. The growing consensus is that a comprehensive verifier
(e.g., a full test suite) should be prioritized over an outcome reward model
(ORM) whenever possible, with little consideration given to the trade-offs
involved. We aim to challenge this assumption by systematically exploring the
tradeoff between speed and accuracy. We find that ORMs play a crucial role in
scaling verification through trading accuracy for speed, even when a
comprehensive verifier is available. Their value becomes especially apparent
when used in a generate-prune-then-rank approach, where a faster but less
accurate verifier removes incorrect solutions prior to ranking -- leading to a
system that is 11.65x faster while only being 8.33% less accurate than the full
test suite. We analyze the generate-prune-then-rank approach and show that it
works by filtering out incorrect but highly ranked solutions. These findings
enable the design of scalable and accurate program ranking systems.

</details>


### [27] [Online Discovery of Simulation Models for Evolving Business Processes (Extended Version)](https://arxiv.org/abs/2506.10049)
*Francesco Vinci,Gyunam Park,Wil van der Aalst,Massimiliano de Leoni*

Main category: cs.SE

TL;DR: The paper introduces a streaming simulation discovery method that uses online machine learning and incremental process discovery, enabling business process simulations to better adapt to real-time changes and handle evolving process dynamics.


<details>
  <summary>Details</summary>
Motivation: Existing business process simulation discovery techniques are not adaptive to real-time operational changes, which is a problem since organizations constantly update and refine their processes.

Method: The paper proposes a streaming process simulation discovery technique that combines Incremental Process Discovery with Online Machine Learning. This method gives priority to recent data while retaining historical knowledge, allowing the simulation to adapt as the business process evolves.

Result: Experiments on four different event logs show that the proposed technique leads to more stable simulations and better handles concept drift compared to existing methods. It achieves a balance between adapting to new data and not forgetting historical patterns.

Conclusion: The proposed streaming process simulation technique improves adaptability and robustness in dynamic business environments by considering both recent and historical data, making simulations more reliable and effective under changing conditions.

Abstract: Business Process Simulation (BPS) refers to techniques designed to replicate
the dynamic behavior of a business process. Many approaches have been proposed
to automatically discover simulation models from historical event logs,
reducing the cost and time to manually design them. However, in dynamic
business environments, organizations continuously refine their processes to
enhance efficiency, reduce costs, and improve customer satisfaction. Existing
techniques to process simulation discovery lack adaptability to real-time
operational changes. In this paper, we propose a streaming process simulation
discovery technique that integrates Incremental Process Discovery with Online
Machine Learning methods. This technique prioritizes recent data while
preserving historical information, ensuring adaptation to evolving process
dynamics. Experiments conducted on four different event logs demonstrate the
importance in simulation of giving more weight to recent data while retaining
historical knowledge. Our technique not only produces more stable simulations
but also exhibits robustness in handling concept drift, as highlighted in one
of the use cases.

</details>


### [28] [Solving Package Management via Hypergraph Dependency Resolution](https://arxiv.org/abs/2506.10803)
*Ryan Gibb,Patrick Ferris,David Allsopp,Michael Winston Dales,Mark Elvers,Thomas Gazagnaire,Sadiq Jaffer,Thomas Leonard,Jon Ludlam,Anil Madhavapeddy*

Main category: cs.SE

TL;DR: The paper introduces HyperRes, a formal system that enables cross-ecosystem package dependency resolution by translating metadata among diverse package managers using hypergraph models, improving interoperability for complex multi-language projects.


<details>
  <summary>Details</summary>
Motivation: There are many package managers, each tailored to specific languages and systems, resulting in poor interoperability. Projects spanning multiple languages cannot clearly define cross-ecosystem dependencies, and external requirements are often not versioned or explicitly stated.

Method: The authors propose HyperRes, a formal system using hypergraphs to model versioned dependency resolution across multiple ecosystems. They develop translations from many popular package managers to this system.

Result: HyperRes enables dependency resolution to work across currently distinct ecosystems, allowing translation of packaging metadata and cross-ecosystem dependency management without forcing users to abandon their existing tools.

Conclusion: HyperRes provides a unifying approach to multi-language and cross-ecosystem dependency resolution, making it possible to manage dependencies more precisely and interoperably without requiring significant changes to existing workflows.

Abstract: Package managers are everywhere, with seemingly every language and operating
system implementing their own solution. The lack of interoperability between
these systems means that multi-lingual projects are unable to express precise
dependencies across language ecosystems, and external system and hardware
dependencies are typically implicit and unversioned. We define HyperRes, a
formal system for describing versioned dependency resolution using a hypergraph
that is expressive enough to model many ecosystems and solve dependency
constraints across them. We define translations from dozens of existing package
managers to HyperRes and comprehensively demonstrate that dependency resolution
can work across ecosystems that are currently distinct. This does not require
users to shift their choice of package managers; instead, HyperRes allows for
the translation of packaging metadata between ecosystems, and for solving to be
precisely specialised to a particular deployment environment.

</details>


### [29] [The Effects of GitHub Copilot on Computing Students' Programming Effectiveness, Efficiency, and Processes in Brownfield Programming Tasks](https://arxiv.org/abs/2506.10051)
*Md Istiak Hossain Shihab,Christopher Hundhausen,Ahsun Tariq,Summit Haque,Yunhan Qiao,Brian Mulanda*

Main category: cs.SE

TL;DR: GitHub Copilot helps students complete brownfield coding tasks more quickly and efficiently, but may compromise their understanding of the code. Educators need to adapt teaching methods to harness GenAI benefits while ensuring students grasp why and how code solutions work.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the gap in understanding how generative AI coding assistants, like GitHub Copilot, impact undergraduate students working on brownfield software development tasks, which are common in the software industry and involve modifying existing legacy code.

Method: The authors conducted a controlled experiment with 10 undergraduate computer science students, having them perform similar brownfield development tasks on a legacy web application both with and without GitHub Copilot. A mixed-methods approach was used, combining quantitative performance and behavioral analysis with qualitative exit interviews.

Result: Students using Copilot completed tasks 35% faster and made 50% more progress compared to not using Copilot. They also spent 11% less time manually coding and 12% less time on web searches. However, students expressed concerns in interviews about not fully understanding the suggestions Copilot provided.

Conclusion: GitHub Copilot significantly improves student performance and efficiency in brownfield programming tasks, but it may reduce their understanding of code. The findings highlight an urgent need for educators to create teaching strategies that balance GenAI's benefits with a focus on comprehension and critical thinking.

Abstract: When graduates of computing degree programs enter the software industry, they
will most likely join teams working on legacy code bases developed by people
other than themselves. In these so-called brownfield software development
settings, generative artificial intelligence (GenAI) coding assistants like
GitHub Copilot are rapidly transforming software development practices, yet the
impact of GenAI on student programmers performing brownfield development tasks
remains underexplored. This paper investigates how GitHub Copilot influences
undergraduate students' programming performance, behaviors, and understanding
when completing brownfield programming tasks in which they add new code to an
unfamiliar code base. We conducted a controlled experiment in which 10
undergraduate computer science students completed highly similar brownfield
development tasks with and without Copilot in a legacy web application. Using a
mixed-methods approach combining performance analysis, behavioral analysis, and
exit interviews, we found that students completed tasks 35% faster (p < 0.05)
and made 50% more solution progress p (< 0.05) when using Copilot. Moreover,
our analysis revealed that, when using Copilot, students spent 11% less time
manually writing code (p < 0.05), and 12% less time conducting web searches (p
< 0.05), providing evidence of a fundamental shift in how they engaged in
programming. In exit interviews, students reported concerns about not
understanding how or why Copilot suggestions work. This research suggests the
need for computing educators to develop new pedagogical approaches that
leverage GenAI assistants' benefits while fostering reflection on how and why
GenAI suggestions address brownfield programming tasks. Complete study results
and analysis are presented at https://ghcopilot-icer.github.io/.

</details>


### [30] [Prompt Variability Effects On LLM Code Generation](https://arxiv.org/abs/2506.10204)
*Andrei Paleyes,Radzim Sendyka,Diana Robinson,Christian Cabrera,Neil D. Lawrence*

Main category: cs.SE

TL;DR: The paper presents new evaluation techniques for code-generating LLMs that test their sensitivity to input and user background, with experimental validation and open-source code for community adoption.


<details>
  <summary>Details</summary>
Motivation: While Large Language Models (LLMs) have advanced code generation, the effectiveness of generated code varies based on the quality of prompts, particularly considering the user's programming background.

Method: The paper proposes a synthetic evaluation pipeline for code generation with LLMs and introduces a systematic persona-based evaluation approach. These methods assess how LLM-generated code quality varies depending on user background and input changes, and are designed to be independent of specific programming tasks or LLM models.

Result: Experimental results show that their evaluation methods can effectively highlight qualitative differences in LLM responses based on varying user personas. The authors also release their code for community use.

Conclusion: The proposed synthetic and persona-based evaluation methods provide robust, widely applicable tools for assessing and understanding the sensitivity of LLM-generated code to input and user background differences.

Abstract: Code generation is one of the most active areas of application of Large
Language Models (LLMs). While LLMs lower barriers to writing code and
accelerate development process, the overall quality of generated programs
depends on the quality of given prompts. Specifically, functionality and
quality of generated code can be sensitive to user's background and familiarity
with software development. It is therefore important to quantify LLM's
sensitivity to variations in the input. To this end we propose a synthetic
evaluation pipeline for code generation with LLMs, as well as a systematic
persona-based evaluation approach to expose qualitative differences of LLM
responses dependent on prospective user background. Both proposed methods are
completely independent from specific programming tasks and LLMs, and thus are
widely applicable. We provide experimental evidence illustrating utility of our
methods and share our code for the benefit of the community.

</details>


### [31] [AI-Based Software Vulnerability Detection: A Systematic Literature Review](https://arxiv.org/abs/2506.10280)
*Samiha Shimmi,Hamed Okhravi,Mona Rahimi*

Main category: cs.SE

TL;DR: A detailed review of 2018-2023 software vulnerability detection research shows that AI, especially graph-based models, dominates, but the field still faces reproducibility and dataset issues. The paper outlines future research possibilities, including federated learning and quantum neural networks.


<details>
  <summary>Details</summary>
Motivation: As software vulnerabilities are a major cybersecurity threat and traditional detection methods are limited, there is a need to comprehensively analyze recent advances in AI-driven vulnerability detection.

Method: This paper conducts a systematic review of literature on software vulnerability detection from 2018 to 2023, classifying methods, feature representations, and embedding strategies.

Result: 91% of studies reviewed employ AI methods, with graph-based models most common. The study also identifies limitations in current research and suggests new research opportunities.

Conclusion: Most current research on software vulnerability detection utilizes AI-based methods, especially graph-based models. However, the field faces challenges regarding dataset quality, reproducibility, and interpretability. New techniques like federated learning and quantum neural networks offer promising directions.

Abstract: Software vulnerabilities in source code pose serious cybersecurity risks,
prompting a shift from traditional detection methods (e.g., static analysis,
rule-based matching) to AI-driven approaches. This study presents a systematic
review of software vulnerability detection (SVD) research from 2018 to 2023,
offering a comprehensive taxonomy of techniques, feature representations, and
embedding methods. Our analysis reveals that 91% of studies use AI-based
methods, with graph-based models being the most prevalent. We identify key
limitations, including dataset quality, reproducibility, and interpretability,
and highlight emerging opportunities in underexplored techniques such as
federated learning and quantum neural networks, providing a roadmap for future
research.

</details>


### [32] [Minimizing False Positives in Static Bug Detection via LLM-Enhanced Path Feasibility Analysis](https://arxiv.org/abs/2506.10322)
*Xueying Du,Kai Yu,Chong Wang,Yi Zou,Wentai Deng,Zuoyu Ou,Xin Peng,Lingming Zhang,Yiling Lou*

Main category: cs.SE

TL;DR: LLM4PFA, an LLM agent-based path feasibility analysis framework, dramatically reduces false positives in static bug detection for large codebases, outperforming previous methods by a wide margin with minimal loss of true positives.


<details>
  <summary>Details</summary>
Motivation: Existing static bug analyzers for large codebases suffer from high false positive rates due to limited capabilities in validating path feasibility, especially with complex branches and data dependencies. LLM-based approaches have not sufficiently solved these issues due to weaknesses in constraint analysis and scalability.

Method: The paper proposes an iterative path feasibility analysis framework called LLM4PFA, which uses LLM agent-based targeted constraint reasoning and context-aware analysis driven by agent planning to enhance inter-procedural path feasibility evaluation.

Result: LLM4PFA filters out 72% to 96% of false positives in static bug detection, outperforming all baselines by 41.1% to 105.7% improvement rates, and only misses 3 real bugs out of 45 true positives.

Conclusion: The LLM4PFA framework significantly reduces false positives in static bug detection while maintaining high true positive rates, making it more effective than current methods.

Abstract: Static bug analyzers play a crucial role in ensuring software quality.
However, existing analyzers for bug detection in large codebases often suffer
from high false positive rates. This is primarily due to the limited
capabilities of analyzers in path feasibility validation with multiple
conditional branches and complex data dependencies. While current LLM-based
approaches attempt to address this issue, their effectiveness remains limited
due to insufficient constraint cascade analysis and scalability challenges in
large projects. To address this challenge, we propose an iterative path
feasibility analysis framework LLM4PFA. By leveraging LLM agent based targeted
constraint reasoning, and key context-aware analysis driven by agent planning,
LLM4PFA effectively enhances complex inter-procedural path feasibility analysis
for minimizing false positives in static bug detection. Evaluation results show
that LLM4PFA precisely filters out 72% to 96% false positives reported during
static bug detection, significantly outperforming all the baselines by 41.1% -
105.7% improvements; meanwhile LLM4PFA only misses 3 real bugs of 45 true
positives.

</details>


### [33] [Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements](https://arxiv.org/abs/2506.10330)
*Seyed Moein Abtahi,Akramul Azim*

Main category: cs.SE

TL;DR: The paper presents an automated approach to detect and correct code problems in large software projects using LLMs, static analysis, retrieval-augmented generation, and post-processing checks. This method significantly improves code quality and streamlines development.


<details>
  <summary>Details</summary>
Motivation: Code issue detection and correction are essential in software development but remain labor-intensive and error-prone. Automating these processes using advanced AI like large language models (LLMs) could greatly improve efficiency and quality while lowering costs.

Method: The study integrates LLMs (such as GPT-3.5 Turbo and GPT-4o) into a software workflow by using static code analysis to detect issues. Detected issues are organized and revised automatically by LLMs, with iterative prompt engineering to optimize LLM responses. Retrieval-augmented generation (RAG) is added to supply relevant real-time knowledge. A custom Code Comparison App reviews and corrects LLM-generated code changes to mitigate errors before codebase integration. Results are validated by rescanning with the original static analysis tools.

Result: Combining LLMs, static code analysis, RAG, and post-processing significantly reduces the number of code issues in large software projects and increases the efficiency and reliability of the revision process.

Conclusion: Integrating LLMs with static code analysis and retrieval-augmented generation, alongside corrective code comparison tools, can effectively automate detecting and fixing code issues, thus improving software quality and reducing development resources.

Abstract: This study examined code issue detection and revision automation by
integrating Large Language Models (LLMs) such as OpenAI's GPT-3.5 Turbo and
GPT-4o into software development workflows. A static code analysis framework
detects issues such as bugs, vulnerabilities, and code smells within a
large-scale software project. Detailed information on each issue was extracted
and organized to facilitate automated code revision using LLMs. An iterative
prompt engineering process is applied to ensure that prompts are structured to
produce accurate and organized outputs aligned with the project requirements.
Retrieval-augmented generation (RAG) is implemented to enhance the relevance
and precision of the revisions, enabling LLM to access and integrate real-time
external knowledge. The issue of LLM hallucinations - where the model generates
plausible but incorrect outputs - is addressed by a custom-built "Code
Comparison App," which identifies and corrects erroneous changes before
applying them to the codebase. Subsequent scans using the static code analysis
framework revealed a significant reduction in code issues, demonstrating the
effectiveness of combining LLMs, static analysis, and RAG to improve code
quality, streamline the software development process, and reduce time and
resource expenditure.

</details>


### [34] [AutoGEEval++: A Multi-Level and Multi-Geospatial-Modality Automated Evaluation Framework for Large Language Models in Geospatial Code Generation on Google Earth Engine](https://arxiv.org/abs/2506.10365)
*Shuyang Hou,Zhangxiao Shen,Huayi Wu,Haoyue Jiao,Ziqi Liu,Lutong Xie,Chang Liu,Jianyuan Liang,Yaxian Qing,Xiaopu Zhang,Dehua Peng,Zhipeng Gui,Xuefeng Guan*

Main category: cs.SE

TL;DR: AutoGEEval++ delivers the first automated and standardized framework for evaluating how large language models generate geospatial code on Google Earth Engine. Using a large benchmark and comprehensive metrics, it reveals performance differences across leading LLMs, setting a foundation for future development and comparison in this field.


<details>
  <summary>Details</summary>
Motivation: There is a growing need for integrating AI into geospatial scientific analysis, especially through geospatial code generation. However, there is a lack of standardized and automated evaluation tools for assessing this integration, particularly regarding the performance of large language models (LLMs) in generating geospatial code on platforms like Google Earth Engine (GEE).

Method: The authors introduce AutoGEEval++, an enhanced framework based on AutoGEEval, designed for automated assessment of LLMs generating geospatial code on GEE. The framework uses the GEE Python API and includes a comprehensive benchmark dataset (AutoGEEval++-Bench) with 6,365 test cases across 26 data types and three task categories (unit, combo, and theme). The evaluation pipeline comprises a code generation, submission, and judge module, and employs multi-dimensional metrics including accuracy, resource usage, run-time efficiency, and error type analysis.

Result: AutoGEEval++ was used to evaluate 24 leading LLMs as of June 2025, representing diverse model types. The results showed differences in performance, stability, and error tendencies depending on task category, model design, and deployment context, validating the framework's value and scalability for code generation in the geospatial domain.

Conclusion: AutoGEEval++ establishes the first standardized evaluation protocol and benchmark for GEE-based LLM code generation, enabling consistent performance comparison and providing a structured framework for systematic evaluation in domain-specific code generation.

Abstract: Geospatial code generation is becoming a key frontier in integrating
artificial intelligence with geo-scientific analysis, yet standardised
automated evaluation tools for this task remain absent. This study presents
AutoGEEval++, an enhanced framework building on AutoGEEval, and the first
automated assessment system for large language models (LLMs) generating
geospatial code on Google Earth Engine (GEE). It supports diverse data
modalities and varying task complexities. Built on the GEE Python API,
AutoGEEval++ features a benchmark dataset-AutoGEEval++-Bench-with 6,365 test
cases across 26 data types and three task categories: unit, combo, and theme
tests. It includes a submission programme and a judge module to realise an
end-to-end automated evaluation pipeline from code generation to
execution-based validation. The framework adopts multi-dimensional
metrics-accuracy, resource usage, run-time efficiency, and error
types-balancing hallucination control and efficiency, and enabling boundary
testing and error pattern analysis. Using AutoGEEval++, we evaluate 24
state-of-the-art LLMs (as of June 2025), including general-purpose,
reasoning-enhanced, code-centric, and geoscience-specific models. Results
reveal clear performance, stability, and error differences across task types,
model designs, and deployment settings, confirming AutoGEEval++'s practical
value and scalability in vertical-domain code generation. This work establishes
the first standardised evaluation protocol and foundational benchmark for
GEE-based LLM code generation, providing a unified basis for performance
comparison and a methodological framework for systematic, domain-specific code
evaluation.

</details>


### [35] [MLLM-Based UI2Code Automation Guided by UI Layout Information](https://arxiv.org/abs/2506.10376)
*Fan Wu,Cuiyun Gao,Shuqing Li,Xin-Cheng Wen,Qing Liao*

Main category: cs.SE

TL;DR: UI-to-code conversion is slow and existing methods don't generalize well. LayoutCoder is a new MLLM-based framework with specialized modules for layout preservation, and it achieves much better results than prior methods on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Converting user interfaces to code is a critical but time-consuming part of web development. Existing deep learning methods for automating this process require large labeled datasets and struggle to handle novel, real-world web designs. Multimodal Large Language Models (MLLMs) could improve automation but have difficulty understanding complex layouts and generating accurate code.

Method: The authors introduce LayoutCoder, an MLLM-based framework for generating UI code from real-world webpage images. It has three core modules: (1) Element Relation Construction to capture layouts by grouping similar components, (2) UI Layout Parsing to build layout trees for guiding code generation, and (3) Layout-Guided Code Fusion to produce layout-preserving code. The team also built a new benchmark dataset, Snap2Code, and used Design2Code for evaluation.

Result: LayoutCoder outperforms state-of-the-art approaches, increasing BLEU score by 10.14% and CLIP score by 3.95% on average across datasets compared to the best-performing baseline.

Conclusion: LayoutCoder effectively addresses the challenges in UI2Code tasks for real-world webpages. Its innovative architecture and new benchmark dataset advance the automation of UI code generation.

Abstract: Converting user interfaces into code (UI2Code) is a crucial step in website
development, which is time-consuming and labor-intensive. The automation of
UI2Code is essential to streamline this task, beneficial for improving the
development efficiency. There exist deep learning-based methods for the task;
however, they heavily rely on a large amount of labeled training data and
struggle with generalizing to real-world, unseen web page designs. The advent
of Multimodal Large Language Models (MLLMs) presents potential for alleviating
the issue, but they are difficult to comprehend the complex layouts in UIs and
generate the accurate code with layout preserved. To address these issues, we
propose LayoutCoder, a novel MLLM-based framework generating UI code from
real-world webpage images, which includes three key modules: (1) Element
Relation Construction, which aims at capturing UI layout by identifying and
grouping components with similar structures; (2) UI Layout Parsing, which aims
at generating UI layout trees for guiding the subsequent code generation
process; and (3) Layout-Guided Code Fusion, which aims at producing the
accurate code with layout preserved. For evaluation, we build a new benchmark
dataset which involves 350 real-world websites named Snap2Code, divided into
seen and unseen parts for mitigating the data leakage issue, besides the
popular dataset Design2Code. Extensive evaluation shows the superior
performance of LayoutCoder over the state-of-the-art approaches. Compared with
the best-performing baseline, LayoutCoder improves 10.14% in the BLEU score and
3.95% in the CLIP score on average across all datasets.

</details>


### [36] [Bug Classification in Quantum Software: A Rule-Based Framework and Its Evaluation](https://arxiv.org/abs/2506.10397)
*Mir Mohammad Yousuf,Shabir Ahmad Sofi*

Main category: cs.SE

TL;DR: The paper introduces a rule-based automated framework for classifying issues in quantum software repositories, validated with Qiskit data. The system reliably identifies most bug types, categories, and quality attributes, though severity classification requires enhancement. The analysis reveals that most bugs are classical and low severity, but a significant portion are unique to quantum software, providing valuable direction for improving quantum software quality.


<details>
  <summary>Details</summary>
Motivation: Despite the growing complexity of quantum software, there is a lack of systematic and automated methods for classifying software bugs, particularly those specific to quantum computing. Accurate bug classification can significantly enhance software quality, maintenance, and development but requires approaches tailored to the unique characteristics of quantum software.

Method: The authors developed a rule-based, automated classification framework that uses keyword and heuristic techniques customized for quantum computing. They validated the framework by manually labeling a stratified sample of issues from Qiskit repositories, then compared automated and manual results using accuracy, precision, recall, F1-score, and kappa statistics.

Result: The framework achieved up to 85.21% accuracy in classifying bugs, with F1-scores varying by classification dimension (0.7075 for severity, 0.8393 for quality attributes). Cohen's Kappa indicated substantial to almost perfect agreement for most labels except for severity, which had only slight agreement (k=0.162). The analysis found most bugs were classical (67.2%), with 27.3% quantum-specific. Most issues were low severity (93.7%). Half of quantum-specific bugs related to circuit-level problems.

Conclusion: The proposed rule-based framework is effective for automated classification of both classical and quantum-specific bugs in quantum software repositories, showing strong agreement with manual labeling for most categories. However, severity classification needs improvement. The study provides new insights into the distribution and types of bugs in quantum software, highlighting important targets for future quality assurance efforts.

Abstract: Accurate classification of software bugs is essential for improving software
quality. This paper presents a rule-based automated framework for classifying
issues in quantum software repositories by bug type, category, severity, and
impacted quality attributes, with additional focus on quantum-specific bug
types. The framework applies keyword and heuristic-based techniques tailored to
quantum computing. To assess its reliability, we manually classified a
stratified sample of 4,984 issues from a dataset of 12,910 issues across 36
Qiskit repositories. Automated classifications were compared with ground truth
using accuracy, precision, recall, and F1-score. The framework achieved up to
85.21% accuracy, with F1-scores ranging from 0.7075 (severity) to 0.8393
(quality attribute). Statistical validation via paired t-tests and Cohen's
Kappa showed substantial to almost perfect agreement for bug type (k = 0.696),
category (k = 0.826), quality attribute (k = 0.818), and quantum-specific bug
type (k = 0.712). Severity classification showed slight agreement (k = 0.162),
suggesting room for improvement. Large-scale analysis revealed that classical
bugs dominate (67.2%), with quantum-specific bugs at 27.3%. Frequent bug
categories included compatibility, functional, and quantum-specific defects,
while usability, maintainability, and interoperability were the most impacted
quality attributes. Most issues (93.7%) were low severity; only 4.3% were
critical. A detailed review of 1,550 quantum-specific bugs showed that over
half involved quantum circuit-level problems, followed by gate errors and
hardware-related issues.

</details>


### [37] [Towards Understanding Bugs in Distributed Training and Inference Frameworks for Large Language Models](https://arxiv.org/abs/2506.10426)
*Xiao Yu,Haoxuan Chen,Feifei Niu,Xing Hu,Jacky Wai Keung,Xin Xia*

Main category: cs.SE

TL;DR: This paper analyzes 308 bugs in popular distributed LLM training/inference frameworks, revealing common causes, challenges in debugging, and the potential for automation. Almost half of fixes are simple, suggesting LLM-based tools could aid in automating many repairs and improving reliability across the ecosystem.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of large language models has made distributed training and inference frameworks essential, but their increasing complexity introduces significant and costly software bugs. Systematically understanding the nature of these bugs is necessary to develop better quality assurance, debugging, and repair methods.

Method: The authors conducted a large-scale empirical analysis of 308 fixed bugs from three major distributed LLM frameworks (DeepSpeed, Megatron-LM, Colossal-AI). Their analysis included studying bug symptoms, root causes, identification and fixing processes, as well as common low-effort repair strategies.

Result: They found that distributed frameworks exhibit unique bug root causes, such as allocation strategy and distributed communication errors. Almost half (48%) of bug fixes required only minor code changes, typically using simple strategies like conditional logic, parameter handling, or version compatibility updates. However, diagnosing and fixing bugs can be difficult due to complex root-symptom disconnects, high costs of bug reproduction, and interactions across low-level or multiple components.

Conclusion: The study provides actionable insights for increasing the reliability of distributed LLM frameworks and their applications. It also identifies promising avenues to automate debugging and repair using LLM-based tools, suggesting significant potential to reduce bug-fixing workloads.

Abstract: With the rapid development of large language models (LLMs), distributed
training and inference frameworks like DeepSpeed have become essential for
scaling model training and inference across multiple GPUs or nodes. However,
the increasing complexity of these frameworks brings non-trivial software bugs,
which may degrade training performance, cause unexpected failures, and result
in significant resource waste. Understanding framework bugs' characteristics is
fundamental for quality assurance, allowing the design of more effective
debugging and repair methods. Thus, our paper conducts the first large-scale
empirical analysis of 308 fixed bugs across three popular distributed
training/inference frameworks: DeepSpeed, Megatron-LM, and Colossal-AI. We
examine bug symptoms, root causes, bug identification and fixing efforts, and
common low-effort fixing strategies. Additionally, the distributed nature of
these frameworks introduces unique bug root causes, such as allocation strategy
error and distributed communication error. Diagnosing and fixing complex bugs
remains challenging due to factors like the disconnect between symptoms and
root causes, high bug reproduction costs, and low-level or cross-component
interactions. Interestingly, we observe that 48% of bug fixes require minimal
code changes (<=10 LOC) and follow simple strategies such as conditional logic
optimization, parameter handling enhancement, or version compatibility
handling, indicating potential for automation. Based on these insights, we
offer several implications for improving the reliability of both distributed
training and inference frameworks and their dependent LLM projects, while also
identifying opportunities to leverage LLM-based tools for automated debugging
and repair.

</details>


### [38] [EXPEREPAIR: Dual-Memory Enhanced LLM-based Repository-Level Program Repair](https://arxiv.org/abs/2506.10484)
*Fangwen Mu,Junjie Wang,Lin Shi,Song Wang,Shoubin Li,Qing Wang*

Main category: cs.SE

TL;DR: ExpeRepair leverages dual memories (episodic and semantic) to dynamically generate experience-driven prompts for software repair using LLMs, significantly surpassing previous methods in benchmark performance.


<details>
  <summary>Details</summary>
Motivation: Current automatic software repair methods using LLMs ignore prior repair experiences and use static prompt strategies, limiting their adaptability and performance. There's a need for a system that learns from history and adapts prompts for more effective repairs.

Method: The paper proposes ExpeRepair, an LLM-based approach that utilizes a dual-memory system: episodic memory for storing concrete past repair demonstrations, and semantic memory for abstract repair insights. During inference, both memories are activated and dynamically integrated to generate context-aware, adaptable prompts for software repair tasks.

Result: On the SWE-bench Lite benchmark, ExpeRepair with Claude 3.7 Sonnet achieved a pass@1 score of 49.3%, outperforming all state-of-the-art open-source software repair methods.

Conclusion: Dual-channel memory accumulation and dynamic prompt composition significantly boost LLM-based automated software repair, making ExpeRepair more effective and adaptable than existing approaches.

Abstract: Automatically repairing software issues remains a fundamental challenge at
the intersection of software engineering and AI. Although recent advancements
in Large Language Models (LLMs) have demonstrated potential for
repository-level repair tasks, current methodologies exhibit two notable
limitations: (1) they often address issues in isolation, neglecting to
incorporate insights from previously resolved issues, and (2) they rely on
static and rigid prompting strategies, which constrain their ability to
generalize across diverse and evolving issue scenarios. Inspired by the dual
memory systems of human cognition, where episodic and semantic memories work
synergistically to support human reasoning and decision-making, we propose
ExpeRepair, a novel LLM-based approach that continuously learns from historical
repair experiences through dual-channel knowledge accumulation. ExpeRepair
organizes historical repair experiences into two complementary memories: an
episodic memory that stores concrete repair demonstrations, and a semantic
memory that encodes abstract reflective insights. At inference time, ExpeRepair
activates both memory systems by retrieving relevant demonstrations from
episodic memory and recalling high-level repair insights from semantic memory.
It further enhances adaptability through dynamic prompt composition,
synergistically integrating both memory types to replace static prompts with
context-aware, experience-driven prompts. Experiments on the SWE-bench Lite
benchmark demonstrate that ExpeRepair achieves a pass@1 score of 49.3% with
Claude 3.7 Sonnet, outperforming all state-of-the-art open-source methods.

</details>


### [39] [BugGen: A Self-Correcting Multi-Agent LLM Pipeline for Realistic RTL Bug Synthesis](https://arxiv.org/abs/2506.10501)
*Surya Jasper,Minh Luu,Evan Pan,Aakash Tyagi,Michael Quinn,Jiang Hu,David Kebo Houngninou*

Main category: cs.SE

TL;DR: BugGen automates the generation of realistic hardware bugs using LLMs, producing high-quality datasets much faster and more accurately than manual or traditional methods, greatly benefiting machine learning-based verification and debugging workflows.


<details>
  <summary>Details</summary>
Motivation: The increasing hardware complexity is overburdening verification processes, prompting a need for scalable and realistic bug datasets to improve machine learning-assisted debugging. Existing bug insertion methods (manual or automated) are inadequate at producing diverse, realistic bugs efficiently.

Method: The paper introduces BugGen, an autonomous, multi-agent pipeline that employs Large Language Models (LLMs) to systematically generate, mutate, and validate realistic bugs in Register Transfer Level (RTL) hardware code. BugGen features module partitioning, mutation target selection via a closed-loop agentic architecture, and uses iterative refinement and rollback mechanisms to ensure both syntactic and functional validity of the inserted bugs.

Result: Across five OpenTitan IP blocks, BugGen generated 500 unique bugs with 94% functional accuracy and a throughput of 17.7 validated bugs per hour—a significant speedup over manual expert insertion. It also uncovered 104 previously unseen bugs and outperformed Certitude in syntactic accuracy, depth of testbench coverage, and bug scenario quality. Training ML-based failure triage models with BugGen-generated datasets resulted in classification accuracy between 88.1% and 93.2%.

Conclusion: BugGen is a scalable and autonomous solution for generating high-quality, realistic hardware bug datasets. It significantly improves verification efficiency, enhances ML-assisted debugging, and exposes coverage gaps more effectively than existing methods.

Abstract: Hardware complexity continues to strain verification resources, motivating
the adoption of machine learning (ML) methods to improve debug efficiency.
However, ML-assisted debugging critically depends on diverse and scalable bug
datasets, which existing manual or automated bug insertion methods fail to
reliably produce. We introduce BugGen, a first of its kind, fully autonomous,
multi-agent pipeline leveraging Large Language Models (LLMs) to systematically
generate, insert, and validate realistic functional bugs in RTL. BugGen
partitions modules, selects mutation targets via a closed-loop agentic
architecture, and employs iterative refinement and rollback mechanisms to
ensure syntactic correctness and functional detectability. Evaluated across
five OpenTitan IP blocks, BugGen produced 500 unique bugs with 94% functional
accuracy and achieved a throughput of 17.7 validated bugs per hour-over five
times faster than typical manual expert insertion. Additionally, BugGen
identified 104 previously undetected bugs in OpenTitan regressions,
highlighting its utility in exposing verification coverage gaps. Compared
against Certitude, BugGen demonstrated over twice the syntactic accuracy,
deeper exposure of testbench blind spots, and more functionally meaningful and
complex bug scenarios. Furthermore, when these BugGen-generated datasets were
employed to train ML-based failure triage models, we achieved high
classification accuracy (88.1%-93.2%) across different IP blocks, confirming
the practical utility and realism of generated bugs. BugGen thus provides a
scalable solution for generating high-quality bug datasets, significantly
enhancing verification efficiency and ML-assisted debugging.

</details>


### [40] [AdaptiveLLM: A Framework for Selecting Optimal Cost-Efficient LLM for Code-Generation Based on CoT Length](https://arxiv.org/abs/2506.10525)
*Junhang Cheng,Fang Liu,Chengru Wu,Li Zhang*

Main category: cs.SE

TL;DR: AdaptiveLLM automatically picks the best LLM for code tasks using an innovative, automated task difficulty measure and machine learning. It significantly improves code generation accuracy while greatly reducing computation costs, outperforming previous model selection methods that rely on human judgment.


<details>
  <summary>Details</summary>
Motivation: Large Language Models improve code generation but are restricted by the need to balance computational cost and performance. Existing methods for choosing the right LLM per task are inefficient and rely on subjective human difficulty ratings, which are often unavailable or unreliable.

Method: The authors propose AdaptiveLLM, a framework that first estimates code task difficulty using Chain-of-Thought (CoT) reasoning lengths, clusters these into difficulty levels with k-means, encodes difficulty-aware features via a fine-tuned CodeBERT, and selects the optimal LLM with an XGBoost classifier to optimize for accuracy and cost.

Result: Experimental results show AdaptiveLLM improves pass@1 code accuracy by 7.86% and cuts resource costs by 88.9% versus ComplexityNet. It also boosts accuracy by ~15% over using a single model, with cost held constant, and its automatic task difficulty assessment outperforms human evaluations for model selection.

Conclusion: AdaptiveLLM is an effective and efficient solution for dynamically matching LLMs to coding tasks, achieving a better balance between accuracy and computational cost than prior systems, and providing a more robust, automated method of assessing task difficulty via CoT-based signals rather than unreliable human labels.

Abstract: While Large Language Models (LLMs) have significantly advanced code
generation efficiency, they face inherent challenges in balancing performance
and inference costs across diverse programming tasks. Dynamically selecting the
optimal LLM based on task difficulty and resource constraints offers a
promising approach to achieve an optimal balance between efficiency and
performance. However, existing model selection methods are resource-intensive
and often neglect cost efficiency. Moreover, these approaches rely on
human-annotated difficulty labels that are frequently inaccessible in
real-world settings and may not align with the LLM's own assessment of task
difficulty. In this paper, we introduce AdaptiveLLM, a framework that
dynamically selects optimal LLMs for a given coding task by automatically
assessing task difficulty. Our framework first estimates task difficulty using
Chain-of-Thought lengths generated by reasoning model, clusters these into
three difficulty levels via k-means, and fine-tunes CodeBERT to embed
difficulty-aware features. A trained XGBoost classifier then selects the best
model for each problem, optimizing the performance-cost trade-off. Experimental
results show that AdaptiveLLM achieves a 7.86% improvement in pass@1 score
while reducing resource consumption by 88.9% compared to baseline method
ComplexityNet. When compared to a single model, AdaptiveLLM demonstrates an
approximately 15% accuracy improvement, while maintaining the same level of
cost consumption. Apart from that, the difficulty assessment using CoT provides
more reliable selection criteria than human evaluation. Our replication package
is available at https://github.com/cjhCoder7/AdaptiveLLM.

</details>


### [41] [Scalable Software Testing in Fast Virtual Platforms: Leveraging SystemC, QEMU and Containerization](https://arxiv.org/abs/2506.10624)
*Lukas Jünger,Jan Henrik Weinstock,Tim Kraus*

Main category: cs.SE

TL;DR: Containerizing open-source virtual platforms allows for early, scalable, and dependency-free software testing in the cloud, helping accelerate HW/SW co-development for complex systems.


<details>
  <summary>Details</summary>
Motivation: The complexity of hardware/software (HW/SW) systems, especially in safety-critical fields like automotive, demands extensive testing. However, hardware availability often lags, delaying software development. The motivation is to improve early-stage software development and testing by overcoming hardware unavailability and complex environment dependencies.

Method: The paper proposes encapsulating Virtual Platforms (VPs) using containerization to reduce environment dependencies and allow cloud-based, parallel test execution. It leverages open-source VP solutions (QEMU, VCML) to avoid proprietary license requirements. The approach is demonstrated via an AI accelerator VP case study.

Result: The case study validates the approach, showing that containerized, cloud-deployed VPs effectively support fast, parallelized HW/SW co-development and pre-silicon testing without dependency or license issues.

Conclusion: The study provides a robust method for addressing HW/SW co-development challenges, enabling earlier, scalable, and more practical pre-silicon software testing through containerization and open-source tools.

Abstract: The ever-increasing complexity of HW/SW systems presents a persistent
challenge, particularly in safety-critical domains like automotive, where
extensive testing is imperative. However, the availability of hardware often
lags behind, hindering early-stage software development. To address this,
Virtual Platforms (VPs) based on the SystemC TLM-2.0 standard have emerged as a
pivotal solution, enabling pre-silicon execution and testing of unmodified
target software. In this study, we propose an approach leveraging
containerization to encapsulate VPs in order to reduce environment dependencies
and enable cloud deployment for fast, parallelized test execution, as well as
open-source VP technologies such as QEMU and VCML to obviate the need for seat
licenses. To demonstrate the efficacy of our approach, we present an Artificial
Intelligence (AI) accelerator VP case study. Through our research, we offer a
robust solution to address the challenges posed by the complexity of HW/SW
systems, with practical implications for accelerating HW/SW co-development.

</details>


### [42] [Not One to Rule Them All: Mining Meaningful Code Review Orders From GitHub](https://arxiv.org/abs/2506.10654)
*Abir Bouraffa,Carolin Brandt,Andy Zaidmann,Walid Maalej*

Main category: cs.SE

TL;DR: Code reviewers often don't follow the default alphabetical order when examining files in pull requests. Instead, they use meaningful strategies (like prioritizing big changes or test files), especially in larger reviews. This suggests a need for better tool support to match real reviewer habits.


<details>
  <summary>Details</summary>
Motivation: Code reviewers typically see changed files in alphabetical order, but this order may not match reviewers' preferences for effective navigation during a review. The study aims to understand how reviewers actually choose the order in which they comment on files in GitHub pull requests.

Method: The authors mined code review comments from 23,241 pull requests across 100 popular Java and Python GitHub repositories. They analyzed the sequence of file comments to determine if reviewers followed non-alphabetical orders, and if so, whether these orders reflected specific strategies (like commenting on largest diffs first, or files similar to PR titles, etc.). They also compared the proportion of reviewed files and approval rates between different navigation orders.

Result: 44.6% of pull requests had reviewers commenting in non-alphabetical order. Of these, some showed clear strategies: 20.6% followed a largest-diff-first approach, 17.6% focused on files similar to the PR title/description, and 29% with both production and test files followed a test-first order. Non-alphabetical reviews involved a higher proportion of reviewed files but received slightly fewer approvals on average.

Conclusion: Reviewers frequently deviate from alphabetical file order when commenting on pull requests, often using more meaningful or strategic review sequences. More sophisticated support is needed for large pull requests to accommodate these diverse reviewer strategies.

Abstract: Developers use tools such as GitHub pull requests to review code, discuss
proposed changes, and request modifications. While changed files are commonly
presented in alphabetical order, this does not necessarily coincide with the
reviewer's preferred navigation sequence. This study investigates the different
navigation orders developers follow while commenting on changes submitted in
pull requests. We mined code review comments from 23,241 pull requests in 100
popular Java and Python repositories on GitHub to analyze the order in which
the reviewers commented on the submitted changes. Our analysis shows that for
44.6% of pull requests, the reviewers comment in a non-alphabetical order.
Among these pull requests, we identified traces of alternative meaningful
orders: 20.6% (2,134) followed a largest-diff-first order, 17.6% (1,827) were
commented in the order of the files' similarity to the pull request's title and
description, and 29% (1,188) of pull requests containing changes to both
production and test files adhered to a test-first order. We also observed that
the proportion of reviewed files to total submitted files was significantly
higher in non-alphabetically ordered reviews, which also received slightly
fewer approvals from reviewers, on average. Our findings highlight the need for
additional support during code reviews, particularly for larger pull requests,
where reviewers are more likely to adopt complex strategies rather than
following a single predefined order.

</details>


### [43] [Formalising Software Requirements using Large Language Models](https://arxiv.org/abs/2506.10704)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: The VERIFAI project aims to automate and enhance the traceability and verification of software requirements by leveraging AI and NLP technologies, especially large language models, for automatically linking natural language requirements to formal specifications.


<details>
  <summary>Details</summary>
Motivation: There is a significant challenge in ensuring that formal specifications accurately reflect and trace back to the original natural language requirements throughout the software development process.

Method: The project explores methods such as Natural Language Processing, ontologies for domain description, reuse of existing software artefacts via similarity, large language models for requirement specifications, and AI-driven guidance for traceability and verification.

Result: The paper introduces the VERIFAI project and describes its planned methodologies, but does not present experimental results yet as it is an initiation phase.

Conclusion: The VERIFAI project aims to address traceability and automated verification of requirements by integrating AI, NLP, and ontology-based tools, with a focus on automating and improving the generation and maintenance of formal specifications from natural language requirements.

Abstract: This paper is a brief introduction to our recently initiated project named
VERIFAI: Traceability and verification of natural language requirements. The
project addresses the challenges in the traceability and verification of formal
specifications through providing support for the automatic generation of the
formal specifications and the traceability of the requirements from the initial
software design stage through the systems implementation and verification.
Approaches explored in this project include Natural Language Processing, use of
ontologies to describe the software system domain, reuse of existing software
artefacts from similar systems (i.e. through similarity based reuse) and large
language models to identify and declare the specifications as well as use of
artificial intelligence to guide the process.

</details>


### [44] [From Tea Leaves to System Maps: Context-awareness in Monitoring Operational Machine Learning Models](https://arxiv.org/abs/2506.10770)
*Joran Leest,Claudia Raibulet,Patricia Lago,Ilias Gerostathopoulos*

Main category: cs.SE

TL;DR: Current ML monitoring focuses too much on statistical data and overlooks context, leading to missed failures. This paper systematically reviews the use of contextual information, introduces the C-SAR framework for structuring context in monitoring, and identifies reusable patterns, paving the way for more robust, context-aware ML monitoring practices.


<details>
  <summary>Details</summary>
Motivation: Machine learning models in production often fail not because of statistical anomalies, but due to contextual misalignments, highlighting gaps in current monitoring approaches that lack meaningful contextual analysis.

Method: The authors conduct a systematic review of 94 primary studies from various related fields. They analyze these studies to characterize and structure types of contextual information relevant to ML monitoring, and introduce the Contextual System--Aspect--Representation (C-SAR) conceptual framework. They also identify 20 reusable patterns and map them to ML monitoring activities.

Result: The study introduces the C-SAR framework, structures the use of contextual information in ML monitoring, identifies 20 recurring patterns, and maps these to specific monitoring activities, providing a more systematic approach to ML monitoring.

Conclusion: The paper offers a new perspective on ML monitoring by shifting from purely statistical alerting to context-informed, systematic practices, enabling deeper root-cause analysis and more reliable ML system management.

Abstract: Machine learning (ML) models in production do not fail due to statistical
anomalies in their input data; they fail due to contextual misalignment -- when
their environment deviates from training assumptions, leading to unreliable
predictions. Effective ML monitoring requires rich contextual information to
move beyond detecting statistical shifts toward meaningful alerts and
systematic root-cause analysis. Yet, surprisingly, despite extensive research
in ML monitoring and related disciplines (drift detection, data validation,
out-of-distribution detection), there is no shared understanding of how to use
contextual information -- striking, given that monitoring involves
interpretation of information in context. In response, this paper presents a
systematic review to characterize and structure the various types of contextual
information in this domain. Our analysis examines 94 primary studies across
data mining, databases, software engineering, and ML. We introduce the
Contextual System--Aspect--Representation (C-SAR) framework, a conceptual model
that synthesizes our findings. We also identify 20 recurring and potentially
reusable patterns of specific system, aspect, and representation combinations,
and map them to the monitoring activities they support. This study provides a
new perspective on ML monitoring: from interpreting "tea leaves" of
observational statistics into constructing and managing "system maps" that
enable systematic and reliable ML monitoring practices.

</details>


### [45] [What Users Value and Critique: Large-Scale Analysis of User Feedback on AI-Powered Mobile Apps](https://arxiv.org/abs/2506.10785)
*Vinaik Chhetri,Krishna Upadhyay,A. B. Siddique,Umar Farooq*

Main category: cs.SE

TL;DR: The paper develops a validated, scalable analysis pipeline to comprehensively extract nuanced user sentiments from nearly 900,000 reviews of AI-powered mobile apps, revealing both general and domain-specific satisfaction and frustrations, and significantly improving understanding compared to traditional, less detailed sentiment analysis methods.


<details>
  <summary>Details</summary>
Motivation: With the rapid increase in AI-powered features in mobile apps across many domains, there is little understanding of how users actually perceive and critique these features, mainly due to the overwhelming quantity and complexity of user feedback. Existing analysis methods often miss the nuanced, fine-grained sentiments users express.

Method: The paper presents a comprehensive, large-scale study of user feedback on AI-enabled mobile apps. It uses a curated dataset of 292 AI-driven apps from Google Play, encompassing 894K AI-specific reviews. They develop a multi-stage analysis pipeline—starting with a human-labeled benchmark set—which systematically evaluates large language models, various prompting strategies, and validates each step (review classification, aspect-sentiment extraction, clustering) for accuracy and consistency.

Result: The pipeline analyzed user feedback at scale, extracting over one million aspect-sentiment pairs and clustering them into 18 positive and 15 negative user topics. Key findings include users highlighting productivity, reliability, and personalized assistance as positives; while negatives focus on technical failures, pricing, and language support limitations. The approach reveals co-occurring positive and negative sentiments within the same feedback, which are often missed in traditional, coarse methods. Category-wise analysis identifies universal sources of satisfaction and domain-specific frustrations.

Conclusion: This work introduces a validated, scalable analysis pipeline that enables nuanced, high-precision analysis of massive user feedback regarding AI-powered apps, revealing detailed user sentiments and providing a truer representation of user experience. The approach outperforms traditional methods that overlook complex, co-occurring sentiments.

Abstract: Artificial Intelligence (AI)-powered features have rapidly proliferated
across mobile apps in various domains, including productivity, education,
entertainment, and creativity. However, how users perceive, evaluate, and
critique these AI features remains largely unexplored, primarily due to the
overwhelming volume of user feedback. In this work, we present the first
comprehensive, large-scale study of user feedback on AI-powered mobile apps,
leveraging a curated dataset of 292 AI-driven apps across 14 categories with
894K AI-specific reviews from Google Play. We develop and validate a
multi-stage analysis pipeline that begins with a human-labeled benchmark and
systematically evaluates large language models (LLMs) and prompting strategies.
Each stage, including review classification, aspect-sentiment extraction, and
clustering, is validated for accuracy and consistency. Our pipeline enables
scalable, high-precision analysis of user feedback, extracting over one million
aspect-sentiment pairs clustered into 18 positive and 15 negative user topics.
Our analysis reveals that users consistently focus on a narrow set of themes:
positive comments emphasize productivity, reliability, and personalized
assistance, while negative feedback highlights technical failures (e.g.,
scanning and recognition), pricing concerns, and limitations in language
support. Our pipeline surfaces both satisfaction with one feature and
frustration with another within the same review. These fine-grained,
co-occurring sentiments are often missed by traditional approaches that treat
positive and negative feedback in isolation or rely on coarse-grained analysis.
To this end, our approach provides a more faithful reflection of the real-world
user experiences with AI-powered apps. Category-aware analysis further uncovers
both universal drivers of satisfaction and domain-specific frustrations.

</details>


### [46] [Evaluating Large Language Models on Non-Code Software Engineering Tasks](https://arxiv.org/abs/2506.10833)
*Fabian C. Peña,Steffen Herbold*

Main category: cs.SE

TL;DR: This paper introduces the SELU benchmark for evaluating LLMs on 17 non-code SE tasks and finds that moderate-scale decoder-only models work best; code-pretraining helps only a little for non-code tasks.


<details>
  <summary>Details</summary>
Motivation: While Large Language Models (LLMs) are known for their success in code-related tasks, their performance on non-code Software Engineering (SE) tasks is not well studied. The paper aims to address this gap by evaluating LLMs specifically on non-code SE tasks.

Method: The authors introduce a new benchmark called SELU, which covers 17 non-code SE tasks including classification, regression, named entity recognition, and masked language modeling. The study involves fine-tuning 22 open-source LLMs, using two proprietary models, and training two baseline models. They evaluate performance with several metrics and compare results using the Bayesian signed-rank test.

Result: Moderate-scale decoder-only LLMs consistently perform the best, showing high mean performance and low variance across tasks. Code-focused pre-training offers only slight improvements for non-code SE tasks.

Conclusion: The SELU benchmark provides the first comprehensive evaluation of LLMs on non-code SE tasks. Moderate-scale decoder-only models are recommended for such tasks, and there is potential for expanding SELU to generative and design-oriented areas.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
code understanding and generation; however, their effectiveness on non-code
Software Engineering (SE) tasks remains underexplored. We present the first
comprehensive benchmark, which we name `Software Engineering Language
Understanding' (SELU), for evaluating LLMs on 17 non-code tasks, spanning from
identifying whether a requirement is functional or non-functional to estimating
the effort and complexity of backlog items. SELU covers classification,
regression, Named Entity Recognition (NER), and Masked Language Modeling (MLM)
targets, with data drawn from diverse sources such as code repositories, issue
tracking systems, and developer forums. We fine-tune 22 open-source LLMs,
prompt two proprietary alternatives, and train two baselines. Performance is
measured using metrics such as F1-macro, SMAPE, F1-micro, and accuracy, and
compared via the Bayesian signed-rank test. Our results show that
moderate-scale decoder-only models consistently form a top-tier, exhibiting
high mean performance and low across-task variance, while domain adaptation via
code-focused pre-training might yield only modest improvements. These insights
guide model selection for non-code SE workflows and highlight directions for
expanding SELU to generative and design-oriented scenarios.

</details>


### [47] [MultiCoSim: A Python-based Multi-Fidelity Co-Simulation Framework](https://arxiv.org/abs/2506.10869)
*Quinn Thibeault,Giulia Pedrielli*

Main category: cs.SE

TL;DR: The paper presents MultiCoSim, a Python-based framework enabling flexible and automated co-simulation of diverse CPS components. It overcomes the rigidity and lack of modularity in traditional tools, as shown in various case studies, making CPS simulation more effective for research and development.


<details>
  <summary>Details</summary>
Motivation: As cyber-physical systems (CPS) become more complex and integrated, especially in safety-critical and learning-enabled domains, there is a growing need for flexible and rapid simulation tools. Current simulation platforms struggle with heterogeneity, modularity, and automation, making analysis, benchmarking, and component integration difficult.

Method: The authors introduce MultiCoSim, a Python-based simulation framework. MultiCoSim enables programmatic definition, composition, and configuration of simulation components, supporting distributed and component-based co-simulation. The framework offers seamless substitution and reconfiguration of elements and is demonstrated through case studies involving both custom controllers and integration with platforms like PX4 for aerial robotics.

Result: MultiCoSim allows for flexible, modular, and automated CPS simulation. The case studies show that the framework can handle heterogeneous co-simulations, integrate with existing platforms, and streamline the research and development process for CPS.

Conclusion: MultiCoSim overcomes the limitations of existing simulation tools by providing a flexible, programmable, and modular co-simulation environment for CPS, enabling improved benchmarking, comparative evaluation, and integration across diverse domains.

Abstract: Simulation is a foundational tool for the analysis and testing of
cyber-physical systems (CPS), underpinning activities such as algorithm
development, runtime monitoring, and system verification. As CPS grow in
complexity and scale, particularly in safety-critical and learning-enabled
settings, accurate analysis and synthesis increasingly rely on the rapid use of
simulation experiments. Because CPS inherently integrate hardware, software,
and physical processes, simulation platforms must support co-simulation of
heterogeneous components at varying levels of fidelity. Despite recent advances
in high-fidelity modeling of hardware, firmware, and physics, co-simulation in
diverse environments remains challenging. These limitations hinder the
development of reusable benchmarks and impede the use of simulation for
automated and comparative evaluation.
  Existing simulation tools often rely on rigid configurations, lack automation
support, and present obstacles to portability and modularity. Many are
configured through static text files or impose constraints on how simulation
components are represented and connected, making it difficult to flexibly
compose systems or integrate components across platforms.
  To address these challenges, we introduce MultiCoSim, a Python-based
simulation framework that enables users to define, compose, and configure
simulation components programmatically. MultiCoSim supports distributed,
component-based co-simulation and allows seamless substitution and
reconfiguration of components. We demonstrate the flexibility of MultiCoSim
through case studies that include co-simulations involving custom
automaton-based controllers, as well as integration with off-the-shelf
platforms like the PX4 autopilot for aerial robotics. These examples highlight
MultiCoSim's capability to streamline CPS simulation pipelines for research and
development.

</details>


### [48] [SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks](https://arxiv.org/abs/2506.10954)
*Lianghong Guo,Yanlin Wang,Caihua Li,Pengyu Yang,Jiachi Chen,Wei Tao,Yingtian Zou,Duyu Tang,Zibin Zheng*

Main category: cs.SE

TL;DR: The authors introduce SWE-Factory, an automated, cost-effective pipeline for building large GitHub issue resolution datasets, eliminating much of the manual work through multi-agent systems and exit-code grading. It produces valid, high-quality data efficiently and accurately, with code and datasets available for community use.


<details>
  <summary>Details</summary>
Motivation: Constructing large-scale, high-quality datasets for GitHub issue resolution is essential for advancing and evaluating the software engineering capability of Large Language Models (LLMs). Traditional dataset creation methods are difficult, time-consuming, and require significant manual labor, especially in environment setup, test grading, and task validation.

Method: The authors propose SWE-Factory, an automated pipeline with three main components: (1) SWE-Builder, a multi-agent system that collaboratively and iteratively builds evaluation environments using an environment memory pool; (2) an exit-code-based grading approach, which standardizes test outcome grading and removes the need for manual custom parsers; (3) an automated fail2pass validation mechanism that relies on exit code signals.

Result: Experiments covering 671 GitHub issues in four programming languages demonstrate that SWE-Factory can efficiently produce valid task instances. For example, using GPT-4.1-mini, SWE-Builder constructs 269 valid instances at $0.045 each, whereas Gemini-2.5-flash achieves similar results at a lower cost of $0.024 per instance. Their exit-code-based grading method matches 100% with manual inspection, and automated fail2pass validation achieves 0.92 precision and 1.00 recall.

Conclusion: SWE-Factory automates and streamlines the creation of large-scale GitHub issue resolution datasets, significantly reducing manual labor and cost while maintaining high accuracy and efficiency. The released code and datasets can accelerate future research and evaluation for LLMs in software engineering tasks.

Abstract: Constructing large-scale datasets for the GitHub issue resolution task is
crucial for both training and evaluating the software engineering capabilities
of Large Language Models (LLMs). However, the traditional process for creating
such benchmarks is notoriously challenging and labor-intensive, particularly in
the stages of setting up evaluation environments, grading test outcomes, and
validating task instances. In this paper, we propose SWE-Factory, an automated
pipeline designed to address these challenges. To tackle these issues, our
pipeline integrates three core automated components. First, we introduce
SWE-Builder, a multi-agent system that automates evaluation environment
construction, which employs four specialized agents that work in a
collaborative, iterative loop and leverages an environment memory pool to
enhance efficiency. Second, we introduce a standardized, exit-code-based
grading method that eliminates the need for manually writing custom parsers.
Finally, we automate the fail2pass validation process using these reliable exit
code signals. Experiments on 671 issues across four programming languages show
that our pipeline can effectively construct valid task instances; for example,
with GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at $0.045 per
instance, while with Gemini-2.5-flash, it achieves comparable performance at
the lowest cost of $0.024 per instance. We also demonstrate that our
exit-code-based grading achieves 100% accuracy compared to manual inspection,
and our automated fail2pass validation reaches a precision of 0.92 and a recall
of 1.00. We hope our automated pipeline will accelerate the collection of
large-scale, high-quality GitHub issue resolution datasets for both training
and evaluation. Our code and datasets are released at
https://github.com/DeepSoftwareAnalytics/swe-factory.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [49] [From Tool Calling to Symbolic Thinking: LLMs in a Persistent Lisp Metaprogramming Loop](https://arxiv.org/abs/2506.10021)
*Jordi de la Torre*

Main category: cs.PL

TL;DR: The paper proposes integrating LLMs with a persistent Lisp environment, allowing them to create, use, and evolve their own tools interactively. This approach enables stateful memory, reflective programming, and lays the groundwork for more interactive and autonomous AI systems.


<details>
  <summary>Details</summary>
Motivation: Current AI systems, especially those based on large language models (LLMs), lack persistent statefulness and the ability to create or modify their own tools dynamically within interactive environments. The paper is motivated by the need to empower LLMs with capabilities for reflective programming, stateful memory, and autonomous tool evolution by effectively integrating symbolic and neural approaches.

Method: The authors introduce an architecture that integrates LLMs with a persistent, interactive Lisp environment. In this setup, LLMs generate and execute Lisp expressions within a live REPL, using a middleware layer to intercept and manage these interactions. This allows the language model to define, invoke, and adapt its own tools programmatically while maintaining contextual awareness.

Result: The system demonstrates that LLMs, when plugged into a symbolic programming environment like Lisp with appropriate middleware, can achieve interactive tool creation, reflective program modification, and persistent state management. The result is a flexible framework for building more interactive and autonomous AI systems.

Conclusion: The paper concludes that combining LLMs with a live symbolic programming platform enables smarter, more flexible AI agents. The proposed architecture supports ongoing tool evolution and stateful interactions, setting principles for future development of interactive and adaptive AI systems.

Abstract: We propose a novel architecture for integrating large language models (LLMs)
with a persistent, interactive Lisp environment. This setup enables LLMs to
define, invoke, and evolve their own tools through programmatic interaction
with a live REPL. By embedding Lisp expressions within generation and
intercepting them via a middleware layer, the system allows for stateful
external memory, reflective programming, and dynamic tool creation. We present
a design framework and architectural principles to guide future implementations
of interactive AI systems that integrate symbolic programming with neural
language generation.

</details>


### [50] [A Language-Agnostic Logical Relation for Message-Passing Protocols](https://arxiv.org/abs/2506.10026)
*Tesla Zhang,Sonya Simkin,Rui Li,Yue Yao,Stephanie Balzer*

Main category: cs.PL

TL;DR: The paper presents a Coq-based, language-agnostic framework for verifying that distributed, message-passing systems (even those mixing different languages, untyped code, and devices) comply with communication protocols, overcoming limitations of previous language- or type-dependent methods.


<details>
  <summary>Details</summary>
Motivation: Modern applications increasingly target distributed and heterogeneous environments, often involving multiple concurrent components that communicate by message-passing and interact with foreign objects, such as external libraries or hardware devices. Verifying that these diverse systems adhere to their communication protocols is challenging because traditional assumptions, like using a single programming language or uniform type system, do not hold.

Method: The paper introduces a framework for certifying protocol compliance in heterogeneous, message-passing systems. It establishes the first mechanization of a language-agnostic logical relation based on labelled transition system semantics, allowing arbitrary components (typed, untyped, or foreign) to be analyzed. The framework is implemented and verified in the Coq proof assistant.

Result: The framework supports two verification scenarios: (1) per-instance verification of individual applications or devices, and (2) general, once-and-for-all verification for all well-typed applications under a given type system. Both scenarios and the logical relation itself are mechanized and demonstrated in Coq, confirming their feasibility and effectiveness.

Conclusion: This work demonstrates that protocol compliance in heterogeneous message-passing systems can be rigorously verified without relying on shared implementation languages or type systems, using a novel logical relation and formal verification in Coq. The approach is broadly applicable to both specific instances and entire classes of well-typed systems.

Abstract: Today's computing landscape has been gradually shifting to applications
targeting distributed and *heterogeneous* systems, such as cloud computing and
Internet of Things (IoT) applications. These applications are predominantly
*concurrent*, employ *message-passing*, and interface with *foreign objects*,
ranging from externally implemented code to actual physical devices such as
sensors. Verifying that the resulting systems adhere to the intended protocol
of interaction is challenging -- the usual assumption of a common
implementation language, let alone a type system, no longer applies, ruling out
any verification method based on them. This paper develops a framework for
certifying *protocol compliance* of heterogeneous message-passing systems. It
contributes the first mechanization of a *language-agnostic logical relation*,
asserting that its inhabitants comply with the protocol specified. This
definition relies entirely on a labelled transition-based semantics,
accommodating arbitrary inhabitants, typed and untyped alike, including foreign
objects. As a case study, the paper considers two scenarios: (1) *per-instance
verification* of a specific application or hardware device, and (2)
*once-and-for-all verification* of well-typed applications for a given type
system. The logical relation and both scenarios are mechanized in the Coq
theorem prover.

</details>


### [51] [Hazel Deriver: A Live Editor for Constructing Rule-Based Derivations](https://arxiv.org/abs/2506.10781)
*Zhiyao Zhong,Cyrus Omar*

Main category: cs.PL

TL;DR: Hazel Deriver is a web-based tool that helps students build derivation trees with interactive, real-time support. Early studies show it makes the task easier and improves understanding, though the right balance of guidance is still under consideration.


<details>
  <summary>Details</summary>
Motivation: Many students have difficulty constructing rule-based derivation trees in programming languages and logic courses. The challenges come from complex inference rules, lack of immediate feedback, and the tediousness of manual proof construction.

Method: The authors introduce Hazel Deriver, a web-based live editor built on the Hazel environment, which offers multilayered scaffolding, structured and interactive experiences, and real-time feedback to walk students through derivation construction.

Result: A preliminary user study with previous students indicates reduced perceived difficulty, greater conceptual understanding, and increased engagement when using Hazel Deriver.

Conclusion: Hazel Deriver can make rule-based derivation construction easier and more engaging. The paper discusses how its design supports learning and brings up the issue of balancing system help with student independence.

Abstract: Students in programming languages and formal logic courses often struggle
with constructing rule-based derivation trees due to the complexity of applying
inference rules, the lack of immediate feedback, and the manual effort required
for handwritten proofs. We present Hazel Deriver, a live, web-based editor
designed to scaffold derivation construction through multiple layers of
support. Built on the Hazel live programming environment, it provides a
structured, interactive experience that encourages iterative exploration and
real-time feedback. A preliminary user study with former students suggests that
Hazel Deriver reduces the perceived difficulty of derivation tasks while
improving conceptual understanding and engagement. We discuss the design of its
layered scaffolding features and raise questions about balancing system
guidance with learner autonomy.

</details>


### [52] [Choreographic Quick Changes: First-Class Location (Set) Polymorphism](https://arxiv.org/abs/2506.10913)
*Ashley Samuelson,Andrew K. Hirsch,Ethan Cecchetti*

Main category: cs.PL

TL;DR: λ_QC is a newly designed, more expressive choreographic programming language supporting dynamic process computation, polymorphism, and advanced data types, with correctness and deadlock-freedom formally verified.


<details>
  <summary>Details</summary>
Motivation: Existing choreographic languages for concurrent systems lack the ability for nodes to dynamically decide who should perform computations and communicate those decisions, as well as missing support for key features like polymorphism over process names/locations and powerful data types.

Method: The paper introduces λ_QC, a new typed choreographic language with first-class process names, polymorphism over types and process locations, and support for complex data types. Results are formalized and mechanically verified in Rocq.

Result: λ_QC is more expressive than prior choreographic languages, enables dynamic process assignment, supports algebraic and recursive data types, and multiply-located values. Formal verification in Rocq demonstrates that the language maintains important properties such as deadlock freedom.

Conclusion: λ_QC closes an important gap in choreographic programming by providing, for the first time, a typed language with dynamic process naming, location polymorphism, and advanced data structuring, all while retaining critical correctness guarantees.

Abstract: Choreographic programming is a promising new paradigm for programming
concurrent systems where a developer writes a single centralized program that
compiles to individual programs for each node. Existing choreographic
languages, however, lack critical features integral to modern systems, like the
ability of one node to dynamically compute who should perform a computation and
send that decision to others. This work addresses this gap with $\lambda_{QC}$,
the first typed choreographic language with \emph{first class process names}
and polymorphism over both types and (sets of) locations. $\lambda_{QC}$ also
improves expressive power over previous work by supporting algebraic and
recursive data types as well as multiply-located values. We formalize and
mechanically verify our results in Rocq, including the standard choreographic
guarantee of deadlock freedom.

</details>


### [53] [From Tool Calling to Symbolic Thinking: LLMs in a Persistent Lisp Metaprogramming Loop](https://arxiv.org/abs/2506.10021)
*Jordi de la Torre*

Main category: cs.PL

TL;DR: The paper proposes a new way to combine LLMs with live Lisp environments, so LLMs can write, use, and improve their own tools in real time, advancing the integration of programmatic and neural AI.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack the ability to dynamically create, use, and modify tools in a persistent, programmable environment. Integrating symbolic programming with neural generation remains an open challenge.

Method: A new architecture integrates LLMs with a live, interactive Lisp REPL. The system embeds Lisp code in LLM outputs and intercepts these calls with a middleware layer, enabling stateful, programmatic interaction and dynamic tool creation.

Result: Demonstrated architecture supports stateful external memory, reflective programming, and allows LLMs to create and evolve tools interactively. Presents a framework and design principles for future systems that blend symbolic and neural AI.

Conclusion: Integrating LLMs with a persistent Lisp environment enables more powerful, self-improving, and interactive AI systems through dynamic tool use and creation, providing a pathway for advanced symbolic-neural integration.

Abstract: We propose a novel architecture for integrating large language models (LLMs)
with a persistent, interactive Lisp environment. This setup enables LLMs to
define, invoke, and evolve their own tools through programmatic interaction
with a live REPL. By embedding Lisp expressions within generation and
intercepting them via a middleware layer, the system allows for stateful
external memory, reflective programming, and dynamic tool creation. We present
a design framework and architectural principles to guide future implementations
of interactive AI systems that integrate symbolic programming with neural
language generation.

</details>


### [54] [A Language-Agnostic Logical Relation for Message-Passing Protocols](https://arxiv.org/abs/2506.10026)
*Tesla Zhang,Sonya Simkin,Rui Li,Yue Yao,Stephanie Balzer*

Main category: cs.PL

TL;DR: This paper presents a new framework for checking protocol compliance in distributed, heterogeneous systems, regardless of programming language or typing. The method, mechanized in Coq, enables formal verification even when systems are composed of mixed, foreign, or untyped objects.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of verifying protocol compliance in distributed and heterogeneous systems, such as cloud computing and IoT, where components often use differing languages and may interact with external objects.

Method: The paper creates a framework for protocol compliance certification, introducing a language-agnostic logical relation based on labelled transition semantics that works for both typed and untyped components, including foreign objects. The framework and its scenarios are implemented in the Coq theorem prover.

Result: The authors successfully mechanize their logical relation in Coq and demonstrate its utility in two scenarios: verifying individual (possibly hardware) instances, and general well-typed applications under a specific type system.

Conclusion: The paper establishes the feasibility of language-agnostic protocol verification in heterogeneous message-passing systems using logical relations and formalizes this in Coq, paving the way for reliable verification across diverse environments.

Abstract: Today's computing landscape has been gradually shifting to applications
targeting distributed and *heterogeneous* systems, such as cloud computing and
Internet of Things (IoT) applications. These applications are predominantly
*concurrent*, employ *message-passing*, and interface with *foreign objects*,
ranging from externally implemented code to actual physical devices such as
sensors. Verifying that the resulting systems adhere to the intended protocol
of interaction is challenging -- the usual assumption of a common
implementation language, let alone a type system, no longer applies, ruling out
any verification method based on them. This paper develops a framework for
certifying *protocol compliance* of heterogeneous message-passing systems. It
contributes the first mechanization of a *language-agnostic logical relation*,
asserting that its inhabitants comply with the protocol specified. This
definition relies entirely on a labelled transition-based semantics,
accommodating arbitrary inhabitants, typed and untyped alike, including foreign
objects. As a case study, the paper considers two scenarios: (1) *per-instance
verification* of a specific application or hardware device, and (2)
*once-and-for-all verification* of well-typed applications for a given type
system. The logical relation and both scenarios are mechanized in the Coq
theorem prover.

</details>


### [55] [Hazel Deriver: A Live Editor for Constructing Rule-Based Derivations](https://arxiv.org/abs/2506.10781)
*Zhiyao Zhong,Cyrus Omar*

Main category: cs.PL

TL;DR: Hazel Deriver is a web-based tool that helps students create rule-based derivations more easily by providing interactive, layered support and feedback, leading to better understanding and engagement in a user study.


<details>
  <summary>Details</summary>
Motivation: Students find it difficult to construct rule-based derivation trees in programming languages and formal logic courses due to the complexity of inference rules, lack of immediate feedback, and challenges with manual, handwritten proofs.

Method: The authors developed Hazel Deriver, a live, web-based editor built on the Hazel environment. It provides structured, interactive, scaffolded support for constructing derivations, with real-time feedback and iterative exploration.

Result: A user study with former students indicated that Hazel Deriver reduced the perceived difficulty of derivation tasks, increased conceptual understanding, and improved engagement.

Conclusion: Hazel Deriver is effective for scaffolding derivation tasks in programming and logic courses, but balancing automated guidance with learner autonomy remains an open question.

Abstract: Students in programming languages and formal logic courses often struggle
with constructing rule-based derivation trees due to the complexity of applying
inference rules, the lack of immediate feedback, and the manual effort required
for handwritten proofs. We present Hazel Deriver, a live, web-based editor
designed to scaffold derivation construction through multiple layers of
support. Built on the Hazel live programming environment, it provides a
structured, interactive experience that encourages iterative exploration and
real-time feedback. A preliminary user study with former students suggests that
Hazel Deriver reduces the perceived difficulty of derivation tasks while
improving conceptual understanding and engagement. We discuss the design of its
layered scaffolding features and raise questions about balancing system
guidance with learner autonomy.

</details>


### [56] [Choreographic Quick Changes: First-Class Location (Set) Polymorphism](https://arxiv.org/abs/2506.10913)
*Ashley Samuelson,Andrew K. Hirsch,Ethan Cecchetti*

Main category: cs.PL

TL;DR: The paper introduces $\lambda_{QC}$, a choreographic language with advanced features like first-class process names and polymorphism, addresses major limitations in prior languages for concurrent systems, and proves its properties (like deadlock freedom) with formal mechanical verification.


<details>
  <summary>Details</summary>
Motivation: Current choreographic programming languages lack features such as dynamic determination of computational responsibility (i.e., letting a node decide who should compute and communicating that decision), which are important for modern concurrent systems.

Method: The authors present $\lambda_{QC}$, a new typed choreographic language that introduces first-class process names, polymorphism over types and over sets of locations, supports algebraic and recursive data types, and multiply-located values. They formalize and mechanically verify their results using Rocq.

Result: $\lambda_{QC}$ extends the expressive power of choreographic programming languages, accommodates critical modern concurrency features, and maintains deadlock freedom, which is mechanically verified in Rocq.

Conclusion: $\lambda_{QC}$ is the first choreographic language to offer both first-class process names and advanced polymorphism, thus bridging gaps in expressiveness and practical usability for modern concurrent systems.

Abstract: Choreographic programming is a promising new paradigm for programming
concurrent systems where a developer writes a single centralized program that
compiles to individual programs for each node. Existing choreographic
languages, however, lack critical features integral to modern systems, like the
ability of one node to dynamically compute who should perform a computation and
send that decision to others. This work addresses this gap with $\lambda_{QC}$,
the first typed choreographic language with \emph{first class process names}
and polymorphism over both types and (sets of) locations. $\lambda_{QC}$ also
improves expressive power over previous work by supporting algebraic and
recursive data types as well as multiply-located values. We formalize and
mechanically verify our results in Rocq, including the standard choreographic
guarantee of deadlock freedom.

</details>
