{"id": "2509.16268", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16268", "abs": "https://arxiv.org/abs/2509.16268", "authors": ["Zhenlan Ji", "Daoyuan Wu", "Wenxuan Wang", "Pingchuan Ma", "Shuai Wang", "Lei Ma"], "title": "Digging Into the Internal: Causality-Based Analysis of LLM Function Calling", "comment": null, "summary": "Function calling (FC) has emerged as a powerful technique for facilitating\nlarge language models (LLMs) to interact with external systems and perform\nstructured tasks. However, the mechanisms through which it influences model\nbehavior remain largely under-explored. Besides, we discover that in addition\nto the regular usage of FC, this technique can substantially enhance the\ncompliance of LLMs with user instructions. These observations motivate us to\nleverage causality, a canonical analysis method, to investigate how FC works\nwithin LLMs. In particular, we conduct layer-level and token-level causal\ninterventions to dissect FC's impact on the model's internal computational\nlogic when responding to user queries. Our analysis confirms the substantial\ninfluence of FC and reveals several in-depth insights into its mechanisms. To\nfurther validate our findings, we conduct extensive experiments comparing the\neffectiveness of FC-based instructions against conventional prompting methods.\nWe focus on enhancing LLM safety robustness, a critical LLM application\nscenario, and evaluate four mainstream LLMs across two benchmark datasets. The\nresults are striking: FC shows an average performance improvement of around\n135% over conventional prompting methods in detecting malicious inputs,\ndemonstrating its promising potential to enhance LLM reliability and capability\nin practical applications.", "AI": {"tldr": "Function calling (FC) is not only a mechanism for LLMs to interact with external systems but also greatly boosts instruction compliance and safety. Causal analysis reveals FC\u2019s substantial impact on model behavior. Experiments show FC outperforms regular prompts by 135% in detecting malicious inputs, highlighting FC\u2019s strong potential to improve LLM robustness and reliability.", "motivation": "Function calling (FC) is a promising method for improving how large language models (LLMs) execute tasks and interact with user instructions, but its influence on model behavior and effectiveness has not been thoroughly studied.", "method": "The study applies causal analysis at both layer and token levels within LLMs to dissect how FC affects the models\u2019 internal computational mechanisms and responses. The researchers also compare FC-based instruction methods with conventional prompting across four mainstream LLMs using two benchmark datasets.", "result": "FC substantially improves the detection of malicious inputs and overall safety robustness of LLMs. Experimental results indicate that FC-based instructions offer an average performance increase of about 135% over conventional methods.", "conclusion": "Function calling significantly enhances LLM compliance with user instructions and boosts their capability, especially in safety-critical applications, compared to traditional prompting techniques."}}
{"id": "2509.16478", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16478", "abs": "https://arxiv.org/abs/2509.16478", "authors": ["Hossein Yousefizadeh", "Shenghui Gu", "Lionel C. Briand", "Ali Nasr"], "title": "Constrained Co-evolutionary Metamorphic Differential Testing for Autonomous Systems with an Interpretability Approach", "comment": null, "summary": "Autonomous systems, such as autonomous driving systems, evolve rapidly\nthrough frequent updates, risking unintended behavioral degradations. Effective\nsystem-level testing is challenging due to the vast scenario space, the absence\nof reliable test oracles, and the need for practically applicable and\ninterpretable test cases. We present CoCoMagic, a novel automated test case\ngeneration method that combines metamorphic testing, differential testing, and\nadvanced search-based techniques to identify behavioral divergences between\nversions of autonomous systems. CoCoMagic formulates test generation as a\nconstrained cooperative co-evolutionary search, evolving both source scenarios\nand metamorphic perturbations to maximize differences in violations of\npredefined metamorphic relations across versions. Constraints and population\ninitialization strategies guide the search toward realistic, relevant\nscenarios. An integrated interpretability approach aids in diagnosing the root\ncauses of divergences. We evaluate CoCoMagic on an end-to-end ADS, InterFuser,\nwithin the Carla virtual simulator. Results show significant improvements over\nbaseline search methods, identifying up to 287\\% more distinct high-severity\nbehavioral differences while maintaining scenario realism. The interpretability\napproach provides actionable insights for developers, supporting targeted\ndebugging and safety assessment. CoCoMagic offers an efficient, effective, and\ninterpretable way for the differential testing of evolving autonomous systems\nacross versions.", "AI": {"tldr": "CoCoMagic is a novel, interpretable test generation system for autonomous systems. It finds many more critical behavioral differences between software versions than previous methods, helping developers improve safety and debugging.", "motivation": "Autonomous systems like self-driving cars undergo frequent updates, which risks introducing unintended behavioral degradations. Testing these systems is difficult due to the large scenario space, lack of reliable test oracles, and the need for test cases that are relevant and interpretable.", "method": "The authors propose CoCoMagic, an automated test generation framework that combines metamorphic testing, differential testing, and search-based techniques. They formulate test generation as a constrained cooperative co-evolutionary search that evolves both source scenarios and metamorphic perturbations to uncover behavioral differences across system versions. Additional constraints and initialization strategies keep the generated scenarios realistic, and an interpretability module helps diagnose divergences.", "result": "Experiments on the InterFuser autonomous driving stack (within the Carla simulator) demonstrate that CoCoMagic outperforms baseline approaches, finding up to 287% more distinct, high-severity behavioral differences while ensuring scenario realism. Interpretability tools help developers understand and debug root causes of those differences.", "conclusion": "CoCoMagic is an efficient, effective, and interpretable test generation framework that significantly enhances the differential testing of evolving autonomous systems, enabling better safety assessment and debugging."}}
{"id": "2509.16525", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16525", "abs": "https://arxiv.org/abs/2509.16525", "authors": ["Anna Mazhar", "Sainyam Galhotra"], "title": "Causal Fuzzing for Verifying Machine Unlearning", "comment": null, "summary": "As machine learning models become increasingly embedded in decision-making\nsystems, the ability to \"unlearn\" targeted data or features is crucial for\nenhancing model adaptability, fairness, and privacy in models which involves\nexpensive training. To effectively guide machine unlearning, a thorough testing\nis essential. Existing methods for verification of machine unlearning provide\nlimited insights, often failing in scenarios where the influence is indirect.\nIn this work, we propose CAF\\'E, a new causality based framework that unifies\ndatapoint- and feature-level unlearning for verification of black-box ML\nmodels. CAF\\'E evaluates both direct and indirect effects of unlearning targets\nthrough causal dependencies, providing actionable insights with fine-grained\nanalysis. Our evaluation across five datasets and three model architectures\ndemonstrates that CAF\\'E successfully detects residual influence missed by\nbaselines while maintaining computational efficiency.", "AI": {"tldr": "CAF'\u00c9 is a new causality-based framework for verifying machine unlearning in black-box models, detecting more subtle influences than existing methods and doing so efficiently.", "motivation": "The motivation is to improve the verification of machine unlearning, which is essential for enhancing adaptability, fairness, and privacy in machine learning models, especially when retraining is expensive. Current verification methods provide limited insights and often miss indirect influences.", "method": "The paper proposes CAF'\u00c9, a causality-based framework that unifies datapoint and feature-level unlearning verification for black-box ML models. CAF'\u00c9 analyzes both direct and indirect effects using causal dependencies, enabling fine-grained and actionable evaluation.", "result": "CAF'\u00c9 was evaluated on five datasets and three model architectures, demonstrating its ability to detect residual influence that baseline approaches missed, while maintaining computational efficiency.", "conclusion": "CAF'\u00c9 provides a more effective and efficient verification method for machine unlearning by considering both direct and indirect causal effects, outperforming existing techniques and offering fine-grained insights."}}
{"id": "2509.16595", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16595", "abs": "https://arxiv.org/abs/2509.16595", "authors": ["Jiaming Ye", "Xiongfei Wu", "Shangzhou Xia", "Fuyuan Zhang", "Jianjun Zhao"], "title": "Is Measurement Enough? Rethinking Output Validation in Quantum Program Testing", "comment": "This paper will be appeared in the proceedings of the 40th IEEE/ACM\n  International Conference on Automated Software Engineering (ASE 2025), NIER\n  track, Seoul, South Korea, November 16 -20, 2025", "summary": "As quantum computing continues to emerge, ensuring the quality of quantum\nprograms has become increasingly critical. Quantum program testing has emerged\nas a prominent research area within the scope of quantum software engineering.\nWhile numerous approaches have been proposed to address quantum program quality\nassurance, our analysis reveals that most existing methods rely on\nmeasurement-based validation in practice. However, due to the inherently\nprobabilistic nature of quantum programs, measurement-based validation methods\nface significant limitations.\n  To investigate these limitations, we conducted an empirical study of recent\nresearch on quantum program testing, analyzing measurement-based validation\nmethods in the literature. Our analysis categorizes existing measurement-based\nvalidation methods into two groups: distribution-level validation and\noutput-value-level validation. We then compare measurement-based validation\nwith statevector-based validation methods to evaluate their pros and cons. Our\nfindings demonstrate that measurement-based validation is suitable for\nstraightforward assessments, such as verifying the existence of specific output\nvalues, while statevector-based validation proves more effective for\ncomplicated tasks such as assessing the program behaviors.", "AI": {"tldr": "Quantum program testing mostly uses measurement-based validation, but this method struggles with complex programs due to quantum probabilism. The study shows measurement-based validation works for simple tasks, while statevector-based methods are better for complex assessments.", "motivation": "The motivation is to improve the quality assurance of quantum programs, which is a critical challenge due to the probabilistic nature of quantum computing and the reliance on measurement-based testing methods.", "method": "The authors conducted an empirical study of recent research on quantum program testing. They analyzed the literature, categorizing measurement-based validation methods into distribution-level and output-value-level groups, and compared these with statevector-based validation methods.", "result": "The study found that measurement-based validation is suitable for simple tasks like checking for the presence of certain output values. In contrast, statevector-based validation is more effective for assessing complex program behaviors.", "conclusion": "Measurement-based validation has inherent limitations for quantum program testing, especially for complex behaviors, and statevector-based validation offers advantages in these situations."}}
{"id": "2509.16246", "categories": ["cs.PL", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.16246", "abs": "https://arxiv.org/abs/2509.16246", "authors": ["Juxin Niu", "Yuxin Du", "Dan Niu", "Xi Wang", "Zhe Jiang", "Nan Guan"], "title": "VerilogMonkey: Exploring Parallel Scaling for Automated Verilog Code Generation with LLMs", "comment": null, "summary": "We present VerilogMonkey, an empirical study of parallel scaling for the\nunder-explored task of automated Verilog generation. Parallel scaling improves\nLLM performance by sampling many outputs in parallel. Across multiple\nbenchmarks and mainstream LLMs, we find that scaling to hundreds of samples is\ncost-effective in both time and money and, even without any additional\nenhancements such as post-training or agentic methods, surpasses prior results\non LLM-based Verilog generation. We further dissect why parallel scaling\ndelivers these gains and show how output randomness in LLMs affects its\neffectiveness.", "AI": {"tldr": "Sampling many outputs in parallel from LLMs greatly improves automated Verilog code generation, making it faster, cheaper, and more effective than previous approaches, even without extra training techniques.", "motivation": "Automated Verilog generation is an important, under-explored task. Recent success in LLM-based code generation can potentially be leveraged, but the effectiveness of parallel scaling for this specific domain is not well studied.", "method": "The authors conduct an empirical study investigating the effect of parallel scaling on automated Verilog code generation using LLMs. They sample many outputs in parallel across multiple benchmarks and LLMs, analyzing the cost (time/money), performance, and the impact of output randomness.", "result": "Parallel scaling to hundreds of samples improves Verilog generation performance, being cost-effective and surpassing previous results, even without additional methods such as post-training or agentic enhancements.", "conclusion": "Parallel sampling with LLMs is highly effective for automated Verilog generation, resulting in better performance, efficiency, and providing insight into the underlying impact of LLM output randomness."}}
