<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [On LLM-Assisted Generation of Smart Contracts from Business Processes](https://arxiv.org/abs/2507.23087)
*Fabian Stiehle,Hans Weytjens,Ingo Weber*

Main category: cs.SE

TL;DR: LLMs are promising for smart contract code generation, but current models are not reliable enough. The authors propose a new evaluation framework and find that better integration and development are needed to ensure safe, correct code.


<details>
  <summary>Details</summary>
Motivation: There is increasing interest in using large language models (LLMs) to automatically generate smart contract code from business process descriptions because traditional rule-based code generation methods have limitations. Current LLM-based approaches are often evaluated with small datasets and superficial checks, making it unclear how reliably they produce correct smart contracts.

Method: The paper presents an exploratory study using an automated evaluation framework to empirically test LLMs of various types and sizes. The models were assessed on their ability to correctly enforce core smart contract properties—such as process flow, resource allocation, and data-based conditions—using large datasets of process models.

Result: LLMs were found to be unreliable for smart contract generation, as their performance did not meet the stringent reliability standards necessary for this application.

Conclusion: While LLMs have potential, they currently do not offer sufficient reliability for critical smart contract code generation tasks. There is a need for more responsible and robust integration of LLMs into existing code generation tools, and the paper’s benchmarking framework can aid in developing and testing safer approaches.

Abstract: Large language models (LLMs) have changed the reality of how software is
produced. Within the wider software engineering community, among many other
purposes, they are explored for code generation use cases from different types
of input. In this work, we present an exploratory study to investigate the use
of LLMs for generating smart contract code from business process descriptions,
an idea that has emerged in recent literature to overcome the limitations of
traditional rule-based code generation approaches. However, current LLM-based
work evaluates generated code on small samples, relying on manual inspection,
or testing whether code compiles but ignoring correct execution. With this
work, we introduce an automated evaluation framework and provide empirical data
from larger data sets of process models. We test LLMs of different types and
sizes in their capabilities of achieving important properties of process
execution, including enforcing process flow, resource allocation, and
data-based conditions. Our results show that LLM performance falls short of the
perfect reliability required for smart contract development. We suggest future
work to explore responsible LLM integrations in existing tools for code
generation to ensure more reliable output. Our benchmarking framework can serve
as a foundation for developing and evaluating such integrations.

</details>


### [2] [FlowETL: An Autonomous Example-Driven Pipeline for Data Engineering](https://arxiv.org/abs/2507.23118)
*Mattia Di Profio,Mingjun Zhong,Yaji Sripada,Marcel Jaspars*

Main category: cs.SE

TL;DR: FlowETL automates the process of creating ETL data transformations using example pairs, reducing reliance on manual engineering and successfully generalizing across different datasets.


<details>
  <summary>Details</summary>
Motivation: Modern ETL workflows require significant human intervention to create context-specific data transformations, making the process labor-intensive and difficult to generalize.

Method: The authors introduce FlowETL, an autonomous ETL pipeline that uses example-based learning to generate transformation plans. A Planning Engine constructs transformation logic from paired input-output samples, which an ETL worker applies to raw data. The system provides monitoring and logging for full observability.

Result: FlowETL demonstrated strong generalization across 14 datasets with varied domains, structures, and sizes, indicating its potential to automate standard ETL tasks.

Conclusion: FlowETL effectively reduces the manual effort needed in designing and applying ETL transformations, successfully generalizing automated data processing across multiple contexts.

Abstract: The Extract, Transform, Load (ETL) workflow is fundamental for populating and
maintaining data warehouses and other data stores accessed by analysts for
downstream tasks. A major shortcoming of modern ETL solutions is the extensive
need for a human-in-the-loop, required to design and implement
context-specific, and often non-generalisable transformations. While related
work in the field of ETL automation shows promising progress, there is a lack
of solutions capable of automatically designing and applying these
transformations. We present FlowETL, a novel example-based autonomous ETL
pipeline architecture designed to automatically standardise and prepare input
datasets according to a concise, user-defined target dataset. FlowETL is an
ecosystem of components which interact together to achieve the desired outcome.
A Planning Engine uses a paired input-output datasets sample to construct a
transformation plan, which is then applied by an ETL worker to the source
dataset. Monitoring and logging provide observability throughout the entire
pipeline. The results show promising generalisation capabilities across 14
datasets of various domains, file structures, and file sizes.

</details>


### [3] [Vibe Modeling: Challenges and Opportunities](https://arxiv.org/abs/2507.23120)
*Jordi Cabot*

Main category: cs.SE

TL;DR: The paper proposes 'vibe modeling', a hybrid approach utilizing both AI (LLMs) and model-driven engineering to address complexity in modern software development, offering future opportunities but also highlighting ongoing challenges.


<details>
  <summary>Details</summary>
Motivation: Growing software complexity and demands require improved development methods. Current techniques, including model-driven engineering (MDE), help improve quality and productivity but face challenges as models become more complex. At the same time, modern AI-based coding ("vibe coding") has usability and reliability drawbacks.

Method: The paper introduces 'vibe modeling', a new paradigm combining the strengths of AI-driven coding (with LLMs) and traditional model-driven engineering, aiming to mitigate the limitations of each approach.

Result: The paper outlines core concepts of 'vibe modeling' and discusses both its potential benefits and the challenges associated with integrating AI and MDE for robust software development.

Conclusion: Vibe modeling has the potential to accelerate the development of reliable, complex systems by fusing AI and MDE methodologies, but presents new research opportunities and open challenges for the modeling community.

Abstract: There is a pressing need for better development methods and tools to keep up
with the growing demand and increasing complexity of new software systems. New
types of user interfaces, the need for intelligent components, sustainability
concerns, ... bring new challenges that we need to handle. In the last years,
model-driven engineering (MDE) has been key to improving the quality and
productivity of software development, but models themselves are becoming
increasingly complex to specify and manage. At the same time, we are witnessing
the growing popularity of vibe coding approaches that rely on Large Language
Models (LLMs) to transform natural language descriptions into running code at
the expenses of code vulnerabilities, scalability issues and maintainability
concerns. In this paper, we introduce the concept of \textit{vibe modeling} as
a novel approach to integrate the best of both worlds (AI and MDE) to speed up
the development of reliable complex systems. We outline the key concepts of
vibe modeling and highlight the opportunities and open challenges it presents
for the future of modeling.

</details>


### [4] [Extension Decisions in Open Source Software Ecosystem](https://arxiv.org/abs/2507.23168)
*Elmira Onagh,Maleknaz Nayebi*

Main category: cs.SE

TL;DR: Most new CI tools in GitHub Marketplace copy existing ones quickly, with a few original tools dominating the ecosystem. This research offers data-driven guidance to avoid redundancy and spot innovation opportunities.


<details>
  <summary>Details</summary>
Motivation: GitHub Marketplace, especially in the Continuous Integration (CI) segment, is experiencing rapid growth, but much of this growth involves tools with overlapping functionality. Understanding the patterns of redundancy and innovation can help developers and maintainers make informed decisions.

Method: The study links 6,983 CI Actions to 3,869 providers, analyzing their version histories. A graph model is constructed to timestamp each functionality's introduction, monitor adoption, and identify clusters of redundant tools.

Result: The analysis shows that about 65% of new CI Actions duplicate existing functionalities, usually within six months. Additionally, a few early Actions lead most subsequent forks and extensions.

Conclusion: The findings provide actionable insights for developers and maintainers to optimize launch timing, focus on unmet needs, and reduce redundancy. The published dataset and models aid further research and strategic decision-making in software ecosystems.

Abstract: GitHub Marketplace is expanding by approximately 41% annually, with new
tools; however, many additions replicate existing functionality. We study this
phenomenon in the platform's largest segment, Continuous Integration (CI), by
linking 6,983 CI Actions to 3,869 providers and mining their version histories.
Our graph model timestamps every functionality's debut, tracks its adoption,
and clusters redundant tools. We find that approximately 65% of new CI Actions
replicate existing capabilities, typically within six months, and that a small
set of first-mover Actions accounts for most subsequent forks and extensions.
These insights enable developers to choose the optimal moment to launch, target
unmet functionality, and help maintainers eliminate redundant tools. We publish
the complete graph and dataset to encourage longitudinal research on innovation
and competition in software ecosystems, and to provide practitioners with a
data-driven roadmap for identifying emerging trends and guiding product
strategy.

</details>


### [5] [AutoBridge: Automating Smart Device Integration with Centralized Platform](https://arxiv.org/abs/2507.23178)
*Siyuan Liu,Zhice Yang,Huangxun Chen*

Main category: cs.SE

TL;DR: AutoBridge is an automated IoT integration framework that drastically reduces human effort and expertise required for device onboarding. It delivers high accuracy and full function coverage across diverse platforms, outperforming expert programmers and current code-generation tools.


<details>
  <summary>Details</summary>
Motivation: Integrating new IoT devices into centralized platforms requires complex and labor-intensive programming, demanding significant human expertise. Automating this integration process can make IoT system adoption and maintenance much easier.

Method: The authors propose AutoBridge, an automated framework for IoT integration code generation. AutoBridge uses a divide-and-conquer approach: it progressively gathers device-specific information to generate control logic, then synthesizes integration code compatible with different IoT platforms. AutoBridge employs a multi-stage debugging process, including automated virtual testing and an interactive, low-effort hardware-in-the-loop debugger that relies on simple yes/no feedback from users for real-device validation.

Result: AutoBridge was evaluated using 34 IoT devices across two open-source platforms, achieving an average success rate of 93.87% and function coverage of 94.87% without human intervention. With minimal binary user feedback, it achieved 100% function coverage. In a user study with 15 participants, AutoBridge's generated code was 50%–80% more accurate than code written by expert programmers, even when they used commercial code LLMs.

Conclusion: AutoBridge significantly automates and improves the integration of new IoT devices into centralized IoT platforms, reducing human effort and boosting code accuracy and coverage.

Abstract: Multimodal IoT systems coordinate diverse IoT devices to deliver
human-centered services. The ability to incorporate new IoT devices under the
management of a centralized platform is an essential requirement. However, it
requires significant human expertise and effort to program the complex IoT
integration code that enables the platform to understand and control the device
functions. Therefore, we propose AutoBridge to automate IoT integration code
generation. Specifically, AutoBridge adopts a divide-and-conquer strategy: it
first generates device control logic by progressively retrieving
device-specific knowledge, then synthesizes platformcompliant integration code
using platform-specific knowledge. To ensure correctness, AutoBridge features a
multi-stage debugging pipeline, including an automated debugger for virtual IoT
device testing and an interactive hardware-in-the-loop debugger that requires
only binary user feedback (yes and no) for real-device verification. We
evaluate AutoBridge on a benchmark of 34 IoT devices across two open-source IoT
platforms. The results demonstrate that AutoBridge can achieves an average
success rate of 93.87% and an average function coverage of 94.87%, without any
human involvement. With minimal binary yes and no feedback from users, the code
is then revised to reach 100% function coverage. A user study with 15
participants further shows that AutoBridge outperforms expert programmers by
50% to 80% in code accuracy, even when the programmers are allowed to use
commercial code LLMs.

</details>


### [6] [XABPs: Towards eXplainable Autonomous Business Processes](https://arxiv.org/abs/2507.23269)
*Peter Fettke,Fabiana Fournier,Lior Limonad,Andreas Metzger,Stefanie Rinderle-Ma,Barbara Weber*

Main category: cs.SE

TL;DR: The paper argues that while autonomous business processes powered by AI/ML offer major efficiency gains, they also introduce challenges like trust and accountability. The authors propose a systematic framework for making such processes explainable (XABPs), detail various forms and structures of explainability, and highlight key research challenges for integration into business process management.


<details>
  <summary>Details</summary>
Motivation: Autonomous business processes (ABPs) promise significant improvements in efficiency and cost, but present challenges regarding stakeholder trust, accountability, explainability, and regulatory compliance due to their reliance on AI/ML.

Method: The paper proposes a systematic approach to 'eXplainable ABPs' (XABPs), which involves characterizing different forms of explainability, structuring frameworks for explainability, and identifying key research challenges in business process management (BPM) to achieve XABPs.

Result: Key forms and structures of explainability within autonomous business processes are outlined, along with a set of BPM research challenges that must be addressed for the realization of XABPs.

Conclusion: Explainable ABPs are essential for building trust, accountability, and regulatory compliance in autonomous workflows, and the paper's approach lays a research roadmap to integrate explainability into ABPs.

Abstract: Autonomous business processes (ABPs), i.e., self-executing workflows
leveraging AI/ML, have the potential to improve operational efficiency, reduce
errors, lower costs, improve response times, and free human workers for more
strategic and creative work. However, ABPs may raise specific concerns
including decreased stakeholder trust, difficulties in debugging, hindered
accountability, risk of bias, and issues with regulatory compliance. We argue
for eXplainable ABPs (XABPs) to address these concerns by enabling systems to
articulate their rationale. The paper outlines a systematic approach to XABPs,
characterizing their forms, structuring explainability, and identifying key BPM
research challenges towards XABPs.

</details>


### [7] [SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution](https://arxiv.org/abs/2507.23348)
*Han Li,Yuling Shi,Shaoxin Lin,Xiaodong Gu,Heng Lian,Xin Wang,Yantao Jia,Tao Huang,Qianxiang Wang*

Main category: cs.SE

TL;DR: SWE-Debate is a new multi-agent debate system for fixing code issues. By enabling agent competition and collaboration, it identifies and resolves complex software bugs better than previous methods, achieving state-of-the-art performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing agent-based issue resolution methods in software engineering mainly rely on independent agent explorations. These approaches struggle with local optima, missing broader issue patterns across the entire codebase, which limits their effectiveness in resolving complex issues.

Method: The authors propose SWE-Debate: a competitive multi-agent framework. SWE-Debate first generates multiple fault propagation traces through code dependency graph traversal, serving as localization proposals. Then, it conducts a structured, three-round debate among specialized agents representing different reasoning approaches along the traces. The winning, consensus-based fix plan is then fed into a Monte Carlo Tree Search (MCTS)-based agent to generate concrete code patches.

Result: On the SWE-bench benchmark, SWE-Debate achieved new state-of-the-art results among open-source agent frameworks and significantly outperformed existing baseline methods.

Conclusion: SWE-Debate’s multi-agent debate framework consolidates diverse reasoning paths, enabling more robust and accurate issue localization and resolution, thus advancing the capabilities of LLM-powered autonomous agents in software engineering.

Abstract: Issue resolution has made remarkable progress thanks to the advanced
reasoning capabilities of large language models (LLMs). Recently, agent-based
frameworks such as SWE-agent have further advanced this progress by enabling
autonomous, tool-using agents to tackle complex software engineering tasks.
While existing agent-based issue resolution approaches are primarily based on
agents' independent explorations, they often get stuck in local solutions and
fail to identify issue patterns that span across different parts of the
codebase. To address this limitation, we propose SWE-Debate, a competitive
multi-agent debate framework that encourages diverse reasoning paths and
achieves more consolidated issue localization. SWE-Debate first creates
multiple fault propagation traces as localization proposals by traversing a
code dependency graph. Then, it organizes a three-round debate among
specialized agents, each embodying distinct reasoning perspectives along the
fault propagation trace. This structured competition enables agents to
collaboratively converge on a consolidated fix plan. Finally, this consolidated
fix plan is integrated into an MCTS-based code modification agent for patch
generation. Experiments on the SWE-bench benchmark show that SWE-Debate
achieves new state-of-the-art results in open-source agent frameworks and
outperforms baselines by a large margin.

</details>


### [8] [Quality Evaluation of COBOL to Java Code Transformation](https://arxiv.org/abs/2507.23356)
*Shmulik Froimovich,Raviv Gal,Wesam Ibraheem,Avi Ziv*

Main category: cs.SE

TL;DR: The paper introduces an automated system that combines analytical and LLM-based techniques to efficiently evaluate COBOL-to-Java translation quality in IBM's WCA4Z, reducing manual effort and providing valuable feedback to improve code modernization.


<details>
  <summary>Details</summary>
Motivation: Evaluating the quality of COBOL-to-Java code translation is challenging due to the opacity of large language models (LLMs) and the complexity involved in assessing translation accuracy. Manual review is time-consuming and does not scale well, creating a need for automated, reliable, and scalable evaluation systems.

Method: The authors developed an automated evaluation system that integrates analytic checkers with LLM-as-a-judge (LaaJ) techniques. This combination allows for comprehensive, multi-faceted evaluation of translation quality within IBM's watsonx Code Assistant for Z (WCA4Z) environment. The system is designed to support continuous integration, large-scale benchmarking, and automated reporting.

Result: The system enables scalable, automated evaluation of code translations, supports large-scale benchmarking, and integrates well with CI workflows. It reduces the need for manual review and provides actionable insights to developers and project managers for improving code quality.

Conclusion: The proposed system effectively addresses key barriers in evaluating LLM-based code translators by automating the assessment process, delivering multi-dimensional insights, and fostering the modernization and quality evolution of codebases.

Abstract: We present an automated evaluation system for assessing COBOL-to-Java code
translation within IBM's watsonx Code Assistant for Z (WCA4Z). The system
addresses key challenges in evaluating LLM-based translators, including model
opacity and the complexity of translation quality assessment. Our approach
combines analytic checkers with LLM-as-a-judge (LaaJ) techniques to deliver
scalable, multi-faceted evaluations. The system supports continuous integration
workflows, enables large-scale benchmarking, and reduces reliance on manual
review. We describe the system architecture, evaluation strategies, and
reporting mechanisms that provide actionable insights for developers and
project managers, facilitating the evolution of high-quality, modernized
codebases.

</details>


### [9] [SWE-Exp: Experience-Driven Software Issue Resolution](https://arxiv.org/abs/2507.23361)
*Silin Chen,Shaoxin Lin,Xiaodong Gu,Yuling Shi,Heng Lian,Longfei Yun,Dong Chen,Weiguo Sun,Lin Cao,Qianxiang Wang*

Main category: cs.SE

TL;DR: SWE-Exp enables software repair agents to continuously learn from past successes and failures, resulting in higher resolution rates and a shift from trial-and-error to experience-driven issue resolution.


<details>
  <summary>Details</summary>
Motivation: Current large language model (LLM) agents for software issue resolution treat each problem separately and do not retain or reuse knowledge from previous experiences. This results in inefficient, redundant exploration and missed opportunities to apply past successful strategies.

Method: The authors propose SWE-Exp, an experience-enhanced approach. It builds a multi-faceted experience bank that distills actionable knowledge from both successful and failed repair attempts across various levels, from high-level understanding to specific code edits. This enables agents to learn continuously from past experiences and apply this knowledge to new problems.

Result: Experiments indicate SWE-Exp achieves a new state-of-the-art resolution rate of 41.6% Pass@1 on the SWE-bench-Verified benchmark using open-source agent frameworks.

Conclusion: SWE-Exp demonstrates that by systematically accumulating and leveraging repair expertise, LLM-based agents can strategically resolve issues rather than relying solely on trial-and-error. This establishes a new experience-driven paradigm for automated software engineering agents.

Abstract: Recent advances in large language model (LLM) agents have shown remarkable
progress in software issue resolution, leveraging advanced techniques such as
multi-agent collaboration and Monte Carlo Tree Search (MCTS). However, current
agents act as memoryless explorers - treating each problem separately without
retaining or reusing knowledge from previous repair experiences. This leads to
redundant exploration of failed trajectories and missed chances to adapt
successful issue resolution methods to similar problems. To address this
problem, we introduce SWE-Exp, an experience - enhanced approach that distills
concise and actionable experience from prior agent trajectories, enabling
continuous learning across issues. Our method introduces a multi-faceted
experience bank that captures both successful and failed repair attempts.
Specifically, it extracts reusable issue resolution knowledge at different
levels - from high-level problem comprehension to specific code changes.
Experiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%
Pass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach
establishes a new paradigm in which automated software engineering agents
systematically accumulate and leverage repair expertise, fundamentally shifting
from trial-and-error exploration to strategic, experience-driven issue
resolution.

</details>


### [10] [Trae Agent: An LLM-based Agent for Software Engineering with Test-time Scaling](https://arxiv.org/abs/2507.23370)
*Trae Research Team,Pengfei Gao,Zhao Tian,Xiangxin Meng,Xinchen Wang,Ruida Hu,Yuanan Xiao,Yizhou Liu,Zhao Zhang,Junjie Chen,Cuiyun Gao,Yun Lin,Yingfei Xiong,Chao Peng,Xia Liu*

Main category: cs.SE

TL;DR: Trae Agent is an agent-based ensemble reasoning approach for software issue resolution at the repository level. It addresses major shortcomings of previous methods, outperforming state-of-the-art baselines on SWE-bench and achieving a leading Pass@1 score. The tool is open-sourced for the community.


<details>
  <summary>Details</summary>
Motivation: Software issue resolution remains a significant challenge in software engineering. While large language models (LLMs) have shown promise, existing ensemble reasoning techniques for LLM-based issue resolution are limited by inefficient exploration of ensemble spaces and lack of repository-level comprehension. This paper aims to overcome these persistent limitations.

Method: The paper introduces 'Trae Agent,' an agent-based ensemble reasoning approach specifically designed for repository-level issue resolution. Trae Agent treats the problem as an optimal solution search and utilizes modular agents for the generation, pruning, and selection of solutions. Extensive comparative experiments are conducted using three popular LLMs on the SWE-bench benchmark, evaluating Trae Agent against four state-of-the-art ensemble reasoning methods.

Result: Experimental results show that Trae Agent outperforms all compared baseline techniques, achieving an average of 10.22% higher Pass@1 score. It secured first place on the SWE-bench Verified leaderboard with a Pass@1 score of 75.20%.

Conclusion: Trae Agent demonstrates the effectiveness of an agent-based modular approach in addressing the limitations of current prompting-based ensemble methods for LLM-based software issue resolution. The method achieves state-of-the-art results and has been released as open source for community use and further research.

Abstract: Software issue resolution is a critical challenge in software engineering and
has garnered increasing attention in recent years. With the rapid advancement
of large language models (LLMs), substantial progress has been made in
addressing real-world software engineering tasks. Recent studies have
introduced ensemble reasoning techniques to enhance the performance of
LLM-based issue resolution. However, existing prompting-based methods still
face limitations in effectively exploring large ensemble spaces and lack the
capacity for repository-level understanding, both of which constrain their
overall effectiveness. In this paper, we propose Trae Agent, the first
agent-based ensemble reasoning approach for repository-level issue resolution.
Trae Agent formulates our goal as an optimal solution search problem and
addresses two key challenges, i.e., large ensemble spaces and repository-level
understanding, through modular agents for generation, pruning, and selection.
We conduct extensive experiments using three leading LLMs on the widely-adopted
SWE-bench benchmark, comparing Trae Agent against four state-of-the-art
ensemble reasoning techniques. Experimental results demonstrate that Trae Agent
consistently achieves superior performance, with an average improvement of
10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first
place on the SWE-bench Verified leaderboard, with a notable Pass@1 score of
75.20%. We are pleased to release Trae Agent as an open-source project to
support the research community, with all resources available at
https://github.com/bytedance/trae-agent.

</details>


### [11] [Dynamic and Static Analysis of Python Software with Kieker Including Reconstructed Architectures](https://arxiv.org/abs/2507.23425)
*Daphné Larrivain,Shinhyung Yang,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: The paper extends the Kieker observability framework, originally for Java, to support Python. By combining static and dynamic analysis, it allows users to build custom observability pipelines and gain structural insights into Python applications.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the increasing popularity of Python as a programming language and the resulting need for structural insights into Python applications. The Kieker observability framework, traditionally focused on Java, lacks native support for Python, making such support desirable.

Method: The method involves developing a Python analysis pipeline which integrates both static and dynamic analysis to provide comprehensive observability of Python applications within the Kieker framework.

Result: The framework now enables users to design custom observability pipelines for Python applications, offering both static and dynamic insights into their structure and operation.

Conclusion: Supporting Python in the Kieker observability framework expands its applicability and utility, allowing developers to gain valuable insights into Python systems similar to what has previously been possible for Java applications.

Abstract: The Kieker observability framework is a tool that provides users with the
means to design a custom observability pipeline for their application.
Originally tailored for Java, supporting Python with Kieker is worthwhile.
Python's popularity has exploded over the years, thus making structural
insights of Python applications highly valuable. Our Python analysis pipeline
combines static and dynamic analysis in order to build a complete picture of a
given system.

</details>


### [12] [An Empirical Study on the Amount of Changes Required for Merge Request Acceptance](https://arxiv.org/abs/2507.23640)
*Samah Kansab,Mohammed Sayagh,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: This paper shows that substantial code changes are often needed after code review in GitLab MRs, and that these changes can be predicted with high accuracy using machine learning models that consider code, developer, and historical project features.


<details>
  <summary>Details</summary>
Motivation: Code review is a crucial but labor-intensive part of software development, yet little is known about how much effort measured in code changes is required after review, especially for GitLab Merge Requests. The motivation is to better understand, quantify, and predict this effort to streamline the code review process.

Method: The authors quantified code review effort as the amount of code modified after submission, analyzing a dataset of 23,600+ GitLab Merge Requests from four projects. They examined the relationship between CR effort and factors like review time and participants. They trained an interpretable machine learning model using metrics (text features, code complexity, developer experience, review history, and branching) to predict review effort.

Result: Up to 71% of MRs require post-submission adjustments; 28% of these require large changes (>200 lines). CR effort is not correlated with review time or number of participants. The machine learning model predicted review effort with strong accuracy (AUC 0.84-0.88), with complexity, developer experience, and text features as the strongest predictors. Project history also impacts effort.

Conclusion: Significant post-submission code modifications are frequent in code review. Predicting review effort with interpretable machine learning is feasible and can highlight important contributing factors, enabling teams to anticipate and manage integration effort more effectively.

Abstract: Code review (CR) is essential to software development, helping ensure that
new code is properly integrated. However, the CR process often involves
significant effort, including code adjustments, responses to reviewers, and
continued implementation. While past studies have examined CR delays and
iteration counts, few have investigated the effort based on the volume of code
changes required, especially in the context of GitLab Merge Requests (MRs),
which remains underexplored. In this paper, we define and measure CR effort as
the amount of code modified after submission, using a dataset of over 23,600
MRs from four GitLab projects. We find that up to 71% of MRs require
adjustments after submission, and 28% of these involve changes to more than 200
lines of code. Surprisingly, this effort is not correlated with review time or
the number of participants. To better understand and predict CR effort, we
train an interpretable machine learning model using metrics across multiple
dimensions: text features, code complexity, developer experience, review
history, and branching. Our model achieves strong performance (AUC 0.84-0.88)
and reveals that complexity, experience, and text features are key predictors.
Historical project characteristics also influence current review effort. Our
findings highlight the feasibility of using machine learning to explain and
anticipate the effort needed to integrate code changes during review.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [13] [Abstractions of Sequences, Functions and Operators](https://arxiv.org/abs/2507.23151)
*Louis Rustenholz,Pedro Lopez-Garcia,Manuel V. Hermenegildo*

Main category: cs.PL

TL;DR: The paper proposes B-bound abstract domains and a domain abstraction functor for analyzing recursively defined functions, enabling the inference of complex numerical invariants and simplifying transfer function creation, with applicability in program and hybrid systems analysis.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the difficulty of inferring closed-form bounds on recursively defined functions, especially as fixed points of operators or as solutions to functional equations. This challenge has important applications in areas such as program analysis, cost analysis, loop acceleration, declarative language analysis, and the analysis of hybrid systems with differential equations.

Method: The authors propose a new family of constraint-based abstract domains—B-bound domains—that abstract a function by conjoining bounds from a specified set of boundary functions. They identify a convexity property in the constraint space, which can simplify and sometimes automate transfer function design. Additionally, they introduce 'domain abstraction,' a functor that lifts arbitrary mappings in value space to Galois connections in function space, thereby enabling abstraction from symbolic to numerical functions and dimensionality reduction. The framework is built using a simple operator language covering various function types.

Result: The B-bound domains allow for the inference of highly non-linear numerical invariants that are often out of reach for classical numerical abstract domains. The convexity property discovered can greatly facilitate transfer function design. The domain abstraction functor broadens the abstraction approaches available, making dimensionality reduction and abstraction from symbolic to numerical functions possible in a principled way.

Conclusion: The paper advances both theoretical foundations and practical methods for higher-order abstract interpretation via new abstract domains and abstraction functors, enabling effective analysis of complex, recursively defined functions in several practical domains.

Abstract: We present theoretical and practical results on the order theory of lattices
of functions, focusing on Galois connections that abstract (sets of) functions
- a topic known as higher-order abstract interpretation.
  We are motivated by the challenge of inferring closed-form bounds on
functions which are defined recursively, i.e. as the fixed point of an operator
or, equivalently, as the solution to a functional equation. This has multiple
applications in program analysis (e.g. cost analysis, loop acceleration,
declarative language analysis) and in hybrid systems governed by differential
equations.
  Our main contribution is a new family of constraint-based abstract domains
for abstracting numerical functions, B-bound domains, which abstract a function
f by a conjunction of bounds from a preselected set of boundary functions. They
allow inferring highly non-linear numerical invariants, which classical
numerical abstract domains struggle with. We uncover a convexity property in
the constraint space that simplifies, and, in some cases, fully automates,
transfer function design.
  We also introduce domain abstraction, a functor that lifts arbitrary mappings
in value space to Galois connections in function space. This supports
abstraction from symbolic to numerical functions (i.e. size abstraction), and
enables dimensionality reduction of equations.
  We base our constructions of transfer functions on a simple operator
language, starting with sequences, and extending to more general functions,
including multivariate, piecewise, and non-discrete domains.

</details>


### [14] [Kernel-FFI: Transparent Foreign Function Interfaces for Interactive Notebooks](https://arxiv.org/abs/2507.23205)
*Hebi Li,Forrest Sheng Bao,Qi Xiao,Jin Tian*

Main category: cs.PL

TL;DR: Kernel-FFI is a new open-source framework enabling easy, seamless cross-language function calls and object manipulation in Jupyter notebooks, eliminating manual binding and supporting advanced features like recursion and OOP.


<details>
  <summary>Details</summary>
Motivation: Existing FFI solutions are cumbersome for the dynamic, interactive workflows found in notebook environments like Jupyter. They typically require manual setup, are verbose, do not support recursive calls, and lack full object-oriented programming (OOP) functionality across languages.

Method: The authors introduce Kernel-FFI, a language-agnostic framework that utilizes source-level transformation to automatically rewrite cross-language invocations. This avoids the need for manual bindings and boilerplate code, and supports OOP by referencing foreign objects and managing resources automatically. They also develop a novel side-channel communication mechanism to support recursive and asynchronous calls within blocking Jupyter kernels.

Result: Kernel-FFI enables seamless, transparent cross-language function calls and object manipulation in interactive notebooks, providing full OOP support and removing the traditional manual and boilerplate barriers. The framework is open-sourced for public use.

Conclusion: Kernel-FFI makes multi-language development more efficient and user-friendly in notebook environments, supporting advanced features like recursive calls and OOP constructs with minimal manual intervention.

Abstract: Foreign Function Interfaces (FFIs) are essential for enabling
interoperability between programming languages, yet existing FFI solutions are
ill-suited for the dynamic, interactive workflows prevalent in modern notebook
environments such as Jupyter. Current approaches require extensive manual
configuration, introduce significant boilerplate, and often lack support for
recursive calls and object-oriented programming (OOP) constructs-features
critical for productive, multi-language development.
  We present Kernel-FFI, a transparent, language-agnostic framework that
enables seamless cross-language function calls and object manipulation within
interactive notebooks. Kernel-FFI employs source-level transformation to
automatically rewrite cross-language invocations, eliminating the need for
manual bindings or boilerplate. Kernel-FFI provides robust support for OOP by
enabling foreign object referencing and automatic resource management across
language boundaries. Furthermore, to address the blocking nature of Jupyter
kernels and support recursive and asynchronous foreign calls, we introduce a
novel side-channel communication mechanism. Our tool will be open-sourced and
available at https://codepod.io/docs/kernel-ffi

</details>
