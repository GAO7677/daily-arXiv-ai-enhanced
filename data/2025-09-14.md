<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Pattern-Based File and Data Access with Python Glob: A Comprehensive Guide for Computational Research](https://arxiv.org/abs/2509.08843)
*Sidney Shapiro*

Main category: cs.SE

TL;DR: This paper positions Python's glob module as a key tool for scalable and reproducible pattern-based file access in data-centric research. Through practical examples and integration with major analytical libraries, the authors present glob as an essential methodological reference for Python workflows in data science, analytics, and AI applications.


<details>
  <summary>Details</summary>
Motivation: Pattern-based file access is essential in computational research but is often under-documented. There is a need for a concise, practical reference for leveraging file pattern matching in Python workflows.

Method: The paper demonstrates the use of Python's glob module through concrete examples integrated with popular analytical libraries such as pandas, scikit-learn, and matplotlib, covering use cases in data science, business analytics, and AI.

Result: The glob module is shown to efficiently manage file traversal and integration in analytical pipelines, supporting reproducible and scalable research workflows.

Conclusion: Glob is established as a versatile and foundational tool for pattern-based file access in Python-powered research, recommended as a default reference for file matching methodology.

Abstract: Pattern-based file access is a fundamental but often under-documented aspect
of computational research. The Python glob module provides a simple yet
powerful way to search, filter, and ingest files using wildcard patterns,
enabling scalable workflows across disciplines. This paper introduces glob as a
versatile tool for data science, business analytics, and artificial
intelligence applications. We demonstrate use cases including large-scale data
ingestion, organizational data analysis, AI dataset construction, and
reproducible research practices. Through concrete Python examples with widely
used libraries such as pandas,scikit-learn, and matplotlib, we show how glob
facilitates efficient file traversal and integration with analytical pipelines.
By situating glob within the broader context of reproducible research and data
engineering, we highlight its role as a methodological building block. Our goal
is to provide researchers and practitioners with a concise reference that
bridges foundational concepts and applied practice, making glob a default
citation for file pattern matching in Python-based research workflows.

</details>


### [2] [A Systematic Mapping Study on Chatbots in Programming Education](https://arxiv.org/abs/2509.08857)
*Marcelino Garcia,Renato Garcia,Arthur Parizotto,Andre Mendes,Pedro Valle,Ricardo Vilela,Renato Balancieri,Williamson Silva*

Main category: cs.SE

TL;DR: This paper systematically reviews 54 studies on educational chatbots for programming, finding most focus on Python and basic concepts with diverse approaches, and highlights opportunities for future tool development.


<details>
  <summary>Details</summary>
Motivation: Educational chatbots are increasingly used to support teaching programming, especially for beginners, but there is limited understanding of how these tools are developed and applied.

Method: The authors conducted a Systematic Mapping Study (SMS) by reviewing 3,216 publications and selecting 54 relevant studies. These were analyzed based on chatbot types, programming languages, educational content, interaction models, and application contexts.

Result: Most chatbots were designed for Python education, focusing on basic programming concepts. There is a diversity of pedagogical methods and technological architectures in these tools.

Conclusion: The study identifies prevalent practices and gaps in the field, offering insights to guide the creation of future educational programming chatbots.

Abstract: Educational chatbots have gained prominence as support tools for teaching
programming, particularly in introductory learning contexts. This paper
presents a Systematic Mapping Study (SMS) that investigated how such agents
have been developed and applied in programming education. From an initial set
of 3,216 publications, 54 studies were selected and analyzed based on five
research subquestions, addressing chatbot types, programming languages used,
educational content covered, interaction models, and application contexts. The
results reveal a predominance of chatbots designed for Python instruction,
focusing on fundamental programming concepts, and employing a wide variety of
pedagogical approaches and technological architectures. In addition to
identifying trends and gaps in the literature, this study provides insights to
inform the development of new educational tools for programming instruction.

</details>


### [3] [GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial Analysis-Function Calling vs Code Generation](https://arxiv.org/abs/2509.08863)
*Qianqian Luo,Liuchang Xu,Qingming Lin,Sensen Wu,Ruichen Mao,Chao Wang,Hailin Feng,Bo Huang,Zhenhong Du*

Main category: cs.SE

TL;DR: GeoJSON Agents: A multi-agent LLM architecture bridges the GIS expertise gap by transforming language tasks into GeoJSON operations using Function Calling and Code Generation. Tested with 70 tasks, results show Code Generation excels in flexibility and accuracy, while Function Calling is more stable. Both beat general models, paving the way for enhanced GeoAI automation.


<details>
  <summary>Details</summary>
Motivation: LLMs have advanced task automation and language understanding, but lack expertise in Geographic Information Systems (GIS), restricting their ability to process spatial data tasks. This paper addresses the need to improve LLM performance in GIS automation.

Method: The authors propose GeoJSON Agents, a multi-agent LLM framework that interprets natural language tasks into structured GeoJSON operation commands. The architecture leverages two LLM enhancement methods: Function Calling and Code Generation. Three components (task parsing via a Planner, agent collaboration with specialized Workers, and results integration) process spatial data. A benchmark of 70 diverse tasks was used to evaluate two approaches (Function Calling and Code Generation), employing GPT-4o.

Result: Function Calling-based GeoJSON Agent achieved 85.71% accuracy; Code Generation-based agent achieved 97.14%. Both outperform a general-purpose model (48.57%). Code Generation offers greater flexibility, while Function Calling provides more stability. System outputs are reusable, standards-compliant GeoJSON files.

Conclusion: This study introduces a novel multi-agent LLM framework for GeoJSON data, systematically comparing two LLM enhancement methods. It highlights new ways to improve GeoAI system performance, revealing respective strengths in flexibility and stability for Code Generation and Function Calling approaches.

Abstract: LLMs have made substantial progress in task automation and natural language
understanding.However,without expertise in GIS,they continue to encounter
limitations.To address these issues, we propose GeoJSON Agents-a multi-agent
LLM architecture.This framework transforms natural language tasks into
structured GeoJSON operation commands and processes spatial data using two
widely adopted LLM enhancement techniques:Function Calling and Code
Generation.The architecture consists of three components-task parsing,agent
collaboration,and result integration-aimed at enhancing both the performance
and scalability of GIS automation.The Planner agent interprets natural language
tasks into structured GeoJSON commands.Then,specialized Worker agents
collaborate according to assigned roles to perform spatial data processing and
analysis,either by invoking predefined function APIs or by dynamically
generating and executing Python-based spatial analysis code.Finally,the system
integrates the outputs from multiple execution rounds into
reusable,standards-compliant GeoJSON files.To systematically evaluate the
performance of the two approaches,we constructed a benchmark dataset of 70
tasks with varying complexity and conducted experiments using OpenAI's GPT-4o
as the core model.Results indicate that the Function Calling-based GeoJSON
Agent achieved an accuracy of 85.71%,while the Code Generation-based agent
reached 97.14%,both significantly outperforming the best-performing
general-purpose model (48.57%).Further analysis reveals that the Code
Generation provides greater flexibility,whereas the Function Calling approach
offers more stable execution.This study is the first to introduce an LLM
multi-agent framework for GeoJSON data and to compare the strengths and
limitations of two mainstream LLM enhancement methods,offering new perspectives
for improving GeoAI system performance.

</details>


### [4] [TraceRAG: A LLM-Based Framework for Explainable Android Malware Detection and Behavior Analysis](https://arxiv.org/abs/2509.08865)
*Guangyu Zhang,Xixuan Wang,Shiyu Sun,Peiyan Xiao,Kun Sun,Yanhai Xiong*

Main category: cs.SE

TL;DR: TraceRAG uses large language models to summarize and index code snippets for Android malware analysis, enabling accurate detection and explainable, human-readable reports. It delivers high detection accuracy (96%) and useful behavioral insights, outperforming traditional tools and assisting security experts.


<details>
  <summary>Details</summary>
Motivation: Malicious Android applications often use sophisticated evasion tactics and hide their harmful logic within legitimate functions, making it difficult for existing analysis techniques to detect and explain these threats effectively.

Method: The paper introduces TraceRAG, a retrieval-augmented generation (RAG) framework that uses large language models to summarize Java code snippets at the method level. These summaries are indexed in a vector database. When a natural language query about app behavior is made, TraceRAG retrieves relevant code snippets for deeper inspection and produces human-readable reports highlighting malicious behaviors and their code sources.

Result: TraceRAG achieves 96% malware detection accuracy and 83.81% behavior identification accuracy according to VirusTotal scans and manual verification. Expert evaluation confirms the practical usefulness of the generated reports.

Conclusion: TraceRAG effectively bridges the gap between natural language queries and code analysis, enabling explainable and accurate malware detection that provides actionable insights for security analysts.

Abstract: Sophisticated evasion tactics in malicious Android applications, combined
with their intricate behavioral semantics, enable attackers to conceal
malicious logic within legitimate functions, underscoring the critical need for
robust and in-depth analysis frameworks. However, traditional analysis
techniques often fail to recover deeply hidden behaviors or provide
human-readable justifications for their decisions. Inspired by advances in
large language models (LLMs), we introduce TraceRAG, a retrieval-augmented
generation (RAG) framework that bridges natural language queries and Java code
to deliver explainable malware detection and analysis. First, TraceRAG
generates summaries of method-level code snippets, which are indexed in a
vector database. At query time, behavior-focused questions retrieve the most
semantically relevant snippets for deeper inspection. Finally, based on the
multi-turn analysis results, TraceRAG produces human-readable reports that
present the identified malicious behaviors and their corresponding code
implementations. Experimental results demonstrate that our method achieves 96\%
malware detection accuracy and 83.81\% behavior identification accuracy based
on updated VirusTotal (VT) scans and manual verification. Furthermore, expert
evaluation confirms the practical utility of the reports generated by TraceRAG.

</details>


### [5] [Benchmarking Energy Efficiency of Large Language Models Using vLLM](https://arxiv.org/abs/2509.08867)
*K. Pronk,Q. Zhao*

Main category: cs.SE

TL;DR: The paper presents a new benchmark for measuring the energy efficiency of LLMs under realistic conditions, offering valuable guidance for developers focused on sustainability.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are consuming significant energy, contributing to climate impact. Existing benchmarks for LLM energy efficiency do not accurately reflect real-world scenarios, so developers lack reliable guidance for building sustainable AI systems.

Method: The authors introduce the LLM Efficiency Benchmark, which is designed to simulate real-world usage conditions. This benchmark uses vLLM, a production-ready LLM serving backend, to test energy efficiency under various factors such as model size, architecture, and concurrent requests.

Result: The study shows that it is feasible to create energy efficiency benchmarks reflecting actual deployment conditions. The results deliver insights into how different variables impact the energy efficiency of LLMs during inference.

Conclusion: The benchmark provides an effective means for developers to evaluate and improve the energy efficiency of LLMs in practical settings, supporting the creation of more sustainable AI products.

Abstract: The prevalence of Large Language Models (LLMs) is having an growing impact on
the climate due to the substantial energy required for their deployment and
use. To create awareness for developers who are implementing LLMs in their
products, there is a strong need to collect more information about the energy
efficiency of LLMs. While existing research has evaluated the energy efficiency
of various models, these benchmarks often fall short of representing realistic
production scenarios. In this paper, we introduce the LLM Efficiency Benchmark,
designed to simulate real-world usage conditions. Our benchmark utilizes vLLM,
a high-throughput, production-ready LLM serving backend that optimizes model
performance and efficiency. We examine how factors such as model size,
architecture, and concurrent request volume affect inference energy efficiency.
Our findings demonstrate that it is possible to create energy efficiency
benchmarks that better reflect practical deployment conditions, providing
valuable insights for developers aiming to build more sustainable AI systems.

</details>


### [6] [CLARA: A Developer's Companion for Code Comprehension and Analysis](https://arxiv.org/abs/2509.09072)
*Ahmed Adnan,Mushfiqur Rahman,Saad Sakib Noor,Kazi Sakib*

Main category: cs.SE

TL;DR: CLARA is a browser extension that uses modern inference models to help developers and researchers understand, refactor, and assess code quality in open-source projects, without requiring complex setup. Evaluations show the tool is accurate, useful, and practical.


<details>
  <summary>Details</summary>
Motivation: Developers and researchers often struggle with existing code analysis tools, which require cumbersome project setup, lack sufficient context-awareness, and demand considerable manual effort.

Method: CLARA, a browser extension, employs a state-of-the-art inference model to assist users in code comprehension, refactoring, and code quality detection. Its effectiveness was evaluated both qualitatively using existing datasets and methodologies and via a user study involving 10 developers and researchers.

Result: CLARA was found to be useful, accurate, and practical for code comprehension and analysis tasks, according to both evaluation methods.

Conclusion: CLARA offers an efficient and practical solution for open-source codebase comprehension and analysis, overcoming manual and setup barriers found in existing tools.

Abstract: Code comprehension and analysis of open-source project codebases is a task
frequently performed by developers and researchers. However, existing tools
that practitioners use for assistance with such tasks often require prior
project setup, lack context-awareness, and involve significant manual effort.
To address this, we present CLARA, a browser extension that utilizes a
state-of-the-art inference model to assist developers and researchers in: (i)
comprehending code files and code fragments, (ii) code refactoring, and (iii)
code quality attribute detection. We qualitatively evaluated CLARA's inference
model using existing datasets and methodology, and performed a comprehensive
user study with 10 developers and academic researchers to assess its usability
and usefulness. The results show that CLARA is useful, accurate, and practical
in code comprehension and analysis tasks. CLARA is an open-source tool
available at https://github.com/SaadNoor555/CLARA_tool_demo. A video showing
the full capabilities of CLARA can be found at
https://youtu.be/VDKVXvIH41Q?si=qBFsmS_Y4m_9x3YH.

</details>


### [7] [Probing Pre-trained Language Models on Code Changes: Insights from ReDef, a High-Confidence Just-in-Time Defect Prediction Dataset](https://arxiv.org/abs/2509.09192)
*Doha Nam,Taehyoun Kim,Duksan Ryu,Jongmoon Baik*

Main category: cs.SE

TL;DR: The paper introduces ReDef, a high-confidence defect prediction dataset, and shows that current language models don't truly understand code changes, relying instead on shallow cues from input encodings.


<details>
  <summary>Details</summary>
Motivation: Existing JIT software defect prediction datasets are plagued by noisy labels and poor precision, making it difficult to reliably identify bug-inducing code changes during code review and continuous integration.

Method: The authors create ReDef, a new high-confidence benchmark dataset for defect prediction by anchoring defective cases on revert commits and verifying clean cases with post-hoc history checks. Ambiguous cases are filtered using a GPT-assisted triage with multiple reviews. They then systematically evaluate several pre-trained language models (CodeBERT, CodeT5+, UniXcoder) using five encoding strategies and counterfactual perturbations to test model understanding.

Result: The ReDef dataset contains 3,164 defective and 10,268 clean modifications and demonstrates higher label reliability than prior datasets. Compact diff-style input encodings outperform whole-function formats for PLMs. However, PLMs show little performance drop under counterfactual perturbations, suggesting they rely on superficial cues rather than true semantic understanding of code changes.

Conclusion: Current pre-trained language models, unlike in snapshot-based tasks, have limited genuine comprehension of code modifications, largely depending on superficial cues in the input rather than deep semantic reasoning.

Abstract: Just-in-Time software defect prediction (JIT-SDP) plays a critical role in
prioritizing risky code changes during code review and continuous integration.
However, existing datasets often suffer from noisy labels and low precision in
identifying bug-inducing commits. To address this, we present ReDef
(Revert-based Defect dataset), a high-confidence benchmark of function-level
modifications curated from 22 large-scale C/C++ projects. Defective cases are
anchored by revert commits, while clean cases are validated through post-hoc
history checks. Ambiguous instances are conservatively filtered out via a
GPT-assisted triage process involving multiple votes and audits. This pipeline
yields 3,164 defective and 10,268 clean modifications, offering substantially
more reliable labels than prior existing resources. Beyond dataset
construction, we provide the first systematic evaluation of how pre-trained
language models (PLMs) reason about code modifications -- specifically, which
input encodings most effectively expose change information, and whether models
genuinely capture edit semantics. We fine-tune CodeBERT, CodeT5+, and UniXcoder
under five encoding strategies, and further probe their sensitivity through
counterfactual perturbations that swap added/deleted blocks, invert diff
polarity, or inject spurious markers. Our results show that compact diff-style
encodings consistently outperform whole-function formats across all PLMs, with
statistical tests confirming large, model-independent effects. However, under
counterfactual tests, performance degrades little or not at all -- revealing
that what appears to be robustness in fact reflects reliance on superficial
cues rather than true semantic understanding. These findings indicate that,
unlike in snapshot-based tasks, current PLMs remain limited in their ability to
genuinely comprehend code modifications.

</details>


### [8] [On Integrating Large Language Models and Scenario-Based Programming for Improving Software Reliability](https://arxiv.org/abs/2509.09194)
*Ayelet Berzack,Guy Katz*

Main category: cs.SE

TL;DR: The paper presents a method to integrate LLMs with scenario-based programming to improve software reliability. Through a case study on Connect4, they demonstrate improved agent performance and partial formal verification, showing promise for safer LLM deployment in development.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used in software development due to their productivity and creativity, but they frequently produce erroneous code confidently, posing reliability risks. There's a need for methods to harness LLM strengths while mitigating their weaknesses, especially in terms of error reduction and verifiability.

Method: The authors propose integrating LLMs with conventional software engineering techniques in a structured manner, specifically leveraging Scenario-Based Programming (SBP). SBP helps developers contribute expert knowledge and enables better inspection and verification of LLM-generated outputs.

Result: In a case study, the authors used their methodology to design and implement the Connect4 game. The resulting agent was able to defeat strong existing agents and, in some cases, its correctness could be formally verified. They also report positive experiences regarding the usability of their approach.

Conclusion: Combining LLMs with SBP can streamline software development, reduce errors, and enhance user confidence in critical program properties, paving the way for more reliable LLM use in software engineering. The authors will provide their code to further validate and disseminate their work.

Abstract: Large Language Models (LLMs) are fast becoming indispensable tools for
software developers, assisting or even partnering with them in crafting complex
programs. The advantages are evident -- LLMs can significantly reduce
development time, generate well-organized and comprehensible code, and
occasionally suggest innovative ideas that developers might not conceive on
their own. However, despite their strengths, LLMs will often introduce
significant errors and present incorrect code with persuasive confidence,
potentially misleading developers into accepting flawed solutions.
  In order to bring LLMs into the software development cycle in a more reliable
manner, we propose a methodology for combining them with ``traditional''
software engineering techniques in a structured way, with the goal of
streamlining the development process, reducing errors, and enabling users to
verify crucial program properties with increased confidence. Specifically, we
focus on the Scenario-Based Programming (SBP) paradigm -- an event-driven,
scenario-based approach for software engineering -- to allow human developers
to pour their expert knowledge into the LLM, as well as to inspect and verify
its outputs.
  To evaluate our methodology, we conducted a significant case study, and used
it to design and implement the Connect4 game. By combining LLMs and SBP we were
able to create a highly-capable agent, which could defeat various strong
existing agents. Further, in some cases, we were able to formally verify the
correctness of our agent. Finally, our experience reveals interesting insights
regarding the ease-of-use of our proposed approach. The full code of our
case-study will be made publicly available with the final version of this
paper.

</details>


### [9] [Altered Histories in Version Control System Repositories: Evidence from the Trenches](https://arxiv.org/abs/2509.09294)
*Solal Rapaport,Laurent Pautet,Samuel Tardieu,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: The paper studies how often and why developers rewrite history in public Git repositories. This affects reproducibility, project governance, and security, with secret removal and license changes being common. They introduce GitHistorian to detect such alterations.


<details>
  <summary>Details</summary>
Motivation: Version Control Systems (VCS) allow history rewriting, which is useful but risky when done on public branches with downstream users. Such alterations can disrupt workflows, undermine integrity and reproducibility, and open doors for supply chain attacks.

Method: The authors perform a large-scale empirical analysis on 111 million public Git repositories archived by Software Heritage to detect and categorize history alterations. They conduct two case studies investigating the nature and motivation behind these changes. Finally, they introduce GitHistorian, an automated tool for identifying and describing history alterations.

Result: The study discovers history alterations in 1.22 million repositories, amounting to 8.7 million rewritten histories. The changes are categorized by repositories, branches, and types of alterations (files or commit metadata). Case studies reveal common use cases such as retroactive license changes and removal of accidentally committed secrets.

Conclusion: History alterations in public repositories are prevalent and sometimes problematic. These practices can lead to governance or security issues that recipients might wish to detect. The tool GitHistorian helps developers identify such alterations.

Abstract: Version Control Systems (VCS) like Git allow developers to locally rewrite
recorded history, e.g., to reorder and suppress commits or specific data in
them. These alterations have legitimate use cases, but become problematic when
performed on public branches that have downstream users: they break push/pull
workflows, challenge the integrity and reproducibility of repositories, and
create opportunities for supply chain attackers to sneak into them nefarious
changes. We conduct the first large-scale investigation of Git history
alterations in public code repositories. We analyze 111 M (millions)
repositories archived by Software Heritage, which preserves VCS histories even
across alterations. We find history alterations in 1.22 M repositories, for a
total of 8.7 M rewritten histories. We categorize changes by where they happen
(which repositories, which branches) and what is changed in them (files or
commit metadata). Conducting two targeted case studies we show that altered
histories recurrently change licenses retroactively, or are used to remove
''secrets'' (e.g., private keys) committed by mistake. As these behaviors
correspond to bad practices-in terms of project governance or security
management, respectively-that software recipients might want to avoid, we
introduce GitHistorian, an automated tool, that developers can use to spot and
describe history alterations in public Git repositories.

</details>


### [10] [Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on Open & Industry Data](https://arxiv.org/abs/2509.09313)
*Moritz Mock,Thomas Forrer,Barbara Russo*

Main category: cs.SE

TL;DR: Academic deep learning vulnerability detectors often don't work out-of-the-box for industrial settings. This paper finds that CodeBERT needs domain-specific fine-tuning and imbalance handling to perform well. A CI/CD-integrated tool built with these insights is practical and valued by industry professionals.


<details>
  <summary>Details</summary>
Motivation: There is a gap between academic deep learning solutions for vulnerability detection and their practical deployment in industrial environments. Developers face challenges around trust, legacy systems, integration, and differing expertise.

Method: The paper evaluates CodeBERT's vulnerability detection performance on both industrial and open-source code, analyzing cross-domain generalization and class imbalance handling. It introduces a CI/CD-integrated recommender system (AI-DO) that uses fine-tuned CodeBERT to aid vulnerability detection in code reviews. The tool's usefulness is assessed via a survey among IT professionals.

Result: Models trained on industrial data perform well within the same domain but poorly on open-source code. In contrast, models fine-tuned on open-source data with undersampling techniques show improved vulnerability detection. The integrated tool is perceived as useful by IT professionals.

Conclusion: Effective transfer of deep learning solutions from academia to industry for vulnerability detection requires careful consideration of domain differences and integration. Fine-tuning on diverse data and handling class imbalance are crucial for performance. Integrating such models into existing workflows is feasible and valuable based on user feedback.

Abstract: Deep learning solutions for vulnerability detection proposed in academic
research are not always accessible to developers, and their applicability in
industrial settings is rarely addressed. Transferring such technologies from
academia to industry presents challenges related to trustworthiness, legacy
systems, limited digital literacy, and the gap between academic and industrial
expertise. For deep learning in particular, performance and integration into
existing workflows are additional concerns. In this work, we first evaluate the
performance of CodeBERT for detecting vulnerable functions in industrial and
open-source software. We analyse its cross-domain generalisation when
fine-tuned on open-source data and tested on industrial data, and vice versa,
also exploring strategies for handling class imbalance. Based on these results,
we develop AI-DO(Automating vulnerability detection Integration for Developers'
Operations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated
recommender system that uses fine-tuned CodeBERT to detect and localise
vulnerabilities during code review without disrupting workflows. Finally, we
assess the tool's perceived usefulness through a survey with the company's IT
professionals. Our results show that models trained on industrial data detect
vulnerabilities accurately within the same domain but lose performance on
open-source code, while a deep learner fine-tuned on open data, with
appropriate undersampling techniques, improves the detection of
vulnerabilities.

</details>


### [11] [ORCA: Unveiling Obscure Containers In The Wild](https://arxiv.org/abs/2509.09322)
*Jacopo Bufalino,Agathe Blaise,Stefano Secci*

Main category: cs.SE

TL;DR: Outdated or hidden third-party components in containers are a security risk, and many existing tools cannot detect them in altered images. This paper introduces ORCA, a tool that analyzes such containers much more effectively—improving file coverage by 40% over popular alternatives.


<details>
  <summary>Details</summary>
Motivation: Modern software development frequently utilizes open-source and third-party components in containers, which raises security concerns when vulnerable or outdated libraries are unknowingly included. Ensuring that container analysis tools (SCA) can reliably identify such issues is critical, but current tools often fail with 'obscure' or unintentionally altered container images.

Method: The authors analyzed 600 popular container images to assess the reliability of both cloud-based and open-source SCA tools in identifying contents within 'obscure' containers. They then proposed and developed a new analysis methodology and tool (ORCA) designed to be resilient to container filesystem obfuscation.

Result: Obscure containers are present even in trusted, well-known registries, and existing SCA tools frequently fail to analyze them thoroughly. ORCA, the proposed tool, was able to detect and analyze the content of such containers much more effectively, with a median 40% improvement in file coverage compared to leading tools like Docker Scout and Syft.

Conclusion: SCA tools currently struggle with incomplete or obfuscated container images, potentially allowing vulnerabilities to go undetected. The new ORCA tool and methodology present a significant advancement, greatly increasing the reliability and file coverage of security analysis for containerized environments.

Abstract: Modern software development increasingly depends on open-source libraries and
third-party components, which are often encapsulated into containerized
environments. While improving the development and deployment of applications,
this approach introduces security risks, particularly when outdated or
vulnerable components are inadvertently included in production environments.
Software Composition Analysis (SCA) is a critical process that helps identify
and manage packages and dependencies inside a container. However, unintentional
modifications to the container filesystem can lead to incomplete container
images, which compromise the reliability of SCA tools. In this paper, we
examine the limitations of both cloud-based and open-source SCA tools when
faced with such obscure images. An analysis of 600 popular containers revealed
that obscure containers exist in well-known registries and trusted images and
that many tools fail to analyze such containers. To mitigate these issues, we
propose an obscuration-resilient methodology for container analysis and
introduce ORCA (Obscuration-Resilient Container Analyzer), its open-source
implementation. We reported our findings to all vendors using their appropriate
channels. Our results demonstrate that ORCA effectively detects the content of
obscure containers and achieves a median 40% improvement in file coverage
compared to Docker Scout and Syft.

</details>


### [12] [LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering](https://arxiv.org/abs/2509.09614)
*Jielin Qiu,Zuxin Liu,Zhiwei Liu,Rithesh Murthy,Jianguo Zhang,Haolin Chen,Shiyu Wang,Ming Zhu,Liangwei Yang,Juntao Tan,Zhepeng Cen,Cheng Qian,Shelby Heinecke,Weiran Yao,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.SE

TL;DR: LoCoBench is a new benchmark for testing long-context language models on realistic software development tasks spanning entire codebases. Results show existing models are not yet capable of the deep codebase understanding required for advanced software engineering, highlighting a key area for future research.


<details>
  <summary>Details</summary>
Motivation: Existing code evaluation benchmarks for language models focus mainly on tasks with short contexts, like single-function completion, and do not address the challenges posed by real-world software systems that contain millions of tokens and require reasoning across large codebases.

Method: The authors developed LoCoBench, a benchmark featuring 8,000 evaluation scenarios in 10 programming languages, spanning context lengths from 10,000 to 1,000,000 tokens. It includes 8 task categories, reflecting practical software development activities, and uses a 5-phase pipeline to generate high-quality, diverse scenarios. LoCoBench employs a comprehensive framework of 17 evaluation metrics (including 8 new ones) across 4 dimensions, summarized by the LoCoBench Score (LCBS).

Result: Evaluation of state-of-the-art long-context language models on LoCoBench indicates notable performance gaps, especially on tasks requiring deep, large-scale codebase understanding. Current models struggle to maintain context and architectural consistency across extensive projects.

Conclusion: Long-context language models, despite their extended context windows, still fall short in complex, realistic software development scenarios, leaving significant challenges unsolved. LoCoBench provides a systematic way to analyze and benchmark these shortcomings.

Abstract: The emergence of long-context language models with context windows extending
to millions of tokens has created new opportunities for sophisticated code
understanding and software development evaluation. We propose LoCoBench, a
comprehensive benchmark specifically designed to evaluate long-context LLMs in
realistic, complex software development scenarios. Unlike existing code
evaluation benchmarks that focus on single-function completion or short-context
tasks, LoCoBench addresses the critical evaluation gap for long-context
capabilities that require understanding entire codebases, reasoning across
multiple files, and maintaining architectural consistency across large-scale
software systems. Our benchmark provides 8,000 evaluation scenarios
systematically generated across 10 programming languages, with context lengths
spanning 10K to 1M tokens, a 100x variation that enables precise assessment of
long-context performance degradation in realistic software development
settings. LoCoBench introduces 8 task categories that capture essential
long-context capabilities: architectural understanding, cross-file refactoring,
multi-session development, bug investigation, feature implementation, code
comprehension, integration testing, and security analysis. Through a 5-phase
pipeline, we create diverse, high-quality scenarios that challenge LLMs to
reason about complex codebases at unprecedented scale. We introduce a
comprehensive evaluation framework with 17 metrics across 4 dimensions,
including 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our
evaluation of state-of-the-art long-context models reveals substantial
performance gaps, demonstrating that long-context understanding in complex
software development represents a significant unsolved challenge that demands
more attention. LoCoBench is released at:
https://github.com/SalesforceAIResearch/LoCoBench.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [13] [Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs](https://arxiv.org/abs/2509.09019)
*Mohit Tekriwal,John Sarracino*

Main category: cs.PL

TL;DR: This paper addresses correctness issues in floating-point compiler optimizations, specifically for Fused-Multiply-Add (FMA) at the LLVM IR level, using formal methods for proof.


<details>
  <summary>Details</summary>
Motivation: Compiler optimizations, especially in scientific computing, must balance performance gains with the need for correctness—particularly tricky for floating-point operations due to their numerical complexity. Fast math optimizations can introduce subtle errors, motivating formal correctness guarantees.

Method: They use formal verification by employing the Verified LLVM framework within the Rocq theorem prover to rigorously prove the correctness of applying FMA optimizations to a specific arithmetic expression in LLVM IR.

Result: Initial success in formally verifying the correctness of FMA optimizations for a basic arithmetic block. The authors propose expanding this work to cover more features and fast math optimizations.

Conclusion: The preliminary work demonstrates that the Verified LLVM framework in the Rocq theorem prover can be used to prove correctness of FMA optimizations, with potential for broader application to other optimizations.

Abstract: Scientific computing programs often undergo aggressive compiler optimization
to achieve high performance and efficient resource utilization. While
performance is critical, we also need to ensure that these optimizations are
correct. In this paper, we focus on a specific class of optimizations,
floating-point optimizations, notably due to fast math, at the LLVM IR level.
We present a preliminary work, which leverages the Verified LLVM framework in
the Rocq theorem prover, to prove the correctness of Fused-Multiply-Add (FMA)
optimization for a basic block implementing the arithmetic expression $a * b +
c$ . We then propose ways to extend this preliminary results by adding more
program features and fast math floating-point optimizations.

</details>


### [14] [Dependent-Type-Preserving Memory Allocation](https://arxiv.org/abs/2509.09059)
*Paulette Koronkevich,William J. Bowman*

Main category: cs.PL

TL;DR: Dependently typed languages lose specification guarantees after compilation, especially when linking with external code. This work introduces a type-preserving compilation approach and a new typed intermediate language to maintain dependent type guarantees, focusing on memory safety, even after linking with other programs.


<details>
  <summary>Details</summary>
Motivation: Dependently typed programming languages allow programmers to formally specify and verify program properties, but these guarantees are compromised during compilation and when linking with external programs, which can violate original specifications—especially regarding memory safety.

Method: The work proposes using type-preserving compilation to maintain type information through the compilation process. It develops a typed intermediate language that supports dependent memory allocation and a compiler pass that preserves dependent types related to memory allocation.

Result: The ongoing project has developed a typed intermediate language for dependent memory allocation and is implementing a dependent-type-preserving compiler pass for memory handling. This approach aims to enforce type-safety guarantees when linking programs, even in the presence of external components.

Conclusion: Type-preserving compilation and the introduction of a typed intermediate language for dependent memory allocation can protect dependent type guarantees through compilation and linking, addressing the vulnerabilities that arise when specifications are erased and external programs are linked.

Abstract: Dependently typed programming languages such as Coq, Agda, Idris, and F*,
allow programmers to write detailed specifications of their programs and prove
their programs meet these specifications. However, these specifications can be
violated during compilation since they are erased after type checking. External
programs linked with the compiled program can violate the specifications of the
original program and change the behavior of the compiled program -- even when
compiled with a verified compiler. For example, since Coq does not allow
explicitly allocating memory, a programmer might link their Coq program with a
C program that can allocate memory. Even if the Coq program is compiled with a
verified compiler, the external C program can still violate the memory-safe
specification of the Coq program by providing an uninitialized pointer to
memory. This error could be ruled out by type checking in a language expressive
enough to indicate whether memory is initialized versus uninitialized. Linking
with a program with an uninitialized pointer could be considered ill-typed, and
our linking process could prevent linking with ill-typed programs. To
facilitate type checking during linking, we can use type-preserving
compilation, which preserves the types through the compilation process. In this
ongoing work, we develop a typed intermediate language that supports dependent
memory allocation, as well as a dependent-type-preserving compiler pass for
memory allocation.

</details>
