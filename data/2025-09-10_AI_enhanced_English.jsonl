{"id": "2509.07449", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.07449", "abs": "https://arxiv.org/abs/2509.07449", "authors": ["Mterorga Ukor"], "title": "Aspect-Oriented Programming in Secure Software Development: A Case Study of Security Aspects in Web Applications", "comment": "10 pages, 3 figures", "summary": "Security remains a critical challenge in modern web applications, where\nthreats such as unauthorized access, data breaches, and injection attacks\ncontinue to undermine trust and reliability. Traditional Object-Oriented\nProgramming (OOP) often intertwines security logic with business functionality,\nleading to code tangling, scattering, and reduced maintainability. This study\ninvestigates the role of Aspect-Oriented Programming (AOP) in enhancing secure\nsoftware development by modularizing cross-cutting security concerns. Using a\ncase study approach, we compare AOP-based implementations of security features\nincluding authentication, authorization, input validation, encryption, logging,\nand session management with conventional OOP or middleware-based approaches.\nData collection involves analyzing code quality metrics (e.g., lines of code,\ncoupling, cohesion, modularity index, reusability), performance metrics\n(response time, throughput, memory usage), and maintainability indicators.\nDeveloper feedback is also incorporated to assess integration and debugging\nexperiences. Statistical methods, guided by the ISO/IEC 25010 software quality\nmodel, are applied to evaluate differences across implementations. The findings\ndemonstrate that AOP enhances modularity, reusability, and maintainability of\nsecurity mechanisms, while introducing only minimal performance overhead. The\nstudy contributes practical insights for software engineers and researchers\nseeking to balance security with software quality in web application\ndevelopment.", "AI": {"tldr": "Aspect-Oriented Programming (AOP) modularizes security in web apps better than traditional OOP, improving maintainability and reusability with minimal performance cost.", "motivation": "Modern web applications face persistent security threats such as unauthorized access, data breaches, and injection attacks. Traditional OOP mixes security logic with business code, causing maintenance and reliability challenges. The motivation is to find better ways to modularize and manage security concerns in software development.", "method": "The study uses a case study approach to compare security feature implementations (like authentication, authorization, input validation, encryption, logging, and session management) using Aspect-Oriented Programming (AOP) versus traditional OOP or middleware-based techniques. It measures code quality metrics (lines of code, coupling, cohesion, modularity, reusability), performance (response time, throughput, memory use), maintainability, and developer feedback, following ISO/IEC 25010 standards. Statistical analysis is used to compare results.", "result": "AOP implementations improve modularity, reusability, and maintainability of security features compared to traditional OOP, while adding only minimal performance overhead.", "conclusion": "AOP provides significant benefits for structuring security logic in web applications, making it easier to maintain and reuse security code with little impact on performance. This helps developers strike a better balance between security and software quality."}}
{"id": "2509.07498", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.07498", "abs": "https://arxiv.org/abs/2509.07498", "authors": ["Hai Dinh-Tuan"], "title": "CRACI: A Cloud-Native Reference Architecture for the Industrial Compute Continuum", "comment": null, "summary": "The convergence of Information Technology (IT) and Operational Technology\n(OT) in Industry 4.0 exposes the limitations of traditional, hierarchical\narchitectures like ISA-95 and RAMI 4.0. Their inherent rigidity, data silos,\nand lack of support for cloud-native technologies impair the development of\nscalable and interoperable industrial systems. This paper addresses this issue\nby introducing CRACI, a Cloud-native Reference Architecture for the Industrial\nCompute Continuum. Among other features, CRACI promotes a decoupled and\nevent-driven model to enable flexible, non-hierarchical data flows across the\ncontinuum. It embeds cross-cutting concerns as foundational pillars: Trust,\nGovernance & Policy, Observability, and Lifecycle Management, ensuring quality\nattributes are core to the design. The proposed architecture is validated\nthrough a two-fold approach: (1) a comparative theoretical analysis against\nestablished standards, operational models, and academic proposals; and (2) a\nquantitative evaluation based on performance data from previously published\nreal-world smart manufacturing implementations. The results demonstrate that\nCRACI provides a viable, state-of-the-art architecture that utilizes the\ncompute continuum to overcome the structural limitations of legacy models and\nenable scalable, modern industrial systems.", "AI": {"tldr": "Traditional industrial architectures are too rigid for Industry 4.0. This paper proposes CRACI, a flexible, cloud-native architecture validated via theory and real-world data, showing improved scalability and suitability for modern industrial systems.", "motivation": "Traditional hierarchical architectures such as ISA-95 and RAMI 4.0 are insufficient for Industry 4.0 due to their rigidity, presence of data silos, and inability to leverage cloud-native technologies. These limitations hinder the scalability and interoperability required by modern industrial systems.", "method": "The paper introduces CRACI, a Cloud-native Reference Architecture for the Industrial Compute Continuum. The architecture embraces decoupled, event-driven data flows and embeds key concerns\u2014Trust, Governance & Policy, Observability, and Lifecycle Management\u2014as fundamental design aspects. The validation uses a two-fold approach: (1) comparative theoretical analysis with standards and proposals, and (2) quantitative evaluation using existing smart manufacturing performance data.", "result": "CRACI successfully addresses legacy model limitations, offering scalable, interoperable, and modern industrial system support. Validation shows that it outperforms traditional architectures and is viable for enabling Industry 4.0 solutions.", "conclusion": "CRACI is a state-of-the-art, cloud-native architecture that fulfills the requirements of today\u2019s industrial compute continuum, overcoming the structural and technological limitations of hierarchical legacy models."}}
{"id": "2509.07540", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.07540", "abs": "https://arxiv.org/abs/2509.07540", "authors": ["Huu Hung Nguyen", "Anh Tuan Nguyen", "Thanh Le-Cong", "Yikun Li", "Han Wei Ang", "Yide Yin", "Frank Liauw", "Shar Lwin Khin", "Ouh Eng Lieh", "Ting Zhang", "David Lo"], "title": "PatchSeeker: Mapping NVD Records to their Vulnerability-fixing Commits with LLM Generated Commits and Embeddings", "comment": null, "summary": "Software vulnerabilities pose serious risks to modern software ecosystems.\nWhile the National Vulnerability Database (NVD) is the authoritative source for\ncataloging these vulnerabilities, it often lacks explicit links to the\ncorresponding Vulnerability-Fixing Commits (VFCs). VFCs encode precise code\nchanges, enabling vulnerability localization, patch analysis, and dataset\nconstruction. Automatically mapping NVD records to their true VFCs is therefore\ncritical. Existing approaches have limitations as they rely on sparse, often\nnoisy commit messages and fail to capture the deep semantics in the\nvulnerability descriptions. To address this gap, we introduce PatchSeeker, a\nnovel method that leverages large language models to create rich semantic links\nbetween vulnerability descriptions and their VFCs. PatchSeeker generates\nembeddings from NVD descriptions and enhances commit messages by synthesizing\ndetailed summaries for those that are short or uninformative. These generated\nmessages act as a semantic bridge, effectively closing the information gap\nbetween natural language reports and low-level code changes. Our approach\nPatchSeeker achieves 59.3% higher MRR and 27.9% higher Recall@10 than the\nbest-performing baseline, Prospector, on the benchmark dataset. The extended\nevaluation on recent CVEs further confirms PatchSeeker's effectiveness.\nAblation study shows that both the commit message generation method and the\nselection of backbone LLMs make a positive contribution to PatchSeeker. We also\ndiscuss limitations and open challenges to guide future work.", "AI": {"tldr": "PatchSeeker leverages large language models to improve mapping between NVD vulnerabilities and fixing commits by generating semantic representations and enriched commit messages, substantially outperforming previous systems and enabling better vulnerability tracking and patch analysis.", "motivation": "The motivation of this paper comes from the gap in current methods for mapping vulnerabilities from the National Vulnerability Database (NVD) to their corresponding vulnerability-fixing commits (VFCs). Existing methods are limited by relying on incomplete or unclear commit messages and fail to capture the deep semantics of vulnerability descriptions. Closing this gap is important for improving vulnerability localization, patch analysis, and constructing precise datasets.", "method": "The paper introduces PatchSeeker, a method utilizing large language models (LLMs) to create semantic links between NVD records and their VFCs. PatchSeeker generates semantic embeddings from NVD vulnerability descriptions and synthesizes more informative summaries for uninformative or brief commit messages using LLMs. These enriched messages act as a bridge, connecting the descriptive language of vulnerabilities with detailed code changes.", "result": "PatchSeeker significantly outperforms the prior state-of-the-art (Prospector), achieving 59.3% higher Mean Reciprocal Rank (MRR) and 27.9% higher Recall@10 on benchmark datasets. Additional evaluation on recent CVEs and ablation studies verify the effectiveness of both the commit message generation process and the choice of LLM backbones in PatchSeeker.", "conclusion": "PatchSeeker effectively bridges the gap between natural language vulnerability descriptions and code-level fixes, outperforming previous methods and confirming the value of enhanced semantic modeling and commit message generation. The study also highlights remaining challenges and opportunities for further improvement."}}
{"id": "2509.07728", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.07728", "abs": "https://arxiv.org/abs/2509.07728", "authors": ["John Gouwar", "Gregory Becker", "Tamara Dahlgren", "Nathan Hanford", "Arjun Guha", "Todd Gamblin"], "title": "Bridging the Gap Between Binary and Source Based Package Management in Spack", "comment": "To appear in SC '25", "summary": "Binary package managers install software quickly but they limit\nconfigurability due to rigid ABI requirements that ensure compatibility between\nbinaries. Source package managers provide flexibility in building software, but\ncompilation can be slow. For example, installing an HPC code with a new MPI\nimplementation may result in a full rebuild. Spack, a widely deployed,\nHPC-focused package manager, can use source and pre-compiled binaries, but\nlacks a binary compatibility model, so it cannot mix binaries not built\ntogether. We present splicing, an extension to Spack that models binary\ncompatibility between packages and allows seamless mixing of source and binary\ndistributions. Splicing augments Spack's packaging language and dependency\nresolution engine to reuse compatible binaries but maintains the flexibility of\nsource builds. It incurs minimal installation-time overhead and allows rapid\ninstallation from binaries, even for ABI-sensitive dependencies like MPI that\nwould otherwise require many rebuilds.", "AI": {"tldr": "Splicing for Spack lets users combine source and binary packages flexibly and quickly, greatly reducing rebuilds and installation time for complex software.", "motivation": "Binary package managers are fast but limited in configurability due to ABI constraints, while source package managers are configurable but slow. Spack, a popular HPC package manager, cannot mix binaries not built together due to lack of a binary compatibility model.", "method": "The paper introduces 'splicing,' an extension for Spack that models binary compatibility between packages, augments Spack\u2019s packaging language, and improves the dependency resolution engine to enable mixing of source and binary packages.", "result": "Splicing makes it possible to reuse compatible binaries and mix source and binary distributions, maintaining configurability while speeding up installation. It works for complex dependencies like MPI with minimal overhead.", "conclusion": "The proposed splicing extension enables quick and flexible software installation in Spack by supporting seamless mixing of binary and source packages without frequent rebuilds, even for ABI-sensitive applications."}}
{"id": "2509.07003", "categories": ["cs.PL", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07003", "abs": "https://arxiv.org/abs/2509.07003", "authors": ["Youjie Li", "Cheng Wan", "Zhiqi Lin", "Hongyu Zhu", "Jiacheng Yang", "Ziang Song", "Xinyi Di", "Jiawei Wu", "Huiyao Shu", "Wenlei Bao", "Yanghua Peng", "Haibin Lin", "Li-Wen Chang"], "title": "veScale: Consistent and Efficient Tensor Programming with Eager-Mode SPMD", "comment": "21 pages, 16 figures, 5 tables", "summary": "Large Language Models (LLMs) have scaled rapidly in size and complexity,\nrequiring increasingly intricate parallelism for distributed training, such as\n3D parallelism. This sophistication motivates a shift toward simpler, more\ndebuggable programming paradigm like Single Program Multiple Data (SPMD).\nHowever, SPMD in eager execution introduces two key challenges: ensuring\nconsistency with single-device execution and achieving high performance at\nscale. In this paper, we introduce veScale, an eager-mode training system that\nfully embraces SPMD paradigm to democratize distributed tensor programming.\nveScale addresses the prevalent issue of inconsistent results in systems like\nPyTorch by introducing a novel algorithm of distributed Random Number\nGeneration (RNG) compatible with arbitrary sharded operators. veScale also\nsignificantly boosts training performance by reducing PyTorch primitive's\noverhead and improving communication efficiency. Evaluations show that veScale\ndelivers up to 2.2x speedup over the state-of-the-art training systems, like\nTorchTitan, and cuts code complexity by 78.4%, while preserving\nsingle-device-equivalent results.", "AI": {"tldr": "veScale is a new training system for large models that simplifies distributed programming with SPMD, improves consistency and performance, and outperforms current solutions in both speed and code simplicity.", "motivation": "The rapid scaling of Large Language Models (LLMs) increases the complexity of distributed training. Existing parallelism approaches (like 3D parallelism) make systems more complicated and harder to debug, motivating a shift towards simpler, more manageable paradigms like SPMD.", "method": "The paper introduces veScale, an eager-mode training system designed around the SPMD paradigm. veScale tackles inconsistency issues in distributed settings (such as those found in PyTorch) by using a new distributed Random Number Generation (RNG) algorithm compatible with various sharded operators. It also optimizes training by lowering PyTorch primitive overhead and boosting communication efficiency.", "result": "veScale achieves up to 2.2x speedup compared to state-of-the-art systems such as TorchTitan. It also reduces code complexity by 78.4%, while ensuring results consistent with single-device execution.", "conclusion": "veScale makes distributed large-scale tensor programming more accessible by fully embracing SPMD in eager execution. It solves key challenges of consistency and performance, presenting a significant advance over previous training frameworks."}}
{"id": "2509.07747", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.07747", "abs": "https://arxiv.org/abs/2509.07747", "authors": ["Maksym Avramenko", "David Chapela-Campa", "Marlon Dumas", "Fredrik Milani"], "title": "What's Coming Next? Short-Term Simulation of Business Processes from Current State", "comment": null, "summary": "Business process simulation is an approach to evaluate business process\nchanges prior to implementation. Existing methods in this field primarily\nsupport tactical decision-making, where simulations start from an empty state\nand aim to estimate the long-term effects of process changes. A complementary\nuse-case is operational decision-making, where the goal is to forecast\nshort-term performance based on ongoing cases and to analyze the impact of\ntemporary disruptions, such as demand spikes and shortfalls in available\nresources. An approach to tackle this use-case is to run a long-term simulation\nup to a point where the workload is similar to the current one (warm-up), and\nmeasure performance thereon. However, this approach does not consider the\ncurrent state of ongoing cases and resources in the process. This paper studies\nan alternative approach that initializes the simulation from a representation\nof the current state derived from an event log of ongoing cases. The paper\naddresses two challenges in operationalizing this approach: (1) Given a\nsimulation model, what information is needed so that a simulation run can start\nfrom the current state of cases and resources? (2) How can the current state of\na process be derived from an event log? The resulting short-term simulation\napproach is embodied in a simulation engine that takes as input a simulation\nmodel and a log of ongoing cases, and simulates cases for a given time horizon.\nAn experimental evaluation shows that this approach yields more accurate\nshort-term performance forecasts than long-term simulations with warm-up\nperiod, particularly in the presence of concept drift or bursty performance\npatterns.", "AI": {"tldr": "The paper presents a new method for short-term business process simulation that starts from the current state derived from event logs, resulting in more accurate short-term forecasts than traditional warm-up-based approaches, particularly under changing or bursty process conditions.", "motivation": "Existing business process simulation methods primarily support tactical, long-term decision-making and do not effectively support operational, short-term forecasting that takes into account the real-time state of ongoing cases and resources.", "method": "This paper proposes and develops a short-term simulation approach that initializes the simulation from the current operational state, as derived from event logs of ongoing cases, instead of starting from an empty state. It specifically addresses the challenges of determining the necessary state information and extracting it from event logs. The approach is implemented in a simulation engine that accepts both a simulation model and an event log, and simulates for a specified time horizon.", "result": "Experimental evaluations demonstrate that this approach produces more accurate short-term forecasts, especially in conditions with concept drift or bursty patterns, compared to traditional long-term simulations that use a warm-up period.", "conclusion": "Initializing simulations directly from the real-time operational state enables more precise short-term performance predictions for business processes, outperforming conventional methods in dynamic environments."}}
{"id": "2509.07551", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.07551", "abs": "https://arxiv.org/abs/2509.07551", "authors": ["Sean Bocirnea", "William J. Bowman"], "title": "Fast and Extensible Hybrid Embeddings with Micros", "comment": "13 pages", "summary": "Macro embedding is a popular approach to defining extensible shallow\nembeddings of object languages in Scheme like host languages. While macro\nembedding has even been shown to enable implementing extensible typed languages\nin systems like Racket, it comes at a cost: compile-time performance. In this\npaper, we revisit micros - syntax to intermediate representation (IR)\ntransformers, rather than source syntax to source syntax transformers (macros).\nMicro embedding enables stopping at an IR, producing a deep embedding and\nenabling high performance compile-time functions over an efficient IR, before\nshallowly embedding the IR back into source syntax. Combining micros with\nseveral design patterns to enable the IR and functions over it to be\nextensible, we achieve extensible hybrid embedding of statically typed\nlanguages with significantly improved compile-time compared to macro-embedding\napproaches. We describe our design patterns and propose new abstractions\npackaging these patterns.", "AI": {"tldr": "Micro embedding (syntax-to-IR) offers a faster, extensible alternative to macro embedding for language embedding in Scheme-like systems. This approach enables efficient extensibility and faster compile-time through the use of new design patterns and abstractions.", "motivation": "Macro embedding is widely used for extensible shallow embeddings in Scheme-like languages, but suffers from poor compile-time performance. The paper seeks to offer a better alternative.", "method": "The paper proposes 'micro embedding,' which uses syntax-to-IR transformers instead of traditional macro-based source-to-source transformation. The authors use design patterns and new abstractions to make the IR and related functions extensible.", "result": "The approach achieved extensible and hybrid embedding of statically typed languages with much better compile-time performance than macro embedding.", "conclusion": "Combining micros (syntax-to-IR) with specific design patterns enables a flexible, extensible embedding methodology that outperforms macro embedding in compile-time efficiency. The paper also introduces new abstractions for packaging these patterns."}}
{"id": "2509.07763", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.07763", "abs": "https://arxiv.org/abs/2509.07763", "authors": ["Mikel Robredo", "Matteo Esposito", "Fabio Palomba", "Rafael Pe\u00f1aloza", "Valentina Lenarduzzi"], "title": "What Were You Thinking? An LLM-Driven Large-Scale Study of Refactoring Motivations in Open-Source Projects", "comment": null, "summary": "Context. Code refactoring improves software quality without changing external\nbehavior. Despite its advantages, its benefits are hindered by the considerable\ncost of time, resources, and continuous effort it demands. Aim. Understanding\nwhy developers refactor, and which metrics capture these motivations, may\nsupport wider and more effective use of refactoring in practice. Method. We\nperformed a large-scale empirical study to analyze developers refactoring\nactivity, leveraging Large Language Models (LLMs) to identify underlying\nmotivations from version control data, comparing our findings with previous\nmotivations reported in the literature. Results. LLMs matched human judgment in\n80% of cases, but aligned with literature-based motivations in only 47%. They\nenriched 22% of motivations with more detailed rationale, often highlighting\nreadability, clarity, and structural improvements. Most motivations were\npragmatic, focused on simplification and maintainability. While metrics related\nto developer experience and code readability ranked highest, their correlation\nwith motivation categories was weak. Conclusions. We conclude that LLMs\neffectively capture surface-level motivations but struggle with architectural\nreasoning. Their value lies in providing localized explanations, which, when\ncombined with software metrics, can form hybrid approaches. Such integration\noffers a promising path toward prioritizing refactoring more systematically and\nbalancing short-term improvements with long-term architectural goals.", "AI": {"tldr": "LLMs can help explain why developers refactor by matching human and literature-based motivations to an extent, especially for readability and maintainability. However, their understanding of deeper architectural motivations is limited. Combining LLM insights with traditional metrics could lead to better and more systematic refactoring strategies.", "motivation": "Understanding why developers refactor code and which software metrics are reflective of these motivations, with the ultimate aim of promoting more effective and widespread adoption of refactoring in practice.", "method": "A large-scale empirical study was conducted, using Large Language Models (LLMs) to analyze version control data to uncover developers\u2019 refactoring motivations. The motivations identified were compared with those reported in prior literature.", "result": "LLMs matched human judgment in 80% of cases, but only aligned with motivations from literature in 47% of cases. LLMs provided richer explanations in 22% of cases, mostly regarding readability, clarity, and structure. Most refactoring motivations were found to be pragmatic, focusing on simplification and maintainability. Metrics such as developer experience and readability were top indicators but showed weak correlation with motivation categories.", "conclusion": "LLMs can effectively detect surface motivations for refactoring, but are less capable with architectural reasoning. Their main value is in generating localized explanations. Combining LLM-generated insights with software metrics could lead to hybrid approaches, potentially improving systematic refactoring prioritization and balancing immediate improvements with long-term software architecture."}}
{"id": "2509.07609", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.07609", "abs": "https://arxiv.org/abs/2509.07609", "authors": ["Yichen Xu", "Oliver Bra\u010devac", "Cao Nguyen Pham", "Martin Odersky"], "title": "What's in the Box: Ergonomic and Expressive Capture Tracking over Generic Data Structures (Extended Version)", "comment": null, "summary": "Capturing types in Scala unify static effect and resource tracking with\nobject capabilities, enabling lightweight effect polymorphism with minimal\nnotational overhead. However, their expressiveness has been insufficient for\ntracking capabilities embedded in generic data structures, preventing them from\nscaling to the standard collections library -- an essential prerequisite for\nbroader adoption. This limitation stems from the inability to name capabilities\nwithin the system's notion of box types.\n  This paper develops System Capless, a new foundation for capturing types that\nprovides the theoretical basis for reach capabilities (rcaps), a novel\nmechanism for naming \"what's in the box.\" The calculus refines the universal\ncapability notion into a new scheme with existential and universal capture set\nquantification. Intuitively, rcaps witness existentially quantified capture\nsets inside the boxes of generic types in a way that does not require exposing\nexistential capture types in the surface language. We have fully mechanized the\nformal metatheory of System Capless in Lean, including proofs of type soundness\nand scope safety. System Capless supports the same lightweight notation of\ncapturing types plus rcaps, as certified by a type-preserving translation, and\nalso enables fully optional explicit capture-set quantification to increase\nexpressiveness.\n  Finally, we present a full reimplementation of capture checking in Scala 3\nbased on System Capless and migrate the entire Scala collections library and an\nasynchronous programming library to evaluate its practicality and ergonomics.\nOur results demonstrate that reach capabilities enable the adoption of capture\nchecking in production code with minimal changes and minimal-to-zero notational\noverhead in a vast majority of cases.", "AI": {"tldr": "System Capless introduces \"reach capabilities\" to improve effect and resource tracking in Scala, making it possible for capturing types to work with generic data structures like collections. This enables practical and lightweight capture checking with minimal changes, paving the way for broader adoption in production Scala code.", "motivation": "The paper addresses the challenge of tracking object capabilities\u2014particularly those embedded in generic data structures\u2014using capturing types in Scala, which previously lacked the expressiveness to scale to standard libraries, hindering widespread adoption.", "method": "The authors develop 'System Capless', a new theoretical foundation that introduces reach capabilities (rcaps) for naming and tracking capabilities within generic types. The system refines the universal capability concept using existential and universal capture set quantification and is formalized in Lean, complete with metatheory proofs. The practical aspect includes re-implementing capture checking in Scala 3 and migrating major libraries to use the new system.", "result": "System Capless successfully supports type-safe, expressive capture and effect tracking even in large codebases like Scala's collections and asynchronous libraries. The migration demonstrates that the approach requires minimal changes and retains lightweight notation, validating the system's practicality and ergonomics.", "conclusion": "Reach capabilities (rcaps) in System Capless overcome prior limitations and enable efficient effect and resource tracking in generic data structures, making capture checking practical for production use in Scala with little notational or code overhead."}}
{"id": "2509.07851", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.07851", "abs": "https://arxiv.org/abs/2509.07851", "authors": ["Irdin Pekaric", "Giovanni Apruzzese"], "title": "\"We provide our resources in a dedicated repository\": Surveying the Transparency of HICSS publications", "comment": null, "summary": "Every day, new discoveries are made by researchers from all across the globe\nand fields. HICSS is a flagship venue to present and discuss such scientific\nadvances. Yet, the activities carried out for any given research can hardly be\nfully contained in a single document of a few pages-the \"paper.\" Indeed, any\ngiven study entails data, artifacts, or other material that is crucial to truly\nappreciate the contributions claimed in the corresponding paper. External\nrepositories (e.g., GitHub) are a convenient tool to store all such resources\nso that future work can freely observe and build upon them -- thereby improving\ntransparency and promoting reproducibility of research as a whole. In this\nwork, we scrutinize the extent to which papers recently accepted to HICSS\nleverage such repositories to provide supplementary material. To this end, we\ncollect all the 5579 papers included in HICSS proceedings from 2017-2024. Then,\nwe identify those entailing either human subject research (850) or technical\nimplementations (737), or both (147). Finally, we review their text, examining\nhow many include a link to an external repository-and, inspect its contents.\nOverall, out of 2028 papers, only 3\\% have a functional and publicly available\nrepository that is usable by downstream research. We release all our tools.", "AI": {"tldr": "Most recent HICSS papers do not share usable supplementary material in public repositories, hindering research transparency and reproducibility; only 3% of relevant papers do so, and the authors release their audit tools.", "motivation": "Research transparency and reproducibility are essential for scientific progress. Many research papers involve data, artifacts, or software necessary to verify and build upon their findings, but these resources often are not shared or accessible.", "method": "The study collected all 5579 papers from 2017-2024 in the HICSS proceedings, identified those with human subject research or technical implementations, reviewed 2028 relevant papers, and checked whether they included functional, publicly accessible repositories of supplementary material.", "result": "Of the 2028 relevant papers analyzed, only 3% provided a functional, publicly available repository that could be used by other researchers.", "conclusion": "There is a significant gap in sharing usable supplementary research material via external repositories among HICSS papers, limiting transparency and reproducibility. The authors also share their analysis tools to facilitate future research."}}
{"id": "2509.07933", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07933", "abs": "https://arxiv.org/abs/2509.07933", "authors": ["Wanni Vidulige Ishan Perera", "Xing Liu", "Fan liang", "Junyi Zhang"], "title": "Breaking Android with AI: A Deep Dive into LLM-Powered Exploitation", "comment": null, "summary": "The rapid evolution of Artificial Intelligence (AI) and Large Language Models\n(LLMs) has opened up new opportunities in the area of cybersecurity, especially\nin the exploitation automation landscape and penetration testing. This study\nexplores Android penetration testing automation using LLM-based tools,\nespecially PentestGPT, to identify and execute rooting techniques. Through a\ncomparison of the traditional manual rooting process and exploitation methods\nproduced using AI, this study evaluates the efficacy, reliability, and\nscalability of automated penetration testing in achieving high-level privilege\naccess on Android devices. With the use of an Android emulator (Genymotion) as\nthe testbed, we fully execute both traditional and exploit-based rooting\nmethods, automating the process using AI-generated scripts. Secondly, we create\na web application by integrating OpenAI's API to facilitate automated script\ngeneration from LLM-processed responses. The research focuses on the\neffectiveness of AI-enabled exploitation by comparing automated and manual\npenetration testing protocols, by determining LLM weaknesses and strengths\nalong the way. We also provide security suggestions of AI-enabled exploitation,\nincluding ethical factors and potential misuse. The findings exhibit that while\nLLMs can significantly streamline the workflow of exploitation, they need to be\ncontrolled by humans to ensure accuracy and ethical application. This study\nadds to the increasing body of literature on AI-powered cybersecurity and its\neffect on ethical hacking, security research, and mobile device security.", "AI": {"tldr": "AI and LLMs, such as PentestGPT, can automate and streamline Android penetration testing to efficiently achieve rooting and privilege escalation, but human oversight remains essential to ensure ethical and accurate application.", "motivation": "The motivation of this paper is to improve and automate Android device penetration testing using AI and Large Language Models (LLMs), specifically exploring how tools like PentestGPT could streamline the process of identifying and executing rooting techniques. The study seeks to address the limitations of manual testing and evaluate whether AI can enhance efficacy, reliability, and scalability in cybersecurity tasks.", "method": "The authors used an Android emulator (Genymotion) as the test environment, carrying out both traditional manual rooting and AI-driven exploitation techniques produced using PentestGPT. The process was automated using AI-generated scripts, and a web application was developed to integrate OpenAI's API for automated script generation. Comparative analysis was then conducted between the results of automated and manual penetration tests.", "result": "The results indicate that LLMs and AI tools can significantly streamline exploitation workflows and automate penetration testing protocols. However, AI-driven methods still require human oversight to ensure accuracy and ethical use. Both strengths and weaknesses of LLMs in this context were identified.", "conclusion": "AI-powered tools like LLMs can enhance efficiency and scalability in Android penetration testing, but human control is necessary to manage accuracy and address ethical considerations. The study contributes useful insights to the field of AI-enabled cybersecurity, suggesting both practical benefits and the importance of guiding responsible use."}}
