{"id": "2507.00347", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00347", "abs": "https://arxiv.org/abs/2507.00347", "authors": ["Sun Ding", "Ude Enebeli", "Atilhan", "Manay", "Ryan Pua", "Kamal Kotak"], "title": "VTS-Guided AI Interaction Workflow for Business Insights", "comment": null, "summary": "Modern firms face a flood of dense, unstructured reports. Turning these\ndocuments into usable insights takes heavy effort and is far from agile when\nquick answers are needed. VTS-AI tackles this gap. It integrates Visual\nThinking Strategies, which emphasize evidence-based observation, linking, and\nthinking, into AI agents, so the agents can extract business insights from\nunstructured text, tables, and images at scale. The system works in three tiers\n(micro, meso, macro). It tags issues, links them to source pages, and rolls\nthem into clear action levers stored in a searchable YAML file. In tests on an\n18-page business report, VTS-AI matched the speed of a one-shot ChatGPT prompt\nyet produced richer findings: page locations, verbatim excerpts, severity\nscores, and causal links. Analysts can accept or adjust these outputs in the\nsame IDE, keeping human judgment in the loop. Early results show VTS-AI spots\nthe direction of key metrics and flags where deeper number-crunching is needed.\nNext steps include mapping narrative tags to financial ratios, adding\nfinance-tuned language models through a Model-Context Protocol, and building a\nRisk & Safety Layer to stress-test models and secure data. These upgrades aim\nto make VTS-AI a production-ready, audit-friendly tool for rapid business\nanalysis."}
{"id": "2507.00352", "categories": ["cs.SE", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.00352", "abs": "https://arxiv.org/abs/2507.00352", "authors": ["Abanoub E. Abdelmalak", "Mohamed A. Elsayed", "David Abercrombie", "Ilhami Torunoglu"], "title": "An AST-guided LLM Approach for SVRF Code Synthesis", "comment": "9 Pages, 5 Figures, 2 Tables", "summary": "Standard Verification Rule Format (SVRF) is essential for semiconductor\napplications like Design Rule Check (DRC), Layout Versus Schematic (LVS), and\nOptical Proximity Correction (OPC) and it faces challenges as advancing nodes\ncreate complex design rules that renders traditional SVRF development\nineffective and highlight an expertise gap. This paper introduces a novel\nmethodology integrating Abstract Syntax Tree (AST) embedding and\nRetrieval-Augmented Generation (RAG) for enhanced SVRF code synthesis, ensuring\nsemantic accuracy and error minimization through structural validation with\ndomain-specific insights for precise code generation.\n  We evaluate different T5-based models and propose an innovative SVRF-specific\nscoring framework that complements standard metrics like BLEU and ROUGE-L. In\nour approach, AST provides rigorous structural validation, while RAG infuses\nrelevant domain knowledge, effectively enhancing the code generation workflow.\n  Testing on a comprehensive benchmark of 740 DRC rule implementations, our\nmethodology demonstrates up to a 40\\% improvement in code generation accuracy\ncompared to basic text-based fine-tuning process. This fusion of industry\nexpertise with advanced coding strategies not only optimizes SVRF development\nunder limited dataset constraints but also creates a more intuitive and\nefficient coding environment. Consequently, users can rapidly iterate through\ndesign cycles, reduce manual error correction, and significantly improve\noverall productivity."}
{"id": "2507.00378", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00378", "abs": "https://arxiv.org/abs/2507.00378", "authors": ["Xikai Sun", "Fan Dang", "Kebin Liu", "Xin Miao", "Zihao Yang", "Haimo Lu", "Yawen Zheng", "Yunhao Liu"], "title": "iPanda: An Intelligent Protocol Testing and Debugging Agent for Conformance Testing", "comment": "14 pages, 6 figures", "summary": "Conformance testing is essential for ensuring that protocol implementations\ncomply with their specifications. However, traditional testing approaches\ninvolve manually creating numerous test cases and scripts, making the process\nlabor-intensive and inefficient. Recently, Large Language Models (LLMs) have\ndemonstrated impressive text comprehension and code generation abilities,\nproviding promising opportunities for automation. In this paper, we propose\niPanda, the first end-to-end framework that leverages LLMs to automate protocol\nconformance testing. Given a protocol specification document and its\nimplementation, iPanda first employs a keyword-based method to automatically\ngenerate comprehensive test cases. Then, it utilizes a code-based\nretrieval-augmented generation approach to effectively interpret the\nimplementation and produce executable test code. To further enhance code\nquality, iPanda incorporates an iterative self-correction mechanism to refine\ngenerated test scripts interactively. Finally, by executing and analyzing the\ngenerated tests, iPanda systematically verifies compliance between\nimplementations and protocol specifications. Comprehensive experiments on\nvarious protocols show that iPanda significantly outperforms pure LLM-based\napproaches, improving the success rate (Pass@1) of test-code generation by\nfactors ranging from 4.675 times to 10.751 times."}
{"id": "2507.00413", "categories": ["cs.SE", "D.2.7"], "pdf": "https://arxiv.org/pdf/2507.00413", "abs": "https://arxiv.org/abs/2507.00413", "authors": ["Taiming Wang", "Hui Liu", "Yuxia Zhang", "Yanjie Jiang"], "title": "Recommending Variable Names for Extract Local Variable Refactorings", "comment": "Accepted by TOSEM", "summary": "Extract local variable is one of the most popular refactorings, and most IDEs\nand refactoring tools provide automated support for this refactoring. However,\nwe find approximately 70% of the names recommended by these IDEs are different\nfrom what developers manually constructed, adding additional renaming burdens\nto developers and providing limited assistance. In this paper, we introduce\nVarNamer, an automated approach designed to recommend variable names for\nextract local variable refactorings. Through a large-scale empirical study, we\nidentify key contexts that are useful for composing variable names. Leveraging\nthese insights, we developed a set of heuristic rules through program static\nanalysis techniques and employ data mining techniques to recommend variable\nnames effectively. Notably, some of our heuristic rules have been successfully\nintegrated into Eclipse, where they are now distributed with the latest\nreleases of the IDE. Evaluation demonstrates its superiority over\nstate-of-the-art IDEs. Specifically, VarNamer significantly increases the\nchance of exact match by 52.6% compared to Eclipse and 40.7% compared to\nIntelliJ IDEA. We also evaluated the proposed approach with real-world extract\nlocal variable refactorings conducted in C++ projects, and the results suggest\nthat the approach can achieve comparable performance on programming languages\nbesides Java. It may suggest the generalizability of VarNamer. Finally, we\ndesigned and conducted a user study and the results of the user study suggest\nthat our approach can speed up the refactoring by 27.8% and reduce 49.3% edits\non the recommended variable names."}
{"id": "2507.00057", "categories": ["cs.PL", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.00057", "abs": "https://arxiv.org/abs/2507.00057", "authors": ["Thomas Valentin", "Ardi Madadi", "Gaetano Sapia", "Marcel BÃ¶hme"], "title": "Estimating Correctness Without Oracles in LLM-Based Code Generation", "comment": "8 pages + refs and appendix", "summary": "Generating code from natural language specifications is one of the most\nsuccessful applications of Large Language Models (LLMs). Yet, they hallucinate:\nLLMs produce outputs that may be grammatically correct but are factually\nincorrect. Without an existing, correct implementation (i.e., an oracle), can\nwe quantify how likely the generated program is correct?\n  In this paper, we propose a measure of incorrectness, called incoherence,\nthat can be estimated efficiently in the absence of an oracle and provides a\nlower bound on the error, i.e., the probability that the LLM-generated program\nfor that specification is incorrect. Our experiments demonstrate an\nextraordinary effectiveness. For the average code generation task, our\nincoherence-based methodology can automatically identify about two-thirds of\nincorrect programs without reports of false positives. In fact, an oracle-based\nevaluation of LLMs can be reliably replaced by an incoherence-based evaluation.\nIn particular, we find a very strong agreement between the ranking of LLMs by\nthe number of programs deemed correct via an oracle (pass@1) and the ranking of\nLLMs by the number of programs deemed correct via our incoherence."}
{"id": "2507.00421", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.00421", "abs": "https://arxiv.org/abs/2507.00421", "authors": ["Parthiv Katapara", "Anand Sharma"], "title": "Embedded DevOps: A Survey on the Application of DevOps Practices in Embedded Software and Firmware Development", "comment": "This paper present survey on DevOps practices which exists in\n  Embedded Software development", "summary": "The adoption of DevOps practices in embedded systems and firmware development\nis emerging as a response to the growing complexity of modern\nhardware--software co-designed products. Unlike cloud-native applications,\nembedded systems introduce challenges such as hardware dependency, real-time\nconstraints, and safety-critical requirements. This literature review\nsynthesizes findings from 20 academic and industrial sources to examine how\nDevOps principles--particularly continuous integration, continuous delivery,\nand automated testing--are adapted to embedded contexts. We categorize efforts\nacross tooling, testing strategies, pipeline automation, and security\npractices. The review highlights current limitations in deployment workflows\nand observability, proposing a roadmap for future research. This work offers\nresearchers and practitioners a consolidated understanding of Embedded DevOps,\nbridging fragmented literature with a structured perspective."}
{"id": "2507.00264", "categories": ["cs.PL", "cs.DC", "cs.PF", "cs.SE", "D.2.13; D.2.8; D.3.3; B.8"], "pdf": "https://arxiv.org/pdf/2507.00264", "abs": "https://arxiv.org/abs/2507.00264", "authors": ["Isabella Basso do Amaral", "Renato Cordeiro Ferreira", "Alfredo Goldman"], "title": "Rust vs. C for Python Libraries: Evaluating Rust-Compatible Bindings Toolchains", "comment": "10 pages, 27 figures (1 diagram, 4 graphs, 9 tables, 13 code\n  listings), submitted to SBAC-PAD 2025", "summary": "The Python programming language is best known for its syntax and scientific\nlibraries, but it is also notorious for its slow interpreter. Optimizing\ncritical sections in Python entails special knowledge of the binary\ninteractions between programming languages, and can be cumbersome to interface\nmanually, with implementers often resorting to convoluted third-party\nlibraries. This comparative study evaluates the performance and ease of use of\nthe PyO3 Python bindings toolchain for Rust against ctypes and cffi. By using\nRust tooling developed for Python, we can achieve state-of-the-art performance\nwith no concern for API compatibility."}
{"id": "2507.00481", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.00481", "abs": "https://arxiv.org/abs/2507.00481", "authors": ["Philipp M. ZÃ¤hl", "Sabine Theis", "Martin R. Wolf"], "title": "The Influence of HEXACO Personality Traits on the Teamwork Quality in Software Teams -- A Preliminary Research Approach", "comment": null, "summary": "Although software engineering research has focused on optimizing processes\nand technology, there is a growing recognition that human factors, particularly\nteamwork, also significantly impact optimization. Recent research suggests that\ndeveloper personality has a strong influence on teamwork. In fact, personality\nconsiderations may have a greater impact on software development than processes\nand tools. This paper aims to design a study that measures the impact of HEXACO\npersonality traits on the Teamwork Quality (TWQ) of software teams. A\npreliminary data collection (n=54) was conducted for this purpose. The analysis\nshowed that several personality traits, as well as their composition, had a\nsignificant impact on TWQ. Additionally, other variables, such as the\nproportion of women and age distribution, also affected TWQ. The study's\ninitial results demonstrate the usefulness and validity of the study design.\nThe results also suggest several opportunities to improve teamwork in IT\norganizations and avenues for further research."}
{"id": "2507.00488", "categories": ["cs.PL", "D.3.3; D.2"], "pdf": "https://arxiv.org/pdf/2507.00488", "abs": "https://arxiv.org/abs/2507.00488", "authors": ["Lloyd Allison"], "title": "Have Object-Oriented Languages Missed a Trick with Class Function and its Subclasses?", "comment": null, "summary": "Compared to functions in mathematics, functions in programming languages seem\nto be under classified. Functional programming languages based on the lambda\ncalculus famously treat functions as first-class values. Object-oriented\nlanguages have adopted ``lambdas'', notably for call-back routines in\nevent-based programming. Typically a programming language has functions, a\nfunction has a type, and some functions act on other functions and/or return\nfunctions but there is generally a lack of (i) ``class Function'' in the OO\nsense of the word class and particularly (ii) subclasses of Function for\nfunctions having specific properties. Some such classes are presented here and\nprogrammed in some popular programming languages as an experimental\ninvestigation into OO languages missing this opportunity."}
{"id": "2507.00496", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.00496", "abs": "https://arxiv.org/abs/2507.00496", "authors": ["Hongjing Guo", "Chuanqi Tao", "Zhiqiu Huang", "Weiqin Zou"], "title": "Coverage-Guided Testing for Deep Learning Models: A Comprehensive Survey", "comment": null, "summary": "As Deep Learning (DL) models are increasingly applied in safety-critical\ndomains, ensuring their quality has emerged as a pressing challenge in modern\nsoftware engineering. Among emerging validation paradigms, coverage-guided\ntesting (CGT) has gained prominence as a systematic framework for identifying\nerroneous or unexpected model behaviors. Despite growing research attention,\nexisting CGT studies remain methodologically fragmented, limiting the\nunderstanding of current advances and emerging trends. This work addresses that\ngap through a comprehensive review of state-of-the-art CGT methods for DL\nmodels, including test coverage analysis, coverage-guided test input\ngeneration, and coverage-guided test input optimization. This work provides\ndetailed taxonomies to organize these methods based on methodological\ncharacteristics and application scenarios. We also investigate evaluation\npractices adopted in existing studies, including the use of benchmark datasets,\nmodel architectures, and evaluation aspects. Finally, open challenges and\nfuture directions are highlighted in terms of the correlation between\nstructural coverage and testing objectives, method generalizability across\ntasks and models, practical deployment concerns, and the need for standardized\nevaluation and tool support. This work aims to provide a roadmap for future\nacademic research and engineering practice in DL model quality assurance."}
{"id": "2507.00686", "categories": ["cs.SE", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.00686", "abs": "https://arxiv.org/abs/2507.00686", "authors": ["Ronny Seiger", "Daniel Locher", "Marco Kaufmann", "Aaron F. Kurz"], "title": "A Domain-specific Language and Architecture for Detecting Process Activities from Sensor Streams in IoT", "comment": "Submitted to Internet of Things (ISSN 2542-6605)", "summary": "Modern Internet of Things (IoT) systems are equipped with a plethora of\nsensors providing real-time data about the current operations of their\ncomponents, which is crucial for the systems' internal control systems and\nprocesses. However, these data are often too fine-grained to derive useful\ninsights into the execution of the larger processes an IoT system might be part\nof. Process mining has developed advanced approaches for the analysis of\nbusiness processes that may also be used in the context of IoT. Bringing\nprocess mining to IoT requires an event abstraction step to lift the low-level\nsensor data to the business process level. In this work, we aim to empower\ndomain experts to perform this step using a newly developed domain-specific\nlanguage (DSL) called Radiant. Radiant supports the specification of patterns\nwithin the sensor data that indicate the execution of higher level process\nactivities. These patterns are translated to complex event processing (CEP)\napplications to be used for detecting activity executions at runtime. We\npropose a corresponding software architecture for online event abstraction from\nIoT sensor streams using the CEP applications. We evaluate these applications\nto monitor activity executions using IoT sensors in smart manufacturing and\nsmart healthcare. The evaluation method and results inform the domain expert\nabout the quality of activity detections and potential for improvement."}
{"id": "2507.00699", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.00699", "abs": "https://arxiv.org/abs/2507.00699", "authors": ["Guoliang Duan", "Mingwei Liu", "Yanlin Wang", "Chong Wang", "Xin Peng", "Zibin Zheng"], "title": "A Hierarchical and Evolvable Benchmark for Fine-Grained Code Instruction Following with Multi-Turn Feedback", "comment": null, "summary": "Large language models (LLMs) have advanced significantly in code generation,\nyet their ability to follow complex programming instructions with layered and\ndiverse constraints remains underexplored. Existing benchmarks often prioritize\nfunctional correctness, overlooking the nuanced requirements found in\nreal-world development. We introduce MultiCodeIF, a comprehensive benchmark\ndesigned to evaluate instruction-following in code generation across multiple\ndimensions: constraint type, hierarchical levels, and iterative refinement.\nBuilt upon a structured taxonomy of 9 categories and 27 constraint types,\nMultiCodeIF enables granular assessment of both functional and non-functional\ninstruction adherence. Using an automated pipeline, ConstraGen, we synthesize\nand evolve 2,021 code tasks sourced from 14 programming languages, supporting\nmulti-turn evaluation through feedback-driven task variants. Empirical\nevaluation of six state-of-the-art LLMs uncovers substantial performance\ndisparities. The top-performing model, Claude-3-7-Sonnet, achieves 63.0%\naverage constraint satisfaction, while smaller models like Qwen3-1.7B fall to\n44.8%. Models perform well on explicit constraints, but struggle with implicit\nor abstract constraints. Tasks with multiple hierarchical constraints\nsignificantly reduce model success rates, from 54.5% in single-level to just\n18.8% in multi-level scenarios. However, structured feedback enables\nprogressive improvement: average constraint satisfaction rises from 63.0% to\n83.4% over four iterative refinement rounds. MultiCodeIF provides a scalable,\nconstraint-aware, and feedback-sensitive framework to benchmark LLMs under\nrealistic code generation scenarios, bridging the gap between synthetic\nevaluations and real-world instruction complexity. The full benchmark dataset,\nevaluation pipeline, and source code are available at\nhttps://github.com/SYSUSELab/MultiCodeIF."}
{"id": "2507.00786", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.00786", "abs": "https://arxiv.org/abs/2507.00786", "authors": ["Jukka Ruohonen", "Qusai Ramadan"], "title": "Snaps: Bloated and Outdated?", "comment": "Submitted as a \"poster paper\" to APSEC", "summary": "Snap is an alternative software packaging system developed by Canonical and\nprovided by default in the Ubuntu Linux distribution. Given the heterogeneity\nof various Linux distributions and their various releases, Snap allows an\ninteroperable delivery of software directly to users. However, concerns and\ncriticism have also been frequently expressed. Regarding this criticism, the\npaper shows that currently distributed snap packages are indeed on average\nbloated in terms of their sizes and outdated in terms updating frequencies.\nWith these empirical observations, this short paper contributes to the research\ndomain of software packaging, software packages, and package managers."}
{"id": "2507.00788", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.00788", "abs": "https://arxiv.org/abs/2507.00788", "authors": ["Markus Borg", "Dave Hewett", "Nadim Hagatulah", "Noric Couderc", "Emma SÃ¶derberg", "Donald Graham", "Uttam Kini", "Dave Farley"], "title": "Echoes of AI: Investigating the Downstream Effects of AI Assistants on Software Maintainability", "comment": "Preprint of study preregistered at ICSME 2025 with In-Principal\n  Acceptance.\n  https://conf.researchr.org/track/icsme-2024/icsme-2024-registered-reports-track", "summary": "[Context] AI assistants, like GitHub Copilot and Cursor, are transforming\nsoftware engineering. While several studies highlight productivity\nimprovements, their impact on maintainability requires further investigation.\n[Objective] This study investigates whether co-development with AI assistants\naffects software maintainability, specifically how easily other developers can\nevolve the resulting source code. [Method] We conducted a two-phase controlled\nexperiment involving 151 participants, 95% of whom were professional\ndevelopers. In Phase 1, participants added a new feature to a Java web\napplication, with or without AI assistance. In Phase 2, a randomized controlled\ntrial, new participants evolved these solutions without AI assistance.\n[Results] AI-assisted development in Phase 1 led to a modest speedup in\nsubsequent evolution and slightly higher average CodeHealth. Although neither\ndifference was significant overall, the increase in CodeHealth was\nstatistically significant when habitual AI users completed Phase 1. For Phase\n1, we also observed a significant effect that corroborates previous\nproductivity findings: using an AI assistant yielded a 30.7% median decrease in\ntask completion time. Moreover, for habitual AI users, the mean speedup was\n55.9%. [Conclusions] Our study adds to the growing evidence that AI assistants\ncan effectively accelerate development. Moreover, we did not observe warning\nsigns of degraded code-level maintainability. We recommend that future research\nfocus on risks such as code bloat from excessive code generation and the\nbuild-up of cognitive debt as developers invest less mental effort during\nimplementation."}
{"id": "2507.00803", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.00803", "abs": "https://arxiv.org/abs/2507.00803", "authors": ["Gillian Daniel", "Chris Hall", "Per Hammer", "Alec-Angus Macdonald", "Hollie Marwick-Best", "Emma McKenzie", "George Popa", "Derek Somerville", "Tim Storer"], "title": "Out of the Day Job: Perspectives of Industry Practitioners in Co-Design and Delivery of Software Engineering Courses", "comment": null, "summary": "Over more than two decades, The University of Glasgow has co-designed and\ndelivered numerous software engineering focused courses with industry partners,\ncovering both technical and discipline specific professional skills. Such\ncollaborations are not unique and many of the benefits are well recognised in\nthe literature. These include enhancing the real-world relevance of curricula,\ndeveloping student professional networks ahead of graduation and easing\nrecruitment opportunities for employers.\n  However, there is relatively little scholarship on the perspectives of\nindustry practitioners who participate in course design and delivery. This gap\nis significant, since the effort invested by practitioners is often substantial\nand may require ongoing support from both the industry partner and academic\ninstitution. Understanding the motivations, expectations and experiences of\npractitioners who engage in course delivery can guide the formation of future\npartnerships and ensure their long-term sustainability.\n  We begin to address this gap by reporting on the outcomes of a retrospective\nconducted amongst the practitioner coauthors of this paper, with the academic\ncoauthors acting as facilitators. All coauthors have participated in the recent\nco-design and delivery of software engineering courses, but we choose to focus\nexplicitly on the perspectives of the practitioners. We report on the themes\nthat emerged from the discussions and our resulting recommendations for future\ncollaborations."}
{"id": "2507.00057", "categories": ["cs.PL", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.00057", "abs": "https://arxiv.org/abs/2507.00057", "authors": ["Thomas Valentin", "Ardi Madadi", "Gaetano Sapia", "Marcel BÃ¶hme"], "title": "Estimating Correctness Without Oracles in LLM-Based Code Generation", "comment": "8 pages + refs and appendix", "summary": "Generating code from natural language specifications is one of the most\nsuccessful applications of Large Language Models (LLMs). Yet, they hallucinate:\nLLMs produce outputs that may be grammatically correct but are factually\nincorrect. Without an existing, correct implementation (i.e., an oracle), can\nwe quantify how likely the generated program is correct?\n  In this paper, we propose a measure of incorrectness, called incoherence,\nthat can be estimated efficiently in the absence of an oracle and provides a\nlower bound on the error, i.e., the probability that the LLM-generated program\nfor that specification is incorrect. Our experiments demonstrate an\nextraordinary effectiveness. For the average code generation task, our\nincoherence-based methodology can automatically identify about two-thirds of\nincorrect programs without reports of false positives. In fact, an oracle-based\nevaluation of LLMs can be reliably replaced by an incoherence-based evaluation.\nIn particular, we find a very strong agreement between the ranking of LLMs by\nthe number of programs deemed correct via an oracle (pass@1) and the ranking of\nLLMs by the number of programs deemed correct via our incoherence."}
{"id": "2507.00264", "categories": ["cs.PL", "cs.DC", "cs.PF", "cs.SE", "D.2.13; D.2.8; D.3.3; B.8"], "pdf": "https://arxiv.org/pdf/2507.00264", "abs": "https://arxiv.org/abs/2507.00264", "authors": ["Isabella Basso do Amaral", "Renato Cordeiro Ferreira", "Alfredo Goldman"], "title": "Rust vs. C for Python Libraries: Evaluating Rust-Compatible Bindings Toolchains", "comment": "10 pages, 27 figures (1 diagram, 4 graphs, 9 tables, 13 code\n  listings), submitted to SBAC-PAD 2025", "summary": "The Python programming language is best known for its syntax and scientific\nlibraries, but it is also notorious for its slow interpreter. Optimizing\ncritical sections in Python entails special knowledge of the binary\ninteractions between programming languages, and can be cumbersome to interface\nmanually, with implementers often resorting to convoluted third-party\nlibraries. This comparative study evaluates the performance and ease of use of\nthe PyO3 Python bindings toolchain for Rust against ctypes and cffi. By using\nRust tooling developed for Python, we can achieve state-of-the-art performance\nwith no concern for API compatibility."}
