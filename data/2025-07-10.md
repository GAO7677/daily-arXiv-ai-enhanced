<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]
- [cs.PL](#cs.PL) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Quality attributes of test cases and test suites -- importance & challenges from practitioners' perspectives](https://arxiv.org/abs/2507.06343)
*Huynh Khanh Vi Tran,Nauman bin Ali,Michael Unterkalmsteiner,Jürgen Börstler,Panagiota Chatzipetrou*

Main category: cs.SE

TL;DR: A large-scale survey of software testers found that practitioners value certain test suite and case attributes (like Fault Detection and Usability) most, but face common challenges achieving them. These insights can guide future research and company support efforts.


<details>
  <summary>Details</summary>
Motivation: While several quality attributes of test cases and test suites are known, their relative importance in real-world practice is less clear. There is a need to understand which attributes practitioners value most and what challenges they face in achieving these attributes.

Method: The authors conducted an industrial survey using a questionnaire that was informed by a comprehensive literature review. The survey targeted a large, diverse group of software testing professionals via LinkedIn, ultimately gathering responses from 354 practitioners with varied experience levels.

Result: Practitioners identified Fault Detection, Usability, Maintainability, Reliability, and Coverage as the most important quality attributes for test cases and test suites. Opinions diverged most for Resource Efficiency, Reusability, and Simplicity, likely due to differing software testing contexts. Main challenges included inadequate attribute definition, lack of useful metrics, absence of a review process, and insufficient external support.

Conclusion: The study pinpoints areas where practitioners need more support to achieve high-quality test cases and test suites, highlighting gaps between recognized attributes and actual practices. The results offer practical guidance for both future research and industry practices to better support software testers.

Abstract: Context: The quality of the test suites and the constituent test cases
significantly impacts confidence in software testing. While research has
identified several quality attributes of test cases and test suites, there is a
need for a better understanding of their relative importance in practice.
Objective: We investigate practitioners' perceptions regarding the relative
importance of quality attributes of test cases and test suites and the
challenges they face in ensuring the perceived important quality attributes.
Method: We conducted an industrial survey using a questionnaire based on the
quality attributes identified in an extensive literature review. We used a
sampling strategy that leverages LinkedIn to draw a large and heterogeneous
sample of professionals with experience in software testing. Results: We
collected 354 responses from practitioners with a wide range of experience. We
found that the majority of practitioners rated Fault Detection, Usability,
Maintainability, Reliability, and Coverage to be the most important quality
attributes. Resource Efficiency, Reusability, and Simplicity received the most
divergent opinions, which, according to our analysis, depend on the
software-testing contexts. We identified common challenges that apply to the
important attributes, namely inadequate definition, lack of useful metrics,
lack of an established review process, and lack of external support.
Conclusion: The findings point out where practitioners actually need further
support with respect to achieving high-quality test cases and test suites under
different software testing contexts. The findings can serve as a guideline for
academic researchers when looking for research directions on the topic. The
findings can also be used to encourage companies to provide more support to
practitioners to achieve high-quality test cases and test suites.

</details>


### [2] [A proposal and assessment of an improved heuristic for the Eager Test smell detection](https://arxiv.org/abs/2507.06354)
*Huynh Khanh Vi Tran,Nauman bin Ali,Michael Unterkalmsteiner,Jürgen Börstler*

Main category: cs.SE

TL;DR: Current methods for detecting the Eager Test smell in unit tests are inadequate. This paper proposes a clearer definition and a new heuristic, which more accurately and consistently identifies these smells in Java unit tests than previous rules.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inadequacy of current detection rules for the Eager Test smell in unit tests, as practitioners find existing methods insufficient and leading to inconsistent detection results.

Method: The authors reviewed the literature to analyze definitions and detection rules for the Eager Test smell, proposed a novel and clear definition along with a heuristic to improve detection, and manually applied this heuristic to 300 Java unit test cases to evaluate its effectiveness against existing rules.

Result: The study found that previous interpretations and detection rules were often imprecise, resulting in inconsistent outcomes. The new heuristic successfully identified both eager and non-eager tests that existing rules missed, capturing the Eager Test smell more accurately.

Conclusion: The proposed heuristic provides a more precise and practitioner-aligned method for detecting the Eager Test smell, potentially solving earlier problems with inadequate detection and improving consistency in smell identification.

Abstract: Context: The evidence for the prevalence of test smells at the unit testing
level has relied on the accuracy of detection tools, which have seen intense
research in the last two decades. The Eager Test smell, one of the most
prevalent, is often identified using simplified detection rules that
practitioners find inadequate. Objective: We aim to improve the rules for
detecting the Eager Test smell. Method: We reviewed the literature on test
smells to analyze the definitions and detection rules of the Eager Test smell.
We proposed a novel, unambiguous definition of the test smell and a heuristic
to address the limitations of the existing rules. We evaluated our heuristic
against existing detection rules by manually applying it to 300 unit test cases
in Java. Results: Our review identified 56 relevant studies. We found that
inadequate interpretations of original definitions of the Eager Test smell led
to imprecise detection rules, resulting in a high level of disagreement in
detection outcomes. Also, our heuristic detected patterns of eager and
non-eager tests that existing rules missed. Conclusion: Our heuristic captures
the essence of the Eager Test smell more precisely; hence, it may address
practitioners' concerns regarding the adequacy of existing detection rules.

</details>


### [3] [Evaluating Efficiency and Novelty of LLM-Generated Code for Graph Analysis](https://arxiv.org/abs/2507.06463)
*Atieh Barati Nia,Mohammad Dindoost,David A. Bader*

Main category: cs.SE

TL;DR: The paper systematically benchmarks top LLMs on efficient C code generation for graph analysis. Claude Sonnet 4 Extended outperforms others and even beats humans on some tasks, showing LLMs are strong in optimizing known algorithms but weak at inventing new ones.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are increasingly utilized for automating software development, but prior research has primarily focused on functional correctness or on high-level languages like Python, leaving a gap in understanding their practical capability for generating efficient, low-level code, especially under strict runtime and memory demands.

Method: The authors conducted a systematic evaluation of eight leading LLMs by tasking them with generating efficient C code for graph-analysis routines. They benchmarked the models with two approaches: (1) generating algorithms that outperform current ones in benchmarks, and (2) creating graph algorithms suitable for direct integration. Performance was compared against human-written baselines, focusing on efficiency and ease of integration.

Result: Claude Sonnet 4 Extended achieved the best results for ready-to-use code generation and efficiency, even surpassing human-written baselines in the triangle counting task. The study found that while modern LLMs are effective at optimizing and integrating existing algorithms, they struggle to create genuinely novel methods.

Conclusion: Contemporary LLMs can match or exceed human experts in implementing and optimizing established algorithms under strict performance constraints but are not yet capable of inventing fundamentally new techniques. The research also supports reproducibility by providing resources like prompts, generated code, and measurement scripts.

Abstract: Large Language Models (LLMs) are increasingly used to automate software
development, yet most prior evaluations focus on functional correctness or
high-level languages such as Python. We present the first systematic study of
LLMs' ability to generate efficient C implementations of graph-analysis
routines--code that must satisfy the stringent runtime and memory constraints.
Eight state-of-the-art models (OpenAI ChatGPT o3 and o4-mini-high, Anthropic
Claude 4 Sonnet and Sonnet Extended, Google Gemini 2.5 Flash and Pro, xAI Grok
3-Think, and DeepSeek DeepThink R1) are benchmarked by two distinct approaches.
The first approach checks the ability of LLMs in generating an algorithm
outperforming other present algorithms in the benchmark. The second approach
evaluates the ability of LLMs to generate graph algorithms for integration into
the benchmark. Results show that Claude Sonnet 4 Extended achieves the best
result in the case of ready-to-use code generation and efficiency,
outperforming human-written baselines in triangle counting. The study confirms
that contemporary LLMs excel at optimizing and integrating established
algorithms but not inventing novel techniques. We provide prompts, the first
approach's generated code, and measurement scripts to foster reproducible
research.

</details>


### [4] [Issue Tracking Ecosystems: Context and Best Practices](https://arxiv.org/abs/2507.06704)
*Lloyd Montgomery*

Main category: cs.SE

TL;DR: This thesis explores the complexity and diversity of Issue Tracking Ecosystems in software engineering, showing existing solutions lack context-awareness. It introduces a Best Practice Ontology to help align research and practice for better management of these complex systems.


<details>
  <summary>Details</summary>
Motivation: Issue Tracking Systems (ITSs) like GitHub and Jira are central to software engineering organizations, supporting the management and traceability of various software artifacts and tasks. However, the broader ecosystem—termed the Issue Tracking Ecosystem (ITE)—encounters challenges related to the complexity and diversity of interconnected artifacts, stakeholders, and workflows. There is a gap in research addressing these broader, context-dependent issues in ITEs.

Method: The author interviewed practitioners and conducted archival analyses on a diverse set of ITSs to understand their broader ecosystems. The research focused on surfacing challenges and context-dependent problems within ITEs by leveraging qualitative and archival data.

Result: Findings indicate that ITE problems are highly context-dependent and that previous research has not sufficiently addressed this complexity. Existing solutions often lack a context-rich, comparable framework. To bridge this gap, the author developed the Best Practice Ontology for ITEs.

Conclusion: A deeper and more context-sensitive understanding of ITEs is essential. The creation of a Best Practice Ontology represents a step towards more consistent and aligned research and practice in the field, paving the way for improved frameworks and solutions for managing ITEs.

Abstract: Issue Tracking Systems (ITSs), such as GitHub and Jira, are popular tools
that support Software Engineering (SE) organisations through the management of
``issues'', which represent different SE artefacts such as requirements,
development tasks, and maintenance items. ITSs also support internal linking
between issues, and external linking to other tools and information sources.
This provides SE organisations key forms of documentation, including forwards
and backwards traceability (e.g., Feature Requests linked to sprint releases
and code commits linked to Bug Reports). An Issue Tracking Ecosystem (ITE) is
the aggregate of the central ITS and the related SE artefacts, stakeholders,
and processes -- with an emphasis on how these contextual factors interact with
the ITS. The quality of ITEs is central to the success of these organisations
and their software products. There are challenges, however, within ITEs,
including complex networks of interlinked artefacts and diverse workflows.
While ITSs have been the subject of study in SE research for decades, ITEs as a
whole need further exploration.
  In this thesis, I undertake the challenge of understanding ITEs at a broader
level, addressing these questions regarding complexity and diversity. I
interviewed practitioners and performed archival analysis on a diverse set of
ITSs. These analyses revealed the context-dependent nature of ITE problems,
highlighting the need for context-specific ITE research. While previous work
has produced many solutions to specific ITS problems, these solutions are not
consistently framed in a context-rich and comparable way, leading to a desire
for more aligned solutions across research and practice. To address this
emergent information and lack of alignment, I created the Best Practice
Ontology for ITEs. <... truncated due to arXiv abstract character limit ...>

</details>


### [5] [Leveraging LLMs for Semantic Conflict Detection via Unit Test Generation](https://arxiv.org/abs/2507.06762)
*Nathalia Barbosa,Paulo Borba,Léuson Da Silva*

Main category: cs.SE

TL;DR: This paper proposes using a large language model (Code Llama 70B) to generate unit tests for detecting code merge semantic conflicts, integrated into the SMAT tool. While generating tests with LLMs is still challenging for complex projects, initial results are promising for better conflict detection.


<details>
  <summary>Details</summary>
Motivation: Traditional merge tools cannot detect semantic conflicts in code that emerge when developers make parallel changes. While tools like SMAT exist, their dependency on automated unit test generation tools with limitations results in many false negatives.

Method: The authors proposed a novel unit test generation tool for semantic conflict detection in code merges, utilizing Code Llama 70B (a large language model). They integrated this tool into SMAT and evaluated it using both simple benchmarks and complex real-world systems, varying interaction strategies, prompts, and configuration parameters.

Result: LLM-based (Code Llama 70B) test generation is challenging and computationally expensive for complex scenarios, but shows promise for improving the detection of semantic conflicts compared to previous unit test generation methods.

Conclusion: Integrating LLM-based test generation with SMAT has potential for enhancing semantic conflict detection, although technical challenges remain, particularly in more complex software systems.

Abstract: Semantic conflicts arise when a developer introduces changes to a codebase
that unintentionally affect the behavior of changes integrated in parallel by
other developers. Traditional merge tools are unable to detect such conflicts,
so complementary tools like SMAT have been proposed. SMAT relies on generating
and executing unit tests: if a test fails on the base version, passes on a
developer's modified version, but fails again after merging with another
developer's changes, a semantic conflict is indicated. While SMAT is effective
at detecting conflicts, it suffers from a high rate of false negatives, partly
due to the limitations of unit test generation tools such as Randoop and
Evosuite. To investigate whether large language models (LLMs) can overcome
these limitations, we propose and integrate a new test generation tool based on
Code Llama 70B into SMAT. We explore the model's ability to generate tests
using different interaction strategies, prompt contents, and parameter
configurations. Our evaluation uses two samples: a benchmark with simpler
systems from related work, and a more significant sample based on complex,
real-world systems. We assess the effectiveness of the new SMAT extension in
detecting conflicts. Results indicate that, although LLM-based test generation
remains challenging and computationally expensive in complex scenarios, there
is promising potential for improving semantic conflict detection.
  --
  Conflitos sem^anticos surgem quando um desenvolvedor introduz mudan\c{c}as em
uma base de c\'odigo que afetam, de forma n~ao intencional, o comportamento de
altera\c{c}~oes integradas em paralelo por outros desenvolvedores. Ferramentas
tradicionais de merge n~ao conseguem detectar esse tipo de conflito, por isso
ferramentas complementares como o SMAT foram propostas. O SMAT depende da
gera\c{c}~ao e execu\c{c}~ao de testes de unidade: se um teste falha na vers~ao
base, passa na vers~ao modificada por um desenvolvedor, mas volta a falhar
ap\'os o merge com as mudan\c{c}as de outro desenvolvedor, um conflito
sem^antico \'e identificado. Embora o SMAT seja eficaz na detec\c{c}~ao de
conflitos, apresenta alta taxa de falsos negativos, em parte devido \`as
limita\c{c}~oes das ferramentas de gera\c{c}~ao de testes como Randoop e
Evosuite. Para investigar se modelos de linguagem de grande porte (LLMs) podem
superar essas limita\c{c}~oes, propomos e integramos ao SMAT uma nova
ferramenta de gera\c{c}~ao de testes baseada no Code Llama 70B. Exploramos a
capacidade do modelo de gerar testes utilizando diferentes estrat\'egias de
intera\c{c}~ao, conte\'udos de prompts e configura\c{c}~oes de par^ametros.
Nossa avalia\c{c}~ao utiliza duas amostras: um benchmark com sistemas mais
simples, usados em trabalhos relacionados, e uma amostra mais significativa
baseada em sistemas complexos e reais. Avaliamos a efic\'acia da nova extens~ao
do SMAT na detec\c{c}~ao de conflitos. Os resultados indicam que, embora a
gera\c{c}~ao de testes por LLM em cen\'arios complexos ainda seja desafiadora e
custosa computacionalmente, h\'a potencial promissor para aprimorar a
detec\c{c}~ao de conflitos sem^anticos.

</details>


### [6] [Formalization of the AADL Run-Time Services with Time](https://arxiv.org/abs/2507.06881)
*Brian R Larson,Ehsan Ahmad*

Main category: cs.SE

TL;DR: This paper extends previous AADL formalizations to explicitly include time, using modal logic and Kripke structures, and broadens runtime service support to reactive state-transition machines. Validation is provided through a HAMR-based example for BLESS behaviors.


<details>
  <summary>Details</summary>
Motivation: Previous formalizations of the Architecture Analysis & Design Language (AADL) did not account for modeling time, despite time being crucial for cyber-physical systems. The paper aims to address this gap by incorporating explicit timing into the AADL's runtime services formalization.

Method: The authors extend previous formalization efforts using modal logic defined by a Kripke structure to explicitly include time in the AADL model semantics. They also expand and adapt runtime services to support reactive state-transitions as specified in both the BA annex and the BLESS language. An example implementation using HAMR for BLESS is provided.

Result: The paper provides a simplified and time-explicit formalization of AADL runtime services. It extends the semantics to support reactive state-transition machines via the BA annex and BLESS. The approach is validated with a HAMR implementation example.

Conclusion: By integrating explicit time modeling into AADL’s formal semantics and extending support for reactive behaviors (BA and BLESS), the paper strengthens the foundation for precise design and analysis of cyber-physical systems using AADL.

Abstract: The Architecture Analysis & Design Language (AADL) is an architecture
description language for design of cyber-physical systems--machines controlled
by software. The AADL standard, SAE International AS5506D, describes Run-Time
Services (RTS) to be provided to execute AADL models in accordance with
semantics defined by the standard. The RTS of primary concern are transport
services and timing services. Although, the study presented in [1] sets a
foundation for the formal semantics of AADL, but without modeling time. This
paper extends and simplifies this formalization using a modal logic defined by
a Kripke structure, to explicitly include time. The RTS defined in the AADL
standard are also expanded to support reactive state-transition machines of the
Behavior Specification annex standard language (BA) and its closely-related,
formally-defined counterpart, the Behavior Language for Embedded Systems with
Software (BLESS). An example of AADL RTS with time, implemented by the High
Assurance Modeling and Rapid Engineering for Embedded Systems (HAMR) for
state-transition machine behavior written in BLESS, is also presented.

</details>


### [7] [Are They All Good? Evaluating the Quality of CoTs in LLM-based Code Generation](https://arxiv.org/abs/2507.06980)
*Binquan Zhang,Li Zhang,Zhiwen Luo,Yuxin Du,Fang Liu,Song Wang,Lin Shi*

Main category: cs.SE

TL;DR: The paper analyzes why chain-of-thought (CoT) prompting sometimes fails in code generation with LLMs, identifies key reasons (mostly vague task descriptions and some model misunderstanding), and shows that clarifying problems helps—improving prompt and task design is key to more reliable LLM code generation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand and improve the reliability and quality of chain-of-thought (CoT) prompting in large language models (LLMs) for code generation tasks, since CoTs act as intermediate reasoning steps that are crucial to generating correct and reliable code.

Method: The authors conduct an empirical analysis by examining 1,023 failed code samples from two common code generation benchmarks to explore both external and internal reasons behind unsatisfactory CoTs. They further study the impact of CoT quality by analyzing 210 CoT-code pairs and test the effects of refining unsatisfactory CoTs via additional LLM prompting.

Result: (1) Most CoT quality issues come from external factors such as vague requirements and lack of context (53.60%), followed by internal misunderstandings by LLMs (40.10%). (2) 18.5% of code generated from correct CoTs still contains errors due to poor instruction following, whereas 11.90% of correct outputs are nevertheless based on flawed CoTs. (3) Improving CoT quality by refining prompts and problem descriptions leads to better LLM performance in code generation.

Conclusion: CoT quality in LLM-based code generation is affected mostly by external task formulation factors, but also by internal misunderstanding. Even with correct reasoning, errors in code can arise. Refining CoTs by clarifying problem descriptions improves reliability, indicating that better task definition and prompting strategies are needed for robust CoT-based code generation.

Abstract: Large language models (LLMs) have demonstrated impressive performance in code
generation, particularly when augmented with chain-of-thought (CoT) prompting
techniques. They break down requirements into intermediate reasoning steps,
which act as design rationales to guide LLMs in writing code like human
programmers. Thus, the quality of these steps is crucial for ensuring the
correctness and reliability of the generated code. However, little is known
about the quality of CoT generated by LLMs. To what extent can we trust the
thoughts generated by LLMs? How good are they? This paper empirically explores
the external and internal factors of why LLMs generate unsatisfactory CoTs by
analyzing 1,023 failed code samples on two widely used code generation
benchmarks. We also evaluate their impact on code generation performance by
analyzing 210 CoT-code pairs and refining the unsatisfied CoTs by prompting
LLMs. Our study reveals three key findings: (1) External factors (53.60%), such
as unclear requirements and lack of context, mainly affect CoT quality, while
internal factors (40.10%) stem from LLMs' misunderstanding prompts. (2) Even
when CoTs are correct, 18.5% of the generated code contains errors due to
instruction-following issues; conversely, 11.90% of correct code is paired with
flawed CoTs. (3) Refining low-quality CoTs is feasible, i.e., LLMs improve when
given detailed problem descriptions. These findings highlight key challenges in
CoT-based code generation and suggest directions for improving LLM reasoning
and reliability.

</details>


### [8] [Exploring Fairness Interventions in Open Source Projects](https://arxiv.org/abs/2507.07026)
*Sadia Afrin Mim,Fatema Tuz Zohra,Justin Smith,Brittany Johnson*

Main category: cs.SE

TL;DR: The paper analyzed the landscape of 62 open-source ML fairness interventions, finding that only a third are actively maintained and half support both detection and mitigation of bias. Lack of practitioner awareness and tool maintenance limits their practical use.


<details>
  <summary>Details</summary>
Motivation: Biased machine learning models cause adverse effects in important areas such as criminal justice and healthcare. Although many fairness interventions have been developed, practitioners often remain unaware of their existence, limiting their real-world adoption.

Method: The authors systematically identified and compiled a dataset of 62 open-source machine learning fairness interventions, assessed their activity and maintenance, and performed an in-depth analysis of their features and specifications.

Result: They found that only 32% of these interventions have been actively maintained in the past year, and half provide both bias detection and mitigation, mostly focusing on in-processing approaches.

Conclusion: Despite a variety of available open-source fairness interventions, their real-world impact is limited by low adoption and maintenance. Active efforts are needed to increase awareness and support for these tools among practitioners.

Abstract: The deployment of biased machine learning (ML) models has resulted in adverse
effects in crucial sectors such as criminal justice and healthcare. To address
these challenges, a diverse range of machine learning fairness interventions
have been developed, aiming to mitigate bias and promote the creation of more
equitable models. Despite the growing availability of these interventions,
their adoption in real-world applications remains limited, with many
practitioners unaware of their existence. To address this gap, we
systematically identified and compiled a dataset of 62 open source fairness
interventions and identified active ones. We conducted an in-depth analysis of
their specifications and features to uncover considerations that may drive
practitioner preference and to identify the software interventions actively
maintained in the open source ecosystem. Our findings indicate that 32% of
these interventions have been actively maintained within the past year, and 50%
of them offer both bias detection and mitigation capabilities, mostly during
inprocessing.

</details>


### [9] [5C Prompt Contracts: A Minimalist, Creative-Friendly, Token-Efficient Design Framework for Individual and SME LLM Usage](https://arxiv.org/abs/2507.07045)
*Ugur Ari*

Main category: cs.SE

TL;DR: The paper proposes the 5C Prompt Contract, a simple framework for designing prompts for LLMs that boosts efficiency and creativity while reducing overhead, making it ideal for users and SMEs with limited resources.


<details>
  <summary>Details</summary>
Motivation: As Large Language Models (LLMs) become increasingly embedded in critical applications, there is a growing need for prompt design frameworks that are systematic, explicit, and practical, without excessive complexity or token overhead.

Method: The paper introduces the 5C Prompt Contract, a prompt design framework composed of five components: Character, Cause, Constraint, Contingency, and Calibration. This framework aims to integrate fallback and output optimization directly into the prompt, promoting clarity and efficiency.

Result: Experimental results show that the 5C framework offers superior input token efficiency and maintains rich, consistent outputs across various LLM platforms (OpenAI, Anthropic, DeepSeek, Gemini). It is especially beneficial for users and SMEs with limited resources.

Conclusion: The 5C Prompt Contract provides a minimal but comprehensive and interpretable schema for prompt design, improving efficiency and flexibility in LLM interactions. It is practical, widely applicable, and supports creative outcomes without introducing significant overhead.

Abstract: The progression from traditional prompt engineering to a more rigorous
discipline of prompt design marks a pivotal shift in human-LLM interaction. As
Large Language Models (LLMs) become increasingly embedded in mission-critical
applications, there emerges a pressing need for frameworks that are not only
explicit and systematic but also minimal enough to remain practical and broadly
accessible. While many existing approaches address prompt structuring through
elaborate Domain-Specific Languages (DSLs) or multi-layered templates, such
methods can impose significant token and cognitive overhead, potentially
constraining the model's creative capacity. In this context, we propose the 5C
Prompt Contract, a framework that distills prompt design into five intuitive
components: Character, Cause, Constraint, Contingency, and Calibration. This
minimal cognitive schema explicitly integrates fallback and output optimization
directives, fostering reliable, interpretable, and creatively flexible AI
interactions. Experimental results demonstrate that the 5C framework
consistently achieves superior input token efficiency while maintaining rich
and consistent outputs across diverse LLM architectures (OpenAI, Anthropic,
DeepSeek, and Gemini), making it particularly suited for individuals and
Small-to-Medium Enterprises (SMEs) with limited AI engineering resources.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [10] [Pyrosome: Verified Compilation for Modular Metatheory](https://arxiv.org/abs/2507.06360)
*Dustin Jamner,Gabriel Kammer,Ritam Nag,Adam Chlipala*

Main category: cs.PL

TL;DR: Pyrosome is a Coq-based framework for modular language metatheory that simplifies the development and extension of verified compilers. It allows for easy language and compiler extension, reusing existing correctness proofs, and supports both functional and imperative language features through a flexible, deeply embedded approach.


<details>
  <summary>Details</summary>
Motivation: Current techniques for semantic reasoning about programming languages and their compilers are often tightly coupled to specific languages or compiler structures, making extensibility and reuse of proofs difficult. There is a need for a flexible framework that supports easy language and compiler extension while preserving correctness guarantees.

Method: The authors propose Pyrosome, a framework implemented in Coq, that enables modular language metatheory using a novel inductive formulation of equivalence preservation. Pyrosome represents programming languages as deeply embedded systems with semantics described by dependently sorted equational theories. The framework allows for the vertical composition of compilers and incremental extension of languages and their compilers, while preserving and reusing existing correctness theorems.

Result: Pyrosome allows verified compilers to be fully extensible; adding new features only requires defining and verifying the compilation of the new feature, with existing correctness proofs remaining valid for unchanged aspects. As a demonstration, the authors start from a compiler for simply typed lambda calculus and incrementally add features such as natural numbers, recursive functions, and a heap, extending judgments as needed. They also present a linear-style CPS translation and compilation of a small imperative language with substructural typing, showing the versatility of the framework.

Conclusion: Pyrosome provides an effective, extensible foundation for formalizing language semantics and verified compilation. It enables modular reasoning and proof reuse across language extensions, supporting both functional and imperative features, and simplifies the development and verification of complex, extensible compilers.

Abstract: We present Pyrosome, a generic framework for modular language metatheory that
embodies a novel approach to extensible semantics and compilation, implemented
in Coq. Common techniques for semantic reasoning are often tied to the specific
structures of the languages and compilers that they support. In Pyrosome,
verified compilers are fully extensible, meaning that to extend a language
(even with a new kind of effect) simply requires defining and verifying the
compilation of the new feature, reusing the old correctness theorem for all
other cases. The novel enabling idea is an inductive formulation of equivalence
preservation that supports the addition of new rules to the source language,
target language, and compiler.
  Pyrosome defines a formal, deeply embedded notion of programming languages
with semantics given by dependently sorted equational theories, so all
compiler-correctness proofs boil down to type-checking and equational
reasoning. We support vertical composition of any compilers expressed in our
framework in addition to feature extension. As a case study, we present a
multipass compiler from System F with simple references, through CPS
translation and closure conversion. Specifically, we demonstrate how we can
build such a compiler incrementally by starting with a compiler for simply
typed lambda-calculus and adding natural numbers, the unit type, recursive
functions, and a global heap, then extending judgments with a type environment
and adding type abstraction, all while reusing the original theorems. We also
present a linear version of the simply typed CPS pass and compile a small
imperative language to the simply typed target to show how Pyrosome handles
substructural typing and imperative features.

</details>


### [11] [Fast Collection Operations from Indexed Stream Fusion](https://arxiv.org/abs/2507.06456)
*Scott Kovach,Praneeth Kolichala,Kyle A. Miller,David Broman,Fredrik Kjolstad*

Main category: cs.PL

TL;DR: The paper presents a system for efficiently traversing and combining associative collection data structures. Unlike other approaches, it does not require special compiler support. It uses indexed streams to avoid extra allocations, is implemented in Lean, Morphic, and Rust, and is formally proven correct in Lean.


<details>
  <summary>Details</summary>
Motivation: Current approaches for traversing and combining associative collection data structures often require specialized compiler features or staged compilation to achieve both efficiency and composability. There is a need for a method that simplifies library usage without sacrificing performance or requiring complex infrastructure.

Method: The authors design a system using indexed streams to represent traversals and combinations of associative collections. This system eliminates the need for intermediate allocations and does not depend on specialized compiler support. The library is implemented in Lean, Morphic, and Rust, with functional correctness formally proven in Lean.

Result: The proposed library efficiently supports complex joins on a variety of input collections without intermediate allocations or the need for advanced compiler infrastructure. Its functional correctness is validated by a mechanized proof in Lean.

Conclusion: The presented system offers an efficient, composable, and practically implementable solution for associative collection traversal and combination across multiple languages, verified by rigorous formal proof.

Abstract: We present a system of efficient methods for traversing and combining
associative collection data structures. A distinguishing feature of the system
is that, like traditional sequential iterator libraries, it does not require
specialized compiler infrastructure or staged compilation for efficiency and
composability. By using a representation based on indexed streams, the library
can express complex joins over input collections while using no intermediate
allocations. We implement the library for the Lean, Morphic, and Rust
programming languages and provide a mechanized proof of functional correctness
in Lean.

</details>


### [12] [Finding Compiler Bugs through Cross-Language Code Generator and Differential Testing](https://arxiv.org/abs/2507.06584)
*Qiong Feng,Xiaotian Ma,Ziyuan Feng,Marat Akhin,Wei Song,Peng Liang*

Main category: cs.PL

TL;DR: The paper introduces a novel fuzzing framework for finding bugs in compilers used for cross-language JVM-based development, successfully uncovering multiple bugs and highlighting the importance of rigorous cross-language compiler verification.


<details>
  <summary>Details</summary>
Motivation: While compiler correctness is vital for reliable code, most prior research addresses only single-language compilation, ignoring the growing importance of cross-language compilation scenarios that occur when different JVM-based languages interact. This leaves a major gap in ensuring multi-language code reliability.

Method: The authors present CrossLangFuzzer, a new framework that uses a universal intermediate representation (IR) for JVM languages to automatically generate and mutate cross-language test programs. The approach includes three mutation strategies (LangShuffler, FunctionRemoval, and TypeChanger) to increase test diversity, and executes both the original and mutated programs across several compiler versions to find bugs.

Result: CrossLangFuzzer discovered 24 compiler bugs across major JVM-based languages (Kotlin, Groovy, Scala 2, Scala 3, Java), with the TypeChanger mutator proving to be the most effective. The authors also provide an analysis of the symptoms and causes of the cross-language compilation bugs and highlight how responsibilities are divided between compilers.

Conclusion: This work pioneers systematic testing for cross-language compiler correctness, demonstrating the effectiveness of IR-based fuzzing for discovering real bugs, and lays groundwork for higher reliability in multi-language software development environments.

Abstract: Compilers play a central role in translating high-level code into executable
programs, making their correctness essential for ensuring code safety and
reliability. While extensive research has focused on verifying the correctness
of compilers for single-language compilation, the correctness of cross-language
compilation - which involves the interaction between two languages and their
respective compilers - remains largely unexplored. To fill this research gap,
we propose CrossLangFuzzer, a novel framework that introduces a universal
intermediate representation (IR) for JVM-based languages and automatically
generates cross-language test programs with diverse type parameters and complex
inheritance structures. After generating the initial IR, CrossLangFuzzer
applies three mutation techniques - LangShuffler, FunctionRemoval, and
TypeChanger - to enhance program diversity. By evaluating both the original and
mutated programs across multiple compiler versions, CrossLangFuzzer
successfully uncovered 10 confirmed bugs in the Kotlin compiler, 4 confirmed
bugs in the Groovy compiler, 7 confirmed bugs in the Scala 3 compiler, 2
confirmed bugs in the Scala 2 compiler, and 1 confirmed bug in the Java
compiler. Among all mutators, TypeChanger is the most effective, detecting 11
of the 24 compiler bugs. Furthermore, we analyze the symptoms and root causes
of cross-compilation bugs, examining the respective responsibilities of
language compilers when incorrect behavior occurs during cross-language
compilation. To the best of our knowledge, this is the firstwork specifically
focused on identifying and diagnosing compiler bugs in cross-language
compilation scenarios. Our research helps to understand these challenges and
contributes to improving compiler correctness in multi-language environments.

</details>


### [13] [Sound Interval-Based Synthesis for Probabilistic Programs](https://arxiv.org/abs/2507.06939)
*Guilherme Espada,Alcides Fonseca*

Main category: cs.PL

TL;DR: The paper introduces a type-directed synthesis method for probabilistic programming, which automatically generates valid and type-safe programs, outperforming random and data-guided methods, and making probabilistic modeling more accessible to non-statisticians.


<details>
  <summary>Details</summary>
Motivation: Domain practitioners often lack deep statistical expertise, making it difficult to select suitable probabilistic models. Current probabilistic programming tools require users to be both domain experts and statistical experts. Automating model selection would make probabilistic programming more accessible, but this is challenging due to the large and complex search space of possible programs, many of which are invalid due to their probabilistic nature.

Method: The authors propose a type system to statically reject invalid probabilistic programs and a type-directed synthesis algorithm that ensures type-safe programs are generated by construction. Additionally, they implement a heuristic search technique to efficiently explore the vast space of possible programs.

Result: By evaluating their approach on a collection of probabilistic programs from the literature, the authors show that their method outperforms both a type-agnostic random search and a data-guided method (DaPPer), especially with more complex programs. Their method enables faster program synthesis and makes techniques like Genetic Programming feasible for use in probabilistic programming.

Conclusion: The proposed method significantly improves the automation, efficiency, and reliability of probabilistic model selection and program synthesis, lowering the barrier for domain experts to apply probabilistic programming without needing deep statistical knowledge.

Abstract: Probabilistic programming has become a standard practice to model stochastic
events and learn about the behavior of nature in different scientific contexts,
ranging from Genetics and Ecology to Linguistics and Psychology. However,
domain practitioners (such as biologists) also need to be experts in statistics
in order to select which probabilistic model is suitable for a given particular
problem, relying then on probabilistic inference engines such as Stan, Pyro or
Edward to fine-tune the parameters of that particular model. Probabilistic
Programming would be more useful if the model selection is made automatic,
without requiring statistics expertise from the end user. Automatically
selecting the model is challenging because of the large search space of
probabilistic programs needed to be explored, because the fact that most of
that search space contains invalid programs, and because invalid programs may
only be detected in some executions, due to its probabilistic nature. We
propose a type system to statically reject invalid probabilistic programs, a
type-directed synthesis algorithm that guarantees that generated programs are
type-safe by construction, and an heuristic search procedure to handle the vast
search space. We collect a number of probabilistic programs from the
literature, and use them to compare our method with both a type-agnostic random
search, and a data-guided method from the literature (DaPPer). Our results show
that our technique both outperforms random search and DaPPer, specially on more
complex programs. This drastic performance difference in synthesis allows for
fast sampling of programs and enables techniques that previously suffered from
the complexity of synthesis, such as Genetic Programming, to be applied.

</details>
