{"id": "2506.20754", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.20754", "abs": "https://arxiv.org/abs/2506.20754", "authors": ["Marina Ara\u00fajo", "J\u00falia Ara\u00fajo", "Romeu Oliveira", "Lucas Romao", "Marcos Kalinowski"], "title": "Domain Knowledge in Requirements Engineering: A Systematic Mapping Study", "comment": "Accepted for publication at the 51st Euromicro Conference Series on\n  Software Engineering and Advanced Applications (SEAA) 2025", "summary": "[Context] Domain knowledge is recognized as a key component for the success\nof Requirements Engineering (RE), as it provides the conceptual support needed\nto understand the system context, ensure alignment with stakeholder needs, and\nreduce ambiguity in requirements specification. Despite its relevance, the\nscientific literature still lacks a systematic consolidation of how domain\nknowledge can be effectively used and operationalized in RE. [Goal] This paper\naddresses this gap by offering a comprehensive overview of existing\ncontributions, including methods, techniques, and tools to incorporate domain\nknowledge into RE practices. [Method] We conducted a systematic mapping study\nusing a hybrid search strategy that combines database searches with iterative\nbackward and forward snowballing. [Results] In total, we found 75 papers that\nmet our inclusion criteria. The analysis highlights the main types of\nrequirements addressed, the most frequently considered quality attributes, and\nrecurring challenges in the formalization, acquisition, and long-term\nmaintenance of domain knowledge. The results provide support for researchers\nand practitioners in identifying established approaches and unresolved issues.\nThe study also outlines promising directions for future research, emphasizing\nthe development of scalable, automated, and sustainable solutions to integrate\ndomain knowledge into RE processes. [Conclusion] The study contributes by\nproviding a comprehensive overview that helps to build a conceptual and\nmethodological foundation for knowledge-driven requirements engineering.", "AI": {"tldr": "The paper systematically maps literature on using domain knowledge in Requirements Engineering, identifies current methods and challenges, and points out future research directions for scalable and sustainable knowledge integration.", "motivation": "Domain knowledge is crucial for successful Requirements Engineering (RE) as it enhances understanding, alignment with stakeholder needs, and clarity in requirements. However, there is a lack of systematic analysis on how to effectively operationalize domain knowledge in RE.", "method": "A systematic mapping study was conducted, using a hybrid approach that combined database searches with iterative backward and forward snowballing to identify relevant literature.", "result": "75 papers were identified and analyzed, revealing key types of requirements, common quality attributes, and challenges such as formalization, acquisition, and maintenance of domain knowledge. The research also points out gaps and future directions for scalable and automated knowledge integration in RE.", "conclusion": "This study offers a comprehensive conceptual and methodological overview for utilizing domain knowledge in RE, thereby supporting future research and practice in knowledge-driven requirements engineering."}}
{"id": "2506.20759", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20759", "abs": "https://arxiv.org/abs/2506.20759", "authors": ["Lucas Romao", "Hugo Villamizar", "Romeu Oliveira", "Silvio Alonso", "Marcos Kalinowski"], "title": "Agile Management for Machine Learning: A Systematic Mapping Study", "comment": "Accepted for publication at the 51st Euromicro Conference Series on\n  Software Engineering and Advanced Applications (SEAA) 2025", "summary": "[Context] Machine learning (ML)-enabled systems are present in our society,\ndriving significant digital transformations. The dynamic nature of ML\ndevelopment, characterized by experimental cycles and rapid changes in data,\nposes challenges to traditional project management. Agile methods, with their\nflexibility and incremental delivery, seem well-suited to address this\ndynamism. However, it is unclear how to effectively apply these methods in the\ncontext of ML-enabled systems, where challenges require tailored approaches.\n[Goal] Our goal is to outline the state of the art in agile management for\nML-enabled systems. [Method] We conducted a systematic mapping study using a\nhybrid search strategy that combines database searches with backward and\nforward snowballing iterations. [Results] Our study identified 27 papers\npublished between 2008 and 2024. From these, we identified eight frameworks and\ncategorized recommendations and practices into eight key themes, such as\nIteration Flexibility, Innovative ML-specific Artifacts, and the Minimal Viable\nModel. The main challenge identified across studies was accurate effort\nestimation for ML-related tasks. [Conclusion] This study contributes by mapping\nthe state of the art and identifying open gaps in the field. While relevant\nwork exists, more robust empirical evaluation is still needed to validate these\ncontributions.", "AI": {"tldr": "This paper systematically maps current research on applying agile methods to ML-enabled systems, highlighting existing frameworks, categorizing key practices, and identifying ongoing challenges, especially in effort estimation. It also calls for more empirical validation in the field.", "motivation": "The motivation is to address the mismatch between traditional project management methods and the dynamic, experimental nature of machine learning (ML) system development, and to explore how agile methodologies can be effectively adapted for ML-enabled systems.", "method": "The paper uses a systematic mapping study with a hybrid search strategy, combining database searches with backward and forward snowballing iterations, to review existing literature.", "result": "The study identified 27 relevant papers from 2008-2024, out of which eight frameworks were found. Recommendations and practices were categorized into eight key themes. The primary challenge noted was effort estimation for ML tasks.", "conclusion": "The paper provides a comprehensive mapping of the state of the art in agile management for ML-enabled systems and points out current research gaps, specifically a need for more empirical validation of proposed frameworks and practices."}}
{"id": "2506.20851", "categories": ["cs.SE", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2506.20851", "abs": "https://arxiv.org/abs/2506.20851", "authors": ["Srikar Reddy Gadusu", "Larry Callahan", "Samir Lababidi", "Arunasri Nishtala", "Sophia Healey", "Hande McGinty"], "title": "Generating Reliable Adverse event Profiles for Health through Automated Integrated Data (GRAPH-AID): A Semi-Automated Ontology Building Approach", "comment": null, "summary": "As data and knowledge expand rapidly, adopting systematic methodologies for\nontology generation has become crucial. With the daily increases in data\nvolumes and frequent content changes, the demand for databases to store and\nretrieve information for the creation of knowledge graphs has become\nincreasingly urgent. The previously established Knowledge Acquisition and\nRepresentation Methodology (KNARM) outlines a systematic approach to address\nthese challenges and create knowledge graphs. However, following this\nmethodology highlights the existing challenge of seamlessly integrating Neo4j\ndatabases with the Web Ontology Language (OWL). Previous attempts to integrate\ndata from Neo4j into an ontology have been discussed, but these approaches\noften require an understanding of description logics (DL) syntax, which may not\nbe familiar to many users. Thus, a more accessible method is necessary to\nbridge this gap. This paper presents a user-friendly approach that utilizes\nPython and its rdflib library to support ontology development. We showcase our\nnovel approach through a Neo4j database we created by integrating data from the\nFood and Drug Administration (FDA) Adverse Event Reporting System (FAERS)\ndatabase. Using this dataset, we developed a Python script that automatically\ngenerates the required classes and their axioms, facilitating a smoother\nintegration process. This approach offers a practical solution to the\nchallenges of ontology generation in the context of rapidly growing adverse\ndrug event datasets, supporting improved drug safety monitoring and public\nhealth decision-making.", "AI": {"tldr": "The paper introduces an accessible Python-based approach for integrating Neo4j graph databases with OWL ontologies, demonstrated using FDA adverse event data, to simplify and streamline ontology creation for improved drug safety monitoring.", "motivation": "As data and knowledge continue to grow rapidly, particularly in the biomedical domain, there is a pressing need for systematic methodologies to generate ontologies and knowledge graphs efficiently. The integration between graph databases like Neo4j and standard ontology languages such as OWL is challenging, especially for users unfamiliar with description logics. This complexity necessitates more accessible tools to facilitate ontology development and data integration.", "method": "The paper builds upon the Knowledge Acquisition and Representation Methodology (KNARM) and proposes a user-friendly approach using Python and its rdflib library. The authors created a Neo4j database by integrating data from the FDA Adverse Event Reporting System (FAERS), then developed a Python script to automatically generate ontology classes and axioms from this database.", "result": "The approach demonstrated successful integration of data from Neo4j into an OWL ontology, streamlining the process and minimizing the need for specialized knowledge in description logics. The Python script facilitated automatic ontology generation, making the process more accessible for users with limited technical background.", "conclusion": "The presented method offers a practical, user-friendly solution for ontology development, particularly suitable for rapidly changing and expanding datasets like adverse drug event records. This enhances ontology generation and supports better drug safety monitoring and public health decision-making."}}
{"id": "2506.20869", "categories": ["cs.SE", "cs.AI", "cs.IR", "D.2.11; I.2.6; H.3.3"], "pdf": "https://arxiv.org/pdf/2506.20869", "abs": "https://arxiv.org/abs/2506.20869", "authors": ["Md Toufique Hasan", "Muhammad Waseem", "Kai-Kristian Kemell", "Ayman Asad Khan", "Mika Saari", "Pekka Abrahamsson"], "title": "Engineering RAG Systems for Real-World Applications: Design, Development, and Evaluation", "comment": "Accepted as a full paper to the 51st Euromicro Conference on Software\n  Engineering and Advanced Applications (SEAA 2025). 9 pages, 4 figures. This\n  is the preprint version and not the final camera ready version", "summary": "Retrieval-Augmented Generation (RAG) systems are emerging as a key approach\nfor grounding Large Language Models (LLMs) in external knowledge, addressing\nlimitations in factual accuracy and contextual relevance. However, there is a\nlack of empirical studies that report on the development of RAG-based\nimplementations grounded in real-world use cases, evaluated through general\nuser involvement, and accompanied by systematic documentation of lessons\nlearned. This paper presents five domain-specific RAG applications developed\nfor real-world scenarios across governance, cybersecurity, agriculture,\nindustrial research, and medical diagnostics. Each system incorporates\nmultilingual OCR, semantic retrieval via vector embeddings, and domain-adapted\nLLMs, deployed through local servers or cloud APIs to meet distinct user needs.\nA web-based evaluation involving a total of 100 participants assessed the\nsystems across six dimensions: (i) Ease of Use, (ii) Relevance, (iii)\nTransparency, (iv) Responsiveness, (v) Accuracy, and (vi) Likelihood of\nRecommendation. Based on user feedback and our development experience, we\ndocumented twelve key lessons learned, highlighting technical, operational, and\nethical challenges affecting the reliability and usability of RAG systems in\npractice.", "AI": {"tldr": "This paper develops and evaluates five domain-specific RAG systems in practical fields, involving 100 users, and documents twelve major lessons about the real-world technical, operational, and ethical challenges of such deployments, providing comprehensive guidance for future RAG applications.", "motivation": "Retrieval-Augmented Generation (RAG) systems are promising for improving Large Language Models (LLMs) by allowing them to ground their outputs in external knowledge, thereby addressing issues with factuality and contextual relevance. However, there is little practical research on real-world applications, user-involved evaluations, and the documentation of practical lessons learned from such deployments.", "method": "The authors developed five domain-specific RAG applications for governance, cybersecurity, agriculture, industrial research, and medical diagnostics. Each system included multilingual OCR, semantic retrieval using vector embeddings, and domain-adapted LLMs, delivered through local servers or cloud APIs. The applications were evaluated by 100 participants through a web-based assessment covering six usability and functionality dimensions. The authors then distilled twelve key lessons learned from their evaluation and development experiences.", "result": "The RAG systems were evaluated across ease of use, relevance, transparency, responsiveness, accuracy, and recommendation likelihood by a diverse user base. The assessment led to the identification of twelve critical lessons learned, addressing technical, operational, and ethical issues that impact the effectiveness and user acceptance of RAG-based systems.", "conclusion": "RAG-based systems can be successfully implemented in diverse, real-world domains by integrating domain-specific features and adapting to user needs. However, their deployment raises significant technical, operational, and ethical challenges which need to be systematically addressed for broader, reliable adoption. The documented lessons provide valuable guidance for future practitioners and researchers developing similar systems."}}
{"id": "2506.20883", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20883", "abs": "https://arxiv.org/abs/2506.20883", "authors": ["Kyanna Dagenais", "Istvan David"], "title": "Complex Model Transformations by Reinforcement Learning with Uncertain Human Guidance", "comment": "Accepted for ACM/IEEE MODELS'25", "summary": "Model-driven engineering problems often require complex model transformations\n(MTs), i.e., MTs that are chained in extensive sequences. Pertinent examples of\nsuch problems include model synchronization, automated model repair, and design\nspace exploration. Manually developing complex MTs is an error-prone and often\ninfeasible process. Reinforcement learning (RL) is an apt way to alleviate\nthese issues. In RL, an autonomous agent explores the state space through trial\nand error to identify beneficial sequences of actions, such as MTs. However, RL\nmethods exhibit performance issues in complex problems. In these situations,\nhuman guidance can be of high utility. In this paper, we present an approach\nand technical framework for developing complex MT sequences through RL, guided\nby potentially uncertain human advice. Our framework allows user-defined MTs to\nbe mapped onto RL primitives, and executes them as RL programs to find optimal\nMT sequences. Our evaluation shows that human guidance, even if uncertain,\nsubstantially improves RL performance, and results in more efficient\ndevelopment of complex MTs. Through a trade-off between the certainty and\ntimeliness of human advice, our method takes a step towards RL-driven\nhuman-in-the-loop engineering methods.", "AI": {"tldr": "The paper proposes a framework where reinforcement learning, guided by possibly uncertain human input, is used to efficiently develop complex model transformation sequences in engineering tasks. Their results show that even uncertain human advice helps RL work better and faster.", "motivation": "Model-driven engineering often involves chaining together complex model transformations (MTs), which is a difficult, error-prone process if done manually. There is a need for a better approach to efficiently develop and optimize these complex MT sequences.", "method": "The paper introduces a framework that integrates reinforcement learning (RL) with the ability to incorporate human guidance\u2014even if the advice is uncertain\u2014while developing sequences of model transformations. The framework maps user-defined MTs to RL primitives, executes them as RL workflows, and optimizes MT sequences through RL, leveraging human input to guide learning.", "result": "The evaluation demonstrates that incorporating human guidance significantly boosts the performance of RL in sequencing complex model transformations. Even when the human advice is not fully certain, it still helps RL find optimal or more efficient solutions faster.", "conclusion": "The proposed approach enables more efficient and reliable development of complex MTs in model-driven engineering by blending RL with human-in-the-loop input. The framework effectively manages trade-offs between the certainty and timeliness of advice, paving the way for advanced human-guided, RL-driven engineering processes."}}
{"id": "2506.21014", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.21014", "abs": "https://arxiv.org/abs/2506.21014", "authors": ["Shaojian Qiu", "Mengyang Huang", "Jiahao Cheng"], "title": "Boosting Vulnerability Detection with Inter-function Multilateral Association Insights", "comment": null, "summary": "Vulnerability detection is a crucial yet challenging technique for ensuring\nthe security of software systems. Currently, most deep learning-based\nvulnerability detection methods focus on stand-alone functions, neglecting the\ncomplex inter-function interrelations, particularly the multilateral\nassociations. This oversight can fail to detect vulnerabilities in these\ninterrelations. To address this gap, we present an Inter-Function Multilateral\nAssociation analysis framework for Vulnerability Detection (IFMA-VD). The\ncornerstone of the IFMA-VD lies in constructing a code behavior hypergraph and\nutilizing hyperedge convolution to extract multilateral association features.\nSpecifically, we first parse functions into a code property graph to generate\nintra-function features. Following this, we construct a code behavior\nhypergraph by segmenting the program dependency graph to isolate and encode\nbehavioral features into hyperedges. Finally, we utilize a hypergraph network\nto capture the multilateral association knowledge for augmenting vulnerability\ndetection. We evaluate IFMA-VD on three widely used vulnerability datasets and\ndemonstrate improvements in F-measure and Recall compared to baseline methods.\nAdditionally, we illustrate that multilateral association features can boost\ncode feature representation and validate the effectiveness of IFMA-VD on\nreal-world datasets.", "AI": {"tldr": "A new hypergraph-based method, IFMA-VD, captures complex inter-function relationships for better vulnerability detection and outperforms previous approaches on multiple datasets.", "motivation": "Existing deep learning methods for vulnerability detection mostly focus on stand-alone functions and overlook complex relationships and interactions between functions. This limitation makes it difficult to detect vulnerabilities that are due to these interrelations.", "method": "The paper proposes a framework called IFMA-VD, which builds a code behavior hypergraph to model inter-function multilateral associations. Functions are first parsed into code property graphs to get intra-function features. A program dependency graph is segmented to encode behavioral features into hyperedges. A hypergraph network is then used to extract multilateral association features and improve vulnerability detection.", "result": "IFMA-VD was tested on three popular vulnerability datasets. The evaluation shows that IFMA-VD achieves better F-measure and Recall compared to existing baseline methods. The framework's use of multilateral association features also leads to improvements in code feature representation. Effectiveness was demonstrated both on standard benchmarks and real-world datasets.", "conclusion": "Modeling multilateral inter-function associations using a hypergraph-based approach enhances vulnerability detection. IFMA-VD outperforms existing methods and effectively leverages complex code relationships."}}
{"id": "2506.21138", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21138", "abs": "https://arxiv.org/abs/2506.21138", "authors": ["Abdelkarim El-Hajjami", "Camille Salinesi"], "title": "How Good Are Synthetic Requirements ? Evaluating LLM-Generated Datasets for AI4RE", "comment": null, "summary": "The shortage of publicly available, labeled requirements datasets remains a\nmajor barrier to advancing Artificial Intelligence for Requirements Engineering\n(AI4RE). While Large Language Models offer promising capabilities for synthetic\ndata generation, systematic approaches to control and optimize the quality of\ngenerated requirements remain underexplored. This paper presents Synthline v1,\nan enhanced Product Line approach for generating synthetic requirements data\nthat extends our earlier v0 version with advanced generation strategies and\ncuration techniques. We investigate four research questions assessing how\nprompting strategies, automated prompt optimization, and post-generation\ncuration affect data quality across four classification tasks: defect\ndetection, functional vs. non-functional, quality vs. non-quality, and security\nvs. non-security. Our evaluation shows that multi-sample prompting\nsignificantly boosts both utility and diversity over single-sample generation,\nwith F1-score gains from 6 to 44 points. The use of PACE (Prompt Actor-Critic\nEditing) for automated prompt optimization yields task-dependent results,\ngreatly improving functional classification (+32.5 points) but reducing\nperformance on others. Interestingly, similarity-based curation improves\ndiversity but often harms classification performance, indicating that some\nredundancy may help ML models. Most importantly, our results show that\nsynthetic requirements can match or outperform human-authored ones for specific\ntasks, with synthetic data surpassing human data for security (+7.8 points) and\ndefect classification (+15.4 points). These findings offer practical insights\nfor AI4RE and chart a viable path to mitigating dataset scarcity through\nsystematic synthetic generation.", "AI": {"tldr": "Synthline v1 systematically improves synthetic requirements data generation for AI in Requirements Engineering, often matching or outperforming human data in key classification tasks, offering a practical approach to overcome data scarcity in the field.", "motivation": "There is a lack of publicly available, labeled requirements datasets, which impedes progress in applying AI to Requirements Engineering (AI4RE). Although Large Language Models (LLMs) can generate synthetic data, there is limited understanding of how to systematically improve the quality of such generated requirements.", "method": "The authors present Synthline v1, an improved Product Line approach for generating synthetic requirements data, which builds on their earlier version. They use advanced generation strategies and curation techniques, and systematically examine how different prompting strategies, automated prompt optimization (PACE), and post-generation curation affect data quality. The evaluation covers four classification tasks: defect detection, functional vs. non-functional, quality vs. non-quality, and security vs. non-security.", "result": "Multi-sample prompting raises utility and diversity, with significant F1-score improvements. PACE improves performance for functional classification but can decrease it for other tasks. Similarity-based curation boosts diversity but may lower classification scores, suggesting that redundancy in data can benefit machine learning models. In some cases, synthetic data generated by their method outperforms human-authored data, notably in security and defect classification.", "conclusion": "Systematic synthetic generation using advanced strategies, automated optimization, and thoughtful curation can produce high-quality requirements data, sometimes even surpassing human-authored examples in specific tasks. These methods offer a practical solution to the shortage of labeled requirements datasets in AI4RE."}}
{"id": "2506.21211", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21211", "abs": "https://arxiv.org/abs/2506.21211", "authors": ["Quanming Liu", "Xupeng Bu", "Zhichao Yan", "Ru Li"], "title": "$T^3$: Multi-level Tree-based Automatic Program Repair with Large Language Models", "comment": null, "summary": "Automatic Program Repair (APR) is a core technology in software development\nand maintenance, with aims to enable automated defect repair with minimal human\nintervention. In recent years, the substantial advancements in Large Language\nModels (LLMs) and the Chain-of-Thought (CoT) techniques have significantly\nenhanced the reasoning capabilities of these models. However, due to the\ncomplex logic and multi-step reasoning ability needed, the application of CoT\ntechniques in the APR domain remains insufficient. This study systematically\nevaluates the performance of several common CoT techniques in APR tasks and\nproposes an innovative framework $T^3$, which integrates the powerful reasoning\ncapabilities of LLMs with tree search, effectively improving the precision of\ngenerating candidate repair solutions. Furthermore, $T^3$ provides valuable\nguidance for optimizing sample selection and repair strategies in APR tasks,\nestablishing a robust framework for achieving efficient automated debugging.", "AI": {"tldr": "This paper analyzes Chain-of-Thought methods for program repair and introduces $T^3$, a framework that uses LLMs and tree search to achieve better automatic software debugging.", "motivation": "Automatic Program Repair (APR) aims to automatically fix software defects with minimal human input. While Large Language Models (LLMs) and Chain-of-Thought (CoT) reasoning have enhanced software reasoning tasks, their potential in APR remains underexplored due to the domain's complex reasoning requirements.", "method": "The study evaluates several CoT techniques on APR tasks to understand their effectiveness. Then, it introduces a new framework called $T^3$, which integrates LLMs with a tree search strategy to improve the precision of candidate repair generation and guides optimal sample selection and repair.", "result": "$T^3$ substantially improves the accuracy of generating repair solutions for APR compared to baseline CoT approaches. The framework also aids in optimizing strategies for automated debugging tasks.", "conclusion": "By combining the reasoning strengths of LLMs with tree search through the $T^3$ framework, the study achieves more reliable and efficient program repair, laying the groundwork for improved automated debugging."}}
{"id": "2506.21266", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.21266", "abs": "https://arxiv.org/abs/2506.21266", "authors": ["Daniil Karol", "Elizaveta Artser", "Ilya Vlasov", "Yaroslav Golubev", "Hieke Keuning", "Anastasiia Birillo"], "title": "KOALA: a Configurable Tool for Collecting IDE Data When Solving Programming Tasks", "comment": "Accepted to CompEd'25, 7 pages, 4 figures", "summary": "Collecting data of students solving programming tasks is incredibly valuable\nfor researchers and educators. It allows verifying that the students correctly\napply the features and concepts they are taught, or finding students'\nmisconceptions. However, existing data collection tools have limitations, e.g.,\nno control over the granularity of the collected code, not collecting the\nspecific events of the programming environment used, and overall being hard to\nconfigure.\n  To overcome these limitations, we propose KOALA, a convenient and highly\nconfigurable tool for collecting code snapshots and feature usage from students\nsolving programming tasks in JetBrains IDEs. The plugin can be installed in\nIDEs and configured to provide the students with the necessary tasks, enable or\ndisable certain IDE features like code completion, and run surveys. During\nproblem solving, the plugin collects code snapshots at the configured\ngranularity, all IDE actions like running and debugging, as well as some data\nnot collected in prior works, like employed hotkeys and switching focus between\nfiles. The collected data is sent to the server that comes with the tool, where\nit is stored and can be converted to the standardized ProgSnap2 format. To\nshowcase the tool, we collected data from 28 students solving tasks in two\ncourses within the IDE, highlighting some insights from this data.", "AI": {"tldr": "KOALA is a versatile JetBrains IDE plugin that enables researchers to collect highly configurable and detailed data on student programming, addressing the limitations of past tools and providing new insights through real classroom use.", "motivation": "Researchers and educators need detailed and configurable tools to collect data on how students solve programming tasks. Existing tools lack control over data granularity, do not capture all relevant IDE events, and are difficult to configure.", "method": "The authors developed KOALA, a plugin for JetBrains IDEs that collects code snapshots, tracks IDE feature usage, and captures additional events like hotkey use and file switching. The plugin can be configured for specific data granularity, tasks, and settings. Data is stored centrally and can be exported in a standard format (ProgSnap2).", "result": "KOALA was deployed in two real university courses with 28 students. It successfully collected rich, granular data, including types of IDE interactions not available in previous tools, enabling new insights into student programming behavior.", "conclusion": "KOALA offers an advanced, configurable way to collect detailed programming behavior data from students in realistic settings, overcoming previous tool limitations and supporting new research possibilities."}}
{"id": "2506.21297", "categories": ["cs.SE", "cs.DC", "D.2.11; D.2.13; D.2.7"], "pdf": "https://arxiv.org/pdf/2506.21297", "abs": "https://arxiv.org/abs/2506.21297", "authors": ["Ricardo Hideki Hangai Kojo", "Luiz Fernando Corte Real", "Renato Cordeiro Ferreira", "Thatiane de Oliveira Rosa", "Alfredo Goldman"], "title": "Exploring Micro Frontends: A Case Study Application in E-Commerce", "comment": "11 pages, 2 figures (2 diagrams), submitted to the workshop AMP 2025", "summary": "In the micro frontends architectural style, the frontend is divided into\nsmaller components, which can range from a simple button to an entire page. The\ngoal is to improve scalability, resilience, and team independence, albeit at\nthe cost of increased complexity and infrastructure demands. This paper seeks\nto understand when it is worth adopting micro frontends, particularly in the\ncontext of industry. To achieve this, we conducted an investigation into the\nstate of the art of micro frontends, based on both academic and gray\nliterature. We then implemented this architectural style in a marketplace for\nhandcrafted products, which already used microservices. Finally, we evaluated\nthe implementation through a semi-open questionnaire with the developers. At\nthe studied marketplace company, the need for architectural change arose due to\nthe tight coupling between their main system (a Java monolith) and a dedicated\nfrontend system. Additionally, there were deprecated technologies and poor\ndeveloper experience. To address these issues, the micro frontends architecture\nwas adopted, along with the API Gateway and Backend for Frontend patterns, and\ntechnologies such as Svelte and Fastify. Although the adoption of Micro\nFrontends was successful, it was not strictly necessary to meet the company's\nneeds. According to the analysis of the mixed questionnaire responses, other\nalternatives, such as a monolithic frontend, could have achieved comparable\nresults. What made adopting micro frontends the most convenient choice in the\ncompany's context was the monolith strangulation and microservices adoption,\nwhich facilitated implementation through infrastructure reuse and knowledge\nsharing between teams.", "AI": {"tldr": "This paper investigates when micro frontends are worth adopting in industry by reviewing literature, implementing the architecture in a real company, and surveying developers. While micro frontends worked well, alternative architectures could have sufficed. The deciding factors were existing microservices and the need to decouple from a monolith, making micro frontends most suitable for this specific context.", "motivation": "The motivation for this paper is to evaluate when it is worthwhile to adopt micro frontends architecture in industry, especially in cases where legacy systems, outdated technology, and developer experience challenges exist.", "method": "The authors conducted a review of both academic and gray literature on micro frontends. They then implemented a micro frontends architecture in an existing marketplace for handcrafted products (with pre-existing microservices), and evaluated the process and outcomes using a semi-open questionnaire answered by developers.", "result": "The implementation of micro frontends was successful in the marketplace company, but other alternatives (like a monolithic frontend) could have achieved similar outcomes. The main value in adopting micro frontends lay in the company's context: the need to gradually decouple from a monolithic architecture (monolith strangulation) and their existing microservices infrastructure, which made the transition smoother and knowledge sharing easier.", "conclusion": "Adopting micro frontends is not strictly necessary for all organizations facing similar issues; in some cases, monolithic frontends can produce comparable results. However, micro frontends are particularly convenient when there are existing microservices, infrastructure to reuse, and a need to gradually move away from a monolithic system. The decision to use micro frontends should thus depend on the particular organization context rather than general assumptions about the architecture's superiority."}}
{"id": "2506.21300", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.21300", "abs": "https://arxiv.org/abs/2506.21300", "authors": ["Yannis Bertrand", "Christian Imenkamp", "Lukas Malburg", "Matthias Ehrendorfer", "Marco Franceschetti", "Joscha Gr\u00fcger", "Francesco Leotta", "J\u00fcrgen Mangler", "Ronny Seiger", "Agnes Koschmider", "Stefanie Rinderle-Ma", "Barbara Weber", "Estefania Serral"], "title": "An object-centric core metamodel for IoT-enhanced event logs", "comment": null, "summary": "Advances in Internet-of-Things (IoT) technologies have prompted the\nintegration of IoT devices with business processes (BPs) in many organizations\nacross various sectors, such as manufacturing, healthcare and smart spaces. The\nproliferation of IoT devices leads to the generation of large amounts of IoT\ndata providing a window on the physical context of BPs, which facilitates the\ndiscovery of new insights about BPs using process mining (PM) techniques.\nHowever, to achieve these benefits, IoT data need to be combined with\ntraditional process (event) data, which is challenging due to the very\ndifferent characteristics of IoT and process data, for instance in terms of\ngranularity levels. Recently, several data models were proposed to integrate\nIoT data with process data, each focusing on different aspects of data\nintegration based on different assumptions and requirements. This fragmentation\nhampers data exchange and collaboration in the field of PM, e.g., making it\ntedious for researchers to share data. In this paper, we present a core model\nsynthesizing the most important features of existing data models. As the core\nmodel is based on common requirements, it greatly facilitates data sharing and\ncollaboration in the field. A prototypical Python implementation is used to\nevaluate the model against various use cases and demonstrate that it satisfies\nthese common requirements.", "AI": {"tldr": "A new core data model integrates IoT and process data for process mining, improving data sharing and collaboration. Its effectiveness is shown by a prototype Python implementation and testing in various use cases.", "motivation": "The rapid adoption of IoT in various industries generates vast amounts of IoT data, which\u2014when combined with traditional business process (event) data\u2014can provide valuable insights via process mining techniques. However, significant challenges arise due to the differing characteristics and granularity of IoT and process data. Existing data integration models are fragmented, causing issues in data exchange and collaboration for process mining research.", "method": "The authors propose a core data model that synthesizes the most important features of existing data models. The design of this model addresses common requirements for effective integration of IoT and process data. They provide a prototypical Python implementation to evaluate the model against diverse use cases.", "result": "The core model facilitates integration of heterogeneous IoT and process data, easing data sharing and enhancing collaboration in the process mining research community. The Python implementation demonstrates that the model meets common requirements through evaluation in various scenarios.", "conclusion": "By providing a unified core data model grounded in common requirements of IoT and process integration, the paper advances data sharing and collaboration in process mining. The model overcomes the limitations of fragmented existing approaches and is validated through prototypical implementation and use case evaluation."}}
