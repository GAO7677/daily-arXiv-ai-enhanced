{"id": "2508.10781", "categories": ["cs.PL", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.10781", "abs": "https://arxiv.org/abs/2508.10781", "authors": ["Abtin Molavi", "Amanda Xu", "Ethan Cecchetti", "Swamit Tannu", "Aws Albarghouthi"], "title": "Generating Compilers for Qubit Mapping and Routing", "comment": null, "summary": "Quantum computers promise to solve important problems faster than classical\ncomputers, potentially unlocking breakthroughs in materials science, chemistry,\nand beyond. Optimizing compilers are key to realizing this potential, as they\nminimize expensive resource usage and limit error rates. A critical compilation\nstep is qubit mapping and routing (QMR), which finds mappings from circuit\nqubits to qubits on a target device and plans instruction execution while\nsatisfying the device's connectivity constraints. The challenge is that the\nlandscape of quantum architectures is incredibly diverse and fast-evolving.\nGiven this diversity, hundreds of papers have addressed the QMR problem for\ndifferent qubit hardware, connectivity constraints, and quantum error\ncorrection schemes.\n  We present an approach for automatically generating qubit mapping and routing\ncompilers for arbitrary quantum architectures. Though each QMR problem is\ndifferent, we identify a common core structure-device state machine-that we use\nto formulate an abstract QMR problem. Our formulation naturally leads to a\ndomain-specific language, Marol, for specifying QMR problems-for example, the\nwell-studied NISQ mapping and routing problem requires only 12 lines of Marol.\nWe demonstrate that QMR problems, defined in Marol, can be solved with a\npowerful parametric solver that can be instantiated for any Marol program. We\nevaluate our approach through case studies of important QMR problems from prior\nand recent work, covering noisy and fault-tolerant quantum architectures on all\nmajor hardware platforms. Our thorough evaluation shows that generated\ncompilers are competitive with handwritten, specialized compilers in terms of\nruntime and solution quality. We envision that our approach will simplify\ndevelopment of future quantum compilers as new quantum architectures continue\nto emerge.", "AI": {"tldr": "This paper proposes a method and language (Marol) for automatically generating qubit mapping and routing compilers for diverse quantum hardware, showing comparable performance to hand-written solutions and promising easier adaptation to future architectures.", "motivation": "Quantum computers have diverse and rapidly-evolving architectures, making it difficult to develop efficient compilers manually for each new platform. Qubit mapping and routing (QMR) are key steps for optimizing quantum computations on real devices, but current solutions are complex and non-generalizable.", "method": "The paper introduces an approach for automatically generating QMR compilers for any quantum architecture. It identifies a common core structure\u2014a device state machine\u2014for abstractly formulating QMR problems. This abstraction leads to the development of a domain-specific language, Marol, for specifying different QMR scenarios, and a parametric solver capable of solving any QMR instance described in Marol.", "result": "The authors implement their method and evaluate it through various case studies on major quantum hardware platforms, for both noisy and fault-tolerant architectures. Results show that their automatically generated compilers are competitive with manually crafted specialized compilers in both runtime and solution quality.", "conclusion": "Automatically generating quantum QMR compilers using their proposed abstract formulation and Marol language is feasible and effective, providing a way to rapidly adapt compilers to new quantum architectures as they emerge."}}
{"id": "2508.10068", "categories": ["cs.SE", "cs.CL", "cs.IR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.10068", "abs": "https://arxiv.org/abs/2508.10068", "authors": ["Xiaohan Chen", "Zhongying Pan", "Quan Feng", "Yu Tian", "Shuqun Yang", "Mengru Wang", "Lina Gong", "Yuxia Geng", "Piji Li", "Xiang Chen"], "title": "SaraCoder: Orchestrating Semantic and Structural Cues for Profit-Oriented Repository-Level Code Completion", "comment": null, "summary": "Retrieval-augmented generation (RAG) for repository-level code completion\ncommonly relies on superficial text similarity, leading to results plagued by\nsemantic misguidance, redundancy, and homogeneity, while also failing to\nresolve external symbol ambiguity. To address these challenges, we introduce\nSaracoder, a Hierarchical Feature-Optimized retrieval framework. Its core\nHierarchical Feature Optimization module systematically refines candidates by\ndistilling deep semantic relationships, pruning exact duplicates, assessing\nstructural similarity with a novel graph-based metric that weighs edits by\ntheir topological importance, and reranking results to maximize both relevance\nand diversity. Furthermore, an External-Aware Identifier Disambiguator module\naccurately resolves cross-file symbol ambiguity via dependency analysis.\nExtensive experiments on the challenging CrossCodeEval and RepoEval-Updated\nbenchmarks demonstrate that Saracoder significantly outperforms existing\nbaselines across multiple programming languages and models. Our work proves\nthat systematically refining retrieval results across multiple dimensions\nprovides a new paradigm for building more accurate and robust repository-level\ncode completion systems.", "AI": {"tldr": "Saracoder refines code completion retrieval using deep semantics, structural analysis, and identifier disambiguation, outperforming previous methods in accuracy and robustness.", "motivation": "Existing retrieval-augmented generation methods for repository-level code completion rely heavily on superficial text similarity. This results in several problems: semantic misguidance, redundancy, homogeneity, and unresolved external symbol ambiguity.", "method": "Introduced Saracoder, a Hierarchical Feature-Optimized retrieval framework, featuring two main modules: (1) Hierarchical Feature Optimization, which refines retrieval candidates by leveraging deep semantic relationships, removing duplicates, assessing structural similarity using a novel graph-based metric, and reranking for relevance and diversity; (2) an External-Aware Identifier Disambiguator, which resolves cross-file symbol ambiguity through dependency analysis.", "result": "Experimental results using CrossCodeEval and RepoEval-Updated benchmarks show Saracoder outperforms existing baseline methods on code completion tasks across multiple programming languages and models.", "conclusion": "Comprehensive, multidimensional refinement of retrieval results\u2014integrating semantic, structural, and dependency information\u2014sets a new standard for building accurate and robust repository-level code completion systems."}}
{"id": "2508.10059", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10059", "abs": "https://arxiv.org/abs/2508.10059", "authors": ["Yueke Zhang", "Yifan Zhang", "Kevin Leach", "Yu Huang"], "title": "FormalGrad: Integrating Formal Methods with Gradient-Based LLM Refinement", "comment": "6 Pages", "summary": "While Large Language Models (LLMs) have demonstrated remarkable capabilities\nin code generation, they often produce solutions that lack guarantees of\ncorrectness, robustness, and efficiency. The limitation is acute in domains\nrequiring strict constraints. FormalGrad introduces a principled framework that\nintegrates formal methods directly into an iterative LLM-based generation loop.\nIt uniquely treats code as a differentiable variable, converting structured\nfeedback and formal constraints into a textual pseudo-gradient. This gradient\nguides the model to iteratively refine solutions, ensuring they are not only\nfunctional but also robust and formally justified. We evaluate FormalGrad on\nthe HumanEval, HumanEval+, and LiveCodeBench benchmarks. Our implementation\noutperforms strong baselines, achieving an absolute improvement of up to 27% on\nHumanEval and a 41% relative improvement on the challenging LiveCodeBench V6.\nFormalGrad generates formally justified code that is robust and efficient,\npaving the way for reliable AI-assisted software development in high-stakes\napplications.", "AI": {"tldr": "FormalGrad combines LLMs and formal methods to create robust, formally justified code, showing significant improvements in benchmarks and enabling reliable AI-driven coding for sensitive applications.", "motivation": "LLMs are powerful in code generation but struggle to guarantee correctness, robustness, and efficiency, especially in domains with strict constraints.", "method": "FormalGrad is a framework that iteratively integrates formal methods and LLM-based generation. It uses feedback and constraints to form a 'pseudo-gradient,' allowing the model to refine code solutions.", "result": "FormalGrad outperformed strong baselines on HumanEval (absolute improvement up to 27%) and LiveCodeBench V6 (41% relative improvement), generating robust and formally justified code.", "conclusion": "FormalGrad enables reliable, efficient, and robust AI-assisted software development, particularly for high-stakes contexts where formal guarantees are essential."}}
{"id": "2508.10074", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10074", "abs": "https://arxiv.org/abs/2508.10074", "authors": ["Ruofan Lu", "Yintong Huo", "Meng Zhang", "Yichen Li", "Michael R. Lyu"], "title": "Next Edit Prediction: Learning to Predict Code Edits from Context and Interaction History", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has led to the\nwidespread adoption of AI-powered coding assistants integrated into a\ndevelopment environment. On one hand, low-latency code completion offers\ncompletion suggestions but is fundamentally constrained to the cursor's current\nposition. On the other hand, chat-based editing can perform complex\nmodifications, yet forces developers to stop their work, describe the intent in\nnatural language, which causes a context-switch away from the code. This\ncreates a suboptimal user experience, as neither paradigm proactively predicts\nthe developer's next edit in a sequence of related edits. To bridge this gap\nand provide the seamless code edit suggestion, we introduce the task of Next\nEdit Prediction, a novel task designed to infer developer intent from recent\ninteraction history to predict both the location and content of the subsequent\nedit. Specifically, we curate a high-quality supervised fine-tuning dataset and\nan evaluation benchmark for the Next Edit Prediction task. Then, we conduct\nsupervised fine-tuning on a series of models and performed a comprehensive\nevaluation of both the fine-tuned models and other baseline models, yielding\nseveral novel findings. This work lays the foundation for a new interaction\nparadigm that proactively collaborate with developers by anticipating their\nfollowing action, rather than merely reacting to explicit instructions.", "AI": {"tldr": "The paper introduces 'Next Edit Prediction,' enabling coding assistants to proactively suggest code edits by predicting developers' next moves, supported by a new dataset, benchmark, and model evaluations.", "motivation": "Current AI-powered coding assistants either focus on code completion at the cursor (which is limited in scope), or require developers to pause and provide natural language input for chat-based editing (which interrupts workflow). This creates a gap in user experience where neither approach proactively anticipates a developer's next editing action.", "method": "The authors introduce the 'Next Edit Prediction' task, which predicts the location and content of a developer's next code edit based on recent editing history. They create a supervised fine-tuning dataset and an evaluation benchmark for this task, fine-tune several models on this data, and evaluate both fine-tuned and baseline models, reporting their findings.", "result": "The study presents a high-quality dataset and benchmark for next edit prediction, fine-tuned several models, and conducted a comprehensive evaluation, discovering several new insights about model performance and capabilities in this proactive prediction context.", "conclusion": "This work establishes the next edit prediction task as a new paradigm for coding assistants, aiming to proactively collaborate with developers by predicting their next actions, thus promising a more seamless and productive coding experience."}}
{"id": "2508.10157", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10157", "abs": "https://arxiv.org/abs/2508.10157", "authors": ["Ajibode Adekunle", "Abdul Ali Bangash", "Bram Adams", "Ahmed E. Hassan"], "title": "On the synchronization between Hugging Face pre-trained language models and their upstream GitHub repository", "comment": null, "summary": "Pretrained language models (PTLMs) have advanced natural language processing\n(NLP), enabling progress in tasks like text generation and translation. Like\nsoftware package management, PTLMs are trained using code and environment\nscripts in upstream repositories (e.g., GitHub, GH) and distributed as variants\nvia downstream platforms like Hugging Face (HF). Coordinating development\nbetween GH and HF poses challenges such as misaligned release timelines,\ninconsistent versioning, and limited reuse of PTLM variants. We conducted a\nmixed-method study of 325 PTLM families (904 HF variants) to examine how commit\nactivities are coordinated. Our analysis reveals that GH contributors typically\nmake changes related to specifying the version of the model, improving code\nquality, performance optimization, and dependency management within the\ntraining scripts, while HF contributors make changes related to improving model\ndescriptions, data set handling, and setup required for model inference.\nFurthermore, to understand the synchronization aspects of commit activities\nbetween GH and HF, we examined three dimensions of these activities -- lag\n(delay), type of synchronization, and intensity -- which together yielded eight\ndistinct synchronization patterns. The prevalence of partially synchronized\npatterns, such as Disperse synchronization and Sparse synchronization, reveals\nstructural disconnects in current cross-platform release practices. These\npatterns often result in isolated changes -- where improvements or fixes made\non one platform are never replicated on the other -- and in some cases,\nindicate an abandonment of one repository in favor of the other. Such\nfragmentation risks exposing end users to incomplete, outdated, or behaviorally\ninconsistent models. Hence, recognizing these synchronization patterns is\ncritical for improving oversight and traceability in PTLM release workflows.", "AI": {"tldr": "The paper analyzes how code and model changes are (poorly) coordinated between GitHub and Hugging Face for pretrained language models, finding frequent misalignments and fragmentation that risk delivering outdated or inconsistent models. Better synchronization is needed for safe and reliable model releases.", "motivation": "The paper is motivated by coordination and synchronization challenges observed between upstream code repositories (like GitHub) and downstream model distribution platforms (like Hugging Face) in the development and dissemination of pretrained language models (PTLMs). The authors seek to understand how misaligned release timelines, inconsistent versioning, and limited reuse impact these workflows.", "method": "The authors conduct a mixed-method study, analyzing 325 PTLM families (904 Hugging Face variants) to investigate commit activities and synchronization between GitHub and Hugging Face. They examine the dimensions of lag, type of synchronization, and intensity to classify synchronization patterns.", "result": "The study finds that there are eight distinct synchronization patterns, with many instances of partial synchronization (such as Disperse and Sparse synchronization). This leads to structural disconnects: improvements or fixes made on one platform are often not replicated on the other, sometimes leading to the abandonment of one repository. This fragmentation can result in distribution of incomplete, outdated, or inconsistent models to end users.", "conclusion": "Recognizing and understanding different synchronization patterns is essential for improving oversight and traceability in PTLM release workflows, ultimately reducing risks of outdated or inconsistent models reaching end users."}}
{"id": "2508.10517", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10517", "abs": "https://arxiv.org/abs/2508.10517", "authors": ["Likai Ye", "Mengliang Li", "Dehai Zhao", "Jiamou Sun", "Xiaoxue Ren"], "title": "Bridging Solidity Evolution Gaps: An LLM-Enhanced Approach for Smart Contract Compilation Error Resolution", "comment": "International Conference on Software Maintenance and Evolution\n  (ICSME) 2025", "summary": "Solidity, the dominant smart contract language for Ethereum, has rapidly\nevolved with frequent version updates to enhance security, functionality, and\ndeveloper experience. However, these continual changes introduce significant\nchallenges, particularly in compilation errors, code migration, and\nmaintenance. Therefore, we conduct an empirical study to investigate the\nchallenges in the Solidity version evolution and reveal that 81.68% of examined\ncontracts encounter errors when compiled across different versions, with 86.92%\nof compilation errors.\n  To mitigate these challenges, we conducted a systematic evaluation of large\nlanguage models (LLMs) for resolving Solidity compilation errors during version\nmigrations. Our empirical analysis across both open-source (LLaMA3, DeepSeek)\nand closed-source (GPT-4o, GPT-3.5-turbo) LLMs reveals that although these\nmodels exhibit error repair capabilities, their effectiveness diminishes\nsignificantly for semantic-level issues and shows strong dependency on prompt\nengineering strategies. This underscores the critical need for domain-specific\nadaptation in developing reliable LLM-based repair systems for smart contracts.\n  Building upon these insights, we introduce SMCFIXER, a novel framework that\nsystematically integrates expert knowledge retrieval with LLM-based repair\nmechanisms for Solidity compilation error resolution. The architecture\ncomprises three core phases: (1) context-aware code slicing that extracts\nrelevant error information; (2) expert knowledge retrieval from official\ndocumentation; and (3) iterative patch generation for Solidity migration.\nExperimental validation across Solidity version migrations demonstrates our\napproach's statistically significant 24.24% improvement over baseline GPT-4o on\nreal-world datasets, achieving near-perfect 96.97% accuracy.", "AI": {"tldr": "Solidity version updates cause widespread contract migration errors. LLMs help but need tailored adaptation. The proposed SMCFIXER framework, combining expert documentation and LLMs, greatly improves error resolution during migration, achieving high accuracy on real-world data.", "motivation": "Solidity frequently updates to improve security and functionality, but these rapid changes lead to significant challenges in compilation errors, code migration, and contract maintenance, prompting a need for better tools and methodologies.", "method": "The authors conducted an empirical study to analyze the prevalence and nature of compilation errors during Solidity version migration. They systematically evaluated LLMs (both open and closed source) for error resolution capability, and proposed SMCFIXER\u2014a framework that integrates expert documentation with LLM-based repair, using code slicing, expert knowledge retrieval, and iterative patch generation.", "result": "Empirical analysis revealed that over 81% of contracts fail to compile across versions, with nearly 87% of errors being compilation-related. LLMs can repair some errors but struggle with semantic issues and are highly sensitive to prompting strategies. The proposed SMCFIXER framework achieved a 24.24% improvement over GPT-4o and reached 96.97% accuracy in fixing Solidity migration errors on real-world datasets.", "conclusion": "Frequent changes in Solidity introduce severe migration challenges. While LLMs provide promising error repair, domain-specific adaptation is crucial. Integrating expert knowledge with LLMs (via SMCFIXER) substantially improves migration success and reliability, highlighting the value of combined approaches for smart contract maintenance."}}
{"id": "2508.10852", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10852", "abs": "https://arxiv.org/abs/2508.10852", "authors": ["Souhaila Serbout", "Diana Carolina Mu\u00f1oz Hurtado", "Hassan Atwi", "Edoardo Riggio", "Cesare Pautasso"], "title": "EVOSCAT: Exploring Software Change Dynamics in Large-Scale Historical Datasets", "comment": "Submitted to VISSOFT 2025. For the hi-resolution version of the\n  paper, see https://design.inf.usi.ch/publications/2025/vissoft", "summary": "Long lived software projects encompass a large number of artifacts, which\nundergo many revisions throughout their history. Empirical software engineering\nresearchers studying software evolution gather and collect datasets with\nmillions of events, representing changes introduced to specific artifacts. In\nthis paper, we propose EvoScat, a tool that attempts addressing temporal\nscalability through the usage of interactive density scatterplot to provide a\nglobal overview of large historical datasets mined from open source\nrepositories in a single visualization. EvoScat intents to provide researchers\nwith a mean to produce scalable visualizations that can help them explore and\ncharacterize evolution datasets, as well as comparing the histories of\nindividual artifacts, both in terms of 1) observing how rapidly different\nartifacts age over multiple-year-long time spans 2) how often metrics\nassociated with each artifacts tend towards an improvement or worsening. The\npaper shows how the tool can be tailored to specific analysis needs (pace of\nchange comparison, clone detection, freshness assessment) thanks to its support\nfor flexible configuration of history scaling and alignment along the time\naxis, artifacts sorting and interactive color mapping, enabling the analysis of\nmillions of events obtained by mining the histories of tens of thousands of\nsoftware artifacts. We include in this paper a gallery showcasing datasets\ngathering specific artifacts (OpenAPI descriptions, GitHub workflow\ndefinitions) across multiple repositories, as well as diving into the history\nof specific popular open source projects.", "AI": {"tldr": "EvoScat is a specialized visualization tool that empowers researchers to analyze the evolution of many software artifacts over time by providing scalable, interactive scatterplot visualizations, demonstrated on large open-source datasets.", "motivation": "Long-lived software projects generate extensive artifact histories across many revisions, requiring scalable tools for effective analysis and visualization by empirical software engineering researchers. Existing approaches struggle with temporal scalability and overview of massive datasets.", "method": "The paper presents EvoScat, an interactive density scatterplot tool, enabling global visualization of large historical datasets from open-source repositories. EvoScat allows for flexible scaling, alignment, artifact sorting, and interactive color mapping to support diverse analysis needs. The tool is demonstrated on various datasets and software artifacts.", "result": "EvoScat supports visualization and exploration of millions of change events across tens of thousands of artifacts, facilitating tasks like pace of change comparison, clone detection, and freshness assessment. Example visualizations are provided from real-world open-source repositories.", "conclusion": "EvoScat enables scalable, interactive visual exploration of software evolution datasets, assisting researchers in analyzing and comparing software artifacts' histories efficiently. The tool can be tailored to specific research requirements and scales to large datasets."}}
