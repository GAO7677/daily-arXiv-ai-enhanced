{"id": "2507.12640", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.12640", "abs": "https://arxiv.org/abs/2507.12640", "authors": ["Tom Smeding", "Miko\u0142aj Konarski", "Simon Peyton Jones", "Andrew Fitzgibbon"], "title": "Dual-Numbers Reverse AD for Functional Array Languages", "comment": null, "summary": "The standard dual-numbers construction works well for forward-mode automatic\ndifferentiation (AD) and is attractive due to its simplicity; recently, it also\nhas been adapted to reverse-mode AD, but practical performance, especially on\narray programs, leaves a lot to be desired. In this paper we introduce\nfirst-class support for multidimensional arrays in dual-numbers reverse-mode AD\nwith little to no performance overhead. The algorithm consists of three\nloosely-coupled components: a semantics-preserving vectorisation code\ntransformation (the bulk-operation transform or BOT), a fairly straightforward\nlifting of the basic dual-numbers reverse AD algorithm to a mostly first-order\narray language, and symbolic interpretation to achieve an end-to-end\ncompilation pipeline. Unfortunately, we lose some of the nice generalisable\naspects of dual-numbers AD in the process, most importantly support for\nhigher-order code.\n  We do support some higher-order array combinators, but only a\ncarefully-chosen set: 'build' (elementwise array construction), 'gather' and\n'scatter'. In return, the BOT can eliminate the essential (for AD)\nhigher-orderness of the input program, meaning that AD gets essentially\npresented with a first-order program. This allows the naive trick of lifting\ndual numbers to \"dual arrays\" to work without much modification.", "AI": {"tldr": "The paper introduces an efficient dual-numbers-based reverse-mode AD approach for multidimensional arrays using a novel code transformation. Although some general higher-order code support is sacrificed, practical array operations are handled efficiently, improving AD performance for array-centric programs.", "motivation": "While dual-numbers construction is attractive for forward-mode AD and has been adapted for reverse-mode AD, its performance on array programs is subpar. There is a need to improve efficiency for reverse-mode AD on multidimensional arrays without added performance costs.", "method": "The paper introduces an algorithm with three main components: (1) a semantics-preserving vectorisation code transformation called the bulk-operation transform (BOT), (2) a straightforward adaptation of the dual-numbers reverse AD algorithm to a mostly first-order array language, and (3) symbolic interpretation for an end-to-end compilation pipeline. The approach reduces higher-order features by focusing on first-order transformations, but selectively supports some higher-order array combinators ('build', 'gather', 'scatter').", "result": "The proposed approach enables first-class multidimensional array support in reverse-mode dual-numbers AD with minimal or no performance overhead. While general higher-order code is not supported, essential array combinators are, making the method practical and efficient for many common operations.", "conclusion": "By using BOT and adapting reverse-mode dual-numbers AD for arrays, the technique overcomes previous performance limitations and makes dual-number approaches viable for array-heavy programs, albeit with some loss of generality regarding higher-order operations."}}
{"id": "2507.13091", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.13091", "abs": "https://arxiv.org/abs/2507.13091", "authors": ["Aur\u00e8le Barri\u00e8re", "Victor Deng", "Cl\u00e9ment Pit-Claudel"], "title": "Formal Verification for JavaScript Regular Expressions: a Proven Semantics and its Applications", "comment": "25 pages, 3 pages of references, 6 pages of appendix", "summary": "We present the first mechanized, succinct, practical, complete, and\nproven-faithful semantics for a modern regular expression language with\nbacktracking semantics. We ensure its faithfulness by proving it equivalent to\na preexisting line-by-line embedding of the official ECMAScript specification\nof JavaScript regular expressions. We demonstrate its practicality by\npresenting two real-world applications. First, a new notion of contextual\nequivalence for modern regular expressions, which we use to prove or disprove\nrewrites drawn from previous work. Second, the first formal proof of the PikeVM\nalgorithm used in many real-world engines. In contrast with the specification\nand other formalization work, our semantics captures not only the top-priority\nmatch, but a full backtracking tree recording all possible matches and their\nrespective priority. All our definitions and results have been mechanized in\nthe Rocq proof assistant.", "AI": {"tldr": "The paper provides the first fully mechanized and proven-faithful semantics for modern regexes (like JavaScript's), enabling formal proofs of regex equivalence, rewrites, and matching algorithms, and all results are checked in the Rocq proof assistant.", "motivation": "Modern regular expression languages (like JavaScript's) have complex semantics, especially with features like backtracking and ambiguous matching. Until now, a fully mechanized, succinct, practical, and provably faithful formal semantics for such languages has been missing, making it hard to reason about or verify behaviors and transformations of regular expressions used in practice.", "method": "The authors developed a fully mechanized and faithful formal semantics for modern regular expressions with backtracking, mechanized in the Rocq proof assistant. They proved equivalence with an existing line-by-line embedding of the official ECMAScript (JavaScript) regex specification and demonstrated applications for this semantics, including contextual equivalence for regex rewrites and a formal proof of the PikeVM algorithm.", "result": "They achieved the first practical, succinct, completely mechanized, and proven-faithful semantics for a modern regex language with backtracking, equivalent to JavaScript's regex spec. This enabled them to formalize contextual equivalence of regexes, prove/disprove rewrites, and formally prove the PikeVM algorithm. Their semantics also captures the full backtracking match tree, which is more informative than most previous approaches.", "conclusion": "The paper provides a rigorous and practical mechanized formal semantics for modern regular expressions, proven faithful to JavaScript's specification, and demonstrates its utility in verifying regex equivalence, correctness of transformations, and regex engine algorithms. All results are mechanically checked in the Rocq proof assistant."}}
{"id": "2507.13290", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13290", "abs": "https://arxiv.org/abs/2507.13290", "authors": ["Aaron Councilman", "David Fu", "Aryan Gupta", "Chengxiao Wang", "David Grove", "Yu-Xiong Wang", "Vikram Adve"], "title": "Towards Formal Verification of LLM-Generated Code from Natural Language Prompts", "comment": "31 pages, 9 figures", "summary": "In the past few years LLMs have emerged as a tool that can aid programmers by\ntaking natural language descriptions and generating code based on it. However,\nLLMs often generate incorrect code that users need to fix and the literature\nsuggests users often struggle to detect these errors. In this work we seek to\noffer formal guarantees of correctness to LLM generated code; such guarantees\ncould improve the experience of using AI Code Assistants and potentially enable\nnatural language programming for users with little or no programming knowledge.\nTo address this challenge we propose to incorporate a formal query language\nthat can represent a user's intent in a formally defined but natural\nlanguage-like manner that a user can confirm matches their intent. Then, using\nsuch a query we propose to verify LLM generated code to ensure it matches the\nuser's intent. We implement these ideas in our system, Astrogator, for the\nAnsible programming language which includes such a formal query language, a\ncalculus for representing the behavior of Ansible programs, and a symbolic\ninterpreter which is used for the verification. On a benchmark suite of 21\ncode-generation tasks, our verifier is able to verify correct code in 83% of\ncases and identify incorrect code in 92%.", "AI": {"tldr": "The paper presents Astrogator, a system that combines a user-confirmable, formal query language and symbolic verification to ensure LLM-generated Ansible code matches user intent. The system verifies correct code in 83% and identifies incorrect code in 92% of cases, making AI coding assistants more reliable and accessible.", "motivation": "LLMs often generate incorrect code, and users\u2014especially those with limited programming knowledge\u2014struggle to identify errors. Providing formal guarantees for code correctness would benefit both novice and experienced users, improving trust and usability of AI Code Assistants.", "method": "The authors introduce a formal query language that enables users to specify their programming intent in a formally defined, yet natural, manner. Their system, Astrogator, targets the Ansible programming language and verifies the correctness of LLM-generated code using a specialized calculus and symbolic interpreter.", "result": "On a benchmark suite of 21 code-generation tasks, Astrogator's verifier was able to verify correct code in 83% of the cases and identify incorrect code in 92%.", "conclusion": "The proposed approach of combining formal query specification and automated verification substantially improves the reliability of LLM-generated code in the Ansible language, providing both assurances of correctness and better user experiences."}}
{"id": "2507.12472", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12472", "abs": "https://arxiv.org/abs/2507.12472", "authors": ["Lingzhe Zhang", "Tong Jia", "Mengxi Jia", "Yifan Wu", "Aiwei Liu", "Yong Yang", "Zhonghai Wu", "Xuming Hu", "Philip S. Yu", "Ying Li"], "title": "A Survey of AIOps in the Era of Large Language Models", "comment": "Accepted By CSUR, an extended version of \"A Survey of AIOps for\n  Failure Management in the Era of Large Language Models\" [arXiv:2406.11213]", "summary": "As large language models (LLMs) grow increasingly sophisticated and\npervasive, their application to various Artificial Intelligence for IT\nOperations (AIOps) tasks has garnered significant attention. However, a\ncomprehensive understanding of the impact, potential, and limitations of LLMs\nin AIOps remains in its infancy. To address this gap, we conducted a detailed\nsurvey of LLM4AIOps, focusing on how LLMs can optimize processes and improve\noutcomes in this domain. We analyzed 183 research papers published between\nJanuary 2020 and December 2024 to answer four key research questions (RQs). In\nRQ1, we examine the diverse failure data sources utilized, including advanced\nLLM-based processing techniques for legacy data and the incorporation of new\ndata sources enabled by LLMs. RQ2 explores the evolution of AIOps tasks,\nhighlighting the emergence of novel tasks and the publication trends across\nthese tasks. RQ3 investigates the various LLM-based methods applied to address\nAIOps challenges. Finally, RQ4 reviews evaluation methodologies tailored to\nassess LLM-integrated AIOps approaches. Based on our findings, we discuss the\nstate-of-the-art advancements and trends, identify gaps in existing research,\nand propose promising directions for future exploration.", "AI": {"tldr": "This paper surveys 183 recent works on using large language models for AIOps, revealing current trends, research gaps, and future directions to optimize IT operations with LLM-based techniques.", "motivation": "Large language models (LLMs) are becoming increasingly sophisticated and widely used in Artificial Intelligence for IT Operations (AIOps). However, their overall impact and limitations in this context are not well understood. The motivation is to fill this knowledge gap by systematically surveying how LLMs are used in AIOps.", "method": "The authors conducted a comprehensive survey of 183 research papers published between January 2020 and December 2024. They structured their analysis around four research questions: the types of failure data sources, evolution and trends of AIOps tasks, LLM-based methods applied to AIOps challenges, and evaluation methodologies for LLM-integrated AIOps approaches.", "result": "The survey revealed how LLMs process both legacy and new data sources, mapped the emergence and publication trends of new AIOps tasks, cataloged LLM-based methods used for AIOps problems, and reviewed specialized evaluation metrics. The study synthesized state-of-the-art advancements, identified research gaps, and outlined future opportunities in LLM-powered AIOps.", "conclusion": "A comprehensive understanding of LLMs in AIOps is still developing. This survey offers a detailed overview of current progress, highlights research gaps, and suggests promising directions for further study, contributing to both academic and practical advancement in AIOps enabled by LLMs."}}
{"id": "2507.12480", "categories": ["cs.SE", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.12480", "abs": "https://arxiv.org/abs/2507.12480", "authors": ["Nazanin Siavash", "Armin Moin"], "title": "LLM-Powered Quantum Code Transpilation", "comment": "IEEE International Conference on Quantum Computing and Engineering\n  (QCE) 2025 - Extended Abstract", "summary": "There exist various Software Development Kits (SDKs) tailored to different\nquantum computing platforms. These are known as Quantum SDKs (QSDKs). Examples\ninclude but are not limited to Qiskit, Cirq, and PennyLane. However, this\ndiversity presents significant challenges for interoperability and\ncross-platform development of hybrid quantum-classical software systems.\nTraditional rule-based transpilers for translating code between QSDKs are\ntime-consuming to design and maintain, requiring deep expertise and rigid\nmappings in the source and destination code. In this study, we explore the use\nof Large Language Models (LLMs) as a flexible and automated solution.\nLeveraging their pretrained knowledge and contextual reasoning capabilities, we\nposition LLMs as programming language-agnostic transpilers capable of\nconverting quantum programs from one QSDK to another while preserving\nfunctional equivalence. Our approach eliminates the need for manually defined\ntransformation rules and offers a scalable solution to quantum software\nportability. This work represents a step toward enabling intelligent,\ngeneral-purpose transpilation in the quantum computing ecosystem.", "AI": {"tldr": "LLMs can automatically translate quantum code between different QSDKs, offering a scalable and flexible alternative to manual rule-based transpilers, thus improving interoperability and software portability in quantum computing.", "motivation": "The motivation behind this paper is the lack of interoperability and the difficulty of cross-platform development among various Quantum Software Development Kits (QSDKs) such as Qiskit, Cirq, and PennyLane. Traditional rule-based transpilers are time-consuming to create and maintain, as they require deep expertise and rigid, manual mappings, making code translation laborious and inflexible.", "method": "The authors propose leveraging Large Language Models (LLMs) as automated, flexible, and programming language-agnostic transpilers. LLMs are used to convert quantum programs from one QSDK to another, preserving functional equivalence, without requiring explicitly programmed transformation rules.", "result": "The approach demonstrates that LLMs, due to their pretrained knowledge and reasoning abilities, can effectively automate the transpilation process across multiple QSDKs, eliminating the reliance on manual, rule-based methods.", "conclusion": "LLM-based transpilation offers a scalable, intelligent, and general-purpose solution for boosting quantum software portability and enabling more seamless cross-platform quantum software development. This work takes a significant step towards ecosystem-wide interoperability in quantum computing."}}
{"id": "2507.12482", "categories": ["cs.SE", "cs.AI", "cs.CE", "cs.LG", "68N30, 68T05, 68T50", "D.2.5; D.2.7; F.3.2; I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.12482", "abs": "https://arxiv.org/abs/2507.12482", "authors": ["Ishraq Khan", "Assad Chowdary", "Sharoz Haseeb", "Urvish Patel"], "title": "Kodezi Chronos: A Debugging-First Language Model for Repository-Scale, Memory-Driven Code Understanding", "comment": "10 pages, 10 figures, 7 tables, IEEE Conference format, Q4 2025 model\n  release, Q1 2026 Kodezi OS deployment", "summary": "Large Language Models (LLMs) have advanced code generation and software\nautomation, but are fundamentally constrained by limited inference-time context\nand lack of explicit code structure reasoning. We introduce Kodezi Chronos, a\nnext-generation architecture for autonomous code understanding, debugging, and\nmaintenance, designed to operate across ultra-long contexts comprising entire\ncodebases, histories, and documentation, all without fixed window limits.\nKodezi Chronos leverages a multi-level embedding memory engine, combining\nvector and graph-based indexing with continuous code-aware retrieval. This\nenables efficient and accurate reasoning over millions of lines of code,\nsupporting repository-scale comprehension, multi-file refactoring, and\nreal-time self-healing actions. Our evaluation introduces a novel Multi Random\nRetrieval benchmark, specifically tailored to the software engineering domain.\nUnlike classical retrieval benchmarks, this method requires the model to\nresolve arbitrarily distant and obfuscated associations across code artifacts,\nsimulating realistic tasks such as variable tracing, dependency migration, and\nsemantic bug localization. Chronos outperforms prior LLMs and code models,\ndemonstrating a 23% improvement in real-world bug detection and reducing\ndebugging cycles by up to 40% compared to traditional sequence-based\napproaches. By natively interfacing with IDEs and CI/CD workflows, Chronos\nenables seamless, autonomous software maintenance, elevating code reliability\nand productivity while reducing manual effort. These results mark a critical\nadvance toward self-sustaining, continuously optimized software ecosystems.", "AI": {"tldr": "Kodezi Chronos is a new AI model for software engineering that can understand, debug, and maintain entire codebases without context limits. By using advanced memory and retrieval techniques, it achieves significant improvements in bug detection and debugging efficiency, moving toward more autonomous and reliable software maintenance.", "motivation": "Traditional Large Language Models (LLMs) have made significant progress in code generation and automation, but face major limitations. They can't reason well about explicit code structure and are hampered by restricted context windows, making it hard to understand and manipulate large codebases or handle complex debugging and maintenance tasks.", "method": "The authors propose Kodezi Chronos, a new architecture that introduces a multi-level embedding memory engine. This engine combines both vector and graph-based indexing, plus continuous retrieval mechanisms designed specifically for code. It allows reasoning over millions of lines of code with no fixed context window. The model is evaluated with a new Multi Random Retrieval benchmark, which requires solving realistic software engineering tasks involving distant and obfuscated associations across code artifacts.", "result": "Kodezi Chronos outperforms existing LLMs and code-specific models. It achieves a 23% improvement in real-world bug detection and reduces debugging cycles by up to 40% compared to traditional sequence-based models. Its design allows seamless integration with IDEs and CI/CD pipelines, enabling more autonomous software maintenance.", "conclusion": "Kodezi Chronos demonstrates that with a sophisticated memory and retrieval system tailored for code, LLM-based models can transcend previous context limitations and lack of structure-aware reasoning, enabling repository-scale code comprehension and autonomous maintenance. This marks a key advance towards self-sustaining software ecosystems."}}
{"id": "2507.12483", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.12483", "abs": "https://arxiv.org/abs/2507.12483", "authors": ["Dong Wang", "Hanmo You", "Lingwei Zhu", "Kaiwei Lin", "Zheng Chen", "Chen Yang", "Junji Yu", "Zan Wang", "Junjie Chen"], "title": "A Survey of Reinforcement Learning for Software Engineering", "comment": null, "summary": "Reinforcement Learning (RL) has emerged as a powerful paradigm for sequential\ndecision-making and has attracted growing interest across various domains,\nparticularly following the advent of Deep Reinforcement Learning (DRL) in 2015.\nSimultaneously, the rapid advancement of Large Language Models (LLMs) has\nfurther fueled interest in integrating RL with LLMs to enable more adaptive and\nintelligent systems. In the field of software engineering (SE), the increasing\ncomplexity of systems and the rising demand for automation have motivated\nresearchers to apply RL to a broad range of tasks, from software design and\ndevelopment to quality assurance and maintenance. Despite growing research in\nRL-for-SE, there remains a lack of a comprehensive and systematic survey of\nthis evolving field. To address this gap, we reviewed 115 peer-reviewed studies\npublished across 22 premier SE venues since the introduction of DRL. We\nconducted a comprehensive analysis of publication trends, categorized SE topics\nand RL algorithms, and examined key factors such as dataset usage, model design\nand optimization, and evaluation practices. Furthermore, we identified open\nchallenges and proposed future research directions to guide and inspire ongoing\nwork in this evolving area. To summarize, this survey offers the first\nsystematic mapping of RL applications in software engineering, aiming to\nsupport both researchers and practitioners in navigating the current landscape\nand advancing the field. Our artifacts are publicly available:\nhttps://github.com/KaiWei-Lin-lanina/RL4SE.", "AI": {"tldr": "This paper presents the first systematic survey of how reinforcement learning (including deep RL and LLMs) is applied to software engineering tasks, analyzing 115 peer-reviewed papers, highlighting research trends, challenges, and future directions, and publishing its datasets and findings for public use.", "motivation": "The motivation for this paper is the increasing complexity of software systems and the rising demand for automation in software engineering, which has led to growing interest in applying reinforcement learning (RL), especially with advancements in deep reinforcement learning (DRL) and large language models (LLMs). Despite increased research activity, the field lacks a comprehensive and systematic survey or mapping of RL applications within software engineering.", "method": "The authors systematically reviewed 115 peer-reviewed studies from 22 leading software engineering venues, analyzing publication trends, categorizing software engineering topics and RL algorithms, and examining factors like dataset usage, model design, optimization, and evaluation practices. They also identified challenges and proposed future research directions. All artifacts are made publicly available.", "result": "The review provides a comprehensive analysis and systematic mapping of RL applications in software engineering, categorizes research efforts by SE tasks and RL methodologies, highlights trends and practices, identifies open challenges, and suggests future directions. The survey produces resources for the community by making its artifacts accessible.", "conclusion": "This paper presents the first systematic survey of RL applications in software engineering. It serves as a foundational resource for both researchers and practitioners, offering a structured overview of the field, critical insights, and guidance for future research. The public artifact repository further supports knowledge transfer and ongoing research."}}
{"id": "2507.12558", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.12558", "abs": "https://arxiv.org/abs/2507.12558", "authors": ["Tien P. T. Le", "Anh M. T. Bui", "Huy N. D. Pham", "Alessio Bucaioni", "Phuong T. Nguyen"], "title": "When Retriever Meets Generator: A Joint Model for Code Comment Generation", "comment": "The paper has been peer-reviewed and accepted for publication in the\n  proceedings of the 19th ACM/IEEE International Symposium on Empirical\n  Software Engineering and Measurement (ESEM 2025)", "summary": "Automatically generating concise, informative comments for source code can\nlighten documentation effort and accelerate program comprehension.\nRetrieval-augmented approaches first fetch code snippets with existing comments\nand then synthesize a new comment, yet retrieval and generation are typically\noptimized in isolation, allowing irrelevant neighbors topropagate noise\ndownstream. To tackle the issue, we propose a novel approach named RAGSum with\nthe aim of both effectiveness and efficiency in recommendations. RAGSum is\nbuilt on top offuse retrieval and generation using a single CodeT5 backbone. We\nreport preliminary results on a unified retrieval-generation framework built on\nCodeT5. A contrastive pre-training phase shapes code embeddings for\nnearest-neighbor search; these weights then seed end-to-end training with a\ncomposite loss that (i) rewards accurate top-k retrieval; and (ii) minimizes\ncomment-generation error. More importantly, a lightweight self-refinement loop\nis deployed to polish the final output. We evaluated theframework on three\ncross-language benchmarks (Java, Python, C), and compared it with three\nwell-established baselines. The results show that our approach substantially\noutperforms thebaselines with respect to BLEU, METEOR, and ROUTE-L. These\nfindings indicate that tightly coupling retrieval and generationcan raise the\nceiling for comment automation and motivateforthcoming replications and\nqualitative developer studies.", "AI": {"tldr": "RAGSum is a new method for generating code comments that combines retrieval and generation in a unified model using CodeT5. By optimizing both retrieval and generation together, and refining results in a feedback loop, it significantly outperforms existing methods across multiple languages.", "motivation": "Automatically generating concise and informative comments for source code can significantly reduce documentation effort and help developers understand code faster. However, traditional retrieval-augmented approaches treat retrieval and generation as separate processes, which often causes irrelevant code snippets (neighbors) to add noise and degrade performance. The motivation of this paper is to effectively and efficiently improve code comment generation by better integrating retrieval and generation steps.", "method": "The paper introduces RAGSum, a novel retrieval-augmented generation approach that fuses retrieval and generation within a single CodeT5 backbone. It uses contrastive pre-training to optimize code embeddings for better nearest-neighbor retrieval and then applies end-to-end training with a composite loss rewarding retrieval precision and comment-generation accuracy. Additionally, a lightweight self-refinement loop further improves output quality.", "result": "RAGSum was evaluated on three cross-language benchmarks (Java, Python, C) and compared against three established baselines. It substantially outperformed these baselines in BLEU, METEOR, and ROUTE-L metrics, evidencing a significant improvement in the quality of generated code comments.", "conclusion": "Tightly coupling the retrieval and generation components using a unified framework can meaningfully improve the effectiveness and efficiency of automated code comment generation. The positive results both encourage further replication and deeper qualitative studies with developers."}}
{"id": "2507.12561", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.12561", "abs": "https://arxiv.org/abs/2507.12561", "authors": ["Samal Nursapa", "Anastassiya Samuilova", "Alessio Bucaioni. Phuong T. Nguyen"], "title": "ROSE: Transformer-Based Refactoring Recommendation for Architectural Smells", "comment": "The paper has been peer-reviewed and accepted for publication in the\n  proceedings of the 19th ACM/IEEE International Symposium on Empirical\n  Software Engineering and Measurement (ESEM 2025)", "summary": "Architectural smells such as God Class, Cyclic Dependency, and Hub-like\nDependency degrade software quality and maintainability. Existing tools detect\nsuch smells but rarely suggest how to fix them. This paper explores the use of\npre-trained transformer models--CodeBERT and CodeT5--for recommending suitable\nrefactorings based on detected smells. We frame the task as a three-class\nclassification problem and fine-tune both models on over 2 million refactoring\ninstances mined from 11,149 open-source Java projects. CodeT5 achieves 96.9%\naccuracy and 95.2% F1, outperforming CodeBERT and traditional baselines. Our\nresults show that transformer-based models can effectively bridge the gap\nbetween smell detection and actionable repair, laying the foundation for future\nrefactoring recommendation systems. We release all code, models, and data under\nan open license to support reproducibility and further research.", "AI": {"tldr": "This paper demonstrates that fine-tuned transformer models, especially CodeT5, can accurately suggest refactorings for fixing architectural smells in Java code, surpassing other methods and supporting further research through open-source contributions.", "motivation": "Architectural smells, such as God Class, Cyclic Dependency, and Hub-like Dependency, negatively affect software quality and maintainability. While detection tools exist, there is a lack of effective solutions that recommend how to fix these smells.", "method": "The authors use pre-trained transformer models, CodeBERT and CodeT5, for refactoring recommendation. They frame the problem as a three-class classification task and fine-tune these models using more than 2 million refactoring instances from 11,149 open-source Java projects.", "result": "CodeT5 achieves 96.9% accuracy and 95.2% F1, outperforming CodeBERT and traditional baseline methods.", "conclusion": "Transformer-based models like CodeT5 can effectively recommend refactorings for architectural smells, providing an actionable step from detection to repair and supporting future research through open-source release of tools and data."}}
{"id": "2507.12642", "categories": ["cs.SE", "cs.AI", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.12642", "abs": "https://arxiv.org/abs/2507.12642", "authors": ["Kiana Kheiri", "Aamna Aamir", "Andriy Miranskyy", "Chen Ding"], "title": "QSpark: Towards Reliable Qiskit Code Generation", "comment": null, "summary": "Quantum circuits must be error-resilient, yet LLMs like Granite-20B-Code and\nStarCoder often output flawed Qiskit code. We fine-tuned a 32 B model with two\nRL methods, Group Relative Policy Optimization (GRPO) and Odds-Ratio Preference\nOptimization (ORPO), using a richly annotated synthetic dataset. On the Qiskit\nHumanEval benchmark, ORPO reaches 56.29\\% Pass@1 ($\\approx+10$ pp over\nGranite-8B-QK) and GRPO hits 49\\%, both beating all general-purpose baselines;\non the original HumanEval they score 65.90\\% and 63.00\\%. GRPO excels on basic\ntasks (42/54), ORPO on intermediate ones (41/68), and neither solves the five\nadvanced tasks, highlighting clear gains yet room for progress in AI-assisted\nquantum programming.", "AI": {"tldr": "Fine-tuning LLMs with advanced reinforcement learning improves AI-generated Qiskit code, achieving better results than existing models on most tasks but still struggling with the hardest ones.", "motivation": "Large language models (LLMs) like Granite-20B-Code and StarCoder generate Qiskit code for quantum circuits, but the code often contains errors. Improving the reliability of AI-generated quantum programming is crucial for real-world applications.", "method": "The authors fine-tuned a 32B parameter model using two reinforcement learning methods: Group Relative Policy Optimization (GRPO) and Odds-Ratio Preference Optimization (ORPO). The model was trained with a richly annotated synthetic dataset and evaluated on the Qiskit HumanEval and original HumanEval benchmarks.", "result": "ORPO achieved 56.29% Pass@1 on Qiskit HumanEval (an increase of about 10 percentage points over Granite-8B-QK), and GRPO achieved 49%. Both outperform general-purpose baselines. On the original HumanEval, ORPO and GRPO achieved 65.90% and 63.00%, respectively. GRPO performed best on basic tasks, ORPO on intermediate ones, but neither method could solve advanced tasks, indicating progress and ongoing challenges.", "conclusion": "Fine-tuning large models with specialized reinforcement learning techniques significantly improves the reliability of AI-generated quantum programming code, though complex tasks remain unsolved, showing both substantial progress and clear limitations."}}
{"id": "2507.12649", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.12649", "abs": "https://arxiv.org/abs/2507.12649", "authors": ["Christine van Stiphoudt", "Sergio Potenciano Menci", "Gilbert Fridgen"], "title": "A Three-Phase Evaluation Approach for new Information and Data Models in the Smart Grid Domain", "comment": null, "summary": "The ongoing digitalisation of the smart grid is resulting in an increase in\nautomated information exchanges across distributed energy systems. This process\nhas led to the development of new information and data models when the existing\nones fall short. To prevent potential disruptions caused by flaws in the newly\ndesigned information and data models, it is essential to evaluate them during\nthe design process before they are implemented in operation.\n  Currently, general explicit evaluation approaches outside the smart grid\ndomain stay at a high level without defining clear steps. Meanwhile, implicit\nevaluation approaches in the smart grid domain focus on testing systems that\nutilise information and data models already in use for functionality in terms\nof conformance and interoperability. Notably, no combination of explicit and\nimplicit evaluation approaches for newly designed information and data models\noffers a clearly defined set of steps during their design process in the smart\ngrid context.\n  Consequently, we design a three-phase evaluation approach using design\nscience research to address this gap. Our evaluation approach combines explicit\nand implicit evaluation methods and is applicable when developing new\ninformation and data models. We use the development of an information model and\ndata model focused on industrial flexibility descriptions to refine our\nevaluation approach. Additionally, we provide lessons learned from our\nexperience.", "AI": {"tldr": "This paper presents a new three-phase evaluation method that merges explicit and implicit approaches to assess information and data models for smart grids during their design, ensuring their robustness before deployment.", "motivation": "The motivation is to ensure the reliability and robustness of new information and data models for smart grids, as digitalization leads to increasingly automated exchanges across distributed energy systems and existing evaluation methods are either too general or only focus on operational systems.", "method": "The authors design a three-phase evaluation approach using design science research. This approach combines explicit and implicit evaluation methods specifically tailored for the design process of new information and data models within the smart grid context.", "result": "The proposed three-phase evaluation approach was applied and refined using the development of an information and data model for industrial flexibility descriptions. The process included lessons learned from this application.", "conclusion": "A novel combined explicit and implicit evaluation methodology with clearly defined steps was created, filling a gap in the smart grid domain for evaluating new information and data models during their design process."}}
{"id": "2507.12653", "categories": ["cs.SE", "cs.CL", "H.4.m"], "pdf": "https://arxiv.org/pdf/2507.12653", "abs": "https://arxiv.org/abs/2507.12653", "authors": ["Jo\u00e3o Granja-Correia", "Remedios Hern\u00e1ndez-Linares", "Luca Ferranti", "Arm\u00e9nio Rego"], "title": "A Fuzzy Approach to Project Success: Measuring What Matters", "comment": "3 pages, 1 figure, presented at FUZZ-IEEE 2025", "summary": "This paper introduces a novel approach to project success evaluation by\nintegrating fuzzy logic into an existing construct. Traditional Likert-scale\nmeasures often overlook the context-dependent and multifaceted nature of\nproject success. The proposed hierarchical Type-1 Mamdani fuzzy system\nprioritizes sustained positive impact for end-users, reducing emphasis on\nsecondary outcomes like stakeholder satisfaction and internal project success.\nThis dynamic approach may provide a more accurate measure of project success\nand could be adaptable to complex evaluations. Future research will focus on\nempirical testing and broader applications of fuzzy logic in social science.", "AI": {"tldr": "The paper introduces a fuzzy logic system for better assessing project success, focusing on long-term end-user impact. It's expected to offer more accurate, adaptable evaluations than traditional Likert-based methods, with future research planned for empirical validation.", "motivation": "Traditional Likert-scale measures for evaluating project success fail to capture its complex, context-dependent nature. There is a need for a more nuanced and adaptable approach.", "method": "The paper proposes the integration of a hierarchical Type-1 Mamdani fuzzy logic system into existing project success constructs. This system gives greater priority to sustained positive impact on end-users while reducing the weight of secondary outcomes.", "result": "The new fuzzy logic-based system is theorized to provide a more accurate and dynamic evaluation of project success, better reflecting its true value in complex settings.", "conclusion": "Prioritizing sustained end-user impact using fuzzy logic provides a more comprehensive and adaptable method for evaluating project success. Further empirical research is needed to validate and expand its application."}}
{"id": "2507.12665", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.12665", "abs": "https://arxiv.org/abs/2507.12665", "authors": ["Salvador D. Escobedo"], "title": "Single Conversation Methodology: A Human-Centered Protocol for AI-Assisted Software Development", "comment": "Style reviewed by a LLM for improving clarity and English syntax", "summary": "We propose the Single Conversation Methodology (SCM), a novel and pragmatic\napproach to software development using large language models (LLMs). In\ncontrast to ad hoc interactions with generative AI, SCM emphasizes a structured\nand persistent development dialogue, where all stages of a project - from\nrequirements to architecture and implementation - unfold within a single,\nlong-context conversation. The methodology is grounded on principles of\ncognitive clarity, traceability, modularity, and documentation. We define its\nphases, best practices, and philosophical stance, while arguing that SCM offers\na necessary correction to the passive reliance on LLMs prevalent in current\npractices. We aim to reassert the active role of the developer as architect and\nsupervisor of the intelligent tool.", "AI": {"tldr": "The paper introduces SCM, a structured, single-conversation approach to software development using LLMs, restoring the developer's active role and promoting clarity, accountability, and best practices throughout the project lifecycle.", "motivation": "Current software development practices using LLMs tend to be ad hoc and passive, leading to inefficiency, lack of structure, and diminished developer control. There is a growing need for a more systematic methodology to leverage LLMs effectively while preserving developer agency and project clarity.", "method": "The paper introduces the Single Conversation Methodology (SCM), which emphasizes maintaining a single, persistent, structured conversation with LLMs throughout the entire software development process. It outlines phases, best practices, and the philosophical stance of active developer involvement.", "result": "The SCM provides a structured approach where all stages of software development use persistent context and dialogue with LLMs, leading to better cognitive clarity, traceability, modularity, and documentation. It shifts the developer\u2019s role to that of an active architect and supervisor.", "conclusion": "SCM addresses the shortcomings of passive and fragmented LLM use by reestablishing the developer as an active leader. This structured methodology promises more effective, accountable, and transparent software development using LLMs."}}
{"id": "2507.13035", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.13035", "abs": "https://arxiv.org/abs/2507.13035", "authors": ["Keila Lucas", "Rohit Gheyi", "M\u00e1rcio Ribeiro", "Fabio Palomba", "Luana Martins", "Elvys Soares"], "title": "Investigating the Performance of Small Language Models in Detecting Test Smells in Manual Test Cases", "comment": "7 pages, Accepted at Insightful Ideas and Emerging Results (IIER)\n  Track of the Brazilian Symposium on Software Engineering (SBES 2025)", "summary": "Manual testing, in which testers follow natural language instructions to\nvalidate system behavior, remains crucial for uncovering issues not easily\ncaptured by automation. However, these test cases often suffer from test\nsmells, quality issues such as ambiguity, redundancy, or missing checks that\nreduce test reliability and maintainability. While detection tools exist, they\ntypically require manual rule definition and lack scalability. This study\ninvestigates the potential of Small Language Models (SLMs) for automatically\ndetecting test smells. We evaluate Gemma3, Llama3.2, and Phi-4 on 143\nreal-world Ubuntu test cases, covering seven types of test smells. Phi-4\nachieved the best results, reaching a pass@2 of 97% in detecting sentences with\ntest smells, while Gemma3 and Llama3.2 reached approximately 91%. Beyond\ndetection, SLMs autonomously explained issues and suggested improvements, even\nwithout explicit prompt instructions. They enabled low-cost, concept-driven\nidentification of diverse test smells without relying on extensive rule\ndefinitions or syntactic analysis. These findings highlight the potential of\nSLMs as efficient tools that preserve data privacy and can improve test quality\nin real-world scenarios.", "AI": {"tldr": "This paper shows that small language models like Phi-4 can accurately detect and explain quality issues in manual software tests, outperforming traditional tools and larger models, and offer scalable, low-cost, and privacy-respecting solutions for improving test reliability.", "motivation": "Manual test cases written in natural language are essential but prone to 'test smells' (quality problems like ambiguity and missing checks) that compromise reliability and maintainability. Existing detection methods require manual rule creation, lack scalability, and may not preserve privacy. Automation and scalable detection methods are needed.", "method": "The study evaluated three SLMs\u2014Gemma3, Llama3.2, and Phi-4\u2014on 143 real-world Ubuntu test cases, targeting the detection of seven types of test smells. Model performance was measured by pass@2 in identifying sentences with test smells. The ability of SLMs to explain and improve test cases was also assessed.", "result": "Phi-4 achieved the highest accuracy, with a pass@2 rate of 97% in detecting test smells; Gemma3 and Llama3.2 reached around 91%. SLMs also offered autonomous issue explanations and suggestions for improvement without explicit instructions, enabling scalable, concept-driven, and privacy-preserving test smell detection.", "conclusion": "Small Language Models (SLMs) like Phi-4 can efficiently and accurately detect test smells in manual test cases, outperforming rule-based tools and larger models in certain aspects. They also provide explanations and improvement suggestions autonomously, indicating their potential for real-world testing scenarios while preserving data privacy."}}
{"id": "2507.13081", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.13081", "abs": "https://arxiv.org/abs/2507.13081", "authors": ["Dongming Jin", "Weisong Sun", "Jiangping Huang", "Peng Liang", "Jifeng Xuan", "Yang Liu", "Zhi Jin"], "title": "iReDev: A Knowledge-Driven Multi-Agent Framework for Intelligent Requirements Development", "comment": "22pages, 4 figures", "summary": "Requirements development is a critical phase as it is responsible for\nproviding a clear understanding of what stakeholders need. It involves\ncollaboration among stakeholders to extract explicit requirements and address\npotential conflicts, which is time-consuming and labor-intensive. Recently,\nmulti-agent systems for software development have attracted much attention.\nHowever, existing research provides limited support for requirements\ndevelopment and overlooks the injection of human knowledge into agents and the\nhuman-agent collaboration. % To address these issues, this paper proposes a\nknowledge-driven multi-agent framework for intelligent requirement development,\nnamed iReDev. iReDev features: iReDev consists of six knowledge-driven agents\nto support the entire requirements development. They collaboratively perform\nvarious tasks to produce a software requirements specification. iReDev focuses\non integrating human knowledge for agents, enabling them to simulate real-world\nstakeholders. iReDev uses an event-driven communication mechanism based on an\nartifact pool. Agents continuously monitor the pool and autonomously trigger\nthe next action based on its changes, enabling iReDev to handle new\nrequirements quickly. iReDev introduces a human-in-the-loop mechanism to\nsupport human-agent collaboration, ensuring that the generated artifacts align\nwith the expectations of stakeholders. We evaluated the generated artifacts and\nresults show that iReDev outperforms existing baselines in multiple aspects. We\nfurther envision three key directions and hope this work can facilitate the\ndevelopment of intelligent requirements development.", "AI": {"tldr": "iReDev is a new multi-agent system for smarter software requirements development. By mixing human knowledge with intelligent agents and providing a human-in-the-loop feedback loop, iReDev outperforms other methods and makes requirement engineering faster and more aligned with stakeholder needs.", "motivation": "Requirements development is crucial in software projects but is often time-consuming and requires heavy collaboration among diverse stakeholders. Existing multi-agent systems for software engineering provide inadequate support for requirements development, especially in harnessing human knowledge and enabling efficient human-agent collaboration.", "method": "The paper proposes a knowledge-driven multi-agent framework called iReDev, which features six specialized agents that collaborate to manage the full lifecycle of requirements development. iReDev integrates human knowledge into agent behaviors, uses an event-driven communication system centered on an artifact pool, and incorporates a human-in-the-loop mechanism to ensure outputs meet stakeholders\u2019 expectations.", "result": "iReDev was evaluated against existing baseline approaches. The results demonstrate that iReDev produces superior artifacts in multiple aspects of requirements engineering.", "conclusion": "The iReDev framework advances intelligent requirements development by leveraging human knowledge, enhancing agent collaboration, and enabling rapid adaptation to new requirements. The approach provides a foundation for further research in the field."}}
{"id": "2507.13095", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.13095", "abs": "https://arxiv.org/abs/2507.13095", "authors": ["Dongming Jin", "Zhi Jin", "Linyu Li", "Xiaohong Chen"], "title": "A Conceptual Framework for Requirements Engineering of Pretrained-Model-Enabled Systems", "comment": "5pages, 1 figure", "summary": "Recent advances in large pretrained models have led to their widespread\nintegration as core components in modern software systems. The trend is\nexpected to continue in the foreseeable future. Unlike traditional software\nsystems governed by deterministic logic, systems powered by pretrained models\nexhibit distinctive and emergent characteristics, such as ambiguous capability\nboundaries, context-dependent behavior, and continuous evolution. These\nproperties fundamentally challenge long-standing assumptions in requirements\nengineering, including functional decomposability and behavioral\npredictability. This paper investigates this problem and advocates for a\nrethinking of existing requirements engineering methodologies. We propose a\nconceptual framework tailored to requirements engineering of\npretrained-model-enabled software systems and outline several promising\nresearch directions within this framework. This vision helps provide a guide\nfor researchers and practitioners to tackle the emerging challenges in\nrequirements engineering of pretrained-model-enabled systems.", "AI": {"tldr": "Traditional requirements engineering struggles with modern software powered by large pretrained models. This paper proposes a new framework and research agenda to better address the unique challenges posed by these systems.", "motivation": "Large pretrained models are increasingly used as central components in software systems, creating challenges in requirements engineering due to their non-deterministic and evolving behavior.", "method": "The paper investigates the effect of integrating pretrained models on software requirements engineering, analyzes the limitations of current methodologies, and proposes a new conceptual framework.", "result": "The paper presents a tailored conceptual framework for requirements engineering in pretrained-model-enabled systems and identifies several research directions for advancing the field.", "conclusion": "A new conceptual framework and future research directions are proposed to address the unique challenges in requirements engineering for systems using large pretrained models."}}
{"id": "2507.13117", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.13117", "abs": "https://arxiv.org/abs/2507.13117", "authors": ["Andreas Pointner", "Josef Pichler", "Herbert Pr\u00e4hofer"], "title": "Inferring Attributed Grammars from Parser Implementations", "comment": "Accepted to ICSME 2025", "summary": "Software systems that process structured inputs often lack complete and\nup-to-date specifications, which specify the input syntax and the semantics of\ninput processing. While grammar mining techniques have focused on recovering\nsyntactic structures, the semantics of input processing remains largely\nunexplored. In this work, we introduce a novel approach for inferring\nattributed grammars from parser implementations. Given an input grammar, our\ntechnique dynamically analyzes the implementation of recursive descent parsers\nto reconstruct the semantic aspects of input handling, resulting in\nspecifications in the form of attributed grammars. By observing program\nexecutions and mapping the program's runtime behavior to the grammar, we\nsystematically extract and embed semantic actions into the grammar rules. This\nenables comprehensive specification recovery. We demonstrate the feasibility of\nour approach using an initial set of programs, showing that it can accurately\nreproduce program behavior through the generated attributed grammars.", "AI": {"tldr": "This paper presents a new technique for extracting complete attributed grammars, including semantics, from existing recursive descent parsers by dynamically analyzing program executions and mapping behaviors to grammar rules; experiments validate the method's accuracy in reproducing program logic.", "motivation": "Many software systems that process structured inputs lack complete and up-to-date specifications, especially regarding both the syntax and semantics of their input processing. Existing grammar mining techniques primarily recover syntax, leaving semantic aspects largely unexplored. There is a need for comprehensive techniques to recover both syntax and semantics from existing systems.", "method": "The authors propose a novel approach to infer attributed grammars from parser implementations. Their method dynamically analyzes recursive descent parser implementations. By observing and mapping runtime program executions to input grammar rules, they reconstruct and embed semantic actions into the grammar, producing attributed grammars that capture both syntax and semantics.", "result": "The approach is demonstrated on an initial set of programs. Results show that the generated attributed grammars can accurately reproduce the original program's behavior.", "conclusion": "It is feasible to recover comprehensive and accurate specifications\u2014including semantic actions\u2014from existing parser implementations. Their method advances specification mining by moving beyond syntactic recovery to full semantic specification extraction."}}
{"id": "2507.13123", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.13123", "abs": "https://arxiv.org/abs/2507.13123", "authors": ["Xin Yin", "Xinrui Li", "Chao Ni", "Xiaodan Xu", "Xiaohu Yang"], "title": "Detecting LLM-generated Code with Subtle Modification by Adversarial Training", "comment": null, "summary": "With the rapid development of Large Language Models (LLMs), their powerful\ncode-generation capabilities have been widely applied in tasks like code\ncompletion and automated development, demonstrating the value of improving\ncoding efficiency. However, the extensive use of LLM-generated code also raises\nseveral new challenges. On the one hand, issues such as the regulation of code\nprovenance, copyright disputes, and code quality have become increasingly\nconcerning. How to effectively detect LLM-generated code and ensure its\ncompliant and responsible use has become a critical and urgent issue. On the\nother hand, in practical applications, LLM-generated code is often subject to\nmanual modifications, such as variable renaming or structural adjustments.\nAlthough some recent studies have proposed training-based and zero-shot methods\nfor detecting LLM-generated code, these approaches show insufficient robustness\nwhen facing modified LLM-generated code, and there is a lack of an effective\nsolution. To address the real-world scenario where LLM-generated code may\nundergo minor modifications, we propose CodeGPTSensor+, an enhanced version of\nCodeGPTSensor, which employs adversarial training to improve robustness against\ninput perturbations. CodeGPTSensor+ integrates an adversarial sample generation\nmodule, Multi-objective Identifier and Structure Transformation (MIST), which\nsystematically generates both high-quality and representative adversarial\nsamples. This module effectively enhances the model's resistance against\ndiverse adversarial attacks. Experimental results on the HMCorp dataset\ndemonstrate that CodeGPTSensor+ significantly improves detection accuracy on\nthe adversarial test set while maintaining high accuracy on the original test\nset, showcasing superior robustness compared to CodeGPTSensor.", "AI": {"tldr": "The paper introduces CodeGPTSensor+, which uses adversarial training and the MIST module to robustly detect LLM-generated code, even after modifications, significantly improving detection accuracy and robustness compared to previous methods.", "motivation": "The proliferation of large language models (LLMs) in code-generation tasks raises concerns about code provenance, copyright disputes, and code quality. Detecting LLM-generated code, especially when it has undergone manual modifications, is challenging. Existing detection methods lack robustness against such modifications, creating a need for more effective and resilient solutions.", "method": "The paper proposes CodeGPTSensor+, an improved model that leverages adversarial training to detect LLM-generated code even when modified. It introduces the MIST module, which generates high-quality and representative adversarial samples to train the model, enhancing its resistance against various adversarial attacks and input perturbations.", "result": "CodeGPTSensor+ demonstrates significant improvement in detecting modified LLM-generated code on the adversarial test set of the HMCorp dataset, while retaining high accuracy on the original test set. It outperforms the original CodeGPTSensor in terms of robustness and accuracy.", "conclusion": "CodeGPTSensor+ provides a more robust and effective solution for detecting LLM-generated code, even after manual modifications, thus addressing real-world needs for compliance, quality, and provenance regulation."}}
