<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 23]
- [cs.PL](#cs.PL) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Knowledge-Guided Multi-Agent Framework for Automated Requirements Development: A Vision](https://arxiv.org/abs/2506.22656)
*Jiangping Huang,Dongming Jin,Weisong Sun,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: KGMAF is a knowledge-driven multi-agent system designed to automate and improve requirements development in software engineering, offering enhanced efficiency through specialized agents and an artifact pool. A case study shows its promise, pointing to future research potential in this area as LLMs become more prevalent.


<details>
  <summary>Details</summary>
Motivation: Current software engineering automation systems focus mainly on code development and do not sufficiently address the complex tasks involved in requirements development. There is a need for frameworks that improve the efficiency and accuracy of these critical upstream tasks.

Method: The paper proposes KGMAF, a knowledge-guided multi-agent framework, consisting of six specialized agents and an artifact pool. The design details each agent's function, actions, and knowledge. They also present a conceptual design for the artifact pool and validate the framework with a case study.

Result: The case study demonstrates the potential effectiveness of KGMAF in real-world software engineering scenarios and identifies research opportunities for advancing multi-agent automated requirements development.

Conclusion: KGMAF is positioned as a pivotal solution for advancing automated requirements development in software engineering, particularly in the era of large language models. The framework addresses gaps in current automation systems by providing specialized agents and a structured approach to complex requirements tasks.

Abstract: This paper envisions a knowledge-guided multi-agent framework named KGMAF for
automated requirements development. KGMAF aims to address gaps in current
automation systems for SE, which prioritize code development and overlook the
complexities of requirements tasks. KGMAF is composed of six specialized agents
and an artifact pool to improve efficiency and accuracy. Specifically, KGMAF
outlines the functionality, actions, and knowledge of each agent and provides
the conceptual design of the artifact pool. Our case study highlights the
potential of KGMAF in real-world scenarios. Finally, we outline several
research opportunities for implementing and enhancing automated requirements
development using multi-agent systems. We believe that KGMAF will play a
pivotal role in shaping the future of automated requirements development in the
era of LLMs.

</details>


### [2] [An LLM-assisted approach to designing software architectures using ADD](https://arxiv.org/abs/2506.22688)
*Humberto Cervantes,Rick Kazman,Yuanfang Cai*

Main category: cs.SE

TL;DR: The paper introduces and assesses an LLM-assisted method for software architecture design using the ADD process, showing promising but imperfect results. LLMs can help generate plausible architectures but still depend on expert human guidance for optimal outcomes.


<details>
  <summary>Details</summary>
Motivation: Software architecture design is a complex task, historically dependent on expert opinion. With the rise of Large Language Models (LLMs), there is a need to evaluate their effectiveness and role in supporting this process, especially using established methods like Attribute-Driven Design (ADD).

Method: The authors propose an LLM-assisted design method based on ADD. The approach gives the LLMs a structured description of ADD, an architect persona, and a step-by-step iteration plan, enabling the LLM to collaboratively develop architecture artifacts with a human architect. They validate this approach through case studies, comparing LLM-generated designs to existing solutions and seeking evaluation from professional architects.

Result: The LLM-assisted ADD approach can generate software architectures that are similar to established solutions and partially satisfy required architectural drivers. However, there are limitations, such as incomplete satisfaction of drivers, indicating the need for ongoing human involvement and oversight.

Conclusion: LLMs show significant potential to assist in software architecture design when guided through explicit methods like ADD. Human expertise and iterative refinement remain essential to achieving high-quality results, as LLMs currently have limitations in fully understanding and satisfying complex architectural requirements.

Abstract: Designing effective software architectures is a complex, iterative process
that traditionally relies on expert judgment. This paper proposes an approach
for Large Language Model (LLM)-assisted software architecture design using the
Attribute-Driven Design (ADD) method. By providing an LLM with an explicit
description of ADD, an architect persona, and a structured iteration plan, our
method guides the LLM to collaboratively produce architecture artifacts with a
human architect. We validate the approach through case studies, comparing
generated designs against proven solutions and evaluating them with
professional architects. Results show that our LLM-assisted ADD process can
generate architectures closely aligned with established solutions and partially
satisfying architectural drivers, highlighting both the promise and current
limitations of using LLMs in architecture design. Our findings emphasize the
importance of human oversight and iterative refinement when leveraging LLMs in
this domain.

</details>


### [3] [P4OMP: Retrieval-Augmented Prompting for OpenMP Parallelism in Serial Code](https://arxiv.org/abs/2506.22703)
*Wali Mohammad Abdullah,Azmain Kabir*

Main category: cs.SE

TL;DR: P4OMP is a framework that uses retrieval-augmented prompts to guide LLMs in generating syntactically correct OpenMP-parallelized code from serial C/C++ sources. It achieves higher compilation and runtime reliability than baseline LLM prompting, offering a robust, practical tool for automatic code parallelization.


<details>
  <summary>Details</summary>
Motivation: Current LLMs often produce incorrect OpenMP pragma annotations when converting serial C/C++ code to parallel code, mainly due to a lack of structured guidance or context, leading to syntactic and scoping errors. Improving prompt reliability and output correctness for parallel code generation is an unsolved and important challenge.

Method: P4OMP is a retrieval-augmented generation (RAG) framework that feeds instructional knowledge, retrieved from OpenMP tutorials, into the prompting context of large language models (specifically, GPT-3.5-Turbo) for code generation. No model fine-tuning or compiler instrumentation is involved. Performance was benchmarked against GPT-3.5-Turbo without retrieval across 108 real-world C++ programs, focusing on compilation correctness and runtime scaling.

Result: P4OMP achieved 100% compilation success for all parallelizable test cases (after excluding those fundamentally incompatible with OpenMP), whereas the baseline (no retrieval augmentation) failed in 20/108 cases. P4OMP effectively prevented common errors like scoping mistakes and invalid directive usage. Additionally, the system demonstrated strong runtime performance and scalability across various compute-intensive benchmarks.

Conclusion: By incorporating retrieval-based prompting with OpenMP instructional content, P4OMP significantly enhances the correctness and reliability of LLM-generated OpenMP parallel code. The modular pipeline also promotes wider applicability and more robust code generation, addressing a critical gap that limits the practical use of LLMs for code parallelization.

Abstract: We present P4OMP, a retrieval-augmented framework for transforming serial
C/C++ code into OpenMP-annotated parallel code using large language models
(LLMs). To our knowledge, this is the first system to apply retrieval-based
prompting for OpenMP pragma correctness without model fine-tuning or compiler
instrumentation. P4OMP leverages Retrieval-Augmented Generation (RAG) with
structured instructional knowledge from OpenMP tutorials to improve the
reliability of prompt-driven code generation. By grounding generation in the
retrieved context, P4OMP improves syntactic correctness compared to baseline
prompting with GPT-3.5-Turbo. We evaluate P4OMP against a baseline,
GPT-3.5-Turbo without retrieval, on a comprehensive benchmark of 108 real-world
C++ programs drawn from Stack Overflow, PolyBench, and NAS benchmark suites.
P4OMP achieves 100% compilation success on all parallelizable cases, while the
baseline fails to compile in 20 out of 108 cases. Six cases that rely on
non-random-access iterators or thread-unsafe constructs are excluded due to
fundamental OpenMP limitations. A detailed analysis demonstrates how P4OMP
consistently avoids scoping errors, syntactic misuse, and invalid directive
combinations that commonly affect baseline-generated code. We further
demonstrate strong runtime scaling across seven compute-intensive benchmarks on
an HPC cluster. P4OMP offers a robust, modular pipeline that significantly
improves the reliability and applicability of LLM-generated OpenMP code.

</details>


### [4] [RAILS: Retrieval-Augmented Intelligence for Learning Software Development](https://arxiv.org/abs/2506.22742)
*Wali Mohammad Abdullah,Md. Morshedul Islam,Devraj Parmar,Happy Hasmukhbhai Patel,Sindhuja Prabhakaran,Baidya Saha*

Main category: cs.SE

TL;DR: RAILS boosts LLM-powered software development by semantically retrieving relevant Java documentation and refining code suggestions via compiler feedback, leading to fewer errors and more accurate imports compared to basic prompting.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) like GPT-3.5-Turbo assist in software development but frequently generate incomplete code or incorrect imports, mainly due to limited access to external or project-specific documentation. There is a need for methods that can improve the reliability and accuracy of LLM-generated code in such settings.

Method: The authors introduce RAILS, a Retrieval-Augmented Intelligence framework that enhances LLM prompts by semantically retrieving context from curated Java resources using FAISS and OpenAI embeddings. It also includes an iterative validation loop that utilizes compiler feedback to refine the suggestions.

Result: In testing with 78 real-world Java import error cases, RAILS showed improved performance over baseline prompting. It was better at preserving developer intent, avoiding hallucinated code, and providing correct imports, even for libraries not available locally.

Conclusion: RAILS significantly augments the capabilities of LLMs in code generation by effectively sourcing relevant context and iteratively validating suggestions. It outperforms traditional prompting, especially regarding Java import errors, and future enhancements are planned for broader applicability.

Abstract: Large Language Models (LLMs) like GPT-3.5-Turbo are increasingly used to
assist software development, yet they often produce incomplete code or
incorrect imports, especially when lacking access to external or
project-specific documentation. We introduce RAILS (Retrieval-Augmented
Intelligence for Learning Software Development), a framework that augments LLM
prompts with semantically retrieved context from curated Java resources using
FAISS and OpenAI embeddings. RAILS incorporates an iterative validation loop
guided by compiler feedback to refine suggestions. We evaluated RAILS on 78
real-world Java import error cases spanning standard libraries, GUI APIs,
external tools, and custom utilities. Despite using the same LLM, RAILS
outperforms baseline prompting by preserving intent, avoiding hallucinations,
and surfacing correct imports even when libraries are unavailable locally.
Future work will integrate symbolic filtering via PostgreSQL and extend support
to other languages and IDEs.

</details>


### [5] [Privacy-Preserving Methods for Bug Severity Prediction](https://arxiv.org/abs/2506.22752)
*Havvanur Dervişoğlu,Ruşen Halepmollası,Elif Eyvaz*

Main category: cs.SE

TL;DR: Federated learning and synthetic data can predict bug severity as effectively as traditional centralized approaches, while preserving data privacy—important for real-world industrial applications where sharing data is a challenge.


<details>
  <summary>Details</summary>
Motivation: Bug severity prediction helps allocate resources efficiently in software maintenance. However, in industrial settings, it is difficult to share or obtain large labeled datasets due to privacy and availability issues, thereby limiting the effectiveness of AI models.

Method: This study investigates bug severity prediction at the method level using source code metrics and Large Language Models (LLMs). It compares centralized learning, federated learning, and synthetic data generation methods for training models, evaluating them on two well-known software defect datasets.

Result: Federated learning and models using synthetic data performed comparably to traditionally trained (centralized) models, even without data sharing.

Conclusion: Privacy-preserving methods like federated learning and synthetic data generation can provide effective bug severity prediction in situations where data sharing is restricted, making them promising for industrial applications.

Abstract: Bug severity prediction is a critical task in software engineering as it
enables more efficient resource allocation and prioritization in software
maintenance. While AI-based analyses and models significantly require access to
extensive datasets, industrial applications face challenges due to data-sharing
constraints and the limited availability of labeled data. In this study, we
investigate method-level bug severity prediction using source code metrics and
Large Language Models (LLMs) with two widely used datasets. We compare the
performance of models trained using centralized learning, federated learning,
and synthetic data generation. Our experimental results, obtained using two
widely recognized software defect datasets, indicate that models trained with
federated learning and synthetic data achieve comparable results to centrally
trained models without data sharing. Our finding highlights the potential of
privacy-preserving approaches such as federated learning and synthetic data
generation to enable effective bug severity prediction in industrial context
where data sharing is a major challenge.
  The source code and dataset are available at our GitHub repository:
https://github.com/drvshavva/EASE2025-Privacy-Preserving-Methods-for-Bug-Severity-Prediction.

</details>


### [6] [Smaller = Weaker? Benchmarking Robustness of Quantized LLMs in Code Generation](https://arxiv.org/abs/2506.22776)
*Sen Fang,Weiyuan Ding,Antonio Mastropaolo,Bowen Xu*

Main category: cs.SE

TL;DR: Quantizing large language models for code generation not only reduces computational needs but also improves robustness to adversarial attacks and noise, outperforming original models in most tested scenarios.


<details>
  <summary>Details</summary>
Motivation: While quantization is commonly used to compress Large Language Models (LLMs) and speed up inference, its impact on model robustness, especially for code generation tasks, has not been systematically explored. Given the importance of both efficiency and reliability in model deployment, understanding the trade-offs, if any, between quantization and robustness is crucial.

Method: The authors conduct a comprehensive evaluation of quantized versus full-precision LLMs across four major model families (LLaMA, DeepSeek, CodeGen, and StarCoder) at multiple parameter scales (350M to 33B). They measure robustness from two perspectives: (1) susceptibility to adversarial attacks on input prompts, and (2) resilience to random noise perturbations applied to the model weights.

Result: Quantized LLMs demonstrate higher robustness than their full-precision counterparts in code generation tasks. Specifically, 51.59% of adversarial attack scenarios favored quantized models (compared to 42.86% for full-precision), and noise perturbation tests showed quantized LLMs generally tolerated higher levels of weight disturbance.

Conclusion: Contrary to expectations, quantization not only compresses LLMs and accelerates inference but also enhances their robustness in code generation tasks. These findings inform strategies for deploying more robust and efficient LLMs.

Abstract: Quantization has emerged as a mainstream method for compressing Large
Language Models (LLMs), reducing memory requirements and accelerating inference
without architectural modifications. While existing research primarily focuses
on evaluating the effectiveness of quantized LLMs compared to their original
counterparts, the impact on robustness remains largely unexplored.In this
paper, we present the first systematic investigation of how quantization
affects the robustness of LLMs in code generation tasks. Through extensive
experiments across four prominent LLM families (LLaMA, DeepSeek, CodeGen, and
StarCoder) with parameter scales ranging from 350M to 33B, we evaluate
robustness from dual perspectives: adversarial attacks on input prompts and
noise perturbations on model architecture. Our findings challenge conventional
wisdom by demonstrating that quantized LLMs often exhibit superior robustness
compared to their full-precision counterparts, with 51.59% versus 42.86% of our
adversarial experiments showing better resilience in quantized LLMs. Similarly,
our noise perturbation experiments also confirm that LLMs after quantitation
generally withstand higher levels of weight disturbances. These results suggest
that quantization not only reduces computational requirements but can actually
enhance LLMs' reliability in code generation tasks, providing valuable insights
for developing more robust and efficient LLM deployment strategies.

</details>


### [7] [Generating Privacy Stories From Software Documentation](https://arxiv.org/abs/2506.23014)
*Wilder Baldwin,Shashank Chintakuntla,Shreyah Parajuli,Ali Pourghasemi,Ryan Shanz,Sepideh Ghanavati*

Main category: cs.SE

TL;DR: The paper introduces a method using LLMs to extract privacy information from software documents and generate privacy user stories, achieving high accuracy and showing promise for integrating privacy into software development.


<details>
  <summary>Details</summary>
Motivation: Privacy is often treated as a security issue or considered as an afterthought by analysts and developers, leading to non-compliant and privacy-violating software. Current methods focus mainly on legal compliance rather than embedding privacy requirements during development.

Method: The paper proposes a new approach that uses chain-of-thought prompting, in-context learning, and Large Language Models (LLMs) like GPT-4o and Llama 3 to automatically extract privacy behaviors from software documents and generate privacy requirements in the form of user stories.

Result: The approach allows popular LLMs to generate privacy user stories from software documents with F1 scores above 0.8. The study also shows that model performance can be further improved via parameter tuning.

Conclusion: LLMs can reliably identify privacy behaviors and generate corresponding privacy user stories during software development. Optimizing these models can further enhance their effectiveness, offering a powerful tool for integrating privacy requirements early in the development lifecycle.

Abstract: Research shows that analysts and developers consider privacy as a security
concept or as an afterthought, which may lead to non-compliance and violation
of users' privacy. Most current approaches, however, focus on extracting legal
requirements from the regulations and evaluating the compliance of software and
processes with them. In this paper, we develop a novel approach based on
chain-of-thought prompting (CoT), in-context-learning (ICL), and Large Language
Models (LLMs) to extract privacy behaviors from various software documents
prior to and during software development, and then generate privacy
requirements in the format of user stories. Our results show that most commonly
used LLMs, such as GPT-4o and Llama 3, can identify privacy behaviors and
generate privacy user stories with F1 scores exceeding 0.8. We also show that
the performance of these models could be improved through parameter-tuning. Our
findings provide insight into using and optimizing LLMs for generating privacy
requirements given software documents created prior to or throughout the
software development lifecycle.

</details>


### [8] [Guiding AI to Fix Its Own Flaws: An Empirical Study on LLM-Driven Secure Code Generation](https://arxiv.org/abs/2506.23034)
*Hao Yan,Swapneel Suhas Vaidya,Xiaokuan Zhang,Ziyu Yao*

Main category: cs.SE

TL;DR: LLMs are prone to producing insecure code, but their security improves noticeably when given vulnerability hints and detailed feedback. The paper offers insights and recommendations to help developers reduce code vulnerabilities when using LLMs.


<details>
  <summary>Details</summary>
Motivation: Despite LLMs' success in automating code generation, they often overlook security, leading to vulnerable code. There is a lack of research on how to guide LLMs toward generating secure code or repairing insecure code, necessitating a thorough study of their security capabilities and improvement strategies.

Method: The paper presents a comprehensive evaluation using established benchmarks, comparing both proprietary and open-weight LLMs at different scales, analyzing their code generation with and without vulnerability hints, and assessing their vulnerability repair effectiveness with varying feedback levels. Both quantitative and qualitative analyses are performed.

Result: LLMs have a tendency to produce insecure code by default. However, state-of-the-art models demonstrate a clear ability to generate more secure code and fix vulnerabilities when provided with vulnerability hints and fine-grained feedback. The paper also offers practical recommendations for safer LLM-based code generation.

Conclusion: LLMs often generate insecure code by default, but advanced models can improve their security with guidance such as vulnerability hints and detailed feedback. Developers can take practical steps to reduce vulnerabilities when relying on LLMs for code generation.

Abstract: Large Language Models (LLMs) have become powerful tools for automated code
generation. However, these models often overlook critical security practices,
which can result in the generation of insecure code that contains
vulnerabilities-weaknesses or flaws in the code that attackers can exploit to
compromise a system. However, there has been limited exploration of strategies
to guide LLMs in generating secure code and a lack of in-depth analysis of the
effectiveness of LLMs in repairing code containing vulnerabilities. In this
paper, we present a comprehensive evaluation of state-of-the-art LLMs by
examining their inherent tendencies to produce insecure code, their capability
to generate secure code when guided by self-generated vulnerability hints, and
their effectiveness in repairing vulnerabilities when provided with different
levels of feedback. Our study covers both proprietary and open-weight models
across various scales and leverages established benchmarks to assess a wide
range of vulnerability types. Through quantitative and qualitative analyses, we
reveal that although LLMs are prone to generating insecure code, advanced
models can benefit from vulnerability hints and fine-grained feedback to avoid
or fix vulnerabilities. We also provide actionable suggestions to developers to
reduce vulnerabilities when using LLMs for code generation.

</details>


### [9] [On the Feasibility of Deduplicating Compiler Bugs with Bisection](https://arxiv.org/abs/2506.23281)
*Xintong Zhou,Zhenyang Xu,Chengnian Sun*

Main category: cs.SE

TL;DR: BugLens uses bisection to make compiler bug deduplication simpler and more effective, requiring less human effort than previous methods.


<details>
  <summary>Details</summary>
Motivation: Random testing finds many compiler bugs, but debugging is hard because many test cases are duplicates of the same bug. Existing deduplication techniques are computationally expensive and lack generalizability.

Method: The authors study using bisection—a debugging technique—to pinpoint failure-inducing commits as a criterion for bug deduplication. They develop BugLens, a tool that improves deduplication by combining bisection with the identification of bug-triggering optimizations.

Result: BugLens was empirically evaluated on four real-world datasets and significantly outperformed two state-of-the-art methods (Tamer and D3), reducing human effort by 26.98% and 9.64% on average to deduplicate the same number of unique bugs.

Conclusion: Bisection, due to its simplicity and generalizability, offers a practical alternative for compiler bug deduplication. BugLens leverages bisection with enhancements to effectively outperform analysis-based methods in reducing deduplication effort.

Abstract: Random testing has proven to be an effective technique for compiler
validation. However, the debugging of bugs identified through random testing
presents a significant challenge due to the frequent occurrence of duplicate
test programs that expose identical compiler bugs. The process to identify
duplicates is a practical research problem known as bug deduplication. Prior
methodologies for compiler bug deduplication primarily rely on program analysis
to extract bug-related features for duplicate identification, which can result
in substantial computational overhead and limited generalizability. This paper
investigates the feasibility of employing bisection, a standard debugging
procedure largely overlooked in prior research on compiler bug deduplication,
for this purpose. Our study demonstrates that the utilization of bisection to
locate failure-inducing commits provides a valuable criterion for
deduplication, albeit one that requires supplementary techniques for more
accurate identification. Building on these results, we introduce BugLens, a
novel deduplication method that primarily uses bisection, enhanced by the
identification of bug-triggering optimizations to minimize false negatives.
Empirical evaluations conducted on four real-world datasets demonstrate that
BugLens significantly outperforms the state-of-the-art analysis-based
methodologies Tamer and D3 by saving an average of 26.98% and 9.64% human
effort to identify the same number of distinct bugs. Given the inherent
simplicity and generalizability of bisection, it presents a highly practical
solution for compiler bug deduplication in real-world applications.

</details>


### [10] [HF-DGF: Hybrid Feedback Guided Directed Grey-box Fuzzing](https://arxiv.org/abs/2506.23063)
*Guangfa Lyu,Zhenzhong Cao,Xiaofei Ren,Fengyu Wang*

Main category: cs.SE

TL;DR: HF-DGF is a new directed grey-box fuzzing framework that combines multiple feedback metrics for improved crash reproduction. It is significantly faster and more efficient than leading tools like AFL, AFLGo, and WindRanger, thanks to its hybrid feedback and selective instrumentation approach.


<details>
  <summary>Details</summary>
Motivation: Directed Grey-box Fuzzing (DGF) faces limitations due to insufficient runtime feedback, reducing its effectiveness in crash reproduction and patch testing. There is a need for improved efficiency and target reachability in DGF tools.

Method: HF-DGF, a new DGF framework, integrates three feedback signals: control-flow distance (using a backward-stepping algorithm on a virtual ICFG), value-flow influence score, and slice coverage. A hybrid feedback guides seed scheduling, and selective instrumentation reduces runtime overhead.

Result: HF-DGF surpasses existing DGF tools on 41 real-world vulnerabilities, reproducing crashes significantly faster than AFL, AFLGo, WindRanger, DAFL, and Beacon. It achieved the lowest code coverage among fuzzers that found crashes, indicating greater directionality and efficiency, and outperformed competitors in static analysis efficiency.

Conclusion: By integrating hybrid feedback mechanisms and novel analysis techniques, HF-DGF offers significant improvements in speed, efficiency, and directionality for crash reproduction and vulnerability detection over previous DGF tools.

Abstract: Directed Grey-box Fuzzing (DGF) has emerged as a widely adopted technique for
crash reproduction and patch testing, leveraging its capability to precisely
navigate toward target locations and exploit vulnerabilities. However, current
DGF tools are constrained by insufficient runtime feedback, limiting their
efficiency in reaching targets and exploring state spaces. This study presents
HF-DGF, a novel directed grey-box fuzzing framework. Its seed scheduling is
guided by a hybrid feedback mechanism integrating control-flow distance,
value-flow influence score, and slice coverage. To enable precise control-flow
distance feedback, we propose a backward-stepping algorithm to calculate basic
block-level seed distances on a virtual inter-procedural control-flow graph
(ICFG). For effective state space exploration, we introduce value-flow
influence and a corresponding metric, the value-flow influence score.
Additionally, to mitigate runtime overhead from hybrid feedback, we adopt a
novel selective instrumentation strategy. Evaluations on 41 real-world
vulnerabilities show HF-DGF outperforms existing tools: it achieves crash
reproduction 5.05 times faster than AFL, 5.79 times faster than AFLGo, 73.75
times faster than WindRanger, 2.56 times faster than DAFL, and 8.45 times
faster than Beacon on average. Notably, when all fuzzers triggered crashes,
HF-DGF exhibited the lowest code coverage, demonstrating superior
directionality and efficiency. It also surpasses AFLGo, WindRanger, DAFL, and
Beacon in static analysis efficiency.

</details>


### [11] [What Challenges Do Developers Face When Using Verification-Aware Programming Languages?](https://arxiv.org/abs/2506.23696)
*Francisco Oliveira,Alexandra Mendes,Carolina Carreira*

Main category: cs.SE

TL;DR: The paper analyzes why Verification-Aware programming languages, which improve software correctness, are underused. By examining forum discussions and surveying developers, it finds that complexity, poor usability, and lack of good integration are main barriers. The authors recommend better interfaces, educational materials, and integration to improve adoption.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the limited adoption of Verification-Aware (VA) programming languages, which offer strong software reliability guarantees through Design by Contract (DbC) and formal verification, yet remain underutilized in practice.

Method: The authors used a mixed-methods approach, analyzing developer discussions from public forums using topic modeling techniques and supplementing these findings with a survey of developers to identify barriers and challenges in adopting VA languages.

Result: Key obstacles to VA language adoption identified include steep learning curves and usability issues. The study reveals that users struggle with complex interfaces, insufficient educational materials, and poor integration into development environments.

Conclusion: By synthesizing findings from discussion analysis and surveys, the study concludes that improving tool interfaces, offering better educational resources, and enhancing integration with standard development environments can make VA languages more usable and accessible, potentially increasing their adoption.

Abstract: Software reliability is critical in ensuring that the digital systems we
depend on function correctly. In software development, increasing software
reliability often involves testing. However, for complex and critical systems,
developers can use Design by Contract (DbC) methods to define precise
specifications that software components must satisfy. Verification-Aware (VA)
programming languages support DbC and formal verification at compile-time or
run-time, offering stronger correctness guarantees than traditional testing.
However, despite the strong guarantees provided by VA languages, their adoption
remains limited. In this study, we investigate the barriers to adopting VA
languages by analyzing developer discussions on public forums using topic
modeling techniques. We complement this analysis with a developer survey to
better understand the practical challenges associated with VA languages. Our
findings reveal key obstacles to adoption, including steep learning curves and
usability issues. Based on these insights, we identify actionable
recommendations to improve the usability and accessibility of VA languages. Our
findings suggest that simplifying tool interfaces, providing better educational
materials, and improving integration with everyday development environments
could improve the usability and adoption of these languages. Our work provides
actionable insights for improving the usability of VA languages and making
verification tools more accessible.

</details>


### [12] [Repair Ingredients Are All You Need: Improving Large Language Model-Based Program Repair via Repair Ingredients Search](https://arxiv.org/abs/2506.23100)
*Jiayi Zhang,Kai Huang,Jian Zhang,Yang Liu,Chunyang Chen*

Main category: cs.SE

TL;DR: ReinFix improves automated program repair by helping LLMs find relevant code info and past fixes, outperforming previous methods on major benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based Automated Program Repair (APR) techniques show promise but often fail to generate contextually relevant and accurate patches due to the neglect of important repair ingredients.

Method: ReinFix, a new framework, enables LLMs to search for both internal and external repair ingredients. In the reasoning phase, it uses static analysis tools to retrieve relevant code information. In the solution phase, it searches historical bug fixes with similar patterns to inform patch generation.

Result: ReinFix outperforms state-of-the-art baselines, fixing 32 more bugs on Defects4J V1.2 and 38 more on Defects4J V2.0. It also maintains top performance on data leakage-free benchmarks.

Conclusion: ReinFix effectively enhances LLM-based APR by integrating ingredient search mechanisms, providing greater contextual relevance and accuracy in automated bug fixing.

Abstract: Automated Program Repair (APR) techniques aim to automatically fix buggy
programs. Among these, Large Language Model-based (LLM-based) approaches have
shown great promise. Recent advances demonstrate that directly leveraging LLMs
can achieve leading results. However, these techniques remain suboptimal in
generating contextually relevant and accurate patches, as they often overlook
repair ingredients crucial for practical program repair. In this paper, we
propose ReinFix, a novel framework that enables LLMs to autonomously search for
repair ingredients throughout both the reasoning and solution phases of bug
fixing. In the reasoning phase, ReinFix integrates static analysis tools to
retrieve internal ingredients, such as variable definitions, to assist the LLM
in root cause analysis when it encounters difficulty understanding the context.
During the solution phase, when the LLM lacks experience in fixing specific
bugs, ReinFix searches for external ingredients from historical bug fixes with
similar bug patterns, leveraging both the buggy code and its root cause to
guide the LLM in identifying appropriate repair actions, thereby increasing the
likelihood of generating correct patches. Evaluations on two popular benchmarks
(Defects4J V1.2 and V2.0) demonstrate the effectiveness of our approach over
SOTA baselines. Notably, ReinFix fixes 146 bugs, which is 32 more than the
baselines on Defects4J V1.2. On Defects4J V2.0, ReinFix fixes 38 more bugs than
the SOTA. Importantly, when evaluating on the recent benchmarks that are free
of data leakage risk, ReinFix also maintains the best performance.

</details>


### [13] [From Release to Adoption: Challenges in Reusing Pre-trained AI Models for Downstream Developers](https://arxiv.org/abs/2506.23234)
*Peerachai Banyongrakkul,Mansooreh Zahedi,Patanamon Thongtanunam,Christoph Treude,Haoyu Gao*

Main category: cs.SE

TL;DR: The paper identifies and categorizes key challenges developers face in reusing pre-trained models in software projects, showing such issues are harder and slower to resolve compared to others. This highlights a need for better support and targeted research for PTM reuse.


<details>
  <summary>Details</summary>
Motivation: Pre-trained models (PTMs) are widely adopted due to their strong performance and accessibility, but the software engineering challenges faced by downstream developers who integrate these models are not well understood.

Method: The authors collected and qualitatively analyzed 840 PTM-related issue reports from 31 open-source GitHub projects, developed a taxonomy of PTM-related challenges, compared these with existing taxonomies, and analyzed issue resolution times using statistical tests.

Result: Seven main categories of challenges were identified, including model usage, model performance, and output quality. PTM-related issues take significantly longer to resolve than non-PTM-related issues, with resolution time varying by category.

Conclusion: There are systematic, under-explored challenges for downstream developers using PTMs, as evidenced by prolonged issue resolution and distinct challenge types. The findings suggest areas for improved support, guidance, and future research directions.

Abstract: Pre-trained models (PTMs) have gained widespread popularity and achieved
remarkable success across various fields, driven by their groundbreaking
performance and easy accessibility through hosting providers. However, the
challenges faced by downstream developers in reusing PTMs in software systems
are less explored. To bridge this knowledge gap, we qualitatively created and
analyzed a dataset of 840 PTM-related issue reports from 31 OSS GitHub
projects. We systematically developed a comprehensive taxonomy of PTM-related
challenges that developers face in downstream projects. Our study identifies
seven key categories of challenges that downstream developers face in reusing
PTMs, such as model usage, model performance, and output quality. We also
compared our findings with existing taxonomies. Additionally, we conducted a
resolution time analysis and, based on statistical tests, found that
PTM-related issues take significantly longer to be resolved than issues
unrelated to PTMs, with significant variation across challenge categories. We
discuss the implications of our findings for practitioners and possibilities
for future research.

</details>


### [14] [Improving vulnerability type prediction and line-level detection via adversarial training-based data augmentation and multi-task learning](https://arxiv.org/abs/2506.23534)
*Siyu Chen,Jiongyi Yang,Xiang Chen,Menglin Zheng,Minnan Wei,Xiaolin Ju*

Main category: cs.SE

TL;DR: This paper presents a unified adversarial multi-task learning method that significantly enhances both vulnerability type prediction and line-level detection in software security, especially for rare vulnerability types, and outperforms previous approaches.


<details>
  <summary>Details</summary>
Motivation: Software vulnerabilities continue to rise, threatening modern systems and underscoring the need for effective automated solutions. The challenges addressed include scarcity of labeled samples, class imbalance in datasets, and the commonly neglected correlation between vulnerability type prediction (VTP) and line-level vulnerability detection (LVD).

Method: The paper proposes a unified approach using Embedding-Layer Driven Adversarial Training (EDAT) combined with Multi-task Learning (MTL). EDAT introduces adversarial perturbations to identifier embeddings (guided by semantic importance) to improve robustness, while MTL leverages shared representations and inter-task correlations between VTP and LVD.

Result: The proposed approach outperforms state-of-the-art baselines on both VTP and LVD. It boosts accuracy, precision, recall, and F1-score, especially for rare vulnerability types in VTP, and improves line-level detection accuracy with fewer false positives in LVD.

Conclusion: Combining EDAT with MTL yields a unified, effective solution for both VTP and LVD, improving performance and highlighting the value of joint task modeling for future research.

Abstract: Context: Software vulnerabilities pose a significant threat to modern
software systems, as evidenced by the growing number of reported
vulnerabilities and cyberattacks. These escalating trends underscore the urgent
need for effective approaches that can automatically detect and understand
software vulnerabilities. Objective: However, the scarcity of labeled samples
and the class imbalance issue in vulnerability datasets present significant
challenges for both Vulnerability Type Prediction (VTP) and Line-level
Vulnerability Detection (LVD), especially for rare yet critical vulnerability
types. Moreover, most existing studies treat VTP and LVD as independent tasks,
overlooking their inherent correlation, which limits the potential to leverage
shared semantic patterns across tasks. Methods: To address these limitations,
we propose a unified approach that integrates Embedding-Layer Driven
Adversarial Training (EDAT) with Multi-task Learning (MTL). Specifically, EDAT
enhances model robustness by introducing adversarial perturbations to
identifier embeddings, guided by semantic importance. Meanwhile, MTL improves
overall performance by leveraging shared representations and inter-task
correlations between VTP and LVD. Results: Extensive experiments demonstrate
that our proposed approach outperforms state-of-the-art baselines on both VTP
and LVD tasks. For VTP, it yields notable improvements in accuracy, precision,
recall, and F1-score, particularly in identifying rare vulnerability types.
Similarly, for LVD, our approach enhances line-level detection accuracy while
significantly reducing false positives. Conclusion: Our study demonstrates that
combining EDAT with MTL provides a unified solution that improves performance
on both tasks and warrants further investigation.

</details>


### [15] [Comparative Analysis of the Code Generated by Popular Large Language Models (LLMs) for MISRA C++ Compliance](https://arxiv.org/abs/2506.23535)
*Malik Muhammad Umer*

Main category: cs.SE

TL;DR: This paper checks how well code generated by various leading Large Language Models (LLMs) such as ChatGPT and Gemini complies with strict safety standards (MISRA C++) needed in domains like avionics. LLM-generated code is compared and assessed for adherence, underlining the importance of oversight in safety-critical applications.


<details>
  <summary>Details</summary>
Motivation: Safety-critical systems, such as those in avionics, require highly reliable software due to their potential for catastrophic failures. With increasing interest in using Large Language Models (LLMs) for automatic code generation, it is important to examine if these AI-generated codes adhere to strict industry standards like MISRA C++.

Method: The paper conducts a comparative analysis of C++ code generated by popular LLMs (OpenAI ChatGPT, Google Gemini, DeepSeek, Meta AI, and Microsoft Copilot) to determine their compliance with the MISRA C++ coding standards, which are essential for safety-critical software development.

Result: The study evaluates and compares the extent to which output from different LLMs meets or violates the MISRA C++ coding guidelines. Specific results in terms of compliance rates or common violations are not included in the abstract.

Conclusion: The analysis highlights the need for careful evaluation of LLM-generated code in the context of safety-critical systems, as adherence to established standards like MISRA C++ is crucial for certification and system reliability.

Abstract: Safety-critical systems are engineered systems whose failure or malfunction
could result in catastrophic consequences. The software development for
safety-critical systems necessitates rigorous engineering practices and
adherence to certification standards like DO-178C for avionics. DO-178C is a
guidance document which requires compliance to well-defined software coding
standards like MISRA C++ to enforce coding guidelines that prevent the use of
ambiguous, unsafe, or undefined constructs. Large Language Models (LLMs) have
demonstrated significant capabilities in automatic code generation across a
wide range of programming languages, including C++. Despite their impressive
performance, code generated by LLMs in safety-critical domains must be
carefully analyzed for conformance to MISRA C++ coding standards. In this
paper, I have conducted a comparative analysis of the C++ code generated by
popular LLMs including: OpenAI ChatGPT, Google Gemini, DeepSeek, Meta AI, and
Microsoft Copilot for compliance with MISRA C++.

</details>


### [16] [QLPro: Automated Code Vulnerability Discovery via LLM and Static Code Analysis Integration](https://arxiv.org/abs/2506.23644)
*Junze Hu,Xiangyu Jin,Yizhe Zeng,Yuling Liu,Yunpeng Li,Dan Du,Kaiyu Xie,Hongsong Zhu*

Main category: cs.SE

TL;DR: QLPro combines LLMs with static analysis, outperforming CodeQL in vulnerability detection and discovering new 0-days in open-source Java code.


<details>
  <summary>Details</summary>
Motivation: Existing static analysis tools like CodeQL miss a significant number of vulnerabilities in open-source projects.

Method: QLPro systematically integrates Large Language Models (LLMs) with static analysis for broader vulnerability detection. The framework is evaluated using JavaTest, a dataset with 10 open-source projects and 62 known vulnerabilities.

Result: QLPro detected 41 vulnerabilities compared to CodeQL's 24, and found 6 previously unknown vulnerabilities (including 2 confirmed 0-days).

Conclusion: QLPro significantly improves vulnerability detection rates over current state-of-the-art tools and can uncover previously unknown security issues.

Abstract: We introduce QLPro, a vulnerability detection framework that systematically
integrates LLMs and static analysis tools to enable comprehensive vulnerability
detection across entire open-source projects.We constructed a new dataset,
JavaTest, comprising 10 open-source projects from GitHub with 62 confirmed
vulnerabilities. CodeQL, a state-of-the-art static analysis tool, detected only
24 of these vulnerabilities while QLPro detected 41. Furthermore, QLPro
discovered 6 previously unknown vulnerabilities, 2 of which have been confirmed
as 0-days.

</details>


### [17] [Towards a Science of Developer eXperience (DevX)](https://arxiv.org/abs/2506.23715)
*Benoit Combemale*

Main category: cs.SE

TL;DR: The paper argues for establishing Developer eXperience (DevX) as a dedicated research field in software engineering, contending that focusing on developers’ human experiences is vital for sustainable and productive software development. It highlights motivations, enabling factors, challenges, and calls for more research and human-centered practices.


<details>
  <summary>Details</summary>
Motivation: Software plays an increasingly crucial role in everyday life, making sustainable, effective, and inclusive development practices more important than ever. However, the personal and collaborative experiences of developers (Developer eXperience, or DevX) remain underexplored, despite significant advances in technical aspects of software engineering.

Method: The paper advocates for recognizing DevX as its own research field. It synthesizes existing efforts to measure and improve DevX, identifies rationales and enabling factors, explores interdisciplinary connections, and outlines unresolved scientific challenges. This is a conceptual, analytical, and agenda-setting paper rather than an empirical study.

Result: The authors identify key rationales for focusing on DevX, describe the interdisciplinary nature of the field, and highlight scientific challenges. They call on the research community to address these challenges and adopt more human-centered approaches in software engineering.

Conclusion: DevX should be formally recognized as a distinct research field because it critically affects development effectiveness and productivity. Focusing on DevX can drive more sustainable, inclusive, and human-centered software engineering practices.

Abstract: As software continues to permeate nearly every facet of modern life, the
complexity and ubiquity of digital services underscore the need for
sustainable, effective, and inclusive software development practices. Although
software engineering has made significant progress in technical challenges
since its inception, the human experience of those involved in software
creation, broadly defined as developers, remains underexplored. This column
advocates for the formal recognition of Developer eXperience (DevX) as a
distinct research field. We argue that DevX profoundly influences critical
development activities and overall productivity, especially as development
becomes increasingly collaborative and diverse in terms of application domains.
Building on existing efforts to measure and enhance DevX, we identify key
rationales, scientific enablers, and interdisciplinary intersections that
support this emerging discipline. We also outline the core scientific
challenges ahead, aiming to call for actions from the research community and to
promote more human-centered approaches to software engineering.

</details>


### [18] [A Survey of LLM-based Automated Program Repair: Taxonomies, Design Paradigms, and Applications](https://arxiv.org/abs/2506.23749)
*Boyang Yang,Zijian Cai,Fengling Liu,Bach Le,Lingming Zhang,Tegawendé F. Bissyandé,Yang Liu,Haoye Tian*

Main category: cs.SE

TL;DR: The paper categorizes 63 recent LLM-based program repair systems into four paradigms, analyzing their trade-offs and challenges. It highlights the importance of context augmentation and suggests future research directions for more reliable, efficient, and cost-effective LLM-powered repair solutions.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) are being increasingly used for automated program repair (APR), but there is a need to categorize and understand recent advancements and persisting challenges in this field.

Method: The authors surveyed and categorized 63 LLM-based APR systems published between January 2022 and June 2025 into four paradigms. They analyzed the strengths, limitations, and trade-offs of each approach, focusing on the impact of retrieval- or analysis-augmented contexts.

Result: They identified four main paradigms: fine-tuning, prompting, procedural pipelines, and agentic frameworks, each with distinct benefits and drawbacks. Augmented contexts enhance the performance of any paradigm. Key challenges remain, such as ensuring semantic correctness, addressing large-scale defects, and minimizing costs.

Conclusion: The taxonomy clarifies important trade-offs in LLM-based APR. The paper highlights persistent challenges and suggests directions for future research, including human feedback integration, smarter retrieval, code analysis, and cost-effective strategies.

Abstract: Large language models (LLMs) are reshaping automated program repair (APR). We
categorize the recent 63 LLM-based APR systems published from January 2022 to
June 2025 into four paradigms, and show how retrieval- or analysis-augmented
contexts strengthen any of them. This taxonomy clarifies key trade-offs:
fine-tuning delivers strong task alignment at high training cost; prompting
enables rapid deployment but is limited by prompt design and context windows;
procedural pipelines offer reproducible control with moderate overhead; agentic
frameworks tackle multi-hunk or cross-file bugs at the price of increased
latency and complexity. Persistent challenges include verifying semantic
correctness beyond test suites, repairing repository-scale defects, and
lowering the costs of LLMs. We outline research directions that combine
lightweight human feedback, repository-aware retrieval, code analysis, and
cost-aware planning to advance reliable and efficient LLM-based APR.

</details>


### [19] [Software Engineering for Large Language Models: Research Status, Challenges and the Road Ahead](https://arxiv.org/abs/2506.23762)
*Hongzhou Rao,Yanjie Zhao,Xinyi Hou,Shenao Wang,Haoyu Wang*

Main category: cs.SE

TL;DR: This paper systematically reviews the challenges in large language model (LLM) development using software engineering concepts, mapping out the key issues and research directions across the lifecycle from requirements to maintenance.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) are rapidly advancing, but the complexity of their development lifecycle presents significant challenges that have not been systematically studied from a software engineering (SE) perspective. There is a lack of comprehensive frameworks addressing these issues across all stages of LLM development.

Method: The authors conduct a systematic analysis of the LLM development lifecycle, breaking it down into six phases: requirements engineering, dataset construction, model development and enhancement, testing and evaluation, deployment and operations, and maintenance and evolution. For each phase, they identify current research statuses, key challenges, and propose potential research directions.

Result: The paper presents a structured overview of LLM development from the SE perspective, highlighting the main challenges in each phase and suggesting potential research opportunities to address them. This approach provides a valuable framework for understanding and improving LLM development processes.

Conclusion: By applying an SE lens to the LLM lifecycle, this research fills an existing gap, providing systematic insights and directions that could facilitate more robust and successful large language model development in the future.

Abstract: The rapid advancement of large language models (LLMs) has redefined
artificial intelligence (AI), pushing the boundaries of AI research and
enabling unbounded possibilities for both academia and the industry. However,
LLM development faces increasingly complex challenges throughout its lifecycle,
yet no existing research systematically explores these challenges and solutions
from the perspective of software engineering (SE) approaches. To fill the gap,
we systematically analyze research status throughout the LLM development
lifecycle, divided into six phases: requirements engineering, dataset
construction, model development and enhancement, testing and evaluation,
deployment and operations, and maintenance and evolution. We then conclude by
identifying the key challenges for each phase and presenting potential research
directions to address these challenges. In general, we provide valuable
insights from an SE perspective to facilitate future advances in LLM
development.

</details>


### [20] [Requirements for Active Assistance of Natural Questions in Software Architecture](https://arxiv.org/abs/2506.23898)
*Diogo Lemos,Ademar Aguiar,Neil B. Harrison*

Main category: cs.SE

TL;DR: Mismanagement of natural questions during software architecture design can lead to knowledge loss and inefficiencies. This paper proposes a lifecycle and environment supported by AI and knowledge tools, validated by expert feedback, to better manage these questions and enhance collaboration and knowledge preservation.


<details>
  <summary>Details</summary>
Motivation: Natural questions are vital to the architectural design process, influencing key decisions and preserving knowledge. However, these questions are often mismanaged or ignored, which can result in architectural drift, inefficiency, and knowledge loss. The motivation is to improve the handling of these questions to enhance architecture design and knowledge retention.

Method: The authors conducted a literature review, a requirements workshop, and three design iterations to propose a lifecycle for natural questions. They elicited both functional and non-functional requirements for an assisted environment. Finally, they validated the proposed requirements and features through a survey with experts.

Result: A lifecycle model for natural questions was proposed. Essential requirements for an assisted environment were elicited, focusing on adaptability, seamless integration of knowledge management, and AI techniques. Survey results from experts validated and analyzed these requirements and proposed features, showing improved collaboration, decision-making, and knowledge preservation.

Conclusion: Addressing and managing natural questions in architectural workflows is necessary to avoid knowledge loss and poor decision-making. The proposed environment and lifecycle model, validated by expert surveys, offer a viable way to enhance architectural knowledge, collaboration, and system understanding beyond current conventional methods.

Abstract: Natural questions are crucial to shaping key architectural decisions and
preserving architectural knowledge. They arise organically during the
architectural design process, often resulting from the existing architectural
experience of the designer and the distinctive characteristics of the system
being designed. However, natural questions are often mismanaged or ignored,
which can lead to architectural drift, knowledge loss, inefficient resource
use, or poor understandability of the system's architecture. We aim to better
understand the lifecycle of natural questions, its key requirements, challenges
and difficulties, and then to envision an assisted environment to properly
support it. The environment should be adaptable and responsive to real-world
constraints and uncertainties by seamlessly integrating knowledge management
tools and artificial intelligence techniques into software development
workflows. Based on existing literature, a requirements workshop, and three
design iterations, we proposed a lifecycle for natural questions and elicited
essential functional and non-functional requirements for such an environment.
At last, the results of a survey conducted with experts helped to analyze and
validate the elicited requirements and proposed features for the environment to
enhance collaboration, decision-making, and the preservation of architectural
knowledge more effectively than conventional methods.

</details>


### [21] [Green Metrics Tool: Measuring for fun and profit](https://arxiv.org/abs/2506.23967)
*Geerd-Dietger Hoffmann,Verena Majuntke*

Main category: cs.SE

TL;DR: The paper introduces GMT, a tool for accurately measuring and optimizing software's environmental impact, enabling informed efforts to reduce resource consumption and carbon emissions.


<details>
  <summary>Details</summary>
Motivation: As software usage and computational demands increase, so does their environmental impact, especially in terms of resource consumption and carbon emissions. There's a growing need to accurately measure and optimize software's environmental footprint.

Method: The authors introduce the Green Metrics Tool (GMT), a framework that uses a containerized, controlled, and reproducible life cycle-based method to measure software resource consumption during key phases. The tool also offers features such as visualization, comparability, and rule-/LLM-based optimizations.

Result: GMT provides accurate measurement of resource use, aids in fact-based decision making, and offers optimization recommendations to minimize the software's environmental impact.

Conclusion: The GMT framework enables developers and researchers to better assess, compare, and optimize the environmental performance of software, supporting efforts to reduce carbon emissions and resource consumption.

Abstract: The environmental impact of software is gaining increasing attention as the
demand for computational resources continues to rise. In order to optimize
software resource consumption and reduce carbon emissions, measuring and
evaluating software is a first essential step. In this paper we discuss what
metrics are important for fact base decision making. We introduce the Green
Metrics Tool (GMT), a novel framework for accurately measuring the resource
consumption of software. The tool provides a containerized, controlled, and
reproducible life cycle-based approach, assessing the resource use of software
during key phases. Finally, we discuss GMT features like visualization,
comparability and rule- and LLM-based optimisations highlighting its potential
to guide developers and researchers in reducing the environmental impact of
their software.

</details>


### [22] [STCLocker: Deadlock Avoidance Testing for Autonomous Driving Systems](https://arxiv.org/abs/2506.23995)
*Mingfei Cheng,Renzhi Wang,Xiaofei Xie,Yuan Zhou,Lei Ma*

Main category: cs.SE

TL;DR: STCLocker is a new testing method that actively creates and detects deadlock scenarios among multiple autonomous vehicles, outperforming existing methods in identifying coordination failures.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicles are increasingly operating in multi-AV environments where cooperation is critical. Deadlocks, where AVs enter a circular wait state and cannot proceed, represent a significant coordination failure that is insufficiently explored in current testing procedures focused on single-AV performance.

Method: The authors propose STCLocker, a novel testing technique specifically designed for deadlock avoidance testing in multi-AV settings. It includes three components: Deadlock Oracle for detecting deadlocks, Conflict Feedback for guiding AVs towards competitive interactions, and Conflict-aware Scenario Generation for creating scenarios that stimulate spatial and temporal conflicts among AVs.

Result: STCLocker was evaluated on two ADSs—Roach and OpenCDA—and was found to generate more deadlock scenarios than the strongest existing alternative technique.

Conclusion: STCLocker is a highly effective tool for uncovering and testing deadlock-prone situations in multi-AV autonomous driving systems, outperforming current baselines in generating challenging deadlock scenarios.

Abstract: Autonomous Driving System (ADS) testing is essential to ensure the safety and
reliability of autonomous vehicles (AVs) before deployment. However, existing
techniques primarily focus on evaluating ADS functionalities in single-AV
settings. As ADSs are increasingly deployed in multi-AV traffic, it becomes
crucial to assess their cooperative performance, particularly regarding
deadlocks, a fundamental coordination failure in which multiple AVs enter a
circular waiting state indefinitely, resulting in motion planning failures.
Despite its importance, the cooperative capability of ADSs to prevent deadlocks
remains insufficiently underexplored. To address this gap, we propose the first
dedicated Spatio-Temporal Conflict-Guided Deadlock Avoidance Testing technique,
STCLocker, for generating DeadLock Scenarios (DLSs), where a group of AVs
controlled by the ADS under test are in a circular wait state. STCLocker
consists of three key components: Deadlock Oracle, Conflict Feedback, and
Conflict-aware Scenario Generation. Deadlock Oracle provides a reliable
black-box mechanism for detecting deadlock cycles among multiple AVs within a
given scenario. Conflict Feedback and Conflict-aware Scenario Generation
collaborate to actively guide AVs into simultaneous competition over spatial
conflict resources (i.e., shared passing regions) and temporal competitive
behaviors (i.e., reaching the conflict region at the same time), thereby
increasing the effectiveness of generating conflict-prone deadlocks. We
evaluate STCLocker on two types of ADSs: Roach, an end-to-end ADS, and OpenCDA,
a module-based ADS supporting cooperative communication. Experimental results
show that, on average, STCLocker generates more DLS than the best-performing
baseline.

</details>


### [23] [Bug Fixing with Broader Context: Enhancing LLM-Based Program Repair via Layered Knowledge Injection](https://arxiv.org/abs/2506.24015)
*Ramtin Ehsani,Esteban Parra,Sonia Haiduc,Preetha Chatterjee*

Main category: cs.SE

TL;DR: Providing LLMs with incrementally more powerful context—starting from local bug data and extending to repository and project knowledge—substantially increases automated program repair success rates, though some complex bugs still resist resolution. The method suggests future APR systems should be adaptive and interactive.


<details>
  <summary>Details</summary>
Motivation: Many software bugs remain unresolved by automated program repair (APR) using LLMs, even when provided with local bug-related context. In practice, developers use broader project and repository-level knowledge to fix bugs. The paper investigates how systematically extracting and injecting such hierarchical knowledge into LLMs can enhance APR effectiveness.

Method: The authors propose a layered knowledge injection framework for LLM-based program repair. The framework incrementally provides LLMs with (1) bug-specific knowledge, (2) repository-level context (e.g., dependencies, related files, commit history), and (3) project-level knowledge (documentation, previously fixed bugs). They evaluate this approach on 314 real bugs from BugsInPy using Llama 3.3 and GPT-4o-mini, measuring fix rates across six different bug types.

Result: The layered knowledge injection framework boosts the program repair fix rate to 79% (250 out of 314) with Llama 3.3, which is a 23% improvement over prior work. Most bug types benefit from repository-level context, though only some see further enhancements with project-level knowledge. However, complex and structurally isolated bugs (e.g., Program Anomaly, GUI bugs) are still challenging to fix, even with all available knowledge.

Conclusion: Injecting structured, multi-layered contextual knowledge (bug, repository, project) significantly improves automated program repair with LLMs, but certain complex bug types remain unresolved. The results highlight the value of adaptive and interactive repair systems tailored to the context needs of different bug categories.

Abstract: Prompting LLMs with bug-related context (e.g., error messages, stack traces)
improves automated program repair, but many bugs still remain unresolved. In
real-world projects, developers often rely on broader repository and
project-level context beyond the local code to resolve such bugs. In this
paper, we investigate how automatically extracting and providing such knowledge
can improve LLM-based program repair. We propose a layered knowledge injection
framework that incrementally augments LLMs with structured context. It starts
with the Bug Knowledge Layer, which includes information such as the buggy
function and failing tests; expands to the Repository Knowledge Layer, which
adds structural dependencies, related files, and commit history; and finally
injects the Project Knowledge Layer, which incorporates relevant details from
documentation and previously fixed bugs. We evaluate this framework on a
dataset of 314 bugs from BugsInPy using two LLMs (Llama 3.3 and GPT-4o-mini),
and analyze fix rates across six bug types. By progressively injecting
knowledge across layers, our approach achieves a fix rate of 79% (250/314)
using Llama 3.3, a significant improvement of 23% over previous work. All bug
types show improvement with the addition of repository-level context, while
only a subset benefit further from project-level knowledge, highlighting that
different bug types require different levels of contextual information for
effective repair. We also analyze the remaining unresolved bugs and find that
more complex and structurally isolated bugs, such as Program Anomaly and GUI
bugs, remain difficult even after injecting all available information. Our
results show that layered context injection improves program repair and suggest
the need for interactive and adaptive APR systems.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [24] [Verifying Properties of Index Arrays in a Purely-Functional Data-Parallel Language](https://arxiv.org/abs/2506.23058)
*Nikolaj Hey Hinnerskov,Robert Schenck,Cosmin E. Oancea*

Main category: cs.PL

TL;DR: A novel verification method for data-parallel programs with non-linear indexing using index functions and algebraic reasoning, implemented in Futhark. Delivers fast and practical verification, improving both correctness and performance (notably for GPU programs) without restricting to decidable logics.


<details>
  <summary>Details</summary>
Motivation: Automatically verifying properties of data-parallel programs, especially those with non-linear indexing and higher-order array constructs, is challenging yet highly desirable for correctness and optimization.

Method: The authors represent arrays as index functions and express program transformations as manipulations over these index functions. Properties are propagated and inferred by transforming them into algebraic inequalities (mainly (in)equalities) which are then checked using a Fourier-Motzkin-based solver. The framework is implemented in the Futhark programming language.

Result: The system proved practical and effective, with an average verification time of 1 second across seven applications. In two case studies, eliminating the need for dynamic verification in GPU programs led to significant speedups.

Conclusion: The framework enables automatic property verification for pure data-parallel programs with non-linear indexing, supports practically useful properties, and can be exploited beyond correctness (e.g., for compiler optimization). Its implementation in Futhark and experimental results demonstrate practicality and performance gains.

Abstract: This paper presents a novel approach to automatically verify properties of
pure data-parallel programs with non-linear indexing -- expressed as pre- and
post-conditions on functions. Programs consist of nests of second-order array
combinators (e.g., map, scan, and scatter) and loops. The key idea is to
represent arrays as index functions: programs are index function
transformations over which properties are propagated and inferred. Our
framework proves properties on index functions by distilling them into
algebraic (in)equalities and discharging them to a Fourier-Motzkin-based
solver. The framework is practical and accessible: properties are not
restricted to a decidable logic, but instead are carefully selected to express
practically useful guarantees that can be automatically reasoned about and
inferred. These guarantees extend beyond program correctness and can be
exploited by the entire compiler pipeline for optimization. We implement our
system in the pure data-parallel language Futhark and demonstrate its
practicality on seven applications, reporting an average verification time of 1
second. Two case studies show how eliminating dynamic verification in GPU
programs results in significant speedups.

</details>


### [25] [A Denotational Semantics for Quantum Loops](https://arxiv.org/abs/2506.23320)
*Nicola Assolini,Alessandra Di Pierro*

Main category: cs.PL

TL;DR: The paper develops a mathematical framework to define and analyze high-level quantum programming constructs, especially quantum-controlled branching and loops, advancing the foundations for quantum programming languages.


<details>
  <summary>Details</summary>
Motivation: Programming quantum computers involves various abstraction levels, similar to classical computers. High-level quantum programming constructs do not yet have a well-established mathematical semantics, especially concerning quantum-controlled branching and iteration.

Method: The authors propose a denotational semantics for high-level quantum programming constructs. They introduce a specific denotational domain capable of capturing the mathematical meaning of quantum control flow, including loops, reflecting the coherent (unitary) evolution of quantum systems.

Result: A framework is developed that provides a mathematical (denotational) semantics for quantum control flow with loops, supporting the conceptual understanding and development of quantum programming languages.

Conclusion: The paper clarifies the meaning of quantum-controlled branching and looping in high-level quantum programming, offering a semantic domain suitable for expressing and analyzing such constructs.

Abstract: Programming a quantum computer, i.e., implementing quantum algorithms on a
quantum processor-based copmputer architecture, is a task that can be addressed
(just as for classical computers) at different levels of abstraction. This
paper proposes a denotational semantics for high-level quantum programming
constructs, focusing on the conceptual meaning of quantum-controlled branching
and iteration. We introduce a denotational domain where a mathematical meaning
of a quantum control flow with loops can be defined, which reflects the
coherent evolution of the quantum system implementing the program.

</details>


### [26] [Compiling a Q# Subset to QASM 3.0 in TypeScript via a JSON Based IR](https://arxiv.org/abs/2506.23407)
*Marcus Edwards*

Main category: cs.PL

TL;DR: The paper presents a TypeScript-based Q# to QASM 3.0 compiler toolchain, demonstrating its functionality on multiple Q# programs and compatibility with web environments, thus expanding application scenarios beyond the official Microsoft tools.


<details>
  <summary>Details</summary>
Motivation: Existing Q# compile toolchains, such as Microsoft's official implementation, are not suitable for web environments and are not written in TypeScript. There is a need for a toolchain that can be used more flexibly, such as in the browser.

Method: The authors implemented a compile toolchain from Q# to QASM 3.0. This included building a full-featured lexer, parser, and a compiler that supports a subset of Q# features. The solution was implemented in TypeScript, and its functionality was tested with various Q# programs and compared to existing Q# toolchains.

Result: The implementation works with various input Q# programs and offers a functional alternative to the Microsoft Q# compile toolchain. It is fully written in TypeScript, making it suitable for web and browser-based environments.

Conclusion: Porting the Q# compile toolchain to TypeScript enables use cases in the web environment, providing a functional compiler from Q# to QASM 3.0 and broadening accessibility beyond the Microsoft toolchain.

Abstract: We implement a compile toolchain from Q# to QASM 3.0 including a
full-featured lexer and parser implementation, as well as a compiler that
supports a subset of Q# features. The lexer, parser and compiler are shown to
work with various input Q# programs and the implementation is compared against
existing Q# compile tools. Unlike the Microsoft implementation of the official
Q# compile toolchain, our implementation is written in TypeScript in order to
port functionality to web environments.

</details>
