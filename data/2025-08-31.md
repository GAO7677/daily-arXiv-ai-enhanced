<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Evaluating LLMs on microservice-based applications: how complex is your specification?](https://arxiv.org/abs/2508.20119)
*Daniel M. Yellin*

Main category: cs.SE

TL;DR: LLMs can generate code for basic and moderately complex microservices, but fail with more difficult, real-world software requirements, highlighting the need for further research and improvements.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are increasingly used for code generation, but their effectiveness in synthesizing code for real-world, complex software like microservice-based applications needs thorough evaluation.

Method: The authors define a standard template for microservice application specifications, introduce a metric for measuring the difficulty of these specifications, and build a framework for automatically testing the generated code via unit tests. They then evaluate how well LLMs synthesize code of varying difficulty levels, particularly analyzing errors and challenging cases.

Result: LLMs such as GPT-3o-mini perform decently on medium difficulty specifications, but their performance drops significantly on higher difficulty tasks, especially those involving complex business logic, use of external services, database integration, and non-functional requirements like authentication.

Conclusion: Current LLMs, while capable for moderately complex coding tasks, struggle significantly with more intricate, real-world software requirements. The authors highlight key challenges and propose areas for future research to improve LLM code synthesis capability.

Abstract: In this paper we evaluate how far LLMs have advanced in generating code for
real-world problems. Specifically, we explore code synthesis for
microservice-based applications, a widely used architecture pattern. We define
a standard template for specifying these applications, and we propose a metric
for judging the difficulty level of a specification. The higher the score, the
more difficult it is to generate code for the specification. We develop a
framework to automate the process of testing LLM-synthesized code for a
microservice using unit tests. Our experimental results show that strong LLMs
(like GPT-3o-mini) do fairly well on medium difficulty specifications but do
very poorly on those of higher difficulty levels. This is due to more intricate
business logic, a greater use of external services, database integration and
inclusion of non-functional capabilities such as authentication. We analyzed
the errors in LLM-synthesized code and report on the key challenges LLMs face
in generating code for these specifications thereby suggesting future research
directions to improve code synthesis for real-world problems.

</details>


### [2] [Towards Better Correctness and Efficiency in Code Generation](https://arxiv.org/abs/2508.20124)
*Yunlong Feng,Yang Xu,Xiao Xu,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: The paper proposes a new reinforcement learning framework to make code generation models both more correct and efficient, showing strong improvements over previous methods.


<details>
  <summary>Details</summary>
Motivation: Current code large language models generate code with poor runtime efficiency, which restricts their use in performance-critical scenarios. The motivation is to improve both efficiency and correctness of code generation.

Method: An efficiency-oriented reinforcement learning framework guided by a novel performance reward, including dynamic exploration, error-insensitive methods, and high-contrast efficiency signals, was used, culminating in a two-stage tuning process.

Result: The experimental results show improvements of 10.18% in code correctness and 7.75% in runtime efficiency for a 7B parameter model, making its performance comparable to much larger models.

Conclusion: The proposed framework and two-stage tuning method significantly improve both code correctness and runtime efficiency in code large language models, making them perform comparably to much larger models but with better balanced performance.

Abstract: While code large language models have demonstrated remarkable progress in
code generation, the generated code often exhibits poor runtime efficiency,
limiting its practical application in performance-sensitive scenarios. To
address this limitation, we propose an efficiency-oriented reinforcement
learning framework guided by a novel performance reward. Based on this
framework, we take a deeper dive into the code efficiency problem, identifying
then proposing methods to overcome key bottlenecks: (1) Dynamic exploration
overcomes the static data constraints of offline fine-tuning, enabling the
discovery of more efficient code implementations. (2) The error-insensitive
reinforcement learning method and high-contrast efficiency signals are crucial
for mitigating systematic errors and achieving effective optimization. (3)
Online exploration is most effective when starting from a high-correctness
baseline, as this allows for efficiency improvements without sacrificing
accuracy. With these discoveries, we finally propose a two-stage tuning method,
which achieves high and balanced performance across correctness and efficiency.
The results of experiments show the effectiveness of the method, which improves
code correctness by 10.18\% and runtime efficiency by 7.75\% on a 7B model,
achieving performance comparable to much larger model.

</details>


### [3] [Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce Formula Generators](https://arxiv.org/abs/2508.20340)
*Maolin Sun,Yibiao Yang,Yuming Zhou*

Main category: cs.SE

TL;DR: Chimera is an efficient LLM-assisted fuzzing tool for SMT solvers that creates syntactically correct, diverse test inputs, finds more bugs, and uses less computational resources than prior LLM-based methods.


<details>
  <summary>Details</summary>
Motivation: SMT solvers are vital for modern system reliability, but their rapid evolution introduces challenges for testing and bug detection. Existing LLM-based testing approaches face problems with syntactic invalidity and high computational cost.

Method: The authors propose Chimera, an LLM-assisted fuzzing framework. Chimera leverages LLMs to extract context-free grammars from documentation and synthesize reusable Boolean term generators. Instead of generating test formulas directly, Chimera produces generators that ensure syntactic correctness and semantic diversity, and only needs a single LLM invocation, improving efficiency.

Result: Chimera was evaluated on Z3 and cvc5 SMT solvers, uncovering 43 confirmed bugs, with 40 already addressed and fixed by developers.

Conclusion: Chimera demonstrates a more effective, efficient approach to fuzzing SMT solvers, improving both bug-finding capability and resource efficiency by generating syntactically valid and diverse test cases.

Abstract: Satisfiability Modulo Theory (SMT) solvers are foundational to modern systems
and programming languages research, providing the foundation for tasks like
symbolic execution and automated verification. Because these solvers sit on the
critical path, their correctness is essential, and high-quality test formulas
are key to uncovering bugs. However, while prior testing techniques performed
well on earlier solver versions, they struggle to keep pace with rapidly
evolving features. Recent approaches based on Large Language Models (LLMs) show
promise in exploring advanced solver capabilities, but two obstacles remain:
nearly half of the generated formulas are syntactically invalid, and iterative
interactions with the LLMs introduce substantial computational overhead. In
this study, we present Chimera, a novel LLM-assisted fuzzing framework that
addresses both issues by shifting from direct formula generation to the
synthesis of reusable term (i.e., logical expression) generators. Particularly,
Chimera uses LLMs to (1) automatically extract context-free grammars (CFGs) for
SMT theories, including solver-specific extensions, from documentation, and (2)
synthesize composable Boolean term generators that adhere to these grammars.
During fuzzing, Chimera populates structural skeletons derived from existing
formulas with the terms iteratively produced by the LLM-synthesized generators.
This design ensures syntactic validity while promoting semantic diversity.
Notably, Chimera requires only one-time LLM interaction investment,
dramatically reducing runtime cost. We evaluated Chimera on two leading SMT
solvers: Z3 and cvc5. Our experiments show that Chimera has identified 43
confirmed bugs, 40 of which have already been fixed by developers.

</details>


### [4] [Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought](https://arxiv.org/abs/2508.20370)
*Lingzhe Zhang,Tong Jia,Kangjin Wang,Weijie Hong,Chiming Duan,Minghua He,Ying Li*

Main category: cs.SE

TL;DR: Microservice systems are vulnerable to frequent failures, making root cause localization crucial. This paper presents RCLAgent, a new adaptive method inspired by SRE practices, which uses a recursion-of-thought approach with multiple agents and LLMs. RCLAgent delivers superior performance with just one request and advances reliability and accuracy compared to current state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Contemporary microservice systems are highly complex and prone to frequent failures due to their multiple fine-grained, interdependent subsystems. Existing root cause localization methods have limitations such as reliance on rigid schemas or lack of interpretability, leaving Site Reliability Engineers (SREs) challenged in accurately identifying the root causes.

Method: The paper introduces RCLAgent, an adaptive root cause localization method specifically designed for microservice systems. RCLAgent uses a novel multi-agent recursion-of-thought framework, leveraging the reasoning capabilities of large language models (LLMs) and integrating multi-source data and tool-assisted analysis to guide the localization process.

Result: Experimental evaluations using various public datasets show that RCLAgent can accurately localize the root cause of failures using only a single system request. RCLAgent outperforms current state-of-the-art methods that require aggregation of multiple requests for effective localization.

Conclusion: RCLAgent significantly enhances both efficiency and precision in root cause localization within complex microservice systems. Its adaptive, interpretable approach based on lessons from real SRE practices establishes a new benchmark for this task.

Abstract: As contemporary microservice systems become increasingly popular and
complex-often comprising hundreds or even thousands of fine-grained,
interdependent subsystems-they are facing more frequent failures. Ensuring
system reliability thus demands accurate root cause localization. While traces
and metrics have proven to be effective data sources for this task, existing
methods either heavily rely on pre-defined schemas, which struggle to adapt to
evolving operational contexts, or lack interpretability in their reasoning
process, thereby leaving Site Reliability Engineers (SREs) confused. In this
paper, we conduct a comprehensive study on how SREs localize the root cause of
failures, drawing insights from multiple professional SREs across different
organizations. Our investigation reveals that human root cause analysis
exhibits three key characteristics: recursiveness, multi-dimensional expansion,
and cross-modal reasoning. Motivated by these findings, we introduce RCLAgent,
an adaptive root cause localization method for microservice systems that
leverages a multi-agent recursion-of-thought framework. RCLAgent employs a
novel recursion-of-thought strategy to guide the LLM's reasoning process,
effectively integrating data from multiple agents and tool-assisted analysis to
accurately pinpoint the root cause. Experimental evaluations on various public
datasets demonstrate that RCLAgent achieves superior performance by localizing
the root cause using only a single request-outperforming state-of-the-art
methods that depend on aggregating multiple requests. These results underscore
the effectiveness of RCLAgent in enhancing the efficiency and precision of root
cause localization in complex microservice environments.

</details>


### [5] [AI and Agile Software Development: A Research Roadmap from the XP2025 Workshop](https://arxiv.org/abs/2508.20563)
*Zheying Zhang,Tomas Herda,Victoria Pichler,Pekka Abrahamsson,Geir K. Hanssen,Joshua Kerievsky,Alex Polyakov,Mohit Chandna,Marius Irgens,Kai-Kristian Kemell,Ayman Asad Khan,Crystal Kwok,Evan Leybourn,Munish Malik,Dorota Mleczko,Morteza Moalagh,Christopher Morales,Yuliia Pieskova,Daniel Planötscher,Mika Saari,Anastasiia Tkalich,Karl Josef Gstettner,Xiaofeng Wang*

Main category: cs.SE

TL;DR: This paper summarizes insights from an interdisciplinary workshop on integrating Generative AI into agile software development, highlighting major challenges, root causes, and a collaboratively built research roadmap to guide future efforts.


<details>
  <summary>Details</summary>
Motivation: There is a rapidly growing intersection between Generative AI and agile software development, presenting both challenges (such as tool fragmentation, governance, data quality, and skills gaps) and new opportunities. The motivation is to better understand and address these challenges to enable successful integration of AI into agile practices.

Method: The authors conducted a full-day workshop with over 30 interdisciplinary participants, using structured and interactive breakout sessions to collectively identify issues, analyze their underlying causes, and collaboratively develop a research roadmap for the field.

Result: Participants identified key pain points (tool fragmentation, governance, data quality, AI literacy, and prompt engineering), analyzed root causes, and jointly produced a multi-thematic research roadmap. This roadmap outlines actionable short-term steps as well as long-term research directions for the responsible, human-centered integration of GenAI in agile development.

Conclusion: A shared research agenda and concrete action items were established to guide future research and practice at the intersection of GenAI and agile software development, aiming to address current barriers and foster human-centered, responsible integration of AI.

Abstract: This paper synthesizes the key findings from a full-day XP2025 workshop on
"AI and Agile: From Frustration to Success", held in Brugg-Windisch,
Switzerland. The workshop brought together over 30 interdisciplinary academic
researchers and industry practitioners to tackle the concrete challenges and
emerging opportunities at the intersection of Generative Artificial
Intelligence (GenAI) and agile software development. Through structured,
interactive breakout sessions, participants identified shared pain points like
tool fragmentation, governance, data quality, and critical skills gaps in AI
literacy and prompt engineering. These issues were further analyzed, revealing
underlying causes and cross-cutting concerns. The workshop concluded by
collaboratively co-creating a multi-thematic research roadmap, articulating
both short-term, implementable actions and visionary, long-term research
directions. This cohesive agenda aims to guide future investigation and drive
the responsible, human-centered integration of GenAI into agile practices.

</details>


### [6] [Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol](https://arxiv.org/abs/2508.20737)
*Wei Ma,Yixiao Yang,Qiang Hu,Shi Ying,Zhi Jin,Bo Du,Zhenchang Xing,Tianlin Li,Junjie Shi,Yang Liu,Linxiao Jiang*

Main category: cs.SE

TL;DR: The paper introduces a three-layer model to understand and test Large Language Model applications, analyzes the limitations of existing software testing approaches, and proposes collaborative strategies and a specialized communication protocol (AICL) for robust quality assurance and standardization.


<details>
  <summary>Details</summary>
Motivation: Large Language Model (LLM) applications have evolved into complex systems, making quality assurance challenging due to their non-determinism, dynamism, and context dependence. Existing software testing approaches may not directly address these issues, prompting the need for new frameworks and protocols.

Method: The paper decomposes LLM applications into a three-layer architecture: System Shell Layer, Prompt Orchestration Layer, and LLM Inference Core. It assesses traditional software testing methods' applicability at each layer and performs a comparative analysis of testing approaches from both software engineering and AI safety perspectives. The study identifies key differences, proposes collaborative strategies for quality assurance, and presents a protocol (AICL) to standardize agent communication and facilitate testing.

Result: The authors identify four fundamental differences leading to six core challenges in LLM application testing. They develop four collaborative strategies (Retain, Translate, Integrate, Runtime) and propose a closed-loop quality assurance framework combining pre-deployment validation and runtime monitoring. They also introduce the Agent Interaction Communication Language (AICL) tailored for testability and integration in agent-based systems.

Conclusion: Traditional testing methods require adaptation or rethinking for LLM systems due to their unique characteristics. The proposed quality assurance framework and AICL facilitate more trustworthy and standardized testing of LLM applications, addressing structural disconnects in current methodologies.

Abstract: Applications of Large Language Models~(LLMs) have evolved from simple text
generators into complex software systems that integrate retrieval augmentation,
tool invocation, and multi-turn interactions. Their inherent non-determinism,
dynamism, and context dependence pose fundamental challenges for quality
assurance. This paper decomposes LLM applications into a three-layer
architecture: \textbf{\textit{System Shell Layer}}, \textbf{\textit{Prompt
Orchestration Layer}}, and \textbf{\textit{LLM Inference Core}}. We then assess
the applicability of traditional software testing methods in each layer:
directly applicable at the shell layer, requiring semantic reinterpretation at
the orchestration layer, and necessitating paradigm shifts at the inference
core. A comparative analysis of Testing AI methods from the software
engineering community and safety analysis techniques from the AI community
reveals structural disconnects in testing unit abstraction, evaluation metrics,
and lifecycle management. We identify four fundamental differences that
underlie 6 core challenges. To address these, we propose four types of
collaborative strategies (\emph{Retain}, \emph{Translate}, \emph{Integrate},
and \emph{Runtime}) and explore a closed-loop, trustworthy quality assurance
framework that combines pre-deployment validation with runtime monitoring.
Based on these strategies, we offer practical guidance and a protocol proposal
to support the standardization and tooling of LLM application testing. We
propose a protocol \textbf{\textit{Agent Interaction Communication Language}}
(AICL) that is used to communicate between AI agents. AICL has the
test-oriented features and is easily integrated in the current agent framework.

</details>


### [7] [From Law to Gherkin: A Human-Centred Quasi-Experiment on the Quality of LLM-Generated Behavioural Specifications from Food-Safety Regulations](https://arxiv.org/abs/2508.20744)
*Shabnam Hassani,Mehrdad Sabetzadeh,Daniel Amyot*

Main category: cs.SE

TL;DR: LLMs (Claude and Llama) automatically translated legal regulations into developer-ready Gherkin specifications, showing high quality across key criteria in a systematic evaluation. Model outputs reduced manual workload for compliance tasks and are promising for software engineering, though some errors like hallucinations and omissions remain.


<details>
  <summary>Details</summary>
Motivation: Increasing legal and regulatory requirements impact software design and quality assurance, but translating technology-neutral legal language into actionable development requirements is difficult, time-intensive, and demands expertise. Engineers face challenges creating compliance artifacts, driving interest in automation solutions.

Method: A systematic human-subject, quasi-experimental study was conducted. Ten participants reviewed behavioral specifications generated by two LLMs (Claude and Llama) from food-safety regulations, using Gherkin language. Sixty specifications were evaluated across five criteria: Relevance, Clarity, Completeness, Singularity, and Time Savings, leading to 120 assessments.

Result: High ratings were achieved for all criteria: Relevance (75% highest), Clarity (90%), Completeness (75%), Singularity (82%), and Time Savings (68%). Llama slightly outperformed Claude in Clarity, Completeness, and Time Savings, while Claude excelled in Singularity. Participant feedback identified some hallucinations and omissions in outputs, but overall utility was confirmed. No significant differences were detected between models or participants statistically.

Conclusion: LLMs can reliably generate high-quality, developer-friendly Gherkin specifications from legal texts. This automation significantly reduces manual effort in compliance artifact creation and supports implementation, assurance, and test generation.

Abstract: Context: Laws and regulations increasingly affect software design and quality
assurance, but legal texts are written in technology-neutral language. This
creates challenges for engineers who must develop compliance artifacts such as
requirements and acceptance criteria. Manual creation is labor-intensive,
error-prone, and requires domain expertise. Advances in Generative AI (GenAI),
especially Large Language Models (LLMs), offer a way to automate deriving such
artifacts.
  Objective: We present the first systematic human-subject study of LLMs'
ability to derive behavioral specifications from legal texts using a
quasi-experimental design. These specifications translate legal requirements
into a developer-friendly form.
  Methods: Ten participants evaluated specifications generated from food-safety
regulations by Claude and Llama. Using Gherkin, a structured BDD language, 60
specifications were produced. Each participant assessed 12 across five
criteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Each
specification was reviewed by two participants, yielding 120 assessments.
  Results: For Relevance, 75% of ratings were highest and 20% second-highest.
Clarity reached 90% highest. Completeness: 75% highest, 19% second.
Singularity: 82% highest, 12% second. Time Savings: 68% highest, 24% second. No
lowest ratings occurred. Mann-Whitney U tests showed no significant differences
across participants or models. Llama slightly outperformed Claude in Clarity,
Completeness, and Time Savings, while Claude was stronger in Singularity.
Feedback noted hallucinations and omissions but confirmed the utility of the
specifications.
  Conclusion: LLMs can generate high-quality Gherkin specifications from legal
texts, reducing manual effort and providing structured artifacts useful for
implementation, assurance, and test generation.

</details>


### [8] [Towards an Architectural Perspective for Sustainability: Bundle the Needs from Industry](https://arxiv.org/abs/2508.20774)
*Markus Funke,Patricia Lago*

Main category: cs.SE

TL;DR: This paper introduces a structured sustainability perspective for software architecture based on literature review and expert input, confirming its practical relevance and outlining elements to guide sustainable design in industry.


<details>
  <summary>Details</summary>
Motivation: There is a growing need for structured guidance on addressing sustainability in software architecture, as it is becoming an important quality property for software-intensive systems.

Method: The authors used a combination of literature snowballing and expert focus group discussions to gather evidence and insights for formulating a sustainability architectural perspective.

Result: The findings validate the usefulness of different architectural perspective elements in addressing sustainability and identify practical implications for developing sustainability-focused architectural guidance that aligns with industry requirements.

Conclusion: A sustainability perspective for software architecture can be effectively shaped by leveraging structured architectural perspectives, informed by both literature and expert opinions.

Abstract: Sustainability is increasingly recognized as an emerging quality property in
software-intensive systems, yet architects lack structured guidance to address
it effectively throughout the software design phase. Architectural
perspectives-an architectural knowledge artifact composed of concerns,
activities, tactics, pitfalls, and checklists-offer a promising approach to
tackle such emerging quality properties across architectural views and are also
independent of architecture frameworks and industry contexts. In this paper, we
present a sustainability perspective vision, i.e., a revised notion of
architectural perspective meant to be filled with its own elements to target
sustainability concerns. We formulate our sustainability perspective vision
through evidence from applying snowballing to seminal literature and from
conducting a focus group with experts in the field. Our findings confirm the
relevance of the different perspective elements in practice and highlight
implications for shaping a sustainability perspective that meets industrial
needs.

</details>


### [9] [Automated Test Oracles for Flaky Cyber-Physical System Simulators: Approach and Evaluation](https://arxiv.org/abs/2508.20902)
*Baharin A. Jodat,Khouloud Gaaloul,Mehrdad Sabetzadeh,Shiva Nejati*

Main category: cs.SE

TL;DR: This paper presents assertion-based test oracles for cyber-physical systems that work without simulation. Using genetic programming, especially with the Ochiai formula, these oracles provide accurate and robust test verdicts, even in inconsistent CPS domains, significantly reducing testing costs and effort.


<details>
  <summary>Details</summary>
Motivation: Testing cyber-physical systems (CPS) is expensive and unreliable due to slow and flaky simulators. There is a need for test oracles that do not require system execution and are both interpretable and robust to simulator inconsistencies.

Method: The authors propose assertion-based test oracles that use sets of logical and arithmetic predicates over test inputs, thus avoiding simulator execution. They introduce two oracle generation methods: one with genetic programming (GP) using SBFL ranking formulas (Ochiai, Tarantula, Naish) as fitness functions, and another using decision trees (DT) and decision rules (DR). The approaches are evaluated in aerospace, networking, and autonomous driving case studies.

Result: The GP-based test oracle with Ochiai significantly outperforms alternatives (Tarantula, Naish, DT, DR), in both accuracy and robustness against flaky simulator behaviours, with only 4% average accuracy variation observed across diverse CPS domains.

Conclusion: Assertion-based test oracles, especially those generated with GP using Ochiai, offer an interpretable, robust, and accurate solution for CPS testing without the need for costly or unreliable simulation runs.

Abstract: Simulation-based testing of cyber-physical systems (CPS) is costly due to the
time-consuming execution of CPS simulators. In addition, CPS simulators may be
flaky, leading to inconsistent test outcomes and requiring repeated test
re-execution for reliable test verdicts. Automated test oracles that do not
require system execution are therefore crucial for reducing testing costs.
Ideally, such test oracles should be interpretable to facilitate human
understanding of test verdicts, and they must be robust against the potential
flakiness of CPS simulators. In this article, we propose assertion-based test
oracles for CPS as sets of logical and arithmetic predicates defined over the
inputs of the system under test. Given a test input, our assertion-based test
oracle determines, without requiring test execution, whether the test passes,
fails, or if the oracle is inconclusive in predicting a verdict. We describe
two methods for generating assertion-based test oracles: one using genetic
programming~(GP) that employs well-known spectrum-based fault localization
(SBFL) ranking formulas, namely Ochiai, Tarantula, and Naish, as fitness
functions; and the other using decision trees (DT) and decision rules (DR). We
evaluate our assertion-based test oracles through case studies in the domains
of aerospace, networking and autonomous driving. We show that test oracles
generated using GP with Ochiai are significantly more accurate than those
obtained using GP with Tarantula and Naish or using DT or DR. Moreover, this
accuracy advantage remains even when accounting for the flakiness of the system
under test. We further show that the assertion-based test oracles generated by
GP with Ochiai are robust against flakiness with only 4% average variation in
their accuracy results across four different network and autonomous driving
systems with flaky behaviours.

</details>


### [10] [Deep Learning Based Concurrency Bug Detection and Localization](https://arxiv.org/abs/2508.20911)
*Zuocheng Feng,Kaiwen Zhang,Miaomiao Wang,Yiming Cheng,Yuandao Cai,Xiaofeng Li,Guanjun Liu*

Main category: cs.SE

TL;DR: This paper proposes a new GNN-based approach, supported by a novel dataset and interpretability techniques, to better detect and localize concurrency bugs, outperforming current methods in accuracy, precision, and recall.


<details>
  <summary>Details</summary>
Motivation: Concurrency bugs are difficult to detect and harm software reliability and security. Deep learning approaches face challenges due to limited datasets, poor representation of concurrency semantics, and coarse detection granularity.

Method: The paper introduces a novel method combining a pre-trained model with a heterogeneous graph neural network (GNN) and a Concurrency-Aware Code Property Graph (CCPG) to represent concurrency semantics. SubgraphX, a GNN-based interpretability tool, is used for bug localization. A dedicated concurrency bug dataset is also constructed for model training and evaluation.

Result: The proposed approach achieves an average improvement of 10% in accuracy and precision, and 26% in recall over existing state-of-the-art methods across various settings.

Conclusion: The method effectively improves the detection and localization of concurrency bugs by addressing key shortcomings of previous approaches and offering more fine-grained debugging information.

Abstract: Concurrency bugs, caused by improper synchronization of shared resources in
multi-threaded or distributed systems, are notoriously hard to detect and thus
compromise software reliability and security. The existing deep learning
methods face three main limitations. First, there is an absence of large and
dedicated datasets of diverse concurrency bugs for them. Second, they lack
sufficient representation of concurrency semantics. Third, binary
classification results fail to provide finer-grained debug information such as
precise bug lines. To address these problems, we propose a novel method for
effective concurrency bug detection as well as localization. We construct a
dedicated concurrency bug dataset to facilitate model training and evaluation.
We then integrate a pre-trained model with a heterogeneous graph neural network
(GNN), by incorporating a new Concurrency-Aware Code Property Graph (CCPG) that
concisely and effectively characterizes concurrency semantics. To further
facilitate debugging, we employ SubgraphX, a GNN-based interpretability method,
which explores the graphs to precisely localize concurrency bugs, mapping them
to specific lines of source code. On average, our method demonstrates an
improvement of 10\% in accuracy and precision and 26\% in recall compared to
state-of-the-art methods across diverse evaluation settings.

</details>


### [11] [ConfLogger: Enhance Systems' Configuration Diagnosability through Configuration Logging](https://arxiv.org/abs/2508.20977)
*Shiwen Shan,Yintong Huo,Yuxin Su,Zhining Wang,Dan Li,Zibin Zheng*

Main category: cs.SE

TL;DR: ConfLogger, a tool that integrates static taint analysis with LLM-generated logs, greatly improves the diagnosis of configuration errors by enhancing log coverage and utility, leading to faster and more accurate troubleshooting in real software systems.


<details>
  <summary>Details</summary>
Motivation: Configuration flexibility in modern systems leads to misconfigurations and latent software bugs. Existing diagnostic methods focus on analyzing software post-failure but do not ensure that failure information is sufficient for effective diagnosis.

Method: The authors propose 'configuration logging' and introduce ConfLogger, a tool that combines configuration-aware static taint analysis with LLM-based log generation. This involves detecting configuration-sensitive code segments via data flow tracing and producing diagnostic logs based on the context.

Result: ConfLogger enhances diagnosability: in tests on eight popular systems, it enabled 100% accuracy for error localization in 30 silent misconfiguration cases, with 80% being directly resolvable. It covers 74% of existing logging points (12-30% more than LLM baselines), and surpasses state-of-the-art baselines in variable logging precision (+8.6%), recall (+79.3%), and F1 (+26.2%). A user study showed 1.25x faster diagnostics and 251.4% better troubleshooting accuracy.

Conclusion: ConfLogger significantly improves configuration diagnosability through increased log coverage and quality, outperforming existing tools both in automated metrics and user studies. Its approach provides explicit configuration information, streamlining misconfiguration diagnosis and resolution.

Abstract: Modern configurable systems offer customization via intricate configuration
spaces, yet such flexibility introduces pervasive configuration-related issues
such as misconfigurations and latent softwarebugs. Existing diagnosability
supports focus on post-failure analysis of software behavior to identify
configuration issues, but none of these approaches look into whether the
software clue sufficient failure information for diagnosis. To fill in the
blank, we propose the idea of configuration logging to enhance existing logging
practices at the source code level. We develop ConfLogger, the first tool that
unifies configuration-aware static taint analysis with LLM-based log generation
to enhance software configuration diagnosability. Specifically, our method 1)
identifies configuration-sensitive code segments by tracing
configuration-related data flow in the whole project, and 2) generates
diagnostic log statements by analyzing configuration code contexts. Evaluation
results on eight popular software systems demonstrate the effectiveness of
ConfLogger to enhance configuration diagnosability. Specifically,
ConfLogger-enhanced logs successfully aid a log-based misconfiguration
diagnosis tool to achieve 100% accuracy on error localization in 30 silent
misconfiguration scenarios, with 80% directly resolvable through explicit
configuration information exposed. In addition, ConfLogger achieves 74%
coverage of existing logging points, outperforming baseline LLM-based loggers
by 12% and 30%. It also gains 8.6% higher in precision, 79.3% higher in recall,
and 26.2% higher in F1 compared to the state-of-the-art baseline in terms of
variable logging while also augmenting diagnostic value. A controlled user
study on 22 cases further validated its utility, speeding up diagnostic time by
1.25x and improving troubleshooting accuracy by 251.4%.

</details>


### [12] [Dynamics of Gender Bias in Software Engineering](https://arxiv.org/abs/2508.21050)
*Thomas J. Misa*

Main category: cs.SE

TL;DR: This paper explores the history and gender dynamics of software engineering, quantitatively showing significant gender exclusion in authorship at a leading conference over several decades, and calls for policy actions to address persistent gender bias.


<details>
  <summary>Details</summary>
Motivation: Software engineering has roots in both engineering and computer science, fields known to exhibit gender biases. The paper aims to understand how these biases are reflected in software engineering, particularly regarding women's participation.

Method: The authors conduct a historical survey of the field, profile five key leaders, review recent literature about gender bias, and perform a quantitative analysis of women's representation among authors at the International Conference of Software Engineering (ICSE) from 1976 to 2010.

Result: The analysis found twelve years within the 34-year span when women's participation as research authors at ICSE was statistically significantly lower, indicating notable gender exclusion. The paper also provides suggestions regarding policy implications for reducing gender bias in computing.

Conclusion: Gender exclusion has been a recurring and measurable issue in software engineering research. Addressing this exclusion is important for making progress toward gender equity, and the field should consider policy changes to foster greater inclusivity.

Abstract: The field of software engineering is embedded in both engineering and
computer science, and may embody gender biases endemic to both. This paper
surveys software engineering's origins and its long-running attention to
engineering professionalism, profiling five leaders; it then examines the
field's recent attention to gender issues and gender bias. It next
quantitatively analyzes women's participation as research authors in the
field's leading International Conference of Software Engineering (1976-2010),
finding a dozen years with statistically significant gender exclusion. Policy
dimensions of research on gender bias in computing are suggested.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [13] [Solvable Tuple Patterns and Their Applications to Program Verification](https://arxiv.org/abs/2508.20365)
*Naoki Kobayashi,Ryosuke Sato,Ayumi Shinohara,Ryo Yoshinaka*

Main category: cs.PL

TL;DR: This paper introduces 'solvable tuple patterns' for expressing and efficiently inferring invariants over recursive, list-like data structures from positive samples only. These invariants can be verified using SMT solvers, and integrating this approach into a CHC solver led to state-of-the-art results in automated program verification competitions.


<details>
  <summary>Details</summary>
Motivation: Automated program verification for recursive data structures is still challenging. The proposed approach aims to address this difficulty by enabling efficient inference of invariants for such programs.

Method: Introducing solvable tuple patterns (STPs), devising an inference algorithm for STPs from positive samples, and incorporating this process into a CHC solver with support for list-like data structures.

Result: The CHC solver using STP inference significantly outperformed competitors, winning the ADT-LIN category of CHC-COMP 2025 by a large margin.

Conclusion: The integration of STP inference into a CHC solver improves automated verification for programs with list-like recursive data structures, evidenced by its performance in a competitive setting.

Abstract: Despite the recent progress of automated program verification techniques,
fully automated verification of programs manipulating recursive data structures
remains a challenge. We introduce the notion of solvable tuple patterns (STPs)
to express invariants between list-like recursive data structures. A
distinguishing feature of STPs is that they can be efficiently inferred from
only a small number of positive samples; no negative samples are required. An
SMT solver that supports the sequence theory can be used to check that an
inferred STP is indeed an inductive invariant. After presenting basic
properties of STPs and an STP inference algorithm, we show how to incorporate
the STP inference into a CHC (Constrained Horn Clauses) solver supporting
list-like data structures, which serves as a uniform backend for automated
program verification tools. A CHC solver incorporating the STP inference has
won the ADT-LIN category of CHC-COMP 2025 by a big margin.

</details>


### [14] [Static Factorisation of Probabilistic Programs With User-Labelled Sample Statements and While Loops](https://arxiv.org/abs/2508.20922)
*Markus Böck,Jürgen Cito*

Main category: cs.PL

TL;DR: This paper bridges the gap between probabilistic programming and graphical models for programs with loops and dynamic random variables, enabling improved inference through new static analysis and program slicing techniques.


<details>
  <summary>Details</summary>
Motivation: While Bayesian networks can be encoded as probabilistic programs, it is not clear how (or if) probabilistic programs—especially those with features like dynamic sampling and loops—can be represented graphically and exploited for inference.

Method: The authors extend operational semantics to handle advanced language features like while loops and sample statements, translate programs to control-flow graphs, and perform static analysis to approximate dependencies. They further design program slicing to assist with three inference optimisations.

Result: They provide a sound static analysis method that successfully derives a graphical representation for these rich classes of probabilistic programs. With this, they achieve equivalent factorisation to Bayesian networks where possible, and novel graphical models otherwise. Their slicing-based optimisations are proven and empirically validated.

Conclusion: The paper demonstrates that probabilistic programs with sample statements and while loops can be represented graphically, and introduces a novel static analysis for these cases. It also develops program slicing techniques that enable efficient inference optimisations.

Abstract: It is commonly known that any Bayesian network can be implemented as a
probabilistic program, but the reverse direction is not so clear. In this work,
we address the open question to what extent a probabilistic program with
user-labelled sample statements and while loops - features found in languages
like Gen, Turing, and Pyro - can be represented graphically. To this end, we
extend existing operational semantics to support these language features. By
translating a program to its control-flow graph, we define a sound static
analysis that approximates the dependency structure of the random variables in
the program. As a result, we obtain a static factorisation of the implicitly
defined program density, which is equivalent to the known Bayesian network
factorisation for programs without loops and constant labels, but constitutes a
novel graphical representation for programs that define an unbounded number of
random variables via loops or dynamic labels. We further develop a sound
program slicing technique to leverage this structure to statically enable three
well-known optimisations for the considered program class: we reduce the
variance of gradient estimates in variational inference and we speed up both
single-site Metropolis Hastings and sequential Monte Carlo. These optimisations
are proven correct and empirically shown to match or outperform existing
techniques.

</details>
