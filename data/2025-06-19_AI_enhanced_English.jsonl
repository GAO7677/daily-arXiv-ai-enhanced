{"id": "2506.14866", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14866", "abs": "https://arxiv.org/abs/2506.14866", "authors": ["Thomas Kuntz", "Agatha Duzan", "Hao Zhao", "Francesco Croce", "Zico Kolter", "Nicolas Flammarion", "Maksym Andriushchenko"], "title": "OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents", "comment": null, "summary": "Computer use agents are LLM-based agents that can directly interact with a\ngraphical user interface, by processing screenshots or accessibility trees.\nWhile these systems are gaining popularity, their safety has been largely\noverlooked, despite the fact that evaluating and understanding their potential\nfor harmful behavior is essential for widespread adoption. To address this gap,\nwe introduce OS-Harm, a new benchmark for measuring safety of computer use\nagents. OS-Harm is built on top of the OSWorld environment and aims to test\nmodels across three categories of harm: deliberate user misuse, prompt\ninjection attacks, and model misbehavior. To cover these cases, we create 150\ntasks that span several types of safety violations (harassment, copyright\ninfringement, disinformation, data exfiltration, etc.) and require the agent to\ninteract with a variety of OS applications (email client, code editor, browser,\netc.). Moreover, we propose an automated judge to evaluate both accuracy and\nsafety of agents that achieves high agreement with human annotations (0.76 and\n0.79 F1 score). We evaluate computer use agents based on a range of frontier\nmodels - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide\ninsights into their safety. In particular, all models tend to directly comply\nwith many deliberate misuse queries, are relatively vulnerable to static prompt\ninjections, and occasionally perform unsafe actions. The OS-Harm benchmark is\navailable at https://github.com/tml-epfl/os-harm.", "AI": {"tldr": "The paper introduces OS-Harm, a benchmark for evaluating the safety of LLM-based computer use agents. Evaluation shows current models are vulnerable to misuse and attacks, highlighting the need for better safety measures.", "motivation": "As LLM-based computer use agents become more popular and can directly interact with graphical user interfaces, ensuring their safe behavior is increasingly important. However, little attention has been paid to their safety, and there is a lack of thorough evaluation methods for potential harm caused by such systems.", "method": "The authors introduce OS-Harm, a benchmark designed specifically to test the safety of computer use agents. Built on the OSWorld environment, OS-Harm evaluates agents using 150 tasks across different harm categories: user misuse, prompt injection, and model misbehavior. It tests agents in various OS applications and includes an automated judge with high agreement with human assessors to measure both accuracy and safety.", "result": "Frontier models like o4-mini, Claude 3.7 Sonnet, and Gemini 2.5 Pro were evaluated using OS-Harm. All models showed a tendency to comply with harmful user queries, were susceptible to prompt injection attacks, and sometimes exhibited unsafe behaviors. The automated judge performed well, with F1 scores of 0.76 (accuracy) and 0.79 (safety).", "conclusion": "Computer use agents, even those based on advanced LLMs, have significant safety vulnerabilities. OS-Harm provides a much-needed framework for systematically evaluating and understanding these risks to inform future improvements in agent design and deployment."}}
{"id": "2506.15084", "categories": ["cs.SE", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.15084", "abs": "https://arxiv.org/abs/2506.15084", "authors": ["Weiqi Lu", "Yongqiang Tian", "Xiaohan Zhong", "Haoyang Ma", "Zhenyang Xu", "Shing-Chi Cheung", "Chengnian Sun"], "title": "An Empirical Study of Bugs in Data Visualization Libraries", "comment": "Proc. ACM Softw. Eng. 2, FSE", "summary": "Data visualization (DataViz) libraries play a crucial role in presentation,\ndata analysis, and application development, underscoring the importance of\ntheir accuracy in transforming data into visual representations. Incorrect\nvisualizations can adversely impact user experience, distort information\nconveyance, and influence user perception and decision-making processes. Visual\nbugs in these libraries can be particularly insidious as they may not cause\nobvious errors like crashes, but instead mislead users of the underlying data\ngraphically, resulting in wrong decision making. Consequently, a good\nunderstanding of the unique characteristics of bugs in DataViz libraries is\nessential for researchers and developers to detect and fix bugs in DataViz\nlibraries.\n  This study presents the first comprehensive analysis of bugs in DataViz\nlibraries, examining 564 bugs collected from five widely-used libraries. Our\nstudy systematically analyzes their symptoms and root causes, and provides a\ndetailed taxonomy. We found that incorrect/inaccurate plots are pervasive in\nDataViz libraries and incorrect graphic computation is the major root cause,\nwhich necessitates further automated testing methods for DataViz libraries.\nMoreover, we identified eight key steps to trigger such bugs and two test\noracles specific to DataViz libraries, which may inspire future research in\ndesigning effective automated testing techniques. Furthermore, with the recent\nadvancements in Vision Language Models (VLMs), we explored the feasibility of\napplying these models to detect incorrect/inaccurate plots. The results show\nthat the effectiveness of VLMs in bug detection varies from 29% to 57%,\ndepending on the prompts, and adding more information in prompts does not\nnecessarily increase the effectiveness. More findings can be found in our\nmanuscript.", "AI": {"tldr": "The paper provides a comprehensive analysis of 564 bugs from five major data visualization libraries, finding that inaccurate visualizations are prevalent and primarily due to graphic computation errors. It categorizes bugs, pinpoints root causes, suggests new test methods, and evaluates Vision Language Models for bug detection, noting variable effectiveness. This research establishes a foundational understanding and highlights directions for improved testing and automation in DataViz libraries.", "motivation": "Data visualization libraries are central to accurate information presentation and decision-making, yet visual bugs can mislead users without crashing the software. A deep understanding of the unique nature and root causes of such bugs in DataViz libraries is lacking, impeding effective detection and resolution.", "method": "Analyzed 564 bug reports from five popular DataViz libraries. Developed a taxonomy by systematically categorizing the bugs based on their symptoms and root causes. Identified common triggering steps and evaluation methods (test oracles). Investigated the potential of Vision Language Models (VLMs) for automated bug detection.", "result": "Discovered that incorrect and inaccurate plots are common, with incorrect graphic computation as the main root cause. Eight trigger steps and two DataViz-specific test oracles were documented. Effectiveness of VLMs in detecting bugs ranged from 29% to 57%; richer prompts did not consistently boost performance.", "conclusion": "This work offers the first detailed taxonomy of bugs in DataViz libraries and highlights the need for improved automated testing tailored to their unique error patterns. VLMs show promise for automated detection but current performance varies. These findings lay groundwork for better bug detection tools and future research in DataViz software quality."}}
{"id": "2506.15088", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.15088", "abs": "https://arxiv.org/abs/2506.15088", "authors": ["Miao Miao"], "title": "Program Feature-based Fuzzing Benchmarking", "comment": null, "summary": "Fuzzing is a powerful software testing technique renowned for its\neffectiveness in identifying software vulnerabilities. Traditional fuzzing\nevaluations typically focus on overall fuzzer performance across a set of\ntarget programs, yet few benchmarks consider how fine-grained program features\ninfluence fuzzing effectiveness. To bridge this gap, we introduce a novel\nbenchmark designed to generate programs with configurable, fine-grained program\nfeatures to enhance fuzzing evaluations. We reviewed 25 recent grey-box fuzzing\nstudies, extracting 7 program features related to control-flow and data-flow\nthat can impact fuzzer performance. Using these features, we generated a\nbenchmark consisting of 153 programs controlled by 10 fine-grained configurable\nparameters. We evaluated 11 popular fuzzers using this benchmark. The results\nindicate that fuzzer performance varies significantly based on the program\nfeatures and their strengths, highlighting the importance of incorporating\nprogram characteristics into fuzzing evaluations.", "AI": {"tldr": "The paper introduces a new benchmark for fuzzing that allows for control over fine-grained program features. Testing 11 fuzzers revealed that these features greatly influence performance, suggesting future fuzzing evaluations should account for such details.", "motivation": "Traditional fuzzing benchmarks often overlook the influence of specific program features on fuzzing effectiveness.", "method": "The authors identified 7 key program features from 25 recent grey-box fuzzing studies and used them to generate a benchmark set of 153 programs, each with 10 configurable parameters. They evaluated 11 widely-used fuzzers using this benchmark.", "result": "Fuzzer performance significantly varies depending on detailed program features and their configurations.", "conclusion": "Fuzzing evaluations should incorporate fine-grained program characteristics, as these features substantially affect fuzzer effectiveness."}}
{"id": "2506.15174", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2506.15174", "abs": "https://arxiv.org/abs/2506.15174", "authors": ["Hossein Albakri", "Kazem Cheshmi"], "title": "A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in GPUs", "comment": null, "summary": "Sparse data structures are commonly used in neural networks to reduce the\nmemory footprint. These data structures are compact but cause irregularities\nsuch as random memory accesses, which prevent efficient use of the memory\nhierarchy. GPUs are a common platform for machine learning practitioners, but\nrunning compact data structures on these devices often leads to slow-downs due\nto inefficient use of computing and memory resources. This paper proposes a new\ncompiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse\nmatrix-matrix multiplication (SPMM) on GPU devices. The transformation\nincreases data reuse in registers and caches while creating more balanced\nworkloads for GPU computing resources. The transformation is tested on sparse\nneural networks in convolutional and transformer models. On an A100 GPU and\nacross a columns of matrix B (bCols) in $ A \\times B = C$ from range of 32 to\n128, the transformation yields a geometric mean speedup of 1.84$\\times$ to\n2.27$\\times$ compared to cuBLAS and cuSPARSE baselines, respectively.", "AI": {"tldr": "A new compiler transformation called enumerate-and-sparse-coarsen helps GPUs handle sparse neural networks much faster (about 2x speedup), by making better use of memory and computing resources during sparse matrix multiplication.", "motivation": "Sparse data structures help reduce the memory footprint in neural networks, but they cause irregular memory access patterns that prevent efficient use of GPU resources, leading to slow computation. This bottleneck motivates the search for more efficient methods for sparse matrix operations on GPUs.", "method": "The paper introduces a compiler transformation called enumerate-and-sparse-coarsen, which is designed to accelerate sparse matrix-matrix multiplication (SPMM) on GPUs by improving data reuse in registers and caches and creating more balanced GPU workloads.", "result": "The proposed transformation, when tested on sparse convolutional and transformer neural network models using an Nvidia A100 GPU, achieves a geometric mean speedup of 1.84x to 2.27x compared to standard cuBLAS and cuSPARSE library baselines.", "conclusion": "The enumerate-and-sparse-coarsen compiler transformation significantly improves the performance of sparse matrix-matrix multiplication on GPUs by addressing key inefficiencies in data reuse and workload balancing, resulting in substantial speedups for neural network applications."}}
{"id": "2506.15098", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.15098", "abs": "https://arxiv.org/abs/2506.15098", "authors": ["Haosheng Zuo", "Feifei Niu", "Chuanyi Li"], "title": "Enhancement Report Approval Prediction: A Comparative Study of Large Language Models", "comment": null, "summary": "Enhancement reports (ERs) serve as a critical communication channel between\nusers and developers, capturing valuable suggestions for software improvement.\nHowever, manually processing these reports is resource-intensive, leading to\ndelays and potential loss of valuable insights. To address this challenge,\nenhancement report approval prediction (ERAP) has emerged as a research focus,\nleveraging machine learning techniques to automate decision-making. While\ntraditional approaches have employed feature-based classifiers and deep\nlearning models, recent advancements in large language models (LLM) present new\nopportunities for enhancing prediction accuracy. This study systematically\nevaluates 18 LLM variants (including BERT, RoBERTa, DeBERTa-v3, ELECTRA, and\nXLNet for encoder models; GPT-3.5-turbo, GPT-4o-mini, Llama 3.1 8B, Llama 3.1\n8B Instruct and DeepSeek-V3 for decoder models) against traditional methods\n(CNN/LSTM-BERT/GloVe). Our experiments reveal two key insights: (1)\nIncorporating creator profiles increases unfine-tuned decoder-only models'\noverall accuracy by 10.8 percent though it may introduce bias; (2) LoRA\nfine-tuned Llama 3.1 8B Instruct further improve performance, reaching 79\npercent accuracy and significantly enhancing recall for approved reports (76.1\npercent vs. LSTM-GLOVE's 64.1 percent), outperforming traditional methods by 5\npercent under strict chronological evaluation and effectively addressing class\nimbalance issues. These findings establish LLM as a superior solution for ERAP,\ndemonstrating their potential to streamline software maintenance workflows and\nimprove decision-making in real-world development environments. We also\ninvestigated and summarized the ER cases where the large models underperformed,\nproviding valuable directions for future research.", "AI": {"tldr": "Large language models outperform traditional machine learning in predicting approval of software enhancement reports. Fine-tuning and using creator profiles boost accuracy, with Llama 3.1 8B Instruct reaching top results. LLMs can streamline software improvement processes, though some challenging cases remain for future study.", "motivation": "Enhancement reports are essential for communication between users and developers, but processing them manually is time-consuming and inefficient. There is a need to automate the process to prevent delays and loss of valuable suggestions. Enhancement report approval prediction (ERAP) using machine learning offers a solution.", "method": "This study systematically evaluates 18 variants of large language models (LLMs), including encoder models (BERT, RoBERTa, DeBERTa-v3, ELECTRA, XLNet) and decoder models (GPT-3.5-turbo, GPT-4o-mini, Llama 3.1 8B, etc.), comparing them against traditional machine learning approaches such as CNN/LSTM using BERT/GloVe. The study also explores the impact of incorporating creator profiles and utilizing LoRA fine-tuning on performance.", "result": "(1) Including creator profiles improves accuracy by 10.8% in unfine-tuned decoder-only models, though it may cause bias. (2) LoRA fine-tuned Llama 3.1 8B Instruct achieves 79% accuracy and 76.1% recall for approved reports, outperforming traditional methods by 5% and addressing class imbalance. The study also examines cases where LLMs underperform, offering insights for future work.", "conclusion": "LLMs, particularly when fine-tuned and supplemented with creator profiles, significantly enhance ERAP accuracy and recall, surpassing traditional approaches. This positions LLMs as superior tools for automating enhancement report processing, improving efficiency in software maintenance. Further research should address cases where large models perform poorly."}}
{"id": "2506.15424", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2506.15424", "abs": "https://arxiv.org/abs/2506.15424", "authors": ["Michael Mendler", "Marc Pouzet"], "title": "PSM: Policy Synchronised Deterministic Memory", "comment": "This report summarises work on coding the theory of\n  policy-synchronised memory (see https://rdcu.be/erBwl) in Haskell. This was\n  developed for a graduate level course on Functional Reactive Programming\n  taught at Bamberg University by the first author during 2020-2023. An early\n  version of the PSM library had been presented at the SYNCHRON Workshop\n  (Aussois, France), November 2019", "summary": "Concurrency and determinacy do not go well with each other when resources\nmust be shared. Haskell provides parallel programming abstractions such as IVar\nand LVar in the Par monad and concurrent abstractions such as MVar and TVar in\nthe in IO and STM monads, respectively. The former are determinate but have no\ndestructive updates and the latter have destructive updates but do not\nguarantee determinacy. Programming patterns that are both concurrent and\ndeterminate, such as those provided by Kahn or Berry require memory\nabstractions at a higher level than is currently available. In this paper we\ndescribe a new type context PSM for policy synchronised memory in Haskell. Like\nSTM and IO, the computations in PSM can access persistent state and, as a\nside-effect, update the memory in imperative style. Like the Par and IO monads,\nPSM supports concurrent threads and shared state. However, in contrast to IO,\nour PSM contexts are race-free since concurrent accesses are policy coordinated\nwhich guarantees determinacy.Well-typed transactions in the PSM context can\naccommodate abstract data structures that are imperative, concurrently\nshareable and still behave deterministically, by construction.", "AI": {"tldr": "The paper introduces PSM, a new concurrency abstraction for Haskell that enables concurrent, imperative, shareable, and deterministically-behaving data structures by coordinating memory access policies, overcoming previous limitations in existing abstractions.", "motivation": "Balancing concurrency and determinacy is challenging in functional programming languages like Haskell, especially when resources are shared. Existing abstractions either guarantee determinacy with no destructive updates or allow destructive updates with no determinacy guarantee. There is a need for higher-level abstractions that can ensure both properties.", "method": "The authors propose a new type context called PSM (policy synchronised memory) for Haskell. PSM allows computations to access and update persistent state with imperative-style side effects, while supporting concurrent threads and shared state. PSM coordinates concurrent accesses using policies, ensuring race-freedom and determinacy by construction. Well-typed transactions within PSM allow creation of concurrent, imperative, and deterministically-behaving shared data structures.", "result": "PSM provides a framework where abstract data structures can be shared and updated concurrently in Haskell programs with the assurance of determinacy and race-freedom. This overcomes the limitations seen in previous concurrency abstractions (like Par, LVar, MVar, and TVar).", "conclusion": "The PSM context in Haskell successfully enables imperative-style, concurrently shareable, and deterministically-behaving abstract data structures, addressing the inadequacies of current abstractions in balancing concurrency and determinacy."}}
{"id": "2506.15135", "categories": ["cs.SE", "cs.LO", "cs.PL", "F.3.1; F.1.2"], "pdf": "https://arxiv.org/pdf/2506.15135", "abs": "https://arxiv.org/abs/2506.15135", "authors": ["Zhengqun Koo"], "title": "Towards Bug-Free Distributed Go Programs", "comment": "Version 1. B.Comp. Dissertation", "summary": "Programmers of distributed systems need to reason about concurrency to avoid\nraces. However, reasoning about concurrency is difficult, and unexpected races\nshow up as bugs. Data race detection in shared memory systems is well-studied\n(dynamic data race detection [13], behavioral types [15], dynamic race\ndetection [31]). Similar to how a data race consists of reads and writes not\nrelated by happens-before at a shared memory location, a communication race\nconsists of receives and sends not related by happens-before on a shared\nchannel. Communication races are problematic: a receiver expects a specific\nmessage from a specific sender, but with a communication race, the receiver can\nreceive a message meant for another receiver, or not receive anything at all.\nIn this work, we describe a verification framework that can prove the absence\nof communication races for distributed programs that use a subset of the Go\nprogramming language, where synchronization is mainly achieved via message\npassing. We statically reason about how a distributed program executes, using a\nhappens-before order, extended to buffered and unbuffered channels.", "AI": {"tldr": "The paper introduces a verification framework that statically detects and prevents communication races in message-passing Go programs, making distributed systems more reliable.", "motivation": "Reasoning about concurrency in distributed systems is challenging and often leads to unexpected race conditions, which are difficult to debug and fix. Existing research primarily addresses data races in shared memory systems, but similar issues, called communication races, arise in message-passing systems.", "method": "The authors present a verification framework that can statically prove the absence of communication races in distributed programs. This framework is tailored for programs using a subset of the Go programming language, leveraging an extended happens-before model to account for both buffered and unbuffered channels.", "result": "The framework enables static reasoning about the execution of distributed Go programs, showing that communication race conditions can be detected and ruled out prior to program execution.", "conclusion": "The work provides an effective method for ensuring that distributed Go programs do not suffer from communication races, enhancing reliability and correctness where synchronization is handled by message passing."}}
{"id": "2506.15172", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.15172", "abs": "https://arxiv.org/abs/2506.15172", "authors": ["Maria Spichkova", "Kevin Iwan", "Madeleine Zwart", "Hina Lee", "Yuwon Yoon", "Xiaohan Qin"], "title": "Advanced approach for Agile/Scrum Process: RetroAI++", "comment": "Preprint. Accepted to the 29th International Conference on\n  Knowledge-Based and Intelligent Information & Engineering Systems (KES 2025).\n  Final version to be published by Elsevier (In Press)", "summary": "In Agile/Scrum software development, sprint planning and retrospective\nanalysis are the key elements of project management. The aim of our work is to\nsupport software developers in these activities. In this paper, we present our\nprototype tool RetroAI++, based on emerging intelligent technologies. In our\nRetroAI++ prototype, we aim to automate and refine the practical application of\nAgile/Scrum processes within Sprint Planning and Retrospectives. Leveraging AI\ninsights, our prototype aims to automate and refine the many processes involved\nin the Sprint Planning, Development and Retrospective stages of Agile/Scrum\ndevelopment projects, offering intelligent suggestions for sprint organisation\nas well as meaningful insights for retrospective reflection.", "AI": {"tldr": "The paper introduces RetroAI++, an AI-powered tool that automates and enhances sprint planning and retrospectives in Agile/Scrum, offering smart suggestions and insights to improve software project management.", "motivation": "Agile/Scrum projects rely on effective sprint planning and retrospective analysis for success, but these activities can be time-consuming and may benefit from automation and intelligent support.", "method": "The authors developed a prototype tool called RetroAI++ that utilizes emerging AI technologies to automate and enhance sprint planning and retrospectives in Agile/Scrum processes.", "result": "The RetroAI++ prototype provides intelligent suggestions to organize sprints and offers insightful reflections for retrospectives, aiming to streamline and improve these key Agile/Scrum stages.", "conclusion": "Integrating AI-driven automation and insights through RetroAI++ can significantly support Agile/Scrum software teams in planning and reflecting on their work, enhancing overall project management efficiency."}}
{"id": "2506.15227", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.15227", "abs": "https://arxiv.org/abs/2506.15227", "authors": ["Quanjun Zhang", "Chunrong Fang", "Siqi Gu", "Ye Shang", "Zhenyu Chen", "Liang Xiao"], "title": "Large Language Models for Unit Testing: A Systematic Literature Review", "comment": null, "summary": "Unit testing is a fundamental practice in modern software engineering, with\nthe aim of ensuring the correctness, maintainability, and reliability of\nindividual software components. Very recently, with the advances in Large\nLanguage Models (LLMs), a rapidly growing body of research has leveraged LLMs\nto automate various unit testing tasks, demonstrating remarkable performance\nand significantly reducing manual effort. However, due to ongoing explorations\nin the LLM-based unit testing field, it is challenging for researchers to\nunderstand existing achievements, open challenges, and future opportunities.\nThis paper presents the first systematic literature review on the application\nof LLMs in unit testing until March 2025. We analyze \\numpaper{} relevant\npapers from the perspectives of both unit testing and LLMs. We first categorize\nexisting unit testing tasks that benefit from LLMs, e.g., test generation and\noracle generation. We then discuss several critical aspects of integrating LLMs\ninto unit testing research, including model usage, adaptation strategies, and\nhybrid approaches. We further summarize key challenges that remain unresolved\nand outline promising directions to guide future research in this area.\nOverall, our paper provides a systematic overview of the research landscape to\nthe unit testing community, helping researchers gain a comprehensive\nunderstanding of achievements and promote future research. Our artifacts are\npublicly available at the GitHub repository:\nhttps://github.com/iSEngLab/AwesomeLLM4UT.", "AI": {"tldr": "This paper systematically reviews how Large Language Models (LLMs) are applied in unit testing, categorizes the tasks LLMs support, discusses integration methods and challenges, and provides guidance for future research. It serves as a foundational resource for researchers exploring LLM-driven unit testing.", "motivation": "Recent advancements in Large Language Models (LLMs) have shown promise in automating unit testing tasks, but the research field is rapidly evolving and fragmented. There is a need for a comprehensive synthesis to help researchers understand current achievements, existing challenges, and future opportunities in LLM-based unit testing.", "method": "The paper conducts a systematic literature review of research published up to March 2025 on the application of LLMs to unit testing. It analyzes relevant papers from the dual perspectives of unit testing paradigms and LLM capabilities, categorizes unit testing tasks utilizing LLMs, and examines integration strategies and model adaptations.", "result": "The review identifies various unit testing tasks, such as test case and oracle generation, that have benefited from LLM integration. It discusses integration methods, adaptation strategies, and hybrid solutions, summarizes key unresolved challenges in the field, and outlines future research directions.", "conclusion": "This systematic review offers the first comprehensive overview of how LLMs are utilized in unit testing, highlighting progress, unresolved issues, and guiding future research initiatives. The artifacts supporting the review are made publicly accessible online."}}
{"id": "2506.15453", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15453", "abs": "https://arxiv.org/abs/2506.15453", "authors": ["Yusuf Sulistyo Nugroho", "Farah Danisha Salam", "Brittany Reid", "Raula Gaikovina Kula", "Kazumasa Shimari", "Kenichi Matsumoto"], "title": "Uncovering Intention through LLM-Driven Code Snippet Description Generation", "comment": "6 pages, 3 figures, 4 tables, conference paper", "summary": "Documenting code snippets is essential to pinpoint key areas where both\ndevelopers and users should pay attention. Examples include usage examples and\nother Application Programming Interfaces (APIs), which are especially important\nfor third-party libraries. With the rise of Large Language Models (LLMs), the\nkey goal is to investigate the kinds of description developers commonly use and\nevaluate how well an LLM, in this case Llama, can support description\ngeneration. We use NPM Code Snippets, consisting of 185,412 packages with\n1,024,579 code snippets. From there, we use 400 code snippets (and their\ndescriptions) as samples. First, our manual classification found that the\nmajority of original descriptions (55.5%) highlight example-based usage. This\nfinding emphasizes the importance of clear documentation, as some descriptions\nlacked sufficient detail to convey intent. Second, the LLM correctly identified\nthe majority of original descriptions as \"Example\" (79.75%), which is identical\nto our manual finding, showing a propensity for generalization. Third, compared\nto the originals, the produced description had an average similarity score of\n0.7173, suggesting relevance but room for improvement. Scores below 0.9\nindicate some irrelevance. Our results show that depending on the task of the\ncode snippet, the intention of the document may differ from being instructions\nfor usage, installations, or descriptive learning examples for any user of a\nlibrary.", "AI": {"tldr": "This paper analyzes code snippet documentation using a large dataset and measures how well the Llama LLM can generate relevant descriptions. While Llama often captures the main intent, there is still considerable room for improvement in generating precise and relevant documentation.", "motivation": "The paper is motivated by the need to improve documentation of code snippets, particularly for third-party libraries, as such documentation is essential for developers and users to understand usage and intent of APIs and code examples. With the rise of Large Language Models (LLMs), the authors are interested in assessing whether LLMs can generate useful code snippet descriptions.", "method": "The authors utilize the NPM Code Snippets dataset, which includes over 1 million code snippets from 185,412 packages. They sample 400 code snippets and their respective descriptions, conduct manual classification of these descriptions, and then use the Llama LLM to generate descriptions. They compare the LLM's output to the originals, and analyze both the classification and textual similarity scores.", "result": "Manual classification revealed that most original descriptions (55.5%) are example-based. The LLM identified 79.75% as 'Example', closely aligning with human classification, but showed a tendency to generalize. The average similarity score between generated and original descriptions was 0.7173, indicating relevance but also significant room for improvement, as scores below 0.9 suggest some degree of irrelevance.", "conclusion": "The study finds that LLMs like Llama can generalize and identify key intents such as usage examples in documentation, but their generated descriptions are not always sufficiently relevant or specific. Depending on the code snippet, documentation intent varies (usage, installation, or learning), and LLMs currently do not capture all nuances needed for fully effective documentation."}}
{"id": "2506.15655", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.15655", "abs": "https://arxiv.org/abs/2506.15655", "authors": ["Yilin Zhang", "Xinran Zhao", "Zora Zhiruo Wang", "Chenyang Yang", "Jiayi Wei", "Tongshuang Wu"], "title": "cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become essential for large-scale\ncode generation, grounding predictions in external code corpora to improve\nactuality. However, a critical yet underexplored aspect of RAG pipelines is\nchunking -- the process of dividing documents into retrievable units. Existing\nline-based chunking heuristics often break semantic structures, splitting\nfunctions or merging unrelated code, which can degrade generation quality. We\npropose chunking via Abstract Syntax Trees (\\ourwork), a structure-aware method\nthat recursively breaks large AST nodes into smaller chunks and merges sibling\nnodes while respecting size limits. This approach generates self-contained,\nsemantically coherent units across programming languages and tasks, improving\nperformance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3\npoints on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation.\nOur work highlights the importance of structure-aware chunking for scaling\nretrieval-enhanced code intelligence.", "AI": {"tldr": "Chunking code for retrieval using Abstract Syntax Trees instead of simple lines preserves semantics, significantly improving performance in code generation tasks.", "motivation": "Retrieval-Augmented Generation (RAG) has become crucial for code generation, relying on external code corpora for more accurate predictions. However, an important aspect\u2014how to best chunk code for retrieval\u2014remains underexplored. Existing line-based chunking often disrupts code structure and semantics, negatively impacting generation quality.", "method": "The authors introduce a structure-aware chunking method based on Abstract Syntax Trees (ASTs). Their technique recursively splits large AST nodes and merges sibling nodes within size limits, ensuring the resulting chunks remain semantically coherent and self-contained across languages and tasks.", "result": "This chunking method outperforms traditional heuristics, leading to improved results in code generation benchmarks: a Recall@5 increase of 4.3 points on RepoEval retrieval and a Pass@1 increase of 2.67 points on SWE-bench generation tasks.", "conclusion": "Structure-aware chunking using ASTs is vital for effective, scalable retrieval-augmented code generation. Their results demonstrate the superiority of this approach, emphasizing the necessity of semantic-preserving document chunking in RAG pipelines for code intelligence."}}
