<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 23]
- [cs.PL](#cs.PL) [Total: 7]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Adversarial Bug Reports as a Security Risk in Language Model-Based Automated Program Repair](https://arxiv.org/abs/2509.05372)
*Piotr Przymus,Andreas Happe,Jürgen Cito*

Main category: cs.SE

TL;DR: LLM-based automated program repair is highly susceptible to adversarial bug reports that can introduce malicious patches. The defenses available today are insufficient, and generating attacks is much easier than defending against them. The paper proposes a framework for attack automation and offers recommendations for improving the security of APR systems.


<details>
  <summary>Details</summary>
Motivation: Automated Program Repair (APR) systems powered by Large Language Models (LLMs) are increasingly being used in software development to automatically fix bugs reported in natural language. However, using these systems introduces a new risk: attackers could submit malicious bug reports to trick the repair system into generating insecure or harmful patches.

Method: The authors create a threat model and empirically study the vulnerability of modern LLM-based APR systems to adversarial bug reports. They generate 51 adversarial bug reports using both manual and automated methods, then test these reports against leading APR models. They evaluate the effectiveness of various pre-repair (input filtering) and post-repair (output analysis) defenses.

Result: The study finds that current defenses are inadequate, as 90% of adversarial bug reports successfully caused the generation of attacker-aligned (malicious) patches. The best pre-repair filter blocked only 47% of attacks, and post-repair checks (sometimes requiring human review) were only effective 58% of the time. The authors also introduce a framework for automating the generation of adversarial bug reports, highlighting that it's much cheaper and easier to create attacks than to defend against them.

Conclusion: Existing LLM-based APR systems are highly vulnerable to adversarial bug reports, and existing defense mechanisms are not sufficient to reliably detect or prevent these attacks. The paper recommends strategies to harden APR systems against such misuse and emphasizes the need for continued research into more trustworthy automated repair solutions.

Abstract: Large Language Model (LLM) - based Automated Program Repair (APR) systems are
increasingly integrated into modern software development workflows, offering
automated patches in response to natural language bug reports. However, this
reliance on untrusted user input introduces a novel and underexplored attack
surface. In this paper, we investigate the security risks posed by adversarial
bug reports -- realistic-looking issue submissions crafted to mislead APR
systems into producing insecure or harmful code changes. We develop a
comprehensive threat model and conduct an empirical study to evaluate the
vulnerability of state-of-the-art APR systems to such attacks. Our
demonstration comprises 51 adversarial bug reports generated across a spectrum
of strategies, from manual curation to fully automated pipelines. We test these
against leading APR model and assess both pre-repair defenses (e.g., LlamaGuard
variants, PromptGuard variants, Granite-Guardian, and custom LLM filters) and
post-repair detectors (GitHub Copilot, CodeQL). Our findings show that current
defenses are insufficient: 90\% of crafted bug reports triggered
attacker-aligned patches. The best pre-repair filter blocked only 47\%, while
post-repair analysis-often requiring human oversight-was effective in just 58\%
of cases. To support scalable security testing, we introduce a prototype
framework for automating the generation of adversarial bug reports. Our
analysis exposes a structural asymmetry: generating adversarial inputs is
inexpensive, while detecting or mitigating them remains costly and error-prone.
We conclude with practical recommendations for improving the robustness of APR
systems against adversarial misuse and highlight directions for future work on
trustworthy automated repair.

</details>


### [2] [Reverse Browser: Vector-Image-to-Code Generator](https://arxiv.org/abs/2509.05394)
*Zoltan Toth-Czifra*

Main category: cs.SE

TL;DR: The paper introduces vector image-based inputs and a new multi-scale metric for automated UI design-to-code conversion, achieving better fidelity than traditional bitmap approaches but still facing some limitations with current models.


<details>
  <summary>Details</summary>
Motivation: Current image-to-code methods relying on bitmap inputs do not achieve high fidelity in reproducing original UI designs. This motivates the author to explore alternative approaches to improve accuracy and quality.

Method: The researcher uses vector images as input for machine learning models in converting UI designs to code. They create large datasets, evaluate various Image Quality Assessment algorithms, introduce a new multi-scale metric, and train a large open-weight model.

Result: By using vector images and introducing a multi-scale metric, the proposed method improves fidelity in image-to-code conversion compared to existing benchmarks, though challenges and limitations persist with the trained models.

Conclusion: The paper concludes that using vector images as input for image-to-code tasks, along with new datasets and improved metrics, offers better fidelity to original UI designs. However, limitations remain, even with large open-weight models.

Abstract: Automating the conversion of user interface design into code (image-to-code
or image-to-UI) is an active area of software engineering research. However,
the state-of-the-art solutions do not achieve high fidelity to the original
design, as evidenced by benchmarks. In this work, I approach the problem
differently: I use vector images instead of bitmaps as model input. I create
several large datasets for training machine learning models. I evaluate the
available array of Image Quality Assessment (IQA) algorithms and introduce a
new, multi-scale metric. I then train a large open-weights model and discuss
its limitations.

</details>


### [3] [Combining TSL and LLM to Automate REST API Testing: A Comparative Study](https://arxiv.org/abs/2509.05540)
*Thiago Barradas,Aline Paes,Vânia de Oliveira Neves*

Main category: cs.SE

TL;DR: RestTSLLM leverages TSL and LLMs for automating REST API test generation. In evaluations, Claude 3.5 Sonnet proved to be the best performer in test success rate, coverage, and robustness, demonstrating LLMs' promise for API test automation.


<details>
  <summary>Details</summary>
Motivation: Testing REST APIs is challenging due to distributed system complexity, numerous scenario possibilities, and limited resources. Existing approaches rely on exhaustive manual efforts, which are inefficient and often miss failures.

Method: RestTSLLM is introduced, combining Test Specification Language (TSL) and Large Language Models (LLMs) to generate automated REST API test cases. Prompt engineering and a fully automated pipeline are employed to assess and compare various LLMs using OpenAPI specifications.

Result: Among the tested models, Claude 3.5 Sonnet delivered the highest success rate, test coverage, and mutation score, outperforming Deepseek R1, Qwen 2.5 32b, and Sabia 3, showing robust and contextually coherent test generation.

Conclusion: LLMs, particularly Claude 3.5 Sonnet, are effective at automating REST API test generation, highlighting their potential to improve software quality and test coverage with reduced manual intervention.

Abstract: The effective execution of tests for REST APIs remains a considerable
challenge for development teams, driven by the inherent complexity of
distributed systems, the multitude of possible scenarios, and the limited time
available for test design. Exhaustive testing of all input combinations is
impractical, often resulting in undetected failures, high manual effort, and
limited test coverage. To address these issues, we introduce RestTSLLM, an
approach that uses Test Specification Language (TSL) in conjunction with Large
Language Models (LLMs) to automate the generation of test cases for REST APIs.
The approach targets two core challenges: the creation of test scenarios and
the definition of appropriate input data. The proposed solution integrates
prompt engineering techniques with an automated pipeline to evaluate various
LLMs on their ability to generate tests from OpenAPI specifications. The
evaluation focused on metrics such as success rate, test coverage, and mutation
score, enabling a systematic comparison of model performance. The results
indicate that the best-performing LLMs - Claude 3.5 Sonnet (Anthropic),
Deepseek R1 (Deepseek), Qwen 2.5 32b (Alibaba), and Sabia 3 (Maritaca) -
consistently produced robust and contextually coherent REST API tests. Among
them, Claude 3.5 Sonnet outperformed all other models across every metric,
emerging in this study as the most suitable model for this task. These findings
highlight the potential of LLMs to automate the generation of tests based on
API specifications.

</details>


### [4] [Natural Language-Programming Language Software Traceability Link Recovery Needs More than Textual Similarity](https://arxiv.org/abs/2509.05585)
*Zhiyuan Zou,Bangchao Wang,Peng Liang,Tingting Bi,Huan Jin*

Main category: cs.SE

TL;DR: Traditional textual similarity-based TLR struggles with NL-PL tasks due to a semantic gap. This paper introduces multi-strategy models based on empirical analysis and demonstrates that integrating domain-specific auxiliary strategies boosts performance over current methods, showing notable F1-score improvements on open-source projects.


<details>
  <summary>Details</summary>
Motivation: Textual similarity has traditionally been the main technique used for software traceability link recovery (TLR). However, this method struggles with tasks involving both natural language and programming language (NL-PL) artifacts due to a semantic gap between the two. There is a need for improved techniques that can overcome these limitations.

Method: The authors perform a large-scale empirical evaluation to identify limitations of textual similarity in NL-PL scenarios. They then propose integrating multiple domain-specific auxiliary strategies into two models: the Heterogeneous Graph Transformer (HGT) using edge types, and the prompt-based Gemini 2.5 Pro via enhanced input. The new approaches are evaluated on requirements-to-code TLR tasks.

Result: Both the multi-strategy HGT and Gemini 2.5 Pro models outperform their baseline versions without auxiliary strategies. Compared to the state-of-the-art HGNNLink method, multi-strategy HGT and Gemini 2.5 Pro achieve average F1-score increases of 3.68% and 8.84%, respectively, on twelve open-source projects.

Conclusion: Multi-strategy integration significantly improves model performance for requirements-to-code traceability link recovery, especially in NL-PL scenarios. The proposed methods surpass baseline models and state-of-the-art techniques.

Abstract: In the field of software traceability link recovery (TLR), textual similarity
has long been regarded as the core criterion. However, in tasks involving
natural language and programming language (NL-PL) artifacts, relying solely on
textual similarity is limited by their semantic gap. To this end, we conducted
a large-scale empirical evaluation across various types of TLR tasks, revealing
the limitations of textual similarity in NL-PL scenarios. To address these
limitations, we propose an approach that incorporates multiple domain-specific
auxiliary strategies, identified through empirical analysis, into two models:
the Heterogeneous Graph Transformer (HGT) via edge types and the prompt-based
Gemini 2.5 Pro via additional input information. We then evaluated our approach
using the widely studied requirements-to-code TLR task, a representative case
of NL-PL TLR. Experimental results show that both the multi-strategy HGT and
Gemini 2.5 Pro models outperformed their original counterparts without strategy
integration. Furthermore, compared to the current state-of-the-art method
HGNNLink, the multi-strategy HGT and Gemini 2.5 Pro models achieved average
F1-score improvements of 3.68% and 8.84%, respectively, across twelve
open-source projects, demonstrating the effectiveness of multi-strategy
integration in enhancing overall model performance for the requirements-code
TLR task.

</details>


### [5] [Verifying Correctness of PLC Software during System Evolution using Model Containment Approach](https://arxiv.org/abs/2509.05596)
*Soumyadip Bandyopadhyay,Santonu Sarkar*

Main category: cs.SE

TL;DR: This paper presents a novel method that uses Petri net models and symbolic path equivalence to efficiently verify the correctness of upgraded PLC software, showing significant performance gains over standard tools on real benchmarks.


<details>
  <summary>Details</summary>
Motivation: Upgrading PLC software is necessary for evolving industrial needs, but ensuring the correctness of such upgrades is difficult. Industrial users need reliable methods to verify that updates do not break existing functionality.

Method: The approach involves converting both the original and upgraded PLC software versions from sequential function charts (SFC) into Petri net models. A novel containment checking algorithm based on symbolic path equivalence is used to verify if the functionality of the older version is contained within the new one. A custom Petri net-based checker is developed for this purpose.

Result: Experimental evaluation was conducted using 80 real-world benchmarks from the OSCAT library. The framework showed strong scalability and effectiveness. When compared to the verifAPS tool, the proposed method was nearly 4 times faster.

Conclusion: The proposed verification-based method is effective and scalable for ensuring the correctness of PLC software upgrades, outperforming existing popular tools by a significant margin.

Abstract: Upgradation of Programmable Logic Controller (PLC) software is quite common
to accommodate evolving industrial requirements. Verifying the correctness of
such upgrades remains a significant challenge. In this paper, we propose a
verification-based approach to ensure the correctness of the existing
functionality in the upgraded version of a PLC software. The method converts
the older and the newer versions of the sequential function chart (SFC) into
two Petri net models. We then verify whether one model is contained within
another, based on a novel containment checking algorithm grounded in symbolic
path equivalence. For this purpose, we have developed a home-grown Petri
net-based containment checker. Experimental evaluation on 80 real-world
benchmarks from the OSCAT library highlights the scalability and effectiveness
of the framework. We have compared our approach with verifAPS, a popular tool
used for software upgradation, and observed nearly 4x performance improvement.

</details>


### [6] [Automating API Documentation with LLMs: A BERTopic Approach](https://arxiv.org/abs/2509.05749)
*AmirHossein Naghshzan*

Main category: cs.SE

TL;DR: This paper demonstrates that automated summarization of Stack Overflow content can complement official Android API documentation, making it more useful and accessible for developers.


<details>
  <summary>Details</summary>
Motivation: Official API documentation is often too lengthy, complex, or lacks practical examples, prompting developers to seek help from community sources like Stack Overflow.

Method: The authors used BERTopic to extract prevalent topics from 3.6 million Stack Overflow posts about Android APIs. They then applied extractive summarization to these posts, creating concise summaries with code snippets. A user study with 30 Android developers evaluated the generated summaries.

Result: The generated summaries were found to be coherent, relevant, informative, and satisfactory in the user study. Developers reported improved productivity using these summaries.

Conclusion: Combining formal API documentation with summaries generated from community-driven content makes API resources more accessible and actionable, enhancing developer productivity and satisfaction.

Abstract: Developers rely on API documentation, but official sources are often lengthy,
complex, or incomplete. Many turn to community-driven forums like Stack
Overflow for practical insights. We propose automating the summarization of
informal sources, focusing on Android APIs. Using BERTopic, we extracted
prevalent topics from 3.6 million Stack Overflow posts and applied extractive
summarization techniques to generate concise summaries, including code
snippets. A user study with 30 Android developers assessed the summaries for
coherence, relevance, informativeness, and satisfaction, showing improved
productivity. Integrating formal API knowledge with community-generated content
enhances documentation, making API resources more accessible and actionable
work.

</details>


### [7] [IoT Miner: Intelligent Extraction of Event Logs from Sensor Data for Process Mining](https://arxiv.org/abs/2509.05769)
*Edyta Brzychczy,Urszula Jessen,Krzysztof Kluza,Sridhar Sriram,Manuel Vargas Nettelnstroth*

Main category: cs.SE

TL;DR: IoT Miner leverages AI to automatically transform raw industrial sensor data into usable event logs for process mining, using LLM-based labeling and a four-stage pipeline, shown to work well with mining equipment data.


<details>
  <summary>Details</summary>
Motivation: In many industrial environments, event logs crucial for process mining are absent, and raw sensor data lacks structure and semantics, making analysis difficult.

Method: IoT Miner is introduced as a four-stage framework: (1) data preprocessing, (2) unsupervised clustering of sensor data, (3) activity labeling using large language models (LLMs) with domain-specific prompts, and (4) event log construction.

Result: Evaluation on mining machine sensor data showed that richer, domain-specific prompts improve label accuracy and consistency. A new metric, Similarity-Weighted Accuracy, is proposed to measure labeling quality.

Conclusion: IoT Miner successfully generates interpretable, structured event logs from raw sensor data, using AI techniques to fill the absence of standard logs. This enables process mining in previously inaccessible industrial settings.

Abstract: This paper presents IoT Miner, a novel framework for automatically creating
high-level event logs from raw industrial sensor data to support process
mining. In many real-world settings, such as mining or manufacturing, standard
event logs are unavailable, and sensor data lacks the structure and semantics
needed for analysis. IoT Miner addresses this gap using a four-stage pipeline:
data preprocessing, unsupervised clustering, large language model (LLM)-based
labeling, and event log construction. A key innovation is the use of LLMs to
generate meaningful activity labels from cluster statistics, guided by
domain-specific prompts. We evaluate the approach on sensor data from a
Load-Haul-Dump (LHD) mining machine and introduce a new metric,
Similarity-Weighted Accuracy, to assess labeling quality. Results show that
richer prompts lead to more accurate and consistent labels. By combining AI
with domain-aware data processing, IoT Miner offers a scalable and
interpretable method for generating event logs from IoT data, enabling process
mining in settings where traditional logs are missing.

</details>


### [8] [GeoAnalystBench: A GeoAI benchmark for assessing large language models for spatial analysis workflow and code generation](https://arxiv.org/abs/2509.05881)
*Qianheng Zhang,Song Gao,Chen Wei,Yibo Zhao,Ying Nie,Ziru Chen,Shijie Chen,Yu Su,Huan Sun*

Main category: cs.SE

TL;DR: GeoAnalystBench, a new benchmark for evaluating LLMs on GIS automation tasks, shows proprietary models outperform open source ones, but spatial reasoning tasks remain hard for all. The study provides a framework for future GeoAI research with human-in-the-loop support.


<details>
  <summary>Details</summary>
Motivation: Recent advances in LLMs suggest great potential for automating geospatial analysis and GIS workflows, but there is uncertainty about their actual abilities. Rigorous evaluation is needed before claims about full GIS automation can be made.

Method: Researchers developed GeoAnalystBench, a benchmark of 50 Python-based geoprocessing tasks from real-world GIS problems, validated by GIS experts. Each task specifies deliverable products and is evaluated for workflow validity, structural alignment, semantic similarity, and code quality (CodeBLEU). Both proprietary and open source LLMs were assessed using this benchmark.

Result: Proprietary models like ChatGPT-4o-mini showed high workflow validity (95%) and stronger code quality (CodeBLEU 0.39), while smaller open source models like DeepSeek-R1-7B performed less well (48.5% validity, 0.272 CodeBLEU). Tasks involving deep spatial reasoning remain challenging for all models.

Conclusion: Current LLMs show potential but also significant limitations in GIS automation. The benchmark created offers a reproducible way to further GeoAI research with human guidance.

Abstract: Recent advances in large language models (LLMs) have fueled growing interest
in automating geospatial analysis and GIS workflows, yet their actual
capabilities remain uncertain. In this work, we call for rigorous evaluation of
LLMs on well-defined geoprocessing tasks before making claims about full GIS
automation. To this end, we present GeoAnalystBench, a benchmark of 50
Python-based tasks derived from real-world geospatial problems and carefully
validated by GIS experts. Each task is paired with a minimum deliverable
product, and evaluation covers workflow validity, structural alignment,
semantic similarity, and code quality (CodeBLEU). Using this benchmark, we
assess both proprietary and open source models. Results reveal a clear gap:
proprietary models such as ChatGPT-4o-mini achieve high validity 95% and
stronger code alignment (CodeBLEU 0.39), while smaller open source models like
DeepSeek-R1-7B often generate incomplete or inconsistent workflows (48.5%
validity, 0.272 CodeBLEU). Tasks requiring deeper spatial reasoning, such as
spatial relationship detection or optimal site selection, remain the most
challenging across all models. These findings demonstrate both the promise and
limitations of current LLMs in GIS automation and provide a reproducible
framework to advance GeoAI research with human-in-the-loop support.

</details>


### [9] [Code2MCP: A Multi-Agent Framework for Automated Transformation of Code Repositories into Model Context Protocol Services](https://arxiv.org/abs/2509.05941)
*Chaoqian Ouyang,Ling Yue,Shimin Di,Libin Zheng,Shaowu Pan,Min-Ling Zhang*

Main category: cs.SE

TL;DR: Paper presents Code2MCP, an automated framework using LLMs to convert any GitHub repo into an MCP-compliant service, handling everything from code analysis to deployment with minimal human input. The system includes self-debugging and documentation generation, directly tackling the $N\times M$ integration challenge in AI agent ecosystems and promoting widespread MCP adoption. Code is open-sourced for broader impact.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) face a major bottleneck in the AI agent ecosystem due to the integration challenge known as the "$N \times M$ problem": every model needs custom integration for each tool, causing fragmentation, stifling innovation, and requiring significant development resources. While the Model Context Protocol (MCP) could resolve this, its adoption is limited because converting existing software to MCP-compliant services is labor-intensive, especially across millions of GitHub repositories.

Method: The authors propose Code2MCP, an agentic, highly automated framework that transforms any GitHub repository into an MCP-compliant service with minimal human input. The system uses a multi-stage workflow that automates code analysis, environment setup, service generation, and deployment. It introduces an LLM-driven, closed-loop Run--Review--Fix cycle for autonomous debugging and code repair, and also produces comprehensive documentation.

Result: Code2MCP successfully enables automated, scalable conversion of GitHub repositories into MCP services, including deployable code and technical documentation. The framework demonstrates significant reduction in manual overhead for integration, accelerating MCP ecosystem growth by unlocking open-source code repositories and automating tool integration. The open-source code is made available for public use and further development.

Conclusion: The Code2MCP framework substantially reduces the integration barrier for LLMs in the AI agent ecosystem by automating the transition of open-source projects to MCP-compliant services, thereby addressing the $N \times M$ problem and accelerating standard protocol adoption. The approach's automation and open-sourcing have the potential to revolutionize tool integration and ecosystem growth.

Abstract: The proliferation of Large Language Models (LLMs) has created a significant
integration challenge in the AI agent ecosystem, often called the "$N \times M$
problem," where N models require custom integrations for M tools. This
fragmentation stifles innovation and creates substantial development overhead.
While the Model Context Protocol (MCP) has emerged as a standard to resolve
this, its adoption is hindered by the manual effort required to convert the
vast universe of existing software into MCP-compliant services. This is
especially true for the millions of open-source repositories on GitHub, the
world's largest collection of functional code. This paper introduces Code2MCP,
a highly automated, agentic framework designed to transform any GitHub
repository into a functional MCP service with minimal human intervention. Our
system employs a multi-stage workflow that automates the entire process, from
code analysis and environment configuration to service generation and
deployment. A key innovation of our framework is an LLM-driven, closed-loop
"Run--Review--Fix" cycle, which enables the system to autonomously debug and
repair the code it generates. Code2MCP produces not only deployable services
but also comprehensive technical documentation, acting as a catalyst to
accelerate the MCP ecosystem by systematically unlocking the world's largest
open-source code repository and automating the critical last mile of tool
integration. The code is open-sourced at
https://github.com/DEFENSE-SEU/MCP-Github-Agent.

</details>


### [10] [GRACE: Graph-Guided Repository-Aware Code Completion through Hierarchical Code Fusion](https://arxiv.org/abs/2509.05980)
*Xingliang Wang,Baoyi Wang,Chen Zhi,Junxiao Han,Xinkui Zhao,Jianwei Yin,Shuiguang Deng*

Main category: cs.SE

TL;DR: GRACE innovates repository-level code completion for LLMs by combining multi-level code graphs and hybrid graph retrieval, outperforming prior methods and addressing context and structure limitations in previous RAG approaches.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based code completion tools perform poorly at repository level due to limited context and complex codebase dependencies. RAG partially helps, but state-of-the-art methods ignore structural relationships and context fusion.

Method: GRACE constructs unified code graphs combining file structure, ASTs, call graphs, class hierarchies, and data flow. It uses a Hybrid Graph Retriever (GNN and textual retriever), a graph attention re-ranker, and a structural fusion mechanism to merge retrieved subgraphs with local code, preserving key dependencies.

Result: GRACE, using DeepSeek-V3, surpasses state-of-the-art graph-RAG baselines by 8.19% EM and 7.51% ES on every public benchmark dataset tested, indicating robust improvement in repository-level code completion and understanding.

Conclusion: GRACE—by integrating multi-level code graphs and hybrid graph retrieval—overcomes the limitations of textual-similarity-based retrieval and naive snippet concatenation, achieving significant performance gains over existing methods.

Abstract: LLMs excel in localized code completion but struggle with repository-level
tasks due to limited context windows and complex semantic and structural
dependencies across codebases. While Retrieval-Augmented Generation (RAG)
mitigates context scarcity by retrieving relevant code snippets, current
approaches face significant limitations. They overly rely on textual similarity
for retrieval, neglecting structural relationships such as call chains and
inheritance hierarchies, and lose critical structural information by naively
concatenating retrieved snippets into text sequences for LLM input. To address
these shortcomings, GRACE constructs a multi-level, multi-semantic code graph
that unifies file structures, abstract syntax trees, function call graphs,
class hierarchies, and data flow graphs to capture both static and dynamic code
semantics. For retrieval, GRACE employs a Hybrid Graph Retriever that
integrates graph neural network-based structural similarity with textual
retrieval, refined by a graph attention network-based re-ranker to prioritize
topologically relevant subgraphs. To enhance context, GRACE introduces a
structural fusion mechanism that merges retrieved subgraphs with the local code
context and preserves essential dependencies like function calls and
inheritance. Extensive experiments on public repository-level benchmarks
demonstrate that GRACE significantly outperforms state-of-the-art methods
across all metrics. Using DeepSeek-V3 as the backbone LLM, GRACE surpasses the
strongest graph-based RAG baselines by 8.19% EM and 7.51% ES points on every
dataset. The code is available at
https://anonymous.4open.science/r/grace_icse-C3D5.

</details>


### [11] [Students' Perception of LLM Use in Requirements Engineering Education: An Empirical Study Across Two Universities](https://arxiv.org/abs/2509.05995)
*Sharon Guardado,Risha Parveen,Zheying Zhang,Maruf Rayhan,Nirnaya Tripathi*

Main category: cs.SE

TL;DR: Integrating LLMs into RE education boosts student comprehension but raises concerns about academic integrity and overreliance. Context and assignment type matter; more research is needed to balance AI's benefits with critical and collaborative learning.


<details>
  <summary>Details</summary>
Motivation: Educators aim to improve student engagement and the practical value of Requirements Engineering (RE) courses by integrating Large Language Models (LLMs) into the curriculum. As AI tools become prevalent in professional practices, there is a need to assess both benefits and challenges in educational settings, guiding effective adoption of LLMs in RE education.

Method: The study empirically evaluated the integration of LLMs in RE coursework by collecting survey data from 179 students across two RE courses at different universities. The research compared individual assignments with team-based Agile projects to analyze experiences and perceptions of LLM use.

Result: LLM integration improved students' understanding of RE concepts, especially in requirements elicitation and documentation. However, concerns emerged about academic integrity, overreliance on AI, and difficulties incorporating AI-generated content. Individual assignments yielded greater perceived benefits than team-based work.

Conclusion: LLMs can enhance student comprehension in RE education, but careful, context-aware integration is vital. Attention must be paid to academic integrity and fostering critical thinking. Recommendations are offered for future course design, and further research is needed on balancing AI support with collaborative and critical learning practices.

Abstract: The integration of Large Language Models (LLMs) in Requirements Engineering
(RE) education is reshaping pedagogical approaches, seeking to enhance student
engagement and motivation while providing practical tools to support their
professional future. This study empirically evaluates the impact of integrating
LLMs in RE coursework. We examined how the guided use of LLMs influenced
students' learning experiences, and what benefits and challenges they perceived
in using LLMs in RE practices. The study collected survey data from 179
students across two RE courses in two universities. LLMs were integrated into
coursework through different instructional formats, i.e., individual
assignments versus a team-based Agile project. Our findings indicate that LLMs
improved students' comprehension of RE concepts, particularly in tasks like
requirements elicitation and documentation. However, students raised concerns
about LLMs in education, including academic integrity, overreliance on AI, and
challenges in integrating AI-generated content into assignments. Students who
worked on individual assignments perceived that they benefited more than those
who worked on team-based assignments, highlighting the importance of contextual
AI integration. This study offers recommendations for the effective integration
of LLMs in RE education. It proposes future research directions for balancing
AI-assisted learning with critical thinking and collaborative practices in RE
courses.

</details>


### [12] [A Rapid Review Regarding the Concept of Legal Requirements in Requirements Engineering](https://arxiv.org/abs/2509.06012)
*Jukka Ruohonen*

Main category: cs.SE

TL;DR: The paper reviews literature on legal requirements in requirements engineering, finding a lack of clear definitions, conceptual confusion, and little empirical evidence, arguing that the field needs better clarity and understanding of LRs.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper is to address the confusion and conceptual misunderstanding surrounding legal requirements (LRs) in requirements engineering (RE) research, stemming from personal observation, peer review feedback, and literature review.

Method: The paper conducts a rapid literature review on the treatment and conception of legal requirements in RE research.

Result: The review finds that normative understandings of LRs predominate but lack proper definitions and clear operationalization. LRs are inconsistently classified as functional or non-functional, often described as vague, complex, and difficult to handle. The literature suggests these problems are linked to knowledge gaps among RE practitioners, and that LRs are frequently reluctantly implemented and poorly prioritized. There is an evident lack of empirical evidence and ongoing conceptual confusion in the field.

Conclusion: There is significant conceptual confusion and lack of empirical grounding regarding legal requirements in RE research. The understanding and implementation of LRs are inconsistent, and there is a pressing need for clearer definitions, empirical studies, and improved conceptual clarity.

Abstract: Out of a personal puzzlement, recent peer review comments, and demonstrable
confusion in the existing literature, the paper presents a rapid review of the
concept of legal requirements (LRs) in requirements engineering (RE) research.
According to reviewing results, a normative understanding of LRs has often been
present, although proper definitions and conceptual operationalizations are
lacking. Some papers also see LRs as functional and others as non-functional
requirements. Legal requirements are often characterized as being vague and
complex, requiring a lot of effort to elicit, implement, and validate. These
characterizations supposedly correlate with knowledge gaps among requirements
engineers. LRs are also seen to often change and overlap. They may be also
prioritized. According to the literature, they seem to be also reluctantly
implemented, often providing only a minimal baseline for other requirements.
With these and other observations, the review raises critical arguments about
apparent knowledge gaps, including a lack of empirical evidence backing the
observations and enduring conceptual confusion.

</details>


### [13] [Empirical Study of Code Large Language Models for Binary Security Patch Detection](https://arxiv.org/abs/2509.06052)
*Qingyuan Li,Binchang Li,Cuiyun Gao,Shuzheng Gao,Zongjie Li*

Main category: cs.SE

TL;DR: Existing learning-based SPD methods don't work for closed-source, binary-only software. Code LLMs can't detect security patches in binaries without fine-tuning. The authors created a large binary patch dataset and showed that fine-tuned LLMs—especially with pseudo-code—excel at binary SPD, closing a critical security gap.


<details>
  <summary>Details</summary>
Motivation: Many existing learning-based approaches for security patch detection (SPD) work only on source code, limiting their applicability to open-source software. However, much of the real-world software is closed-source and ships only binary patches, leaving vulnerabilities unaddressed in such cases. There is a research gap in using code large language models (LLMs) for detecting security patches in binary files, despite their demonstrated capabilities in related tasks.

Method: The authors constructed a large-scale binary patch dataset (19,448 samples) with assembly code and pseudo-code representations. They systematically evaluated 19 different code LLMs for binary SPD using both direct prompting and fine-tuning methods. They analyzed model performance with both representations and tested state-of-the-art prompting techniques and fine-tuning strategies to inject SPD-specific domain knowledge into the models.

Result: Directly prompting vanilla code LLMs—even with advanced prompting techniques—was ineffective for binary SPD due to insufficient domain knowledge. Fine-tuning code LLMs with domain-specific data (on both assembly and pseudo-code) substantially improved their performance, with the best results on pseudo-code representations.

Conclusion: While code LLMs show promise for binary security patch detection, vanilla models lack the required domain knowledge. Fine-tuning with binary patch data, especially in pseudo-code format, enables code LLMs to achieve excellent performance, bridging a key security gap for closed-source and binary-only software.

Abstract: Security patch detection (SPD) is crucial for maintaining software security,
as unpatched vulnerabilities can lead to severe security risks. In recent
years, numerous learning-based SPD approaches have demonstrated promising
results on source code. However, these approaches typically cannot be applied
to closed-source applications and proprietary systems that constitute a
significant portion of real-world software, as they release patches only with
binary files, and the source code is inaccessible. Given the impressive
performance of code large language models (LLMs) in code intelligence and
binary analysis tasks such as decompilation and compilation optimization, their
potential for detecting binary security patches remains unexplored, exposing a
significant research gap between their demonstrated low-level code
understanding capabilities and this critical security task. To address this
gap, we construct a large-scale binary patch dataset containing \textbf{19,448}
samples, with two levels of representation: assembly code and pseudo-code, and
systematically evaluate \textbf{19} code LLMs of varying scales to investigate
their capability in binary SPD tasks. Our initial exploration demonstrates that
directly prompting vanilla code LLMs struggles to accurately identify security
patches from binary patches, and even state-of-the-art prompting techniques
fail to mitigate the lack of domain knowledge in binary SPD within vanilla
models. Drawing on the initial findings, we further investigate the fine-tuning
strategy for injecting binary SPD domain knowledge into code LLMs through two
levels of representation. Experimental results demonstrate that fine-tuned LLMs
achieve outstanding performance, with the best results obtained on the
pseudo-code representation.

</details>


### [14] [Software Dependencies 2.0: An Empirical Study of Reuse and Integration of Pre-Trained Models in Open-Source Projects](https://arxiv.org/abs/2509.06085)
*Jerin Yasmin,Wenxin Jiang,James C. Davis,Yuan Tian*

Main category: cs.SE

TL;DR: This paper studies how open-source projects reuse and manage pre-trained models (PTMs) as a new type of software dependency, analyzing patterns and developer practices using a large GitHub dataset. It finds diverse management strategies and highlights unique challenges posed by PTMs compared to traditional dependencies.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the increasing adoption of pre-trained models (PTMs) in software projects, creating a new type of software dependency. These dependencies differ from traditional ones, representing learned behaviors and artifacts, raising concerns about software maintainability and reliability.

Method: The study utilizes a mixed-methods approach, analyzing a random sample of 401 GitHub repositories from the PeaTMOSS dataset. It combines quantitative identification of PTM reuse patterns with qualitative investigation into developers' integration and management of these models.

Result: The research finds various ways OSS projects structure and document PTM dependencies, revealing distinct stages and organizational patterns in PTM reuse pipelines. It also uncovers complex interactions between PTMs and other learned components across pipeline stages.

Conclusion: The integration of PTMs as software dependencies introduces new challenges for maintainability and reliability in modern software systems. Understanding how OSS projects manage these dependencies is crucial for supporting sustainable software development.

Abstract: Pre-trained models (PTMs) are machine learning models that have been trained
in advance, often on large-scale data, and can be reused for new tasks, thereby
reducing the need for costly training from scratch. Their widespread adoption
introduces a new class of software dependency, which we term Software
Dependencies 2.0, extending beyond conventional libraries to learned behaviors
embodied in trained models and their associated artifacts. The integration of
PTMs as software dependencies in real projects remains unclear, potentially
threatening maintainability and reliability of modern software systems that
increasingly rely on them. Objective: In this study, we investigate Software
Dependencies 2.0 in open-source software (OSS) projects by examining the reuse
of PTMs, with a focus on how developers manage and integrate these models.
Specifically, we seek to understand: (1) how OSS projects structure and
document their PTM dependencies; (2) what stages and organizational patterns
emerge in the reuse pipelines of PTMs within these projects; and (3) the
interactions among PTMs and other learned components across pipeline stages. We
conduct a mixed-methods analysis of a statistically significant random sample
of 401 GitHub repositories from the PeaTMOSS dataset (28,575 repositories
reusing PTMs from Hugging Face and PyTorch Hub). We quantitatively examine PTM
reuse by identifying patterns and qualitatively investigate how developers
integrate and manage these models in practice.

</details>


### [15] [Agentic Software Engineering: Foundational Pillars and a Research Roadmap](https://arxiv.org/abs/2509.06216)
*Ahmed E. Hassan,Hao Li,Dayi Lin,Bram Adams,Tse-Hsun Chen,Yutaro Kashiwa,Dong Qiu*

Main category: cs.SE

TL;DR: This paper introduces the vision of Agentic Software Engineering (SE 3.0), promoting a dual approach of SE for humans and SE for agents. It proposes new frameworks, tools, and processes for structured human-AI collaboration in software engineering, and outlines a research roadmap to guide the community toward a scalable and trustworthy agentic future.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the emerging era of Agentic Software Engineering (SE 3.0), where intelligent agents not only assist with code generation but are also responsible for achieving broader, complex SE objectives. There is a need to rethink the foundational aspects of software engineering to accommodate and leverage these agentic capabilities while ensuring trust and reliability.

Method: The paper proposes a conceptual framework for Agentic Software Engineering, centered around a dual-modality approach: SE for Humans and SE for Agents. It introduces two tailored workbenches (Agent Command Environment and Agent Execution Environment) to facilitate human-agent collaboration and orchestration. The authors present structured processes and vocabularies, culminating in a roadmap highlighting research challenges and future directions.

Result: The paper lays out the vision of Structured Agentic Software Engineering (SASE), establishing new foundational pillars and proposing structured processes that redefine the relationship between humans and intelligent agents in software engineering. It identifies key challenges, outlines the dualities and workbenches, and discusses the potential impacts on SE education and community thinking.

Conclusion: The paper concludes by advocating for a disciplined, scalable, and trustworthy approach to software engineering in the agentic era, encouraging the community to move beyond traditional, human-centric methods. It provides a scaffold for new research and discussion, rather than a prescriptive solution, to catalyze progress toward agentic SE.

Abstract: Agentic Software Engineering (SE 3.0) represents a new era where intelligent
agents are tasked not with simple code generation, but with achieving complex,
goal-oriented SE objectives. To harness these new capabilities while ensuring
trustworthiness, we must recognize a fundamental duality within the SE field in
the Agentic SE era, comprising two symbiotic modalities: SE for Humans and SE
for Agents. This duality demands a radical reimagining of the foundational
pillars of SE (actors, processes, tools, and artifacts) which manifest
differently across each modality. We propose two purpose-built workbenches to
support this vision. The Agent Command Environment (ACE) serves as a command
center where humans orchestrate and mentor agent teams, handling outputs such
as Merge-Readiness Packs (MRPs) and Consultation Request Packs (CRPs). The
Agent Execution Environment (AEE) is a digital workspace where agents perform
tasks while invoking human expertise when facing ambiguity or complex
trade-offs. This bi-directional partnership, which supports agent-initiated
human callbacks and handovers, gives rise to new, structured engineering
activities (i.e., processes) that redefine human-AI collaboration, elevating
the practice from agentic coding to true agentic software engineering. This
paper presents the Structured Agentic Software Engineering (SASE) vision,
outlining several of the foundational pillars for the future of SE. The paper
culminates in a research roadmap that identifies a few key challenges and
opportunities while briefly discussing the resulting impact of this future on
SE education. Our goal is not to offer a definitive solution, but to provide a
conceptual scaffold with structured vocabulary to catalyze a community-wide
dialogue, pushing the SE community to think beyond its classic, human-centric
tenets toward a disciplined, scalable, and trustworthy agentic future.

</details>


### [16] [Learning From Software Failures: A Case Study at a National Space Research Center](https://arxiv.org/abs/2509.06301)
*Dharun Anandayuvaraj,Zain Hammadeh,Andreas Lund,Alexandra Holloway,James C. Davis*

Main category: cs.SE

TL;DR: The paper investigates how engineers in high-reliability organizations learn from software failures. It finds that learning is mostly informal and unstructured, resulting in recurring failures due to lack of systematic processes. Key obstacles include time limits, knowledge loss, and poor process enforcement. The study offers insights and recommendations for better failure management in software engineering.


<details>
  <summary>Details</summary>
Motivation: Software failures can have major consequences, particularly in high-reliability organizations (HROs) where errors can be catastrophic. Although postmortems and learning from failures are recommended, the effectiveness and adoption of these practices are inconsistent across organizations. The study aims to understand how engineers actually gather, document, share, and use lessons from failures, in order to improve software reliability and prevent repeated problems.

Method: The authors conducted a case study involving 10 in-depth interviews with research software engineers at a national space research center. To assess if the findings are broadly applicable, they also included 5 interviews from other HROs. The aim was to observe and analyze real-world practices in learning from software failures.

Result: The study found that failure learning in these organizations tends to be informal, ad hoc, and not systematically integrated into the software development life cycle (SDLC). Recurring failures are common because there is a lack of structured processes. Key challenges undermining systematic learning include time constraints, knowledge loss due to staff turnover and fragmented documentation, and weak enforcement of processes.

Conclusion: The research increases our understanding of how software engineers learn from failures in HROs. It emphasizes that current failure management is inconsistent and informal, leading to persistent recurring failures. The study provides guidance for improving processes and tools that support systematic learning from failures, with the aim of improving reliability and preventing repeated mistakes.

Abstract: Software failures can have significant consequences, making learning from
failures a critical aspect of software engineering. While software
organizations are recommended to conduct postmortems, the effectiveness and
adoption of these practices vary widely. Understanding how engineers gather,
document, share, and apply lessons from failures is essential for improving
reliability and preventing recurrence. High-reliability organizations (HROs)
often develop software systems where failures carry catastrophic risks,
requiring continuous learning to ensure reliability. These organizations
provide a valuable setting to examine practices and challenges for learning
from software failures. Such insight could help develop processes and tools to
improve reliability and prevent recurrence. However, we lack in-depth industry
perspectives on the practices and challenges of learning from failures.
  To address this gap, we conducted a case study through 10 in-depth interviews
with research software engineers at a national space research center. We
examine how they learn from failures: how they gather, document, share, and
apply lessons. To assess transferability, we include data from 5 additional
interviews at other HROs. Our findings provide insight into how engineers learn
from failures in practice. To summarize: (1) failure learning is informal, ad
hoc, and inconsistently integrated into SDLC; (2) recurring failures persist
due to absence of structured processes; and (3) key challenges, including time
constraints, knowledge loss from turnover and fragmented documentation, and
weak process enforcement, undermine systematic learning. Our findings deepen
understanding of how software engineers learn from failures and offer guidance
for improving failure management practices.

</details>


### [17] [A Generic and Efficient Python Runtime Verification System and its Large-scale Evaluation](https://arxiv.org/abs/2509.06324)
*Zhuohang Shen,Mohammed Yaseen,Denini Silva,Kevin Guan,Junho Lee,Marcelo d'Amorim,Owolabi Legunsen*

Main category: cs.SE

TL;DR: PyMOP introduces a fast, extensible runtime verification solution for Python, outperforming existing tools and helping developers fix numerous bugs across many projects.


<details>
  <summary>Details</summary>
Motivation: Current runtime verification (RV) systems for Python are either limited to specific domains, specification logic, or have performance issues, unlike the scalable and effective RV solutions available for Java.

Method: The authors propose PyMOP—a generic, extensible, and efficient RV system for Python. PyMOP supports five specification logics, implements five monitoring algorithms, ships with 73 API specifications, supports three instrumentation strategies, and enables easy extensibility.

Result: On 290,133 unit tests in 1,463 GitHub projects, PyMOP demonstrated its generality and efficiency, finding that the default Java algorithm is not the fastest for Python, PyMOP is up to 1,168.3x faster than recent dynamic analysis systems, and it contributed to 44 of 121 detected bugs being fixed by developers.

Conclusion: PyMOP is a highly general and efficient platform for runtime verification in Python, making it well-suited for future advances in the field.

Abstract: Runtime verification (RV) now scales for testing thousands of open-source
Java projects, helping find hundreds of bugs. The popular Python ecosystem
could use such benefits. But, today's Python RV systems are limited to a domain
or specification logic, or slow. We propose PyMOP, a generic, extensible, and
efficient RV system for Python. PyMOP supports five logics, implements five
existing monitoring algorithms, ships with 73 API specs of Python and
widely-used libraries, supports three instrumentation strategies, and users can
easily add more of these. On 290,133 unit tests in 1,463 GitHub projects, we
find mainly that (i) the default monitoring algorithm for Java is often not the
fastest for Python; (ii) PyMOP is up to 1,168.3x faster than two recent dynamic
analysis systems; and (iii) 44 of 121 bugs that PyMOP helped find so far were
fixed by developers. PyMOP's generality and efficiency position it well as an
excellent platform for the next advances on RV for Python.

</details>


### [18] [Analyzing the Instability of Large Language Models in Automated Bug Injection and Correction](https://arxiv.org/abs/2509.06429)
*Mehmet Bilal Er,Nagehan İlhan,Umut Kuran*

Main category: cs.SE

TL;DR: Higher temperature settings in LLMs like ChatGPT lead to more unstable and inconsistent bug fix outputs, raising concerns about their reliability for software engineering. Low temperature settings provide more consistent results, but overall dependability remains an issue for automatic bug fixing tasks.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are increasingly used in software engineering for bug fixing and code generation. However, their outputs can be unstable, generating different results for the same input. While instability has been studied for code generation, its impact on bug-fixing tasks is less explored. The motivation is to assess the consistency and reliability of LLMs in bug-fixing scenarios.

Method: The study investigates the instability of ChatGPT in bug-fixing by generating multiple fix suggestions for 20 code problems at different temperature settings (0, 0.5, and 1). For each issue, the model outputs three suggestions per temperature, resulting in nine outputs per problem. The structural, syntactic, and functional differences between these outputs are analyzed using the Syntax Similarity and Output Equivalence Rate (OER) metrics.

Result: At higher temperature settings, the model's outputs become significantly more unstable and more likely to fail functionally. High temperatures yield greater structural differences between suggestions, according to syntax similarity metrics, while low temperatures produce more similar outputs. Functional failures are especially pronounced at high temperatures.

Conclusion: LLMs like ChatGPT display considerable variability in bug fix recommendations as temperature increases. Their reliability and consistency are questionable at higher temperatures. The findings reveal important insights for developing more dependable LLM-based error correction systems for software engineering.

Abstract: The use of Large Language Models (LLMs) in software engineering tasks is
growing, especially in the areas of bug fixing and code generation.
Nevertheless, these models often yield unstable results; when executed at
different times with the same input, they can generate radically different
code. The consistency of LLMs in bug-fixing tasks has not yet been thoroughly
assessed, despite the fact that this instability has typically been discussed
in the literature in relation to code generation. The purpose of this study is
to look into how unstable an LLM like ChatGPT is when it comes to fixing code
bugs. We examine the structural, syntactic, and functional variations among
several fix recommendations made in response to the same prompt using code
samples with various error types. Additionally, we assess how instability is
affected by the temperature settings (0, 0.5, and 1) used for the model's
deterministic operation. For a total of 20 problems in the experimental
analysis, the model produced three fix suggestions at each temperature value,
comparing nine distinct outputs for each problem. The Syntax Similarity and
Output Equivalence Rate (OER) metrics were used to assess the outputs'
structural and functional consistency. The results demonstrate that the model's
outputs become much more unstable and variable as the temperature rises, with
high temperatures showing especially high rates of functional failure.
According to syntax similarity analyses, the suggested fixes show notable
structural differences at high temperatures but are fairly similar at low
temperatures. The purpose of this study is to provide important methodological
insights into how LLM-based error correction systems can be applied more
consistently in software development processes while also casting doubt on
their dependability.

</details>


### [19] [Modeling in the Design Multiverse](https://arxiv.org/abs/2509.06530)
*Sylvain Guérin,Salvador Martinez,Ciprian Teodorov*

Main category: cs.SE

TL;DR: The paper proposes the Design Multiverse, a conceptual framework that natively manages design alternatives and their evolution within modeling environments, improving traceability and collaboration while minimizing reliance on external tools.


<details>
  <summary>Details</summary>
Motivation: Current modeling spaces cannot directly manage the variability and evolution of design paths when multiple stakeholders or teams handle complex systems. This leads to reliance on external tools, which fragment the design process.

Method: The paper introduces a conceptual framework called the Design Multiverse, which integrates revisions and design variants within the modeling space. Usage scenarios are discussed, and an implementation is proposed using the model federation paradigm.

Result: The Design Multiverse supports the representation and management of alternative design paths and their interdependencies, allowing stakeholders to seamlessly trace and analyze design decisions. Example scenarios include model product lines and co-evolution of models and metamodels.

Conclusion: Integrating the Design Multiverse into modeling spaces significantly improves the management of design variability, revisions, and interdependencies, reducing dependence on external methodologies and enhancing collaboration among stakeholders.

Abstract: Real-world design processes often involve the evolution and divergence of
design paths (by branching, revising, merging, etc.), especially when multiple
stakeholders or teams operate concurrently and/or explore different
alternatives for complex and heterogeneous systems. Unfortunately, this
variability in time and space can not be directly managed in current modeling
spaces but requires resorting to external tools and methodologies.
  In order to tackle this problem, we introduce the Design Multiverse. The
Design Multiverse aims to integrate in the modeling space a selection of
revisions and variants, representing snapshots of a design state composed of
multiple artifacts. This enables stakeholders to seamlessly trace, analyze, and
manage design decisions, system variants, and their interdependencies.
Concretely, in this paper we present a conceptual definition of the Design
Multiverse, discuss usage scenarios such as model product lines and
model/metamodel co-evolution, and propose an implementation leveraging the
model federation paradigm.

</details>


### [20] [Design and Implementation of a Domain-specific Language for Modelling Evacuation Scenarios Using Eclipse EMG/GMF Tool](https://arxiv.org/abs/2509.06688)
*Heerok Banerjee*

Main category: cs.SE

TL;DR: The paper introduces Bmod, a new domain-specific language for modeling evacuation scenarios, built with Eclipse EMF/GMF, and compares it to alternative tools.


<details>
  <summary>Details</summary>
Motivation: There is a need for modeling frameworks with graphical interfaces and easy implementation to help novice users and address enterprise dependencies. Existing tools lack user-friendliness or sufficient features for business process modeling.

Method: Development of the Bmod DSL using Eclipse Modeling Framework and Eclipse Graphical Modeling Framework, followed by comparative analysis with other modeling tools (AToMPM, metaDepth, Sirius).

Result: Bmod demonstrates effectiveness as a modeling tool for evacuation scenarios and offers improvements in usability and performance compared to other available modeling tools.

Conclusion: Bmod, developed using Eclipse EMF/GMF, provides a graphical and user-friendly framework for modeling evacuation scenarios and shows competitive advantages over other tools in terms of expressiveness, ease of learning, and performance.

Abstract: Domain-specific languages (DSLs) play a crucial role in resolving internal
dependencies across enterprises and boosts their upfront business management
processes. Yet, a lot of development is needed to build modelling frameworks
which support graphical interfaces (canvas, pallettes etc.), hierarchical
structures and easy implementation to shorten the gap for novice users. In this
paper, a DSL namely, Bmod is introduced, which can be used to model evacuation
scenarios. The language is built using Eclipse Modelling Framework (EMF) and
Eclipse Graphical Modelling Framework (GMF). Furthermore, a comparison is also
shown between Eclipse EMF/GMF and other modelling tools such as AToMPM,
metaDepth, Sirius etc with respect to expressiveness, learning curve and
performance.

</details>


### [21] [Efficiently Ranking Software Variants with Minimal Benchmarks](https://arxiv.org/abs/2509.06716)
*Théo Matricon,Mathieu Acher,Helge Spieker,Arnaud Gotlieb*

Main category: cs.SE

TL;DR: The paper proposes BISection Sampling (BISS), a method to cut down computational cost in benchmarking software variants by optimizing test selections. BISS achieves significant cost reduction (up to 99% in many cases) while keeping the rankings of software variants stable, demonstrating its efficiency on diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Benchmarking is valuable for comparing, optimizing, and maintaining software variants, but running extensive benchmarks consumes significant time and computational resources. There is a need for methods that can reduce the benchmarking effort while preserving reliable comparative results.

Method: The paper introduces BISection Sampling (BISS), a test suite optimization approach. BISS selectively retains critical benchmark tests using a divide-and-conquer procedure to minimize the total number of tests executed, attempting to keep the ranking of software variants stable.

Result: Experiments on datasets from LLM leaderboards, SAT competitions, and configurable systems show that BISS can reduce benchmarking computational cost on average to 44%, and in over half of the cases by up to 99%, all without compromising ranking stability.

Conclusion: BISS provides an effective solution to reduce the cost of benchmarking in software engineering by strategically sampling tests, maintaining accurate variant rankings, and outperforming baseline methods in both efficiency and ranking stability.

Abstract: Benchmarking is a common practice in software engineering to assess the
qualities and performance of software variants, coming from multiple competing
systems or from configurations of the same system. Benchmarks are used notably
to compare and understand variant performance, fine-tune software, detect
regressions, or design new software systems. The execution of benchmarks to get
a complete picture of software variants is highly costly in terms of
computational resources and time. In this paper, we propose a novel approach
for reducing benchmarks while maintaining stable rankings, using test suite
optimization techniques. That is, we remove instances from the benchmarks while
trying to keep the same rankings of the variants on all tests. Our method,
BISection Sampling, BISS, strategically retains the most critical tests and
applies a novel divide-and-conquer approach to efficiently sample among
relevant remaining tests. We experiment with datasets and use cases from LLM
leaderboards, SAT competitions, and configurable systems for performance
modeling. Our results show that our method outperforms baselines even when
operating on a subset of variants. Using BISS, we reduce the computational cost
of the benchmarks on average to 44% and on more than half the benchmarks by up
to 99% without loss in ranking stability.

</details>


### [22] [OpenCoderRank: AI-Driven Technical Assessments Made Easy](https://arxiv.org/abs/2509.06774)
*Hridoy Sankar Dutta,Sana Ansari,Swati Kumari,Shounak Ravi Bhalerao*

Main category: cs.SE

TL;DR: OpenCoderRank is an easy-to-use, free platform for hosting and simulating technical coding assessments, benefiting both educators/interviewers and students/interviewees by providing meaningful, customizable evaluation experiences in resource-constrained settings.


<details>
  <summary>Details</summary>
Motivation: In technical assessments for coding and problem-solving, balancing challenge with relevance is essential for fair evaluation. However, the advent of Large Language Models (LLMs) has challenged the assessment integrity by making solutions easily accessible to problem solvers. There is also a need for low-cost, customizable, and easy-to-use platforms that allow both educators/interviewers and students/interviewees to participate in meaningful technical evaluations.

Method: The authors introduce OpenCoderRank, a platform that simulates real technical assessments. It enables problem setters to create, host, and manage challenges, ensuring customizable assessments. It also helps problem solvers practice under realistic time constraints and exposure to unfamiliar tasks. The system is designed to be self-hosted and free, supporting resource-limited environments.

Result: OpenCoderRank provides an accessible, no-cost solution for conducting technical assessments, bridging the gap between problem setters and solvers. It enhances preparation for problem solvers and gives problem setters greater control over assessment integrity and relevance.

Conclusion: OpenCoderRank is a promising tool for both problem setters and solvers in technical coding assessments. It strengthens the integrity and relevance of assessments, offers cost-effective and customizable hosting, and helps prepare solvers for high-pressure assessment situations.

Abstract: Organizations and educational institutions use time-bound assessment tasks to
evaluate coding and problem-solving skills. These assessments measure not only
the correctness of the solutions, but also their efficiency. Problem setters
(educator/interviewer) are responsible for crafting these challenges, carefully
balancing difficulty and relevance to create meaningful evaluation experiences.
Conversely, problem solvers (student/interviewee) apply coding efficiency and
logical thinking to arrive at correct solutions. In the era of Large Language
Models (LLMs), LLMs assist problem setters in generating diverse and
challenging questions, but they can undermine assessment integrity for problem
solvers by providing easy access to solutions. This paper introduces
OpenCoderRank, an easy-to-use platform designed to simulate technical
assessments. It acts as a bridge between problem setters and problem solvers,
helping solvers prepare for time constraints and unfamiliar problems while
allowing setters to self-host assessments, offering a no-cost and customizable
solution for technical assessments in resource-constrained environments.

</details>


### [23] [Hypergraph-Guided Regex Filter Synthesis for Event-Based Anomaly Detection](https://arxiv.org/abs/2509.06911)
*Margarida Ferreira,Victor Nicolet,Luan Pham,Joey Dodds,Daniel Kroening,Ines Lynce,Ruben Martins*

Main category: cs.SE

TL;DR: HyGLAD is an interpretable and efficient anomaly detection algorithm for event data that beats deep-learning baselines on accuracy and resource use, thanks to its use of regular expressions to model entity behavior.


<details>
  <summary>Details</summary>
Motivation: There is a growing need for interpretable and efficient anomaly detection methods in event-based systems, where existing deep-learning approaches often lack transparency and require significant computational resources.

Method: HyGLAD is introduced as an algorithm that learns interpretable patterns through regular expressions, modeling entity behavior in event data. It infers equivalence classes and creates regular expressions that represent behavioral norms, allowing for interpretable anomaly detection.

Result: HyGLAD was tested against seven unsupervised anomaly detection methods from DeepOD across five real-world datasets. It outperformed deep-learning baselines in both precision (1.2x improvement) and recall (1.3x improvement), while being significantly more efficient in training and inference (using only a single CPU).

Conclusion: HyGLAD provides superior accuracy and efficiency compared to deep-learning methods for event-based anomaly detection, while also offering interpretability through regular expressions.

Abstract: We propose HyGLAD, a novel algorithm that automatically builds a set of
interpretable patterns that model event data. These patterns can then be used
to detect event-based anomalies in a stationary system, where any deviation
from past behavior may indicate malicious activity. The algorithm infers
equivalence classes of entities with similar behavior observed from the events,
and then builds regular expressions that capture the values of those entities.
As opposed to deep-learning approaches, the regular expressions are directly
interpretable, which also translates to interpretable anomalies. We evaluate
HyGLAD against all 7 unsupervised anomaly detection methods from DeepOD on five
datasets from real-world systems. The experimental results show that on average
HyGLAD outperforms existing deep-learning methods while being an order of
magnitude more efficient in training and inference (single CPU vs GPU).
Precision improved by 1.2x and recall by 1.3x compared to the second-best
baseline.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [24] [Comparing Methods for the Cross-Level Verification of SystemC Peripherals with Symbolic Execution](https://arxiv.org/abs/2509.05504)
*Karl Aaron Rudkowski,Sallar Ahmadi-Pour,Rolf Drechsler*

Main category: cs.PL

TL;DR: This work introduces two tools, CrosSym and SEFOS, enabling symbolic execution of hardware peripherals in SystemC VPs across different abstraction levels. SEFOS avoids kernel modification, while CrosSym optimizes performance. Both offer powerful, flexible verification exceeding the limitations of current TLM-based methods.


<details>
  <summary>Details</summary>
Motivation: Virtual Prototypes (VPs) are extensively used in hardware design to verify complex systems, typically involving multiple components. However, enabling powerful verification methods like symbolic execution requires modifying the SystemC kernel, which can limit flexibility, especially for targeted subsystem analysis.

Method: The paper proposes two different approaches for symbolic execution of hardware peripherals in SystemC-based VPs: CrosSym, which modifies the SystemC kernel, and SEFOS, which modifies the symbolic execution engine instead. Both tools were evaluated with various peripherals at different abstraction levels.

Result: Both CrosSym and SEFOS offer a wide range of features, demonstrated in multiple verification scenarios and successfully identifying over 300 mutants. SEFOS maintains compatibility with the original SystemC kernel and peripheral, while CrosSym has better runtime and memory efficiency.

Conclusion: SEFOS stands out for not requiring kernel or peripheral modification, enhancing workflow integration, while CrosSym offers improved performance. Compared to the state-of-the-art, both allow cross-level symbolic execution with verification capabilities comparable in runtime to existing TLM approaches.

Abstract: Virtual Prototypes (VPs) are important tools in modern hardware development.
At high abstractions, they are often implemented in SystemC and offer early
analysis of increasingly complex designs. These complex designs often combine
one or more processors, interconnects, and peripherals to perform tasks in
hardware or interact with the environment. Verifying these subsystems is a
well-suited task for VPs, as they allow reasoning across different abstraction
levels. While modern verification techniques like symbolic execution can be
seamlessly integrated into VP-based workflows, they require modifications in
the SystemC kernel. Hence, existing approaches therefore modify and replace the
SystemC kernel, or ignore the opportunity of cross-level scenarios completely,
and would not allow focusing on special challenges of particular subsystems
like peripherals. We propose CrosSym and SEFOS, two opposing approaches for a
versatile symbolic execution of peripherals. CrosSym modifies the SystemC
kernel, while SEFOS instead modifies a modern symbolic execution engine. Our
extensive evaluation applies our tools to various peripherals on different
levels of abstractions. Both tools extensive sets of features are demonstrated
for (1) different verification scenarios, and (2) identifying 300+ mutants. In
comparison with each other, SEFOS convinces with the unmodified SystemC kernel
and peripheral, while CrosSym offers slightly better runtime and memory usage.
In comparison to the state-of-the-art, that is limited to Transaction Level
Modelling (TLM), our tools offered comparable runtime, while enabling
cross-level verification with symbolic execution.

</details>


### [25] [Fixed Parameter Tractable Linearizability Monitoring for Stack, Queue and Anagram Agnostic Data Types](https://arxiv.org/abs/2509.05586)
*Lee Zheng Han,Umang Mathur*

Main category: cs.PL

TL;DR: The paper introduces efficient, fixed-parameter tractable algorithms for verifying linearizability in concurrent stacks, queues, and anagram-agnostic types, making verification feasible under bounded concurrency via state-space reduction and specialized techniques for each data structure.


<details>
  <summary>Details</summary>
Motivation: Linearizability verification for concurrent data structures is computationally difficult (NP-hard), even for basic structures like stacks and queues. Efficient and practical verification is crucial to ensure correctness of such systems in real-world concurrent computing.

Method: The authors introduce fixed-parameter tractable algorithms for monitoring linearizability in stacks, queues, and anagram-agnostic data types (AADTs), with the parameter being the maximum concurrency. The approach exploits frontier graphs and partition states to reduce the verification search space. For stacks, a grammar-based method is used with a sub-cubic reduction to matrix multiplication. For queues, a split-sequence transition system enables efficient dynamic programming. For AADTs, the algorithm uses the equivalence of linearizations to achieve log-linear monitoring time.

Result: The proposed algorithms provide tractable linearizability monitoring for multiple concurrent data structures under bounded concurrency. Specifically, they demonstrate log-linear monitoring for AADTs, improved stack monitoring via grammar and matrix multiplication reduction, and efficient queue monitoring via dynamic programming.

Conclusion: By parameterizing by maximum concurrency and specializing algorithms to data type characteristics, the authors achieve fixed-parameter tractable verification for a range of concurrent data structures, improving the practicality of correctness verification in concurrent computing.

Abstract: Verifying linearizability of concurrent data structures is NP-hard, even for
simple types. We present fixed-parameter tractable algorithms for monitoring
stacks, queues, and anagram-agnostic data types (AADTs), parameterized by the
maximum concurrency. Our approach leverages frontier graphs and partition
states to bound the search space. For AADTs, equivalence of linearizations
enables monitoring in log-linear time. For stacks, we introduce a grammar-based
method with a sub-cubic reduction to matrix multiplication, and for queues, a
split-sequence transition system supporting efficient dynamic programming.
These results unify tractability guarantees for both order-sensitive and
anagram-agnostic data types under bounded concurrency.

</details>


### [26] [Pacing Types: Safe Monitoring of Asynchronous Streams](https://arxiv.org/abs/2509.06724)
*Florian Kohn,Arthur Correnson,Jan Baumeister,Bernd Finkbeiner*

Main category: cs.PL

TL;DR: This paper introduces 'pacing types,' a new type system for stream-based monitoring in safety-critical systems, ensuring reliable handling of asynchronous data streams in RTLola. The method is formalized and proven sound, addressing the risk of runtime errors from complex synchronization policies.


<details>
  <summary>Details</summary>
Motivation: Stream-based monitoring is crucial for real-time safety in cyber-physical systems, such as UAVs, but is challenged by asynchronous sensor data and potential runtime errors. Current frameworks simplify design via expressive synchronization policies, which can introduce subtle errors.

Method: The paper presents 'pacing types,' a novel type system for RTLola, and formalizes their essence for a core fragment of RTLola. It establishes a soundness proof through a new logical relation.

Result: Pacing types were implemented in RTLola, ensuring well-behaved monitors for asynchronous streams at runtime. The soundness of pacing types was formally proven.

Conclusion: The proposed pacing type system effectively ensures the correctness and reliability of stream-based monitors handling asynchronous data, mitigating the introduction of runtime errors from modern specification languages.

Abstract: Stream-based monitoring is a real-time safety assurance mechanism for complex
cyber-physical systems such as unmanned aerial vehicles. In this context, a
monitor aggregates streams of input data from sensors and other sources to give
real-time statistics and assessments of the system's health. Since monitors are
safety-critical components, it is crucial to ensure that they are free of
potential runtime errors. One of the central challenges in designing reliable
stream-based monitors is to deal with the asynchronous nature of data streams:
in concrete applications, the different sensors being monitored produce values
at different speeds, and it is the monitor's responsibility to correctly react
to the asynchronous arrival of different streams of values. To ease this
process, modern frameworks for stream-based monitoring such as RTLola feature
an expressive specification language that allows to finely specify data
synchronization policies. While this feature dramatically simplifies the design
of monitors, it can also lead to subtle runtime errors. To mitigate this issue,
this paper presents pacing types, a novel type system implemented in RTLola to
ensure that monitors for asynchronous streams are well-behaved at runtime. We
formalize the essence of pacing types for a core fragment of RTLola, and
present a soundness proof of the pacing type system using a new logical
relation.

</details>


### [27] [Termination Analysis of Linear-Constraint Programs](https://arxiv.org/abs/2509.06752)
*Amir M. Ben-Amram,Samir Genaim,Joël Ouaknine,James Worrell*

Main category: cs.PL

TL;DR: This survey reviews methods for analyzing termination of programs with linear numerics, comparing their strengths, weaknesses, and computational trade-offs, but does not cover practical language implementations or more complex models.


<details>
  <summary>Details</summary>
Motivation: Termination analysis for programs with numerical variables and linear constraints is inherently challenging due to undecidability issues. There is a need to systematically understand and compare the methods that address these challenges.

Method: The paper surveys various techniques, including foundational decidability results, ranking functions, and disjunctive well-founded transition invariants. It also discusses non-termination witnesses and analyzes the algorithmic and complexity aspects of these approaches.

Result: The survey presents different methods for termination analysis, highlighting the trade-offs between their expressive power and computational complexity. It thoroughly examines how each method mitigates the difficulties arising from undecidability in this domain.

Conclusion: The paper concludes that while termination analysis for such constrained programs remains difficult, existing methods offer varying balances between power and complexity. However, the survey specifically does not address implementation in real-world languages or models with advanced features like non-linear arithmetic or probabilistic choice.

Abstract: This Survey provides an overview of techniques in termination analysis for
programs with numerical variables and transitions defined by linear
constraints. This subarea of program analysis is challenging due to the
existence of undecidable problems, and this Survey systematically explores
approaches that mitigate this inherent difficulty. These include foundational
decidability results, the use of ranking functions, and disjunctive
well-founded transition invariants. The Survey also discusses non-termination
witnesses, used to prove that a program will not halt. We examine the
algorithmic and complexity aspects of these methods, showing how different
approaches offer a trade-off between expressive power and computational
complexity. The Survey does not discuss how termination analysis is performed
on real-world programming languages, nor does it consider more expressive
abstract models that include non-linear arithmetic, probabilistic choice, or
term rewriting systems.

</details>


### [28] [Dato: A Task-Based Programming Model for Dataflow Accelerators](https://arxiv.org/abs/2509.06794)
*Shihan Fang,Hongzheng Chen,Niansong Zhang,Jiajie Li,Han Meng,Adrian Liu,Zhiru Zhang*

Main category: cs.PL

TL;DR: Dato provides a new Python-based programming model for dataflow accelerators, making explicit dataflow and sharding easy for developers. It achieves high performance and efficiency on commercial AI hardware, outpacing existing solutions and reducing development effort.


<details>
  <summary>Details</summary>
Motivation: Current deep learning workloads are bottlenecked by memory and data movement, with hardware solutions underutilized due to programming model limitations. There is a need for an expressive, efficient interface balancing control and abstraction.

Method: Dato introduces a Python-embedded, task-based programming model where developers explicitly define data communication and sharding via specialized types. The compiler maps these tasks to hardware, generating optimized physical mappings.

Result: Experiments on AMD Ryzen AI NPU and Alveo FPGA show Dato achieves up to 84% hardware utilization and 2.81x speedup over commercial frameworks for key kernels, and attains 98% peak performance with custom systolic arrays.

Conclusion: Dato significantly streamlines programming for dataflow accelerators, offering both high performance and a reduced development burden compared to existing frameworks.

Abstract: Recent deep learning workloads increasingly push computational demand beyond
what current memory systems can sustain, with many kernels stalling on data
movement rather than computation. While modern dataflow accelerators
incorporate on-chip streaming to mitigate off-chip bandwidth limitations,
existing programming models struggle to harness these capabilities effectively.
Low-level interfaces provide fine-grained control but impose significant
development overhead, whereas high-level tile-based languages abstract away
communication details, restricting optimization and forcing compilers to
reconstruct the intended dataflow. We present Dato, a Python-embedded,
task-based programming model for dataflow accelerators that elevates data
communication and sharding to first-class type constructs. Developers write
programs as a graph of tasks connected via explicit stream types, with sharded
inputs specified using layout types. These tasks are first mapped virtually
onto the accelerator's spatial fabric, and the compiler then generates a
physical mapping that respects hardware constraints. Experimental results on
both AMD Ryzen AI NPU and Alveo FPGA devices demonstrate that Dato achieves
high performance while significantly reducing the burden of writing optimized
code. On the NPU, Dato attains up to 84% hardware utilization for GEMM and
delivers a 2.81x speedup on attention kernels compared to a state-of-the-art
commercial framework. On the FPGA, Dato surpasses leading frameworks in
performance when generating custom systolic arrays, achieving 98% of the
theoretical peak performance.

</details>


### [29] [MIO: Multiverse Debugging in the Face of Input/Output -- Extended Version with Additional Appendices](https://arxiv.org/abs/2509.06845)
*Tom Lauwaerts,Maarten Steevens,Christophe Scholliers*

Main category: cs.PL

TL;DR: A new multiverse debugging method ensures only reachable program states are explored—even with complex I/O—demonstrated on microcontrollers with a prototype and proof of correctness.


<details>
  <summary>Details</summary>
Motivation: Debugging non-deterministic programs on microcontrollers is particularly difficult when bugs relate to unpredictable or input-dependent paths, and current multiverse debuggers further introduce the problem of exposing inaccessible program states during I/O operations.

Method: The paper proposes a novel approach to multiverse debugging that provides semantics and proof of correctness such that only accessible (reachable) program states are explored, even in the presence of complex I/O operations. The authors developed a prototype called MIO based on the WARDuino WebAssembly virtual machine.

Result: Their debugger avoids exploring unreachable states, supporting a broad spectrum of I/O, and is demonstrated on a tangible example using a color dial built with Lego Mindstorms hardware and an STM32 microcontroller.

Conclusion: Their approach provides efficient, correct multiverse debugging on microcontrollers with comprehensive I/O, avoiding pitfalls of existing tools and only exploring states that are possible in real program runs.

Abstract: Debugging non-deterministic programs on microcontrollers is notoriously
challenging, especially when bugs manifest in unpredictable, input-dependent
execution paths. A recent approach, called multiverse debugging, makes it
easier to debug non-deterministic programs by allowing programmers to explore
all potential execution paths. Current multiverse debuggers enable both forward
and backward traversal of program paths, and some facilitate jumping to any
previously visited states, potentially branching into alternative execution
paths within the state space.
  Unfortunately, debugging programs that involve input/output operations using
existing multiverse debuggers can reveal inaccessible program states, i.e.
states which are not encountered during regular execution. This can
significantly hinder the debugging process, as the programmer may spend
substantial time exploring and examining inaccessible program states, or worse,
may mistakenly assume a bug is present in the code, when in fact, the issue is
caused by the debugger.
  This paper presents a novel approach to multiverse debugging, which can
accommodate a broad spectrum of input/output operations. We provide the
semantics of our approach and prove the correctness of our debugger, ensuring
that despite having support for a wide range of input/output operations the
debugger will only explore those program states which can be reached during
regular execution.
  We have developed a prototype, called MIO, leveraging the WARDuino
WebAssembly virtual machine to demonstrate the feasibility and efficiency of
our techniques. As a demonstration of the approach we highlight a color dial
built with a Lego Mindstorms motor, and color sensor, providing a tangible
example of how our approach enables multiverse debugging for programs running
on an STM32 microcontroller.

</details>


### [30] [Mechanized Metatheory of Forward Reasoning for End-to-End Linearizability Proofs](https://arxiv.org/abs/2509.06872)
*Zachary Kent,Ugur Y. Yavuz,Siddhartha Jayanti,Stephanie Balzer,Guy Blelloch*

Main category: cs.PL

TL;DR: The paper mechanizes and formally verifies a key forward reasoning technique for proving linearizability of concurrent data structures, demonstrating its effectiveness through a verified proof for a simple concurrent register.


<details>
  <summary>Details</summary>
Motivation: Linearizability is a key correctness property for concurrent data structures, but fully verified, end-to-end proofs remain challenging. Although forward reasoning techniques are promising, mechanized and formally verified metatheoretic results are missing.

Method: The paper formalizes Jayanti et al.'s forward reasoning technique for proving linearizability and mechanizes its soundness and completeness proofs using Rocq. As a demonstration, a verified proof for a concurrent register is constructed.

Result: The authors provide formalization and mechanization of the soundness and completeness proofs for forward reasoning in Rocq. They successfully produce a verified, end-to-end proof of linearizability for a concurrent register as a case study.

Conclusion: This work reduces the trusted computing base for verifying linearizability by mechanizing crucial metatheoretic results, paving the way for fully verified proofs of concurrent data structures.

Abstract: In the past decade, many techniques have been developed to prove
linearizability, the gold standard of correctness for concurrent data
structures. Intuitively, linearizability requires that every operation on a
concurrent data structure appears to take place instantaneously, even when
interleaved with other operations. Most recently, Jayanti et al. presented the
first sound and complete "forward reasoning" technique for proving
linearizability that relates the behavior of a concurrent data structure to a
reference atomic data structure as time moves forward. This technique can be
used to produce machine-checked proofs of linearizability in TLA+. However,
while Jayanti et al.'s approach is shown to be sound and complete, a
mechanization of this important metatheoretic result is still outstanding. As a
result, it is not possible to produce verified end-to-end proofs of
linearizability. To reduce the size of this trusted computing base, we
formalize this forward reasoning technique and mechanize proofs of its
soundness and completeness in Rocq. As a case study, we use the approach to
produce a verified end-to-end proof of linearizability for a simple concurrent
register.

</details>
