<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 15]
- [cs.PL](#cs.PL) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [How Do Community Smells Influence Self-Admitted Technical Debt in Machine Learning Projects?](https://arxiv.org/abs/2506.15884)
*Shamse Tasnim Cynthia,Nuri Almarimi,Banani Roy*

Main category: cs.SE

TL;DR: This paper finds that poor organizational practices ('community smells') are widespread and strongly linked to technical debt in open-source ML projects, especially as projects grow. Early identification and mitigation of these issues are important for maintaining high-quality, sustainable ML systems.


<details>
  <summary>Details</summary>
Motivation: Machine learning (ML)-based projects are becoming increasingly complex, and poor organizational practices can lead to socio-technical issues and technical debt. However, the relationship between community smells (organizational issues) and self-admitted technical debt (SATD) is not well-understood in the context of open-source ML projects.

Method: The authors analyzed 155 open-source ML-based systems at the release level, identifying ten types of community smells and six categories of SATD. They applied statistical analysis to detect correlations between community smells and SATD, examined how these issues co-occur, and studied their evolution over project releases and across different project sizes.

Result: Community smells are common in ML projects, with distinct patterns across project sizes. Certain community smells, specifically those related to communication and organizational structure, correlate strongly with higher levels of SATD, particularly code and design debt. Authority- and communication-related smells often co-occur with technical debt types. The trends and trajectories of these issues also depend on project size.

Conclusion: Early detection and addressing of community smells and SATD are critical for preserving quality and sustainability in ML-based software projects. Socio-technical issues and their evolution over the project lifecycle need careful management, especially as project size influences these issues.

Abstract: Community smells reflect poor organizational practices that often lead to
socio-technical issues and the accumulation of Self-Admitted Technical Debt
(SATD). While prior studies have explored these problems in general software
systems, their interplay in machine learning (ML)-based projects remains
largely underexamined. In this study, we investigated the prevalence of
community smells and their relationship with SATD in open-source ML projects,
analyzing data at the release level. First, we examined the prevalence of ten
community smell types across the releases of 155 ML-based systems and found
that community smells are widespread, exhibiting distinct distribution patterns
across small, medium, and large projects. Second, we detected SATD at the
release level and applied statistical analysis to examine its correlation with
community smells. Our results showed that certain smells, such as Radio Silence
and Organizational Silos, are strongly correlated with higher SATD occurrences.
Third, we considered the six identified types of SATD to determine which
community smells are most associated with each debt category. Our analysis
revealed authority- and communication-related smells often co-occur with
persistent code and design debt. Finally, we analyzed how the community smells
and SATD evolve over the releases, uncovering project size-dependent trends and
shared trajectories. Our findings emphasize the importance of early detection
and mitigation of socio-technical issues to maintain the long-term quality and
sustainability of ML-based systems.

</details>


### [2] [Regression Testing Optimization for ROS-based Autonomous Systems: A Comprehensive Review of Techniques](https://arxiv.org/abs/2506.16101)
*Yupeng Jiang,Shuaiyi Sun,Xi Zheng*

Main category: cs.SE

TL;DR: This paper provides the first comprehensive survey of regression testing optimization techniques for ROS-based autonomous systems (ROSAS), categorizing 122 studies, identifying domain-specific challenges and gaps, and mapping future research directions to enhance testing efficiency and effectiveness in autonomous systems.


<details>
  <summary>Details</summary>
Motivation: Regression testing for ROS-based autonomous systems (ROSAS) is challenging due to dynamic behaviors, complex sensor data, distributed architectures, and strict safety constraints, but existing optimization research in this context is sparse.

Method: A comprehensive survey was conducted, systematically reviewing and categorizing 122 representative studies on regression testing optimization. The studies are analyzed and grouped by prioritization, minimization, and selection methods, and a structured taxonomy is presented to clarify their applicability and limitations for ROSAS.

Result: The survey identified the unique challenges of regression testing in ROSAS and mapped out existing methods' strengths and weaknesses. It also highlighted gaps and proposed future research directions, such as new coverage metrics and the use of advanced AI techniques for more effective testing.

Conclusion: This work offers a foundational survey and practical roadmap, establishing a baseline reference for advancing regression testing optimization in ROSAS.

Abstract: Regression testing plays a critical role in maintaining software reliability,
particularly for ROS-based autonomous systems (ROSAS), which frequently undergo
continuous integration and iterative development. However, conventional
regression testing techniques face significant challenges when applied to
autonomous systems due to their dynamic and non-deterministic behaviors,
complex multi-modal sensor data, asynchronous distributed architectures, and
stringent safety and real-time constraints. Although numerous studies have
explored test optimization in traditional software contexts, regression testing
optimization specifically for ROSAS remains largely unexplored. To address this
gap, we present the first comprehensive survey systematically reviewing
regression testing optimization techniques tailored for ROSAS. We analyze and
categorize 122 representative studies into regression test case prioritization,
minimization, and selection methods. A structured taxonomy is introduced to
clearly illustrate their applicability and limitations within ROSAS contexts.
Furthermore, we highlight major challenges specific to regression testing for
ROSAS, including effectively prioritizing tests in response to frequent system
modifications, efficiently minimizing redundant tests, and difficulty in
accurately selecting impacted test cases. Finally, we propose research insights
and identify promising future directions, such as leveraging frame-to-vector
coverage metrics, multi-source foundation models, and neurosymbolic reasoning
to enhance regression testing efficiency and effectiveness. This survey
provides a foundational reference and practical roadmap for advancing the
state-of-the-art in regression testing optimization for ROSAS.

</details>


### [3] [Seeing is Fixing: Cross-Modal Reasoning with Multimodal LLMs for Visual Software Issue Fixing](https://arxiv.org/abs/2506.16136)
*Kai Huang,Jian Zhang,Xiaofei Xie,Chunyang Chen*

Main category: cs.SE

TL;DR: GUIRepair introduces cross-modal reasoning to automated program repair, using both textual and visual information to significantly outperform existing methods in fixing issues that involve GUIs.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based automated program repair techniques struggle to resolve multimodal issues that require interpretation of visual information from GUIs, limiting their effectiveness to unimodal (text-based) scenarios.

Method: The authors introduce GUIRepair, a cross-modal reasoning system with two core components: Image2Code (translates GUI images into contextually relevant code for fault comprehension) and Code2Image (replays and captures patched program GUI to aid patch validation). This allows the system to understand and utilize visual information in issue resolution.

Result: When evaluated on the SWE-bench M benchmark, GUIRepair, with GPT-4o as the base model, successfully solved 157 instances, surpassing the best open-source baseline by 26. Using o4-mini, it solved 175 instances, outperforming the top commercial system by 22.

Conclusion: GUIRepair's integration of cross-modal (visual and textual) reasoning fundamentally improves automated program repair performance on multimodal issues, demonstrating the importance and effectiveness of leveraging visual information for bug understanding and validation.

Abstract: Large language model-(LLM) based automated program repair (APR) techniques
have shown promising results in resolving real-world GitHub issue tasks.
Existing APR systems are primarily evaluated in unimodal settings (e.g.,
SWE-bench). However, these autonomous systems struggle to resolve multimodal
problem scenarios (e.g., SWE-bench M) due to limitations in interpreting and
leveraging visual information. In multimodal scenarios, LLMs need to rely on
visual information in the graphical user interface (GUI) to understand bugs and
generate fixes. To bridge this gap, we propose GUIRepair, a cross-modal
reasoning approach for resolving multimodal issue scenarios by understanding
and capturing visual information. Specifically, GUIRepair integrates two key
components, Image2Code and Code2Image, to enhance fault comprehension and patch
validation. Image2Code extracts relevant project documents based on the issue
report, then applies this domain knowledge to generate the reproduced code
responsible for the visual symptoms, effectively translating GUI images into
executable context for better fault comprehension. Code2Image replays the
visual issue scenario using the reproduced code and captures GUI renderings of
the patched program to assess whether the fix visually resolves the issue,
providing feedback for patch validation. We evaluate GUIRepair on SWE-bench M,
and the approach demonstrates significant effectiveness. When utilizing GPT-4o
as the base model, GUIRepair solves 157 instances, outperforming the best
open-source baseline by 26 instances. Furthermore, when using o4-mini as the
base model, GUIRepair can achieve even better results and solve 175 instances,
outperforming the top commercial system by 22 instances. This emphasizes the
success of our new perspective on incorporating cross-modal reasoning by
understanding and capturing visual information to resolve multimodal issues.

</details>


### [4] [The Technical Debt Gamble: A Case Study on Technical Debt in a Large-Scale Industrial Microservice Architecture](https://arxiv.org/abs/2506.16214)
*Klara Borowa,Andrzej Ratkowski,Roberto Verdecchia*

Main category: cs.SE

TL;DR: This paper investigates how technical debt arises and can be managed in large-scale microservice systems, using a case study that blends static code analysis with team interviews. Key findings suggest that straightforward tools like static analyzers are effective at spotting technical debt, but human factors like communication and structure play an equally important role. Strategies are provided for better technical debt management.


<details>
  <summary>Details</summary>
Motivation: Microservice architectures are believed to enhance maintainability and evolvability due to their loose coupling. However, technical debt can threaten these promised benefits, and there is limited research on how technical debt manifests in large-scale microservice systems.

Method: The study uses a mixed-method case study involving a system with over 100 microservices operating in more than 15,000 locations. It combines quantitative data from static code analyzers with qualitative data from a focus group discussion with the development team and a follow-up interview with the lead architect.

Result: The study finds that static source code analysis is an effective approach for detecting technical debt at scale. It also identifies inadequate communication and misalignment between architecture and organizational structure as key contributors to technical debt. Additionally, microservices can cycle quickly between accumulating and resolving technical debt, termed as 'microservice architecture technical debt gamble.' The research also proposes strategies for managing technical debt in microservice systems.

Conclusion: Technical debt is a significant concern in large-scale microservice architectures, driven by communication issues and structural misalignments. Simple static analysis can help identify debt, but ongoing management requires both technical and organizational strategies.

Abstract: Microservice architectures provide an intuitive promise of high
maintainability and evolvability due to loose coupling. However, these quality
attributes are notably vulnerable to technical debt (TD). Few studies address
TD in microservice systems, particularly on a large scale. This research
explores how TD manifests in a large-scale microservice-based industrial
system. The research is based on a mixed-method case study of a project
including over 100 microservices and serving over 15k locations. Results are
collected via a quantitative method based static code analyzers combined with
qualitative insights derived from a focus group discussion with the development
team and a follow-up interview with the lead architect of the case study
system. Results show that (1) simple static source code analysis can be an
efficient and effective entry point for holistic TD discovery, (2) inadequate
communication significantly contributes to TD, (3) misalignment between
architectural and organizational structures can exacerbate TD accumulation, (4)
microservices can rapidly cycle through TD accumulation and resolution, a
phenomenon referred to as "microservice architecture technical debt gamble".
Finally, we identify a set of fitting strategies for TD management in
microservice architectures.

</details>


### [5] [Evaluating the Use of LLMs for Documentation to Code Traceability](https://arxiv.org/abs/2506.16440)
*Ebube Alor,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: LLMs substantially improve automated documentation-to-code traceability over current baselines, but require careful task design and possibly human-in-the-loop approaches due to persistent error patterns, particularly in explanation and multi-step linking tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper is to address the underexplored capabilities of Large Language Models (LLMs) in automating documentation-to-code traceability in software engineering, which has traditionally been a labor-intensive process.

Method: The authors conduct systematic experiments using three advanced LLMs (Claude 3.5 Sonnet, GPT-4o, and o3-mini) and compare their performance against classic and neural IR baselines (TF-IDF, BM25, and CodeBERT) on two newly created datasets derived from open-source projects. The study assesses three main capabilities: trace link identification accuracy, relationship explanation quality, and multi-step chain reconstruction.

Result: The best-performing LLM achieves F1-scores of 79.4% and 80.4% on the two datasets, significantly outperforming the baselines. Relationship explanation quality is solid, with fully correct explanations between 42.9% and 71.1%, and partial accuracy above 97%. Multi-step chain reconstruction shows high endpoint accuracy but more variability in intermediate links. Error analysis identifies common error patterns such as naming assumptions and overgeneralization.

Conclusion: LLMs are powerful tools for discovering trace links between documentation and code, clearly surpassing current baselines. However, their limitations in explanation quality and chain reconstruction suggest a need for human oversight and further research into mitigating specific error patterns.

Abstract: Large Language Models (LLMs) offer new potential for automating
documentation-to-code traceability, yet their capabilities remain
underexplored. We present a comprehensive evaluation of LLMs (Claude 3.5
Sonnet, GPT-4o, and o3-mini) in establishing trace links between various
software documentation (including API references and user guides) and source
code. We create two novel datasets from two open-source projects (Unity Catalog
and Crawl4AI). Through systematic experiments, we assess three key
capabilities: (1) trace link identification accuracy, (2) relationship
explanation quality, and (3) multi-step chain reconstruction. Results show that
the best-performing LLM achieves F1-scores of 79.4% and 80.4% across the two
datasets, substantially outperforming our baselines (TF-IDF, BM25, and
CodeBERT). While fully correct relationship explanations range from 42.9% to
71.1%, partial accuracy exceeds 97%, indicating that fundamental connections
are rarely missed. For multi-step chains, LLMs maintain high endpoint accuracy
but vary in capturing precise intermediate links. Error analysis reveals that
many false positives stem from naming-based assumptions, phantom links, or
overgeneralization of architectural patterns. We demonstrate that task-framing,
such as a one-to-many matching strategy, is critical for performance. These
findings position LLMs as powerful assistants for trace discovery, but their
limitations could necessitate human-in-the-loop tool design and highlight
specific error patterns for future research.

</details>


### [6] [Understanding the Challenges and Promises of Developing Generative AI Apps: An Empirical Study](https://arxiv.org/abs/2506.16453)
*Buthayna AlMulla,Maram Assi,Safwat Hassan*

Main category: cs.SE

TL;DR: By analyzing over 670,000 reviews from 173 generative AI apps using a novel LLM-based methodology, this paper identifies key user concerns, tracks evolving expectations, and offers guidance for improving future Gen-AI app development.


<details>
  <summary>Details</summary>
Motivation: Despite the growing popularity and widespread adoption of generative AI mobile apps following ChatGPT's release, there is limited understanding of how users actually perceive and evaluate these apps in real-world settings. The study aims to address this gap by systematically analyzing user feedback.

Method: The authors analyze 676,066 user reviews from 173 Gen-AI apps in the Google Play Store. They introduce a four-phase methodology called SARA (Selection, Acquisition, Refinement, and Analysis), utilizing prompt-based large language model techniques for extracting user insights. The methodology is validated for topic extraction accuracy (91%) using five-shot prompting and review filtering. They identify and track key topics over time to understand user discussions and changing patterns.

Result: The study demonstrates that large language models can reliably extract topics from user reviews. Ten key topics discussed by users, such as AI Performance, Content Quality, and Content Policy & Censorship, are identified. The evolution of these topics highlights changing user expectations and engagement patterns. The research offers actionable insights for app developers and researchers.

Conclusion: LLM-driven, prompt-based methodologies are effective for extracting and analyzing large-scale mobile app user feedback, revealing critical user concerns and trends in Gen-AI app use. The insights can inform future app development and research into generative AI user experiences.

Abstract: The release of ChatGPT in 2022 triggered a rapid surge in generative
artificial intelligence mobile apps (i.e., Gen-AI apps). Despite widespread
adoption, little is known about how end users perceive and evaluate these
Gen-AI functionalities in practice. In this work, we conduct a user-centered
analysis of 676,066 reviews from 173 Gen-AI apps on the Google Play Store. We
introduce a four-phase methodology, SARA (Selection, Acquisition, Refinement,
and Analysis), that enables the systematic extraction of user insights using
prompt-based LLM techniques. First, we demonstrate the reliability of LLMs in
topic extraction, achieving 91% accuracy through five-shot prompting and
non-informative review filtering. Then, we apply this method to the informative
reviews, identify the top 10 user-discussed topics (e.g., AI Performance,
Content Quality, and Content Policy & Censorship) and analyze the key
challenges and emerging opportunities. Finally, we examine how these topics
evolve over time, offering insight into shifting user expectations and
engagement patterns with Gen-AI apps. Based on our findings and observations,
we present actionable implications for developers and researchers.

</details>


### [7] [Scaling GR(1) Synthesis via a Compositional Framework for LTL Discrete Event Control](https://arxiv.org/abs/2506.16557)
*Hernán Gagliardi,Victor Braberman,Sebastian Uchitel*

Main category: cs.SE

TL;DR: The paper proposes a modular, compositional approach to synthesizing controllers for large systems with LTL goals, avoiding state explosion and enabling solutions for much larger problems than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional, monolithic controller synthesis methods for discrete event systems with LTL goals suffer from the state explosion problem, making them impractical for large or modular systems. The authors aim to address these scalability challenges.

Method: A compositional approach is used, leveraging the modular structure of the plant, modeled as labelled transition systems (LTS). Controllers are iteratively synthesized for subsets of the system, using maximally permissive safe controllers and weakened control problems. Observational synthesis equivalence helps abstract away local events, and the result is a set of controllers that work together to satisfy the LTL goals. The approach is implemented for the GR(1) subset of LTL in the MTSA tool.

Result: The compositional method can handle synthesis problems that are up to 1000 times larger than those solvable by conventional monolithic approaches. The approach leads to practical controller synthesis for much larger systems.

Conclusion: Compositional controller synthesis, with proper modularization and abstraction, enables tractable solutions for large discrete event systems with LTL goals, outperforming monolithic solutions in scalability and efficiency.

Abstract: We present a compositional approach to controller synthesis of discrete event
system controllers with linear temporal logic (LTL) goals. We exploit the
modular structure of the plant to be controlled, given as a set of labelled
transition systems (LTS), to mitigate state explosion that monolithic
approaches to synthesis are prone to. Maximally permissive safe controllers are
iteratively built for subsets of the plant LTSs by solving weaker control
problems. Observational synthesis equivalence is used to reduce the size of the
controlled subset of the plant by abstracting away local events. The result of
synthesis is also compositional, a set of controllers that when run in parallel
ensure the LTL goal. We implement synthesis in the MTSA tool for an expressive
subset of LTL, GR(1), and show it computes solutions to that can be up to 1000
times larger than those that the monolithic approach can solve.

</details>


### [8] [AI-Driven Tools in Modern Software Quality Assurance: An Assessment of Benefits, Challenges, and Future Directions](https://arxiv.org/abs/2506.16586)
*Ihor Pysmennyi,Roman Kyslyi,Kyrylo Kleshch*

Main category: cs.SE

TL;DR: AI-powered QA tools can greatly improve software quality assurance efficiency and scope, but practical use is challenged by coverage, explainability, and reliability issues, requiring careful, methodical implementation for real-world adoption.


<details>
  <summary>Details</summary>
Motivation: Traditional QA methods struggle with modern software's complexity, scale, rapid iteration, and limited resources, leading to high costs from poor quality.

Method: The study conducts a comprehensive analysis of integrating AI-oriented tools into QA processes for distributed applications. It examines impacts on verification and validation, covering exploratory testing, metamorphic testing, static analysis, test case/unit test generation, and end-to-end regression testing using AI agents over generated test scenarios as a proof of concept.

Result: AI-driven QA showed promising results—with only 8.3% flaky test executions—but practical challenges were found. These include issues with achieving semantically identical coverage, black-box nature and limited explainability of LLMs, and their tendency to adapt test cases improperly, all necessitating better verification processes.

Conclusion: AI technologies have significant transformative potential for QA, but successful adoption requires a strategic approach and development of robust verification methodologies to address the outlined limitations.

Abstract: Traditional quality assurance (QA) methods face significant challenges in
addressing the complexity, scale, and rapid iteration cycles of modern software
systems and are strained by limited resources available, leading to substantial
costs associated with poor quality. The object of this research is the Quality
Assurance processes for modern distributed software applications. The subject
of the research is the assessment of the benefits, challenges, and prospects of
integrating modern AI-oriented tools into quality assurance processes. We
performed comprehensive analysis of implications on both verification and
validation processes covering exploratory test analyses, equivalence
partitioning and boundary analyses, metamorphic testing, finding
inconsistencies in acceptance criteria (AC), static analyses, test case
generation, unit test generation, test suit optimization and assessment, end to
end scenario execution. End to end regression of sample enterprise application
utilizing AI-agents over generated test scenarios was implemented as a proof of
concept highlighting practical use of the study. The results, with only 8.3%
flaky executions of generated test cases, indicate significant potential for
the proposed approaches. However, the study also identified substantial
challenges for practical adoption concerning generation of semantically
identical coverage, "black box" nature and lack of explainability from
state-of-the-art Large Language Models (LLMs), the tendency to correct mutated
test cases to match expected results, underscoring the necessity for thorough
verification of both generated artifacts and test execution results. The
research demonstrates AI's transformative potential for QA but highlights the
importance of a strategic approach to implementing these technologies,
considering the identified limitations and the need for developing appropriate
verification methodologies.

</details>


### [9] [LLM-based Satisfiability Checking of String Requirements by Consistent Data and Checker Generation](https://arxiv.org/abs/2506.16639)
*Boqi Chen,Aren A. Babikian,Shuzhao Feng,Dániel Varró,Gunter Mussbacher*

Main category: cs.SE

TL;DR: The paper shows that combining LLMs with automatically generated checkers significantly improves the automated verification of natural language string requirements, more than doubling generation success rates and F1-scores.


<details>
  <summary>Details</summary>
Motivation: Verifying sets of requirements written in natural language (NL), particularly those involving strings, is difficult due to manual analysis limitations, the theoretical limitations of formal approaches (e.g., SMT solvers), and the high manual effort required for translating NL into formal constraints. With LLMs showing promise for formal reasoning tasks, their effectiveness in this domain has not been thoroughly explored.

Method: The authors propose a hybrid approach that: (1) uses Large Language Models (LLMs) to derive the satisfiability result (and a consistent string if possible) from NL requirements over strings, and (2) employs LLMs to generate declarative (SMT) and imperative (Python) checkers, which then validate the correctness of the outcome from step (1). The approach is experimentally evaluated with four different LLMs.

Result: LLMs demonstrated effectiveness in translating NL requirements into functional checkers, achieving perfect test accuracy for Python-based checkers. The use of these checkers substantially improved LLMs’ ability to generate consistent strings and to identify unsatisfiable requirements, resulting in more than double the generation success rate and F1-score compared to baselines that did not use checkers.

Conclusion: Combining LLM-based reasoning with automatically generated checkers provides a practical and effective way to verify the satisfiability of NL requirements over strings, improving outcome reliability compared to both manual methods and purely LLM-based or formal methods without such hybrid validation.

Abstract: Requirements over strings, commonly represented using natural language (NL),
are particularly relevant for software systems due to their heavy reliance on
string data manipulation. While individual requirements can usually be analyzed
manually, verifying properties (e.g., satisfiability) over sets of NL
requirements is particularly challenging. Formal approaches (e.g., SMT solvers)
may efficiently verify such properties, but are known to have theoretical
limitations. Additionally, the translation of NL requirements into formal
constraints typically requires significant manual effort. Recently, large
language models (LLMs) have emerged as an alternative approach for formal
reasoning tasks, but their effectiveness in verifying requirements over strings
is less studied. In this paper, we introduce a hybrid approach that verifies
the satisfiability of NL requirements over strings by using LLMs (1) to derive
a satisfiability outcome (and a consistent string, if possible), and (2) to
generate declarative (i.e., SMT) and imperative (i.e., Python) checkers, used
to validate the correctness of (1). In our experiments, we assess the
performance of four LLMs. Results show that LLMs effectively translate natural
language into checkers, even achieving perfect testing accuracy for
Python-based checkers. These checkers substantially help LLMs in generating a
consistent string and accurately identifying unsatisfiable requirements,
leading to more than doubled generation success rate and F1-score in certain
cases compared to baselines without generated checkers.

</details>


### [10] [SemAgent: A Semantics Aware Program Repair Agent](https://arxiv.org/abs/2506.16650)
*Anvith Pabba,Alex Mathai,Anindya Chakraborty,Baishakhi Ray*

Main category: cs.SE

TL;DR: SemAgent is a new workflow for automated program repair that uses deeper understanding of code, issues, and execution context to generate better patches. It outperforms previous methods, particularly on complex, multi-line bug fixes.


<details>
  <summary>Details</summary>
Motivation: Current automated program repair (APR) systems using large language models tend to focus only on the most suspicious lines of code and apply localized fixes, often missing the broader issue semantics, code semantics, or execution semantics. This can lead to overfitting, producing patches that only address a particular user issue without creating a more general or robust solution.

Method: The authors introduce SemAgent, which uses a novel workflow-based pipeline. This involves (a) using execution semantics to retrieve relevant code context, (b) understanding issue-semantics through abstraction, (c) isolating code-semantics within the defined abstraction, and (d) applying a two-stage architecture: first, a repair stage makes detailed fixes, then a reviewer stage filters fixes according to the inferred semantics of the issue.

Result: Their approach achieves a 44.66% solve rate on the SWEBench-Lite benchmark, outperforming all other workflow-based methods. It demonstrates an absolute improvement of 7.66% over a baseline system that doesn't use deep semantic understanding. SemAgent works especially well for issues that require reasoning across multiple code lines and for edge-case scenarios.

Conclusion: By integrating issue, code, and execution semantics in APR systems, more complete and semantically consistent code repairs can be made, resulting in more generalizable and robust fixes than previous approaches.

Abstract: Large Language Models (LLMs) have shown impressive capabilities in downstream
software engineering tasks such as Automated Program Repair (APR). In
particular, there has been a lot of research on repository-level
issue-resolution benchmarks such as SWE-Bench. Although there has been
significant progress on this topic, we notice that in the process of solving
such issues, existing agentic systems tend to hyper-localize on immediately
suspicious lines of code and fix them in isolation, without a deeper
understanding of the issue semantics, code semantics, or execution semantics.
Consequently, many existing systems generate patches that overfit to the user
issue, even when a more general fix is preferable. To address this limitation,
we introduce SemAgent, a novel workflow-based procedure that leverages issue,
code, and execution semantics to generate patches that are complete -
identifying and fixing all lines relevant to the issue. We achieve this through
a novel pipeline that (a) leverages execution semantics to retrieve relevant
context, (b) comprehends issue-semantics via generalized abstraction, (c)
isolates code-semantics within the context of this abstraction, and (d)
leverages this understanding in a two-stage architecture: a repair stage that
proposes fine-grained fixes, followed by a reviewer stage that filters relevant
fixes based on the inferred issue-semantics. Our evaluations show that our
methodology achieves a solve rate of 44.66% on the SWEBench-Lite benchmark
beating all other workflow-based approaches, and an absolute improvement of
7.66% compared to our baseline, which lacks such deep semantic understanding.
We note that our approach performs particularly well on issues requiring
multi-line reasoning (and editing) and edge-case handling, suggesting that
incorporating issue and code semantics into APR pipelines can lead to robust
and semantically consistent repairs.

</details>


### [11] [LLMs in Coding and their Impact on the Commercial Software Engineering Landscape](https://arxiv.org/abs/2506.16653)
*Vladislav Belozerov,Peter J Barclay,Askhan Sami*

Main category: cs.SE

TL;DR: While LLM coding tools are valuable for developer productivity, they also introduce serious risks (data leaks, security flaws, sycophancy). The paper urges careful governance and technical safeguards to use these tools safely and responsibly.


<details>
  <summary>Details</summary>
Motivation: Large language model (LLM) coding tools are widely used, shifting human work higher in software development. However, their use introduces new risks: data leakage, security vulnerabilities, and sycophancy (uncritical agreement with incorrect ideas).

Method: The authors analyze risks associated with LLM coding tools by examining the rate of data leaks, presence of security flaws in generated code, and the occurrence of sycophantic responses. They propose organizational policies and safety practices for mitigating these risks.

Result: Their findings reveal that 10% of prompts leak private data, 42% of generated code snippets contain security vulnerabilities, and LLMs can exhibit sycophancy. Specific recommendations are provided for safer integration of such tools into development workflows.

Conclusion: Firms need to implement robust review and tagging of AI-generated code, ensure private/on-premises deployment, follow safety regulations, and introduce sycophancy-detecting tests to balance efficiency with security and correctness.

Abstract: Large-language-model coding tools are now mainstream in software engineering.
But as these same tools move human effort up the development stack, they
present fresh dangers: 10% of real prompts leak private data, 42% of generated
snippets hide security flaws, and the models can even ``agree'' with wrong
ideas, a trait called sycophancy. We argue that firms must tag and review every
AI-generated line of code, keep prompts and outputs inside private or
on-premises deployments, obey emerging safety regulations, and add tests that
catch sycophantic answers -- so they can gain speed without losing security and
accuracy.

</details>


### [12] [Accountability of Robust and Reliable AI-Enabled Systems: A Preliminary Study and Roadmap](https://arxiv.org/abs/2506.16831)
*Filippo Scaramuzza,Damian A. Tamburri,Willem-Jan van den Heuvel*

Main category: cs.SE

TL;DR: This vision paper reviews literature and presents a case study to underline the importance of robustness, reliability, and accountability in AI systems. It identifies challenges, research gaps, and future research needs for building trustworthy, responsible AI.


<details>
  <summary>Details</summary>
Motivation: As AI systems are increasingly deployed in practical and high-stakes applications, ensuring their safety, accountability, and trustworthiness is critical.

Method: The paper conducts a literature review, analyzes evolving definitions, and presents a real-world case study to identify challenges and illustrate practical applications.

Result: The study highlights major challenges in current AI system robustness and reliability, demonstrates the practical importance of accountability, and proposes future research directions.

Conclusion: Robustness, reliability, and accountability are essential for trustworthy AI systems. Addressing challenges and research gaps in these areas is key to responsible AI development.

Abstract: This vision paper presents initial research on assessing the robustness and
reliability of AI-enabled systems, and key factors in ensuring their safety and
effectiveness in practical applications, including a focus on accountability.
By exploring evolving definitions of these concepts and reviewing current
literature, the study highlights major challenges and approaches in the field.
A case study is used to illustrate real-world applications, emphasizing the
need for innovative testing solutions. The incorporation of accountability is
crucial for building trust and ensuring responsible AI development. The paper
outlines potential future research directions and identifies existing gaps,
positioning robustness, reliability, and accountability as vital areas for the
development of trustworthy AI systems of the future.

</details>


### [13] [Revolutionizing Validation and Verification: Explainable Testing Methodologies for Intelligent Automotive Decision-Making Systems](https://arxiv.org/abs/2506.16876)
*Halit Eris,Stefan Wagner*

Main category: cs.SE

TL;DR: This vision paper proposes an explainable and transparent framework for validating autonomous driving systems, leveraging large language models and real-time simulations to improve diagnostic efficiency and user trust.


<details>
  <summary>Details</summary>
Motivation: Autonomous Driving Systems rely on complex decision-making with various sensory inputs, making validation and verification crucial. However, diagnosing failures and ensuring transparency is challenging, and manual testing is inefficient.

Method: The paper proposes a methodology that embeds explainability and interpretability into the V&V process. This involves refining V&V requirements through reviews and stakeholder feedback, utilizing large language models to generate explainable test scenarios, and implementing real-time validation in simulation. The framework includes a test oracle, explanation generation, and a chatbot to aid testing.

Result: Empirical studies are planned to evaluate if the framework improves diagnostic efficiency and transparency in V&V processes for ADS.

Conclusion: The methodology aims to streamline validation and verification, reduce resource requirements, and increase user trust in autonomous driving systems by incorporating explainability and transparency.

Abstract: Autonomous Driving Systems (ADS) use complex decision-making (DM) models with
multimodal sensory inputs, making rigorous validation and verification (V&V)
essential for safety and reliability. These models pose challenges in
diagnosing failures, tracing anomalies, and maintaining transparency, with
current manual testing methods being inefficient and labor-intensive. This
vision paper presents a methodology that integrates explainability,
transparency, and interpretability into V&V processes. We propose refining V&V
requirements through literature reviews and stakeholder input, generating
explainable test scenarios via large language models (LLMs), and enabling
real-time validation in simulation environments. Our framework includes test
oracle, explanation generation, and a test chatbot, with empirical studies
planned to evaluate improvements in diagnostic efficiency and transparency. Our
goal is to streamline V&V, reduce resources, and build user trust in autonomous
technologies.

</details>


### [14] [Quantum Optimization for Software Engineering: A Survey](https://arxiv.org/abs/2506.16878)
*Man Zhang,Yuechen Li,Tao Yue,Kai-Yuan Cai*

Main category: cs.SE

TL;DR: The paper reviews how quantum and quantum-inspired algorithms are being applied to classic Software Engineering optimization problems, identifying research trends, gaps, and opportunities to adopt quantum approaches in solving next-generation SE challenges.


<details>
  <summary>Details</summary>
Motivation: While classical optimization methods are well-established in Software Engineering (SE), the increasing complexity of modern software systems calls for novel optimization approaches. Quantum computing offers fresh possibilities, but there is little synthesized knowledge about its impact on classical SE optimization problems.

Method: The study conducts a Systematic Literature Review (SLR), selecting 77 primary studies from an initial pool of 2083 papers, using systematic searches across six digital databases with carefully constructed search strings. The focus is on applications of quantum or quantum-inspired algorithms for SE optimization tasks.

Result: The review identifies concentrated research in areas like SE operations and software testing, while finding significant gaps in the application of quantum optimization to other SE activities. It also finds pertinent research beyond traditional SE publication venues.

Conclusion: This SLR offers an extensive overview of the emerging intersection between quantum computing and SE optimization, highlighting existing efforts, knowledge gaps, and opportunities for integrating quantum advancements into SBSE to tackle future software challenges.

Abstract: Quantum computing, particularly in the area of quantum optimization, is
steadily progressing toward practical applications, supported by an expanding
range of hardware platforms and simulators. While Software Engineering (SE)
optimization has a strong foundation, which is exemplified by the active
Search-Based Software Engineering (SBSE) community and numerous classical
optimization methods, the growing complexity of modern software systems and
their engineering processes demands innovative solutions. This Systematic
Literature Review (SLR) focuses specifically on studying the literature that
applies quantum or quantum-inspired algorithms to solve classical SE
optimization problems. We examine 77 primary studies selected from an initial
pool of 2083 publications obtained through systematic searches of six digital
databases using carefully crafted search strings. Our findings reveal
concentrated research efforts in areas such as SE operations and software
testing, while exposing significant gaps across other SE activities.
Additionally, the SLR uncovers relevant works published outside traditional SE
venues, underscoring the necessity of this comprehensive review. Overall, our
study provides a broad overview of the research landscape, empowering the SBSE
community to leverage quantum advancements in addressing next-generation SE
challenges.

</details>


### [15] [Identifying Explanation Needs: Towards a Catalog of User-based Indicators](https://arxiv.org/abs/2506.16997)
*Hannah Deters,Laura Reinhardt,Jakob Droste,Martin Obaidi,Kurt Schneider*

Main category: cs.SE

TL;DR: This paper identifies and categorizes a set of runtime indicators (behavioral, system, emotional) that signal when users may need explanations from complex software. These indicators help improve real-time and post-deployment responsiveness to user explanation needs, supporting more explainable systems.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the increasing complexity of software systems and the growing importance of explainability. A central challenge is identifying when users need explanations, since self-reporting can be biased and unreliable.

Method: The study conducts explorative, online research, gathering self-reported indicators from users that signal a need for explanation. The researchers compile and categorize these indicators into behavioral, system, and emotional/physical types, and analyze their relationships to different explanation needs.

Result: A catalog of 17 user behavior indicators, 8 system event indicators, and 14 emotional/physical reaction indicators is created. The relationships between these indicators and types of explanation needs are mapped.

Conclusion: The identified indicators can be used both during development and after deployment of software systems to better elicit user explanation needs. They can improve requirement gathering and enable runtime triggering of explanations for users, enhancing explainability in practice.

Abstract: In today's digitalized world, where software systems are becoming
increasingly ubiquitous and complex, the quality aspect of explainability is
gaining relevance. A major challenge in achieving adequate explanations is the
elicitation of individual explanation needs, as it may be subject to severe
hypothetical or confirmation biases. To address these challenges, we aim to
establish user-based indicators concerning user behavior or system events that
can be captured at runtime to determine when a need for explanations arises. In
this work, we conducted explorative research in form of an online study to
collect self-reported indicators that could indicate a need for explanation. We
compiled a catalog containing 17 relevant indicators concerning user behavior,
8 indicators concerning system events and 14 indicators concerning emotional
states or physical reactions. We also analyze the relationships between these
indicators and different types of need for explanation. The established
indicators can be used in the elicitation process through prototypes, as well
as after publication to gather requirements from already deployed applications
using telemetry and usage data. Moreover, these indicators can be used to
trigger explanations at appropriate moments during the runtime.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [16] [A System Level Compiler for Massively-Parallel, Spatial, Dataflow Architectures](https://arxiv.org/abs/2506.15875)
*Dirk Van Essendelft,Patrick Wingo,Terry Jordan,Ryan Smith,Wissam Saidi*

Main category: cs.PL

TL;DR: MACH is a new compiler framework that simplifies programming for advanced parallel dataflow hardware and regular unified-memory systems. By using a virtual machine abstraction and a custom domain-specific language, MACH can lower high-level code, like NumPy tensor operations, to run efficiently on architectures such as Cerebras' Wafer Scale Engine.


<details>
  <summary>Details</summary>
Motivation: There is a growing need for efficient compilation tools targeting massively-parallel spatial dataflow architectures, which pose unique challenges not addressed by traditional compilers developed for unified-memory architectures. Existing tools lack flexibility, generality, and ease of support for new hardware.

Method: The paper proposes a novel compiler architecture (MACH) built with a conceptual Virtual Machine, a domain-specific language, and a flexible compiler pipeline capable of lowering high-level languages (e.g., NumPy) to hardware-specific implementations, particularly for spatial architectures like the Wafer Scale Engine.

Result: MACH enables compilation of high-level dense tensor operations (e.g., from NumPy) down to specific implementations on spatial architectures such as Cerebras' Wafer Scale Engine. It demonstrates operability across diverse architectures and incorporation of both standardized and user-defined data mappings.

Conclusion: MACH successfully provides a unified compilation framework for both advanced spatial dataflow and traditional unified-memory architectures, demonstrating flexible and efficient code generation capabilities.

Abstract: We have developed a novel compiler called the Multiple-Architecture Compiler
for Advanced Computing Hardware (MACH) designed specifically for
massively-parallel, spatial, dataflow architectures like the Wafer Scale
Engine. Additionally, MACH can execute code on traditional unified-memory
devices. MACH addresses the complexities in compiling for spatial architectures
through a conceptual Virtual Machine, a flexible domain-specific language, and
a compiler that can lower high-level languages to machine-specific code in
compliance with the Virtual Machine concept. While MACH is designed to be
operable on several architectures and provide the flexibility for several
standard and user-defined data mappings, we introduce the concept with dense
tensor examples from NumPy and show lowering to the Wafer Scale Engine by
targeting Cerebras' hardware specific languages.

</details>


### [17] [WAMI: Compilation to WebAssembly through MLIR without Losing Abstraction](https://arxiv.org/abs/2506.16048)
*Byeongjee Kang,Harsh Desai,Limin Jia,Brandon Lucia*

Main category: cs.PL

TL;DR: The paper proposes a Wasm-centric MLIR compilation pipeline that keeps high-level abstractions, making it easier to adopt new Wasm features without significant performance loss compared to traditional LLVM-based compilers.


<details>
  <summary>Details</summary>
Motivation: WebAssembly (Wasm) is evolving to better support high-level language features but current compiler approaches either require duplicated efforts for each language or lose crucial high-level abstractions. Existing pipelines, such as those using MLIR, still depend on LLVM, which does not preserve these abstractions for Wasm code generation.

Method: The paper introduces a new compilation pipeline for Wasm that utilizes Wasm-specific dialects in MLIR, allowing high-level Wasm constructs to be directly represented and generated. The approach offers modularity and extensibility for adopting Wasm high-level features. This is demonstrated via a case study on the Stack Switching feature, and performance benchmarking is conducted using PolyBench.

Result: The new pipeline can maintain high-level abstractions and directly generate Wasm code, facilitating easier adoption of Wasm features. Benchmarks show that the generated code is at most 7.7% slower -- and sometimes even faster -- than code created by LLVM-based compilers, indicating competitive performance.

Conclusion: A Wasm-centric MLIR compilation pipeline can produce efficient, high-level code and support the evolving Wasm feature set while maintaining competitive performance with traditional LLVM-based methods.

Abstract: WebAssembly (Wasm) is a portable bytecode format that serves as a compilation
target for high-level languages, enabling their secure and efficient execution
across diverse platforms, including web browsers and embedded systems. To
improve support for high-level languages without incurring significant code
size or performance overheads, Wasm continuously evolves by integrating
high-level features such as Garbage Collection and Stack Switching. However,
existing compilation approaches either lack reusable design -- requiring
redundant implementation efforts for each language -- or lose abstraction by
lowering high-level constructs into low-level shared representations like LLVM
IR, which hinder the adoption of high-level features. MLIR compiler
infrastructure provides the compilation pipeline with multiple levels of
abstraction, preserving high-level abstractions throughout the compilation
pipeline, yet the current MLIR pipeline relies on the LLVM backend for Wasm
code generation, thereby inheriting LLVM's limitations.
  This paper presents a novel compilation pipeline for Wasm, featuring Wasm
dialects explicitly designed to represent high-level Wasm constructs within
MLIR. Our approach enables direct generation of high-level Wasm code from
corresponding high-level MLIR dialects without losing abstraction, providing a
modular and extensible way to incorporate high-level Wasm features. We
illustrate this extensibility through a case study that leverages Stack
Switching, a recently introduced high-level feature of Wasm. Performance
evaluations on PolyBench benchmarks show that our pipeline, benefiting from
optimizations within the MLIR and Wasm ecosystems, produces code with at most
7.7\% slower, and faster in some execution environments, compared to LLVM-based
compilers.

</details>


### [18] [Low Overhead Allocation Sampling in a Garbage Collected Virtual Machine](https://arxiv.org/abs/2506.16883)
*Christoph Jung,C. F. Bolz-Tereick*

Main category: cs.PL

TL;DR: The paper presents a low-overhead, sampling-based allocation profiler for PyPy, integrated into its garbage collector, allowing efficient profiling of allocation-heavy Python programs with a moderate performance cost.


<details>
  <summary>Details</summary>
Motivation: Allocation profiling in dynamically typed, allocation-heavy languages offers important insights not visible in traditional time-based profiling, but existing allocation profiling methods are highly inefficient due to the overhead of tracking every allocation.

Method: The authors introduce a sampling-based allocation profiler that integrates directly with the garbage collector in PyPy, enabling selective and efficient profiling with adjustable granularity.

Result: With a sampling period of 4 MB, the profiler incurs a maximum runtime overhead of 25% in the benchmarks tested.

Conclusion: A garbage-collector-integrated sampling allocation profiler can deliver useful allocation profiling data in PyPy with manageable and tunable overhead, making it a practical tool for performance analysis in allocation-heavy programs.

Abstract: Compared to the more commonly used time-based profiling, allocation profiling
provides an alternate view of the execution of allocation heavy dynamically
typed languages. However, profiling every single allocation in a program is
very inefficient. We present a sampling allocation profiler that is deeply
integrated into the garbage collector of PyPy, a Python virtual machine. This
integration ensures tunable low overhead for the allocation profiler, which we
measure and quantify. Enabling allocation sampling profiling with a sampling
period of 4 MB leads to a maximum time overhead of 25% in our benchmarks, over
un-profiled regular execution.

</details>
