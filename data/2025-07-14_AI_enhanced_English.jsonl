{"id": "2507.08759", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.08759", "abs": "https://arxiv.org/abs/2507.08759", "authors": ["Maximilian Dor\u00e9"], "title": "Dependent Multiplicities in Dependent Linear Type Theory", "comment": null, "summary": "We present a novel dependent linear type theory in which the multiplicity of\nsome variable - i.e., the number of times the variable can be used in a program\n- can depend on other variables. This allows us to give precise resource\nannotations to many higher-order functions that cannot be adequately typed in\nany other system. Inspired by the Dialectica translation, our typing discipline\nis obtained by embedding linear logic into dependent type theory and specifying\nhow the embedded logic interacts with the host theory. We can then use a\nstandard natural numbers type to obtain a quantitative typing system with\ndependent multiplicities. We characterise the semantics for our theory as a\ncombination of standard models of dependent type theory and linear logic. Our\nsystem can be added to any dependently typed language, which we demonstrate\nwith an implementation in Agda.", "AI": {"tldr": "This paper proposes a dependent linear type theory allowing variable usage multiplicity to depend on other variables, improving resource annotations for higher-order functions. The approach works by embedding linear logic into dependent type theory and is implemented in Agda.", "motivation": "Traditional type systems cannot adequately express resource usage (multiplicity) dependent on other variables, especially for higher-order functions. The paper aims to address this gap by allowing a variable's multiplicity to be dependent on other variables for more precise resource annotations.", "method": "The authors embed linear logic into dependent type theory and define the interaction between the embedded logic and the host theory. They use natural number types to represent quantitative multiplicities, characterize the semantics using standard models, and implement their theory in Agda.", "result": "The new type theory successfully expresses dependent multiplicities and refines resource typing for higher-order functions. The framework is generic enough to be integrated into any dependently typed language and is validated through an Agda implementation.", "conclusion": "The paper introduces a new dependent linear type theory that supports variables whose usage multiplicity can depend on other variables, enabling more precise resource management in higher-order functions. The system is shown to be compatible with existing dependently typed languages and has been implemented in Agda."}}
{"id": "2507.08796", "categories": ["cs.PL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08796", "abs": "https://arxiv.org/abs/2507.08796", "authors": ["Owen Lewis", "Neil Ghani", "Andrew Dudzik", "Christos Perivolaropoulos", "Razvan Pascanu", "Petar Veli\u010dkovi\u0107"], "title": "Filter Equivariant Functions: A symmetric account of length-general extrapolation on lists", "comment": "18 pages, 2 figures", "summary": "What should a function that extrapolates beyond known input/output examples\nlook like? This is a tricky question to answer in general, as any function\nmatching the outputs on those examples can in principle be a correct\nextrapolant. We argue that a \"good\" extrapolant should follow certain kinds of\nrules, and here we study a particularly appealing criterion for rule-following\nin list functions: that the function should behave predictably even when\ncertain elements are removed. In functional programming, a standard way to\nexpress such removal operations is by using a filter function. Accordingly, our\npaper introduces a new semantic class of functions -- the filter equivariant\nfunctions. We show that this class contains interesting examples, prove some\nbasic theorems about it, and relate it to the well-known class of map\nequivariant functions. We also present a geometric account of filter\nequivariants, showing how they correspond naturally to certain simplicial\nstructures. Our highlight result is the amalgamation algorithm, which\nconstructs any filter-equivariant function's output by first studying how it\nbehaves on sublists of the input, in a way that extrapolates perfectly.", "AI": {"tldr": "The authors define and study filter equivariant functions, which predictably handle input lists even when elements are removed. They connect this concept to map equivariant functions, give geometric interpretations, and present an algorithm for perfect extrapolation based on sublists.", "motivation": "The motivation is to determine what characteristics make a function a 'good' extrapolator\u2014specifically, how functions should behave beyond the set of known input/output pairs. Since any function matching known examples could be deemed correct, the goal is to set meaningful criteria for extrapolation, focusing on rule-following behavior in list functions.", "method": "The authors introduce and formalize the concept of filter equivariant functions\u2014functions whose outputs are predictably consistent even when some list elements are removed (modeled via filter operations in functional programming). The approach includes defining this class, exploring its mathematical properties, connecting it to map equivariant functions, and analyzing it from a geometric (simplicial structures) perspective. They culminate in proposing an algorithm\u2014the amalgamation algorithm\u2014to build outputs based on sublist analysis.", "result": "The paper shows that filter equivariant functions form an interesting and mathematically rich class. The authors prove foundational theorems, draw connections to established function classes, provide a geometric interpretation, and present the amalgamation algorithm, which enables perfect extrapolation using sublist behavior.", "conclusion": "The paper establishes the practical and theoretical significance of filter equivariant functions, both as an analytic tool for function behavior under element removal and as a basis for developing extrapolating algorithms. Their amalgamation algorithm exemplifies an optimal approach to constructing filter-equivariant extrapolants."}}
{"id": "2507.08061", "categories": ["cs.SE", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2507.08061", "abs": "https://arxiv.org/abs/2507.08061", "authors": ["Andrea Morales Coto", "Aditi Verma"], "title": "The State of Computational Science in Fission and Fusion Energy", "comment": null, "summary": "The tools used to engineer something are just as important as the thing that\nis actually being engineered. In fact, in many cases, the tools can indeed\ndetermine what is engineerable. In fusion and fission1 energy engineering,\nsoftware has become the dominant tool for design. For that reason, in 2024, for\nthe first time ever, we asked 103 computational scientists developing the codes\nused in fusion and fission energy about the problems they are attempting to\nsolve with their codes, the tools available to them to solve them, and their\nend to end developer experience with said tools.\n  The results revealed a changing tide in software tools in fusion and fission,\nwith more and more computational scientists preferring modern programming\nlanguages, open-source codes, and modular software. These trends represent a\npeek into what will happen 5 to 10 years in the future of nuclear engineering.\nSince the majority of our respondents belonged to US national labs and\nuniversities, these results hint at the most cutting-edge trends in the\nindustry. The insights included in the State of Computational Science in\nFission and Fusion Energy indicate a dramatic shift toward multiphysics codes,\na drop-off in the use of FORTRAN in favor of more modern languages like Python\nand C++, and ever-rising budgets for code development, at times reaching $50M\nin a single organization.\n  Our survey paints a future of nuclear engineering codes that is modular in\nnature, small in terms of compute, and increasingly prioritized by\norganizations. Access to our results in web form are available online.", "AI": {"tldr": "A 2024 survey of 103 computational scientists in fusion and fission energy reveals a shift toward modern, open-source, modular software tools, reduced reliance on FORTRAN, and increased investment in code development, highlighting the future direction of nuclear engineering software.", "motivation": "Software has become the dominant tool in fusion and fission energy engineering. The paper is motivated to understand what problems computational scientists are tackling with their codes and the tools and experiences involved, amidst a rapidly changing landscape.", "method": "A survey was conducted in 2024 with 103 computational scientists working in fusion and fission energy. The survey asked them about the problems they are addressing, the tools they use, and their overall developer experience.", "result": "Results show a significant shift in software preferences: there is increasing adoption of modern programming languages, open-source, and modular software, while usage of languages like FORTRAN is declining. The trend also highlights growing investment in code development and a move toward multiphysics codes.", "conclusion": "Nuclear engineering is witnessing a transformation in computational practices, with trends toward modular design, smaller compute requirements, and organizational prioritization of software development. These insights likely reflect the direction of the field over the next 5 to 10 years, especially within US labs and universities."}}
{"id": "2507.08149", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.08149", "abs": "https://arxiv.org/abs/2507.08149", "authors": ["Valerie Chen", "Ameet Talwalkar", "Robert Brennan", "Graham Neubig"], "title": "Code with Me or for Me? How Increasing AI Automation Transforms Developer Workflows", "comment": null, "summary": "Developers now have access to a growing array of increasingly autonomous AI\ntools to support software development. While numerous studies have examined\ndeveloper use of copilots, which can provide chat assistance or code\ncompletions, evaluations of coding agents, which can automatically write files\nand run code, still largely rely on static benchmarks without\nhumans-in-the-loop. In this work, we conduct the first academic study to\nexplore developer interactions with coding agents and characterize how more\nautonomous AI tools affect user productivity and experience, compared to\nexisting copilots. We evaluate two leading copilot and agentic coding\nassistants, GitHub Copilot and OpenHands, recruiting participants who regularly\nuse the former. Our results show agents have the potential to assist developers\nin ways that surpass copilots (e.g., completing tasks that humans might not\nhave accomplished before) and reduce the user effort required to complete\ntasks. However, there are challenges involved in enabling their broader\nadoption, including how to ensure users have an adequate understanding of agent\nbehaviors. Our results not only provide insights into how developer workflows\nchange as a result of coding agents but also highlight how user interactions\nwith agents differ from those with existing copilots, motivating a set of\nrecommendations for researchers building new agents. Given the broad set of\ndevelopers who still largely rely on copilot-like systems, our work highlights\nkey challenges of adopting more agentic systems into developer workflows.", "AI": {"tldr": "The paper compares current copilot tools and emerging autonomous coding agents, finds agents can outdo copilots and lessen user workload, but broader adoption is limited by challenges like user trust and workflow integration.", "motivation": "While copilot-like AI aids are widely adopted by developers, more autonomous 'coding agents' are emerging. However, most evaluations of these agents lack real human interaction and rely on benchmarks. This study aims to fill this gap and explore their actual impact on developer productivity and experience.", "method": "The authors conducted an academic user study comparing the developer experience with two leading AI coding tools: a copilot (GitHub Copilot) and a more autonomous agentic assistant (OpenHands). They recruited regular Copilot users and observed their interaction with both systems.", "result": "Agentic coding tools demonstrated greater assistance potential, sometimes achieving tasks beyond human or copilot capabilities and requiring less user effort. However, a lack of user understanding of agent behavior poses challenges to wider agent adoption.", "conclusion": "Coding agents can surpass traditional copilots in supporting developers, but their successful integration demands addressing usability and transparency concerns. The study highlights new workflow patterns and calls for better design and understanding to facilitate broader adoption."}}
{"id": "2507.08160", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.08160", "abs": "https://arxiv.org/abs/2507.08160", "authors": ["Ot\u00e1vio Cury", "Guilherme Avelino"], "title": "The Impact of Generative AI on Code Expertise Models: An Exploratory Study", "comment": null, "summary": "Generative Artificial Intelligence (GenAI) tools for source code generation\nhave significantly boosted productivity in software development. However, they\nalso raise concerns, particularly the risk that developers may rely heavily on\nthese tools, reducing their understanding of the generated code. We hypothesize\nthat this loss of understanding may be reflected in source code knowledge\nmodels, which are used to identify developer expertise. In this work, we\npresent an exploratory analysis of how a knowledge model and a Truck Factor\nalgorithm built upon it can be affected by GenAI usage. To investigate this, we\ncollected statistical data on the integration of ChatGPT-generated code into\nGitHub projects and simulated various scenarios by adjusting the degree of\nGenAI contribution. Our findings reveal that most scenarios led to measurable\nimpacts, indicating the sensitivity of current expertise metrics. This suggests\nthat as GenAI becomes more integrated into development workflows, the\nreliability of such metrics may decrease.", "AI": {"tldr": "The paper shows that using AI-generated code in projects can distort traditional ways of measuring developer expertise, suggesting a need to revise these metrics as GenAI tools become more common.", "motivation": "The widespread adoption of Generative AI tools in coding has improved productivity but raised concerns about developers' decreasing understanding of generated code and the potential effects on expert identification models.", "method": "The authors collected data on ChatGPT-generated code integrated into public GitHub projects and simulated different scenarios by varying the level of GenAI contributions to analyze the effects on a source code knowledge model and on a Truck Factor algorithm.", "result": "The analysis showed that, under most simulated scenarios, increased use of GenAI-generated code measurably affected the outputs of expertise metrics, making them less reliable.", "conclusion": "As GenAI becomes more prevalent in software development, existing metrics for assessing developer expertise and ownership (e.g., knowledge models and Truck Factor) may become less trustworthy due to the reduced link between code contributions and individual understanding."}}
{"id": "2507.08250", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.08250", "abs": "https://arxiv.org/abs/2507.08250", "authors": ["Yasaman Abedini", "Abbas Heydarnoori"], "title": "Leveraging Large Language Models for Classifying App Users' Feedback", "comment": null, "summary": "In recent years, significant research has been conducted into classifying\napplication (app) user feedback, primarily relying on supervised machine\nlearning algorithms. However, fine-tuning more generalizable classifiers based\non existing labeled datasets remains an important challenge, as creating large\nand accurately labeled datasets often requires considerable time and resources.\nIn this paper, we evaluate the capabilities of four advanced LLMs, including\nGPT-3.5-Turbo, GPT-4, Flan-T5, and Llama3-70b, to enhance user feedback\nclassification and address the challenge of the limited labeled dataset. To\nachieve this, we conduct several experiments on eight datasets that have been\nmeticulously labeled in prior research. These datasets include user reviews\nfrom app stores, posts from the X platform, and discussions from the public\nforums, widely recognized as representative sources of app user feedback. We\nanalyze the performance of various LLMs in identifying both fine-grained and\ncoarse-grained user feedback categories. Given the substantial volume of daily\nuser feedback and the computational limitations of LLMs, we leverage these\nmodels as an annotation tool to augment labeled datasets with general and\napp-specific data. This augmentation aims to enhance the performance of\nstate-of-the-art BERT-based classification models. Our findings indicate that\nLLMs when guided by well-crafted prompts, can effectively classify user\nfeedback into coarse-grained categories. Moreover, augmenting the training\ndataset with datasets labeled using the consensus of LLMs can significantly\nenhance classifier performance.", "AI": {"tldr": "LLMs such as GPT-4 and Llama3-70b can be used to augment labeled datasets for app user feedback classification, improving the performance of existing BERT-based models and reducing the reliance on costly human annotation.", "motivation": "Classifying app user feedback is essential for app improvement, but building large, high-quality labeled datasets for supervised machine learning is costly and time-consuming. The study seeks to find more scalable, generalizable methods for handling this problem.", "method": "The paper evaluates four large language models (LLMs) \u2014 GPT-3.5-Turbo, GPT-4, Flan-T5, and Llama3-70b \u2014 on eight existing, carefully labeled datasets of user feedback drawn from app stores, social platforms, and forums. The models are assessed for their abilities to classify feedback into both specific (fine-grained) and general (coarse-grained) categories. Additionally, the researchers leverage LLMs as annotation tools to expand (augment) labeled datasets, which are then used to train BERT-based classification models.", "result": "LLMs, when directed by high-quality prompts, accurately classify user feedback at the coarse-grained level. Furthermore, using LLM-labeled augmented data to expand training sets meaningfully improves the performance of state-of-the-art BERT classifiers.", "conclusion": "Well-guided LLMs can serve as effective tools for classifying and labeling user feedback data, helping to overcome the challenge of limited labeled datasets. LLM-augmented data enhances the classification performance of existing machine learning models."}}
{"id": "2507.08467", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.08467", "abs": "https://arxiv.org/abs/2507.08467", "authors": ["Youshuai Tan", "Zhanwei Zhang", "Jinfu Chen", "Zishuo Ding", "Jifeng Xuan", "Weiyi Shang"], "title": "Computing Floating-Point Errors by Injecting Perturbations", "comment": "arXiv admin note: text overlap with arXiv:2412.20804", "summary": "Floating-point programs form the foundation of modern science and\nengineering, providing the essential computational framework for a wide range\nof applications, such as safety-critical systems, aerospace engineering, and\nfinancial analysis. Floating-point errors can lead to severe consequences.\nAlthough floating-point errors widely exist, only a subset of inputs may\ntrigger significant errors in floating-point programs. Therefore, it is crucial\nto determine whether a given input could produce such errors. Researchers tend\nto take the results of high-precision floating-point programs as oracles for\ndetecting floating-point errors, which introduces two main limitations: (1)\ndifficulty of implementation and (2) prolonged execution time. The two recent\ntools, ATOMU and FPCC, can partially address these issues. However, ATOMU\nsuffers from false positives; while FPCC, though eliminating false positives,\noperates at a considerably slower speed.\n  To address these two challenges, we propose a novel approach named\nPI-detector to computing floating-point errors effectively and efficiently. Our\napproach is based on the observation that floating-point errors stem from large\ncondition numbers in atomic operations (such as addition and subtraction),\nwhich then propagate and accumulate. PI-detector injects small perturbations\ninto the operands of individual atomic operations within the program and\ncompares the outcomes of the original program with the perturbed version to\ncompute floating-point errors. We evaluate PI-detector with datasets from ATOMU\nand HSED, as well as a complex linear system-solving program. Experimental\nresults demonstrate that PI-detector can perform efficient and accurate\nfloating-point error computation.", "AI": {"tldr": "PI-detector is a new tool that efficiently and accurately detects floating-point errors in programs by injecting small perturbations and comparing outcomes, outperforming existing solutions in both speed and correctness.", "motivation": "Floating-point errors in programs can result in serious consequences in fields like safety-critical systems and engineering. However, not all inputs trigger significant errors, and current methods to detect such errors are either hard to implement, slow, or prone to false positives.", "method": "The paper proposes PI-detector, a novel approach that injects small perturbations into the operands of atomic operations (like addition and subtraction) in floating-point programs. By comparing the outcome of the original and perturbed executions, the method estimates floating-point errors. The approach is then evaluated with standard datasets and a complex linear system program.", "result": "The experimental results show that PI-detector achieves both efficiency and accuracy in computing floating-point errors, addressing the limitations of existing tools like ATOMU (false positives) and FPCC (slow performance).", "conclusion": "PI-detector is an effective and efficient solution for detecting significant floating-point errors in programs, overcoming the shortcomings of previous solutions by precisely and swiftly identifying errors for specific inputs."}}
{"id": "2507.08523", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.08523", "abs": "https://arxiv.org/abs/2507.08523", "authors": ["Yilun Wang", "Pengfei Chen", "Haiyu Huang", "Zilong He", "Gou Tan", "Chuanfu Zhang", "Jingkai He", "Zibin Zheng"], "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via ICL-oriented Prefix Caching", "comment": null, "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.", "AI": {"tldr": "InferLog is a new method to optimize LLM inference for log parsing, greatly improving inference speed while maintaining accuracy and addressing production deployment issues such as latency, throughput, and privacy.", "motivation": "Modern software systems produce vast amounts of runtime logs. Efficient and accurate log parsing is crucial for tasks like anomaly detection and root cause analysis. Existing LLM-based parsers succeed in accuracy but struggle with production deployment due to privacy concerns with commercial LLMs and challenges meeting strict latency and throughput demands in large-scale environments.", "method": "The paper introduces InferLog, an LLM inference optimization technique for online log parsing. InferLog includes: (1) A Prefix-aware In-Context Learning (ICL) Refinement policy for better prefix caching, and (2) a meta-learning-based configuration tuning pipeline for rapid, optimal LLM scheduling according to dynamic workloads.", "result": "Experiments using the Loghub dataset and vLLM show that InferLog surpasses existing inference optimization methods. It greatly speeds up state-of-the-art LLM-based log parsers, all without sacrificing parsing accuracy.", "conclusion": "InferLog addresses the main bottleneck in online LLM-based log parsing\u2014 inference efficiency\u2014 and demonstrates significant improvements over previous solutions in speed while retaining accuracy."}}
{"id": "2507.08594", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.08594", "abs": "https://arxiv.org/abs/2507.08594", "authors": ["Fernando Ayach", "Vitor Lameir\u00e3o", "Raul Le\u00e3o", "Jerfferson Felizardo", "Rafael Sobrinho", "Vanessa Borges", "Patr\u00edcia Matsubara", "Awdren Font\u00e3o"], "title": "Generating Proto-Personas through Prompt Engineering: A Case Study on Efficiency, Effectiveness and Empathy", "comment": "12 pages; 2 figures; Preprint with the original submission accepted\n  for publication at 39th Brazilian Symposium on Software Engineering (SBES)", "summary": "Proto-personas are commonly used during early-stage Product Discovery, such\nas Lean Inception, to guide product definition and stakeholder alignment.\nHowever, the manual creation of proto-personas is often time-consuming,\ncognitively demanding, and prone to bias. In this paper, we propose and\nempirically investigate a prompt engineering-based approach to generate\nproto-personas with the support of Generative AI (GenAI). Our goal is to\nevaluate the approach in terms of efficiency, effectiveness, user acceptance,\nand the empathy elicited by the generated personas. We conducted a case study\nwith 19 participants embedded in a real Lean Inception, employing a qualitative\nand quantitative methods design. The results reveal the approach's efficiency\nby reducing time and effort and improving the quality and reusability of\npersonas in later discovery phases, such as Minimum Viable Product (MVP)\nscoping and feature refinement. While acceptance was generally high, especially\nregarding perceived usefulness and ease of use, participants noted limitations\nrelated to generalization and domain specificity. Furthermore, although\ncognitive empathy was strongly supported, affective and behavioral empathy\nvaried significantly across participants. These results contribute novel\nempirical evidence on how GenAI can be effectively integrated into software\nProduct Discovery practices, while also identifying key challenges to be\naddressed in future iterations of such hybrid design processes.", "AI": {"tldr": "This paper shows that using Generative AI to create proto-personas during product discovery can save time and improve quality, but attention is needed to avoid overly generic results and ensure domain relevance and empathy.", "motivation": "Manual creation of proto-personas during early-stage Product Discovery is time-consuming, cognitively demanding, and prone to bias. The paper seeks to address these challenges by leveraging Generative AI for persona generation.", "method": "A prompt engineering-based approach using Generative AI was proposed. The authors conducted a case study with 19 participants engaged in a real Lean Inception process, employing both qualitative and quantitative research methods.", "result": "The GenAI-supported approach reduced time and effort required for persona creation, improved the quality and reusability of personas in subsequent phases, and was generally well accepted regarding usefulness and ease of use. However, limitations related to persona generalization and lack of domain specificity were noted. Empathy support was high for cognitive empathy but varied for affective and behavioral empathy.", "conclusion": "GenAI can be efficiently and effectively integrated into software Product Discovery for generating proto-personas, offering significant advantages in productivity and quality, but challenges remain\u2014especially regarding generalization, domain specificity, and eliciting different types of empathy. Future efforts should address these limitations to enhance hybrid design processes."}}
{"id": "2507.08627", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.08627", "abs": "https://arxiv.org/abs/2507.08627", "authors": ["Chi-en Amy Tai", "Pengyu Nie", "Lukasz Golab", "Alexander Wong"], "title": "NL in the Middle: Code Translation with LLMs and Intermediate Representations", "comment": null, "summary": "Studies show that large language models (LLMs) produce buggy code\ntranslations. One avenue to improve translation accuracy is through\nintermediate representations, which could provide structured insights to guide\nthe model's understanding. We explore whether code translation using LLMs can\nbenefit from intermediate representations via natural language (NL) and\nabstract syntax trees (ASTs). Since prompt engineering greatly affects LLM\nperformance, we consider several ways to integrate these representations, from\none-shot to chain-of-thought (CoT) prompting. Using Open Gpt4 8X7B and\nspecialized StarCoder and CodeGen models on popular code translation benchmarks\n(CodeNet and AVATAR), we find that CoT with an intermediate NL summary performs\nbest, with an increase of 13.8% and 6.7%, respectively, in successful\ntranslations for the best-performing model (Open Gpt4 8X7B) compared to the\nzero-shot prompt.", "AI": {"tldr": "Using chain-of-thought prompting with natural language summaries as intermediate steps significantly increases the accuracy of code translation by large language models compared to standard prompting approaches.", "motivation": "Large language models often generate buggy code translations. The motivation is to improve translation accuracy, potentially by introducing intermediate representations to guide the model\u2019s understanding.", "method": "The study explores the effect of integrating intermediate representations, specifically natural language (NL) summaries and abstract syntax trees (ASTs), into LLM code translation tasks. Different integration techniques, such as one-shot prompting and chain-of-thought (CoT) prompting, are tested. Experiments are conducted using Open Gpt4 8X7B, StarCoder, and CodeGen on established code translation benchmarks (CodeNet and AVATAR).", "result": "The results reveal that chain-of-thought prompting with an intermediate NL summary achieves the highest performance, boosting successful translations by 13.8% and 6.7% for Open Gpt4 8X7B, compared to zero-shot prompting.", "conclusion": "Incorporating intermediate natural language summaries using chain-of-thought prompting significantly improves LLM code translation performance. This approach offers a promising direction for better code translation outcomes with large language models."}}
{"id": "2507.08671", "categories": ["cs.SE", "D.2.3; D.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.08671", "abs": "https://arxiv.org/abs/2507.08671", "authors": ["Hua Ge", "Juan Zhai", "Minxue Pan", "Fusen He", "Ziyue Tan"], "title": "LLMCup: Ranking-Enhanced Comment Updating with LLMs", "comment": "13 pages, 10 figures", "summary": "While comments are essential for enhancing code readability and\nmaintainability in modern software projects, developers are often motivated to\nupdate code but not comments, leading to outdated or inconsistent documentation\nthat hinders future understanding and maintenance. Recent approaches such as\nCUP and HebCup have attempted automatic comment updating using neural\nsequence-to-sequence models and heuristic rules, respectively. However, these\nmethods can miss or misinterpret crucial information during comment updating,\nresulting in inaccurate comments, and they often struggle with complex update\nscenarios. Given these challenges, a promising direction lies in leveraging\nlarge language models (LLMs), which have shown impressive performance in\nsoftware engineering tasks such as comment generation, code synthesis, and\nprogram repair. This suggests their strong potential to capture the logic\nbehind code modifications - an ability that is crucial for the task of comment\nupdating. Nevertheless, selecting an appropriate prompt strategy for an LLM on\neach update case remains challenging. To address this, we propose a novel\ncomment updating framework, LLMCup, which first uses multiple prompt strategies\nto provide diverse candidate updated comments via an LLM, and then employs a\nranking model, CupRank, to select the best candidate as final updated comment.\nExperimental results demonstrate the effectiveness of LLMCup, with improvements\nover state-of-the-art baselines (CUP and HebCup) by 49.0%-116.9% in Accuracy,\n10.8%-20% in BLEU-4, 4.6% in METEOR, 0.9%-1.9% in F1, and 2.1%-3.4% in\nSentenceBert similarity. Furthermore, a user study shows that comments updated\nby LLMCup sometimes surpass human-written updates, highlighting the importance\nof incorporating human evaluation in comment quality assessment.", "AI": {"tldr": "This paper introduces LLMCup, a novel framework using large language models and strategic prompting for automatic code comment updating. It significantly outperforms previous baseline methods and can sometimes exceed human-written updates, demonstrating the promise of LLM-based solutions for code documentation tasks.", "motivation": "Comments are crucial for code readability and maintainability, but developers often neglect updating them when modifying code, leading to inconsistent documentation that hampers future maintenance.", "method": "The paper proposes LLMCup, a framework that utilizes multiple prompting strategies to generate diverse comment update candidates with a large language model (LLM). It then employs a ranking model (CupRank) to select the best candidate for the final comment update.", "result": "LLMCup outperforms existing methods (CUP and HebCup) in several evaluation metrics: 49.0%-116.9% in Accuracy, 10.8%-20% in BLEU-4, 4.6% in METEOR, 0.9%-1.9% in F1, and 2.1%-3.4% in SentenceBert similarity. User studies also indicate LLMCup can sometimes produce better outputs than human updates.", "conclusion": "LLMCup improves the accuracy and quality of automated comment updating, addressing the limitations of prior approaches and even rivaling or surpassing human performance in some cases. Human assessment remains important for evaluating comment quality."}}
{"id": "2507.08730", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08730", "abs": "https://arxiv.org/abs/2507.08730", "authors": ["Zezhen Xiang", "Jingzhi Gong", "Tao Chen"], "title": "Dually Hierarchical Drift Adaptation for Online Configuration Performance Learning", "comment": "Accepted by ICSE 2026", "summary": "Modern configurable software systems need to learn models that correlate\nconfiguration and performance. However, when the system operates in dynamic\nenvironments, the workload variations, hardware changes, and system updates\nwill inevitably introduce concept drifts at different levels - global drifts,\nwhich reshape the performance landscape of the entire configuration space; and\nlocal drifts, which only affect certain sub-regions of that space. As such,\nexisting offline and transfer learning approaches can struggle to adapt to\nthese implicit and unpredictable changes in real-time, rendering configuration\nperformance learning challenging. To address this, we propose DHDA, an online\nconfiguration performance learning framework designed to capture and adapt to\nthese drifts at different levels. The key idea is that DHDA adapts to both the\nlocal and global drifts using dually hierarchical adaptation: at the upper\nlevel, we redivide the data into different divisions, within each of which the\nlocal model is retrained, to handle global drifts only when necessary. At the\nlower level, the local models of the divisions can detect local drifts and\nadapt themselves asynchronously. To balance responsiveness and efficiency, DHDA\ncombines incremental updates with periodic full retraining to minimize\nredundant computation when no drifts are detected. Through evaluating eight\nsoftware systems and against state-of-the-art approaches, we show that DHDA\nachieves considerably better accuracy and can effectively adapt to drifts with\nup to 2x improvements, while incurring reasonable overhead and is able to\nimprove different local models in handling concept drift.", "AI": {"tldr": "DHDA is a new framework for online configuration performance learning that adaptively handles both global and local performance drifts in dynamic software systems, outperforming existing methods in accuracy and adaptability while keeping computational costs reasonable.", "motivation": "Modern configurable software systems face the challenge of unpredictable and dynamic environments, where changes in workload, hardware, or system updates cause shifts ('concept drifts') in how configurations impact performance. Existing approaches struggle to adapt in real-time to these global and local drifts, making effective configuration performance prediction difficult.", "method": "The paper proposes DHDA, an online adaptive learning framework that addresses both global and local concept drifts in configuration performance prediction. DHDA uses a hierarchical adaptation scheme: at the upper level, it repartitions the data and retrains local models to respond to detected global drifts; at the lower level, local models detect and asynchronously adapt to local drifts. DHDA balances efficiency and adaptability through a mix of incremental updates and periodic full retraining, minimizing unnecessary computation.", "result": "DHDA was evaluated on eight different software systems and compared with state-of-the-art methods. The results show DHDA achieves much higher prediction accuracy and adapts more effectively to concept drifts (up to 2x improvement), with reasonable computational overhead and improved handling of local drifts.", "conclusion": "The proposed DHDA framework effectively addresses both global and local concept drifts in configurable software systems, providing better accuracy and adaptability for configuration performance prediction in dynamic environments, with manageable overhead."}}
