<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Fuzzing: Randomness? Reasoning! Efficient Directed Fuzzing via Large Language Models](https://arxiv.org/abs/2507.22065)
*Xiaotao Feng,Xiaogang Zhu,Kun Hu,Jincheng Wang,Yingjie Cao,Guang Gong,Jianfeng Pan*

Main category: cs.SE

TL;DR: RandLuzz uses LLMs to create more targeted seeds and mutators for fuzzing, greatly accelerating bug discovery (up to 4.8× faster) compared to existing tools by removing much of the randomness that slows down traditional fuzzing.


<details>
  <summary>Details</summary>
Motivation: Fuzzing is effective at exposing software bugs due to its use of randomness, but this same randomness leads to inefficiency and slow bug discovery. Even directed fuzzers still struggle with the negative effects of randomness, particularly in the choice of input seeds and mutator strategies, which are closely related to whether bugs are triggered.

Method: The paper proposes RandLuzz, a system that leverages large language models (LLMs) to reduce randomness in fuzzing. RandLuzz uses LLMs to generate high-quality, targeted seeds by analyzing function call chains or program functionality, and to construct bug-specific mutators by analyzing bugs and making mutation suggestions. This integration aims to create seeds and mutators that are more likely to expose bugs quickly.

Result: When evaluated against four state-of-the-art directed fuzzers (AFLGo, Beacon, WindRanger, SelectFuzz), RandLuzz significantly improved efficiency, with fuzzers using RandLuzz-generated seeds achieving 2.1× to 4.8× speedup over traditional seeds. On specific bugs, RandLuzz achieved up to 2.7× faster exposure than the second-best tool, and it was able to expose 8 bugs within 60 seconds.

Conclusion: Integrating LLMs with directed fuzzing as done in RandLuzz substantially improves the efficiency of bug exposure by reducing randomness in seed and mutator selection. This demonstrates the potential of LLMs to guide fuzzing in a targeted and highly efficient manner.

Abstract: Fuzzing is highly effective in detecting bugs due to the key contribution of
randomness. However, randomness significantly reduces the efficiency of
fuzzing, causing it to cost days or weeks to expose bugs. Even though directed
fuzzing reduces randomness by guiding fuzzing towards target buggy locations,
the dilemma of randomness still challenges directed fuzzers. Two critical
components, which are seeds and mutators, contain randomness and are closely
tied to the conditions required for triggering bugs. Therefore, to address the
challenge of randomness, we propose to use large language models (LLMs) to
remove the randomness in seeds and reduce the randomness in mutators. With
their strong reasoning and code generation capabilities, LLMs can be used to
generate reachable seeds that target pre-determined locations and to construct
bug-specific mutators tailored for specific bugs. We propose RandLuzz, which
integrates LLMs and directed fuzzing, to improve the quality of seeds and
mutators, resulting in efficient bug exposure. RandLuzz analyzes function call
chain or functionality to guide LLMs in generating reachable seeds. To
construct bug-specific mutators, RandLuzz uses LLMs to perform bug analysis,
obtaining information such as bug causes and mutation suggestions, which
further help generate code that performs bug-specific mutations. We evaluate
RandLuzz by comparing it with four state-of-the-art directed fuzzers, AFLGo,
Beacon, WindRanger, and SelectFuzz. With RandLuzz-generated seeds, the fuzzers
achieve an average speedup ranging from 2.1$\times$ to 4.8$\times$ compared to
using widely-used initial seeds. Additionally, when evaluated on individual
bugs, RandLuzz achieves up to a 2.7$\times$ speedup compared to the
second-fastest exposure. On 8 bugs, RandLuzz can even expose them within 60
seconds.

</details>


### [2] [Automated Test Data Generation for Enterprise Protobuf Systems: A Metaclass-Enhanced Statistical Approach](https://arxiv.org/abs/2507.22070)
*Y. Du*

Main category: cs.SE

TL;DR: This paper proposes a Python-based framework combining dynamic typing and statistical analysis to automate test data generation for complex protobuf structures in enterprise systems, achieving significant gains in speed and coverage over existing methods.


<details>
  <summary>Details</summary>
Motivation: Performance testing of large-scale enterprise systems using Protocol Buffers (protobuf) is challenging, especially with complex, deeply nested data structures found in business interfaces. Traditional test data generation methods are not effective for these intricate schemas.

Method: The paper introduces a novel framework that uses Python's metaclass system for dynamic type manipulation, analyzes production logs to extract realistic value domains, employs automatic schema introspection, performs statistical value distribution analysis, and uses recursive descent algorithms to manage deeply nested protobuf structures.

Result: Experiments on three real-world enterprise systems showed that the new framework reduced test data preparation time by up to 95% and improved test coverage by 80% compared to traditional approaches. It effectively handled protobuf schemas with up to 15 levels of nesting and could generate over 100,000 test cases in seconds.

Conclusion: The proposed framework dramatically improves both the efficiency and effectiveness of test data generation for complex enterprise protobuf systems, making it a practical solution for challenging performance testing needs.

Abstract: Large-scale enterprise systems utilizing Protocol Buffers (protobuf) present
significant challenges for performance testing, particularly when targeting
intermediate business interfaces with complex nested data structures.
Traditional test data generation approaches are inadequate for handling the
intricate hierarchical and graph-like structures inherent in enterprise
protobuf schemas. This paper presents a novel test data generation framework
that leverages Python's metaclass system for dynamic type enhancement and
statistical analysis of production logs for realistic value domain extraction.
Our approach combines automatic schema introspection, statistical value
distribution analysis, and recursive descent algorithms for handling deeply
nested structures. Experimental evaluation on three real-world enterprise
systems demonstrates up to 95\% reduction in test data preparation time and
80\% improvement in test coverage compared to existing approaches. The
framework successfully handles protobuf structures with up to 15 levels of
nesting and generates comprehensive test suites containing over 100,000 test
cases within seconds.

</details>


### [3] [TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories](https://arxiv.org/abs/2507.22086)
*Honghua Dong,Jiacheng Yang,Xun Deng,Yuhe Jiang,Gennady Pekhimenko,Fan Long,Xujie Si*

Main category: cs.SE

TL;DR: TypyBench is a new benchmark for evaluating LLMs' type inference across whole Python repositories. It shows that while LLMs are okay at predicting similar types, they often fail at being consistent with types project-wide, especially with complex or nested types. Improving repository-level type consistency should be a key focus going forward.


<details>
  <summary>Details</summary>
Motivation: Type inference in dynamic languages, particularly Python, is difficult, and while large language models show promise in code understanding, their abilities at type inference (especially at the repository scale) are not well evaluated.

Method: The authors introduce TypyBench, a specialized benchmark to evaluate LLMs' type inference abilities on full Python repositories. It includes two metrics: TypeSim (measuring semantic similarity of predicted and ground truth types) and TypeCheck (assessing type consistency at repository level). They apply TypyBench to various LLMs using a dataset of 50 high-quality Python repos.

Result: LLMs perform fairly well on TypeSim (showing decent similarity between predicted and actual types) but struggle with complex/nested types and with maintaining type consistency across entire repositories, leading to significant errors.

Conclusion: Current LLMs are limited in repository-level type consistency, even if they achieve reasonable type similarity on simpler cases. Future research should focus more on ensuring type consistency across projects rather than only improving type similarity. TypyBench provides an effective way to evaluate and encourage this shift.

Abstract: Type inference for dynamic languages like Python is a persistent challenge in
software engineering. While large language models (LLMs) have shown promise in
code understanding, their type inference capabilities remain underexplored. We
introduce TypyBench, a benchmark designed to evaluate LLMs' type inference
across entire Python repositories. TypyBench features two novel metrics:
TypeSim, which captures nuanced semantic relationships between predicted and
ground truth types, and TypeCheck, which assesses type consistency across
codebases. Our evaluation of various LLMs on a curated dataset of 50
high-quality Python repositories reveals that, although LLMs achieve decent
TypeSim scores, they struggle with complex nested types and exhibit significant
type consistency errors. These findings suggest that future research should
shift focus from improving type similarity to addressing repository-level
consistency. TypyBench provides a foundation for this new direction, offering
insights into model performance across different type complexities and usage
contexts. Our code and data are available at
https://github.com/typybench/typybench.

</details>


### [4] [RedCoder: Automated Multi-Turn Red Teaming for Code LLMs](https://arxiv.org/abs/2507.22063)
*Wenjie Jacky Mo,Qin Liu,Xiaofei Wen,Dongwon Jung,Hadi Askari,Wenxuan Zhou,Zhe Zhao,Muhao Chen*

Main category: cs.SE

TL;DR: RedCoder is an automated, multi-turn red-teaming agent that effectively uncovers vulnerabilities in code LLMs, surpassing prior methods in scalability and vulnerability induction through dynamic, realistic adversarial dialogue.


<details>
  <summary>Details</summary>
Motivation: Existing red-teaming approaches for code LLMs require significant human effort and typically focus on single-turn interactions, neglecting the multi-turn, interactive nature of real-world AI-assisted programming. This limits their scalability and effectiveness for identifying vulnerabilities in code generated by LLMs.

Method: The proposed method, RedCoder, is a red-teaming agent designed to autonomously engage code LLMs in multi-turn conversations to elicit vulnerable code. RedCoder is built by first simulating adversarial, multi-agent interactions to produce prototype conversations and reusable attack strategies, then fine-tuning an LLM on these prototypes. During deployment, RedCoder dynamically applies relevant attack strategies in conversations to induce vulnerabilities.

Result: Experiments show that RedCoder outperforms existing single-turn and multi-turn red-team methods in generating vulnerable code from several Code LLMs, demonstrating superior effectiveness and scalability.

Conclusion: RedCoder represents a scalable, effective tool for robustly evaluating and probing the security boundaries of code-generation LLMs through realistic, multi-turn adversarial interactions.

Abstract: Large Language Models (LLMs) for code generation (i.e., Code LLMs) have
demonstrated impressive capabilities in AI-assisted software development and
testing. However, recent studies have shown that these models are prone to
generating vulnerable or even malicious code under adversarial settings.
Existing red-teaming approaches rely on extensive human effort, limiting their
scalability and practicality, and generally overlook the interactive nature of
real-world AI-assisted programming, which often unfolds over multiple turns. To
bridge these gaps, we present RedCoder, a red-teaming agent that engages victim
models in multi-turn conversation to elicit vulnerable code. The pipeline to
construct RedCoder begins with a multi-agent gaming process that simulates
adversarial interactions, yielding a set of prototype conversations and an
arsenal of reusable attack strategies. We then fine-tune an LLM on these
prototype conversations to serve as the backbone of RedCoder. Once deployed,
RedCoder autonomously engages Code LLMs in multi-turn conversations,
dynamically retrieving relevant strategies from the arsenal to steer the
dialogue toward vulnerability-inducing outputs. Experiments across multiple
Code LLMs show that our approach outperforms prior single-turn and multi-turn
red-team methods in inducing vulnerabilities in code generation, offering a
scalable and effective tool for evaluating the security boundaries of modern
code-generation systems.

</details>


### [5] [Machine Learning Experiences: A story of learning AI for use in enterprise software testing that can be used by anyone](https://arxiv.org/abs/2507.22064)
*Michael Cohoon,Debbie Furman*

Main category: cs.SE

TL;DR: This paper shows how following a structured, CRISP-DM-like workflow makes it easier for anyone to apply machine learning to software testing projects successfully.


<details>
  <summary>Details</summary>
Motivation: The paper aims to guide people interested in leveraging machine learning for software testing by demonstrating a practical workflow.

Method: A step-by-step process, mirroring the CRISP-DM methodology, is used to detail ML project execution: data gathering, cleaning, feature engineering, train-test split, model selection, training, testing, and evaluation.

Result: By using this systematic workflow, practitioners can successfully implement ML solutions in any project.

Conclusion: A well-defined, repeatable workflow enables effective use of ML techniques in software testing and potentially other domains.

Abstract: This paper details the machine learning (ML) journey of a group of people
focused on software testing. It tells the story of how this group progressed
through a ML workflow (similar to the CRISP-DM process). This workflow consists
of the following steps and can be used by anyone applying ML techniques to a
project: gather the data; clean the data; perform feature engineering on the
data; splitting the data into two sets, one for training and one for testing;
choosing a machine learning model; training the model; testing the model and
evaluating the model performance. By following this workflow, anyone can
effectively apply ML to any project that they are doing.

</details>


### [6] [CodableLLM: Automating Decompiled and Source Code Mapping for LLM Dataset Generation](https://arxiv.org/abs/2507.22066)
*Dylan Manuel,Paul Rad*

Main category: cs.SE

TL;DR: CodableLLM automates the alignment of decompiled and source code, making high-quality dataset generation for code language models easier and more effective than existing tools.


<details>
  <summary>Details</summary>
Motivation: Generating large, high-quality datasets for code understanding and generation is challenging, especially when trying to accurately align decompiled binaries to their original source code, which is crucial for training effective code-focused language models.

Method: The authors introduce CodableLLM, a Python framework that automates the mapping of decompiled functions to their source counterparts. The framework supports various programming languages, integrates with standard decompilers and parsers, and streamlines the data creation process for code LLMs.

Result: CodableLLM is shown to perform well in automating dataset creation for code-language models, providing better alignment between decompiled and source code compared to existing tools.

Conclusion: CodableLLM offers a robust and efficient solution for generating high-quality, well-aligned datasets, supporting the development of advanced code-focused LLMs.

Abstract: The generation of large, high-quality datasets for code understanding and
generation remains a significant challenge, particularly when aligning
decompiled binaries with their original source code. To address this, we
present CodableLLM, a Python framework designed to automate the creation and
curation of datasets by mapping decompiled functions to their corresponding
source functions. This process enhances the alignment between decompiled and
source code representations, facilitating the development of large language
models (LLMs) capable of understanding and generating code across multiple
abstraction levels. CodableLLM supports multiple programming languages and
integrates with existing decompilers and parsers to streamline dataset
generation. This paper presents the design and implementation of CodableLLM,
evaluates its performance in dataset creation, and compares it to existing
tools in the field. The results demonstrate that CodableLLM offers a robust and
efficient solution for generating datasets tailored for code-focused LLMS.

</details>


### [7] [Analyzing and Evaluating the Behavior of Git Diff and Merge](https://arxiv.org/abs/2507.22071)
*Niels Glodny*

Main category: cs.SE

TL;DR: The paper thoroughly investigates Git's diff and merge algorithms, revealing several unexpected behaviors, such as excessive changes flagged by histogram diff, expensive merge computations, non-commutative operations, and unpredictable merge results. These findings emphasize the complexity and sometimes surprising outcomes when using Git for collaboration.


<details>
  <summary>Details</summary>
Motivation: Git's diff and merge algorithms are widely used but not thoroughly understood, and their behaviors can occasionally be counterintuitive or problematic. By investigating these algorithms, the paper aims to expose their complexities and weaknesses, which is valuable for developers and researchers who may want to use or improve upon them in other contexts.

Method: The paper documents and analyzes the main functionalities of Git, particularly how diffs are computed and used in merges. It details observed behaviors through examples and possibly experimental investigation, focusing on revealing unexpected or pathological scenarios.

Result: The study identifies several non-obvious behaviors: the histogram diff algorithm can flag extensive changes due to single-line edits; Git's default merge strategy (ort) can be computationally expensive in certain histories; merges and rebases in Git are not commutative; and some merge results depend on the diff algorithm. Additionally, some merge conflicts do not trigger warnings but merge both changes in no guaranteed order.

Conclusion: Git's diff and merge algorithms exhibit unexpected behaviors and limitations that are not commonly documented or understood. Such insights are crucial for users relying on these tools and for developers aiming to design better collaborative systems or improve Git itself.

Abstract: Despite being widely used, the algorithms that enable collaboration with Git
are not well understood. The diff and merge algorithms are particularly
interesting, as they could be applied in other contexts. In this thesis, I
document the main functionalities of Git: how diffs are computed, how they are
used to run merges, and how merges enable more complex operations. In the
process, I show multiple unexpected behaviors in Git, including the following:
The histogram diff algorithm has pathological cases where a single-line change
can cause the entire rest of the file to be marked as changed. The default
merge strategy (ort) can result in merges requiring exponential time in the
number of commits in the history. Merges and rebases are not commutative, and
even when merges do not result in a conflict, the result is not specified but
depends on the diff algorithm used. And finally, sometimes when two sides of a
merge add different lines at the same position, the result is not a conflict,
but a merge containing both changes after each other, in arbitrary order.

</details>


### [8] [CodeEvo: Interaction-Driven Synthesis of Code-centric Data through Hybrid and Iterative Feedback](https://arxiv.org/abs/2507.22080)
*Qiushi Sun,Jinyang Gong,Lei Li,Qipeng Guo,Fei Yuan*

Main category: cs.SE

TL;DR: CodeEvo uses two interacting LLM agents and hybrid feedback to generate superior code data, resulting in better code generation performance than existing methods.


<details>
  <summary>Details</summary>
Motivation: High-quality instruction-code pair data is vital for training LLMs for code generation, but manual curation is expensive and difficult to scale. Existing synthetic data methods rely on augmentation or heuristics and suffer from poor validation, resulting in low-quality and ungrounded data.

Method: The paper introduces CodeEvo, a novel framework where two LLM agents (a Coder and a Reviewer) interact iteratively. The Coder generates code and test cases from instructions, while the Reviewer provides new instructions and feedback. The process incorporates a hybrid feedback system that merges compiler checks with LLM-based feedback for automated quality control.

Result: Models trained with data synthesized through CodeEvo outperform those trained on existing baselines across a range of code generation tasks. Detailed analyses show improved diversity, grounding, and effectiveness in synthesized datasets.

Conclusion: CodeEvo offers a scalable, automated framework for producing high-quality instruction-code pairs by leveraging agent interaction and a hybrid feedback mechanism. This results in superior performance and better-quality training data for code generation LLMs.

Abstract: Acquiring high-quality instruction-code pairs is essential for training Large
Language Models (LLMs) for code generation. Manually curated data is expensive
and inherently limited in scale, motivating the development of code-centric
synthesis methods. Yet, current approaches either focus on augmenting existing
code or rely on predefined heuristics, both lacking rigorous data validation,
which results in synthetic data that is ungrounded, repetitive, or overly
simplistic. Inspired by collaborative programming practices, we propose
CodeEvo, a framework that synthesizes code data through iterative interactions
between two LLM agents: a Coder, which generates candidate code and test cases
based on given instructions, and a Reviewer, which guides the synthesis process
by producing new instructions and feedback. We further introduce a hybrid
feedback mechanism that combines compiler determinism with the generative
flexibility of agents, enabling automatic quality control throughout synthesis.
Extensive experiments demonstrate that models fine-tuned on CodeEvo data
significantly outperform established baselines across code generation
benchmarks with various difficulties. In-depth analyses further provide
insights from multiple perspectives into effective code-centric data synthesis.

</details>


### [9] [BOOP: Write Right Code](https://arxiv.org/abs/2507.22085)
*Vaani Goenka,Aalok D. Thakkar*

Main category: cs.SE

TL;DR: BOOP is a structured programming framework that improves students' algorithmic reasoning and code correctness by enforcing a four-phase process, showing better results than traditional trial-and-error methods.


<details>
  <summary>Details</summary>
Motivation: Novice programmers often focus on making code work via repeated trial-and-error rather than developing correct solutions through structured reasoning. The use of AI coding tools can worsen this by providing code that is syntactically correct but conceptually incorrect.

Method: The paper introduces BOOP, a four-phase structured programming framework: (1) formal specification, (2) language-agnostic algorithm development, (3) implementation, and (4) correctness proof. The framework was implemented via a VS Code extension and preprocessor, which enforce the phases and monitor coding patterns. Effectiveness was evaluated at the authors' institution.

Result: Initial results demonstrate that students using BOOP show improved algorithmic reasoning, understand edge cases better, and are less reliant on trial-and-error debugging. Instructors noticed stronger foundational skills, although some students found the framework initially verbose.

Conclusion: The BOOP framework successfully shifts the focus from making code simply work to understanding code correctness, enhancing foundational skills in programming education, despite some initial resistance to the framework's verbosity.

Abstract: Novice programmers frequently adopt a syntax-specific and test-case-driven
approach, writing code first and adjusting until programs compile and test
cases pass, rather than developing correct solutions through systematic
reasoning. AI coding tools exacerbate this challenge by providing syntactically
correct but conceptually flawed solutions. In this paper, we introduce BOOP
(Blueprint, Operations, OCaml, Proof), a structured framework requiring four
mandatory phases: formal specification, language-agnostic algorithm
development, implementation, and correctness proof. This shifts focus from
``making code work'' to understanding why code is correct.
  BOOP was implemented at our institution using a VS Code extension and
preprocessor that enforces constraints and identifies counterproductive
patterns. Initial evaluation shows improved algorithmic reasoning and reduced
trial-and-error debugging. Students reported better edge case understanding and
problem decomposition, though some initially found the format verbose.
Instructors observed stronger foundational skills compared to traditional
approaches.

</details>


### [10] [Secure coding for web applications: Frameworks, challenges, and the role of LLMs](https://arxiv.org/abs/2507.22223)
*Kiana Kiashemshaki,Mohammad Jalili Torkamani,Negin Mahmoudi*

Main category: cs.SE

TL;DR: This paper reviews secure coding practices across domains, categorizes threats, compares frameworks, and shows how LLMs can evaluate secure code—offering actionable insights for improving real-world secure development.


<details>
  <summary>Details</summary>
Motivation: Despite awareness of secure coding, real-world uptake is poor due to organizational, educational, and technical challenges. There's a need for practical guidance and new tools to boost adoption and effectiveness.

Method: A comprehensive review and framework comparison were conducted, integrating threat classification (aligned with OWASP Top 10) and a case study using LLMs to evaluate secure code across major vulnerabilities.

Result: The study provides a detailed categorization of secure coding practices across frameworks and domains, offers a comparison framework, and demonstrates, through a case study, how LLMs can help in analyzing and recommending secure code.

Conclusion: The paper concludes that a structured evaluation of secure coding practices, framework comparison, and threat categorization can help bridge existing adoption gaps. It emphasizes LLMs’ emerging potential in advancing secure coding.

Abstract: Secure coding is a critical yet often overlooked practice in software
development. Despite extensive awareness efforts, real-world adoption remains
inconsistent due to organizational, educational, and technical barriers. This
paper provides a comprehensive review of secure coding practices across major
frameworks and domains, including web development, DevSecOps, and cloud
security. It introduces a structured framework comparison and categorizes
threats aligned with the OWASP Top 10. Additionally, we explore the rising role
of Large Language Models (LLMs) in evaluating and recommending secure code,
presenting a reproducible case study across four major vulnerability types.
This paper offers practical insights for researchers, developers, and educators
on integrating secure coding into real-world development processes.

</details>


### [11] [From Articles to Code: On-Demand Generation of Core Algorithms from Scientific Publications](https://arxiv.org/abs/2507.22324)
*Cameron S. Movassaghi,Amanda Momenzadeh,Jesse G. Meyer*

Main category: cs.SE

TL;DR: LLMs can now generate high-quality code just from detailed scientific descriptions, matching traditional library performance, which could dramatically cut software maintenance needs.


<details>
  <summary>Details</summary>
Motivation: Software package maintenance is expensive due to complex factors like dependencies, bug fixes, and versioning. The research explores if using scientific publications with thorough method descriptions can reduce this burden by empowering LLMs to generate code on demand.

Method: The authors benchmarked leading large language models (GPT-o4-mini-high, Gemini Pro 2.5, Claude Sonnet 4) by asking them to implement various core algorithms directly from published method descriptions, and then compared the quality and performance of these generated codes to that of traditional software packages.

Result: State-of-the-art LLMs were able to accurately reproduce the functionality of established software libraries, achieving similar performance based solely on rich method descriptions from scientific publications.

Conclusion: Modern LLMs, given high-quality scientific method descriptions, can generate code with reliability comparable to hand-maintained libraries, indicating a shift toward automated, on-demand code creation and reduced reliance on static, manually maintained packages.

Abstract: Maintaining software packages imposes significant costs due to dependency
management, bug fixes, and versioning. We show that rich method descriptions in
scientific publications can serve as standalone specifications for modern large
language models (LLMs), enabling on-demand code generation that could supplant
human-maintained libraries. We benchmark state-of-the-art models
(GPT-o4-mini-high, Gemini Pro 2.5, Claude Sonnet 4) by tasking them with
implementing a diverse set of core algorithms drawn from original publications.
Our results demonstrate that current LLMs can reliably reproduce package
functionality with performance indistinguishable from conventional libraries.
These findings foreshadow a paradigm shift toward flexible, on-demand code
generation and away from static, human-maintained packages, which will result
in reduced maintenance overhead by leveraging published articles as sufficient
context for the automated implementation of analytical workflows.

</details>


### [12] [AutoCodeSherpa: Symbolic Explanations in AI Coding Agents](https://arxiv.org/abs/2507.22414)
*Sungmin Kang,Haifeng Ruan,Abhik Roychoudhury*

Main category: cs.SE

TL;DR: This paper presents a workflow for LLM code repair agents that generates symbolic, executable explanations for software bugs and fixes, improving developer trust and enabling more reliable, automated patch acceptance in production environments.


<details>
  <summary>Details</summary>
Motivation: Large Language Model (LLM) agents have become popular in software engineering for tasks such as automated program repair. However, for deployment in real-world production environments, these agents need to provide trustworthy and explainable software patches. There is a need for approaches that increase developer confidence by providing clear explanations for bug fixes and automating issue resolution more effectively.

Method: The authors introduce a workflow in which an LLM agent produces explanations about code bugs through symbolic formulae, specifically input, infection, and output conditions. These are implemented using property-based tests (PBT) and symbolic program expressions. The explanations, being executable, serve both human developers and automated systems in assessing the validity of proposed patches.

Result: The workflow enables LLM agents to generate executable and human-understandable explanations for code fixes. Human developers can use these explanations to test and better understand issues, while automated systems can utilize them for patch evaluation in a fully autonomous pipeline. The approach is also compatible with other agentic AI repair techniques beyond the studied systems, potentially improving overall patch quality.

Conclusion: The proposed workflow enhances both the transparency and reliability of LLM-driven automated code repair systems. By providing symbolic, executable explanations, agentic workflows can aid human understanding and drive further automation. This benefits both manual and automated acceptance of patches, improving the viability of LLM agents in production environments.

Abstract: Large Language Model (LLM) agents autonomously use external tools on top of
one or more LLMs to accomplish specific tasks. Lately LLM agents for software
engineering tasks have become popular. These agents can benefit from the use of
program analysis tools working on program representations. This is demonstrated
by existing agentic AI solutions such as AutoCodeRover or SpecRover which
perform automated program repair. Specifically the goal of these works is to
use program analysis to improve the patch quality. These agents are currently
being used to automatically fix static analysis issues from the widely used
SonarQube static analyzer.
  Nevertheless, for the agents to be deployed in a production environment,
agents need to suggest software artifacts, such as patches, with evidence and
with high confidence. In this work, we provide a workflow where an agent
provides explanations of the bug in the form of symbolic formulae. The
explanations are in the form of input conditions, infection conditions and
output conditions, implemented as property based tests (PBT) and
program-internal symbolic expressions. These can help in human developer
cognition of the agent outputs as well as in achieving completely automated
agentic workflows for software. The human developer can benefit from the input
condition, represented as a PBT, to generate various concrete inputs showing a
given issue. Furthermore, since the PBTs are executable, our explanations are
executable as well. We can thus also use the explanations in a completely
automated issue resolution environment for accepting or rejecting the patches
that are suggested by patching agents such as AutoCodeRover. Finally, as
agentic AI approaches continue to develop, the program analysis driven
explanations can be provided to other LLM-based repair techniques such as
Agentless to improve their output.

</details>


### [13] [Ensemble Fuzzing with Dynamic Resource Scheduling and Multidimensional Seed Evaluation](https://arxiv.org/abs/2507.22442)
*Yukai Zhao,Shaohua Wang,Jue Wang,Xing Hu,Xin Xia*

Main category: cs.SE

TL;DR: Legion is a new ensemble fuzzing framework that dynamically manages resources using a novel algorithm and better performance evaluation, outperforming previous tools by finding more vulnerabilities in tests and real-world projects.


<details>
  <summary>Details</summary>
Motivation: Ensemble fuzzing aims to improve bug and vulnerability detection by combining multiple fuzzers. However, existing approaches struggle with effective resource allocation and comprehensive performance evaluation, causing resource inefficiencies.

Method: The authors present Legion, a new ensemble fuzzing framework. Legion uses a resource scheduling algorithm based on the upper confidence bound to reduce resource waste and introduces a multidimensional seed evaluation strategy for finer performance assessment.

Result: Legion was implemented and tested on Google's fuzzer-test-suite and real-world open-source projects. It outperformed current state-of-the-art base fuzzers and ensemble fuzzing techniques, discovering 20 vulnerabilities, including five previously unknown and three CVEs.

Conclusion: Legion provides significant improvements over existing ensemble fuzzing methods by dynamically optimizing resource allocation and refining performance evaluation, resulting in better bug and vulnerability detection.

Abstract: Fuzzing is widely used for detecting bugs and vulnerabilities, with various
techniques proposed to enhance its effectiveness. To combine the advantages of
multiple technologies, researchers proposed ensemble fuzzing, which integrates
multiple base fuzzers. Despite promising results, state-of-the-art ensemble
fuzzing techniques face limitations in resource scheduling and performance
evaluation, leading to unnecessary resource waste. In this paper, we propose
Legion, a novel ensemble fuzzing framework that dynamically schedules resources
during the ensemble fuzzing campaign. We designed a novel resource scheduling
algorithm based on the upper confidence bound algorithm to reduce the resource
consumption of ineffective base fuzzers. Additionally, we introduce a
multidimensional seed evaluation strategy, which considers multiple metrics to
achieve more comprehensive fine-grained performance evaluation. We implemented
Legion as a prototype tool and evaluated its effectiveness on Google's
fuzzer-test-suite as well as real-world open-source projects. Results show that
Legion outperforms existing state-of-the-art base fuzzers and ensemble fuzzing
techniques, detecting 20 vulnerabilities in real-world open-source
projects-five previously unknown and three classified as CVEs.

</details>


### [14] [Inside madupite: Technical Design and Performance](https://arxiv.org/abs/2507.22538)
*Matilde Gargiani,Robin Sieber,Philip Pawlowsky,John Lygeros*

Main category: cs.SE

TL;DR: Madupite is a new, scalable solver for large-scale Markov decision processes that computes exact solutions even on problems too big for standard computers, thanks to distributed computing and customizability. It is more flexible and efficient than previous approaches and marks a major step forward in solving such problems.


<details>
  <summary>Details</summary>
Motivation: There is a need for scalable and efficient solvers that can compute exact solutions to large-scale discounted infinite-horizon Markov decision processes (MDPs), especially when such problems exceed the memory capacity of standard hardware. Current methods often require function approximation or cannot fully exploit problem structure, limiting their effectiveness.

Method: The paper introduces 'madupite', a high-performance solver for large-scale discounted infinite-horizon MDPs with finite state and action spaces. The authors detail the solver’s mathematical optimization foundation, its technical implementation, and its ability to run in a distributed manner across high-performance computing clusters. Madupite’s algorithm can also be customized based on specific problem structures to improve convergence speed.

Result: Madupite is demonstrated to be both scalable and efficient across MDPs from diverse fields such as epidemiology and control. It uniquely enables exact solutions even for problems exceeding the memory of typical laptops, leveraging distributed computation. It also accelerates convergence for problems with high discount factors by allowing algorithm customization.

Conclusion: Madupite marks a new standard for solving large-scale MDPs, providing unmatched scalability and flexibility. It handles medium to large problems exactly, avoids relying on approximations, and works efficiently in near-undiscounted settings, thus representing a significant advancement in the field.

Abstract: In this work, we introduce and benchmark madupite, a newly proposed
high-performance solver designed for large-scale discounted infinite-horizon
Markov decision processes with finite state and action spaces. After a brief
overview of the class of mathematical optimization methods on which madupite
relies, we provide details on implementation choices, technical design and
deployment. We then demonstrate its scalability and efficiency by showcasing
its performance on the solution of Markov decision processes arising from
different application areas, including epidemiology and classical control.
Madupite sets a new standard as, to the best of our knowledge, it is the only
solver capable of efficiently computing exact solutions for large-scale Markov
decision processes, even when these exceed the memory capacity of modern
laptops and operate in near-undiscounted settings. This is possible as madupite
can work in a fully distributed manner and therefore leverage the memory
storage and computation capabilities of modern high-performance computing
clusters. This key feature enables the solver to efficiently handle problems of
medium to large size in an exact manner instead of necessarily resorting to
function approximations. Moreover, madupite is unique in allowing users to
customize the solution algorithm to better exploit the specific structure of
their problem, significantly accelerating convergence especially in
large-discount factor settings. Overall, madupite represents a significant
advancement, offering unmatched scalability and flexibility in solving
large-scale Markov decision processes.

</details>


### [15] [RePaCA: Leveraging Reasoning Large Language Models for Static Automated Patch Correctness Assessment](https://arxiv.org/abs/2507.22580)
*Marcos Fuster-Pena,David de-Fitero-Dominguez,Antonio Garcia-Cabot,Eva Garcia-Lopez*

Main category: cs.SE

TL;DR: This paper presents RePaCA, a novel method using finetuned reasoning LLMs for more accurate, generalizable, and explainable automated assessment of program repair patch correctness. It achieves state-of-the-art results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Automated Program Repair (APR) tools often produce overfitting patches that only satisfy test cases without fixing the real bug. Current static methods to automatically assess the correctness of these patches (APCA) suffer from poor reliability, flexibility, and transparency.

Method: The authors introduce RePaCA, a new static APCA technique that uses specialized Large Language Models (LLMs) prompted with buggy and fixed code snippets. The model generates a Chain of Thought reasoning trace and classifies patches as correct or overfitting. The LLM is finetuned for this specific task using Reinforcement Learning with the Group Relative Policy Optimization algorithm.

Result: RePaCA achieves state-of-the-art results on a standard APCA benchmark (Defects4J-derived), reaching 83.1% accuracy and an 84.8% F1-score. The model also generalizes better than previous techniques when trained on different datasets, and provides more explainable patch assessments.

Conclusion: Finetuned reasoning LLMs like RePaCA significantly improve the accuracy, generalization, and explainability of static APCA, showcasing the potential of advanced LLMs for automated patch correctness assessment.

Abstract: Automated Program Repair (APR) seeks to automatically correct software bugs
without requiring human intervention. However, existing tools tend to generate
patches that satisfy test cases without fixing the underlying bug, those are
known as overfitting patches. To address this issue, Automated Patch
Correctness Assessment (APCA) attempts to identify overfitting patches
generated by APR tools. It can be solved as a static approach, meaning that no
additional information is needed beyond the original and fixed code snippets.
Current static techniques often struggle with reliability, flexibility and
transparency. To address these issues, we introduce RePaCA, a novel static APCA
technique that leverages Large Language Models (LLMs) specialized in thinking
tasks. Our model is prompted with both buggy and fixed code snippets and guided
to generate a Chain of Thought that analyses code differences, reasons about
how the patch addresses the root cause, and ultimately provides a binary
classification: correct or overfitting. To enhance these reasoning capabilities
for the APCA task specifically, the LLM is finetuned using Reinforcement
Learning with the Group Relative Policy Optimization algorithm. When evaluated
on a standard Defects4J-derived test, our approach achieves state-of-the-art
performance, with 83.1% accuracy and an 84.8% F1-score. Furthermore, our model
demonstrates superior generalization capabilities when trained on different
datasets, outperforming the leading technique. This reasoning capability also
provides enhanced explainability for the patch assessment. These findings
underscore the considerable promise of finetuned, reasoning LLMs to advance
static APCA by enhancing accuracy, generalization, and explainability.

</details>


### [16] [Metamorphic Testing of Deep Code Models: A Systematic Literature Review](https://arxiv.org/abs/2507.22610)
*Ali Asgari,Milan de Koning,Pouria Derakhshanfar,Annibale Panichella*

Main category: cs.SE

TL;DR: A literature review of 45 papers reveals how metamorphic testing is used to evaluate the robustness of deep code models, outlining common practices, challenges, and opportunities for improving reliability in software engineering AI.


<details>
  <summary>Details</summary>
Motivation: Large language models for code are highly accurate in code-related tasks but may lack robustness when subjected to adversarial conditions or semantic-preserving changes. Ensuring their consistent performance is crucial for their integration into software engineering.

Method: A systematic literature review was conducted, focusing on 45 primary papers that applied metamorphic testing to deep code models. The review analyzed the types of semantic-preserving transformations, robustness evaluation techniques, and assessment methods used in these studies.

Result: The review identified commonly evaluated models, tasks, datasets, programming languages, and metrics. It mapped the state of robustness assessment for deep code models, summarizing the methods, highlighting key trends, and noting challenges that remain in robust evaluation.

Conclusion: Metamorphic testing is actively used to evaluate and improve the robustness of code intelligence models, but the field faces ongoing challenges. The paper provides a comprehensive summary of current practices and offers guidance for future research directions.

Abstract: Large language models and deep learning models designed for code intelligence
have revolutionized the software engineering field due to their ability to
perform various code-related tasks. These models can process source code and
software artifacts with high accuracy in tasks such as code completion, defect
detection, and code summarization; therefore, they can potentially become an
integral part of modern software engineering practices. Despite these
capabilities, robustness remains a critical quality attribute for deep-code
models as they may produce different results under varied and adversarial
conditions (e.g., variable renaming). Metamorphic testing has become a widely
used approach to evaluate models' robustness by applying semantic-preserving
transformations to input programs and analyzing the stability of model outputs.
While prior research has explored testing deep learning models, this systematic
literature review focuses specifically on metamorphic testing for deep code
models. By studying 45 primary papers, we analyze the transformations,
techniques, and evaluation methods used to assess robustness. Our review
summarizes the current landscape, identifying frequently evaluated models,
programming tasks, datasets, target languages, and evaluation metrics, and
highlights key challenges and future directions for advancing the field.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [17] [A Compute-Matched Re-Evaluation of TroVE on MATH](https://arxiv.org/abs/2507.22069)
*Tobias Sesterhenn,Ian Berlot-Attwell,Janis Zenkner,Christian Bartelt*

Main category: cs.PL

TL;DR: The paper finds that TroVE's toolbox-based approach does not meaningfully improve math problem solving by code-generating LLMs on the MATH benchmark—its marginal gains come mainly from using more computational resources, not from genuinely effective tool reuse.


<details>
  <summary>Details</summary>
Motivation: Mathematical problem-solving often relies on reusing previously established theorems and formulas. TroVE, a recent approach, claims that Large Language Models (LLMs) can similarly improve performance on the MATH benchmark by leveraging reusable toolboxes rather than just direct problem-solving. There is skepticism regarding whether the gains are from tool reuse or other factors like increased computation.

Method: The authors conduct a re-evaluation of the TroVE approach on the MATH benchmark, analyzing the contributions of its three modes (direct code generation, tool creation, tool reuse). They correct a minor flaw in TroVE's selection mechanism and perform comparisons between TroVE and a direct-code-generation (PRIMITIVE) baseline, controlling for computational budget.

Result: The authors find that most of TroVE's claimed improvements originate from a higher computational budget relative to the baseline, not from actual tool reuse or toolbox mechanisms. With computational budget controlled, TroVE's measured benefit drops to a marginal 1% improvement. A minor correction to the tool selection slightly increases TroVE's performance by 3%.

Conclusion: TroVE's toolbox mechanism does not provide a substantial benefit for mathematical problem-solving on the MATH benchmark beyond what can be explained by a greater computation budget.

Abstract: Reusing established theorems and formulas is central to mathematical problem
solving, serving as essential building blocks for tackling increasingly complex
challenges. Recent work, TroVE, argues that code-generating Large Language
Models (LLMs) can benefit similarly on the MATH benchmark by inducing and
reusing higher-level toolboxes. By allocating computational budget across an
ensemble of three modes -- directly generating code, creating tools, and
reusing tools -- TroVE claims to outperform a PRIMITIVE baseline that only
performs direct generation. However, recent analysis (Berlot-Attwell et al.,
2024) casts doubt on these gains, noting that the tools created are often
trivial or rarely reused, suggesting that improvements may stem from
self-consistency or self-correction. In this work, we re-evaluate TroVE on
MATH, analyze the impact of each of its modes, and show that its benefit does
not come from these mechanisms, but simply from a higher computational budget
spent for TroVE compared to PRIMITIVE. To this end, we also perform a small
correction in the original implementation of TroVE's selection mechanism,
boosting TroVE's performance on MATH by 3\% in accuracy. After matching for
compute, the benefit of TroVE reduces to a marginal improvement of 1\%,
suggesting that this toolbox approach does not provide a significant benefit on
MATH.

</details>
