{"id": "2507.07480", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.07480", "abs": "https://arxiv.org/abs/2507.07480", "authors": ["Tobias Kapp\u00e9"], "title": "On Propositional Program Equivalence (extended abstract)", "comment": null, "summary": "General program equivalence is undecidable. However, if we abstract away the\nsemantics of statements, then this problem becomes not just decidable, but\npractically feasible. For instance, a program of the form \"if $b$ then $e$ else\n$f$\" should be equivalent to \"if not $b$ then $f$ else $e$\" - no matter what\n$b$, $e$ and $f$ are. This kind of equivalence is known as propositional\nequivalence. In this extended abstract, we discuss recent developments in\npropositional program equivalence from the perspective of (Guarded) Kleene\nAlgebra with Tests, or (G)KAT.", "AI": {"tldr": "General program equivalence is undecidable, but by using (G)KAT to abstract semantics, propositional equivalence becomes both decidable and practical to analyze.", "motivation": "General program equivalence is undecidable, making direct analysis infeasible. The authors address whether abstraction over statement semantics can make equivalence checking feasible, focusing on propositional equivalence.", "method": "The paper discusses recent developments in propositional program equivalence through the framework of (Guarded) Kleene Algebra with Tests ((G)KAT), which abstracts program semantics for analysis.", "result": "It is demonstrated that, by abstracting away from the underlying statement semantics, propositional program equivalence is both decidable and practically feasible using (G)KAT.", "conclusion": "Abstraction using (G)KAT leads to practical and decidable methods for propositional program equivalence, opening ways to effectively analyze program structure without dealing with undecidability."}}
{"id": "2507.07325", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07325", "abs": "https://arxiv.org/abs/2507.07325", "authors": ["Martin Obaidi", "Marc Herrmann", "Elisa Schmid", "Raymond Ochsner", "Kurt Schneider", "Jil Kl\u00fcnder"], "title": "A German Gold-Standard Dataset for Sentiment Analysis in Software Engineering", "comment": "This paper has been accepted at the 33rd IEEE International\n  Requirements Engineering Workshop (REW 2025)", "summary": "Sentiment analysis is an essential technique for investigating the emotional\nclimate within developer teams, contributing to both team productivity and\nproject success. Existing sentiment analysis tools in software engineering\nprimarily rely on English or non-German gold-standard datasets. To address this\ngap, our work introduces a German dataset of 5,949 unique developer statements,\nextracted from the German developer forum Android-Hilfe.de. Each statement was\nannotated with one of six basic emotions, based on the emotion model by Shaver\net al., by four German-speaking computer science students. Evaluation of the\nannotation process showed high interrater agreement and reliability. These\nresults indicate that the dataset is sufficiently valid and robust to support\nsentiment analysis in the German-speaking software engineering community.\nEvaluation with existing German sentiment analysis tools confirms the lack of\ndomain-specific solutions for software engineering. We also discuss approaches\nto optimize annotation and present further use cases for the dataset.", "AI": {"tldr": "A new German dataset of developer statements for sentiment analysis was created and validated, filling a gap for the software engineering field. Existing tools perform poorly in this domain, underlining the importance of this contribution.", "motivation": "Existing sentiment analysis tools in software engineering are primarily based on English or non-German datasets, leaving a gap for robust German-language resources. There is a need for domain-specific sentiment analysis datasets for the German-speaking software engineering community.", "method": "The authors created a dataset of 5,949 unique German developer statements from the Android-Hilfe.de forum. Each statement was annotated with one of six emotions (per Shaver et al.\u2019s model) by four German-speaking computer science students. The annotation process was evaluated for interrater agreement and reliability. The dataset was then tested with existing German sentiment analysis tools.", "result": "The annotation process showed high interrater agreement and reliability, indicating the dataset is valid and robust. Evaluation with current sentiment analysis tools exposed the lack of suitable German domain-specific solutions for software engineering. Potential optimizations for annotation and additional use cases for the dataset are presented.", "conclusion": "The paper introduces a new, well-annotated German sentiment analysis dataset tailored for software engineering, which is valid, reliable, and addresses a critical resource gap in the community. Existing tools fail to capture domain-specific sentiment well, highlighting the need for such data. Further work is suggested on annotation methods and dataset applications."}}
{"id": "2507.07344", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07344", "abs": "https://arxiv.org/abs/2507.07344", "authors": ["Martin Obaidi", "Jannik Fischbach", "Jakob Droste", "Hannah Deters", "Marc Herrmann", "Jil Kl\u00fcnder", "Steffen Kr\u00e4tzig", "Hugo Villamizar", "Kurt Schneider"], "title": "Automatic Generation of Explainability Requirements and Software Explanations From User Reviews", "comment": "This paper has been accepted at the 33rd IEEE International\n  Requirements Engineering Workshop (REW 2025)", "summary": "Explainability has become a crucial non-functional requirement to enhance\ntransparency, build user trust, and ensure regulatory compliance. However,\ntranslating explanation needs expressed in user feedback into structured\nrequirements and corresponding explanations remains challenging. While existing\nmethods can identify explanation-related concerns in user reviews, there is no\nestablished approach for systematically deriving requirements and generating\naligned explanations. To contribute toward addressing this gap, we introduce a\ntool-supported approach that automates this process. To evaluate its\neffectiveness, we collaborated with an industrial automation manufacturer to\ncreate a dataset of 58 user reviews, each annotated with manually crafted\nexplainability requirements and explanations. Our evaluation shows that while\nAI-generated requirements often lack relevance and correctness compared to\nhuman-created ones, the AI-generated explanations are frequently preferred for\ntheir clarity and style. Nonetheless, correctness remains an issue,\nhighlighting the importance of human validation. This work contributes to the\nadvancement of explainability requirements in software systems by (1)\nintroducing an automated approach to derive requirements from user reviews and\ngenerate corresponding explanations, (2) providing empirical insights into the\nstrengths and limitations of automatically generated artifacts, and (3)\nreleasing a curated dataset to support future research on the automatic\ngeneration of explainability requirements.", "AI": {"tldr": "The paper proposes and evaluates an automated tool for turning user feedback into software explainability requirements and explanations. While AI can generate clear explanations, human validation is still required to ensure the correctness of both requirements and explanations.", "motivation": "There is a growing need to provide explainability in software systems for transparency, trust, and compliance, but it is difficult to systematically transform user feedback into structured explainability requirements and explanations.", "method": "A tool-supported, automated approach was developed to extract explainability requirements from user reviews and generate aligned explanations. The approach was evaluated using a dataset of 58 annotated user reviews from collaboration with an industrial manufacturer.", "result": "AI-generated requirements were less relevant and correct compared to manually created ones, but the AI-generated explanations were often favored for their clarity and style. However, correctness issues persist, highlighting the need for human validation.", "conclusion": "The paper introduces an automated method to derive explainability requirements and generate explanations from user reviews but notes that human oversight is necessary to ensure correctness. The work also provides empirical insights and a curated dataset to advance future research."}}
{"id": "2507.07468", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07468", "abs": "https://arxiv.org/abs/2507.07468", "authors": ["Sten Gr\u00fcner", "Nafise Eskandani"], "title": "Towards an Engineering Workflow Management System for Asset Administration Shells using BPMN", "comment": "7 pages, 7 figures, Accepted at IFAC EAAS 2025\n  (https://j3c.org/eaas.php)", "summary": "The integration of Industry 4.0 technologies into engineering workflows is an\nessential step toward automating and optimizing plant and process engineering\nprocesses. The Asset Administration Shell (AAS) serves as a key enabler for\ncreating interoperable Digital Twins that facilitate engineering data exchange\nand automation. This paper explores the use of AAS within engineering\nworkflows, particularly in combination with Business Process Model and Notation\n(BPMN) to define structured and automated processes. We propose a distributed\nAAS copy-on-write infrastructure that enhances security and scalability while\nenabling seamless cross organizational collaboration. We also introduce a\nworkflow management prototype automating AAS operations and engineering\nworkflows, improving efficiency and traceability.", "AI": {"tldr": "The paper presents a secure, scalable, and automated approach for integrating Industry 4.0 technologies into engineering workflows using AAS, BPMN, and a novel infrastructure, leading to improved efficiency and collaboration.", "motivation": "Industry 4.0 aims for automation and optimization in plant and process engineering, requiring technologies that enable interoperability, automation, and secure data exchange across organizations. The Asset Administration Shell (AAS) is positioned as a key element to address these needs.", "method": "The paper explores integrating the Asset Administration Shell (AAS) into engineering workflows, using Business Process Model and Notation (BPMN) for structured process definition. It proposes a distributed copy-on-write infrastructure for the AAS and implements a workflow management prototype to automate AAS operations and engineering workflows.", "result": "A distributed AAS copy-on-write infrastructure was developed, improving both security and scalability, thus enabling better cross-organizational collaboration. A workflow management prototype was also introduced, automating AAS operations and engineering workflows, thereby increasing efficiency and traceability.", "conclusion": "Combining AAS with BPMN and introducing a distributed infrastructure provides significant improvements in workflow automation, security, scalability, and cross-organization engineering collaboration."}}
{"id": "2507.07548", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07548", "abs": "https://arxiv.org/abs/2507.07548", "authors": ["Jonathan Ullrich", "Matthias Koch", "Andreas Vogelsang"], "title": "From Requirements to Code: Understanding Developer Practices in LLM-Assisted Software Engineering", "comment": "This paper has been accepted for publication at the 33rd IEEE\n  International Requirements Engineering (RE) conference", "summary": "With the advent of generative LLMs and their advanced code generation\ncapabilities, some people already envision the end of traditional software\nengineering, as LLMs may be able to produce high-quality code based solely on\nthe requirements a domain expert feeds into the system. The feasibility of this\nvision can be assessed by understanding how developers currently incorporate\nrequirements when using LLMs for code generation-a topic that remains largely\nunexplored. We interviewed 18 practitioners from 14 companies to understand how\nthey (re)use information from requirements and other design artifacts to feed\nLLMs when generating code. Based on our findings, we propose a theory that\nexplains the processes developers employ and the artifacts they rely on. Our\ntheory suggests that requirements, as typically documented, are too abstract\nfor direct input into LLMs. Instead, they must first be manually decomposed\ninto programming tasks, which are then enriched with design decisions and\narchitectural constraints before being used in prompts. Our study highlights\nthat fundamental RE work is still necessary when LLMs are used to generate\ncode. Our theory is important for contextualizing scientific approaches to\nautomating requirements-centric SE tasks.", "AI": {"tldr": "LLMs can't yet turn standard requirements documents directly into working code; developers must manually translate and enrich requirements before LLMs are useful, so traditional software engineering skills are still needed.", "motivation": "With the rise of powerful generative Large Language Models (LLMs) capable of advanced code generation, there is speculation about these tools replacing traditional software engineering by generating code directly from requirements. However, the real-world process of feeding requirements into LLMs and how developers adapt is not well understood.", "method": "The authors conducted interviews with 18 practitioners from 14 different companies. These interviews explored how developers use information from requirements and design artifacts to prompt LLMs for code generation.", "result": "Their findings show that requirements, as they are typically written, are too abstract for LLMs to use directly. Instead, developers manually break down requirements into more concrete programming tasks and infuse prompts with specific design decisions and architectural constraints.", "conclusion": "Despite the abilities of LLMs, classic requirements engineering (RE) activities remain critical. Developers must still interpret and restructure requirements before LLMs can effectively assist with code generation. The study's theory informs future research on automating requirements-oriented software engineering tasks."}}
{"id": "2507.07682", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07682", "abs": "https://arxiv.org/abs/2507.07682", "authors": ["Kaicheng Huang", "Fanyu Wang", "Yutan Huang", "Chetan Arora"], "title": "Prompt Engineering for Requirements Engineering: A Literature Review and Roadmap", "comment": null, "summary": "Advancements in large language models (LLMs) have led to a surge of prompt\nengineering (PE) techniques that can enhance various requirements engineering\n(RE) tasks. However, current LLMs are often characterized by significant\nuncertainty and a lack of controllability. This absence of clear guidance on\nhow to effectively prompt LLMs acts as a barrier to their trustworthy\nimplementation in the RE field. We present the first roadmap-oriented\nsystematic literature review of Prompt Engineering for RE (PE4RE). Following\nKitchenham's and Petersen's secondary-study protocol, we searched six digital\nlibraries, screened 867 records, and analyzed 35 primary studies. To bring\norder to a fragmented landscape, we propose a hybrid taxonomy that links\ntechnique-oriented patterns (e.g., few-shot, Chain-of-Thought) to task-oriented\nRE roles (elicitation, validation, traceability). Two research questions, with\nfive sub-questions, map the tasks addressed, LLM families used, and prompt\ntypes adopted, and expose current limitations and research gaps. Finally, we\noutline a step-by-step roadmap showing how today's ad-hoc PE prototypes can\nevolve into reproducible, practitioner-friendly workflows.", "AI": {"tldr": "This paper systematically reviews Prompt Engineering techniques for Requirements Engineering, offering a taxonomy and a practical roadmap to guide future research and application, with the goal of making LLMs more reliable and accessible in RE tasks.", "motivation": "The motivation for this paper is to address the significant uncertainty and lack of controllability in the use of large language models (LLMs) for requirements engineering (RE) tasks. The absence of clear, standardized guidance for prompt engineering (PE) hinders the trustworthy and effective use of LLMs in this field.", "method": "The authors perform a systematic literature review based on Kitchenham's and Petersen's secondary-study protocol. They searched six digital libraries, screened 867 records, and analyzed 35 primary studies to map current PE techniques and their applications to RE.", "result": "The study proposes a hybrid taxonomy linking technique-oriented prompt engineering patterns to task-oriented RE roles. It maps out what RE tasks have been addressed, which LLM families and prompt types have been used, and identifies existing limitations and research gaps in the field.", "conclusion": "The paper provides the first systematic roadmap for PE in RE, presenting a step-by-step guide for evolving current ad-hoc prompt engineering prototypes into reproducible, practitioner-friendly workflows."}}
{"id": "2507.07689", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07689", "abs": "https://arxiv.org/abs/2507.07689", "authors": ["Chetan Arora", "Fanyu Wang", "Chakkrit Tantithamthavorn", "Aldeida Aleti", "Shaun Kenyon"], "title": "From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry", "comment": null, "summary": "Requirements engineering (RE) in the space industry is inherently complex,\ndemanding high precision, alignment with rigorous standards, and adaptability\nto mission-specific constraints. Smaller space organisations and new entrants\noften struggle to derive actionable requirements from extensive, unstructured\ndocuments such as mission briefs, interface specifications, and regulatory\nstandards. In this innovation opportunity paper, we explore the potential of\nRetrieval-Augmented Generation (RAG) models to support and (semi-)automate\nrequirements generation in the space domain. We present a modular, AI-driven\napproach that preprocesses raw space mission documents, classifies them into\nsemantically meaningful categories, retrieves contextually relevant content\nfrom domain standards, and synthesises draft requirements using large language\nmodels (LLMs). We apply the approach to a real-world mission document from the\nspace domain to demonstrate feasibility and assess early outcomes in\ncollaboration with our industry partner, Starbound Space Solutions. Our\npreliminary results indicate that the approach can reduce manual effort,\nimprove coverage of relevant requirements, and support lightweight compliance\nalignment. We outline a roadmap toward broader integration of AI in RE\nworkflows, intending to lower barriers for smaller organisations to participate\nin large-scale, safety-critical missions.", "AI": {"tldr": "The paper explores using AI, specifically Retrieval-Augmented Generation models, to automate and improve requirements engineering in the space industry, showing early success in reducing effort and improving requirement coverage for smaller organizations.", "motivation": "Requirements engineering in the space industry is complex and demanding, particularly for smaller organizations, due to the need to extract actionable requirements from large, unstructured documents and adhere to stringent standards.", "method": "The paper proposes a modular, AI-driven approach using Retrieval-Augmented Generation (RAG) models. The system preprocesses raw mission documents, classifies their contents, retrieves relevant information from domain standards, and uses large language models (LLMs) to synthesize draft requirements. The approach is demonstrated on a real-world mission document in partnership with an industry collaborator.", "result": "Preliminary results suggest the method reduces manual effort, enhances coverage of relevant requirements, and helps achieve lightweight compliance alignment.", "conclusion": "AI-driven approaches like RAG can significantly support and (semi-)automate requirements generation in space missions, lowering entry barriers for smaller organizations and enhancing the requirements engineering process."}}
