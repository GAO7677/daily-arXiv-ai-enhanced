{"id": "2506.17306", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17306", "abs": "https://arxiv.org/abs/2506.17306", "authors": ["Jake Zappin", "Trevor Stalnaker", "Oscar Chaparro", "Denys Poshyvanyk"], "title": "Challenges and Practices in Quantum Software Testing and Debugging: Insights from Practitioners", "comment": null, "summary": "Quantum software engineering is an emerging discipline with distinct\nchallenges, particularly in testing and debugging. As quantum computing\ntransitions from theory to implementation, developers face issues not present\nin classical software development, such as probabilistic execution, limited\nobservability, shallow abstractions, and low awareness of quantum-specific\ntools. To better understand current practices, we surveyed 26 quantum software\ndevelopers from academia and industry and conducted follow-up interviews\nfocused on testing, debugging, and recurring challenges. All participants\nreported engaging in testing, with unit testing (88%), regression testing\n(54%), and acceptance testing (54%) being the most common. However, only 31%\nreported using quantum-specific testing tools, relying instead on manual\nmethods. Debugging practices were similarly grounded in classical strategies,\nsuch as print statements, circuit visualizations, and simulators, which\nrespondents noted do not scale well. The most frequently cited sources of bugs\nwere classical in nature-library updates (81%), developer mistakes (68%), and\ncompatibility issues (62%)-often worsened by limited abstraction in existing\nSDKs. These findings highlight the urgent need for better-aligned testing and\ndebugging tools, integrated more seamlessly into the workflows of quantum\ndevelopers. We present these results in detail and offer actionable\nrecommendations grounded in the real-world needs of practitioners."}
{"id": "2506.17313", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17313", "abs": "https://arxiv.org/abs/2506.17313", "authors": ["Jonathan Reif", "Daniel Dittler", "Milapji Singh Gill", "Tam√°s Farkas", "Valentin Stegmaier", "Felix Gehlhoff", "Tobias Kleinert", "Michael Weyrich"], "title": "An Expert Survey on Models and Digital Twins", "comment": "This article is accepted at CIRP ICME and for publication in Procedia\n  CIRP", "summary": "Digital Twins (DTs) are becoming increasingly vital for future industrial\napplications, enhancing monitoring, control, and optimization of physical\nassets. This enhancement is made possible by integrating various Digital Models\n(DMs) within DTs, which must interoperate to represent different system aspects\nand fulfill diverse application purposes. However, industry perspectives on the\nchallenges and research needs for integrating these models are rarely obtained.\nThus, this study conducts an expert survey across multiple application domains\nto identify and analyze the challenges in utilizing diverse DMs within DTs. The\nresults reveal missing standardized interfaces, high manual adaptation effort,\nand limited support for model reuse across lifecycle phases, highlighting\nfuture research needs in automated model composition and semantics-based\ninteroperability."}
{"id": "2506.17330", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17330", "abs": "https://arxiv.org/abs/2506.17330", "authors": ["Simon Thorne"], "title": "Large Language Models for Spreadsheets: Benchmarking Progress and Evaluating Performance with FLARE", "comment": "18 Pages, 10 Tables, 1 Colour Figure", "summary": "Large Language Models (LLMs) have demonstrated some significant capabilities\nacross various domains; however, their effectiveness in spreadsheet related\ntasks remains underexplored. This study introduces a foundation for a\ncomprehensive benchmark framework to evaluate the performance of leading LLMs\nin executing spreadsheet functions, formula generation and data manipulation\ntasks. The benchmark encompasses tasks ranging from basic formula creation to\ncomplex, real world spreadsheet scenarios. Our findings reveal that while LLMs\nexhibit proficiency in straightforward tasks, they often falter in complex,\nmulti step operations, frequently producing plausible yet incorrect outputs.\nThese results underscore the limitations of current LLMs in handling\nspreadsheet tasks that require precise logical reasoning and highlight the need\nfor integrating symbolic reasoning capabilities into LLM architectures. To\nsupport this, we introduce FLARE (Formula Logic, Auditing, Reasoning and\nEvaluation) a new benchmark for evaluating LLM performance on real-world\nspreadsheet logic, auditing, and reasoning tasks."}
{"id": "2506.17335", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17335", "abs": "https://arxiv.org/abs/2506.17335", "authors": ["Shuo Yan", "Ruochen Li", "Ziming Luo", "Zimu Wang", "Daoyang Li", "Liqiang Jing", "Kaiyu He", "Peilin Wu", "George Michalopoulos", "Yue Zhang", "Ziyang Zhang", "Mian Zhang", "Zhiyu Chen", "Xinya Du"], "title": "LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research", "comment": null, "summary": "Large language model (LLM) agents have demonstrated remarkable potential in\nadvancing scientific discovery. However, their capability in the fundamental\nyet crucial task of reproducing code from research papers, especially in the\nNLP domain, remains underexplored. This task includes unique complex reasoning\nchallenges in the intellectual synthesis of abstract concepts and the\ncomprehension of code repositories with interdependent files. Motivated by this\ngap, we present LMR-BENCH, a benchmark designed to systematically evaluate the\ncapability of LLM agents on code reproduction from Language Modeling Research.\nIt consists of 28 code reproduction tasks derived from 23 research papers\npublished in top-tier NLP venues over the past five years, spanning nine\nfundamental categories. Models are provided with a research paper, a code\nrepository containing one or more masked functions, and instructions for\nimplementing these functions. We conduct extensive experiments in standard\nprompting and LLM agent settings with state-of-the-art LLMs, evaluating the\naccuracy of unit tests and performing LLM-based evaluation of code correctness.\nExperimental results reveal that even the most advanced models still exhibit\npersistent limitations in scientific reasoning and code synthesis, highlighting\ncritical gaps in LLM agents' ability to autonomously reproduce scientific\nresearch"}
{"id": "2506.17369", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17369", "abs": "https://arxiv.org/abs/2506.17369", "authors": ["Zhiyuan Pan", "Xing Hu", "Xin Xia", "Xiaohu Yang"], "title": "Re-Evaluating Code LLM Benchmarks Under Semantic Mutation", "comment": null, "summary": "In the era of large language models (LLMs), code benchmarks have become an\nimportant research area in software engineering and are widely used by\npractitioners. These benchmarks evaluate the performance of LLMs on specific\ncode-related tasks, such as code understanding and generation. A critical step\nin constructing code benchmarks is the design of prompts. However, as existing\ncode benchmarks typically rely on a single prompt template per task, they are\nprone to the issue of prompt sensitivity, where minor prompt variations could\nresult in substantial performance variations, leading to unreliable evaluations\nof model capabilities.\n  While previous studies have explored prompt sensitivity, their experimental\ndesigns and findings are limited to traditional natural language processing\n(NLP) tasks. In this paper, we present an empirical study to investigate prompt\nsensitivity in code benchmarks. We first propose a general framework that\nmodifies prompt templates in a manner that preserves both their semantics and\ntheir structure as much as possible. Based on the framework, we conduct\nextensive experiments across eight code benchmark tasks on 10 representative\nopen-source LLMs, with each task featuring 100 semantically similar prompt\ntemplates. We then analyze the evaluation results using various statistical\nmetrics, focusing on both absolute and relative model performance. Our findings\nsuggest that even slight prompt variations can lead to significant shifts in\nperformance. Additionally, we observe that such variations can introduce\ninconsistencies in the performance rankings across different models. These\ninsights highlight the need for considering prompt sensitivity when designing\nfuture code benchmarks, to ensure more reliable and accurate evaluation of LLM\ncapabilities."}
{"id": "2506.17539", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17539", "abs": "https://arxiv.org/abs/2506.17539", "authors": ["Sidong Feng", "Changhao Du", "Huaxiao Liu", "Qingnan Wang", "Zhengwei Lv", "Mengfei Wang", "Chunyang Chen"], "title": "Breaking Single-Tester Limits: Multi-Agent LLMs for Multi-User Feature Testing", "comment": "Accepted to International Conference on Software Engineering (ICSE\n  2026)", "summary": "The growing dependence on mobile phones and their apps has made multi-user\ninteractive features, like chat calls, live streaming, and video conferencing,\nindispensable for bridging the gaps in social connectivity caused by physical\nand situational barriers. However, automating these interactive features for\ntesting is fraught with challenges, owing to their inherent need for timely,\ndynamic, and collaborative user interactions, which current automated testing\nmethods inadequately address. Inspired by the concept of agents designed to\nautonomously and collaboratively tackle problems, we propose MAdroid, a novel\nmulti-agent approach powered by the Large Language Models (LLMs) to automate\nthe multi-user interactive task for app feature testing. Specifically, MAdroid\nemploys two functional types of multi-agents: user agents (Operator) and\nsupervisor agents (Coordinator and Observer). Each agent takes a specific role:\nthe Coordinator directs the interactive task; the Operator mimics user\ninteractions on the device; and the Observer monitors and reviews the task\nautomation process. Our evaluation, which included 41 multi-user interactive\ntasks, demonstrates the effectiveness of our approach, achieving 82.9% of the\ntasks with 96.8% action similarity, outperforming the ablation studies and\nstate-of-the-art baselines. Additionally, a preliminary investigation\nunderscores MAdroid's practicality by helping identify 11 multi-user\ninteractive bugs during regression app testing, confirming its potential value\nin real-world software development contexts."}
{"id": "2506.17627", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17627", "abs": "https://arxiv.org/abs/2506.17627", "authors": ["Hongzhou Rao", "Yanjie Zhao", "Wenjie Zhu", "Ling Xiao", "Meizhen Wang", "Haoyu Wang"], "title": "CodeMorph: Mitigating Data Leakage in Large Language Model Assessment", "comment": "Accepted by ICSE 2025 (Industry Challenge Track)", "summary": "Concerns about benchmark leakage in large language models for code (Code\nLLMs) have raised issues of data contamination and inflated evaluation metrics.\nThe diversity and inaccessibility of many training datasets make it difficult\nto prevent data leakage entirely, even with time lag strategies. Consequently,\ngenerating new datasets through code perturbation has become essential.\nHowever, existing methods often fail to produce complex and diverse variations,\nstruggle with complex cross-file dependencies, and lack support for multiple\nprogramming languages, which limits their effectiveness in enhancing LLM\nevaluations for coding tasks. To fill this gap, we propose CodeMorph, an\napproach designed to support multiple programming languages while preserving\ncross-file dependencies to mitigate data leakage. CodeMorph consists of two\nmain components that work together to enhance the perturbation process. The\nfirst component employs 26 semantic-preserving transformation methods to\niteratively perturb code, generating diverse variations while ensuring that the\nmodified code remains compilable. The second component introduces a genetic\nalgorithm-based selection algorithm, PESO, to identify the more effective\nperturbation method for each iteration by targeting lower similarity scores\nbetween the perturbed and original code, thereby enhancing overall perturbation\neffectiveness. Experimental results demonstrate that after applying CodeMorph,\nthe accuracy of the LLM on code completion tasks across five programming\nlanguages decreased by an average of 24.67%, with Python showing the most\nsignificant reduction at 45%. The similarity score of code optimized by PESO\nis, on average, 7.01% lower than that of randomly perturbed code, peaking at a\nreduction of 42.86%."}
{"id": "2506.17638", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17638", "abs": "https://arxiv.org/abs/2506.17638", "authors": ["Yanzhou Mu", "Rong Wang", "Juan Zhai", "Chunrong Fang", "Xiang Chen", "Zhiyuan Peng", "Peiran Yang", "Ruixiang Qian", "Shaoyu Yang", "Zhenyu Chen"], "title": "Deep Learning Framework Testing via Model Mutation: How Far Are We?", "comment": "27 pages, 9 figures", "summary": "Deep Learning (DL) frameworks are a fundamental component of DL development.\nTherefore, the detection of DL framework defects is important and challenging.\nAs one of the most widely adopted DL testing techniques, model mutation has\nrecently gained significant attention. In this study, we revisit the defect\ndetection ability of existing mutation-based testing methods and investigate\nthe factors that influence their effectiveness. To begin with, we reviewed\nexisting methods and observed that many of them mutate DL models (e.g.,\nchanging their parameters) without any customization, ignoring the unique\nchallenges in framework testing. Another issue with these methods is their\nlimited effectiveness, characterized by a high rate of false positives caused\nby illegal mutations arising from the use of generic, non-customized mutation\noperators. Moreover, we tracked the defects identified by these methods and\ndiscovered that most of them were ignored by developers. Motivated by these\nobservations, we investigate the effectiveness of existing mutation-based\ntesting methods in detecting important defects that have been authenticated by\nframework developers. We begin by collecting defect reports from three popular\nframeworks and classifying them based on framework developers' ratings to build\na comprehensive dataset. We then perform an in-depth analysis to uncover\nvaluable insights. Based on our findings, we propose optimization strategies to\naddress the shortcomings of existing approaches. Following these optimizations,\nwe identified seven new defects, four of which were confirmed by developers as\nhigh-priority issues, with three resolved. In summary, we identified 39 unique\ndefects across just 23 models, of which 31 were confirmed by developers, and\neight have been fixed."}
{"id": "2506.17642", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17642", "abs": "https://arxiv.org/abs/2506.17642", "authors": ["Shaoyu Yang", "Chunrong Fang", "Haifeng Lin", "Xiang Chen", "Zhenyu Chen"], "title": "May the Feedback Be with You! Unlocking the Power of Feedback-Driven Deep Learning Framework Fuzzing via LLMs", "comment": null, "summary": "Artificial Intelligence (AI) Infrastructures, represented by Deep Learning\n(DL) frameworks, have served as fundamental DL systems over the last decade.\nHowever, the bugs in DL frameworks could lead to catastrophic consequences in\nsome critical scenarios (e.g., healthcare and autonomous driving). A simple yet\neffective way to find bugs in DL frameworks is fuzz testing (Fuzzing).\nUnfortunately, existing fuzzing techniques have not comprehensively considered\nmultiple types of feedback. Additionally, they analyze feedback in a\ncoarse-grained manner, such as mutating the test cases only according to\nwhether the coverage increases. Recently, researchers introduced Large Language\nModels (LLMs) into fuzzing. However, current LLM-based fuzzing techniques only\nfocus on using LLMs to generate test cases while overlooking their potential to\nanalyze feedback information, failing to create more valid and diverse test\ncases. To fill this gap, we propose FUEL to break the seal of Feedback-driven\nfuzzing for DL frameworks. The backbone of FUEL comprises two LLM-based agents,\nnamely analysis LLM and generation LLM. Analysis LLM agent infers analysis\nsummaries from feedback information, while the generation LLM agent creates\ntests guided by these analysis summaries. So far, FUEL has detected 104 bugs\nfor PyTorch and TensorFlow, with 93 confirmed as new bugs, 47 already fixed,\nand 5 assigned with CVE IDs. Our work indicates that considering multiple types\nof feedback is beneficial to fuzzing performance, and leveraging LLMs to\nanalyze feedback information is a promising direction. Our artifact is\navailable at https://github.com/NJU-iSE/FUEL"}
{"id": "2506.17647", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17647", "abs": "https://arxiv.org/abs/2506.17647", "authors": ["Yixian Qi", "Jiajun Jiang", "Fengjie Li", "Bowen Chen", "Hongyu Zhang", "Junjie Chen"], "title": "Improving Compiler Bug Isolation by Leveraging Large Language Models", "comment": "12 pages, 7 figures", "summary": "Compilers play a foundational role in building reliable software systems, and\nbugs within them can lead to catastrophic consequences. The compilation process\ntypically involves hundreds of files, making traditional automated bug\nisolation techniques inapplicable due to scalability or effectiveness issues.\nCurrent mainstream compiler bug localization techniques have limitations in\ntest program mutation and resource consumption. Inspired by the recent advances\nof pre-trained Large Language Models (LLMs), we propose an innovative approach\nnamed AutoCBI, which (1) uses LLMs to summarize compiler file functions and (2)\nemploys specialized prompts to guide LLM in reordering suspicious file\nrankings. This approach leverages four types of information: the failing test\nprogram, source file function summaries, lists of suspicious files identified\nthrough analyzing test coverage, as well as compilation configurations with\nrelated output messages, resulting in a refined ranking of suspicious files.\nOur evaluation of AutoCBI against state-of-the-art approaches (DiWi, RecBi and\nFuseFL) on 120 real-world bugs from the widely-used GCC and LLVM compilers\ndemonstrates its effectiveness. Specifically, AutoCBI isolates 66.67%/69.23%,\n300%/340%, and 100%/57.14% more bugs than RecBi, DiWi, and FuseFL,\nrespectively, in the Top-1 ranked results for GCC/LLVM. Additionally, the\nablation study underscores the significance of each component in our approach."}
{"id": "2506.17772", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17772", "abs": "https://arxiv.org/abs/2506.17772", "authors": ["Haoran Xue", "Gias Uddin", "Song Wang"], "title": "PAGENT: Learning to Patch Software Engineering Agents", "comment": null, "summary": "LLM Agents produce patches automatically to resolve an issue. However, they\ncan generate inaccurate patches. Little is known about the root causes behind\nthose failed patches or how those could be fixed. This paper reports an\nempirical study of the failed patches generated by seven top LLM code agents.\nWe collected 114 issues from the SWE-bench Lite dataset that remained\nunresolved across the agents. The seven agents produced a total of 769 failed\npatches for those issues, which we checked with a combination of GPT-4o and\nmanual analysis. We present a taxonomy of the failure reasons across the\npatches. The taxonomy contains six categories, with several sub-categories\nunder each category. For example, a frequently observed category is the\ninability of an LLM to correctly infer/produce the appropriate variable type in\nthe produced patch. As a first step towards addressing such type-related\nerrors, we designed PAGENT (Patch Agent). PAGENT utilizes program analysis\ntechniques like CFG creation and exploration to infer the type of information\nof a patch. PAGENT does this by applying repository-level static code analysis\ntechniques. Then, PAGENT refines the inferred type by further utilizing an\nLLM-based inference technique. We tested PAGENT on all 127 type-related failed\npatches from the top three agents in our study. PAGENT could fix 29 of the 127\nfailed patches."}
{"id": "2506.17798", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.17798", "abs": "https://arxiv.org/abs/2506.17798", "authors": ["Wang Lingxiang", "Quanzhi Fu", "Wenjia Song", "Gelei Deng", "Yi Liu", "Dan Williams", "Ying Zhang"], "title": "SAVANT: Vulnerability Detection in Application Dependencies through Semantic-Guided Reachability Analysis", "comment": null, "summary": "The integration of open-source third-party library dependencies in Java\ndevelopment introduces significant security risks when these libraries contain\nknown vulnerabilities. Existing Software Composition Analysis (SCA) tools\nstruggle to effectively detect vulnerable API usage from these libraries due to\nlimitations in understanding API usage semantics and computational challenges\nin analyzing complex codebases, leading to inaccurate vulnerability alerts that\nburden development teams and delay critical security fixes.\n  To address these challenges, we proposed SAVANT by leveraging two insights:\nproof-of-vulnerability test cases demonstrate how vulnerabilities can be\ntriggered in specific contexts, and Large Language Models (LLMs) can understand\ncode semantics. SAVANT combines semantic preprocessing with LLM-powered context\nanalysis for accurate vulnerability detection. SAVANT first segments source\ncode into meaningful blocks while preserving semantic relationships, then\nleverages LLM-based reflection to analyze API usage context and determine\nactual vulnerability impacts. Our evaluation on 55 real-world applications\nshows that SAVANT achieves 83.8% precision, 73.8% recall, 69.0% accuracy, and\n78.5% F1-score, outperforming state-of-the-art SCA tools."}
{"id": "2506.17812", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17812", "abs": "https://arxiv.org/abs/2506.17812", "authors": ["Noble Saji Mathews", "Meiyappan Nagappan"], "title": "Is Your Automated Software Engineer Trustworthy?", "comment": null, "summary": "Large Language Models (LLMs) are being increasingly used in software\nengineering tasks, with an increased focus on bug report resolution over the\npast year. However, most proposed systems fail to properly handle uncertain or\nincorrect inputs and outputs. Existing LLM-based tools and coding agents\nrespond to every issue and generate a patch for every case, even when the input\nis vague or their own output is incorrect. There are no mechanisms in place to\nabstain when confidence is low. This leads to unreliable behaviour, such as\nhallucinated code changes or responses based on vague issue reports. We\nintroduce BouncerBench, a benchmark that evaluates whether LLM-based software\nagents can refuse to act when inputs are ill-defined or refuse to respond when\ntheir own outputs are likely to be incorrect. Unlike prior benchmarks that\nimplicitly incentivize models to generate responses even when uncertain,\nBouncerBench aims to improve precision by targeting two overlooked failure\npoints: (1) vague or underspecified issue descriptions in tickets and (2)\nlogically or functionally incorrect code patches created by the system. It\nmeasures whether proposed systems can distinguish actionable issues from vague\ntickets and valid patches from untrustworthy ones. We also implement a basic\ninput and output bouncer, evaluating how well current LLMs can abstain when\nneeded. Our results show that most models fail to abstain from underspecified\ninputs or incorrect outputs. Hence, we conclude that there is significant room\nfor improvement before LLMs can be trusted to make correct decisions and\nrecommendations in real-world software engineering workflows. BouncerBench\nprovides a first step toward evaluating and building more cautious, trustworthy\ncode agents. The replication package, dataset, and leaderboard can be found at\nbouncerbench.com"}
{"id": "2506.17833", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17833", "abs": "https://arxiv.org/abs/2506.17833", "authors": ["Giorgio Amasanti", "Jasmin Jahic"], "title": "The Impact of AI-Generated Solutions on Software Architecture and Productivity: Results from a Survey Study", "comment": "Accepted for presentation at the International Workshop on\n  AI-Assisted Software Architecting (AISA 2025), colocated with the 19th\n  European Conference on Software Architecture (ECSA 2025), to be held 15-19\n  September 2025 in Limassol, Cyprus", "summary": "AI-powered software tools are widely used to assist software engineers.\nHowever, there is still a need to understand the productivity benefits of such\ntools for software engineers. In addition to short-term benefits, there is a\nquestion of how adopting AI-generated solutions affects the quality of software\nover time (e.g., maintainability and extendability).\n  To provide some insight on these questions, we conducted a survey among\nsoftware practitioners who use AI tools. Based on the data collected from our\nsurvey, we conclude that AI tools significantly increase the productivity of\nsoftware engineers. However, the productivity benefits of using AI tools reduce\nas projects become more complex. The results also show that there are no\nsignificant negative influences of adopting AI-generated solutions on software\nquality, as long as those solutions are limited to smaller code snippets.\nHowever, when solving larger and more complex problems, AI tools generate\nsolutions of a lower quality, indicating the need for architects to perform\nproblem decomposition and solution integration."}
{"id": "2506.17937", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.17937", "abs": "https://arxiv.org/abs/2506.17937", "authors": ["Tommi Mikkonen", "Antero Taivalsaari"], "title": "Software Reuse in the Generative AI Era: From Cargo Cult Towards AI Native Software Engineering", "comment": null, "summary": "Software development is currently under a paradigm shift in which artificial\nintelligence and generative software reuse are taking the center stage in\nsoftware creation. Consequently, earlier software reuse practices and methods\nare rapidly being replaced by AI-assisted approaches in which developers place\ntheir trust on code that has been generated by artificial intelligence. This is\nleading to a new form of software reuse that is conceptually not all that\ndifferent from cargo cult development. In this paper we discuss the\nimplications of AI-assisted generative software reuse in the context of\nemerging \"AI native\" software engineering, bring forth relevant questions, and\ndefine a tentative research agenda and call to action for tackling some of the\ncentral issues associated with this approach."}
{"id": "2506.17948", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.17948", "abs": "https://arxiv.org/abs/2506.17948", "authors": ["Mahzabin Tamanna", "Yash Chandrani", "Matthew Burrows", "Brandon Wroblewski", "Laurie Williams", "Dominik Wermke"], "title": "Build It Clean: Large-Scale Detection of Code Smells in Build Scripts", "comment": "12 pages, 5 tables, 2 figures", "summary": "Build scripts are files that automate the process of compiling source code,\nmanaging dependencies, running tests, and packaging software into deployable\nartifacts. These scripts are ubiquitous in modern software development\npipelines for streamlining testing and delivery. While developing build\nscripts, practitioners may inadvertently introduce code smells. Code smells are\nrecurring patterns of poor coding practices that may lead to build failures or\nincrease risk and technical debt. The goal of this study is to aid\npractitioners in avoiding code smells in build scripts through an empirical\nstudy of build scripts and issues on GitHub. We employed a mixed-methods\napproach, combining qualitative and quantitative analysis. We conducted a\nqualitative analysis of 2000 build-script-related GitHub issues. Next, we\ndeveloped a static analysis tool, Sniffer, to identify code smells in 5882\nbuild scripts of Maven, Gradle, CMake, and Make files, collected from 4877\nopen-source GitHub repositories. We identified 13 code smell categories, with a\ntotal of 10,895 smell occurrences, where 3184 were in Maven, 1214 in Gradle,\n337 in CMake, and 6160 in Makefiles.\n  Our analysis revealed that Insecure URLs were the most prevalent code smell\nin Maven build scripts, while Hardcoded Paths/URLs were commonly observed in\nboth Gradle and CMake scripts. Wildcard Usage emerged as the most frequent\nsmell in Makefiles. The co-occurrence analysis revealed strong associations\nbetween specific smell pairs of Hardcoded Paths/URLs with Duplicates, and\nInconsistent Dependency Management with Empty or Incomplete Tags, indicating\npotential underlying issues in the build script structure and maintenance\npractices. Based on our findings, we recommend strategies to mitigate the\nexistence of code smells in build scripts to improve the efficiency,\nreliability, and maintainability of software projects."}
{"id": "2506.18050", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18050", "abs": "https://arxiv.org/abs/2506.18050", "authors": ["Lyuye Zhang", "Jian Zhang", "Kaixuan Li", "Chong Wang", "Chengwei Liu", "Jiahui Wu", "Sen Chen", "Yaowen Zheng", "Yang Liu"], "title": "VFArchƒì: A Dual-Mode Framework for Locating Vulnerable Functions in Open-Source Software", "comment": "15 pages", "summary": "Software Composition Analysis (SCA) has become pivotal in addressing\nvulnerabilities inherent in software project dependencies. In particular,\nreachability analysis is increasingly used in Open-Source Software (OSS)\nprojects to identify reachable vulnerabilities (e.g., CVEs) through call\ngraphs, enabling a focus on exploitable risks. Performing reachability analysis\ntypically requires the vulnerable function (VF) to track the call chains from\ndownstream applications. However, such crucial information is usually\nunavailable in modern vulnerability databases like NVD. While directly\nextracting VF from modified functions in vulnerability patches is intuitive,\npatches are not always available. Moreover, our preliminary study shows that\nover 26% of VF do not exist in the modified functions. Meanwhile, simply\nignoring patches to search vulnerable functions suffers from overwhelming\nnoises and lexical gaps between descriptions and source code. Given that almost\nhalf of the vulnerabilities are equipped with patches, a holistic solution that\nhandles both scenarios with and without patches is required. To meet real-world\nneeds and automatically localize VF, we present VFArch\\=e, a dual-mode approach\ndesigned for disclosed vulnerabilities, applicable in scenarios with or without\navailable patch links. The experimental results of VFArch\\=e on our constructed\nbenchmark dataset demonstrate significant efficacy regarding three metrics,\nachieving 1.3x and 1.9x Mean Reciprocal Rank over the best baselines for\nPatch-present and Patch-absent modes, respectively. Moreover, VFArch\\=e has\nproven its applicability in real-world scenarios by successfully locating VF\nfor 43 out of 50 latest vulnerabilities with reasonable efforts and\nsignificantly reducing 78-89% false positives of SCA tools."}
{"id": "2506.18191", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.18191", "abs": "https://arxiv.org/abs/2506.18191", "authors": ["Masudul Hasan Masud Bhuiyan", "Gianluca De Stefano", "Giancarlo Pellegrino", "Cristian-Alexandru Staicu"], "title": "Call Me Maybe: Enhancing JavaScript Call Graph Construction using Graph Neural Networks", "comment": null, "summary": "Static analysis plays a key role in finding bugs, including security issues.\nA critical step in static analysis is building accurate call graphs that model\nfunction calls in a program. However, due to hard-to-analyze language features,\nexisting call graph construction algorithms for JavaScript are neither sound\nnor complete. Prior work shows that even advanced solutions produce false edges\nand miss valid ones. In this work, we assist these tools by identifying missed\ncall edges. Our main idea is to frame the problem as link prediction on full\nprogram graphs, using a rich representation with multiple edge types. Our\napproach, GRAPHIA, leverages recent advances in graph neural networks to model\nnon-local relationships between code elements. Concretely, we propose\nrepresenting JavaScript programs using a combination of syntactic- and\nsemantic-based edges. GRAPHIA can learn from imperfect labels, including static\ncall edges from existing tools and dynamic edges from tests, either from the\nsame or different projects. Because call graphs are sparse, standard machine\nlearning metrics like ROC are not suitable. Instead, we evaluate GRAPHIA by\nranking function definitions for each unresolved call site. We conduct a\nlarge-scale evaluation on 50 popular JavaScript libraries with 163K call edges\n(150K static and 13K dynamic). GRAPHIA builds program graphs with 6.6M\nstructural and 386K semantic edges. It ranks the correct target as the top\ncandidate in over 42% of unresolved cases and within the top 5 in 72% of cases,\nreducing the manual effort needed for analysis. Our results show that\nlearning-based methods can improve the recall of JavaScript call graph\nconstruction. To our knowledge, this is the first work to apply GNN-based link\nprediction to full multi-file program graphs for interprocedural analysis."}
{"id": "2506.18219", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18219", "abs": "https://arxiv.org/abs/2506.18219", "authors": ["Ulrike M. Graetsch", "Rashina Hoda", "Hourieh Khalazjadeh", "Mojtaba Shahin", "John Grundy"], "title": "Managing Technical Debt in a Multidisciplinary Data Intensive Software Team: an Observational Case Study", "comment": "25 pages", "summary": "Context: There is an increase in the investment and development of\ndata-intensive (DI) solutions, systems that manage large amounts of data.\nWithout careful management, this growing investment will also grow associated\ntechnical debt (TD). Delivery of DI solutions requires a multidisciplinary\nskill set, but there is limited knowledge about how multidisciplinary teams\ndevelop DI systems and manage TD.\n  Objective: This research contributes empirical, practice based insights about\nmultidisciplinary DI team TD management practices.\n  Method: This research was conducted as an exploratory observation case study.\nWe used socio-technical grounded theory (STGT) for data analysis to develop\nconcepts and categories that articulate TD and TDs debt management practices.\n  Results: We identify TD that the DI team deals with, in particular technical\ndata components debt and pipeline debt. We explain how the team manages the TD,\nassesses TD, what TD treatments they consider and how they implement TD\ntreatments to fit sprint capacity constraints.\n  Conclusion: We align our findings to existing TD and TDM taxonomies, discuss\ntheir implications and highlight the need for new implementation patterns and\ntool support for multidisciplinary DI teams."}
{"id": "2506.18289", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18289", "abs": "https://arxiv.org/abs/2506.18289", "authors": ["Saurabhsingh Rajput", "Mootez Saad", "Tushar Sharma"], "title": "Tu(r)ning AI Green: Exploring Energy Efficiency Cascading with Orthogonal Optimizations", "comment": "In review", "summary": "AI's exponential growth intensifies computational demands and energy\nchallenges. While practitioners employ various optimization techniques, that we\nrefer as \"knobs\" in this paper, to tune model efficiency, these are typically\nafterthoughts and reactive ad-hoc changes applied in isolation without\nunderstanding their combinatorial effects on energy efficiency. This paper\nemphasizes on treating energy efficiency as the first-class citizen and as a\nfundamental design consideration for a compute-intensive pipeline. We show that\nstrategic selection across five AI pipeline phases (data, model, training,\nsystem, inference) creates cascading efficiency. Experimental validation shows\northogonal combinations reduce energy consumption by up to $94.6$% while\npreserving $95.95$% of the original F1 score of non-optimized pipelines. This\ncurated approach provides actionable frameworks for informed sustainable AI\nthat balance efficiency, performance, and environmental responsibility."}
{"id": "2506.18315", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18315", "abs": "https://arxiv.org/abs/2506.18315", "authors": ["Lehan He", "Zeren Chen", "Zhe Zhang", "Jing Shao", "Xiang Gao", "Lu Sheng"], "title": "Use Property-Based Testing to Bridge LLM Code Generation and Validation", "comment": null, "summary": "Large Language Models (LLMs) excel at code generation, but ensuring their\noutputs to be functionally correct, especially in complex programming tasks, is\na persistent challenge. While traditional Test-Driven Development (TDD) offers\na path for code refinement, its efficacy with LLMs is often undermined by the\nscarcity of high-quality test cases or the pitfalls of automated test\ngeneration, including biased tests or inaccurate output predictions that can\nmisdirect the correction process. This paper introduces Property-Generated\nSolver, a novel framework that leverages Property-Based Testing (PBT) to\nvalidate high-level program properties or invariants, instead of relying on\nspecific input-output examples. These properties are often simpler to define\nand verify than directly predicting exhaustive test oracles, breaking the\n\"cycle of self-deception\" where tests might share flaws with the code they are\nmeant to validate. Property-Generated Solver employs two collaborative\nLLM-based agents: a Generator dedicated to code generation and iterative\nrefinement, and a Tester that manages the PBT life-cycle and formulate\nsemantically rich feedback from property violations. The resulting\ncomprehensive and actionable feedback then guides the Generator in its\nrefinement efforts. By establishing PBT as the core validation engine within\nthis iterative, closed-loop paradigm, Property-Generated Solver provides a\nrobust mechanism for steering LLMs towards more correct and generalizable code.\nExtensive experimental results on multiple code generation benchmarks\ndemonstrate that Property-Generated Solver achieves substantial pass@1\nimprovements, ranging from 23.1% to 37.3% relative gains over established TDD\nmethods."}
{"id": "2506.18329", "categories": ["cs.SE", "D.2.8; C.4; I.2.7; I.2.6; I.5.1"], "pdf": "https://arxiv.org/pdf/2506.18329", "abs": "https://arxiv.org/abs/2506.18329", "authors": ["Elijah Zolduoarrati", "Sherlock A. Licorish", "Nigel Stanger"], "title": "Predictive Analytics for Collaborators Answers, Code Quality, and Dropout on Stack Overflow", "comment": "46 pages, 17 tables, 7 figures", "summary": "Previous studies that used data from Stack Overflow to develop predictive\nmodels often employed limited benchmarks of 3-5 models or adopted arbitrary\nselection methods. Despite being insightful, their limited scope suggests the\nneed to benchmark more models to avoid overlooking untested algorithms. Our\nstudy evaluates 21 algorithms across three tasks: predicting the number of\nquestion a user is likely to answer, their code quality violations, and their\ndropout status. We employed normalisation, standardisation, as well as\nlogarithmic and power transformations paired with Bayesian hyperparameter\noptimisation and genetic algorithms. CodeBERT, a pre-trained language model for\nboth natural and programming languages, was fine-tuned to classify user dropout\ngiven their posts (questions and answers) and code snippets. We found Bagging\nensemble models combined with standardisation achieved the highest R2 value\n(0.821) in predicting user answers. The Stochastic Gradient Descent regressor,\nfollowed by Bagging and Epsilon Support Vector Machine models, consistently\ndemonstrated superior performance to other benchmarked algorithms in predicting\nuser code quality across multiple quality dimensions and languages. Extreme\nGradient Boosting paired with log-transformation exhibited the highest F1-score\n(0.825) in predicting user dropout. CodeBERT was able to classify user dropout\nwith a final F1-score of 0.809, validating the performance of Extreme Gradient\nBoosting that was solely based on numerical data. Overall, our benchmarking of\n21 algorithms provides multiple insights. Researchers can leverage findings\nregarding the most suitable models for specific target variables, and\npractitioners can utilise the identified optimal hyperparameters to reduce the\ninitial search space during their own hyperparameter tuning processes."}
{"id": "2506.18359", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18359", "abs": "https://arxiv.org/abs/2506.18359", "authors": ["Juanita Gomez", "Emily Lovell", "Stephanie Lieggi", "Alvaro A. Cardenas", "James Davis"], "title": "Recipe for Discovery: A Framework for Systematic Open Source Project Identification", "comment": null, "summary": "Open source software development, particularly within institutions such as\nuniversities and research laboratories, is often decentralized and difficult to\ntrack. Despite producing highly impactful tools in science, these efforts often\ngo unrecognized due to a lack of visibility and institutional awareness. This\npaper addresses the challenge of discovering, classifying, and analyzing open\nsource software projects developed across distributed institutional systems. We\npresent a framework for systematically identifying institutional affiliated\nrepositories, using the University of California (UC) system as a case study.\n  Using GitHub's REST API, we build a pipeline to discover relevant\nrepositories and extract meaningful metadata. We then propose and evaluate\nmultiple classification strategies, including both traditional machine learning\nmodels and large language models (LLMs), to distinguish affiliated projects\nfrom unrelated repositories and generate accurate insights into the academic\nopen source landscape. Our results show that the framework is effective at\nscale, discovering over 52,000 repositories and predicting institutional\naffiliation with high accuracy."}
{"id": "2506.18394", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18394", "abs": "https://arxiv.org/abs/2506.18394", "authors": ["Xiao Cheng", "Zhihao Guo", "Huan Huo", "Yulei Sui"], "title": "Tracing Errors, Constructing Fixes: Repository-Level Memory Error Repair via Typestate-Guided Context Retrieval", "comment": null, "summary": "Memory-related errors in C programming continue to pose significant\nchallenges in software development, primarily due to the complexities of manual\nmemory management inherent in the language. These errors frequently serve as\nvectors for severe vulnerabilities, while their repair requires extensive\nknowledge of program logic and C's memory model. Automated Program Repair (APR)\nhas emerged as a critical research area to address these challenges.\nTraditional APR approaches rely on expert-designed strategies and predefined\ntemplates, which are labor-intensive and constrained by the effectiveness of\nmanual specifications. Deep learning techniques offer a promising alternative\nby automatically extracting repair patterns, but they require substantial\ntraining datasets and often lack interpretability.\n  This paper introduces LTFix, a novel approach that harnesses the potential of\nLarge Language Models (LLMs) for automated memory error repair, especially for\ncomplex repository-level errors that span multiple functions and files. We\naddress two fundamental challenges in LLM-based memory error repair: a limited\nunderstanding of interprocedural memory management patterns and context window\nlimitations for repository-wide analysis. Our approach utilizes a finite\ntypestate automaton to guide the tracking of error-propagation paths and\ncontext trace, capturing both spatial (memory states) and temporal (execution\nhistory) dimensions of error behavior. This typestate-guided context retrieval\nstrategy provides the LLM with concise yet semantically rich information\nrelevant to erroneous memory management, effectively addressing the token\nlimitation of LLMs."}
{"id": "2506.18398", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18398", "abs": "https://arxiv.org/abs/2506.18398", "authors": ["Hao Wu", "Haijun Wang", "Shangwang Li", "Yin Wu", "Ming Fan", "Wuxia Jin", "Yitao Zhao", "Ting Liu"], "title": "Your Token Becomes Worthless: Unveiling Rug Pull Schemes in Crypto Token via Code-and-Transaction Fusion Analysis", "comment": null, "summary": "Rug pull scams have emerged as a persistent threat to cryptocurrency, causing\nsignificant financial losses. A typical scenario involves scammers deploying\nhoneypot contracts to attract investments, restricting token sales, and\ndraining the funds, which leaves investors with worthless tokens. Current\nmethods either rely on predefined patterns to detect code risks or utilize\nstatistical transaction data to train detection models. However, real-world Rug\nPull schemes often involve a complex interplay between malicious code and\nsuspicious transaction behaviors. These methods, which solely focus on one\naspect, fall short in detecting such schemes effectively.\n  In this paper, we propose RPhunter, a novel technique that integrates code\nand transaction for Rug Pull detection. First, RPhunter establishes declarative\nrules and performs flow analysis to extract code risk information, further\nconstructing a semantic risk code graph (SRCG). Meanwhile, to leverage\ntransaction information, RPhunter formulates dynamic token transaction\nactivities as a token flow behavior graph (TFBG) in which nodes and edges are\ncharacterized from network structure and market manipulation perspectives.\nFinally, RPhunter employs graph neural networks to extract complementary\nfeatures from SRCG and TFBG, integrating them through an attention fusion model\nto enhance the detection of Rug Pull. We manually analyzed 645 Rug Pull\nincidents from code and transaction aspects and constructed a ground-truth\ndataset. We evaluated RPhunter on our dataset, achieving a precision of 95.3%,\na recall of 93.8% and an F1 score of 94.5%, which highlights superior\nperformance compared to existing state-of-the-art methods. Furthermore, when\napplied to the real-world scenarios, RPhunter has identified 4801 Rug Pull\ntokens, achieving a precision of 91%."}
{"id": "2506.18403", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18403", "abs": "https://arxiv.org/abs/2506.18403", "authors": ["Muntasir Adnan", "Carlos C. N. Kuhn"], "title": "The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs", "comment": null, "summary": "The effectiveness of AI debugging follows a predictable exponential decay\npattern; most models lose 60-80% of their debugging capability within just 2-3\nattempts, despite iterative debugging being a critical capability for practical\ncode generation systems. We introduce the Debugging Decay Index (DDI), a\nmathematical framework that quantifies when debugging becomes ineffective and\npredicts intervention points. Our strategic fresh start approach shifts from\nexploitation to exploration at strategic points in the debugging process,\ndemonstrating that well-timed interventions can rescue the effectiveness of\ndebugging. DDI reveals a fundamental limitation in current AI debugging and\nprovides the first quantitative framework for optimising iterative code\ngeneration strategies."}
{"id": "2506.18790", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18790", "abs": "https://arxiv.org/abs/2506.18790", "authors": ["Mohamad Omar Nachawati"], "title": "ModeliHub: A Web-based, Federated Analytics Platform for Modelica-centric, Model-based Systems Engineering", "comment": null, "summary": "This paper introduces ModeliHub, a Web-based, federated analytics platform\ndesigned specifically for model-based systems engineering with Modelica.\nModeliHub's key innovation lies in its Modelica-centric, hub-and-spoke\nfederation architecture that provides systems engineers with a Modelica-based,\nunified system model of repositories containing heterogeneous engineering\nartifacts. From this unified system model, ModeliHub's Virtual Twin engine\nprovides a real-time, interactive simulation environment for deploying Modelica\nsimulation models that represent digital twins of the virtual prototype of the\nsystem under development at a particular iteration of the iterative systems\nengineering life cycle. The implementation of ModeliHub is centered around its\nextensible, Modelica compiler frontend developed in Isomorphic TypeScript that\ncan run seamlessly across browser, desktop and server environments. This\narchitecture aims to strike a balance between rigor and agility, enabling\nseamless integration and analysis across various engineering domains."}
{"id": "2506.18796", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.18796", "abs": "https://arxiv.org/abs/2506.18796", "authors": ["Kishanthan Thangarajah", "Boyuan Chen", "Shi Chang", "Ahmed E. Hassan"], "title": "Context-Aware CodeLLM Eviction for AI-assisted Coding", "comment": "12 pages, 6 figures", "summary": "AI-assisted coding tools powered by Code Large Language Models (CodeLLMs) are\nincreasingly integrated into modern software development workflows. To address\nconcerns around privacy, latency, and model customization, many enterprises opt\nto self-host these models. However, the diversity and growing number of\nCodeLLMs, coupled with limited accelerator memory, introduce practical\nchallenges in model management and serving efficiency. This paper presents\nCACE, a novel context-aware model eviction strategy designed specifically to\noptimize self-hosted CodeLLM serving under resource constraints. Unlike\ntraditional eviction strategies based solely on recency (e.g., Least Recently\nUsed), CACE leverages multiple context-aware factors, including model load\ntime, task-specific latency sensitivity, expected output length, and recent\nusage and future demand tracked through a sliding window. We evaluate CACE\nusing realistic workloads that include both latency-sensitive code completion\nand throughput-intensive code reasoning tasks. Our experiments show that CACE\nreduces Time-to-First-Token (TTFT) and end-to-end (E2E) latency, while\nsignificantly lowering the number of model evictions compared to\nstate-of-the-art systems. Ablation studies further demonstrate the importance\nof multi-factor eviction in balancing responsiveness and resource efficiency.\nThis work contributes practical strategies for deploying scalable, low-latency\nAI coding assistants in real-world software engineering environments."}
{"id": "2506.18824", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18824", "abs": "https://arxiv.org/abs/2506.18824", "authors": ["Islem Bouzenia", "Michael Pradel"], "title": "Understanding Software Engineering Agents: A Study of Thought-Action-Result Trajectories", "comment": null, "summary": "Large Language Model (LLM)-based agents are increasingly employed to automate\ncomplex software engineering tasks such as program repair and issue resolution.\nThese agents operate by autonomously generating natural language thoughts,\ninvoking external tools, and iteratively refining their solutions. Despite\ntheir widespread adoption, the internal decision-making processes of these\nagents remain largely unexplored, limiting our understanding of their\noperational dynamics and failure modes. In this paper, we present a large-scale\nempirical study of the thought-action-result trajectories of three\nstate-of-the-art LLM-based agents: \\textsc{RepairAgent},\n\\textsc{AutoCodeRover}, and \\textsc{OpenHands}. We unify their interaction logs\ninto a common format, capturing 120 trajectories and 2822 LLM interactions\nfocused on program repair and issue resolution. Our study combines quantitative\nanalyses of structural properties, action patterns, and token usage with\nqualitative assessments of reasoning coherence and feedback integration. We\nidentify key trajectory characteristics such as iteration counts and token\nconsumption, recurring action sequences, and the semantic coherence linking\nthoughts, actions, and their results. Our findings reveal behavioral motifs and\nanti-patterns that distinguish successful from failed executions, providing\nactionable insights for improving agent design, including prompting strategies,\nfailure diagnosis, and anti-pattern detection. We release our dataset and\nannotation framework to support further research on transparent and robust\nautonomous software engineering agents."}
