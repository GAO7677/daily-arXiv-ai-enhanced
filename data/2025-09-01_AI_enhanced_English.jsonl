{"id": "2508.21097", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21097", "abs": "https://arxiv.org/abs/2508.21097", "authors": ["Nazanin Siavash", "Armin Moin"], "title": "Model-Driven Quantum Code Generation Using Large Language Models and Retrieval-Augmented Generation", "comment": "This paper is accepted to the New Ideas and Emerging Results (NIER)\n  track of the ACM/IEEE 28th International Conference on Model Driven\n  Engineering Languages and Systems (MODELS)", "summary": "This paper introduces a novel research direction for model-to-text/code\ntransformations by leveraging Large Language Models (LLMs) that can be enhanced\nwith Retrieval-Augmented Generation (RAG) pipelines. The focus is on quantum\nand hybrid quantum-classical software systems, where model-driven approaches\ncan help reduce the costs and mitigate the risks associated with the\nheterogeneous platform landscape and lack of developers' skills. We validate\none of the proposed ideas regarding generating code out of UML model instances\nof software systems. This Python code uses a well-established library, called\nQiskit, to execute on gate-based or circuit-based quantum computers. The RAG\npipeline that we deploy incorporates sample Qiskit code from public GitHub\nrepositories. Experimental results show that well-engineered prompts can\nimprove CodeBLEU scores by up to a factor of four, yielding more accurate and\nconsistent quantum code. However, the proposed research direction can go beyond\nthis through further investigation in the future by conducting experiments to\naddress our other research questions and ideas proposed here, such as deploying\nsoftware system model instances as the source of information in the RAG\npipelines, or deploying LLMs for code-to-code transformations, for instance,\nfor transpilation use cases.", "AI": {"tldr": "The paper demonstrates that augmenting large language models with RAG\u2014and incorporating real-world quantum code samples\u2014significantly improves the quality of quantum code generated from UML models. More research will explore additional applications in the future.", "motivation": "Quantum and hybrid quantum-classical software systems face challenges such as heterogeneous platforms and a shortage of skilled developers. Model-driven approaches could help address these issues, and augmenting LLMs with RAG could further improve automation and code quality in this domain.", "method": "The authors propose and validate leveraging LLMs enhanced by Retrieval-Augmented Generation (RAG) pipelines to generate code from UML model instances for quantum software systems. They used sample Qiskit code from public GitHub repositories within the RAG pipeline and tested prompt engineering effects on CodeBLEU scores for quantum code generation.", "result": "Experimental results show that engineered prompts in the RAG-enhanced LLM pipeline can improve CodeBLEU scores by up to four times, leading to more accurate and consistent quantum code.", "conclusion": "Model-to-code generation for quantum systems using LLMs enhanced with RAG is a promising approach. Well-designed prompts and relevant retrieved examples can greatly boost code quality, but there is further research needed to explore broader applications such as code transpilation and using software model instances as primary RAG sources."}}
{"id": "2508.21107", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21107", "abs": "https://arxiv.org/abs/2508.21107", "authors": ["Dongjun Lee", "Changho Hwang", "Kimin Lee"], "title": "Learning to Generate Unit Test via Adversarial Reinforcement Learning", "comment": "Code is available at: https://github.com/dgjun32/UTRL", "summary": "Unit testing is a core practice in programming, enabling systematic\nevaluation of programs produced by human developers or large language models\n(LLMs). Given the challenges in writing comprehensive unit tests, LLMs have\nbeen employed to automate test generation, yet methods for training LLMs to\nproduce high-quality tests remain underexplored. In this work, we propose UTRL,\na novel reinforcement learning framework that trains an LLM to generate\nhigh-quality unit tests given a programming instruction. Our key idea is to\niteratively train two LLMs, the unit test generator and the code generator, in\nan adversarial manner via reinforcement learning. The unit test generator is\ntrained to maximize a discrimination reward, which reflects its ability to\nproduce tests that expose faults in the code generator's solutions, and the\ncode generator is trained to maximize a code reward, which reflects its ability\nto produce solutions that pass the unit tests generated by the test generator.\nIn our experiments, we demonstrate that unit tests generated by Qwen3-4B\ntrained via UTRL show higher quality compared to unit tests generated by the\nsame model trained via supervised fine-tuning on human-written ground-truth\nunit tests, yielding code evaluations that more closely align with those\ninduced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL\noutperforms frontier models such as GPT-4.1 in generating high-quality unit\ntests, highlighting the effectiveness of UTRL in training LLMs for this task.", "AI": {"tldr": "The paper proposes UTRL, an adversarial reinforcement learning framework for training LLMs to generate high-quality unit tests. Experiments show that models trained with UTRL outperform both supervised methods and state-of-the-art models like GPT-4.1.", "motivation": "Writing comprehensive unit tests is challenging and automating this process with high quality is an unsolved problem. There is a need for better methods to train large language models (LLMs) to generate effective unit tests.", "method": "The paper introduces UTRL, a reinforcement learning framework where two LLMs (a unit test generator and a code generator) are trained adversarially. The test generator is rewarded for generating tests that reveal faults, while the code generator is rewarded for producing code that passes these tests. Both are refined through iterative adversarial training.", "result": "Experimental results show that the Qwen3-4B model trained with UTRL generates higher quality unit tests than when it is trained via supervised fine-tuning on human-written tests. The model also outperforms leading models like GPT-4.1 in creating effective unit tests.", "conclusion": "UTRL effectively trains LLMs to generate superior unit tests compared to traditional supervised fine-tuning or even top-performing LLMs, advancing the automation of high-quality software testing."}}
{"id": "2508.21156", "categories": ["cs.SE", "D.2.7; I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.21156", "abs": "https://arxiv.org/abs/2508.21156", "authors": ["Kiana Kiashemshaki", "Arsham Khosravani", "Alireza Hosseinpour", "Arshia Akhavan"], "title": "Automated Bug Triaging using Instruction-Tuned Large Language Models", "comment": "11 pages, 7 figures", "summary": "Bug triaging, the task of assigning new issues to developers, is often slow\nand inconsistent in large projects. We present a lightweight framework that\ninstruction-tuned large language model (LLM) with LoRA adapters and uses\ncandidate-constrained decoding to ensure valid assignments. Tested on\nEclipseJDT and Mozilla datasets, the model achieves strong shortlist quality\n(Hit at 10 up to 0.753) despite modest exact Top-1 accuracy. On recent\nsnapshots, accuracy rises sharply, showing the framework's potential for\nreal-world, human-in-the-loop triaging. Our results suggest that\ninstruction-tuned LLMs offer a practical alternative to costly feature\nengineering and graph-based methods.", "AI": {"tldr": "Proposes an LLM-based framework for faster, consistent bug triaging, outperforming traditional methods in shortlist quality, with practical real-world potential.", "motivation": "Bug triaging in large projects is slow and inconsistent, and current solutions are expensive or complex. The study aims to provide a more practical and scalable alternative.", "method": "Lightweight framework using instruction-tuned LLMs with LoRA adapters and candidate-constrained decoding for bug triaging. Evaluated on EclipseJDT and Mozilla datasets.", "result": "The model shows high shortlist quality (Hit@10 up to 0.753) but moderate exact Top-1 accuracy. Recent test data shows increased accuracy, indicating strong real-world applicability.", "conclusion": "Instruction-tuned LLMs, with proper adaptation and validation constraints, provide a feasible and cost-effective approach for bug triaging versus traditional expensive or complex solutions."}}
{"id": "2508.21433", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21433", "abs": "https://arxiv.org/abs/2508.21433", "authors": ["Tobias Lindenbauer", "Igor Slinko", "Ludwig Felder", "Egor Bogomolov", "Yaroslav Zharov"], "title": "The Complexity Trap: Simple Observation Masking Is as Efficient as LLM Summarization for Agent Context Management", "comment": null, "summary": "Large Language Model (LLM)-based agents solve complex tasks through iterative\nreasoning, exploration, and tool-use, a process that can result in long,\nexpensive context histories. While state-of-the-art Software Engineering ( SE)\nagents like OpenHands or Cursor use LLM-based summarization to tackle this\nissue, it is unclear whether the increased complexity offers tangible\nperformance benefits compared to simply omitting older observations. We present\na systematic comparison of these strategies within SWE-agent on SWE-bench\nVerified across five diverse model configurations. We find that a simple\nobservation-masking strategy halves cost relative to a raw agent while\nmatching, and sometimes slightly exceeding, the solve rate of LLM\nsummarization. For example, with Qwen3-Coder 480B, masking improves solve rate\nfrom 53.8% (raw agent) to 54.8%, while remaining competitive with summarization\nat a lower cost. These results suggest that, at least within SWE-agent on\nSWE-bench Verified, the most effective and efficient context management can be\nthe simplest. We release code and data for reproducibility", "AI": {"tldr": "Simple observation-masking manages LLM context in engineering agents as effectively\u2014or better than\u2014summarization methods, at half the computational cost.", "motivation": "LLM-based agents generate long, costly histories when solving complex tasks. Summarization is used to control this, but its benefit over much simpler strategies (like omitting old observations) has not been robustly compared.", "method": "Empirical, systematic comparison between observation-masking and LLM summarization methods for context pruning in SWE-agent on SWE-bench Verified using five model configurations. Key metrics: solve rate and computational cost.", "result": "The paper compares two strategies for managing long context histories in LLM-based software engineering agents (like SWE-agent): summarization (used by tools like OpenHands and Cursor) versus a simple observation-masking (pruning old observations). The experiments use the SWE-bench Verified benchmark across five different model configurations. Results show: masking reduces cost by half compared to using the raw agent and matches or slightly exceeds the solve rate of LLM-based summarization. With a large model (Qwen3-Coder 480B), masking even slightly improves the solve rate over the raw agent and remains as effective as summarization at a lower cost.", "conclusion": "In the SWE-agent on SWE-bench Verified, simple observation-masking is as effective and more efficient than LLM-based summarization for context management."}}
{"id": "2508.21256", "categories": ["cs.PL", "cs.CL", "cs.GR", "68N20, 68N15, 68W10", "D.3.4; D.3.2; D.1.3"], "pdf": "https://arxiv.org/pdf/2508.21256", "abs": "https://arxiv.org/abs/2508.21256", "authors": ["Nripesh Niketan", "Vaatsalya Shrivastva"], "title": "CrossTL: A Universal Programming Language Translator with Unified Intermediate Representation", "comment": "15 Pages, 5 Figures, 1 Table. Introduces CrossTL, a universal\n  programming language translator enabling bidirectional translation between 8\n  programming languages (CUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan\n  SPIR-V, Rust, Mojo) through a unified intermediate representation called\n  CrossGL. Includes comprehensive evaluation with complex real-world examples", "summary": "We present CrossTL, a universal programming language translator enabling\nbidirectional translation between multiple languages through a unified\nintermediate representation called CrossGL. Traditional approaches require\nseparate translators for each language pair, leading to exponential complexity\ngrowth. CrossTL uses a single universal IR to facilitate translations between\nCUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan SPIR-V, Rust, and Mojo,\nwith Slang support in development. Our system consists of: language-specific\nlexers/parsers converting source code to ASTs, bidirectional CrossGL\ntranslation modules implementing ToCrossGLConverter classes for importing code\nand CodeGen classes for target generation, and comprehensive backend\nimplementations handling full translation pipelines. We demonstrate\neffectiveness through comprehensive evaluation across programming domains,\nachieving successful compilation and execution across all supported backends.\nThe universal IR design enables adding new languages with minimal effort,\nrequiring only language-specific frontend/backend components. Our contributions\ninclude: (1) a unified IR capturing semantics of multiple programming\nparadigms, (2) a modular architecture enabling extensibility, (3) a\ncomprehensive framework supporting GPU compute, graphics programming, and\nsystems languages, and (4) empirical validation demonstrating practical\nviability of universal code translation. CrossTL represents a significant step\ntoward language-agnostic programming, enabling write-once, deploy-everywhere\ndevelopment.", "AI": {"tldr": "CrossTL introduces a universal programming language translator capable of bidirectional translation among many languages using a single intermediate representation, CrossGL, greatly simplifying language interoperability.", "motivation": "Existing language translation systems require separate, complex translators for each language pair, leading to inefficiency and scalability challenges.", "method": "CrossTL employs language-specific frontends to convert source code to ASTs and bidirectional translation modules utilizing a unified IR (CrossGL), with backends generating code for each target language.", "result": "The system supports translation and execution between CUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan SPIR-V, Rust, and Mojo, demonstrating successful compilation and practical extensibility for new languages.", "conclusion": "CrossTL enables practical universal code translation across several languages and paradigms, facilitating write-once, deploy-everywhere development."}}
{"id": "2508.21454", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.21454", "abs": "https://arxiv.org/abs/2508.21454", "authors": ["Baijun Cheng", "Kailong Wang", "Ling Shi", "Haoyu Wang", "Yao Guo", "Ding Li", "Xiangqun Chen"], "title": "Enhancing Semantic Understanding in Pointer Analysis using Large Language Models", "comment": "Accepted by LMPL 2025", "summary": "Pointer analysis has been studied for over four decades. However, existing\nframeworks continue to suffer from the propagation of incorrect facts. A major\nlimitation stems from their insufficient semantic understanding of code,\nresulting in overly conservative treatment of user-defined functions. Recent\nadvances in large language models (LLMs) present new opportunities to bridge\nthis gap. In this paper, we propose LMPA (LLM-enhanced Pointer Analysis), a\nvision that integrates LLMs into pointer analysis to enhance both precision and\nscalability. LMPA identifies user-defined functions that resemble system APIs\nand models them accordingly, thereby mitigating erroneous cross-calling-context\npropagation. Furthermore, it enhances summary-based analysis by inferring\ninitial points-to sets and introducing a novel summary strategy augmented with\nnatural language. Finally, we discuss the key challenges involved in realizing\nthis vision.", "AI": {"tldr": "LMPA integrates large language models into pointer analysis to improve precision and scalability by better handling user-defined functions and enhancing summary-based analysis.", "motivation": "Pointer analysis frameworks often propagate incorrect facts due to conservative treatments and lack of semantic understanding, especially regarding user-defined functions. This issue limits their precision and scalability.", "method": "LMPA uses LLMs to analyze code semantics, recognize user-defined functions similar to system APIs, infer initial points-to sets, and augment summary-based analysis with natural language strategies.", "result": "The proposed LMPA vision incorporates LLMs into pointer analysis to identify and properly model user-defined functions that resemble system APIs, reducing erroneous propagation and improving analysis quality.", "conclusion": "Incorporating LLMs into pointer analysis is a promising strategy to address longstanding issues of incorrect fact propagation, though there remain key implementation challenges."}}
{"id": "2508.21593", "categories": ["cs.PL", "cs.MS", "math.HO"], "pdf": "https://arxiv.org/pdf/2508.21593", "abs": "https://arxiv.org/abs/2508.21593", "authors": ["Anne Baanen", "Matthew Robert Ballard", "Johan Commelin", "Bryan Gin-ge Chen", "Michael Rothgang", "Damiano Testa"], "title": "Growing Mathlib: maintenance of a large scale mathematical library", "comment": "21 pages, 1 figure. To appear at Conference on Intelligent Computer\n  Mathematics (CICM) 2025", "summary": "The Lean mathematical library Mathlib is one of the fastest-growing libraries\nof formalised mathematics. We describe various strategies to manage this\ngrowth, while allowing for change and avoiding maintainer overload. This\nincludes dealing with breaking changes via a deprecation system, using code\nquality analysis tools (linters) to provide direct user feedback about common\npitfalls, speeding up compilation times through conscious library (re-)design,\ndealing with technical debt as well as writing custom tooling to help with the\nreview and triage of new contributions.", "AI": {"tldr": "Mathlib manages rapid growth by using deprecation systems, linters, optimized design, technical debt management, and custom tools\u2014helping maintain quality and sustainability without overburdening maintainers.", "motivation": "The rapid growth of Mathlib as a formalized mathematics library creates challenges in maintainability, quality control, and contributor scalability, necessitating systematic solutions to prevent maintainer overload and to ensure high-quality evolution of the codebase.", "method": "The paper presents and discusses various practical strategies and tooling: deprecation systems for breaking changes, linters for code quality, design adjustments to improve compilation times, active management of technical debt, and custom review tools for new contributions.", "result": "The library has adopted several effective practices and tools\u2014including deprecation mechanisms, linters, redesign for performance, debt management, and contribution review tools\u2014to balance swift expansion with manageable maintenance workload and quality assurance.", "conclusion": "Effective strategies can support the sustainable growth of large-scale mathematical libraries such as Mathlib, addressing change management, code quality, technical debt, and contributor workflow."}}
{"id": "2508.21553", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.21553", "abs": "https://arxiv.org/abs/2508.21553", "authors": ["J\u00f8rn Eirik Betten", "Quentin Mazouni", "Dennis Gross", "Pedro Lind", "Helge Spieker"], "title": "Reusable Test Suites for Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) agents show great promise in solving sequential\ndecision-making tasks. However, validating the reliability and performance of\nthe agent policies' behavior for deployment remains challenging. Most\nreinforcement learning policy testing methods produce test suites tailored to\nthe agent policy being tested, and their relevance to other policies is\nunclear. This work presents Multi-Policy Test Case Selection (MPTCS), a novel\nautomated test suite selection method for RL environments, designed to extract\ntest cases generated by any policy testing framework based on their\nsolvability, diversity, and general difficulty. MPTCS uses a set of policies to\nselect a diverse collection of reusable policy-agnostic test cases that reveal\ntypical flaws in the agents' behavior. The set of policies selects test cases\nfrom a candidate pool, which can be generated by any policy testing method,\nbased on a difficulty score. We assess the effectiveness of the difficulty\nscore and how the method's effectiveness and cost depend on the number of\npolicies in the set. Additionally, a method for promoting diversity in the test\nsuite, a discretized general test case descriptor surface inspired by\nquality-diversity algorithms, is examined to determine how it covers the state\nspace and which policies it triggers to produce faulty behaviors.", "AI": {"tldr": "The paper introduces MPTCS, a new automated method for selecting diverse and reusable test cases in reinforcement learning environments, improving reliability testing across multiple policies.", "motivation": "Validating RL policies for deployment is difficult, and most policy testing methods generate test cases tailored to specific agents with unclear relevance to others.", "method": "The method, MPTCS, uses a set of diverse policies to select test cases from a candidate pool (from any testing framework) using a difficulty score. It promotes diversity using a discretized test case descriptor surface inspired by quality-diversity algorithms and analyzes the effect of policy count on effectiveness and cost.", "result": "MPTCS can select diverse, reusable, and policy-agnostic test cases that reveal common flaws, with its effectiveness and cost dependent on policy set size. The quality-diversity-inspired approach increases coverage and exposes faulty behaviors across different policies.", "conclusion": "This method enables more generalizable and effective RL policy testing by generating test suites that expose typical agent flaws regardless of the agent tested, promoting broader state space coverage."}}
{"id": "2508.21634", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.21634", "abs": "https://arxiv.org/abs/2508.21634", "authors": ["Domenico Cotroneo", "Cristina Improta", "Pietro Liguori"], "title": "Human-Written vs. AI-Generated Code: A Large-Scale Study of Defects, Vulnerabilities, and Complexity", "comment": "Accepted to the 36th IEEE International Symposium on Software\n  Reliability Engineering (ISSRE, 2025)", "summary": "As AI code assistants become increasingly integrated into software\ndevelopment workflows, understanding how their code compares to human-written\nprograms is critical for ensuring reliability, maintainability, and security.\nIn this paper, we present a large-scale comparison of code authored by human\ndevelopers and three state-of-the-art LLMs, i.e., ChatGPT, DeepSeek-Coder, and\nQwen-Coder, on multiple dimensions of software quality: code defects, security\nvulnerabilities, and structural complexity. Our evaluation spans over 500k code\nsamples in two widely used languages, Python and Java, classifying defects via\nOrthogonal Defect Classification and security vulnerabilities using the Common\nWeakness Enumeration. We find that AI-generated code is generally simpler and\nmore repetitive, yet more prone to unused constructs and hardcoded debugging,\nwhile human-written code exhibits greater structural complexity and a higher\nconcentration of maintainability issues. Notably, AI-generated code also\ncontains more high-risk security vulnerabilities. These findings highlight the\ndistinct defect profiles of AI- and human-authored code and underscore the need\nfor specialized quality assurance practices in AI-assisted programming.", "AI": {"tldr": "AI-generated code is less complex but more prone to certain defects and security risks, while human-written code is structurally complex but with more maintainability issues. Both types have unique quality profiles, necessitating targeted quality assurance.", "motivation": "As AI code assistants like ChatGPT, DeepSeek-Coder, and Qwen-Coder are increasingly used in software development, there's a critical need to understand how their code quality compares to that of human developers, focusing on reliability, maintainability, and security.", "method": "A large-scale study evaluating over 500,000 code samples in Python and Java, comparing code from human developers and three LLMs across code defects (using Orthogonal Defect Classification), security vulnerabilities (using Common Weakness Enumeration), and structural complexity.", "result": "The study finds that AI-generated code is typically simpler and more repetitive, but tends to have more unused constructs and hardcoded debugging statements. Human-written code is more structurally complex with more maintainability concerns. Importantly, AI-generated code has more high-risk security vulnerabilities than human code.", "conclusion": "AI- and human-generated code each have distinct profiles of defects and vulnerabilities. AI code presents particular risks, notably higher security vulnerabilities, emphasizing the necessity of tailored quality assurance approaches for AI-assisted development."}}
{"id": "2508.21811", "categories": ["cs.SE", "68", "D.2.9"], "pdf": "https://arxiv.org/pdf/2508.21811", "abs": "https://arxiv.org/abs/2508.21811", "authors": ["Ashley Hourigan", "Ridewaan Hanslo"], "title": "The Integration of Agile Methodologies in DevOps Practices within the Information Technology Industry", "comment": "10 pages, 2 figures, conference", "summary": "The demand for rapid software delivery in the Information Technology (IT)\nindustry has significantly intensified, emphasising the need for faster\nsoftware products and service releases with enhanced features to meet customer\nexpectations. Agile methodologies are replacing traditional approaches such as\nWaterfall, where flexibility, iterative development and adaptation to change\nare favoured over rigid planning and execution. DevOps, a subsequent evolution\nfrom Agile, emphasises collaborative efforts in development and operations\nteams, focusing on continuous integration and deployment to deliver resilient\nand high-quality software products and services. This study aims to critically\nassess both Agile and DevOps practices in the IT industry to identify the\nfeasibility and applicability of Agile methods in DevOps practices. Eleven\nsemi-structured interviews were conducted with Agile and DevOps practitioners\nin varying capacities across several sectors within the IT industry. Through\nthematic analysis, 51 unique codes were extracted and synthesised into 19\nthemes that reported on each phase of the DevOps lifecycle, specifically\nregarding the integration and implementation of Agile methods into DevOps\npractices. Based on the findings, a new understanding detailing the\ninterrelationship of Agile methods in DevOps practices was discussed that met\nthe research objectives.", "AI": {"tldr": "This paper investigates how Agile methods are integrated into DevOps practices in the IT industry by interviewing practitioners and analysing their insights. The study identifies key themes demonstrating how Agile approaches fit within each stage of the DevOps lifecycle, revealing a new conceptual understanding of how both methodologies work together to deliver faster and better software.", "motivation": "The IT industry faces increasing pressure for faster software delivery and better features to meet growing customer expectations. Traditional development models like Waterfall are seen as too rigid, prompting a shift toward Agile and DevOps methodologies, which offer more flexibility and speed. The motivation is to understand how Agile practices are being adapted and integrated into the newer DevOps processes to maintain high-quality, resilient software delivery.", "method": "The study conducted eleven semi-structured interviews with Agile and DevOps practitioners from various IT sectors. The collected qualitative interview data was then analysed using thematic analysis, leading to the extraction and synthesis of 51 unique codes into 19 distinct themes. These themes described the interplay between Agile practices and each phase of the DevOps lifecycle.", "result": "The research identified 19 themes regarding the integration of Agile methods within DevOps, each corresponding to specific phases of the DevOps lifecycle. The findings shed light on the practical feasibility and applicability of Agile approaches within DevOps environments across different IT sectors.", "conclusion": "A new understanding of the relationships and interdependencies between Agile and DevOps practices was established. The study provided evidence for how Agile methods can be integrated within DevOps workflows to satisfy contemporary IT industry demands for rapid, reliable, and high-quality software deliveries."}}
