{"id": "2507.19549", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19549", "abs": "https://arxiv.org/abs/2507.19549", "authors": ["Nadeen Fathallah", "Daniel Hern\u00e1ndez", "Steffen Staab"], "title": "AccessGuru: Leveraging LLMs to Detect and Correct Web Accessibility Violations in HTML Code", "comment": null, "summary": "The vast majority of Web pages fail to comply with established Web\naccessibility guidelines, excluding a range of users with diverse abilities\nfrom interacting with their content. Making Web pages accessible to all users\nrequires dedicated expertise and additional manual efforts from Web page\nproviders. To lower their efforts and promote inclusiveness, we aim to\nautomatically detect and correct Web accessibility violations in HTML code.\nWhile previous work has made progress in detecting certain types of\naccessibility violations, the problem of automatically detecting and correcting\naccessibility violations remains an open challenge that we address. We\nintroduce a novel taxonomy classifying Web accessibility violations into three\nkey categories - Syntactic, Semantic, and Layout. This taxonomy provides a\nstructured foundation for developing our detection and correction method and\nredefining evaluation metrics. We propose a novel method, AccessGuru, which\ncombines existing accessibility testing tools and Large Language Models (LLMs)\nto detect violations and applies taxonomy-driven prompting strategies to\ncorrect all three categories. To evaluate these capabilities, we develop a\nbenchmark of real-world Web accessibility violations. Our benchmark quantifies\nsyntactic and layout compliance and judges semantic accuracy through\ncomparative analysis with human expert corrections. Evaluation against our\nbenchmark shows that AccessGuru achieves up to 84% average violation score\ndecrease, significantly outperforming prior methods that achieve at most 50%.", "AI": {"tldr": "This paper introduces AccessGuru, a method using LLMs and existing tools to automatically detect and fix web accessibility violations, achieving much higher correction rates than previous approaches.", "motivation": "Web pages often do not meet accessibility guidelines, preventing people with diverse abilities from using them. Manual corrections are costly and require expertise, so there is a need for automated methods to detect and fix accessibility issues in HTML code.", "method": "The authors introduce a taxonomy of accessibility violations (Syntactic, Semantic, Layout) and propose AccessGuru, a method combining existing testing tools and Large Language Models (LLMs). Taxonomy-driven prompting strategies are used for detecting and correcting violations. An evaluation benchmark is created to assess performance based on compliance and expert comparison.", "result": "AccessGuru achieves up to 84% reduction in average violation scores, surpassing previous methods which reach at most 50%.", "conclusion": "AccessGuru, by integrating LLMs and existing tools using a novel taxonomy, effectively automates the detection and correction of a broad range of web accessibility issues, outperforming prior solutions."}}
{"id": "2507.19687", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.19687", "abs": "https://arxiv.org/abs/2507.19687", "authors": ["Joao Pedro Duarte", "Paulo Borba", "Guilherme Cavalcanti"], "title": "LastMerge: A language-agnostic structured tool for code integration", "comment": null, "summary": "Unstructured line-based merge tools are widely used in practice. Structured\nAST-based merge tools show significantly improved merge accuracy, but are\nrarely used in practice because they are language specific and costly,\nconsequently not being available for many programming languages. To improve\nmerge accuracy for a wide range of languages, we propose LastMerge, a generic\nstructured merge tool that can be configured through a thin interface that\nsignificantly reduces the effort of supporting structured merge. To understand\nthe impact that generic structured merge might have on merge accuracy and\nperformance, we run an experiment with four structured merge tools: two Java\nspecific tools, jDime and Spork, and their generic counterparts, respectively\nLastMerge and Mergiraf. Using each tool, we replay merge scenarios from a\nsignificant dataset, and collect data on runtime, behavioral divergences, and\nmerge accuracy. Our results show no evidence that generic structured merge\nsignificantly impacts merge accuracy. Although we observe a difference rate of\napproximately 10% between the Java specific tools and their generic\ncounterparts, most of the differences stem from implementation details and\ncould be avoided. We find that LastMerge reports 15% fewer false positives than\njDime while Mergiraf misses 42% fewer false negatives than Spork. Both generic\ntools exhibit comparable runtime performance to the state of the art language\nspecific implementations. These results suggest that generic structured merge\ntools can effectively replace language-specific ones, paving the way for\nbroader adoption of structured merge in industry.", "AI": {"tldr": "LastMerge, a generic structured merge tool, offers similar accuracy and performance as language-specific tools, with some advantages, making structured merge feasible for more programming languages and increasing its practical adoption.", "motivation": "Unstructured line-based merge tools are widely used but have limited accuracy. Structured AST-based (Abstract Syntax Tree) merge tools improve accuracy but are typically language-specific, difficult to develop, and not available for many programming languages. There is a need for a more generic, broadly applicable structured merge tool to increase merge accuracy across different languages.", "method": "The authors propose LastMerge, a generic structured merge tool configurable via a thin interface to reduce setup cost for new languages. They analyze the impact of generic structured merge on merge accuracy and runtime by comparing four tools (jDime and Spork for Java, and their generic counterparts LastMerge and Mergiraf), replaying merge scenarios from a large dataset, and collecting metrics on runtime, behavioral differences, and merge accuracy.", "result": "The experiment found that generic structured merge does not significantly hurt merge accuracy. While there is a roughly 10% difference rate between language-specific and generic tools, most differences are due to implementation details. LastMerge had 15% fewer false positives than jDime, and Mergiraf had 42% fewer false negatives than Spork. Both generic tools had runtime performance comparable to language-specific tools.", "conclusion": "Generic structured merge tools like LastMerge can match the accuracy and performance of language-specific tools and therefore could replace them, enabling wider and easier adoption of structured merge in the software industry."}}
{"id": "2507.19714", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.19714", "abs": "https://arxiv.org/abs/2507.19714", "authors": ["Feifei Niu", "Junqian Shao", "Christoph Mayr-Dorn", "Liguo Huang", "Wesley K. G. Assun\u00e7\u00e3o", "Chuanyi Li", "Jidong Ge", "Alexander Egyed"], "title": "Refactoring $\\neq$ Bug-Inducing: Improving Defect Prediction with Code Change Tactics Analysis", "comment": null, "summary": "Just-in-time defect prediction (JIT-DP) aims to predict the likelihood of\ncode changes resulting in software defects at an early stage. Although code\nchange metrics and semantic features have enhanced prediction accuracy, prior\nresearch has largely ignored code refactoring during both the evaluation and\nmethodology phases, despite its prevalence. Refactoring and its propagation\noften tangle with bug-fixing and bug-inducing changes within the same commit\nand statement. Neglecting refactoring can introduce bias into the learning and\nevaluation of JIT-DP models. To address this gap, we investigate the impact of\nrefactoring and its propagation on six state-of-the-art JIT-DP approaches. We\npropose Code chAnge Tactics (CAT) analysis to categorize code refactoring and\nits propagation, which improves labeling accuracy in the JIT-Defects4J dataset\nby 13.7%. Our experiments reveal that failing to consider refactoring\ninformation in the dataset can diminish the performance of models, particularly\nsemantic-based models, by 18.6% and 37.3% in F1-score. Additionally, we propose\nintegrating refactoring information to enhance six baseline approaches,\nresulting in overall improvements in recall and F1-score, with increases of up\nto 43.2% and 32.5%, respectively. Our research underscores the importance of\nincorporating refactoring information in the methodology and evaluation of\nJIT-DP. Furthermore, our CAT has broad applicability in analyzing refactoring\nand its propagation for software maintenance.", "AI": {"tldr": "Ignoring refactoring in just-in-time defect prediction leads to biased results and lower model performance. By categorizing and integrating refactoring information, the authors improve labeling and significantly boost model recall and F1-scores, urging its inclusion in future JIT-DP research and evaluation.", "motivation": "Prior work in just-in-time defect prediction (JIT-DP) improved accuracy using code metrics and semantic features, but largely ignored the role of code refactoring, even though refactoring is common and can be intertwined with bug-fixing or inducing changes. Neglecting refactoring may introduce bias into model learning and evaluation.", "method": "The paper investigates the impact of refactoring and its propagation on six state-of-the-art JIT-DP models. It introduces Code chAnge Tactics (CAT) analysis to categorize refactoring and its propagation, and refines dataset labeling. Experiments assess the performance of various models with and without consideration of refactoring.", "result": "Including refactoring improves labeling accuracy in the JIT-Defects4J dataset by 13.7%. Ignoring refactoring can substantially reduce model F1-score (by 18.6% and 37.3% for semantic-based models). Integrating refactoring enhances model recall and F1-score, with improvements up to 43.2% and 32.5%, respectively.", "conclusion": "Refactoring should be incorporated into both methodology and evaluation of JIT-DP, as it significantly affects model accuracy and assessment. The proposed CAT analysis has broader applications for software maintenance beyond defect prediction."}}
{"id": "2507.19721", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.19721", "abs": "https://arxiv.org/abs/2507.19721", "authors": ["Dapeng Yan", "Wenjie Yang", "Kui Liu", "Zhiming Liu", "Zhikuang Cai"], "title": "Clean Code In Practice: Challenges and Opportunities", "comment": null, "summary": "Reliability prediction is crucial for ensuring the safety and security of\nsoftware systems, especially in the context of industry practices. While\nvarious metrics and measurements are employed to assess software reliability,\nthe complexity of modern systems necessitates a deeper understanding of how\nthese metrics interact with security and safety concerns. This paper explores\nthe interplay between software reliability, safety, and security, offering a\ncomprehensive analysis of key metrics and measurement techniques used in the\nindustry for reliability prediction. We identify critical threats to software\nreliability and provide a threat estimation framework that incorporates both\nsafety and security aspects. Our findings suggest that integrating reliability\nmetrics with safety and security considerations can enhance the robustness of\nsoftware systems. Furthermore, we propose a set of actionable guidelines for\npractitioners to improve their reliability prediction models while\nsimultaneously addressing the security and safety challenges of contemporary\nsoftware applications.", "AI": {"tldr": "Integrating security and safety metrics with traditional reliability measurements creates a stronger framework for predicting and ensuring software robustness. The paper suggests actionable steps for practitioners to improve reliability prediction by considering security and safety together.", "motivation": "With the increasing complexity of software systems, traditional reliability prediction metrics may fall short in accounting for intertwined security and safety concerns. There is a growing need for methods that integrate these factors to better ensure system robustness.", "method": "The paper conducts a comprehensive analysis of existing industry metrics and measurement techniques for software reliability prediction. It also introduces a framework for threat estimation that factors in both security and safety aspects.", "result": "The study identifies key threats affecting software reliability and demonstrates that integrating security and safety considerations into reliability metrics leads to more robust software systems. Actionable guidelines for improving reliability prediction are provided.", "conclusion": "Incorporating security and safety considerations into software reliability prediction enhances the effectiveness of reliability modeling and strengthens system robustness. The proposed guidelines can help practitioners improve both reliability and overall system security and safety."}}
{"id": "2507.19728", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.19728", "abs": "https://arxiv.org/abs/2507.19728", "authors": ["Lalita Na Nongkhai", "Jingyun Wang", "Takahiko Mendori"], "title": "Development and Evaluation of Adaptive LearningSupport System Based on Ontology of MultipleProgramming Languages", "comment": "This document provides corrections to the published article.\n  Corrections include clarifying figure legends and addressing grammatical\n  issues to enhance clarity. The authors state that the scientific conclusions\n  are unaffected", "summary": "This paper introduces an ontology-based approach within an adaptive learning\nsupport system for computer programming. This system (named ADVENTURE) is\ndesigned to deliver personalized programming exercises that are tailored to\nindividual learners' skill levels. ADVENTURE utilizes an ontology, named\nCONTINUOUS, which encompasses common concepts across multiple programming\nlanguages. The system leverages this ontology not only to visualize programming\nconcepts but also to provide hints during practice programming exercises and\nrecommend subsequent programming concepts. The adaptive mechanism is driven by\nthe Elo Rating System, applied in an educational context to dynamically\nestimate the most appropriate exercise difficulty for each learner. An\nexperimental study compared two instructional modes, adaptive and random, based\non six features derived from 1,186 code submissions across all the experimental\ngroups. The results indicate significant differences in four of six analyzed\nfeatures between these two modes. Notably, the adaptive mode demonstrates a\nsignificant difference over the random mode in two features, the submission of\ncorrect answers and the number of pass concepts. Therefore, these results\nunderscore that this adaptive learning support system may support learners in\npracticing programming exercises.", "AI": {"tldr": "An adaptive programming exercise system using an ontology and the Elo Rating System significantly improves learning outcomes over random exercise selection. The system personalizes tasks by assessing skills and adapting recommendations, leading to more correct answers and conceptual mastery.", "motivation": "There is a need to personalize programming education to individual skill levels, as learners benefit from exercises that are tailored to their current understanding and abilities. Existing systems may not sufficiently leverage domain knowledge or adapt exercise difficulty accurately.", "method": "The paper proposes an ontology-based adaptive learning system called ADVENTURE. This system uses an ontology (CONTINUOUS) that covers concepts common to multiple programming languages. ADVENTURE visualizes these concepts, offers hints, and recommends new topics, adjusting difficulty in real time using an educational adaptation of the Elo Rating System. The system's effectiveness was evaluated through an experimental study with 1,186 code submissions, comparing adaptive and random assignment modes across six features.", "result": "The study found significant differences in four out of six features between adaptive and random modes. In particular, the adaptive mode led to a higher submission of correct answers and more pass concepts, suggesting better learning outcomes.", "conclusion": "The adaptive, ontology-driven approach supports programming learners more effectively than a non-adaptive (random) approach, as indicated by improved outcomes in key metrics. Personalization via ontologies and adaptive difficulty mechanisms enhances practice in programming exercises."}}
{"id": "2507.19743", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19743", "abs": "https://arxiv.org/abs/2507.19743", "authors": ["Zhuolin Xu", "Chenglin Li", "Qiushi Li", "Shin Hwei Tan"], "title": "Defining ethically sourced code generation", "comment": null, "summary": "Several code generation models have been proposed to help reduce time and\neffort in solving software-related tasks. To ensure responsible AI, there are\ngrowing interests over various ethical issues (e.g., unclear licensing,\nprivacy, fairness, and environment impact). These studies have the overarching\ngoal of ensuring ethically sourced generation, which has gained growing\nattentions in speech synthesis and image generation. In this paper, we\nintroduce the novel notion of Ethically Sourced Code Generation (ES-CodeGen) to\nrefer to managing all processes involved in code generation model development\nfrom data collection to post-deployment via ethical and sustainable practices.\nTo build a taxonomy of ES-CodeGen, we perform a two-phase literature review\nwhere we read 803 papers across various domains and specific to AI-based code\ngeneration. We identified 71 relevant papers with 10 initial dimensions of\nES-CodeGen. To refine our dimensions and gain insights on consequences of\nES-CodeGen, we surveyed 32 practitioners, which include six developers who\nsubmitted GitHub issues to opt-out from the Stack dataset (these impacted users\nhave real-world experience of ethically sourcing issues in code generation\nmodels). The results lead to 11 dimensions of ES-CodeGen with a new dimension\non code quality as practitioners have noted its importance. We also identified\nconsequences, artifacts, and stages relevant to ES-CodeGen. Our post-survey\nreflection showed that most practitioners tend to ignore social-related\ndimensions despite their importance. Most practitioners either agreed or\nstrongly agreed that our survey help improve their understanding of ES-CodeGen.\nOur study calls for attentions of various ethical issues towards ES-CodeGen.", "AI": {"tldr": "The paper introduces the concept of Ethically Sourced Code Generation (ES-CodeGen), reviews 803 papers and surveys 32 practitioners to define 11 dimensions for ensuring ethical AI code generation. It finds social issues are often neglected and urges greater attention to comprehensive ethical practices in this emerging domain.", "motivation": "There is an increasing need to address ethical issues in AI-assisted code generation, such as uncertain licensing, privacy, fairness, and environmental impacts, to promote responsible AI use. Other domains like speech and image generation have seen efforts to source data ethically, but code generation has lacked a comprehensive approach.", "method": "The authors conducted a two-phase literature review, analyzing 803 papers (including those specifically on AI code generation), and identified 71 relevant papers. They built a taxonomy with 10 initial dimensions for ethical code generation. They surveyed 32 practitioners, including developers affected by ethical concerns, to refine these dimensions and understand their real-world effects.", "result": "The study identified 11 dimensions (expanding from the original 10, adding code quality) crucial to ethically sourced code generation (ES-CodeGen). The research also highlights the gap where practitioners often overlook socially related ethical dimensions, despite their importance. The study provides consequences, artifacts, and development stages relevant to ES-CodeGen.", "conclusion": "There is a strong need for more awareness and attention to ethical considerations in the development and deployment of code generation models. The research introduces a comprehensive taxonomy and calls on the community to address gaps, particularly in social aspects, to ensure truly ethical AI code generation."}}
{"id": "2507.20251", "categories": ["cs.PL", "cs.CC", "cs.DB", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.20251", "abs": "https://arxiv.org/abs/2507.20251", "authors": ["Angelos Charalambidis", "Babis Kostopoulos", "Christos Nomikos", "Panos Rondogiannis"], "title": "The Power of Negation in Higher-Order Datalog", "comment": null, "summary": "We investigate the expressive power of Higher-Order Datalog$^\\neg$ under both\nthe well-founded and the stable model semantics, establishing tight connections\nwith complexity classes. We prove that under the well-founded semantics, for\nall $k\\geq 1$, $(k+1)$-Order Datalog$^\\neg$ captures k-EXP, a result that holds\nwithout explicit ordering of the input database. The proof of this fact can be\nperformed either by using the powerful existential predicate variables of the\nlanguage or by using partially applied relations and relation enumeration.\nFurthermore, we demonstrate that this expressive power is retained within a\nstratified fragment of the language. Under the stable model semantics, we show\nthat $(k+1)$-Order Datalog$^\\neg$ captures co-(k-NEXP) using cautious reasoning\nand k-NEXP using brave reasoning, again with analogous results for the\nstratified fragment augmented with choice rules. Our results establish a\nhierarchy of expressive power, highlighting an interesting trade-off between\norder and non-determinism in the context of higher-order logic programming:\nincreasing the order of programs under the well-founded semantics can surpass\nthe expressive power of lower-order programs under the stable model semantics.", "AI": {"tldr": "This paper maps out how Higher-Order Datalog with negation relates to computational complexity, showing higher orders align with higher complexity classes under different semantics, and revealing trade-offs between program order and non-determinism in logic programming.", "motivation": "The authors aim to understand the expressive power of Higher-Order Datalog with negation under different semantics, particularly how it relates to established computational complexity classes. This helps clarify the capabilities and limitations of logic programming languages in representing complex computational tasks.", "method": "They analyze Higher-Order Datalog$^\\neg$ under both the well-founded and stable model semantics, and establish connections to complexity classes (k-EXP, k-NEXP, co-(k-NEXP)). They use logical and theoretical techniques to demonstrate these equivalences, including existential predicate variables, partially applied relations, and stratified fragments.", "result": "Under well-founded semantics, $(k+1)$-Order Datalog$^\\neg$ captures k-EXP for all $k\\geq 1$. Under stable model semantics, $(k+1)$-Order Datalog$^\\neg$ captures co-(k-NEXP) with cautious reasoning and k-NEXP with brave reasoning. These results also hold for certain stratified and augmented fragments. A hierarchy of expressive power is shown, with higher order increasing capabilities over lower order plus non-determinism.", "conclusion": "Higher-Order Datalog$^\\neg$ exhibits a structured hierarchy in expressive power depending on both semantic choices and program order. Increasing order can provide more expressive power than increasing non-determinism, illuminating a trade-off rooted in logic programming paradigms."}}
{"id": "2507.19806", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19806", "abs": "https://arxiv.org/abs/2507.19806", "authors": ["Xinlong Zhao", "Tong Jia", "Minghua He", "Yihan Wu", "Ying Li", "Gang Huang"], "title": "From Few-Label to Zero-Label: An Approach for Cross-System Log-Based Anomaly Detection with Meta-Learning", "comment": "5 pages, 1 figures, FSE 2025", "summary": "Log anomaly detection plays a critical role in ensuring the stability and\nreliability of software systems. However, existing approaches rely on large\namounts of labeled log data, which poses significant challenges in real-world\napplications. To address this issue, cross-system transfer has been identified\nas a key research direction. State-of-the-art cross-system approaches achieve\npromising performance with only a few labels from the target system. However,\ntheir reliance on labeled target logs makes them susceptible to the cold-start\nproblem when labeled logs are insufficient. To overcome this limitation, we\nexplore a novel yet underexplored setting: zero-label cross-system log anomaly\ndetection, where the target system logs are entirely unlabeled. To this end, we\npropose FreeLog, a system-agnostic representation meta-learning method that\neliminates the need for labeled target system logs, enabling cross-system log\nanomaly detection under zero-label conditions. Experimental results on three\npublic log datasets demonstrate that FreeLog achieves performance comparable to\nstate-of-the-art methods that rely on a small amount of labeled data from the\ntarget system.", "AI": {"tldr": "FreeLog is a meta-learning based method for log anomaly detection that works across software systems without any labeled target data, achieving results similar to leading methods that require some labeled data.", "motivation": "Log anomaly detection is crucial for maintaining software stability, but current methods require many labeled logs, which are hard to obtain. Reducing the labeling requirement, particularly for different software systems (cross-system), is an important research goal.", "method": "The paper introduces FreeLog, a system-agnostic representation meta-learning approach. FreeLog enables log anomaly detection across systems without needing any labeled logs from the target system (zero-label setting).", "result": "FreeLog performs comparably to state-of-the-art cross-system log anomaly detection methods that rely on a small amount of labeled data from the target system.", "conclusion": "FreeLog eliminates the need for labeled target logs in cross-system log anomaly detection and effectively solves the cold-start problem, broadening the applicability of anomaly detection in real-world scenarios."}}
{"id": "2507.20674", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.20674", "abs": "https://arxiv.org/abs/2507.20674", "authors": ["Nima Karimipour", "Michael Pradel", "Martin Kellogg", "Manu Sridharan"], "title": "LLM-Based Repair of Static Nullability Errors", "comment": null, "summary": "Modern Java projects increasingly adopt static analysis tools that prevent\nnull-pointer exceptions by treating nullness as a type property. However,\nintegrating such tools into large, existing codebases remains a significant\nchallenge. While annotation inference can eliminate many errors automatically,\na subset of residual errors -- typically a mix of real bugs and false positives\n-- often persist and can only be resolved via code changes. Manually addressing\nthese errors is tedious and error-prone. Large language models (LLMs) offer a\npromising path toward automating these repairs, but naively-prompted LLMs often\ngenerate incorrect, contextually-inappropriate edits. Resolving a nullability\nerror demands a deep understanding of how a symbol is used across the codebase,\noften spanning methods, classes, and packages. We present NullRepair, a system\nthat integrates LLMs into a structured workflow for resolving the errors from a\nnullability checker. NullRepair's decision process follows a flowchart derived\nfrom manual analysis of 200 real-world errors. It leverages static analysis to\nidentify safe and unsafe usage regions of symbols, using error-free usage\nexamples to contextualize model prompts. Patches are generated through an\niterative interaction with the LLM that incorporates project-wide context and\ndecision logic. Our evaluation on 12 real-world Java projects shows that\nNullRepair resolves an average of 72% of the errors that remain after applying\na state-of-the-art annotation inference technique. Unlike a naively-prompted\nLLM, NullRepair also largely preserves program semantics, with all unit tests\npassing in 10/12 projects after applying every edit proposed by NullRepair, and\n98% or more tests passing in the remaining two projects.", "AI": {"tldr": "NullRepair uses LLMs within a structured workflow to automatically fix tough nullability errors in Java code. It resolves most errors left after annotation inference, with minimal impact on program functionality.", "motivation": "Null-pointer exceptions are a common and difficult problem in Java codebases. While static analysis tools that track nullness as a type property can help, integrating them into large, existing projects results in many persistent errors that are hard to address manually. Naive application of large language models (LLMs) does not solve the problem due to lacking context and appropriate code edits. There is a need for a systematic and automated approach to resolve nullability errors using LLMs effectively.", "method": "The authors present NullRepair, a system that integrates LLMs into a structured workflow for resolving nullability errors as identified by static analysis. The workflow uses a decision process guided by a flowchart based on manual study of 200 real-world errors. NullRepair employs static analysis to differentiate safe and unsafe regions for null usage and augments LLM prompts with relevant, error-free code usage examples. The LLM communicates iteratively to generate patches, incorporating broad project context and guided decision logic.", "result": "NullRepair was evaluated on 12 real-world Java projects. It successfully resolved an average of 72% of the errors left after state-of-the-art annotation inference. Additionally, application of the edits produced by NullRepair preserved program behavior in most cases: all unit tests passed in 10 out of 12 projects, and at least 98% of tests passed in the remaining two.", "conclusion": "NullRepair provides a practical and effective system for resolving hard-to-fix nullability errors in Java by combining static analysis and structured LLM interaction. It significantly automates and improves upon current error-correction efforts while maintaining program correctness."}}
{"id": "2507.19842", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19842", "abs": "https://arxiv.org/abs/2507.19842", "authors": ["Mohammad Azarijafari", "Luisa Mich", "Michele Missikoff", "Oleg Missikoff"], "title": "A Cooperative Approach for Knowledge-based Business Process Design in a Public Authority", "comment": null, "summary": "Enterprises are currently undergoing profound transformations due to the\nunpostponable digital transformation. Then, to remain competitive, enterprises\nmust adapt their organisational structures and operations. This organisational\nshift is also important for small and medium-sized enterprises. A key\ninnovation frontier is the adoption of process-oriented production models. This\npaper presents a knowledge-based method to support business experts in\ndesigning business processes. The method requires no prior expertise in\nKnowledge Engineering and guides designers through a structured sequence of\nsteps to produce a diagrammatic workflow of the target process. The\nconstruction of the knowledge base starts from simple, text-based, knowledge\nartefacts and then progresses towards more structured, formal representations.\nThe approach has been conceived to allow a shared approach for all stakeholders\nand actors who participate in the BP design.", "AI": {"tldr": "The paper presents an accessible, step-by-step knowledge-based method that helps business experts, especially in SMEs, design business processes efficiently\u2014no prior technical expertise required\u2014and promotes collaborative workflow creation.", "motivation": "Enterprises, especially small and medium-sized ones, need to adapt to the digital transformation to stay competitive. There is a demand for innovative, process-oriented production models, and a barrier exists as business process design often requires expertise in Knowledge Engineering, which many business experts lack.", "method": "The paper introduces a knowledge-based method that assists business experts in designing business processes without needing prior Knowledge Engineering expertise. The method provides a structured, step-by-step approach, beginning with simple text-based knowledge artefacts and gradually building towards formal, structured process representations. It also emphasizes facilitating collaboration among all stakeholders involved in the business process design.", "result": "The method enables business experts to create diagrammatic workflows for target processes efficiently, making process-oriented production models more accessible. It simplifies and shares the process design approach, supporting collaboration among diverse stakeholders.", "conclusion": "The proposed knowledge-based method successfully supports business experts in designing business processes, reduces the entry barrier by eliminating the need for prior Knowledge Engineering expertise, and fosters a shared, collaborative approach."}}
{"id": "2507.19902", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19902", "abs": "https://arxiv.org/abs/2507.19902", "authors": ["Sourena Khanzadeh"], "title": "AgentMesh: A Cooperative Multi-Agent Generative AI Framework for Software Development Automation", "comment": null, "summary": "Software development is a complex, multi-phase process traditionally\nrequiring collaboration among individuals with diverse expertise. We propose\nAgentMesh, a Python-based framework that uses multiple cooperating LLM-powered\nagents to automate software development tasks. In AgentMesh, specialized agents\n- a Planner, Coder, Debugger, and Reviewer - work in concert to transform a\nhigh-level requirement into fully realized code. The Planner agent first\ndecomposes user requests into concrete subtasks; the Coder agent implements\neach subtask in code; the Debugger agent tests and fixes the code; and the\nReviewer agent validates the final output for correctness and quality. We\ndescribe the architecture and design of these agents and their communication,\nand provide implementation details including prompt strategies and workflow\norchestration. A case study illustrates AgentMesh handling a non-trivial\ndevelopment request via sequential task planning, code generation, iterative\ndebugging, and final code review. We discuss how dividing responsibilities\namong cooperative agents leverages the strengths of large language models while\nmitigating single-agent limitations. Finally, we examine current limitations -\nsuch as error propagation and context scaling - and outline future work toward\nmore robust, scalable multi-agent AI systems for software engineering\nautomation.", "AI": {"tldr": "AgentMesh leverages multiple specialized AI agents to automate the software development process, improving over single-agent approaches by dividing tasks among Planner, Coder, Debugger, and Reviewer agents. This multi-agent method enhances code quality and efficiency but also faces challenges in scaling and error management.", "motivation": "Software development is traditionally a complex and collaborative process, requiring input from people with varied expertise. Automating this process with AI could streamline development and make it more efficient, but previous single-agent approaches have limitations. The motivation is to leverage multiple specialized AI agents working together to improve automation, code quality, and efficiency in software engineering tasks.", "method": "The authors propose AgentMesh, a Python-based framework that orchestrates multiple cooperative LLM-powered agents: Planner, Coder, Debugger, and Reviewer. Each agent is assigned a specialized role in the software development workflow such as task planning, code generation, debugging, and code review. Communication, prompt strategies, and workflow orchestration among agents are described, as well as a case study to demonstrate AgentMesh's effectiveness.", "result": "A case study shows that AgentMesh can handle a complex software development request through stepwise planning, code generation, iterative debugging, and code review by specialized cooperating agents. The approach demonstrates how responsibility division and agent collaboration harness LLM strengths while addressing the limitations seen in single-agent systems. The study also highlights ongoing challenges such as error propagation and context scaling.", "conclusion": "Dividing software development tasks among multiple LLM-powered agents within AgentMesh improves automation potential and addresses some issues of single-agent systems. The multi-agent framework presents a step forward toward robust AI-supported software engineering, though challenges remain in error handling and scaling."}}
{"id": "2507.19904", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19904", "abs": "https://arxiv.org/abs/2507.19904", "authors": ["Zhanhang Xiong", "Dongxia Wang", "Yuekang Li", "Xinyuan An", "Wenhai Wang"], "title": "CrossPL: Evaluating Large Language Models on Cross Programming Language Code Generation", "comment": null, "summary": "As large language models (LLMs) become increasingly embedded in software\nengineering workflows, a critical capability remains underexplored: generating\ncorrect code that enables cross-programming-language (CPL) interoperability.\nThis skill is essential for building complex systems that integrate components\nwritten in multiple languages via mechanisms like inter-process communication\n(IPC). To bridge this gap, we present CrossPL, the first benchmark designed to\nsystematically evaluate LLMs' ability to generate CPL-interoperating code.\nCrossPL comprises 1,982 tasks centered around IPC, covering six widely-used\nprogramming languages and seven representative CPL techniques. We construct\nthis benchmark by (i) analyzing 19,169 multi-language GitHub repositories using\n156 hand-crafted finite state machines (FSMs), and (ii) developing an LLM-based\npipeline that automatically extracts CPL code snippets, generates task\ninstructions, and validates functional correctness. We evaluate 14\nstate-of-the-art general-purpose LLMs and 6 code-oriented LLMs released in the\npast three years on CrossPL via FSM-based validation. Results reveal that even\nthe best-performing models struggle with CPL scenarios, underscoring the need\nfor more targeted research in this space. Our benchmark and code are available\nat: https://anonymous.4open.science/r/crosspl-2814.", "AI": {"tldr": "CrossPL is a new benchmark for testing LLMs' ability to generate interoperable code across programming languages. Evaluations show existing LLMs struggle with these tasks, indicating the need for specialized improvements.", "motivation": "There is a lack of systematic evaluation for large language models' (LLMs) ability to generate code supporting cross-programming-language (CPL) interoperability, a vital skill for building software systems integrating multiple languages through mechanisms like inter-process communication (IPC).", "method": "The authors introduce CrossPL, a benchmark comprising 1,982 tasks focused on IPC, spanning six programming languages and seven CPL strategies. The benchmark was constructed by analyzing 19,169 multi-language GitHub repositories using 156 designed finite state machines (FSMs), alongside an LLM-based extraction pipeline to gather code snippets, generate tasks, and validate correctness. They evaluated 20 LLMs (14 general-purpose, 6 code-based) using FSM-driven validation.", "result": "The results show that even the best current LLMs underperform in generating correct CPL-interoperating code, revealing significant gaps in their interoperability capabilities.", "conclusion": "CrossPL reveals the inability of current LLMs to reliably generate code for cross-language interoperability, highlighting a critical area in need of further research and development."}}
{"id": "2507.19909", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19909", "abs": "https://arxiv.org/abs/2507.19909", "authors": ["Roman Mach\u00e1\u010dek", "Anastasiia Grishina", "Max Hort", "Leon Moonen"], "title": "The Impact of Fine-tuning Large Language Models on Automated Program Repair", "comment": "Accepted for publication in the research track of the 41th\n  International Conference on Software Maintenance and Evolution (ICSME 2025)", "summary": "Automated Program Repair (APR) uses various tools and techniques to help\ndevelopers achieve functional and error-free code faster. In recent years,\nLarge Language Models (LLMs) have gained popularity as components in APR tool\nchains because of their performance and flexibility. However, training such\nmodels requires a significant amount of resources. Fine-tuning techniques have\nbeen developed to adapt pre-trained LLMs to specific tasks, such as APR, and\nenhance their performance at far lower computational costs than training from\nscratch. In this study, we empirically investigate the impact of various\nfine-tuning techniques on the performance of LLMs used for APR. Our experiments\nprovide insights into the performance of a selection of state-of-the-art LLMs\npre-trained on code. The evaluation is done on three popular APR benchmarks\n(i.e., QuixBugs, Defects4J and HumanEval-Java) and considers six different LLMs\nwith varying parameter sizes (resp. CodeGen, CodeT5, StarCoder, DeepSeekCoder,\nBloom, and CodeLlama-2). We consider three training regimens: no fine-tuning,\nfull fine-tuning, and parameter-efficient fine-tuning (PEFT) using LoRA and\nIA3. We observe that full fine-tuning techniques decrease the benchmarking\nperformance of various models due to different data distributions and\noverfitting. By using parameter-efficient fine-tuning methods, we restrict\nmodels in the amount of trainable parameters and achieve better results.\n  Keywords: large language models, automated program repair,\nparameter-efficient fine-tuning, AI4Code, AI4SE, ML4SE.", "AI": {"tldr": "Parameter-efficient fine-tuning outperforms full fine-tuning for adapting LLMs to Automated Program Repair, avoiding overfitting and achieving better benchmark results with lower computational costs.", "motivation": "Automated Program Repair (APR) is increasingly utilizing Large Language Models (LLMs) for better code correction, but training these models from scratch is computationally expensive. Fine-tuning pre-trained LLMs offers a cost-effective approach to enhance APR capabilities, yet the optimal fine-tuning strategy is unclear.", "method": "The study empirically examines the impact of different fine-tuning techniques on LLMs when applied to APR tasks. It evaluates six state-of-the-art code-pretrained LLMs on three well-known APR benchmarks (QuixBugs, Defects4J, HumanEval-Java), testing three training regimens: no fine-tuning, full fine-tuning, and parameter-efficient fine-tuning (LoRA and IA3).", "result": "Full fine-tuning often reduces model performance on benchmarks due to overfitting and data distribution discrepancies. In contrast, parameter-efficient fine-tuning approaches limit the number of trainable parameters and yield superior performance results.", "conclusion": "Parameter-efficient fine-tuning frameworks are more effective than full fine-tuning for adapting LLMs to APR tasks, mitigating issues such as overfitting and mismatched data distributions."}}
{"id": "2507.19942", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.19942", "abs": "https://arxiv.org/abs/2507.19942", "authors": ["Zimin Chen", "Yue Pan", "Siyu Lu", "Jiayi Xu", "Claire Le Goues", "Martin Monperrus", "He Ye"], "title": "Prometheus: Unified Knowledge Graphs for Issue Resolution in Multilingual Codebases", "comment": null, "summary": "Language model (LM) agents, such as SWE-agent and OpenHands, have made\nprogress toward automated issue resolution. However, existing approaches are\noften limited to Python-only issues and rely on pre-constructed containers in\nSWE-bench with reproduced issues, restricting their applicability to real-world\nand work for multi-language repositories. We present Prometheus, designed to\nresolve real-world issues beyond benchmark settings. Prometheus is a\nmulti-agent system that transforms an entire code repository into a unified\nknowledge graph to guide context retrieval for issue resolution. Prometheus\nencodes files, abstract syntax trees, and natural language text into a graph of\ntyped nodes and five general edge types to support multiple programming\nlanguages. Prometheus uses Neo4j for graph persistence, enabling scalable and\nstructured reasoning over large codebases. Integrated by the DeepSeek-V3 model,\nPrometheus resolves 28.67% and 13.7% of issues on SWE-bench Lite and SWE-bench\nMultilingual, respectively, with an average API cost of $0.23 and $0.38 per\nissue. Prometheus resolves 10 unique issues not addressed by prior work and is\nthe first to demonstrate effectiveness across seven programming languages.\nMoreover, it shows the ability to resolve real-world GitHub issues in the\nLangChain and OpenHands repositories. We have open-sourced Prometheus at:\nhttps://github.com/Pantheon-temple/Prometheus", "AI": {"tldr": "Prometheus is a multi-agent system that turns codebases into knowledge graphs to resolve real-world, multi-language repository issues. It surpasses previous LM agents by working on seven languages and solving unique issues with cost efficiency, and is available open source.", "motivation": "Existing language model agents for automated issue resolution are restricted to Python-only issues and require pre-constructed benchmark settings, limiting their applicability to real-world, multi-language code repositories. There is a need for systems that can operate on real-world, multi-language projects for broader utility.", "method": "The proposed system, Prometheus, is a multi-agent framework that converts an entire code repository into a knowledge graph, encoding files, abstract syntax trees, and natural language into a graph with typed nodes and five kinds of edges. Prometheus uses Neo4j for graph storage and is integrated with the DeepSeek-V3 model, allowing context-aware reasoning for issue resolution across multiple programming languages.", "result": "Prometheus resolves 28.67% of issues on SWE-bench Lite and 13.7% on SWE-bench Multilingual, outperforms prior work by solving 10 additional unique issues, and is the first to be effective in seven programming languages. It demonstrates capability on real-world GitHub issues in major repositories, with average API costs per issue.", "conclusion": "Prometheus advances automated issue resolution by addressing multi-language and real-world settings, going beyond Python-only or benchmark-limited solutions, and demonstrates broad applicability and cost-effective performance. The system is open-sourced for further research and adoption."}}
{"id": "2507.19951", "categories": ["cs.SE", "D.2"], "pdf": "https://arxiv.org/pdf/2507.19951", "abs": "https://arxiv.org/abs/2507.19951", "authors": ["Shengcheng Duan", "Yihua Xu", "Sheng Zhang", "Shen Wang", "Yue Duan"], "title": "PDLogger: Automated Logging Framework for Practical Software Development", "comment": "10 pages, 10 figures", "summary": "Logging is indispensable for maintaining the reliability and diagnosability\nof modern software, yet developers still struggle to decide where and how to\nlog effectively. Existing automated logging techniques focus on isolated\nsub-tasks - predicting a single log position, level, or message - and therefore\ncannot produce complete, high-quality log statements that reflect real-world\npractice in which multiple logs often appear inside one method. They also\nneglect deeper semantic dependencies among methods and consider only a narrow\nset of candidate variables, leading to superficial or incomplete logs. In this\npaper, we present PDLogger, the first end-to-end log generation technique\nexpressly designed for practical, multi-log scenarios. PDLogger operates in\nthree phases. (1) Log position prediction: block-type-aware structured prompts\nguide a large language model (LLM) to suggest candidate positions across all\ncontrol-flow blocks of a method. (2) Log generation: backward program slicing\nsupplies precise inter-procedural control and data-dependency context, while an\nexpanded variable extractor captures both member and external function\nexpressions; the enriched prompt enables the LLM to emit a full log statement\n(position, level, message, variables). (3) Log refinement: level correction and\ncontext-sensitive deduplication prune false positives and redundant logs. We\nevaluate PDLogger on 3,113 log statements drawn from two widely used Java\nprojects. Compared with the strongest prior systems, PDLogger improves\nlog-position precision by 139.0 percent, F1 by 69.2 percent, level accuracy by\n82.3 percent, variable precision by 131.8 percent, and message quality\n(BERTScore) by 65.7 percent. The framework consistently performs well with\ndifferent mainstream LLMs, demonstrating robustness and generality. PDLogger's\nimplementation is available as open source to foster future research and\nadoption.", "AI": {"tldr": "PDLogger is an end-to-end automated log generation technique that outperforms prior methods in precision, accuracy, and message quality for real-world, multi-log scenarios in software, leveraging LLMs and advanced code analysis.", "motivation": "Developers often struggle with effective software logging\u2014deciding where and how to add logs. Existing automated techniques only address isolated log parameters and create incomplete or superficial logs, without considering real-world multi-log scenarios or deep code dependencies.", "method": "PDLogger is introduced as a three-phase, end-to-end log generation technique. First, it predicts log positions using structured prompts to a large language model (LLM). Second, it generates full log statements using program slicing for context and an expanded variable extractor, with the LLM building the complete log (position, level, message, variables). Third, it refines logs by correcting levels and deduplicating contexts to remove redundancy and false positives.", "result": "PDLogger was evaluated on 3,113 log statements from two popular Java projects, significantly outperforming previous methods in log position precision (139.0% improvement), F1 score (69.2% improvement), log level accuracy (82.3% improvement), variable precision (131.8% improvement), and message quality (BERTScore; 65.7% improvement). Its robustness is verified across different LLMs.", "conclusion": "PDLogger provides a robust, generalizable, and significantly more effective approach for automatic, real-world logging in software, improving all major metrics and offering open-source implementation for further research and use."}}
{"id": "2507.20081", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.20081", "abs": "https://arxiv.org/abs/2507.20081", "authors": ["Matheus Barbosa", "Paulo Borba", "Rodrigo Bonif\u00e1cio", "Victor Lira", "Galileu Santos"], "title": "The Effect of Pointer Analysis on Semantic Conflict Detection", "comment": null, "summary": "Current merge tools don't detect semantic conflicts, which occur when changes\nfrom different developers are textually integrated but semantically interfere\nwith each other. Although researchers have proposed static analyses for\ndetecting semantic conflicts, these analyses suffer from significant false\npositive rates. To understand whether such false positives could be reduced by\nusing pointer analysis in the implementation of semantic conflict static\nanalyses, we conduct an empirical study. We implement the same analysis with\nand without pointer analysis, run them on two datasets, observe how often they\ndiffer, and compare their accuracy and computational performance. Although\npointer analysis is known to improve precision in static analysis, we find that\nits effect on semantic conflict detection can be drastic: we observe a\nsignificant reduction in timeouts and false positives, but also a significant\nincrease in false negatives, with prohibitive drops in recall and F1-score.\nThese results suggest that, in the context of semantic conflict detection, we\nshould explore hybrid analysis techniques, combining aspects of both\nimplementations we compare in our study.", "AI": {"tldr": "Adding pointer analysis to semantic conflict detection in merge tools reduces some errors but introduces serious new ones. The study finds that neither approach alone is ideal, and future work should explore hybrid analyses to balance accuracy and practicality.", "motivation": "Current merge tools fail to detect semantic conflicts, which may result from textually integrated but semantically incompatible changes made by different developers. Existing static analyses for detecting such conflicts have high false positive rates. The paper aims to explore whether incorporating pointer analysis can reduce these false positives.", "method": "The authors conduct an empirical study by implementing semantic conflict detection analysis both with and without pointer analysis. They run these implementations on two datasets, comparing differences in the results, accuracy, and computational performance, specifically measuring false positives, false negatives, recall, and F1-score.", "result": "Incorporating pointer analysis into semantic conflict detection resulted in fewer timeouts and false positives. However, it also led to a significant increase in false negatives, which caused a notable drop in recall and F1-score, potentially making the tool less reliable for practical use.", "conclusion": "While pointer analysis helps reduce some issues like timeouts and false positives in static semantic conflict detection, it introduces severe drawbacks, including more false negatives and reduced recall/F1-score. The paper suggests that hybrid approaches combining the strengths of both methods may offer a better solution."}}
{"id": "2507.20095", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.20095", "abs": "https://arxiv.org/abs/2507.20095", "authors": ["Nitika Chopra", "Taher A. Ghaleb"], "title": "From First Use to Final Commit: Studying the Evolution of Multi-CI Service Adoption", "comment": "Accepted at the 41st IEEE International Conference on Software\n  Maintenance and Evolution 2025 (ICSME'25)", "summary": "Continuous Integration (CI) services, such as GitHub Actions and Travis CI,\nare widely adopted in open-source development to automate testing and\ndeployment. Though existing research often examines individual services in\nisolation, it remains unclear how projects adopt and transition between\nmultiple services over time. To understand how CI adoption is evolving across\nservices, we present a preliminary study analyzing the historical CI adoption\nof 18,924 Java projects hosted on GitHub between January 2008 and December\n2024, adopting at least one of eight CI services, namely Travis CI, AppVeyor,\nCircleCI, Azure Pipelines, GitHub Actions, Bitbucket, GitLab CI, and Cirrus CI.\nSpecifically, we investigate: (1) how frequently CI services are co-adopted or\nreplaced, and (2) how maintenance activity varies across different services.\nOur analysis shows that the use of multiple CI services within the same project\nis a recurring pattern observed in nearly one in five projects, often\nreflecting migration across CI services. Our study is among the first to\nexamine multi-CI adoption in practice, offering new insights for future\nresearch and highlighting the need for strategies and tools to support service\nselection, coordination, and migration in evolving CI environments.", "AI": {"tldr": "The paper studies the adoption of multiple CI services in nearly 19,000 GitHub Java projects over 16 years, finding that using and migrating between CI services is common and motivates future research and tools on supporting these transitions.", "motivation": "CI services are crucial for modern software development, but prior research mainly focuses on individual services. There is limited understanding of how projects use, switch, or maintain multiple CI services over time. The authors want to clarify these patterns to help developers and researchers.", "method": "A large-scale historical analysis of 18,924 Java projects on GitHub, from 2008 to 2024, was conducted. The study tracked the adoption of eight popular CI services and assessed how often services are used together, replaced, or impact project maintenance.", "result": "Nearly 20% of projects co-adopted or migrated between different CI services, indicating that using multiple CI services is common. Maintenance activity varies across services, and migration between CI systems is a significant trend.", "conclusion": "Multi-CI service adoption is widespread, posing challenges and opportunities. There is a need for improved tools and strategies for service selection, coordination, and migration. This research provides foundational insights for further studies."}}
{"id": "2507.20109", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20109", "abs": "https://arxiv.org/abs/2507.20109", "authors": ["Xin Yin", "Chao Ni", "Liushan Chen", "Xiaohu Yang"], "title": "Learning to Align Human Code Preferences", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable potential in\nautomating software development tasks. While recent advances leverage\nSupervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to align\nmodels with human preferences, the optimal training strategy remains unclear\nacross diverse code preference scenarios. This paper systematically\ninvestigates the roles of SFT and DPO in aligning LLMs with different code\npreferences. Through both theoretical analysis and empirical observation, we\nhypothesize that SFT excels in scenarios with objectively verifiable optimal\nsolutions, while applying SFT followed by DPO (S&D) enables models to explore\nsuperior solutions in scenarios without objectively verifiable optimal\nsolutions. Based on the analysis and experimental evidence, we propose Adaptive\nPreference Optimization (APO), a dynamic integration approach that adaptively\namplifies preferred responses, suppresses dispreferred ones, and encourages\nexploration of potentially superior solutions during training. Extensive\nexperiments across six representative code preference tasks validate our\ntheoretical hypotheses and demonstrate that APO consistently matches or\nsurpasses the performance of existing SFT and S&D strategies. Our work provides\nboth theoretical foundations and practical guidance for selecting appropriate\ntraining strategies in different code preference alignment scenarios.", "AI": {"tldr": "This paper introduces Adaptive Preference Optimization (APO), a new adaptive training strategy for code-preference alignment in LLMs. APO dynamically tailors model response amplification and suppression during training, providing superior or comparable performance to traditional strategies (SFT or SFT+DPO) across multiple code preference tasks. The study offers theory and guidance for choosing training approaches based on task characteristics.", "motivation": "Existing training strategies for aligning large language models (LLMs) with human code preferences\u2014specifically Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO)\u2014lack clarity on which approach is best suited for different coding scenarios. There is a need to systematically determine optimal approaches for maximizing model performance in scenarios with varying degrees of solution objectivity.", "method": "The paper conducts both theoretical analysis and comprehensive empirical studies. It introduces a new dynamic training strategy called Adaptive Preference Optimization (APO), which adaptively amplifies, suppresses, and encourages exploration of model responses during training. The approach is validated through extensive experiments across six code preference tasks.", "result": "The experiments show that SFT is best when there are objectively verifiable optimal solutions, while a combination of SFT and DPO helps in open-ended scenarios without clear optimal answers. APO, the proposed method, consistently matches or outperforms existing strategies for all tested code preference tasks.", "conclusion": "APO provides a dynamic and adaptive approach to aligning LLMs with code preferences, outperforming traditional SFT and S&D (SFT + DPO) strategies in various scenarios. The paper offers theoretical and practical insights for choosing the best training method according to the nature of the coding task."}}
{"id": "2507.20122", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.20122", "abs": "https://arxiv.org/abs/2507.20122", "authors": ["Khairul Alam", "Banani Roy"], "title": "From Prompt to Pipeline: Large Language Models for Scientific Workflow Development in Bioinformatics", "comment": "36 pages", "summary": "The increasing complexity of bioinformatics data analysis has made Scientific\nWorkflow Systems (SWSs) like Galaxy and Nextflow essential for enabling\nscalable, reproducible, and automated workflows. However, creating and\nunderstanding these workflows remains challenging, particularly for domain\nexperts without programming expertise. This study investigates whether modern\nLarge Language Models (LLMs), GPT-4o, Gemini 2.5 Flash, and DeepSeek-V3, can\nsupport the generation of accurate, complete, and usable bioinformatics\nworkflows, and examines which prompting strategies most effectively guide this\nprocess. We evaluate these models using diverse tasks such as SNP analysis,\nRNA-seq, DNA methylation, and data retrieval, spanning both graphical (Galaxy)\nand script-based (Nextflow) platforms. Expert reviewers assess the generated\nworkflows against community-curated baselines from the Galaxy Training Network\nand nf-core repositories. The results show that Gemini 2.5 Flash excels in\ngenerating Galaxy workflows, while DeepSeek-V3 performs strongly in Nextflow.\nPrompting strategies significantly impact quality, with role-based and\nchain-of-thought prompts improving completeness and correctness. While GPT-4o\nbenefits from structured inputs, DeepSeek-V3 offers rich technical detail,\nalbeit with some verbosity. Overall, the findings highlight the potential of\nLLMs to lower the barrier for workflow development, improve reproducibility,\nand democratize access to computational tools in bioinformatics, especially\nwhen combined with thoughtful prompt engineering.", "AI": {"tldr": "LLMs like GPT-4o, Gemini 2.5 Flash, and DeepSeek-V3 can help non-programmers build bioinformatics workflows on systems like Galaxy and Nextflow. The best results come from pairing the right LLM with the right system and using smart prompting techniques, making workflow creation more accessible and reproducible.", "motivation": "The growing complexity of bioinformatics data analysis requires scalable and user-friendly workflow systems, but domain experts without programming skills struggle to create and understand workflows. The paper aims to explore whether recent large language models (LLMs) can assist in generating accurate, usable workflows for these systems.", "method": "The study evaluates three advanced LLMs\u2014GPT-4o, Gemini 2.5 Flash, and DeepSeek-V3\u2014across a range of representative bioinformatics tasks (e.g., SNP analysis, RNA-seq, DNA methylation, data retrieval), using both graphical (Galaxy) and script-based (Nextflow) workflow platforms. The experiments also assess the effect of different prompting strategies. Expert reviewers compare LLM-generated workflows to community standards from the Galaxy Training Network and nf-core.", "result": "Gemini 2.5 Flash performs best in generating workflows for Galaxy, while DeepSeek-V3 excels for Nextflow. Prompting strategies, particularly role-based and chain-of-thought prompts, significantly improve performance. GPT-4o works better with structured inputs, and DeepSeek-V3 produces detailed but sometimes verbose workflows.", "conclusion": "Modern LLMs can significantly lower the barrier for developing bioinformatics workflows, improving reproducibility and accessibility. Their utility is maximized when paired with effective prompt engineering, enabling domain experts to participate in workflow creation without advanced programming knowledge."}}
{"id": "2507.20135", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.20135", "abs": "https://arxiv.org/abs/2507.20135", "authors": ["Ganesh Pai"], "title": "Relating System Safety and Machine Learnt Model Performance", "comment": "17 pages, 4 figures, Expanded version of the paper: G. Pai, \"Deriving\n  Safety-related Performance Requirements for Machine Learnt Aeronautical\n  Applications\", Proceedings of the 44th AIAA DATC/IEEE Digital Avionics\n  Systems Conference (DASC 2025)", "summary": "The prediction quality of machine learnt models and the functionality they\nultimately enable (e.g., object detection), is typically evaluated using a\nvariety of quantitative metrics that are specified in the associated model\nperformance requirements. When integrating such models into aeronautical\napplications, a top-down safety assessment process must influence both the\nmodel performance metrics selected, and their acceptable range of values.\nOften, however, the relationship of system safety objectives to model\nperformance requirements and the associated metrics is unclear. Using an\nexample of an aircraft emergency braking system containing a machine learnt\ncomponent (MLC) responsible for object detection and alerting, this paper first\ndescribes a simple abstraction of the required MLC behavior. Then, based on\nthat abstraction, an initial method is given to derive the minimum\nsafety-related performance requirements, the associated metrics, and their\ntargets for the both MLC and its underlying deep neural network, such that they\nmeet the quantitative safety objectives obtained from the safety assessment\nprocess. We give rationale as to why the proposed method should be considered\nvalid, also clarifying the assumptions made, the constraints on applicability,\nand the implications for verification.", "AI": {"tldr": "The paper proposes a systematic method to derive machine learning model performance requirements and metrics directly from system safety objectives in aeronautical applications, using an emergency braking system as a case study.", "motivation": "There is currently a gap between high-level system safety objectives in aeronautical applications and the quantitative metrics used to measure the performance of machine-learned models, making it unclear how model-level requirements align with overall safety goals.", "method": "The paper uses an example of an aircraft emergency braking system that includes a machine-learned component for object detection and alerting. It abstracts the required behavior of this component and proposes a method to derive minimum safety-related performance requirements, corresponding metrics, and their target values to ensure the machine-learned component meets the system's safety objectives.", "result": "The proposed method provides a systematic approach to tracing system safety objectives down to the selection of model performance metrics and their minimum acceptable values. This is demonstrated through a case study and supported by rationale, a clarification of assumptions, constraints on applicability, and discussion of verification implications.", "conclusion": "A method is provided to connect safety objectives from system-level safety assessment to explicit model performance requirements for machine-learned components, facilitating more rigorous, safety-conscious integration in safety-critical contexts like aviation."}}
{"id": "2507.20218", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.20218", "abs": "https://arxiv.org/abs/2507.20218", "authors": ["Muhammad Azeem Akbar", "Arif Ali Khan", "Saima Rafi", "Damian Kedziora", "Sami Hyrynsalmi"], "title": "Strategic Motivators for Ethical AI System Development: An Empirical and Holistic Model", "comment": null, "summary": "Artificial Intelligence (AI) presents transformative opportunities for\nindustries and society, but its responsible development is essential to prevent\nunintended consequences. Ethically sound AI systems demand strategic planning,\nstrong governance, and an understanding of the key drivers that promote\nresponsible practices. This study aims to identify and prioritize the\nmotivators that drive the ethical development of AI systems. A Multivocal\nLiterature Review (MLR) and a questionnaire-based survey were conducted to\ncapture current practices in ethical AI. We applied Interpretive Structure\nModeling (ISM) to explore the relationships between motivator categories,\nfollowed by MICMAC analysis to classify them by their driving and dependence\npower. Fuzzy TOPSIS was used to rank these motivators by importance. Twenty key\nmotivators were identified and grouped into eight categories: Human Resource,\nKnowledge Integration, Coordination, Project Administration, Standards,\nTechnology Factor, Stakeholders, and Strategy & Matrices. ISM results showed\nthat 'Human Resource' and 'Coordination' heavily influence other factors.\nMICMAC analysis placed categories like Human Resource (CA1), Coordination\n(CA3), Stakeholders (CA7), and Strategy & Matrices (CA8) in the independent\ncluster, indicating high driving but low dependence power. Fuzzy TOPSIS ranked\nmotivators such as promoting team diversity, establishing AI governance bodies,\nappointing oversight leaders, and ensuring data privacy as most critical. To\nsupport ethical AI adoption, organizations should align their strategies with\nthese motivators and integrate them into their policies, governance models, and\ndevelopment frameworks.", "AI": {"tldr": "The paper identifies and ranks crucial motivators for ethical AI development, finding Human Resource and Coordination as the most influential, and recommends organizations incorporate these motivators into their governance and frameworks.", "motivation": "The motivation is to determine and prioritize the key drivers for the ethical development of Artificial Intelligence, recognizing the need to prevent unintended consequences as AI transforms industries and society.", "method": "The study conducted a Multivocal Literature Review (MLR) and a questionnaire survey to gather data, then used Interpretive Structure Modeling (ISM) to analyze relationships between motivator categories, MICMAC analysis to assess their driving and dependence power, and Fuzzy TOPSIS to rank their importance.", "result": "Twenty key motivators were identified and grouped into eight categories. 'Human Resource' and 'Coordination' are most influential over others. Several categories (Human Resource, Coordination, Stakeholders, Strategy & Matrices) have high driving but low dependence power. Top motivators include promoting team diversity, governance bodies, oversight leaders, and data privacy.", "conclusion": "Organizations should integrate the identified key motivators into their strategies, policies, and development frameworks to support ethical AI adoption."}}
{"id": "2507.20358", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.20358", "abs": "https://arxiv.org/abs/2507.20358", "authors": ["Tanni Dev", "Sayma Sultana", "Amiangshu Bosu"], "title": "Beyond Binary Moderation: Identifying Fine-Grained Sexist and Misogynistic Behavior on GitHub with Large Language Models", "comment": "The 19th ACM/IEEE International Symposium on Empirical Software\n  Engineering and Measurement", "summary": "Background: Sexist and misogynistic behavior significantly hinders inclusion\nin technical communities like GitHub, causing developers, especially\nminorities, to leave due to subtle biases and microaggressions. Current\nmoderation tools primarily rely on keyword filtering or binary classifiers,\nlimiting their ability to detect nuanced harm effectively.\n  Aims: This study introduces a fine-grained, multi-class classification\nframework that leverages instruction-tuned Large Language Models (LLMs) to\nidentify twelve distinct categories of sexist and misogynistic comments on\nGitHub.\n  Method: We utilized an instruction-tuned LLM-based framework with systematic\nprompt refinement across 20 iterations, evaluated on 1,440 labeled GitHub\ncomments across twelve sexism/misogyny categories. Model performances were\nrigorously compared using precision, recall, F1-score, and the Matthews\nCorrelation Coefficient (MCC).\n  Results: Our optimized approach (GPT-4o with Prompt 19) achieved an MCC of\n0.501, significantly outperforming baseline approaches. While this model had\nlow false positives, it struggled to interpret nuanced, context-dependent\nsexism and misogyny reliably.\n  Conclusion: Well-designed prompts with clear definitions and structured\noutputs significantly improve the accuracy and interpretability of sexism\ndetection, enabling precise and practical moderation on developer platforms\nlike GitHub.", "AI": {"tldr": "This paper presents a new approach to detecting nuanced sexist and misogynistic comments in technical communities like GitHub using a fine-grained, multi-class classification framework based on instruction-tuned LLMs.", "motivation": "Existing moderation tools are insufficient for capturing subtle and context-dependent sexist behaviors, leading to exclusion and attrition among minority developers. More nuanced detection methods are needed to better protect online technical communities.", "method": "The researchers trained and iteratively refined prompts for a large language model over 20 iterations, tested on 1,440 GitHub comments categorized into 12 types of sexism/misogyny. Performance was evaluated using metrics like F1-score, precision, recall, and MCC.", "result": "The best model (GPT-4o with Prompt 19) achieved an MCC of 0.501, outperforming baselines with low false positive rates. However, challenges remain in accurately detecting context-dependent, nuanced sexism.", "conclusion": "Instruction-tuned LLMs, when paired with well-crafted prompts and structured outputs, can significantly improve the accuracy and clarity of detecting sexism and misogyny, supporting more effective moderation."}}
{"id": "2507.20402", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.20402", "abs": "https://arxiv.org/abs/2507.20402", "authors": ["Md Nazmul Hossain", "Taher A. Ghaleb"], "title": "CIgrate: Automating CI Service Migration with Large Language Models", "comment": "Registered Report Accepted at the 41st IEEE International Conference\n  on Software Maintenance and Evolution 2025 (ICSME'25)", "summary": "Continuous Integration (CI) configurations often need to be migrated between\nservices (e.g., Travis CI to GitHub Actions) as projects evolve, due to changes\nin service capabilities, usage limits, or service deprecation. Previous studies\nreported that migration across CI services is a recurring need in open-source\ndevelopment. However, manual migration can be time-consuming and error-prone.\nThe state-of-the-art approach, CIMig, addresses this challenge by analyzing\npast migration examples to create service-specific rules and produce equivalent\nconfigurations across CI services. However, its relatively low accuracy raises\nconcerns about the overall feasibility of automated CI migration using\nrule-based techniques alone. Meanwhile, Large Language Models (LLMs) have\ndemonstrated strong capabilities in code generation and transformation tasks,\nsuggesting potential to improve the automation, usability, and generalizability\nof CI configuration migration. This registered report presents a study in which\nwe aim to assess whether CI migration can be improved using LLMs. To this end,\nwe propose CIgrate, an LLM-based framework for automatically migrating CI\nconfigurations. We plan to evaluate the performance of CIgrate compared to\nCIMig as a baseline, in different setups (a) zero-shot/few-shot prompting of\nLLMs for configuration migration and (b) fine-tuning an LLM on a dataset of\nalready established CI service migrations. We will also seek developer feedback\non the quality and usability of the generated configurations. We formulate\nresearch questions focusing on the accuracy of LLM-generated migrations versus\nground truth and the output of CIMig. The expected contributions include the\nfirst LLM-powered approach for CI service migration, a comparative evaluation\nof its effectiveness compared to rule-based approaches, and insight into\nleveraging LLMs to support software configuration evolution.", "AI": {"tldr": "This paper introduces CIgrate, a framework leveraging large language models to improve automation and accuracy in migrating CI configurations between services, and compares its performance and usability to existing rule-based methods.", "motivation": "Continuous Integration (CI) service migration is often needed in open-source development due to evolving requirements or limitations, but current rule-based automation methods like CIMig are not accurate enough. Manual migration is difficult and error-prone, motivating the search for better automated solutions.", "method": "The paper proposes CIgrate, a framework using Large Language Models (LLMs) to automatically migrate CI configurations. The study compares CIgrate to the rule-based baseline CIMig using zero-shot/few-shot prompting and fine-tuning techniques, and also collects developer feedback on output quality and usability.", "result": "The study is designed to evaluate whether LLMs can improve the accuracy and usability of automated CI migration compared to existing rule-based systems. It aims to produce and analyze comparative performance data and user feedback.", "conclusion": "The expected findings are that LLMs can provide a more accurate and generalizable solution for CI migration, offering the first LLM-powered approach and deeper insights into supporting configuration evolution with AI."}}
{"id": "2507.20407", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.20407", "abs": "https://arxiv.org/abs/2507.20407", "authors": ["Davi Gama Hardman", "Cesar Fran\u00e7a", "Brody Stuart-Verner", "Ronnie de Souza Santos"], "title": "Testing Is Not Boring: Characterizing Challenge in Software Testing Tasks", "comment": null, "summary": "As software systems continue to grow in complexity, testing has become a\nfundamental part of ensuring the quality and reliability of software products.\nYet, software testing is still often perceived, both in industry and academia,\nas a repetitive, low-skill activity. This perception fails to recognize the\ncreativity, problem-solving, and adaptability required in testing work. Tasks\nsuch as designing complex test cases, automating testing processes, and\nhandling shifting requirements illustrate the challenges testing professionals\nregularly face. To better understand these experiences, we conducted a study\nwith software testing professionals to explore the nature of challenging tasks\nin software testing and how they affect these professionals. Our findings show\nthat tasks involving creativity, ongoing learning, and time pressure are often\nseen as motivating and rewarding. On the other hand, a lack of challenge or\noverwhelming demands can lead to frustration and disengagement. These findings\ndemonstrate the importance of balancing task complexity to sustain motivation\nand present software testing as a dynamic and intellectually engaging field.", "AI": {"tldr": "Software testing requires creativity and adaptability, not just repetitive skills. Challenging tasks motivate professionals, while lack of challenge or excessive demands cause frustration. Striking the right balance in task complexity is key to sustaining engagement in testing roles.", "motivation": "The motivation is to address the misconception that software testing is a repetitive and low-skill activity by highlighting the creativity, problem-solving, and adaptability involved in testing work. The authors aim to better understand the experiences of software testing professionals, especially regarding challenging tasks.", "method": "The authors conducted a study involving software testing professionals to explore the nature of challenging tasks in their work and their effects.", "result": "The study reveals that tasks involving creativity, continuous learning, and time pressure are generally perceived as motivating and rewarding by testing professionals. However, tasks lacking challenge or with overwhelming demands can result in frustration and disengagement.", "conclusion": "The paper concludes that balancing task complexity is crucial in maintaining motivation among testing professionals and upholds software testing as a dynamic and intellectually engaging discipline."}}
{"id": "2507.20439", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20439", "abs": "https://arxiv.org/abs/2507.20439", "authors": ["Maya Larbi", "Amal Akli", "Mike Papadakis", "Rihab Bouyousfi", "Maxime Cordy", "Federica Sarro", "Yves Le Traon"], "title": "When Prompts Go Wrong: Evaluating Code Model Robustness to Ambiguous, Contradictory, and Incomplete Task Descriptions", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive performance in code\ngeneration tasks under idealized conditions, where task descriptions are clear\nand precise. However, in practice, task descriptions frequently exhibit\nambiguity, incompleteness, or internal contradictions. In this paper, we\npresent the first empirical study examining the robustness of state-of-the-art\ncode generation models when faced with such unclear task descriptions. We\nextend the HumanEval and MBPP benchmarks by systematically introducing\nrealistic task descriptions flaws through guided mutation strategies, producing\na dataset that mirrors the messiness of informal developer instructions. We\nevaluate multiple LLMs of varying sizes and architectures, analyzing their\nfunctional correctness and failure modes across task descriptions categories.\nOur findings reveal that even minor imperfections in task description phrasing\ncan cause significant performance degradation, with contradictory task\ndescriptions resulting in numerous logical errors. Moreover, while larger\nmodels tend to be more resilient than smaller variants, they are not immune to\nthe challenges posed by unclear requirements. We further analyze semantic error\npatterns and identify correlations between description clarity, model behavior,\nand error types. Our results underscore the critical need for developing LLMs\nthat are not only powerful but also robust to the imperfections inherent in\nnatural user tasks, highlighting important considerations for improving model\ntraining strategies, designing more realistic evaluation benchmarks, and\nensuring reliable deployment in practical software development environments.", "AI": {"tldr": "Even top-performing LLMs for code generation struggle with ambiguous, incomplete, or contradictory task descriptions\u2014common in real-world scenarios. The paper shows that current models are fragile under such conditions, calls for improvement in model robustness, and suggests changes to evaluation frameworks to better reflect actual usage.", "motivation": "Current code generation benchmarks use idealized, clear, and unambiguous task descriptions, whereas real-world developer instructions are often unclear, incomplete, or contradictory. The motivation is to understand how state-of-the-art code generation models perform under realistic, messy conditions and to identify their robustness to ambiguous requirements.", "method": "The authors systematically altered (mutated) task descriptions in established code generation benchmarks (HumanEval and MBPP) to introduce ambiguity, incompleteness, and contradictions, simulating real-world developer instructions. They evaluated several large language models (LLMs) of different sizes and architectures by measuring their performance and analyzing error types on these flawed benchmarks.", "result": "The study found that LLMs experience significant performance degradation when task descriptions are even slightly unclear. Contradictory descriptions in particular lead to many logical errors. Although larger models are somewhat more resilient to these flaws, none are immune. Error patterns correlate with the level of description clarity.", "conclusion": "There is a critical need to improve LLM robustness to real-world, imperfect task descriptions. Advances in model training, better evaluation benchmarks, and considerations for deployment in software development should focus on handling the inherent ambiguities of natural user instructions."}}
{"id": "2507.20475", "categories": ["cs.SE", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.20475", "abs": "https://arxiv.org/abs/2507.20475", "authors": ["Ahmik Virani", "Devraj", "Anirudh Suresh", "Lei Zhang", "M V Panduranga Rao"], "title": "Distinguishing Quantum Software Bugs from Hardware Noise: A Statistical Approach", "comment": "12 pages, 30 figures, accepted by the IEEE International Conference\n  on Quantum Computing and Engineering (QCE), IEEE Quantum Week, 2025", "summary": "Quantum computing in the Noisy Intermediate-Scale Quantum (NISQ) era presents\nsignificant challenges in differentiating quantum software bugs from hardware\nnoise. Traditional debugging techniques from classical software engineering\ncannot directly resolve this issue due to the inherently stochastic nature of\nquantum computation mixed with noises from NISQ computers. To address this gap,\nwe propose a statistical approach leveraging probabilistic metrics to\ndifferentiate between quantum software bugs and hardware noise. We evaluate our\nmethodology empirically using well-known quantum algorithms, including Grover's\nalgorithm, Deutsch-Jozsa algorithm, and Simon's algorithm. Experimental results\ndemonstrate the efficacy and practical applicability of our approach, providing\nquantum software developers with a reliable analytical tool to identify and\nclassify unexpected behavior in quantum programs.", "AI": {"tldr": "The paper introduces a statistical method to help developers tell apart quantum software bugs from hardware noise in NISQ computers, and demonstrates its effectiveness through experiments on key quantum algorithms.", "motivation": "In the NISQ era, quantum computers are prone to both software bugs and hardware-induced noise, making it difficult to identify the root cause of unexpected behavior in quantum programs. Classical debugging approaches are inadequate due to quantum computation's probabilistic nature and noise.", "method": "The authors propose a statistical approach using probabilistic metrics to differentiate between quantum software bugs and hardware-induced noise. Their methodology is empirically evaluated on well-known quantum algorithms like Grover's, Deutsch-Jozsa, and Simon's algorithms.", "result": "The experimental results show that the proposed statistical approach is effective and practically applicable in distinguishing between software bugs and hardware noise in quantum programs.", "conclusion": "This statistical methodology offers quantum software developers a reliable tool for analyzing and classifying unexpected behaviors, helping to improve debugging in noisy intermediate-scale quantum computers."}}
{"id": "2507.20502", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.20502", "abs": "https://arxiv.org/abs/2507.20502", "authors": ["Howell Xia", "Jonah Gluck", "Sevval Simsek", "David Sastre Medina", "David Starobinski"], "title": "VDGraph: A Graph-Theoretic Approach to Unlock Insights from SBOM and SCA Data", "comment": null, "summary": "The high complexity of modern software supply chains necessitates tools such\nas Software Bill of Materials (SBOMs) to manage component dependencies, and\nSoftware Composition Analysis (SCA) tools to identify vulnerabilities. While\nthere exists limited integration between SBOMs and SCA tools, a unified view of\ncomplex dependency-vulnerability relationships remains elusive. In this paper,\nwe introduce VDGraph, a novel knowledge graph-based methodology for integrating\nvulnerability and dependency data into a holistic view. VDGraph consolidates\nSBOM and SCA outputs into a graph representation of software projects'\ndependencies and vulnerabilities. We provide a formal description and analysis\nof the theoretical properties of VDGraph and present solutions to manage\npossible conflicts between the SBOM and SCA data. We further introduce and\nevaluate a practical, proof-of-concept implementation of VDGraph using two\npopular SBOM and SCA tools, namely CycloneDX Maven plugin and Google's\nOSV-Scanner. We apply VDGraph on 21 popular Java projects. Through the\nformulation of appropriate queries on the graphs, we uncover the existence of\nconcentrated risk points (i.e., vulnerable components of high severity\nreachable through numerous dependency paths). We further show that\nvulnerabilities predominantly emerge at a depth of three dependency levels or\nhigher, indicating that direct or secondary dependencies exhibit lower\nvulnerability density and tend to be more secure. Thus, VDGraph contributes a\ngraph-theoretic methodology that improves visibility into how vulnerabilities\npropagate through complex, transitive dependencies. Moreover, our\nimplementation, which combines open SBOM and SCA standards with Neo4j, lays a\nfoundation for scalable and automated analysis across real-world projects.", "AI": {"tldr": "This paper presents VDGraph, a knowledge graph framework that unifies SBOM and SCA data to analyze vulnerabilities in software supply chains. By implementing and applying VDGraph to real Java projects, the authors reveal that most severe vulnerabilities reside deep in dependency trees and demonstrate how the tool improves risk analysis and visibility.", "motivation": "Modern software supply chains are highly complex, making it difficult to manage component dependencies and identify vulnerabilities effectively. While tools like SBOM and SCA exist, their integration is limited, and there is no unified way to view the relationships between dependencies and vulnerabilities.", "method": "The paper introduces VDGraph, a knowledge graph-based methodology that integrates vulnerability (from SCA) and dependency (from SBOM) data into a comprehensive graph. The authors provide a formal analysis of VDGraph, implement a proof-of-concept using CycloneDX Maven plugin and OSV-Scanner, and apply it to 21 Java projects to evaluate its effectiveness.", "result": "The application of VDGraph enabled the discovery of concentrated risk points\u2014vulnerable components with high severity reachable through multiple paths. It was also found that vulnerabilities mainly occur at three or more levels of dependency depth, while direct and secondary dependencies are generally more secure.", "conclusion": "VDGraph offers a graph-based approach that enhances visibility into vulnerability propagation through complex software dependencies. The proof-of-concept, combining open source tools and graph technology, demonstrates scalability and potential for automated real-world analysis."}}
{"id": "2507.20553", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.20553", "abs": "https://arxiv.org/abs/2507.20553", "authors": ["Guanyu Chen", "Haoyue Jiao", "Shuyang Hou", "Ziqi Liu", "Lutong Xie", "Shaowen Wu", "Huayi Wu", "Xuefeng Guan", "Zhipeng Gui"], "title": "GeoJSEval: An Automated Evaluation Framework for Large Language Models on JavaScript-Based Geospatial Computation and Visualization Code Generation", "comment": null, "summary": "With the widespread adoption of large language models (LLMs) in code\ngeneration tasks, geospatial code generation has emerged as a critical frontier\nin the integration of artificial intelligence and geoscientific analysis. This\ntrend underscores the urgent need for systematic evaluation methodologies to\nassess LLMs generation capabilities in geospatial contexts. In particular,\ngeospatial computation and visualization tasks in JavaScript environments rely\nheavily on orchestrating diverse frontend libraries and ecosystems, placing\nelevated demands on a model's semantic understanding and code synthesis\nabilities. To address this challenge, we propose GeoJSEval--the first\nmultimodal, function-level automatic evaluation framework for LLMs in\nJavaScript-based geospatial code generation. GeoJSEval comprises three core\ncomponents: a standardized test suite (GeoJSEval-Bench), a code submission\nengine, and an evaluation module. It includes 432 function-level tasks and\n2,071 structured test cases spanning five widely used JavaScript geospatial\nlibraries and 25 mainstream geospatial data types. GeoJSEval enables\nmultidimensional quantitative evaluation across metrics such as accuracy,\noutput stability, execution efficiency, resource consumption, and error type\ndistribution, and integrates boundary testing mechanisms to enhance robustness\nand coverage. We conduct a comprehensive evaluation of 18 state-of-the-art LLMs\nusing GeoJSEval, revealing significant performance disparities and bottlenecks\nin spatial semantic understanding, code reliability, and function invocation\naccuracy. GeoJSEval provides a foundational methodology, evaluation resource,\nand practical toolkit for the standardized assessment and optimization of\ngeospatial code generation models, with strong extensibility and applicability\nin real-world scenarios.", "AI": {"tldr": "The paper introduces GeoJSEval, the first automatic multimodal evaluation framework for LLM-generated geospatial JavaScript code. Covering hundreds of tasks and test cases across key libraries and data types, GeoJSEval enables detailed, robust measurement of model performance. The evaluation of 18 LLMs reveals key strengths and weaknesses, providing a standardized and extensible tool for advancing geospatial code generation research and applications.", "motivation": "The paper is motivated by the increasing use of large language models (LLMs) for code generation in geospatial tasks, particularly in JavaScript environments, where orchestrating multiple libraries and handling complex geospatial data poses unique challenges. There is an urgent need for systematic and robust evaluation methods for the performance of LLMs in these contexts.", "method": "To address this, the authors propose GeoJSEval, a multimodal, function-level automatic evaluation framework that consists of a standardized test suite (GeoJSEval-Bench), a code submission engine, and an evaluation module. The framework provides 432 function-level tasks and 2,071 structured test cases across five major JavaScript geospatial libraries and 25 geospatial data types. It enables multidimensional evaluation along several metrics including accuracy, output stability, execution efficiency, resource consumption, and error types, also incorporating boundary testing for enhanced robustness.", "result": "Using GeoJSEval, the authors conducted a comprehensive analysis of 18 state-of-the-art LLMs. The evaluation revealed significant performance differences and exposed bottlenecks in spatial semantic understanding, code reliability, and function invocation accuracy in generated geospatial code.", "conclusion": "GeoJSEval offers a standardized, extensible, and practical toolkit for assessing and improving LLMs in geospatial code generation. It provides a crucial foundational methodology and resource for future research and real-world applications in this domain."}}
{"id": "2507.20619", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.20619", "abs": "https://arxiv.org/abs/2507.20619", "authors": ["Binhang Qi", "Yun Lin", "Xinyi Weng", "Yuhuan Huang", "Chenyan Liu", "Hailong Sun", "Jin Song Dong"], "title": "Intention-Driven Generation of Project-Specific Test Cases", "comment": null, "summary": "Test cases are valuable assets for maintaining software quality. While\nnumerous automated techniques have been proposed for generating tests (either\nby maximizing code coverage or by translating focal code into test code),\npractical tests are seldom driven by coverage alone. In real projects, each\ntest reflects a developer's validation intention for a specific behaviour and\nembodies rich, project-specific knowledge: which specific APIs to call and what\nassertions truly matter. Without considering such knowledge, tests can hardly\npass code review and be integrated into the software product.\n  In this work, we propose IntentionTest, which generates project-specific\ntests with validation intention as a structured description. Our design is\nmotivated by two insights: (1) a description of validation intention, compared\nto coverage and focal code, carries more crucial information about what to\ntest; and (2) practical tests exhibit high code duplication, indicating that\ndomain knowledge is highly reusable for writing new tests. Given a focal code\nand a description of validation intention (in the form of either an informal\ncomment or a formal test plan), IntentionTest retrieves a referable test in the\nproject to guide test generation. Moreover, IntentionTest reduces the test\ngeneration problem into an editing problem on the test code regarding the\nvalidation intention. It generates a test including both test prefix and\noracle, which aims to be executable and semantically correct.\n  We evaluate IntentionTest against state-of-the-art baselines on 4,146 test\ncases from 13 open-source projects. Specifically, compared to ChatTester,\nIntentionTest can (1) generate significantly more semantically correct tests,\nimproving common mutation scores by 39.03% and coverage overlap with\nground-truth tests by 40.14%; (2) generate 21.30% more successful passing\ntests.", "AI": {"tldr": "Automated test generation methods focused only on code coverage are insufficient for real-world usage. IntentionTest leverages structured validation intentions and domain-specific knowledge to guide test generation, producing significantly more correct and usable tests than state-of-the-art methods, as validated on thousands of open-source project cases.", "motivation": "Existing automated test generation approaches mostly focus on code coverage or translating code directly into tests. However, these approaches overlook developers' specific validation intentions and project-specific knowledge, which are crucial for creating practical and useful tests that can pass code review and be integrated into real software projects.", "method": "The paper proposes 'IntentionTest', a technique that generates project-specific test cases using structured descriptions of validation intentions. It retrieves relevant existing tests from the project to guide the new test generation. The approach frames test generation as an editing task on the retrieved test code, aligning it with the specified validation intention, and aims to produce executable and semantically correct tests including both setup (prefix) and assertion (oracle). Its effectiveness is evaluated against state-of-the-art baselines using 4,146 test cases from 13 open-source projects.", "result": "IntentionTest outperformed existing methods, specifically generating significantly more semantically correct tests. It improved mutation scores by 39.03% and coverage overlap with ground-truth tests by 40.14% compared to the ChatTester baseline, as well as generating 21.30% more successful passing tests.", "conclusion": "Incorporating validation intentions and reusing project-specific domain knowledge enables more practical and effective automated test generation. IntentionTest demonstrates superior ability to generate semantically correct and executable tests that meet real development needs, outperforming existing test generation techniques based solely on code coverage or code-to-test translation."}}
{"id": "2507.20814", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.20814", "abs": "https://arxiv.org/abs/2507.20814", "authors": ["Gustave Monce", "Thomas Degueule", "Jean-R\u00e9my Falleri", "Romain Robbes"], "title": "Client--Library Compatibility Testing with API Interaction Snapshots", "comment": null, "summary": "Modern software development heavily relies on third-party libraries to speed\nup development and enhance quality. As libraries evolve, they may break the\ntacit contract established with their clients by introducing behavioral\nbreaking changes (BBCs) that alter run-time behavior and silently break client\napplications without being detected at compile time. Traditional regression\ntests on the client side often fail to detect such BBCs, either due to limited\nlibrary coverage or weak assertions that do not sufficiently exercise the\nlibrary's expected behavior. To address this issue, we propose a novel approach\nto client--library compatibility testing that leverages existing client tests\nin a novel way. Instead of relying on developer-written assertions, we propose\nrecording the actual interactions at the API boundary during the execution of\nclient tests (protocol, input and output values, exceptions, etc.). These\nsequences of API interactions are stored as snapshots which capture the exact\ncontract expected by a client at a specific point in time. As the library\nevolves, we compare the original and new snapshots to identify perturbations in\nthe contract, flag potential BBCs, and notify clients. We implement this\ntechnique in our prototype tool Gilesi, a Java framework that automatically\ninstruments library APIs, records snapshots, and compares them. Through a\npreliminary case study on several client--library pairs with artificially\nseeded BBCs, we show that Gilesi reliably detects BBCs missed by client test\nsuites.", "AI": {"tldr": "Relying only on traditional regression tests leaves client applications exposed to subtle, hard-to-detect breaking changes in third-party libraries. This paper proposes and validates a method\u2014implemented in the tool Gilesi\u2014to record and compare actual API interactions (snapshots) during testing, which successfully catches these breaking changes where standard methods fail.", "motivation": "Traditional regression tests for software that rely on third-party libraries often fail to detect silent, behavioral breaking changes (BBCs) introduced by library updates. This is due to limited test coverage or weak assertions in client tests, leaving applications vulnerable to undetected run-time errors.", "method": "The authors propose a compatibility testing approach that records actual API interactions (inputs, outputs, exceptions, etc.) at the client-library boundary during client test execution. These recorded interactions form 'snapshots' which represent the contract expected by the client. When the library evolves, new snapshots are compared with the original ones to identify changes, potentially flagging BBCs. This technique is implemented in a Java framework called Gilesi, which automates API instrumentation, snapshot recording, and comparison.", "result": "The prototype tool, Gilesi, was evaluated in a preliminary case study on several client-library pairs where BBCs were artificially inserted. Gilesi successfully detected BBCs that were missed by the client test suites, demonstrating its effectiveness in revealing subtle, contract-breaking changes.", "conclusion": "The paper concludes that snapshot-based API interaction recording and comparison is a practical and effective way to flag behavioral breaking changes that would evade traditional client-side regression testing. The approach is automated, works with existing client tests, and increases resilience to silent incompatibilities between evolving libraries and their clients."}}
{"id": "2507.20848", "categories": ["cs.SE", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.20848", "abs": "https://arxiv.org/abs/2507.20848", "authors": ["Hernan Ghianni", "Man Zhang", "Juan P. Galeotti", "Andrea Arcuri"], "title": "Search-Based Fuzzing For RESTful APIs That Use MongoDB", "comment": null, "summary": "In RESTful APIs, interactions with a database are a common and crucial\naspect. When generating whitebox tests, it is essential to consider the\ndatabase's state (i.e., the data contained in the database) to achieve higher\ncode coverage and uncover more hidden faults. This article presents novel\ntechniques to enhance search-based software test generation for RESTful APIs\ninteracting with NoSQL databases. Specifically, we target the popular MongoDB\ndatabase, by dynamically analyzing (via automated code instrumentation) the\nstate of the database during the test generation process. Additionally, to\nachieve better results, our novel approach allows inserting NoSQL data directly\nfrom test cases. This is particularly beneficial when generating the correct\nsequence of events to set the NoSQL database in an appropriate state is\nchallenging or time-consuming. This method is also advantageous for testing\nread-only microservices. Our novel techniques are implemented as an extension\nof EvoMaster, the only open-source tool for white-box fuzzing RESTful APIs.\nExperiments conducted on six RESTful APIs demonstrated significant improvements\nin code coverage, with increases of up to 18% compared to existing white-box\napproaches. To better highlight the improvements of our novel techniques,\ncomparisons are also carried out with four state-of-the-art black-box fuzzers.", "AI": {"tldr": "Enhancing white-box testing for RESTful APIs using MongoDB by dynamically analyzing and inserting database states during test generation, leading to up to 18% higher code coverage compared to existing techniques.", "motivation": "Testing RESTful APIs that interact with NoSQL databases requires consideration of the dynamic database state, which is often neglected by existing white-box testing techniques. Achieving high code coverage and fault detection is more challenging when the database state is not easily or effectively manipulated during test generation.", "method": "The authors propose novel search-based software test generation techniques for RESTful APIs that interact with NoSQL databases (specifically MongoDB). These techniques dynamically analyze the runtime state of the database through automated code instrumentation and allow direct insertion of NoSQL data from within test cases. The approach is implemented as an extension to EvoMaster, an open-source white-box fuzzing tool.", "result": "In experiments conducted on six RESTful APIs, the proposed methods led to significant improvements in code coverage, with up to 18% higher coverage compared to previous white-box approaches. The new techniques also outperformed four state-of-the-art black-box fuzzers.", "conclusion": "Taking the dynamic state of NoSQL databases into account and allowing direct data insertion from test cases can greatly improve the effectiveness of white-box test generation for RESTful APIs. The authors\u2019 extension to EvoMaster demonstrates a substantial increase in code coverage and shows clear advantages over existing tools."}}
{"id": "2507.20888", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.20888", "abs": "https://arxiv.org/abs/2507.20888", "authors": ["Le Deng", "Xiaoxue Ren", "Chao Ni", "Ming Liang", "David Lo", "Zhongxin Liu"], "title": "Enhancing Project-Specific Code Completion by Inferring Internal API Information", "comment": null, "summary": "Project-specific code completion is a critical task that leverages context\nfrom a project to generate accurate code. State-of-the-art methods use\nretrieval-augmented generation (RAG) with large language models (LLMs) and\nproject information for code completion. However, they often struggle to\nincorporate internal API information, which is crucial for accuracy, especially\nwhen APIs are not explicitly imported in the file.\n  To address this, we propose a method to infer internal API information\nwithout relying on imports. Our method extends the representation of APIs by\nconstructing usage examples and semantic descriptions, building a knowledge\nbase for LLMs to generate relevant completions. We also introduce ProjBench, a\nbenchmark that avoids leaked imports and consists of large-scale real-world\nprojects.\n  Experiments on ProjBench and CrossCodeEval show that our approach\nsignificantly outperforms existing methods, improving code exact match by\n22.72% and identifier exact match by 18.31%. Additionally, integrating our\nmethod with existing baselines boosts code match by 47.80% and identifier match\nby 35.55%.", "AI": {"tldr": "Current code completion models struggle with internal APIs lacking explicit imports. This paper presents a way to infer and represent such APIs for LLM-based completion, yielding large performance gains on new and existing benchmarks.", "motivation": "Project-specific code completion struggles when internal APIs aren't explicitly imported, which hinders code accuracy in large, modular projects. Existing LLMs with RAG can't easily incorporate these unseen APIs during code generation, creating a gap in performance.", "method": "The proposed method infers internal API information even when imports are missing. This is done by constructing usage examples and semantic descriptions for APIs to create a knowledge base. LLMs use this enriched context for improved code completions. The authors also introduce ProjBench, a real-world project benchmark designed to avoid information leakage from import statements.", "result": "Experiments on ProjBench and CrossCodeEval benchmarks show the method significantly outperforms previous approaches, raising code exact match by 22.72% and identifier exact match by 18.31%. Integrating the method with baselines further boosts code match by 47.80% and identifier match by 35.55%.", "conclusion": "Inferring and enriching internal API representations without relying on imports results in much more accurate project-specific code completion. Using knowledge bases constructed from examples and semantic information allows LLMs to address the internal API gap effectively, as shown on rigorous new benchmarks."}}
{"id": "2507.20977", "categories": ["cs.SE", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20977", "abs": "https://arxiv.org/abs/2507.20977", "authors": ["Maria Camporese", "Fabio Massacci"], "title": "Repairing vulnerabilities without invisible hands. A differentiated replication study on LLMs", "comment": null, "summary": "Background: Automated Vulnerability Repair (AVR) is a fast-growing branch of\nprogram repair. Recent studies show that large language models (LLMs)\noutperform traditional techniques, extending their success beyond code\ngeneration and fault detection.\n  Hypothesis: These gains may be driven by hidden factors -- \"invisible hands\"\nsuch as training-data leakage or perfect fault localization -- that let an LLM\nreproduce human-authored fixes for the same code.\n  Objective: We replicate prior AVR studies under controlled conditions by\ndeliberately adding errors to the reported vulnerability location in the\nprompt. If LLMs merely regurgitate memorized fixes, both small and large\nlocalization errors should yield the same number of correct patches, because\nany offset should divert the model from the original fix.\n  Method: Our pipeline repairs vulnerabilities from the Vul4J and VJTrans\nbenchmarks after shifting the fault location by n lines from the ground truth.\nA first LLM generates a patch, a second LLM reviews it, and we validate the\nresult with regression and proof-of-vulnerability tests. Finally, we manually\naudit a sample of patches and estimate the error rate with the\nAgresti-Coull-Wilson method.", "AI": {"tldr": "This paper questions whether LLMs' impressive results in automated vulnerability repair are genuine or inflated by hidden factors, by experimentally disturbing fault localization information in repair tasks and rigorously assessing the impact on LLM patching performance.", "motivation": "The motivation behind this paper is to investigate whether the impressive performance of large language models (LLMs) in Automated Vulnerability Repair (AVR) is genuinely due to their capabilities, or if it is influenced by hidden biases such as training-data leakage or overly accurate fault localization in benchmarks.", "method": "The paper uses a controlled experiment where errors are deliberately introduced to the reported location of vulnerabilities in AVR prompts. Utilizing the Vul4J and VJTrans benchmarks, LLMs are tasked with generating and reviewing patches with fault locations shifted by various numbers of lines from the actual site. The effectiveness and accuracy of the repairs are then validated using regression and proof-of-vulnerability tests, followed by manual auditing and estimation of error rates via the Agresti-Coull-Wilson method.", "result": "The results will reveal whether LLM-based AVR performance diminishes when the fault localization is less accurate, thereby assessing if LLMs are simply memorizing existing fixes or can generalize repairs away from ground truth fault positions. The specific outcome of the experiment is not detailed in the abstract.", "conclusion": "The conclusion aims to clarify whether current LLM AVR benchmarks reflect true repair capability or are overly optimistic due to underlying experimental or data biases. The study's outcome will inform best practices for future AVR benchmarking and development."}}
