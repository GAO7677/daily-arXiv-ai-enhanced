<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 5]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Vision: An Extensible Methodology for Formal Software Verification in Microservice Systems](https://arxiv.org/abs/2509.02860)
*Connor Wojtak,Darek Gajewski,Tomas Cerny*

Main category: cs.SE

TL;DR: The paper introduces a static analysis methodology for reconstructing microservice source code into a formal model, enabling formal verification (via SMT) for system architecture and other concerns like security. This helps address maintainability and reliability challenges caused by decentralized development and supports further extensibility.


<details>
  <summary>Details</summary>
Motivation: Microservices, while popular for their scalability, CI/CD support, and decentralized development, can suffer from miscommunication and incompatible implementations due to independent teams and continual evolution. This threatens maintainability and reliability.

Method: The paper proposes a novel methodology that statically analyzes and reconstructs microservice source code into a formal system model. An SMT (Satisfiability Modulo Theories) constraint set is generated from this model, which is then used for formal verification of various cross-cutting concerns.

Result: The methodology is demonstrated as extensible, capable of supporting formal verification across multiple concerns, with an emphasis on system architecture. Formal reasoning is provided to validate its correctness for this concern, and applicability to security policy implementation is noted.

Conclusion: The proposed static reconstruction and verification approach can increase maintainability and reliability by systematically uncovering miscommunications and incompatibilities in microservice systems. The paper outlines future directions for expansion and evaluation of the methodology.

Abstract: Microservice systems are becoming increasingly adopted due to their
scalability, decentralized development, and support for continuous integration
and delivery (CI/CD). However, this decentralized development by separate teams
and continuous evolution can introduce miscommunication and incompatible
implementations, undermining system maintainability and reliability across
aspects from security policy to system architecture. We propose a novel
methodology that statically reconstructs microservice source code into a formal
system model. From this model, a Satisfiability Modulo Theories (SMT)
constraint set can be derived, enabling formal verification. Our methodology is
extensible, supporting software verification across multiple cross-cutting
concerns. We focus on applying the methodology to verify the system
architecture concern, presenting formal reasoning to validate the methodology's
correctness and applicability for this concern. Additional concerns such as
security policy implementation are considered. Future directions are
established to extend and evaluate the methodology.

</details>


### [2] [Are We SOLID Yet? An Empirical Study on Prompting LLMs to Detect Design Principle Violations](https://arxiv.org/abs/2509.03093)
*Fatih Pehlivan,Arçin Ülkü Ergüzen,Sahand Moslemi Yengejeh,Mayasah Lami,Anil Koyuncu*

Main category: cs.SE

TL;DR: This paper benchmarks multiple LLMs and prompt strategies for detecting SOLID design principle violations across several languages. Results show GPT-4o Mini leads but nobody model or prompt is best for all situations, and accuracy declines with code complexity. Tailoring model-prompt combinations to each context is crucial for effective, AI-driven code analysis and maintainability.


<details>
  <summary>Details</summary>
Motivation: Traditional static analysis tools can hardly detect semantic design flaws related to the SOLID principles, especially in multi-language codebases. Existing research typically focuses on single principles or languages, leaving a significant gap in broad, effective detection across all SOLID principles.

Method: The paper introduces a method that uses custom prompt engineering to evaluate large language models (LLMs) for their skill in detecting SOLID principle violations in code. The study benchmarks four LLMs (CodeLlama, DeepSeekCoder, QwenCoder, GPT-4o Mini) using a new dataset of 240 manually validated code examples. Four different prompt strategies—zero-shot, few-shot, chain-of-thought, and ensemble—are applied and their impact on detection performance is systematically analyzed.

Result: GPT-4o Mini outperforms other models in overall detection but still struggles with complex principles (e.g., DIP). No single prompt strategy works best for all cases; for instance, ENSEMBLE excels at OCP detection, while EXAMPLE prompts are better for DIP. Language characteristics and code complexity heavily affect detection accuracy, with performance dropping as complexity rises.

Conclusion: AI-driven code design analysis can't rely on a one-size-fits-all approach. Optimal detection demands careful pairing of model and prompt strategy to the specific coding context and principle. Tailored combinations offer the most potential for maintaining quality and supporting developers.

Abstract: Traditional static analysis methods struggle to detect semantic design flaws,
such as violations of the SOLID principles, which require a strong
understanding of object-oriented design patterns and principles. Existing
solutions typically focus on individual SOLID principles or specific
programming languages, leaving a gap in the ability to detect violations across
all five principles in multi-language codebases. This paper presents a new
approach: a methodology that leverages tailored prompt engineering to assess
LLMs on their ability to detect SOLID violations across multiple languages. We
present a benchmark of four leading LLMs-CodeLlama, DeepSeekCoder, QwenCoder,
and GPT-4o Mini-on their ability to detect violations of all five SOLID
principles. For this evaluation, we construct a new benchmark dataset of 240
manually validated code examples. Using this dataset, we test four distinct
prompt strategies inspired by established zero-shot, few-shot, and
chain-of-thought techniques to systematically measure their impact on detection
accuracy. Our emerging results reveal a stark hierarchy among models, with
GPT-4o Mini decisively outperforming others, yet even struggles with
challenging principles like DIP. Crucially, we show that prompt strategy has a
dramatic impact, but no single strategy is universally best; for instance, a
deliberative ENSEMBLE prompt excels at OCP detection while a hint-based EXAMPLE
prompt is superior for DIP violations. Across all experiments, detection
accuracy is heavily influenced by language characteristics and degrades sharply
with increasing code complexity. These initial findings demonstrate that
effective, AI-driven design analysis requires not a single best model, but a
tailored approach that matches the right model and prompt to the specific
design context, highlighting the potential of LLMs to support maintainability
through AI-assisted code analysis.

</details>


### [3] [AI Safety Assurance in Electric Vehicles: A Case Study on AI-Driven SOC Estimation](https://arxiv.org/abs/2509.03270)
*Martin Skoglund,Fredrik Warg,Aria Mirzai,Anders Thorsen,Karl Lundgren,Peter Folkesson,Bastian Havers-zulka*

Main category: cs.SE

TL;DR: The paper presents a method to independently assess the safety of AI components in electric vehicles by combining ISO 26262 and ISO/PAS 8800 standards, using robustness testing to evaluate resilience against faulty sensor inputs.


<details>
  <summary>Details</summary>
Motivation: AI technologies are increasingly being integrated into electric vehicles (EVs), but current safety standards like ISO 26262 are insufficient for evaluating AI-based functions, necessitating updated assessment practices.

Method: The paper proposes combining ISO 26262 with ISO/PAS 8800 for safety assessment of AI components in EVs. It uses the example of an AI-driven State of Charge (SOC) battery estimation and performs robustness testing via fault injection to test the system's resilience to sensor input perturbations.

Result: The study identifies key aspects needed for independent assessment under the combined standards and demonstrates how AI component robustness can be evaluated through systematic fault injection experiments.

Conclusion: Integrating traditional functional safety standards with new AI safety standards and employing robustness testing can enhance the independent safety assessment of AI components in EVs.

Abstract: Integrating Artificial Intelligence (AI) technology in electric vehicles (EV)
introduces unique challenges for safety assurance, particularly within the
framework of ISO 26262, which governs functional safety in the automotive
domain. Traditional assessment methodologies are not geared toward evaluating
AI-based functions and require evolving standards and practices. This paper
explores how an independent assessment of an AI component in an EV can be
achieved when combining ISO 26262 with the recently released ISO/PAS 8800,
whose scope is AI safety for road vehicles. The AI-driven State of Charge (SOC)
battery estimation exemplifies the process. Key features relevant to the
independent assessment of this extended evaluation approach are identified. As
part of the evaluation, robustness testing of the AI component is conducted
using fault injection experiments, wherein perturbed sensor inputs are
systematically introduced to assess the component's resilience to input
variance.

</details>


### [4] [VulnRepairEval: An Exploit-Based Evaluation Framework for Assessing Large Language Model Vulnerability Repair Capabilities](https://arxiv.org/abs/2509.03331)
*Weizhe Wang,Wei Ma,Qiang Hu,Yao Zhang,Jianfei Sun,Bin Wu,Yang Liu,Guangquan Xu,Lingxiao Jiang*

Main category: cs.SE

TL;DR: VulnRepairEval tests LLMs on real exploit-based vulnerability patching and finds that even the best models succeed on only ~22% of cases, exposing major weaknesses and the need for tougher, more realistic evaluation in security settings.


<details>
  <summary>Details</summary>
Motivation: Current evaluation sets for LLM-based automated software vulnerability patching are not reliable because they use superficial validation, not real exploit-based verification. This leads to overestimated effectiveness, which is dangerous for real-world security applications.

Method: The authors introduce VulnRepairEval, an evaluation framework that uses real Proof-of-Concept exploits to test patched vulnerabilities. They curated a dataset from over 400 CVEs, extracting 23 Python vulnerabilities with working PoCs. The framework benchmarks 12 popular LLMs in a reproducible, containerized pipeline, requiring that the exploit fails after a patch for it to be considered successful.

Result: Only 5 out of 23 vulnerabilities were successfully patched by the best-performing LLM (about 21.7%). Most failures came from poor vulnerability identification or syntactic/semantic errors in patches. Improved prompting and multi-agent methods didn’t significantly help.

Conclusion: The paper provides a robust evaluation framework that reflects real exploit conditions, revealing critical limitations in current LLMs for vulnerability patching. Stronger and more realistic assessment methods are urgently needed for security-focused AI applications.

Abstract: The adoption of Large Language Models (LLMs) for automated software
vulnerability patching has shown promising outcomes on carefully curated
evaluation sets. Nevertheless, existing datasets predominantly rely on
superficial validation methods rather than exploit-based verification, leading
to overestimated performance in security-sensitive applications. This paper
introduces VulnRepairEval, an evaluation framework anchored in functional
Proof-of-Concept (PoC) exploits. Our framework delivers a comprehensive,
containerized evaluation pipeline that enables reproducible differential
assessment, where repair success requires the original exploit to fail
execution against the modified code. The benchmark construction involved
extensive data curation: we processed over 400 CVEs and approximately 2,500
potential sources to extract a collection of authentic vulnerability instances
(23 Python CVEs) amenable to automated testing with working PoCs. Through
VulnRepairEval, we conduct a comprehensive evaluation of 12 popular LLMs and
observe a significant performance deficit: even the top-performing model
successfully addresses merely 5/23 instances (about 21.7%), exposing critical
weaknesses in security-focused applications. Our failure analysis reveals that
most unsuccessful attempts stem from imprecise vulnerability identification and
patches containing syntactic or semantic errors. Enhanced prompting strategies
and multi-agent approaches yield minimal improvements, with overall
effectiveness remaining largely unaffected. This work contributes a stringent,
practical evaluation framework for LLM-driven vulnerability remediation and
underscores the necessity for assessment protocols that authentically reflect
real-world exploitation scenarios.

</details>


### [5] [The Impact of Critique on LLM-Based Model Generation from Natural Language: The Case of Activity Diagrams](https://arxiv.org/abs/2509.03463)
*Parham Khamsepour,Mark Cole,Ish Ashraf,Sandeep Puri,Mehrdad Sabetzadeh,Shiva Nejati*

Main category: cs.SE

TL;DR: The paper presents LADEX, a system for extracting activity diagrams from text using a critique-refine loop between LLMs and algorithms. While LLMs alone often miss structural errors, adding algorithmic checks significantly boosts accuracy. The best results come from combining these approaches, achieving over 86% correctness and 88% completeness with few LLM calls.


<details>
  <summary>Details</summary>
Motivation: Automating the extraction of structured models (such as activity diagrams) from natural-language process descriptions remains challenging, especially regarding both structural correctness and semantic alignment. Existing LLM-based approaches often struggle with maintaining model quality and accurately reflecting intended meanings.

Method: The authors introduce LADEX, a pipeline that uses a generate-critique-refine loop powered by LLMs to derive activity diagrams from natural-language descriptions. Structural correctness is checked algorithmically or by the LLM, while semantic alignment relies on LLM-based checks. Five ablated versions of LADEX are designed to study the roles of critique-refine loops, LLM-based semantic checking, and the comparative advantage of algorithmic structural checks. Evaluation is performed via automated comparisons with expert-generated ground truths on two datasets.

Result: Experiments show that the critique-refine loop enhances structural validity, correctness, and completeness over single-pass approaches. Algorithmic structural checks outperform LLM-only checks by improving correctness by 17.81% and completeness by 13.24%. The combination of algorithmic structural and LLM-based semantic checks (O4 Mini) yields the highest performance: up to 86.37% correctness and 88.56% completeness, with under five LLM calls per instance on average.

Conclusion: Integrating algorithmic structural checks with LLM-based semantic checking in an iterative critique-refine loop significantly improves the quality and efficiency of model extraction from natural language. The LADEX pipeline, particularly when using a hybrid approach, offers strong performance in generating accurate and complete process models with minimal LLM resource consumption.

Abstract: Large Language Models (LLMs) show strong potential for automating the
generation of models from natural-language descriptions. A common approach is
an iterative generate-critique-refine loop, where candidate models are
produced, evaluated, and updated based on detected issues. This process needs
to address: (1) structural correctness - compliance with well-formedness rules
- and (2) semantic alignment - accurate reflection of the intended meaning in
the source text. We present LADEX (LLM-based Activity Diagram Extractor), a
pipeline for deriving activity diagrams from natural-language process
descriptions using an LLM-driven critique-refine process. Structural checks in
LADEX can be performed either algorithmically or by an LLM, while alignment
checks are always performed by an LLM. We design five ablated variants of LADEX
to study: (i) the impact of the critique-refine loop itself, (ii) the role of
LLM-based semantic checks, and (iii) the comparative effectiveness of
algorithmic versus LLM-based structural checks.
  To evaluate LADEX, we compare the generated activity diagrams with
expert-created ground truths using trace-based operational semantics. This
enables automated measurement of correctness and completeness. Experiments on
two datasets indicate that: (1) the critique-refine loop improves structural
validity, correctness, and completeness compared to single-pass generation; (2)
algorithmic structural checks eliminate inconsistencies that LLM-based checks
fail to detect, improving correctness by an average of 17.81% and completeness
by 13.24% over LLM-only checks; and (3) combining algorithmic structural checks
with LLM-based semantic checks, implemented using the reasoning-focused O4
Mini, achieves the best overall performance - yielding average correctness of
up to 86.37% and average completeness of up to 88.56% - while requiring fewer
than five LLM calls on average.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [6] [Semantically Reflected Programs](https://arxiv.org/abs/2509.03318)
*Eduard Kamburjan,Vidar Norstein Klungre,Yuanwei Qu,Rudolf Schlatte,Egor V. Kostylev,Martin Giese,Einar Broch Johnsen*

Main category: cs.PL

TL;DR: This paper introduces, formalizes, and demonstrates 'semantic lifting'—a technique to map program state into knowledge graphs, unifying structural and behavioral knowledge representation within programs. The approach is validated by a custom language, SMOL, and geological modeling case study, with open-source implementation provided.


<details>
  <summary>Details</summary>
Motivation: There exists a dichotomy between structural knowledge (represented by knowledge graphs and ontologies) and behavioral knowledge (captured by programming languages). The motivation is to bridge this gap, enabling seamless integration and utilization of both knowledge types within software systems.

Method: The paper introduces the concept of 'semantic lifting,' transforming program states in an object-oriented programming language into knowledge graphs. It provides a formalization of semantic lifting and semantic reflection in a custom language called SMOL. It then explains the operational behavior, considers type correctness and virtualization for runtime program queries, and demonstrates the approach through a geological modeling case study. The implementation is made available as open source.

Result: The authors successfully formalize semantic lifting and reflection in the SMOL language, demonstrate its feasibility in practice with a case study, and provide an open-source implementation. The integration allows programmers to query and leverage structural (domain) knowledge in behavioral (programming) contexts.

Conclusion: Semantic lifting provides a systematic way to bridge the dichotomy between structural and behavioral knowledge by combining the strengths of knowledge graphs and programming language semantics. This approach is practical, formally sound, and broadly applicable, as shown by a concrete case study and an open-source implementation.

Abstract: This paper addresses the dichotomy between the formalization of structural
and the formalization of behavioral knowledge by means of semantically lifted
programs, which explore an intuitive connection between programs and knowledge
graphs. While knowledge graphs and ontologies are eminently useful to represent
formal knowledge about a system's individuals and universals, programming
languages are designed to describe the system's evolution. To address this
dichotomy, we introduce a semantic lifting of the program states of an
executing program into a knowledge graph, for an object-oriented programming
language. The resulting graph is exposed as a semantic reflection layer within
the programming language, allowing programmers to leverage knowledge of the
application domain in their programs. In this paper, we formalize semantic
lifting and semantic reflection for a small programming language, SMOL, explain
the operational aspects of the language, and consider type correctness and
virtualisation for runtime program queries through the semantic reflection
layer. We illustrate semantic lifting and semantic reflection through a case
study of geological modelling and discuss different applications of the
technique. The language implementation is open source and available online.

</details>
