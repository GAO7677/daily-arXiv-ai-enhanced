{"id": "2508.10059", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10059", "abs": "https://arxiv.org/abs/2508.10059", "authors": ["Yueke Zhang", "Yifan Zhang", "Kevin Leach", "Yu Huang"], "title": "FormalGrad: Integrating Formal Methods with Gradient-Based LLM Refinement", "comment": "6 Pages", "summary": "While Large Language Models (LLMs) have demonstrated remarkable capabilities\nin code generation, they often produce solutions that lack guarantees of\ncorrectness, robustness, and efficiency. The limitation is acute in domains\nrequiring strict constraints. FormalGrad introduces a principled framework that\nintegrates formal methods directly into an iterative LLM-based generation loop.\nIt uniquely treats code as a differentiable variable, converting structured\nfeedback and formal constraints into a textual pseudo-gradient. This gradient\nguides the model to iteratively refine solutions, ensuring they are not only\nfunctional but also robust and formally justified. We evaluate FormalGrad on\nthe HumanEval, HumanEval+, and LiveCodeBench benchmarks. Our implementation\noutperforms strong baselines, achieving an absolute improvement of up to 27% on\nHumanEval and a 41% relative improvement on the challenging LiveCodeBench V6.\nFormalGrad generates formally justified code that is robust and efficient,\npaving the way for reliable AI-assisted software development in high-stakes\napplications.", "AI": {"tldr": "This paper presents FormalGrad, an approach that integrates formal methods with LLMs to iteratively generate code that is correct, robust, and efficient. Tested on standard benchmarks, FormalGrad significantly outperforms existing models, offering a pathway toward dependable AI-generated software for demanding applications.", "motivation": "Large Language Models (LLMs) excel in code generation but struggle to produce code that is guaranteed to be correct, robust, and efficient, particularly for applications where strict constraints are vital.", "method": "FormalGrad is a novel framework that incorporates formal methods directly into an iterative LLM-based code generation process. It treats code as a differentiable variable and converts structured feedback and formal constraints into a textual pseudo-gradient, guiding the model to iteratively refine and improve its solutions.", "result": "FormalGrad was evaluated on benchmarks including HumanEval, HumanEval+, and LiveCodeBench. The framework achieved up to 27% absolute improvement on HumanEval and a 41% relative improvement on LiveCodeBench V6 compared to strong baselines.", "conclusion": "FormalGrad enables LLMs to generate code that is not only functional, but also robust and formally justified, supporting the development of reliable AI-driven software for critical domains."}}
{"id": "2508.10068", "categories": ["cs.SE", "cs.CL", "cs.IR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.10068", "abs": "https://arxiv.org/abs/2508.10068", "authors": ["Xiaohan Chen", "Zhongying Pan", "Quan Feng", "Yu Tian", "Shuqun Yang", "Mengru Wang", "Lina Gong", "Yuxia Geng", "Piji Li", "Xiang Chen"], "title": "SaraCoder: Orchestrating Semantic and Structural Cues for Profit-Oriented Repository-Level Code Completion", "comment": null, "summary": "Retrieval-augmented generation (RAG) for repository-level code completion\ncommonly relies on superficial text similarity, leading to results plagued by\nsemantic misguidance, redundancy, and homogeneity, while also failing to\nresolve external symbol ambiguity. To address these challenges, we introduce\nSaracoder, a Hierarchical Feature-Optimized retrieval framework. Its core\nHierarchical Feature Optimization module systematically refines candidates by\ndistilling deep semantic relationships, pruning exact duplicates, assessing\nstructural similarity with a novel graph-based metric that weighs edits by\ntheir topological importance, and reranking results to maximize both relevance\nand diversity. Furthermore, an External-Aware Identifier Disambiguator module\naccurately resolves cross-file symbol ambiguity via dependency analysis.\nExtensive experiments on the challenging CrossCodeEval and RepoEval-Updated\nbenchmarks demonstrate that Saracoder significantly outperforms existing\nbaselines across multiple programming languages and models. Our work proves\nthat systematically refining retrieval results across multiple dimensions\nprovides a new paradigm for building more accurate and robust repository-level\ncode completion systems.", "AI": {"tldr": "Saracoder is a new code completion system that improves retrieval by optimizing semantic and structural relevance, removing duplicates, and resolving symbol ambiguity, leading to better performance over existing methods.", "motivation": "Existing retrieval-augmented generation (RAG) methods for repository-level code completion often rely heavily on text similarity, resulting in semantic misguidance, redundant or overly similar results, and an inability to resolve ambiguity in external symbols.", "method": "Saracoder introduces a Hierarchical Feature-Optimized retrieval framework composed of two main modules: (1) a Hierarchical Feature Optimization module that refines retrieval candidates by distilling semantic relations, removing duplicates, using a graph-based metric for structural similarity, and reranking for relevance and diversity; (2) an External-Aware Identifier Disambiguator that resolves cross-file symbol ambiguity through dependency analysis.", "result": "In experiments on benchmarks like CrossCodeEval and RepoEval-Updated, Saracoder outperformed state-of-the-art baselines in code completion tasks across several programming languages and models.", "conclusion": "By systematically refining retrieval results across multiple dimensions, Saracoder establishes a more accurate and robust framework for repository-level code completion, representing a paradigm shift from existing text-similarity-based approaches."}}
{"id": "2508.10074", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10074", "abs": "https://arxiv.org/abs/2508.10074", "authors": ["Ruofan Lu", "Yintong Huo", "Meng Zhang", "Yichen Li", "Michael R. Lyu"], "title": "Next Edit Prediction: Learning to Predict Code Edits from Context and Interaction History", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has led to the\nwidespread adoption of AI-powered coding assistants integrated into a\ndevelopment environment. On one hand, low-latency code completion offers\ncompletion suggestions but is fundamentally constrained to the cursor's current\nposition. On the other hand, chat-based editing can perform complex\nmodifications, yet forces developers to stop their work, describe the intent in\nnatural language, which causes a context-switch away from the code. This\ncreates a suboptimal user experience, as neither paradigm proactively predicts\nthe developer's next edit in a sequence of related edits. To bridge this gap\nand provide the seamless code edit suggestion, we introduce the task of Next\nEdit Prediction, a novel task designed to infer developer intent from recent\ninteraction history to predict both the location and content of the subsequent\nedit. Specifically, we curate a high-quality supervised fine-tuning dataset and\nan evaluation benchmark for the Next Edit Prediction task. Then, we conduct\nsupervised fine-tuning on a series of models and performed a comprehensive\nevaluation of both the fine-tuned models and other baseline models, yielding\nseveral novel findings. This work lays the foundation for a new interaction\nparadigm that proactively collaborate with developers by anticipating their\nfollowing action, rather than merely reacting to explicit instructions.", "AI": {"tldr": "The paper proposes 'Next Edit Prediction' to proactively suggest developer code edits, introduces a new dataset and benchmark, fine-tunes and evaluates models, and shows improved collaboration and user experience over existing techniques.", "motivation": "Current AI coding assistants either offer low-latency, context-limited code completion or chat-based interaction, each with significant usability drawbacks. Neither approach anticipates the developer's likely next edits, leading to a suboptimal user experience.", "method": "The authors introduce the 'Next Edit Prediction' task to infer and predict both the location and content of developers' next code edits, based on recent developer interaction history. They create a high-quality supervised dataset and evaluation benchmark for this task, and conduct supervised fine-tuning and comprehensive evaluation on various models against baseline approaches.", "result": "Several novel findings emerged from evaluating the fine-tuned and baseline models, demonstrating the feasibility and benefits of proactively predicting the next edit. The models showed promise in anticipating developer actions, rather than just responding to instructions.", "conclusion": "This work establishes a foundation for proactive AI assistants in code development, introducing a new interaction paradigm that anticipates developer actions and improves developer experience."}}
{"id": "2508.10157", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10157", "abs": "https://arxiv.org/abs/2508.10157", "authors": ["Ajibode Adekunle", "Abdul Ali Bangash", "Bram Adams", "Ahmed E. Hassan"], "title": "On the synchronization between Hugging Face pre-trained language models and their upstream GitHub repository", "comment": null, "summary": "Pretrained language models (PTLMs) have advanced natural language processing\n(NLP), enabling progress in tasks like text generation and translation. Like\nsoftware package management, PTLMs are trained using code and environment\nscripts in upstream repositories (e.g., GitHub, GH) and distributed as variants\nvia downstream platforms like Hugging Face (HF). Coordinating development\nbetween GH and HF poses challenges such as misaligned release timelines,\ninconsistent versioning, and limited reuse of PTLM variants. We conducted a\nmixed-method study of 325 PTLM families (904 HF variants) to examine how commit\nactivities are coordinated. Our analysis reveals that GH contributors typically\nmake changes related to specifying the version of the model, improving code\nquality, performance optimization, and dependency management within the\ntraining scripts, while HF contributors make changes related to improving model\ndescriptions, data set handling, and setup required for model inference.\nFurthermore, to understand the synchronization aspects of commit activities\nbetween GH and HF, we examined three dimensions of these activities -- lag\n(delay), type of synchronization, and intensity -- which together yielded eight\ndistinct synchronization patterns. The prevalence of partially synchronized\npatterns, such as Disperse synchronization and Sparse synchronization, reveals\nstructural disconnects in current cross-platform release practices. These\npatterns often result in isolated changes -- where improvements or fixes made\non one platform are never replicated on the other -- and in some cases,\nindicate an abandonment of one repository in favor of the other. Such\nfragmentation risks exposing end users to incomplete, outdated, or behaviorally\ninconsistent models. Hence, recognizing these synchronization patterns is\ncritical for improving oversight and traceability in PTLM release workflows.", "AI": {"tldr": "The study investigates how pretrained language models are maintained and updated across GitHub and Hugging Face, discovering significant coordination issues that often result in outdated or inconsistent models. Recognizing and addressing these cross-repository synchronization issues is crucial to ensuring better release practices and model reliability.", "motivation": "Despite the advancements of pretrained language models (PTLMs) and their positive impact on NLP tasks, challenges exist in managing development and releases between upstream (e.g., GitHub) and downstream (e.g., Hugging Face) repositories. Issues such as misaligned release timelines, inconsistent versioning, and poor reuse of PTLM variants arise, which may affect model quality and user experience.", "method": "The authors conducted a mixed-method study analyzing 325 PTLM families and 904 HF variants. They scrutinized commit activities on both GH and HF, categorizing changes and examining synchronization across three dimensions: delay (lag), type of synchronization, and intensity. This analysis identified eight distinct cross-repository synchronization patterns.", "result": "GH contributors tend to focus on versioning, code improvement, optimization, and dependencies, while HF contributors primarily refine model descriptions, data handling, and inference setup. The study identified prevalent partially synchronized patterns, such as Disperse and Sparse synchronization, indicating a structural disconnect in how updates are propagated between platforms. These patterns lead to isolated changes and sometimes abandonment of repositories, risking incomplete or inconsistent models for end users.", "conclusion": "Recognizing and understanding the synchronization patterns in PTLM release management is essential to enhance oversight, traceability, and model integrity across platforms. Addressing structural disconnects can help prevent inconsistencies and improve the reliability of distributed models."}}
{"id": "2508.10781", "categories": ["cs.PL", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.10781", "abs": "https://arxiv.org/abs/2508.10781", "authors": ["Abtin Molavi", "Amanda Xu", "Ethan Cecchetti", "Swamit Tannu", "Aws Albarghouthi"], "title": "Generating Compilers for Qubit Mapping and Routing", "comment": null, "summary": "Quantum computers promise to solve important problems faster than classical\ncomputers, potentially unlocking breakthroughs in materials science, chemistry,\nand beyond. Optimizing compilers are key to realizing this potential, as they\nminimize expensive resource usage and limit error rates. A critical compilation\nstep is qubit mapping and routing (QMR), which finds mappings from circuit\nqubits to qubits on a target device and plans instruction execution while\nsatisfying the device's connectivity constraints. The challenge is that the\nlandscape of quantum architectures is incredibly diverse and fast-evolving.\nGiven this diversity, hundreds of papers have addressed the QMR problem for\ndifferent qubit hardware, connectivity constraints, and quantum error\ncorrection schemes.\n  We present an approach for automatically generating qubit mapping and routing\ncompilers for arbitrary quantum architectures. Though each QMR problem is\ndifferent, we identify a common core structure-device state machine-that we use\nto formulate an abstract QMR problem. Our formulation naturally leads to a\ndomain-specific language, Marol, for specifying QMR problems-for example, the\nwell-studied NISQ mapping and routing problem requires only 12 lines of Marol.\nWe demonstrate that QMR problems, defined in Marol, can be solved with a\npowerful parametric solver that can be instantiated for any Marol program. We\nevaluate our approach through case studies of important QMR problems from prior\nand recent work, covering noisy and fault-tolerant quantum architectures on all\nmajor hardware platforms. Our thorough evaluation shows that generated\ncompilers are competitive with handwritten, specialized compilers in terms of\nruntime and solution quality. We envision that our approach will simplify\ndevelopment of future quantum compilers as new quantum architectures continue\nto emerge.", "AI": {"tldr": "This paper proposes a new method for automatically generating quantum qubit mapping and routing compilers using a unified device state machine model and the Marol DSL, demonstrating competitive performance across diverse quantum architectures.", "motivation": "Quantum computing has the potential to outperform classical computing on important problems, but this requires efficient compilation due to diverse and rapidly changing quantum hardware architectures. The QMR step is crucial and challenging, and hundreds of papers have addressed it separately for different architectures and error correction methods.", "method": "The paper introduces an abstract formulation for the QMR problem using a device state machine as the common core. It presents Marol, a domain-specific language designed to succinctly specify QMR problems. QMR problems described in Marol are solved using a parametric solver, capable of handling any Marol specification.", "result": "Case studies on a variety of quantum architectures show that compilers generated using this approach perform comparably to specialized, handwritten quantum compilers with respect to runtime and solution quality.", "conclusion": "Automatically generating QMR compilers using the proposed framework and Marol language can effectively handle current and emerging quantum architectures, streamlining quantum compiler development and maintaining high performance."}}
{"id": "2508.10517", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10517", "abs": "https://arxiv.org/abs/2508.10517", "authors": ["Likai Ye", "Mengliang Li", "Dehai Zhao", "Jiamou Sun", "Xiaoxue Ren"], "title": "Bridging Solidity Evolution Gaps: An LLM-Enhanced Approach for Smart Contract Compilation Error Resolution", "comment": "International Conference on Software Maintenance and Evolution\n  (ICSME) 2025", "summary": "Solidity, the dominant smart contract language for Ethereum, has rapidly\nevolved with frequent version updates to enhance security, functionality, and\ndeveloper experience. However, these continual changes introduce significant\nchallenges, particularly in compilation errors, code migration, and\nmaintenance. Therefore, we conduct an empirical study to investigate the\nchallenges in the Solidity version evolution and reveal that 81.68% of examined\ncontracts encounter errors when compiled across different versions, with 86.92%\nof compilation errors.\n  To mitigate these challenges, we conducted a systematic evaluation of large\nlanguage models (LLMs) for resolving Solidity compilation errors during version\nmigrations. Our empirical analysis across both open-source (LLaMA3, DeepSeek)\nand closed-source (GPT-4o, GPT-3.5-turbo) LLMs reveals that although these\nmodels exhibit error repair capabilities, their effectiveness diminishes\nsignificantly for semantic-level issues and shows strong dependency on prompt\nengineering strategies. This underscores the critical need for domain-specific\nadaptation in developing reliable LLM-based repair systems for smart contracts.\n  Building upon these insights, we introduce SMCFIXER, a novel framework that\nsystematically integrates expert knowledge retrieval with LLM-based repair\nmechanisms for Solidity compilation error resolution. The architecture\ncomprises three core phases: (1) context-aware code slicing that extracts\nrelevant error information; (2) expert knowledge retrieval from official\ndocumentation; and (3) iterative patch generation for Solidity migration.\nExperimental validation across Solidity version migrations demonstrates our\napproach's statistically significant 24.24% improvement over baseline GPT-4o on\nreal-world datasets, achieving near-perfect 96.97% accuracy.", "AI": {"tldr": "Solidity version changes break most smart contracts, often causing compilation errors. Popular LLMs help but need expert guidance. The proposed SMCFIXER framework, which integrates expert documentation with LLMs, drastically improves error repair accuracy over generic LLMs.", "motivation": "Solidity's frequent version updates cause significant challenges such as compilation errors, code migration difficulties, and maintenance overhead for Ethereum smart contracts.", "method": "The authors conducted an empirical study to measure the prevalence of errors in Solidity contracts across versions. They evaluated multiple LLMs (open-source and closed-source) on their capabilities to repair compilation errors, and designed SMCFIXER, a framework combining expert knowledge retrieval and LLM-based repair to automate error resolution.", "result": "They found that over 81% of contracts encounter errors across versions, with most errors due to compilation. LLMs can repair some errors, but struggle with semantic issues and rely heavily on carefully designed prompts. Their SMCFIXER system demonstrated a 24.24% accuracy improvement over GPT-4o, achieving 96.97% accuracy in migration tasks.", "conclusion": "Frequent version updates in Solidity are highly error-prone. Generic LLMs are insufficient for robust repair without domain adaptation and expertise. Combining expert knowledge with LLM capabilities (as in SMCFIXER) substantially boosts repair success for smart contract migrations."}}
{"id": "2508.10852", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10852", "abs": "https://arxiv.org/abs/2508.10852", "authors": ["Souhaila Serbout", "Diana Carolina Mu\u00f1oz Hurtado", "Hassan Atwi", "Edoardo Riggio", "Cesare Pautasso"], "title": "EVOSCAT: Exploring Software Change Dynamics in Large-Scale Historical Datasets", "comment": "Submitted to VISSOFT 2025. For the hi-resolution version of the\n  paper, see https://design.inf.usi.ch/publications/2025/vissoft", "summary": "Long lived software projects encompass a large number of artifacts, which\nundergo many revisions throughout their history. Empirical software engineering\nresearchers studying software evolution gather and collect datasets with\nmillions of events, representing changes introduced to specific artifacts. In\nthis paper, we propose EvoScat, a tool that attempts addressing temporal\nscalability through the usage of interactive density scatterplot to provide a\nglobal overview of large historical datasets mined from open source\nrepositories in a single visualization. EvoScat intents to provide researchers\nwith a mean to produce scalable visualizations that can help them explore and\ncharacterize evolution datasets, as well as comparing the histories of\nindividual artifacts, both in terms of 1) observing how rapidly different\nartifacts age over multiple-year-long time spans 2) how often metrics\nassociated with each artifacts tend towards an improvement or worsening. The\npaper shows how the tool can be tailored to specific analysis needs (pace of\nchange comparison, clone detection, freshness assessment) thanks to its support\nfor flexible configuration of history scaling and alignment along the time\naxis, artifacts sorting and interactive color mapping, enabling the analysis of\nmillions of events obtained by mining the histories of tens of thousands of\nsoftware artifacts. We include in this paper a gallery showcasing datasets\ngathering specific artifacts (OpenAPI descriptions, GitHub workflow\ndefinitions) across multiple repositories, as well as diving into the history\nof specific popular open source projects.", "AI": {"tldr": "EvoScat is a visualization tool for researchers to efficiently analyze and compare the evolution of thousands of software artifacts and millions of historical events from open source repositories, enabling global, scalable insights using interactive scatterplots.", "motivation": "Long-lived software projects produce vast amounts of change data across many artifacts, making evolutionary analysis and visualization challenging at scale. Researchers lack effective tools to globally analyze millions of events from software histories.", "method": "The paper proposes EvoScat, a visualization tool employing interactive density scatterplots to globally and scalably represent large historical datasets mined from open source repositories. EvoScat offers flexible history scaling, artifact sorting, interactive color mapping, and tailored configuration to support comparative and detailed analysis.", "result": "EvoScat enables exploration and characterization of evolution datasets, allowing comparison of artifact histories (e.g., rate of aging, metric improvement/worsening) across multi-year spans. It supports analyses such as pace of change, clone detection, and freshness assessment, handling millions of events from thousands of artifacts.", "conclusion": "EvoScat provides researchers with scalable, interactive visualizations for large-scale software evolution datasets, supporting varied analysis needs and enabling a global overview and comparison across projects and artifact types."}}
