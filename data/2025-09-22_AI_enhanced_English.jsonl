{"id": "2509.15834", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.15834", "abs": "https://arxiv.org/abs/2509.15834", "authors": ["Shardul Chiplunkar", "Cl\u00e9ment Pit-Claudel"], "title": "Automatic layout of railroad diagrams", "comment": "24 pages (+2 appendix, +3 references); 22 figures (+4 appendix); 3\n  tables", "summary": "Railroad diagrams (also called \"syntax diagrams\") are a common, intuitive\nvisualization of grammars, but limited tooling and a lack of formal attention\nto their layout mostly confines them to hand-drawn documentation. We present\nthe first formal treatment of railroad diagram layout along with a principled,\npractical implementation. We characterize the problem as compiling a *diagram\nlanguage* (specifying conceptual components and how they connect and compose)\nto a *layout language* (specifying basic graphical shapes and their sizes and\npositions). We then implement a compiler that performs *line wrapping* to meet\na target width, as well as vertical *alignment* and horizontal *justification*\nper user-specified policies. We frame line wrapping as an optimization problem,\nwhere we describe principled dimensions of optimality and implement\ncorresponding heuristics. For front-end evaluation, we show that our diagram\nlanguage is well-suited for common applications by describing how regular\nexpressions and Backus-Naur form can be compiled to it. For back-end\nevaluation, we argue that our compiler is practical by comparing its output to\ndiagrams laid out by hand and by other tools.", "AI": {"tldr": "This paper introduces a formal layout model and compiler for railroad diagrams, enabling automatic, high-quality visualization of grammars and making these diagrams more practical and standardized than traditional, hand-drawn versions.", "motivation": "Railroad diagrams are an intuitive way to visualize grammars, but currently, they are mainly hand-drawn due to insufficient tooling and lack of formal study on their layout. This limits their practicality and broader adoption.", "method": "The paper provides the first formal framework for the layout of railroad diagrams. It conceptualizes the design process as a compilation from a high-level diagram language to a low-level layout language. The authors built a compiler that introduces features like line wrapping (to fit a target width), alignment, and justification, guided by user preferences. Line wrapping is treated as an optimization problem, with defined optimality criteria and implemented heuristics. The approach is evaluated on both the front-end (showing suitability for describing regular expressions and Backus-Naur form) and the back-end (comparing generated diagrams to manually created and tool-generated diagrams).", "result": "The results show that the proposed diagram language effectively models typical use cases, such as regular expressions and Backus-Naur form. The compiler creates layouts comparable in quality to those produced by hand and alternative tools, demonstrating practicality and effectiveness.", "conclusion": "The paper establishes a formal foundation for railroad diagram layout and provides a practical implementation, advancing the state of tooling and usability for grammar visualization."}}
{"id": "2509.15283", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.PL", "I.2.7; F.2.2; I.2.2"], "pdf": "https://arxiv.org/pdf/2509.15283", "abs": "https://arxiv.org/abs/2509.15283", "authors": ["Kadin Matotek", "Heather Cassel", "Md Amiruzzaman", "Linh B. Ngo"], "title": "Evaluating the Limitations of Local LLMs in Solving Complex Programming Challenges", "comment": "Comments: 16 pages, 3 figures, 8 tables, accepted to CCSC Eastern\n  2025", "summary": "This study examines the performance of today's open-source, locally hosted\nlarge-language models (LLMs) in handling complex competitive programming tasks\nwith extended problem descriptions and contexts. Building on the original\nFramework for AI-driven Code Generation Evaluation (FACE), the authors retrofit\nthe pipeline to work entirely offline through the Ollama runtime, collapsing\nFACE's sprawling per-problem directory tree into a handful of consolidated JSON\nfiles, and adding robust checkpointing so multi-day runs can resume after\nfailures. The enhanced framework generates, submits, and records solutions for\nthe full Kattis corpus of 3,589 problems across eight code-oriented models\nranging from 6.7-9 billion parameters. The submission results show that the\noverall pass@1 accuracy is modest for the local models, with the best models\nperforming at approximately half the acceptance rate of the proprietary models,\nGemini 1.5 and ChatGPT-4. These findings expose a persistent gap between\nprivate, cost-controlled LLM deployments and state-of-the-art proprietary\nservices, yet also highlight the rapid progress of open models and the\npractical benefits of an evaluation workflow that organizations can replicate\non in-house hardware.", "AI": {"tldr": "The paper presents an improved, offline benchmarking framework for code-generation LLMs and finds that locally hosted open-source models currently lag well behind proprietary solutions but are improving, with practical evaluation now more accessible on in-house hardware.", "motivation": "To investigate if modern open-source, locally hosted LLMs are capable of solving complex competitive programming tasks when compared to proprietary models, especially under realistic problem settings and with a more robust, offline evaluation pipeline.", "method": "The authors retrofit the FACE code-generation evaluation pipeline to operate entirely offline, simplifying data management and introducing checkpointing for stability. They apply this system to evaluate eight open-source LLMs (6.7-9B parameters) by generating and submitting solutions to 3,589 programming problems from Kattis, comparing results with proprietary models like Gemini 1.5 and ChatGPT-4.", "result": "Open-source local models achieve only about half the pass@1 accuracy of proprietary models on complex programming challenges. However, the results highlight the steady progress of open models and demonstrate the feasibility of robust, fully offline benchmarking on local infrastructure.", "conclusion": "There remains a significant performance gap between local open-source LLMs and leading proprietary ones in solving advanced programming problems. Nevertheless, the improved evaluation framework enables reproducible, practical benchmarking on private hardware\u2014supporting further research and real-world adoption."}}
{"id": "2509.15397", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15397", "abs": "https://arxiv.org/abs/2509.15397", "authors": ["Simantika Bhattacharjee Dristi", "Matthew B. Dwyer"], "title": "LoCaL: Countering Surface Bias in Code Evaluation Metrics", "comment": null, "summary": "With the increasing popularity of large language models (LLMs) and LLM-based\nagents, reliable and effective code evaluation metrics (CEMs) have become\ncrucial for progress across several software engineering tasks. While popular\nbenchmarks often provide test cases to assess the correctness of generated\ncode, crafting and executing test cases is expensive. Reference-based CEMs\nprovide a cheaper alternative by scoring a candidate program based on its\nfunctional similarity to a reference. Although prior research has focused on\nreporting the weak correlation between these CEMs and functional correctness,\nthe causes are only assumed, and plausible solutions remain unexplored. In this\nwork, we critically evaluate four state-of-the-art reference-based CEMs,\nrevealing their strong bias towards surface-level features rather than code\nfunctionality. Despite this surface bias, current evaluation datasets for these\nCEMs rarely include code pairs that are surface-similar yet functionally\ndissimilar, or functionally similar yet surface-dissimilar. To mitigate this\ngap, we propose LoCaL (Looks Can Lie), a CEM evaluation benchmark, with 3117\ncode pairs at both the method and program levels. Each pair is labeled with a\nfunctional similarity score and aims to target regions where CEMs are likely to\nperform poorly. The functional similarity scores are calculated through\ndifferential fuzzing, which eliminates the need for predefined test cases and,\nat the same time, improves the reliability of the scores by executing an order\nof magnitude more tests than prior work. We find that all four CEMs show\nsignificant performance degradation on LoCaL, compared to the baselines.\nFinally, based on our findings, we draw the implication that exposing CEMs to\nLoCaL-like data might facilitate the development of metrics that are robust to\nsurface bias.", "AI": {"tldr": "Current code evaluation metrics for LLMs are biased toward superficial code similarity and struggle to gauge true functional equivalence. The LoCaL benchmark exposes these weaknesses, showing that all evaluated metrics perform poorly when surface and functional similarities diverge. Using such challenging datasets can guide the creation of more reliable metrics focused on functionality, not just appearance.", "motivation": "Current code evaluation metrics (CEMs) used for assessing code generated by large language models are often biased towards surface-level similarity rather than true functional similarity. Additionally, existing benchmarks do not adequately challenge CEMs with functionally distinct but surface-similar code, leaving their weaknesses untested and unresolved.", "method": "The authors critically evaluated four state-of-the-art reference-based CEMs by designing a new benchmark, LoCaL (Looks Can Lie), containing 3117 code pairs at both the method and program levels. Each code pair in the dataset received a functional similarity score, determined with differential fuzzing rather than conventional test cases (which are expensive to create), allowing for more comprehensive and reliable testing.", "result": "All four state-of-the-art CEMs exhibited significant performance declines on the LoCaL benchmark, indicating their strong surface bias and lack of robustness when distinguishing between functionally similar and dissimilar code pairs, especially when surface features are misleading.", "conclusion": "The study demonstrates that current reference-based CEMs are not reliable indicators of functional similarity. The authors suggest that exposure to LoCaL-like data, which includes challenging code pairs, may help in the development of more robust and functionally aware code evaluation metrics."}}
{"id": "2509.15567", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15567", "abs": "https://arxiv.org/abs/2509.15567", "authors": ["Hongyu Kuang", "Ning Zhang", "Hui Gao", "Xin Zhou", "Wesley K. G. Assun\u00e7\u00e3o", "Xiaoxing Ma", "Dong Shao", "Guoping Rong", "He Zhang"], "title": "Brevity is the Soul of Wit: Condensing Code Changes to Improve Commit Message Generation", "comment": null, "summary": "Commit messages are valuable resources for describing why code changes are\ncommitted to repositories in version control systems (e.g., Git). They\neffectively help developers understand code changes and better perform software\nmaintenance tasks. Unfortunately, developers often neglect to write\nhigh-quality commit messages in practice. Therefore, a growing body of work is\nproposed to generate commit messages automatically. These works all\ndemonstrated that how to organize and represent code changes is vital in\ngenerating good commit messages, including the use of fine-grained graphs or\nembeddings to better represent code changes. In this study, we choose an\nalternative way to condense code changes before generation, i.e., proposing\nbrief yet concise text templates consisting of the following three parts: (1)\nsummarized code changes, (2) elicited comments, and (3) emphasized code\nidentifiers. Specifically, we first condense code changes by using our proposed\ntemplates with the help of a heuristic-based tool named ChangeScribe, and then\nfine-tune CodeLlama-7B on the pairs of our proposed templates and corresponding\ncommit messages. Our proposed templates better utilize pre-trained language\nmodels, while being naturally brief and readable to complement generated commit\nmessages for developers. Our evaluation based on a widely used dataset showed\nthat our approach can outperform six baselines in terms of BLEU-Norm, METEOR,\nand ROUGE-L, with average improvements of 51.7%, 78.7%, and 62.5%,\nrespectively. The ablation study and human evaluation also provide further\ninsights into the effectiveness of our approach.", "AI": {"tldr": "The paper introduces a template-based method for generating commit messages using CodeLlama-7B, outperforming current state-of-the-art methods in both automated metrics and human evaluation.", "motivation": "Developers often fail to write high-quality commit messages, making it hard for others to understand code changes and maintain software. Current automatic generation methods focus on code representation but overlook the potential of concise templates.", "method": "The authors propose text templates to structure code changes into three parts: summarized code changes, elicited comments, and emphasized code identifiers. They use a heuristic tool, ChangeScribe, to create template pairs with commit messages and fine-tune CodeLlama-7B on these pairs.", "result": "Their approach, leveraging these structured templates, significantly outperforms six baseline models in BLEU-Norm, METEOR, and ROUGE-L by 51.7%, 78.7%, and 62.5% on average, respectively. Additional ablation studies and human evaluation confirm the method's effectiveness.", "conclusion": "The proposed template-based approach for condensing code changes enhances automated commit message generation. It results in more readable and informative messages, showing notable improvements over existing baselines and strong empirical performance."}}
{"id": "2509.15777", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15777", "abs": "https://arxiv.org/abs/2509.15777", "authors": ["Haoran Xu", "Zhi Chen", "Junxiao Han", "Xinkui Zhao", "Jianwei Yin", "Shuiguang Deng"], "title": "How Far Are We? An Empirical Analysis of Current Vulnerability Localization Approaches", "comment": null, "summary": "Open-source software vulnerability patch detection is a critical component\nfor maintaining software security and ensuring software supply chain integrity.\nTraditional manual detection methods face significant scalability challenges\nwhen processing large volumes of commit histories, while being prone to human\nerrors and omissions. Existing automated approaches, including heuristic-based\nmethods and pre-trained model solutions, suffer from limited accuracy, poor\ngeneralization capabilities, and inherent methodological constraints that\nhinder their practical deployment. To address these fundamental challenges,\nthis paper conducts a comprehensive empirical study of existing vulnerability\npatch detection methods, revealing four key insights that guide the design of\neffective solutions: the critical impact of search space reduction, the\nsuperiority of pre-trained semantic understanding over architectural\ncomplexity, the temporal limitations of web crawling approaches, and the\nadvantages of knowledge-driven methods. Based on these insights, we propose a\nnovel two-stage framework that combines version-driven candidate filtering with\nlarge language model-based multi-round dialogue voting to achieve accurate and\nefficient vulnerability patch identification. Extensive experiments on a\ndataset containing 750 real vulnerabilities demonstrate that our method\noutperforms current approaches.", "AI": {"tldr": "Traditional vulnerability patch detection methods do not scale and are inaccurate. This paper introduces a two-stage framework using version filtering and language models, notably improving patch detection in extensive experiments.", "motivation": "Manual detection of vulnerability patches in open-source software is inefficient, error-prone, and cannot scale to large codebases. Existing automated methods have poor accuracy and generalization, making them impractical for real-world use.", "method": "The paper presents a comprehensive empirical study on current vulnerability patch detection methods, identifying key challenges and success factors. It then proposes a novel two-stage framework: (1) version-driven candidate filtering to reduce the search space, and (2) large language model-based multi-round dialogue voting to select accurate patches.", "result": "Experiments conducted on a dataset of 750 real vulnerabilities show that the proposed method significantly improves detection accuracy and efficiency over existing approaches.", "conclusion": "Leveraging search space reduction and large language model-based dialogue yields a more effective and scalable solution for vulnerability patch detection in open-source software."}}
{"id": "2509.15893", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15893", "abs": "https://arxiv.org/abs/2509.15893", "authors": ["Andrea Bombarda", "Federico Conti", "Marcello Minervini", "Aurora Zanenga", "Claudio Menghi"], "title": "Failure Modes and Effects Analysis: An Experience from the E-Bike Domain", "comment": "12 pages", "summary": "Software failures can have catastrophic and costly consequences. Functional\nFailure Mode and Effects Analysis (FMEA) is a standard technique used within\nCyber-Physical Systems (CPS) to identify software failures and assess their\nconsequences. Simulation-driven approaches have recently been shown to be\neffective in supporting FMEA. However, industries need evidence of the\neffectiveness of these approaches to increase practical adoption. This\nindustrial paper presents our experience with using FMEA to analyze the safety\nof a CPS from the e-Bike domain. We used Simulink Fault Analyzer, an industrial\ntool that supports engineers with FMEA. We identified 13 realistic faults,\nmodeled them, and analyzed their effects. We sought expert feedback to analyze\nthe appropriateness of our models and the effectiveness of the faults in\ndetecting safety breaches. Our results reveal that for the faults we\nidentified, our models were accurate or contained minor imprecision that we\nsubsequently corrected. They also confirm that FMEA helps engineers improve\ntheir models. Specifically, the output provided by the simulation-driven\nsupport for 38.4% (5 out of 13) of the faults did not match the engineers'\nexpectations, helping them discover unexpected effects of the faults. We\npresent a thorough discussion of our results and ten lessons learned. Our\nfindings are useful for software engineers who work as Simulink engineers, use\nthe Simulink Fault Analyzer, or work as safety analysts.", "AI": {"tldr": "This paper evaluates simulation-driven FMEA for safety analysis of e-Bike CPS using Simulink Fault Analyzer. Modeling 13 faults and incorporating expert feedback showed that simulation helped uncover unexpected fault effects and improved model quality, suggesting strong industrial relevance for similar tools and approaches.", "motivation": "Software failures in Cyber-Physical Systems (CPS) can lead to severe consequences, necessitating effective methods to identify and analyze such failures. The motivation is to provide practical evidence of the value of simulation-driven Functional Failure Mode and Effects Analysis (FMEA) for industrial adoption, specifically within the e-Bike domain using commercial tools.", "method": "The authors performed FMEA on an industrial CPS (e-Bike domain) using Simulink Fault Analyzer. They identified and modeled 13 realistic faults in the system, then analyzed their effects through simulation. Feedback from domain experts was sought to evaluate model accuracy and effectiveness in detecting safety issues.", "result": "The study found that the fault models were mostly accurate, with only minor corrections required after expert feedback. In 38.4% of the cases, simulation-driven FMEA revealed fault effects that did not match engineers\u2019 expectations, thus highlighting unexpected safety issues. The process helped improve model quality and provided valuable lessons for practitioners.", "conclusion": "Simulation-driven support for FMEA, using tools like Simulink Fault Analyzer, is beneficial for safety analysis in CPS. It improves model accuracy, helps engineers identify unexpected faults, and increases confidence in safety assessments. The paper delivers actionable lessons for engineers and safety analysts working with similar approaches."}}
{"id": "2509.15971", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.15971", "abs": "https://arxiv.org/abs/2509.15971", "authors": ["Owen Truong", "Terrence Zhang", "Arnav Marchareddy", "Ryan Lee", "Jeffery Busold", "Michael Socas", "Eman Abdullah AlOmar"], "title": "LeakageDetector 2.0: Analyzing Data Leakage in Jupyter-Driven Machine Learning Pipelines", "comment": null, "summary": "In software development environments, code quality is crucial. This study\naims to assist Machine Learning (ML) engineers in enhancing their code by\nidentifying and correcting Data Leakage issues within their models. Data\nLeakage occurs when information from the test dataset is inadvertently included\nin the training data when preparing a data science model, resulting in\nmisleading performance evaluations. ML developers must carefully separate their\ndata into training, evaluation, and test sets to avoid introducing Data Leakage\ninto their code. In this paper, we develop a new Visual Studio Code (VS Code)\nextension, called LeakageDetector, that detects Data Leakage, mainly Overlap,\nPreprocessing and Multi-test leakage, from Jupyter Notebook files. Beyond\ndetection, we included two correction mechanisms: a conventional approach,\nknown as a quick fix, which manually fixes the leakage, and an LLM-driven\napproach that guides ML developers toward best practices for building ML\npipelines.", "AI": {"tldr": "The paper introduces LeakageDetector, a VS Code extension that automatically spots and helps fix data leakage issues in ML Jupyter Notebook code, using both manual and AI-guided methods to improve code quality and reliability.", "motivation": "Data Leakage is a common and serious issue in machine learning model development, leading to inaccurate performance evaluations. Many ML engineers may unknowingly introduce leakage due to improper data separation, especially while using interactive tools like Jupyter Notebooks.", "method": "This paper develops a Visual Studio Code extension named LeakageDetector, capable of detecting various types of Data Leakage (Overlap, Preprocessing, and Multi-test) in Jupyter Notebook files. The extension offers two correction mechanisms: a quick fix that manually repairs detected leakage, and an LLM-guided approach that advises ML developers on best practices.", "result": "The authors built and integrated LeakageDetector into the VS Code environment, enabling automatic detection and correction (either manual or LLM-assisted) of data leakage issues in ML code within Jupyter Notebooks.", "conclusion": "LeakageDetector supports ML engineers by detecting and helping correct Data Leakage in their code, thus promoting more reliable evaluation and development of machine learning models."}}
{"id": "2509.16081", "categories": ["cs.SE", "cs.MS", "G.1.3; D.2.11"], "pdf": "https://arxiv.org/pdf/2509.16081", "abs": "https://arxiv.org/abs/2509.16081", "authors": ["Marcel Koch", "Tobias Ribizel", "Pratik Nayak", "Fritz G\u00f6bel", "Gregor Olenik", "Terry Cojean"], "title": "Software Development Aspects of Integrating Linear Algebra Libraries", "comment": "16 pages, 2 figures", "summary": "Many scientific discoveries are made through, or aided by, the use of\nsimulation software. These sophisticated software applications are not built\nfrom the ground up, instead they rely on smaller parts for specific use cases,\nusually from domains unfamiliar to the application scientists. The software\nlibrary Ginkgo is one of these building blocks to handle sparse numerical\nlinear algebra on different platforms. By using Ginkgo, applications are able\nto ease the transition to modern systems, and speed up their simulations\nthrough faster numerical linear algebra routines. This paper discusses the\nchallenges and benefits for application software in adopting Ginkgo. It will\npresent examples from different domains, such as CFD, power grid simulation, as\nwell as electro-cardiophysiology. For these cases, the impact of the\nintegrations on the application code is discussed from a software engineering\nstandpoint, and in particular, the approaches taken by Ginkgo and the\napplications to enable sustainable software development are highlighted.", "AI": {"tldr": "The paper examines the integration of the Ginkgo numerical linear algebra library into various simulation applications, highlighting its performance benefits, software engineering challenges, and its support for sustainable development practices.", "motivation": "Application scientists often rely on simulation software that integrates external libraries for specific functionalities, such as numerical linear algebra, but these libraries are frequently from domains outside their expertise. This creates challenges in software integration, performance, and sustainability.", "method": "The paper analyzes the adoption of the Ginkgo library\u2014a platform-independent sparse numerical linear algebra package\u2014across various application domains. It presents real-world examples in CFD, power grid simulation, and electro-cardiophysiology. The analysis addresses software engineering implications and software integration strategies.", "result": "The integration of Ginkgo into scientific applications allowed for faster numerical computations and easier adaptation to modern computing systems. The analysis revealed both challenges and benefits, particularly with respect to sustainable software development and the engineering approaches used by both Ginkgo and the application developers.", "conclusion": "Ginkgo serves as a valuable tool for application software, improving performance and maintainability by providing efficient linear algebra routines. The software\u2019s engineering practices and integration strategies support sustainable development across different scientific domains."}}
{"id": "2509.16140", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16140", "abs": "https://arxiv.org/abs/2509.16140", "authors": ["Avinash Patil"], "title": "When Bugs Linger: A Study of Anomalous Resolution Time Outliers and Their Themes", "comment": "7 pages, 2 tables, 21 figures", "summary": "Efficient bug resolution is critical for maintaining software quality and\nuser satisfaction. However, specific bug reports experience unusually long\nresolution times, which may indicate underlying process inefficiencies or\ncomplex issues. This study presents a comprehensive analysis of bug resolution\nanomalies across seven prominent open-source repositories: Cassandra, Firefox,\nHadoop, HBase, SeaMonkey, Spark, and Thunderbird. Utilizing statistical methods\nsuch as Z-score and Interquartile Range (IQR), we identify anomalies in bug\nresolution durations. To understand the thematic nature of these anomalies, we\napply Term Frequency-Inverse Document Frequency (TF-IDF) for textual feature\nextraction and KMeans clustering to group similar bug summaries. Our findings\nreveal consistent patterns across projects, with anomalies often clustering\naround test failures, enhancement requests, and user interface issues. This\napproach provides actionable insights for project maintainers to prioritize and\neffectively address long-standing bugs.", "AI": {"tldr": "The paper analyzes bug resolution anomalies in major open-source projects using statistical and text analysis techniques, revealing that long-standing bugs often cluster around recurring themes like test failures and UI issues, helping maintainers target common underlying problems.", "motivation": "The motivation is to help software projects maintain quality and user satisfaction by understanding why certain bug reports take longer to resolve, potentially due to process inefficiencies or complex underlying problems.", "method": "The research uses statistical methods (Z-score and IQR) to detect anomalies in bug resolution durations. It then applies TF-IDF for feature extraction from bug report text, followed by KMeans clustering to group bug reports with similar themes.", "result": "The results show that anomalies in bug resolution times consistently appear across different projects and are thematically grouped, making persistent problems easier to identify and prioritize.", "conclusion": "The study concludes that anomalies in bug resolution times are commonly linked to specific recurring themes like test failures, enhancement requests, and UI issues. Identifying and analyzing these patterns offers actionable advice for maintainers to efficiently target and resolve persistent bugs."}}
{"id": "2509.16187", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16187", "abs": "https://arxiv.org/abs/2509.16187", "authors": ["Ali Reza Ibrahimzada", "Brandon Paulsen", "Reyhaneh Jabbarvand", "Joey Dodds", "Daniel Kroening"], "title": "MatchFixAgent: Language-Agnostic Autonomous Repository-Level Code Translation Validation and Repair", "comment": null, "summary": "Code translation transforms source code from one programming language (PL) to\nanother. Validating the functional equivalence of translation and repairing, if\nnecessary, are critical steps in code translation. Existing automated\nvalidation and repair approaches struggle to generalize to many PLs due to high\nengineering overhead, and they rely on existing and often inadequate test\nsuites, which results in false claims of equivalence and ineffective\ntranslation repair. We develop MatchFixAgent, a large language model\n(LLM)-based, PL-agnostic framework for equivalence validation and repair of\ntranslations. MatchFixAgent features a multi-agent architecture that divides\nequivalence validation into several sub-tasks to ensure thorough and consistent\nsemantic analysis of the translation. Then it feeds this analysis to test agent\nto write and execute tests. Upon observing a test failure, the repair agent\nattempts to fix the translation bug. The final (in)equivalence decision is made\nby the verdict agent, considering semantic analyses and test execution results.\n  We compare MatchFixAgent's validation and repair results with four\nrepository-level code translation techniques. We use 2,219 translation pairs\nfrom their artifacts, which cover 6 PL pairs, and are collected from 24 GitHub\nprojects totaling over 900K lines of code. Our results demonstrate that\nMatchFixAgent produces (in)equivalence verdicts for 99.2% of translation pairs,\nwith the same equivalence validation result as prior work on 72.8% of them.\nWhen MatchFixAgent's result disagrees with prior work, we find that 60.7% of\nthe time MatchFixAgent's result is actually correct. In addition, we show that\nMatchFixAgent can repair 50.6% of inequivalent translation, compared to prior\nwork's 18.5%. This demonstrates that MatchFixAgent is far more adaptable to\nmany PL pairs than prior work, while producing highly accurate validation\nresults.", "AI": {"tldr": "MatchFixAgent uses a language-model-powered, multi-agent system to thoroughly validate and repair code translation between different programming languages, achieving higher accuracy and broader applicability than past solutions.", "motivation": "Code translation between programming languages requires reliable validation of functional equivalence and effective repair of incorrect translations, but current automated approaches are difficult to generalize and rely on inadequate test suites.", "method": "The authors introduce MatchFixAgent, a programming language-agnostic framework based on large language models with a multi-agent architecture. The system divides equivalence validation into sub-tasks, generates and executes tests, repairs failed translations, and delivers a final verdict using collaborative agents.", "result": "MatchFixAgent achieved (in)equivalence verdicts for 99.2% of translation pairs, matched previous validation results in 72.8% of cases, and when differing, was found correct 60.7% of the time. It repaired 50.6% of inequivalent translations, outperforming the prior approach's 18.5%.", "conclusion": "MatchFixAgent is highly adaptable across many programming language pairs, offering superior and accurate functional equivalence validation and repair outcomes compared to previous methods."}}
