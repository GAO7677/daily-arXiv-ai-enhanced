{"id": "2509.00140", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.00140", "abs": "https://arxiv.org/abs/2509.00140", "authors": ["Songhui Yue"], "title": "LLM-based Triplet Extraction for Automated Ontology Generation in Software Engineering Standards", "comment": null, "summary": "Ontologies have supported knowledge representation and whitebox reasoning for\ndecades; thus, the automated ontology generation (AOG) plays a crucial role in\nscaling their use. Software engineering standards (SES) consist of long,\nunstructured text (with high noise) and paragraphs with domain-specific terms.\nIn this setting, relation triple extraction (RTE), together with term\nextraction, constitutes the first stage toward AOG. This work proposes an\nopen-source large language model (LLM)-assisted approach to RTE for SES.\nInstead of solely relying on prompt-engineering-based methods, this study\npromotes the use of LLMs as an aid in constructing ontologies and explores an\neffective AOG workflow that includes document segmentation, candidate term\nmining, LLM-based relation inference, term normalization, and cross-section\nalignment. Golden-standard benchmarks at three granularities are constructed\nand used to evaluate the ontology generated from the study. The results show\nthat it is comparable and potentially superior to the OpenIE method of triple\nextraction.", "AI": {"tldr": "This paper presents an LLM-assisted method for extracting relationships and building ontologies from messy, domain-specific texts in software engineering standards. Its comprehensive workflow outperforms traditional extraction tools and sets a new standard for automated ontology generation in such contexts.", "motivation": "Automated ontology generation (AOG) is essential for efficiently using ontologies, especially in domains with unstructured and noise-heavy text like software engineering standards (SES). However, extracting meaningful relationships from such complex documents is challenging.", "method": "The paper proposes an open-source, large language model (LLM)-assisted approach for relation triple extraction (RTE) from SES documents. The workflow includes document segmentation, candidate term mining, LLM-based relation inference, term normalization, and cross-section alignment. Benchmarks at three granularities are constructed to evaluate performance.", "result": "The LLM-assisted ontology generation approach provides results that are comparable to, and potentially superior to, the widely-used OpenIE triple extraction method.", "conclusion": "Using LLMs as aids instead of relying solely on prompt engineering enhances the AOG process for SES, improving relation extraction and ontology quality. The multi-stage workflow developed in this study proves effective and benchmarks validate its superiority over existing methods."}}
{"id": "2509.00256", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.00256", "abs": "https://arxiv.org/abs/2509.00256", "authors": ["Yutong Wang", "Cindy Rubio-Gonz\u00e1lez"], "title": "LLM-Based Program Generation for Triggering Numerical Inconsistencies Across Compilers", "comment": null, "summary": "Floating-point inconsistencies across compilers can undermine the reliability\nof numerical software. We present LLM4FP, the first framework that uses Large\nLanguage Models (LLMs) to generate floating-point programs specifically\ndesigned to trigger such inconsistencies. LLM4FP combines Grammar-Based\nGeneration and Feedback-Based Mutation to produce diverse and valid programs.\nWe evaluate LLM4FP across multiple compilers and optimization levels, measuring\ninconsistency rate, time cost, and program diversity. LLM4FP detects over twice\nas many inconsistencies compared to the state-of-the-art tool, Varity. Notably,\nmost of the inconsistencies involve real-valued differences, rather than\nextreme values like NaN or infinities. LLM4FP also uncovers inconsistencies\nacross a wider range of optimization levels, and finds the most mismatches\nbetween host and device compilers. These results show that LLM-guided program\ngeneration improves the detection of numerical inconsistencies.", "AI": {"tldr": "LLM4FP uses LLMs to automatically generate programs that reveal floating-point inconsistencies across compilers, outperforming prior tools and improving detection for subtle and broad cases.", "motivation": "Floating-point computations can yield different results on different compilers or optimization levels, which can undermine the reliability of numerical software. Detecting these inconsistencies is crucial for robust scientific and engineering applications.", "method": "LLM4FP framework leverages Large Language Models for generating floating-point programs that are likely to expose inconsistencies. It integrates grammar-based generation with feedback-driven mutation to systematically craft diverse and valid test programs.", "result": "LLM4FP detects more than twice as many inconsistencies as the previous best tool (Varity). The majority detected are subtle, real-valued differences rather than obvious anomalies such as NaN or infinities. It also identifies mismatches across various optimization levels and between host/device compilers.", "conclusion": "Using LLM-guided program generation markedly enhances the discovery of floating-point inconsistencies. The framework offers improved coverage and effectiveness compared to previous methodologies, highlighting the value of LLMs in automated software reliability testing."}}
{"id": "2509.00466", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.00466", "abs": "https://arxiv.org/abs/2509.00466", "authors": ["Negar Hashemi", "Amjed Tahir", "Shawn Rasheed", "August Shi", "Rachel Blagojevic"], "title": "JS-TOD: Detecting Order-Dependent Flaky Tests in Jest", "comment": null, "summary": "We present JS-TOD (JavaScript Test Order-dependency Detector), a tool that\ncan extract, reorder, and rerun Jest tests to reveal possible order-dependent\ntest flakiness. Test order dependency is one of the leading causes of test\nflakiness. Ideally, each test should operate in isolation and yield consistent\nresults no matter the sequence in which tests are run. However, in practice,\ntest outcomes can vary depending on their execution order. JS-TOD employed a\nsystematic approach to randomising tests, test suites, and describe blocks. The\ntool is highly customisable, as one can set the number of orders and reruns\nrequired (the default setting is 10 reorder and 10 reruns for each test and\ntest suite). Our evaluation using JS-TOD reveals two main causes of test order\ndependency flakiness: shared files and shared mocking state between tests.", "AI": {"tldr": "JS-TOD is a customizable tool for detecting test order dependency in JavaScript/Jest tests by randomizing and rerunning their execution order. It reveals that shared files and shared mocking state are primary sources of flaky tests.", "motivation": "Test order dependency is a common cause of test flakiness in JavaScript projects using Jest. Ensuring each test is robust and independent is vital, but in reality, tests can affect one another depending on their execution order.", "method": "JS-TOD is a tool that systematically randomizes the execution order of Jest tests, test suites, and describe blocks. It extracts, reorders, and reruns tests to detect order-dependent flakiness, allowing customization of the number of reorders and reruns.", "result": "Using JS-TOD, the authors identify two main causes of test order dependency: shared files and shared mocking state among tests.", "conclusion": "JS-TOD effectively detects order-dependent test flakiness in Jest-based JavaScript projects and helps developers identify underlying issues such as shared files and mocking states."}}
{"id": "2509.00785", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.00785", "abs": "https://arxiv.org/abs/2509.00785", "authors": ["Elena Masserini", "Daniela Micucci", "Leonardo Mariani"], "title": "Bug Whispering: Towards Audio Bug Reporting", "comment": "2 pages, 1 figure, IEEE International Symposium on Software\n  Reliability Engineering (ISSRE), 2025, Fast Abstracts Session", "summary": "Bug reporting is a key feature of mobile applications, as it enables\ndevelopers to collect information about faults that escaped testing and thus\naffected end-users. This paper explores the idea of allowing end-users to\nimmediately report the problems that they experience by recording and\nsubmitting audio messages. Audio recording is simple to implement and has the\npotential to increase the number of bug reports that development teams can\ngather, thus potentially improving the rate at which bugs are identified and\nfixed. However, audio bug reports exhibit specific characteristics that\nchallenge existing techniques for reproducing bugs. This paper discusses these\nchallenges based on a preliminary experiment, and motivates further research on\nthe collection and analysis of audio-based bug reports", "AI": {"tldr": "Audio bug reporting for mobile apps may boost bug collection and fixing rates, but presents unique challenges for reproduction and analysis; further research is needed.", "motivation": "Traditional bug reporting relies heavily on text-based descriptions, which may limit the information developers receive about faults experienced by end-users. This paper is motivated by the opportunity to enhance bug reporting in mobile applications by leveraging audio messages, making it simpler and possibly more motivating for end-users to report bugs.", "method": "The paper explores the idea of using audio recordings as bug reports by allowing end-users to record and submit audio messages describing the problems they encounter. It discusses the design and implementation simplicity of audio reporting and conducts a preliminary experiment to analyze its effectiveness and the challenges.", "result": "Audio bug reports increase the potential quantity of bug reports developers can collect, potentially improving bug identification and resolution rates. However, audio bug reports present unique processing and analysis challenges compared to traditional approaches, mainly in bug reproduction.", "conclusion": "Audio-based bug reporting has promise to enhance bug data collection and support faster bug resolution in mobile applications, but specific challenges in analyzing and reproducing bugs from audio must be addressed. The paper encourages further research in this direction."}}
{"id": "2509.00360", "categories": ["cs.PL", "D.3.0"], "pdf": "https://arxiv.org/pdf/2509.00360", "abs": "https://arxiv.org/abs/2509.00360", "authors": ["Shaan Nagy", "Timothy Zhou", "Nadia Polikarpova", "Loris D'Antoni"], "title": "ChopChop: a Programmable Framework for Semantically Constraining the Output of Language Models", "comment": null, "summary": "Language models (LMs) can generate code, but cannot guarantee its\ncorrectness--producing outputs that often violate type safety, program\ninvariants, or semantic equivalence. Constrained decoding offers a solution by\nrestricting generation to programs that satisfy desired properties. Yet,\nexisting methods are limited to shallow syntactic constraints or rely on\nbrittle, ad hoc encodings of semantics over token sequences.\n  We present ChopChop, the first programmable framework for semantic\nconstrained decoding, enabling LMs to generate code that provably satisfies\nrich semantic properties. ChopChop connects token-level generation with\nreasoning over abstract program structures using a coinduction-based formalism\nand reduces constraint enforcement to a realizability problem over regular\ncodata. We demonstrate ChopChop's generality through generation constrained by\ntype safety and program equivalence, showing how formal methods can be\nseamlessly integrated into LM-driven code generation. ChopChop transforms\nsemantic constrained decoding from a niche technique into a systematic,\nprincipled extension of LMs--improving success rates across models and tasks\nwhile maintaining practical decoding latency.", "AI": {"tldr": "ChopChop is a framework that lets language models generate code guaranteed to meet deep semantic requirements (like type safety), bridging formal methods with modern code generation without sacrificing performance.", "motivation": "Language models can generate code but often produce code violating crucial properties like type safety and invariants. Existing solutions for controlling generation are limited and not robust for semantic constraints, motivating a principled and programmable approach.", "method": "The paper introduces ChopChop, a programmable framework for semantic constrained decoding in language models. It connects token-level generation to program structure reasoning using a coinduction-based formalism, and reduces semantic constraint enforcement to a realizability problem over regular codata.", "result": "ChopChop enables language models to provably satisfy rich semantic properties such as type safety and program equivalence during code generation. It works systematically across different models and tasks and maintains practical decoding latency.", "conclusion": "ChopChop systematically extends language models to generate code with provable semantic properties, transforming semantic constrained decoding into a practical tool that improves the reliability and practical applicability of code generation."}}
{"id": "2509.01006", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.01006", "abs": "https://arxiv.org/abs/2509.01006", "authors": ["Daniela Damian", "Bachan Ghimire", "Ze Shi Li"], "title": "REConnect: Participatory RE that Matters", "comment": "23 pages", "summary": "Software increasingly shapes the infrastructures of daily life, making\nrequirements engineering (RE) central to ensuring that systems align with human\nvalues and lived experiences. Yet, current popular practices such as CrowdRE\nand AI-assisted elicitation strategies risk detaching requirements work from\nthe cultural, social, and political contexts that shape lived experiences,\nhuman values, and real user needs. In this paper, we introduce REConnect that\nre-centers RE on the human connection as central to the understanding of lived\nexperiences where impact is sought. REConnect advocates for a human-centered\nparticipatory approach \"that matters\" to the communities and beneficiaries\ninvolved, ensuring alignment with their values and aspirations. Drawing on\nthree case studies of societal impact: BloodSync in rural Nepal, Herluma\nsupporting women at risk of homelessness in Canada, and BridgingRoots to\nrevitalize Indigenous languages in the Canadian Arctic. REConnect argues that\nthree key principles and enablers: building trusting relationships,\nco-designing with and alongside stakeholders, and empowering users as agents of\nchange, can yield requirements that are culturally grounded, socially\nlegitimate, and sustainable beyond system delivery. REConnect also proposes a\nset of actionable practices (REActions) that embed relationality and ongoing\nstakeholder engagement throughout requirements elicitation, analysis, and\nvalidation of solution development. Finally, we situate REConnect in the era of\nGenerative AI. While AI can accelerate and scale certain RE tasks, its\nintegration must be guided by participatory practices that not only preserve\nhuman agency but also empower humans' roles to become guardians of values and\nethics, inclusion amplifiers, curators of AI outputs, and co-reflectors in\niterative review cycles.", "AI": {"tldr": "REConnect is a human-centered participatory framework for requirements engineering that focuses on stakeholder relationships, co-design, and empowerment to produce culturally grounded, socially legitimate outcomes. Through case studies, it shows practical ways to ensure sustained impact and ethical use of AI in the RE process.", "motivation": "Current popular RE practices like CrowdRE and AI-assisted elicitation are often disconnected from real users\u2019 values, lived experiences, and social/political contexts. The motivation is to re-center RE on meaningful human connections and participatory processes that matter to communities, leading to socially legitimate and sustainable software systems.", "method": "The paper introduces REConnect through three case studies\u2014BloodSync in Nepal, Herluma in Canada, and BridgingRoots in the Canadian Arctic\u2014to illustrate its participatory, relationship-based approach. It also proposes practical actionable guidelines (REActions) for ongoing stakeholder engagement and relationality in requirements engineering tasks.", "result": "REConnect demonstrates, through three societal impact projects, that building trust, co-designing with stakeholders, and empowering users result in requirements that are culturally sensitive, socially accepted, and sustainable. It also shows how human-centered participatory approaches can guide the ethical and effective integration of AI in RE processes.", "conclusion": "REConnect is an approach that improves requirements engineering (RE) by focusing on human connections, participatory practices, and culturally grounded outcomes. It asserts that requirements work should empower users, ensure community alignment, and sustain impact even as generative AI tools become popular in RE."}}
{"id": "2509.00587", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.00587", "abs": "https://arxiv.org/abs/2509.00587", "authors": ["Vaibhav Mehta", "Justin Hsu"], "title": "A Hoare Logic for Symmetry Properties", "comment": "Accepted to OOPSLA '25", "summary": "Many natural program correctness properties can be stated in terms of\n  symmetries, but existing formal methods have little support for reasoning\n  about such properties. We consider how to formally verify a broad class of\n  symmetry properties expressed in terms of group actions. To specify these\n  properties, we design a syntax for group actions, supporting standard\n  constructions and a natural notion of entailment. Then, we develop a\n  Hoare-style logic for verifying symmetry properties of imperative programs,\n  where group actions take the place of the typical pre- and post-condition\n  assertions. Finally, we develop a prototype tool $\\mathsf{SymVerif}$, and use\n  it to verify symmetry properties on a series of handcrafted benchmarks. Our\n  tool uncovered an error in a model of a dynamical system described by\n\\citet{McLachlan_Quispel_2002}.", "AI": {"tldr": "This paper introduces a logic and tool for verifying program symmetries via group actions, showing it can find real errors missed by previous approaches.", "motivation": "Many program correctness properties are expressed as symmetries, but formal methods currently lack effective support for reasoning about symmetries.", "method": "The authors design a syntax for specifying symmetry properties using group actions, create a Hoare-style logic replacing assertions with group actions, and implement a prototype tool called SymVerif to verify these properties.", "result": "The SymVerif tool successfully verifies symmetry properties on various benchmarks and discovers an error in a known model from prior literature.", "conclusion": "It is feasible to formally verify symmetry properties of programs using group action-based logic, and the proposed tool demonstrates practical utility."}}
{"id": "2509.01048", "categories": ["cs.SE", "D.2.1"], "pdf": "https://arxiv.org/pdf/2509.01048", "abs": "https://arxiv.org/abs/2509.01048", "authors": ["Ateeq Sharfuddin", "Travis Breaux"], "title": "Generative Goal Modeling", "comment": "11 pages,", "summary": "In software engineering, requirements may be acquired from stakeholders\nthrough elicitation methods, such as interviews, observational studies, and\nfocus groups. When supporting acquisition from interviews, business analysts\nmust review transcripts to identify and document requirements. Goal modeling is\na popular technique for representing early stakeholder requirements as it lends\nitself to various analyses, including refinement to map high-level goals into\nsoftware operations, and conflict and obstacle analysis. In this paper, we\ndescribe an approach to use textual entailment to reliably extract goals from\ninterview transcripts and to construct goal models. The approach has been\nevaluated on 15 interview transcripts across 29 application domains. The\nfindings show that GPT-4o can reliably extract goals from interview\ntranscripts, matching 62.0% of goals acquired by humans from the same\ntranscripts, and that GPT-4o can trace goals to originating text in the\ntranscript with 98.7% accuracy. In addition, when evaluated by human\nannotators, GPT-4o generates goal model refinement relationships among\nextracted goals with 72.2% accuracy.", "AI": {"tldr": "The paper demonstrates that GPT-4o can automate the extraction and modeling of stakeholder goals from interview transcripts with good accuracy, potentially reducing manual effort for business analysts in software engineering.", "motivation": "Business analysts spend significant effort manually reviewing interview transcripts to extract and document stakeholder requirements. Automating this process can save time and improve consistency. Goal modeling is a key technique in requirements engineering, and leveraging AI could enhance its efficiency.", "method": "The paper proposes an approach that utilizes textual entailment via GPT-4o to automatically extract goals from interview transcripts and construct goal models. The approach was evaluated on 15 transcripts across 29 domains. Accuracy metrics were used to assess goal extraction, traceability, and modeling refinement by AI compared to human performance.", "result": "GPT-4o was able to extract 62% of the goals identified by humans, trace goals to their transcript origins with 98.7% accuracy, and generate goal model refinement relationships with 72.2% accuracy as judged by human annotators.", "conclusion": "GPT-4o is a reliable tool for automating goal extraction and modeling from interview transcripts in requirements engineering, showing strong performance in traceability and reasonable accuracy in goal identification and refinement compared to humans."}}
{"id": "2509.00699", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.00699", "abs": "https://arxiv.org/abs/2509.00699", "authors": ["Yumeng He", "Chandrakana Nandi", "Sreepathi Pai"], "title": "Formalizing Linear Motion G-code for Invariant Checking and Differential Testing of Fabrication Tools", "comment": null, "summary": "The computational fabrication pipeline for 3D printing is much like a\ncompiler - users design models in Computer Aided Design (CAD) tools that are\nlowered to polygon meshes to be ultimately compiled to machine code by 3D\nslicers. For traditional compilers and programming languages, techniques for\nchecking program invariants are well-established. Similarly, methods like\ndifferential testing are often used to uncover bugs in compilers themselves,\nwhich makes them more reliable. The fabrication pipeline would benefit from\nsimilar techniques but traditional approaches do not directly apply to the\nrepresentations used in this domain. Unlike traditional programs, 3D models\nexist both as geometric objects as well as machine code that ultimately runs on\nthe hardware. The machine code, like in traditional compiling, is affected by\nmany factors like the model, the slicer being used, and numerous\nuser-configurable parameters that control the slicing process. In this work, we\npropose a new algorithm for lifting G-code (a common language used in\nfabrication pipelines) by denoting a G-code program to a set of cuboids, and\nthen defining an approximate point cloud representation for efficiently\noperating on these cuboids. Our algorithm opens up new opportunities: we show\nthree use cases that demonstrate how it enables error localization in CAD\nmodels through invariant checking, quantitative comparisons between slicers,\nand evaluating the efficacy of mesh repair tools. We present a prototype\nimplementation of our algorithm in a tool, GlitchFinder, and evaluate it on 58\nreal-world CAD models. Our results show that GlitchFinder is particularly\neffective in identifying slicing issues due to small features, can highlight\ndifferences in how popular slicers (Cura and PrusaSlicer) slice the same model,\nand can identify cases where mesh repair tools (MeshLab and Meshmixer)\nintroduce new errors during repair.", "AI": {"tldr": "This paper introduces an algorithm and tool (GlitchFinder) that adapts techniques from compiler validation to 3D printing pipelines, allowing detection of slicing and mesh repair issues, and comparison of slicer outputs.", "motivation": "Despite the similarities between 3D printing fabrication pipelines and programming language compilers, the former lacks established techniques like invariant checking and differential testing, which can improve reliability. The unique nature of 3D models and their representation as both geometric objects and machine code brings challenges to directly applying traditional compiler verification techniques.", "method": "The authors propose a novel algorithm that lifts G-code (the machine code for 3D printing) into a set of cuboids and further into an approximate point cloud representation. This facilitates efficient operations and analysis, enabling various validation and comparison tasks in the fabrication pipeline. They implement this approach in a tool called GlitchFinder and evaluate its performance on 58 real-world CAD models.", "result": "GlitchFinder effectively identifies slicing issues, particularly those caused by small features. It also highlights the impact of different slicers (Cura and PrusaSlicer) on the same model and detects errors introduced by mesh repair tools (MeshLab and Meshmixer).", "conclusion": "The proposed algorithm and GlitchFinder tool successfully bring advanced invariant checking and validation techniques from traditional compiling into the 3D printing domain, enhancing error localization and pipeline reliability."}}
{"id": "2509.01068", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.01068", "abs": "https://arxiv.org/abs/2509.01068", "authors": ["Chong Wang", "Haoning Wu", "Peng Liang", "Maya Daneva", "Marten van Sinderen"], "title": "A Survey on the Techniques and Tools for Automated Requirements Elicitation and Analysis of Mobile Apps", "comment": null, "summary": "[Background:] Research on automated requirements elicitation and analysis of\nmobile apps employed lots of techniques and tools proposed by RE researchers\nand practitioners. However, little is known about the characteristics of these\ntechniques and tools as well as the RE tasks in requirements elicitation and\nanalysis that got supported with the help of respective techniques and tools.\n[Aims:] The goal of this paper is to investigate the state-of-the-art of the\ntechniques and tools used in automated requirements elicitation and analysis of\nmobile apps. [Method:] We carried out a systematic mapping study by following\nthe guidelines of Kitchenham et al. [Results:] Based on 73 selected papers, we\nfound the most frequently used techniques - semi-automatic techniques, and the\nmain characteristics of the tools - open-sourced and non-self-developed tools\nfor requirements analysis and text pre-processing. Plus, the most three\ninvestigated RE tasks are requirements analysis, mining and classification.\n[Conclusions:] Our most important conclusions are: (1) there is a growth in the\nuse of techniques and tools in automated requirements elicitation and analysis\nof mobile apps, (2) semi-automatic techniques are mainly used in the\npublications on this research topic, (3) requirements analysis, mining and\nclassification are the top three RE tasks with the support of automatic\ntechniques and tools, and (4) the most popular tools are open-sourced and\nnon-self-developed, and they are mainly used in requirements analysis and text\nprocessing.", "AI": {"tldr": "This paper systematically reviews automated requirements elicitation and analysis methods for mobile apps, finding a trend toward semi-automatic, open-sourced, and third-party tools mainly used for analysis, mining, and classification tasks.", "motivation": "There is a lack of understanding regarding the characteristics of techniques and tools used in automated requirements elicitation and analysis for mobile apps, as well as the specific RE tasks they support.", "method": "The authors conducted a systematic mapping study adhering to Kitchenham et al.'s guidelines, reviewing 73 selected papers in the domain.", "result": "Semi-automatic techniques are most frequently used. The main tool characteristics are open-source and non-self-developed, primarily catering to requirements analysis and text processing. The top tasks investigated are requirements analysis, mining, and classification.", "conclusion": "Automated techniques and tools for requirements elicitation and analysis in mobile apps are increasingly utilized, with semi-automatic methods prevailing. The most supported RE tasks are analysis, mining, and classification, relying mainly on open-sourced, non-self-developed tools for text processing and analysis."}}
{"id": "2509.00948", "categories": ["cs.PL", "cs.FL"], "pdf": "https://arxiv.org/pdf/2509.00948", "abs": "https://arxiv.org/abs/2509.00948", "authors": ["Denghang Hu", "Taolue Chen", "Philipp R\u00fcmmer", "Fu Song", "Zhilin Wu"], "title": "Decision Procedure for A Theory of String Sequences", "comment": "21 pages, 2 tables, APLAS 2025", "summary": "The theory of sequences, supported by many SMT solvers, can model program\ndata types including bounded arrays and lists. Sequences are parameterized by\nthe element data type and provide operations such as accessing elements,\nconcatenation, forming sub-sequences and updating elements. Strings and\nsequences are intimately related; many operations, e.g., matching a string\naccording to a regular expression, splitting strings, or joining strings in a\nsequence, are frequently used in string-manipulating programs. Nevertheless,\nthese operations are typically not directly supported by existing SMT solvers,\nwhich instead only consider the generic theory of sequences. In this paper, we\npropose a theory of string sequences and study its satisfiability. We show\nthat, while it is undecidable in general, the decidability can be recovered by\nrestricting to the straight-line fragment. This is shown by encoding each\nstring sequence as a string, and each string sequence operation as a\ncorresponding string operation. We provide pre-image computation for the\nresulting string operations with respect to automata, effectively casting it\ninto the generic OSTRICH string constraint solving framework. We implement the\nnew decision procedure as a tool $\\ostrichseq$, and carry out experiments on\nbenchmark constraints generated from real-world JavaScript programs,\nhand-crafted templates and unit tests. The experiments confirm the efficacy of\nour approach.", "AI": {"tldr": "A new theory for string sequences enables SMT solvers to support direct string operations (e.g., regex match, split, join) in program analysis. While full satisfiability is undecidable, the straight-line fragment is decidable when encoded into the existing OSTRICH framework. The proposed tool is effective on real-world benchmarks.", "motivation": "Many SMT solvers support sequences to model bounded arrays and lists, but lack direct support for critical string-related operations (like regex matching, splitting, joining) that are widely used in practical string-manipulating programs.", "method": "The paper proposes a theory of string sequences and analyzes its satisfiability. It presents an encoding scheme that transforms string sequence operations into equivalent string operations. Decidability is regained for the straight-line fragment through this encoding. Furthermore, pre-image computation with respect to automata enables leveraging the OSTRICH string constraint solving framework. The authors implement their decision procedure as a tool, $\text{ostrichseq}$, and evaluate it on constraints generated from JS programs and benchmarks.", "result": "The proposed theory is undecidable in the general case, but decidable for the straight-line fragment. The encoding and integration into the OSTRICH framework work effectively. Experimental results show that the new tool is effective on various benchmarks from real-world applications.", "conclusion": "Direct string sequence operations\u2014previously unavailable in SMT solvers\u2014can be supported via the new decision procedure. This expands the power of solvers for string-manipulating program analysis, especially for straight-line code fragments. The provided tool demonstrates practical efficacy."}}
{"id": "2509.01149", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.01149", "abs": "https://arxiv.org/abs/2509.01149", "authors": ["Hui Zeng", "Zhihao Xu", "Hui Li", "Siwen Wang", "Qian Ma"], "title": "Compiler Bugs Detection in Logic Synthesis Tools via Linear Upper Confidence Bound", "comment": null, "summary": "Field-Programmable Gate Arrays (FPGAs) play an indispensable role in\nElectronic Design Automation (EDA), translating Register-Transfer Level (RTL)\ndesigns into gate-level netlists. The correctness and reliability of FPGA logic\nsynthesis tools are critically important, as unnoticed bugs in these tools may\ninfect the final hardware implementations. However, recent approaches often\nrely heavily on random selection strategies, limiting the structural diversity\nof the generated HDL test cases and resulting in inadequate exploration of the\ntool's feature space. To address this limitation, we propose Lin-Hunter, a\nnovel testing framework designed to systematically enhance the diversity of HDL\ntest cases and the efficiency of FPGA logic synthesis tool validation.\nSpecifically, Lin-Hunter introduces a principled set of metamorphic\ntransformation rules to generate functionally equivalent yet structurally\ndiverse HDL test case variants, effectively addressing the limited diversity of\nexisting test inputs. To further enhance bug discovery efficiency, Lin-Hunter\nintegrates an adaptive strategy selection mechanism based on the Linear Upper\nConfidence Bound (LinUCB) method. This method leverages feedback from synthesis\nlogs of previously executed test cases to dynamically prioritize transformation\nstrategies that have empirically demonstrated a higher likelihood of triggering\nsynthesis bugs. Comprehensive experiments conducted over a three-month period\ndemonstrate the practical effectiveness of Lin-Hunter. Our method has\ndiscovered 18 unique bugs, including 10 previously unreported defects, which\nhave been confirmed by official developers. Moreover, our method outperforms\nstate-of-the-art testing methods in both test-case diversity and bug-discovery\nefficiency.", "AI": {"tldr": "Lin-Hunter is a new testing framework for FPGA logic synthesis tools that generates diverse, bug-triggering test cases via clever transformation rules and adaptive strategy selection. It found more, and previously unknown, bugs than prior methods, confirming its effectiveness and coverage.", "motivation": "The motivation behind this work is to address the shortcomings of current FPGA logic synthesis tool testing methods, which mainly use random selection strategies. These strategies limit the structural diversity of HDL test cases, making it difficult to thoroughly test and uncover bugs in FPGA logic synthesis tools. Given the critical importance of these tools for hardware reliability, improved methods are needed for systematic validation.", "method": "The authors propose Lin-Hunter, a novel testing framework that introduces a set of metamorphic transformation rules to generate functionally equivalent but structurally diverse HDL test cases. The framework also incorporates an adaptive strategy selection mechanism based on the Linear Upper Confidence Bound (LinUCB) algorithm. This enables dynamic prioritization of transformations that are more likely to uncover bugs, using feedback from previous test results.", "result": "Lin-Hunter was evaluated through comprehensive experiments over three months. It discovered 18 unique bugs in FPGA synthesis tools, including 10 previously unknown bugs confirmed by official developers. The framework outperformed existing methods in both the diversity of test cases and the efficiency of bug discovery.", "conclusion": "Lin-Hunter significantly enhances both the diversity and effectiveness of test cases in FPGA logic synthesis tool validation. Its principled, adaptive approach uncovers more bugs and provides better coverage of design tool features compared to state-of-the-art methods."}}
{"id": "2509.01511", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.01511", "abs": "https://arxiv.org/abs/2509.01511", "authors": ["Zhe Zhou", "Benjamin Delaware", "Suresh Jagannathan"], "title": "Type-Based Incorrectness Reasoning", "comment": null, "summary": "A coverage type generalizes refinement types found in many functional\nlanguages with support for must-style underapproximate reasoning.\nProperty-based testing frameworks are one particularly useful domain where such\ncapabilities are useful as they allow us to verify the completeness, as well as\nsafety, of test generators. There is a surprising connection between the kind\nof underapproximate reasoning coverage types offer and the style of reasoning\nenabled by recently proposed Incorrectness Logic frameworks. In our\npresentation, we propose to explore this connection more deeply, identifying\nmechanisms that more systematically integrate incorrectness reasoning within an\nexpressive refinement type system and the opportunities that such integration\noffers to functional programmers, program verifiers, and program analyzers and\nrelated tools.", "AI": {"tldr": "This paper explores how coverage types extend refinement types with must-style underapproximate reasoning, highlighting their connection to Incorrectness Logic. It proposes systematic integration of these frameworks, boosting verification and analysis in functional languages, especially for property-based testing.", "motivation": "To enable must-style underapproximate reasoning in functional languages, which is particularly valuable for property-based testing frameworks because it verifies both safety and completeness of test generators.", "method": "Exploring coverage types as a generalization of refinement types, and examining their connection with Incorrectness Logic frameworks. The approach is to systematically integrate incorrectness reasoning within expressive refinement type systems.", "result": "Identified mechanisms for integrating incorrectness reasoning into refinement type systems, offering new opportunities for functional programmers and tool developers to improve program verification and analysis.", "conclusion": "Integrating incorrectness reasoning into refinement type systems can enhance the expressiveness and verification power of functional programming languages and related tools, benefiting property-based testing and program analysis tasks."}}
{"id": "2509.01255", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.01255", "abs": "https://arxiv.org/abs/2509.01255", "authors": ["Oleksii Novikov", "Davide Fucci", "Oleksandr Adamov", "Daniel Mendez"], "title": "Policy-driven Software Bill of Materials on GitHub: An Empirical Study", "comment": "To be published in the proceedings of PROFES2025", "summary": "Background. The Software Bill of Materials (SBOM) is a machine-readable list\nof all the software dependencies included in a software. SBOM emerged as way to\nassist securing the software supply chain. However, despite mandates from\ngovernments to use SBOM, research on this artifact is still in its early\nstages. Aims. We want to understand the current state of SBOM in open-source\nprojects, focusing specifically on policy-driven SBOMs, i.e., SBOM created to\nachieve security goals, such as enhancing project transparency and ensuring\ncompliance, rather than being used as fixtures for tools or artificially\ngenerated for benchmarking or academic research purposes. Method. We performed\na mining software repository study to collect and carefully select SBOM files\nhosted on GitHub. We analyzed the information reported in policy-driven SBOMs\nand the vulnerabilities associated with the declared dependencies by means of\ndescriptive statistics. Results. We show that only 0.56% of popular GitHub\nrepositories contain policy-driven SBOM. The declared dependencies contain\n2,202 unique vulnerabilities, while 22% of them do not report licensing\ninformation. Conclusion. Our findings provide insights for SBOM usage to\nsupport security assessment and licensing.", "AI": {"tldr": "SBOMs meant for security and compliance are rare in top GitHub repos; those that exist reveal many vulnerabilities and missing license details, showing room for improvement in practical SBOM use.", "motivation": "Although SBOMs are mandated by governments to secure the software supply chain, their practical usage and research, particularly for security-oriented SBOMs in open-source projects, is limited. The study seeks to understand the real-world adoption and effectiveness of SBOMs for security and compliance.", "method": "A mining software repository study was conducted by collecting and filtering SBOM files hosted on GitHub. The study focused on policy-driven SBOMs and analyzed their reported data, as well as the vulnerabilities associated with their dependencies, using descriptive statistics.", "result": "Only 0.56% of popular open-source GitHub repositories contain policy-driven SBOMs. The dependencies listed in these SBOMs collectively have 2,202 unique vulnerabilities, and 22% lack licensing information.", "conclusion": "Only a small fraction (0.56%) of popular GitHub repositories utilize policy-driven SBOMs, which are intended to improve security and transparency. The declared dependencies have thousands of unique vulnerabilities, and a significant portion lack licensing information. This highlights gaps in current SBOM adoption and its effectiveness for security and compliance."}}
{"id": "2509.02428", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.02428", "abs": "https://arxiv.org/abs/2509.02428", "authors": ["Yongwei Yuan", "Zhe Zhou", "Julia Belyakova", "Benjamin Delaware", "Suresh Jagannathan"], "title": "From Traces to Program Incorrectness: A Type-Theoretic Approach", "comment": null, "summary": "We present a type-theoretic framework for reasoning about incorrectness in\nfunctional programs that interact with effectful, opaque library APIs. Our\napproach centers on traces -- temporally-ordered sequences of library API\ninvocations -- which naturally characterize both the preconditions of\nindividual APIs and their composite behavior. We represent these traces using\nsymbolic regular expressions (SREs), enabling formal specification of incorrect\nabstract data type (ADT) behaviors across function boundaries. The core\ncontribution is a novel type inference algorithm that operates modulo specified\nincorrectness properties and leverages the symbolic finite automata (SFAs)\nrepresentations of regexes for compositional reasoning of traces. When the\nalgorithm succeeds, the inferred types witness that an ADT implementation can\nexhibit some subset of the specified incorrect behaviors. This represents the\nfirst systematic approach to underapproximate reasoning against trace-based\nincorrectness specifications, enabling a new form of trace-guided compositional\nanalysis.", "AI": {"tldr": "The paper proposes a trace-based type system using symbolic regular expressions and automata to systematically reason about the possible incorrect behaviors in functional programs interacting with effectful, opaque libraries.", "motivation": "To enable formal and compositional reasoning about incorrect behaviors in functional programs using effectful, opaque library APIs, which is important because current methods lack systematic approaches for such underapproximate analyses.", "method": "They use symbolic regular expressions (SREs) and symbolic finite automata (SFAs) to represent and reason about traces of API invocations, together with a novel type inference algorithm that operates modulo specified incorrectness properties.", "result": "The approach can infer types that witness that an ADT implementation can exhibit specified incorrect behaviors, using compositional reasoning across function boundaries.", "conclusion": "The paper introduces the first systematic framework for underapproximating reasoning about trace-based incorrectness specifications in functional programs, enabling trace-guided compositional analysis."}}
{"id": "2509.01294", "categories": ["cs.SE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.01294", "abs": "https://arxiv.org/abs/2509.01294", "authors": ["Helge Spieker", "Nadjib Lazaar", "Arnaud Gotlieb", "Nassim Belmecheri"], "title": "Metamorphic Testing of Multimodal Human Trajectory Prediction", "comment": "Information and Software Technology", "summary": "Context: Predicting human trajectories is crucial for the safety and\nreliability of autonomous systems, such as automated vehicles and mobile\nrobots. However, rigorously testing the underlying multimodal Human Trajectory\nPrediction (HTP) models, which typically use multiple input sources (e.g.,\ntrajectory history and environment maps) and produce stochastic outputs\n(multiple possible future paths), presents significant challenges. The primary\ndifficulty lies in the absence of a definitive test oracle, as numerous future\ntrajectories might be plausible for any given scenario. Objectives: This\nresearch presents the application of Metamorphic Testing (MT) as a systematic\nmethodology for testing multimodal HTP systems. We address the oracle problem\nthrough metamorphic relations (MRs) adapted for the complexities and stochastic\nnature of HTP. Methods: We present five MRs, targeting transformations of both\nhistorical trajectory data and semantic segmentation maps used as an\nenvironmental context. These MRs encompass: 1) label-preserving geometric\ntransformations (mirroring, rotation, rescaling) applied to both trajectory and\nmap inputs, where outputs are expected to transform correspondingly. 2)\nMap-altering transformations (changing semantic class labels, introducing\nobstacles) with predictable changes in trajectory distributions. We propose\nprobabilistic violation criteria based on distance metrics between probability\ndistributions, such as the Wasserstein or Hellinger distance. Conclusion: This\nstudy introduces tool, a MT framework for the oracle-less testing of\nmultimodal, stochastic HTP systems. It allows for assessment of model\nrobustness against input transformations and contextual changes without\nreliance on ground-truth trajectories.", "AI": {"tldr": "Testing human trajectory prediction models is difficult because there's no single correct path for each scenario. This paper introduces a metamorphic testing framework that uses input transformations and probabilistic criteria to systematically evaluate these models\u2019 robustness\u2014without needing ground-truth answers.", "motivation": "Testing human trajectory prediction (HTP) models, which are crucial for autonomous systems, is challenging because there is no single correct answer\u2014multiple future paths can be plausible in any scenario. Current testing is limited by the lack of a clear test oracle for these stochastic, multimodal models, hindering rigorous evaluation.", "method": "The paper applies Metamorphic Testing (MT) to HTP models by developing five Metamorphic Relations (MRs). These MRs target both historical trajectory data and environmental maps through geometric transformations, label changes, and obstacle introductions. Outputs are evaluated by measuring probabilistic violations using distance metrics (such as Wasserstein or Hellinger distances) between predicted trajectory distributions.", "result": "The proposed MT framework successfully enables systematic, oracle-less testing of multimodal, stochastic HTP models. It assesses model robustness to various input and contextual transformations without needing ground-truth future trajectories.", "conclusion": "This research demonstrates that metamorphic testing is a viable methodology for evaluating the robustness of HTP models in autonomous systems, providing a framework that overcomes the test oracle problem and facilitates thorough, reliable testing."}}
{"id": "2509.01313", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.01313", "abs": "https://arxiv.org/abs/2509.01313", "authors": ["Zhao Tian", "Junjie Chen"], "title": "Aligning Requirement for Large Language Model's Code Generation", "comment": "Accepted by ICSE 2026", "summary": "Code generation refers to the automatic generation of source code based on a\ngiven programming specification, which has garnered significant attention\nparticularly with the advancement of large language models (LLMs). However, due\nto the inherent complexity of real-world problems, the LLM-generated code often\nfails to fully align with the provided specification. While state-of-the-art\nagent-based techniques have been proposed to enhance LLM code generation, they\noverlook the critical issue of specification perception, resulting in\npersistent misalignment issues. Given that accurate perception of programming\nspecifications serves as the foundation of the LLM-based code generation\nparadigm, ensuring specification alignment is particularly crucial. In this\nwork, we draw on software requirements engineering to propose Specine, a novel\nspecification alignment technique for LLM code generation. Its key idea is to\nidentify misaligned input specifications, lift LLM-perceived specifications,\nand align them to enhance the code generation performance of LLMs. Our\ncomprehensive experiments on four state-of-the-art LLMs across five challenging\ncompetitive benchmarks by comparing with ten state-of-the-art baselines,\ndemonstrate the effectiveness of Specine. For example, Specine outperforms the\nmost effective baseline, achieving an average improvement of 29.60\\% across all\nsubjects in terms of Pass@1.", "AI": {"tldr": "Specine is a new technique that improves how LLMs align code generation with programming specifications, solving a key problem overlooked by current methods. Experiments show it boosts baseline results by nearly 30%.", "motivation": "Existing agent-based methods for LLM code generation do not adequately address the misalignment between generated code and programming specifications, which limits their effectiveness. Proper specification perception is essential for accurate code generation.", "method": "The authors propose Specine, a specification alignment technique that identifies misaligned input specifications, lifts LLM-perceived specifications, and aligns them to improve code generation. They validate their approach through comprehensive experiments using four state-of-the-art LLMs across five competitive benchmarks and compare performance against ten baselines.", "result": "Specine outperforms the strongest baseline by an average of 29.60% (Pass@1) across all tested subjects, demonstrating its efficacy in ensuring specification-code alignment.", "conclusion": "Specine significantly improves LLM-based code generation by focusing on specification alignment, achieving a substantial performance increase over existing baselines."}}
{"id": "2509.01318", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.01318", "abs": "https://arxiv.org/abs/2509.01318", "authors": ["Chiara Ghinami", "Jonas Winzer", "Nils Bosbach", "Lennart M. Reimann", "Lukas J\u00fcnger", "Simon W\u00f6rner", "Rainer Leupers"], "title": "Leveraging SystemC-TLM-based Virtual Prototypes for Embedded Software Fuzzing", "comment": null, "summary": "SystemC-based virtual prototypes have emerged as widely adopted tools to test\nsoftware ahead of hardware availability, reducing the time-to-market and\nimproving software reliability. Recently, fuzzing has become a popular method\nfor automated software testing due to its ability to quickly identify\ncorner-case errors. However, its application to embedded software is still\nlimited. Simulator tools can help bridge this gap by providing a more powerful\nand controlled execution environment for testing. Existing solutions, however,\noften tightly couple fuzzers with built-in simulators that lack support for\nhardware peripherals and of- fer limited flexibility, restricting their ability\nto test embedded software. To address these limitations, we present a framework\nthat allows the integration of American-Fuzzy-Lop-based fuzzers and\nSystemC-based simulators. The framework provides a harness to decouple the\nadopted fuzzer and simulator. In addition, it intercepts peripheral accesses\nand queries the fuzzer for values, effectively linking peripheral behavior to\nthe fuzzer. This solution enables flexible interchangeability of peripher- als\nwithin the simulation environment and supports the interfacing of different\nSystemC-based virtual prototypes. The flexibility of the pro- posed solution is\ndemonstrated by integrating the harness with different simulators and by\ntesting various softwares.", "AI": {"tldr": "This paper introduces a novel framework that connects AFL-based fuzzers to SystemC simulators, enabling better peripheral support and flexible, thorough fuzz-testing of embedded software. The solution allows easy integration with various simulators and improves testing capabilities for embedded systems.", "motivation": "Fuzzing has proven highly effective in automated software testing, but its application in embedded software is limited due to simulators lacking hardware peripheral support and flexibility. Existing tools tightly couple fuzzers and simulators, restricting the scope and effectiveness of tests for embedded systems.", "method": "The authors present a framework that integrates American-Fuzzy-Lop-based fuzzers with SystemC-based simulators via a decoupled harness. The framework intercepts peripheral accesses during simulation and queries the fuzzer for input values, enabling dynamic and flexible simulation of hardware peripheral behavior.", "result": "The framework allows for flexible interchangeability of peripherals within simulation environments and supports interfacing of various SystemC-based virtual prototypes. The authors demonstrate the solution's flexibility by integrating the harness with different simulators and testing various software.", "conclusion": "The proposed framework successfully addresses limitations in existing solutions, offering improved flexibility and peripheral support for fuzzing embedded software in SystemC-based simulation environments."}}
{"id": "2509.01389", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.01389", "abs": "https://arxiv.org/abs/2509.01389", "authors": ["Diego Clerissi", "Elena Masserini", "Daniela Micucci", "Leonardo Mariani"], "title": "Towards Multi-Platform Mutation Testing of Task-based Chatbots", "comment": "4 pages, 1 figure, Accepted at 9th International Workshop on Software\n  Faults 2025", "summary": "Chatbots, also known as conversational agents, have become ubiquitous,\noffering services for a multitude of domains. Unlike general-purpose chatbots,\ntask-based chatbots are software designed to prioritize the completion of tasks\nof the domain they handle (e.g., flight booking). Given the growing popularity\nof chatbots, testing techniques that can generate full conversations as test\ncases have emerged. Still, thoroughly testing all the possible conversational\nscenarios implemented by a task-based chatbot is challenging, resulting in\nincorrect behaviors that may remain unnoticed. To address this challenge, we\nproposed MUTABOT, a mutation testing approach for injecting faults in\nconversations and producing faulty chatbots that emulate defects that may\naffect the conversational aspects. In this paper, we present our extension of\nMUTABOT to multiple platforms (Dialogflow and Rasa), and present experiments\nthat show how mutation testing can be used to reveal weaknesses in test suites\ngenerated by the Botium state-of-the-art test generator.", "AI": {"tldr": "MUTABOT is a mutation testing framework for injecting conversation faults in chatbots, helping to reveal weaknesses in current automated test suites on platforms like Dialogflow and Rasa.", "motivation": "Thoroughly testing all possible conversational scenarios in task-based chatbots is difficult, often resulting in undetected incorrect behaviors. The increasing prevalence of chatbots demands improved testing approaches to ensure higher reliability.", "method": "The authors propose MUTABOT, a mutation testing approach that injects faults into chatbot conversations, creating defective chatbots to emulate potential conversational defects. MUTABOT is extended to popular chatbot platforms (Dialogflow and Rasa), and experiments are conducted to evaluate its effectiveness against test suites generated by the Botium test generator.", "result": "Experiments demonstrate that MUTABOT's mutation testing can expose weaknesses and gaps in the Botium-generated test suites, revealing scenarios where conversational defects might go unnoticed.", "conclusion": "Mutation testing via MUTABOT, across multiple platforms, is an effective strategy to uncover inadequacies in automated test suites for task-based chatbots by simulating realistic conversation faults."}}
{"id": "2509.01445", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.01445", "abs": "https://arxiv.org/abs/2509.01445", "authors": ["Muhammad Ovais Ahmad", "Tomas Gustavsson"], "title": "Non Technical Debt in Agile Software Development", "comment": null, "summary": "NonTechnical Debt (NTD) is a common challenge in agile software development,\nmanifesting in four critical forms, Process Debt, Social Debt, People Debt,\nOrganizational debt. NODLA project is a collaboration between Karlstad\nUniversity and four leading Swedish industrial partners, reveals how various\ndebt types disrupt large scale Agile Software Development (ASD) environments.\nThrough extensive surveys, indepth interviews, and statistical analyses\ninvolving a diverse group of software professionals, we identified key drivers\nof NTD and their impacts. Our findings emphasize (1) Well structured, highly\ncohesive teams learn faster, adapt more effectively, and innovate consistently.\n(2) Psychological safety, fostered by proactive leadership, is essential for\ninnovation, experimentation, and keeping employees. (3) Inefficient processes\nand unclear roles contribute significantly to drops in job satisfaction,\nproductivity and team morale. (4) Social fragmentation, particularly in remote\nand hybrid settings, breeds rework, delays, and increased costs. (5) Neglected\nhuman resource needs, such as delayed hiring or insufficient training, limit an\norganization ability to meet growing demands. This white paper distils these\ninsights into practical, evidence based strategies, such as refining team\ncomposition, clarifying roles, fostering psychological safety, streamlining\nworkflows, and embracing failure as a learning tool. By implementing these\nstrategies, organizations can reduce NTD, reclaim agility, and unlock their\nteams full potential.", "AI": {"tldr": "This paper analyzes the causes and impacts of NonTechnical Debt in agile software development and offers actionable strategies to improve team cohesion, leadership, processes, and HR practices, helping organizations reduce NTD and enhance agility.", "motivation": "NonTechnical Debt (NTD), including process, social, people, and organizational debt, presents major challenges in agile software development. There is a lack of practical strategies and evidence-based understanding to address these issues, especially at scale.", "method": "The study combined extensive surveys, in-depth interviews, and statistical analyses involving a diverse set of software professionals across multiple organizations. This collaboration allowed for comprehensive data collection and analysis of the impacts and drivers of NTD.", "result": "The research identified major NTD drivers and their effects. Key findings include the importance of structured teams, psychological safety, efficient processes, social cohesion, and meeting human resource needs. Neglecting these areas reduces satisfaction, productivity, innovation, and increases costs and delays, especially in remote/hybrid settings.", "conclusion": "The paper provides practical, evidence-based strategies to reduce NTD: refine team composition, clarify roles, prioritize psychological safety, streamline workflows, and treat failure as a learning opportunity. Implementing these can help organizations reclaim agility and maximize team potential."}}
{"id": "2509.01494", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.01494", "abs": "https://arxiv.org/abs/2509.01494", "authors": ["Zhengran Zeng", "Ruikai Shi", "Keke Han", "Yixin Li", "Kaicheng Sun", "Yidong Wang", "Zhuohao Yu", "Rui Xie", "Wei Ye", "Shikun Zhang"], "title": "Benchmarking and Studying the LLM-based Code Review", "comment": null, "summary": "Automated Code Review (ACR) is crucial for software quality, yet existing\nbenchmarks often fail to reflect real-world complexities, hindering the\nevaluation of modern Large Language Models (LLMs). Current benchmarks\nfrequently focus on fine-grained code units, lack complete project context, and\nuse inadequate evaluation metrics. To address these limitations, we introduce\nSWRBench , a new benchmark comprising 1000 manually verified Pull Requests\n(PRs) from GitHub, offering PR-centric review with full project context.\nSWRBench employs an objective LLM-based evaluation method that aligns strongly\nwith human judgment (~90 agreement) by verifying if issues from a structured\nground truth are covered in generated reviews. Our systematic evaluation of\nmainstream ACR tools and LLMs on SWRBench reveals that current systems\nunderperform, and ACR tools are more adept at detecting functional errors.\nSubsequently, we propose and validate a simple multi-review aggregation\nstrategy that significantly boosts ACR performance, increasing F1 scores by up\nto 43.67%. Our contributions include the SWRBench benchmark, its objective\nevaluation method, a comprehensive study of current ACR capabilities, and an\neffective enhancement approach, offering valuable insights for advancing ACR\nresearch.", "AI": {"tldr": "This paper introduces SWRBench, a more realistic code review benchmark for evaluating ACR tools and LLMs. Existing systems don't perform well on it, but a multi-review aggregation approach boosts scores dramatically. The work highlights gaps in current methods and offers effective solutions to improve automated code review.", "motivation": "Existing Automated Code Review (ACR) benchmarks fail to reflect the complexity of real-world scenarios, focusing on small code units without project context and using inadequate evaluation metrics. This limits the ability to properly assess modern Large Language Models (LLMs) for code review tasks.", "method": "The paper introduces SWRBench, a new benchmark with 1000 manually verified GitHub Pull Requests, capturing full project context. SWRBench uses an LLM-based evaluation method closely aligned with human judgment. The study systematically evaluates current ACR tools and LLMs, and proposes a multi-review aggregation strategy to improve review performance.", "result": "Current ACR systems underperform on the new benchmark; ACR tools are more effective at detecting functional errors than LLMs. Applying a multi-review aggregation strategy substantially increases performance, with F1 score improvements up to 43.67%.", "conclusion": "SWRBench is a valuable new benchmark with a robust, objective evaluation methodology. It reveals gaps in current ACR and LLM capabilities and demonstrates a simple method to significantly enhance performance. This work provides new directions and insights for advancing automated code review research."}}
{"id": "2509.01527", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.01527", "abs": "https://arxiv.org/abs/2509.01527", "authors": ["Amirreza Nayyeri", "Abbas Rasoolzadegan"], "title": "A Privacy-Preserving Recommender for Filling Web Forms Using a Local Large Language Model", "comment": null, "summary": "Web applications are increasingly used in critical domains such as education,\nfinance, and e-commerce. This highlights the need to ensure their failure-free\nperformance. One effective method for evaluating failure-free performance is\nweb form testing, where defining effective test scenarios is key to a complete\nand accurate evaluation. A core aspect of this process involves filling form\nfields with suitable values to create effective test cases. However, manually\ngenerating these values is time-consuming and prone to errors. To address this,\nvarious tools have been developed to assist testers. With the appearance of\nlarge language models (LLMs), a new generation of tools seeks to handle this\ntask more intelligently. Although many LLM-based tools have been introduced, as\nthese models typically rely on cloud infrastructure, their use in testing\nconfidential web forms raises concerns about unintended data leakage and\nbreaches of confidentiality. This paper introduces a privacy-preserving\nrecommender that operates locally using a large language model. The tool\nassists testers in web form testing by suggesting effective field values. This\ntool analyzes the HTML structure of forms, detects input types, and extracts\nconstraints based on each field's type and contextual content, guiding proper\nfield filling.", "AI": {"tldr": "This paper introduces a local, privacy-preserving LLM-powered tool that aids web form testing by intelligently recommending field values, thereby enhancing testing efficiency and protecting confidential data.", "motivation": "Web applications are critical in domains such as education, finance, and e-commerce, necessitating reliable and failure-free performance. Traditional web form testing involves time-consuming and error-prone manual generation of form field values. While LLM-based tools can automate this, they often depend on cloud infrastructure, creating concerns about data confidentiality and privacy when testing sensitive web forms.", "method": "This paper presents a privacy-preserving recommender system powered by a Large Language Model (LLM) that operates locally rather than on the cloud. The tool analyzes web form HTML structure, detects various input types, extracts field constraints based on type and context, and then suggests effective field values for testers.", "result": "The proposed system enables testers to efficiently generate suitable test case values for web forms without compromising confidential data, offering an intelligent, context-aware, and local solution to the problem of web form testing.", "conclusion": "A locally operated LLM-based recommender can assist web form testing by suggesting accurate and context-relevant field values while preserving user privacy and preventing sensitive data leakage."}}
{"id": "2509.01612", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.01612", "abs": "https://arxiv.org/abs/2509.01612", "authors": ["Omur Sahin", "Man Zhang", "Andrea Arcuri"], "title": "WFC/WFD: Web Fuzzing Commons, Dataset and Guidelines to Support Experimentation in REST API Fuzzing", "comment": null, "summary": "Fuzzing REST APIs is an important research problem, with practical\napplications and impact in industry. As such, a lot of research work has been\ncarried out on this topic in the last few years. However, there are three major\nissues that hinder further progress: how to deal with API authentication; how\nto catalog and compare different fault types found by different fuzzers; and\nwhat to use as case study to facilitate fair comparisons among fuzzers. To\naddress these important challenges, we present Web Fuzzing Commons (WFC) and\nWeb Fuzzing Dataset (WFD). WFC is a set of open-source libraries and schema\ndefinitions to declaratively specify authentication info and catalog different\ntypes of faults that fuzzers can automatically detect. WFD is a collection of\n36 open-source APIs with all necessary scaffolding to easily run experiments\nwith fuzzers, supported by WFC. To show the usefulness of WFC/WFD, a set of\nexperiments is carried out with EvoMaster, a state-of-the-art fuzzer for Web\nAPIs. However, any fuzzer can benefit from WFC and WFD. We compare EvoMaster\nwith other state-of-the-art tools such as ARAT-RL, EmRest, LLamaRestTest,\nRESTler, and Schemathesis. We discuss common pitfalls in tool comparisons, as\nwell as providing guidelines with support of WFC/WFD to avoid them.", "AI": {"tldr": "This paper introduces WFC (libraries/schemas for authentication and fault cataloging) and WFD (a REST API benchmarking dataset) to advance API fuzzing research, validated with EvoMaster and compared to major fuzzers, providing tools and guidelines to establish fair, standardized evaluations.", "motivation": "Fuzzing REST APIs has critical industry relevance, but current research faces hurdles in handling authentication, fault cataloging, and conducting fair tool comparisons, hindering further progress.", "method": "Development of open-source libraries (WFC) and dataset (WFD), experimental validation using EvoMaster, and comparative analysis against other state-of-the-art fuzzers.", "result": "WFC and WFD facilitate more effective, standardized fuzzing experiments. Experiments with EvoMaster and comparisons to existing fuzzers highlight benefits and demonstrate guidelines to improve tool benchmarking.", "conclusion": "The introduction of Web Fuzzing Commons (WFC) and Web Fuzzing Dataset (WFD) addresses key challenges in REST API fuzzing, providing tools for better authentication management, fault cataloging, and fair comparison benchmarks."}}
{"id": "2509.01616", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2509.01616", "abs": "https://arxiv.org/abs/2509.01616", "authors": ["Konstantinos Kitsios", "Marco Castelluccio", "Alberto Bacchelli"], "title": "Automated Generation of Issue-Reproducing Tests by Combining LLMs and Search-Based Testing", "comment": "13 pages, 8 figures, accepted for publication (to appear) in the 40th\n  IEEE/ACM International Conference on Automated Software Engineering, ASE 2025", "summary": "Issue-reproducing tests fail on buggy code and pass once a patch is applied,\nthus increasing developers' confidence that the issue has been resolved and\nwill not be re-introduced. However, past research has shown that developers\noften commit patches without such tests, making the automated generation of\nissue-reproducing tests an area of interest. We propose BLAST, a tool for\nautomatically generating issue-reproducing tests from issue-patch pairs by\ncombining LLMs and search-based software testing (SBST). For the LLM part, we\ncomplement the issue description and the patch by extracting relevant context\nthrough git history analysis, static analysis, and SBST-generated tests. For\nthe SBST part, we adapt SBST for generating issue-reproducing tests; the issue\ndescription and the patch are fed into the SBST optimization through an\nintermediate LLM-generated seed, which we deserialize into SBST-compatible\nform. BLAST successfully generates issue-reproducing tests for 151/426 (35.4%)\nof the issues from a curated Python benchmark, outperforming the\nstate-of-the-art (23.5%). Additionally, to measure the real-world impact of\nBLAST, we built a GitHub bot that runs BLAST whenever a new pull request (PR)\nlinked to an issue is opened, and if BLAST generates an issue-reproducing test,\nthe bot proposes it as a comment in the PR. We deployed the bot in three\nopen-source repositories for three months, gathering data from 32 PRs-issue\npairs. BLAST generated an issue-reproducing test in 11 of these cases, which we\nproposed to the developers. By analyzing the developers' feedback, we discuss\nchallenges and opportunities for researchers and tool builders. Data and\nmaterial: https://doi.org/10.5281/zenodo.16949042", "AI": {"tldr": "BLAST effectively combines LLMs and SBST to automatically generate tests that reproduce reported issues from patches, substantially outperforming previous approaches and showing promise in live open-source projects.", "motivation": "Many developers commit patches without accompanying tests that reproduce the issue, risking unresolved or recurring bugs. Automating the generation of such tests improves reliability and confidence in bug resolution.", "method": "Introduces BLAST, which uses large language models (LLMs) and search-based software testing (SBST) to automatically generate issue-reproducing tests from issue-patch pairs. LLMs extract relevant context and seed SBST generation, with SBST optimized using this intermediate seed.", "result": "BLAST generated issue-reproducing tests for 35.4% of issues in a Python benchmark, outperforming the previous state-of-the-art (23.5%). Real-world deployment in three open-source repos saw BLAST propose tests for 11 out of 32 PRs-issue pairs, leading to developer interaction and feedback.", "conclusion": "BLAST advances automated issue-reproducing test generation, outperforming existing tools and gaining traction in real development scenarios. Developer feedback highlights both opportunities for tool improvement and broader adoption."}}
{"id": "2509.01946", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.01946", "abs": "https://arxiv.org/abs/2509.01946", "authors": ["Aarsh Shah", "Cleyton Magalhaes", "Kiev Gama", "Ronnie de Souza Santos"], "title": "Tether: A Personalized Support Assistant for Software Engineers with ADHD", "comment": null, "summary": "Equity, diversity, and inclusion in software engineering often overlook\nneurodiversity, particularly the experiences of developers with Attention\nDeficit Hyperactivity Disorder (ADHD). Despite the growing awareness about that\npopulation in SE, few tools are designed to support their cognitive challenges\n(e.g., sustained attention, task initiation, self-regulation) within\ndevelopment workflows. We present Tether, an LLM-powered desktop application\ndesigned to support software engineers with ADHD by delivering adaptive,\ncontext-aware assistance. Drawing from engineering research methodology, Tether\ncombines local activity monitoring, retrieval-augmented generation (RAG), and\ngamification to offer real-time focus support and personalized dialogue. The\nsystem integrates operating system level system tracking to prompt engagement\nand its chatbot leverages ADHD-specific resources to offer relevant responses.\nPreliminary validation through self-use revealed improved contextual accuracy\nfollowing iterative prompt refinements and RAG enhancements. Tether\ndifferentiates itself from generic tools by being adaptable and aligned with\nsoftware-specific workflows and ADHD-related challenges. While not yet\nevaluated by target users, this work lays the foundation for future\nneurodiversity-aware tools in SE and highlights the potential of LLMs as\npersonalized support systems for underrepresented cognitive needs.", "AI": {"tldr": "The paper introduces Tether, an LLM-powered desktop app to help developers with ADHD focus and manage workflow challenges. The tool combines activity monitoring, RAG, and gamification, and is more aligned to developers' needs than generic solutions. Initial self-evaluation showed improvement, but user studies remain outstanding.", "motivation": "Existing software engineering equity, diversity, and inclusion efforts often neglect neurodiversity, especially ADHD, and few tools are tailored to address the specific cognitive challenges faced by this community.", "method": "The authors used engineering research methodology, including iterative prompt refinement, retrieval-augmented generation (RAG), and self-use validation. The system incorporates local activity monitoring, OS-level tracking, and ADHD-specific resources.", "result": "Preliminary self-use validation showed improved contextual accuracy after refining prompts and RAG, helping support focus and dialogue needs. Tether proved more adaptable and relevant than generic tools, but further evaluation with target users is still needed.", "conclusion": "Tether provides a foundation for developing neurodiversity-aware tools in software engineering, demonstrating the potential for LLMs to deliver personalized support for cognitive challenges like those experienced by developers with ADHD."}}
{"id": "2509.01947", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.01947", "abs": "https://arxiv.org/abs/2509.01947", "authors": ["Mahdi Farzandway", "Fatemeh Ghassemi"], "title": "Automated Repair of C Programs Using Large Language Models", "comment": null, "summary": "This study explores the potential of Large Language Models (LLMs) in\nautomating the repair of C programs. We present a framework that integrates\nspectrum-based fault localization (SBFL), runtime feedback, and\nChain-of-Thought-structured prompting into an autonomous repair loop. Unlike\nprior approaches, our method explicitly combines statistical program analysis\nwith LLM reasoning. The iterative repair cycle leverages a structured\nChain-of-Thought (CoT) prompting approach, where the model reasons over failing\ntests, suspicious code regions, and prior patch outcomes, before generating new\ncandidate patches. The model iteratively changes the code, evaluates the\nresults, and incorporates reasoning from previous attempts into subsequent\nmodifications, reducing repeated errors and clarifying why some bugs remain\nunresolved. Our evaluation spans 3,902 bugs from the Codeflaws benchmark, where\nour approach achieves 44.93% repair accuracy, representing a 3.61% absolute\nimprovement over strong state-of-the-art APR baselines such as GPT-4 with CoT.\nThis outcome highlights a practical pathway toward integrating statistical\nprogram analysis with generative AI in automated debugging.", "AI": {"tldr": "This paper introduces an improved autonomous repair framework for C programs, integrating statistical analysis and LLM reasoning. Using an iterative CoT-structured repair loop, it achieves better bug-fixing accuracy than leading baselines, demonstrating the value of combining program analysis with generative AI.", "motivation": "Automating the repair of C programs is challenging, and current approaches using Large Language Models (LLMs) have limitations. The authors aim to improve automated program repair (APR) by explicitly integrating statistical program analysis with LLM reasoning.", "method": "The paper presents a framework that combines spectrum-based fault localization (SBFL), runtime feedback, and structured Chain-of-Thought (CoT) prompting in an autonomous repair loop. The LLM reasons over failing tests, suspicious code regions, and previous patch outcomes, generating new candidate patches iteratively and learning from previous attempts.", "result": "On the Codeflaws benchmark of 3,902 bugs, the proposed method achieves 44.93% repair accuracy, which is a 3.61% absolute improvement over strong state-of-the-art APR baselines (such as GPT-4 with CoT).", "conclusion": "Integrating statistical program analysis with LLM-based reasoning significantly improves automated repair of C programs, pointing to a practical direction for AI-driven debugging."}}
{"id": "2509.02012", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.02012", "abs": "https://arxiv.org/abs/2509.02012", "authors": ["Katrine Christensen", "Mahsa Varshosaz", "Ra\u00fal Pardo"], "title": "ProbTest: Unit Testing for Probabilistic Programs (Extended Version)", "comment": "Pre-print of paper to appear in the proceedings of the 23nd edition\n  of the International Conference on Software Engineering and Formal Methods\n  (SEFM'25)", "summary": "Testing probabilistic programs is non-trivial due to their stochastic nature.\nGiven an input, the program may produce different outcomes depending on the\nunderlying stochastic choices in the program. This means testing the expected\noutcomes of probabilistic programs requires repeated test executions unlike\ndeterministic programs where a single execution may suffice for each test\ninput. This raises the following question: how many times should we run a\nprobabilistic program to effectively test it? This work proposes a novel\nblack-box unit testing method, ProbTest, for testing the outcomes of\nprobabilistic programs. Our method is founded on the theory surrounding a\nwell-known combinatorial problem, the coupon collector's problem. Using this\nmethod, developers can write unit tests as usual without extra effort while the\nnumber of required test executions is determined automatically with statistical\nguarantees for the results. We implement ProbTest as a plug-in for PyTest, a\nwell-known unit testing tool for python programs. Using this plug-in,\ndevelopers can write unit tests similar to any other Python program and the\nnecessary test executions are handled automatically. We evaluate the method on\ncase studies from the Gymnasium reinforcement learning library and a randomized\ndata structure.", "AI": {"tldr": "ProbTest is a PyTest plug-in that automatically determines and manages the number of test runs needed for probabilistic program testing, ensuring statistically sound results without extra developer work.", "motivation": "Testing probabilistic programs is challenging because their outcomes can vary due to random choices, unlike deterministic programs. Developers struggle to determine how many test executions are needed to effectively verify expected behaviors, making traditional single-execution testing insufficient.", "method": "The paper introduces ProbTest, a black-box unit testing method based on the coupon collector's problem, to statistically guarantee the coverage of possible outcomes. ProbTest automatically determines how many times a probabilistic program needs to be executed for effective testing, minimizing required manual intervention. ProbTest is implemented as a plug-in for PyTest, allowing seamless integration with Python unit testing workflows.", "result": "ProbTest automates the execution of probabilistic tests and provides statistical guarantees for outcome coverage. Case studies using the Gymnasium reinforcement learning library and a randomized data structure demonstrate the method's effectiveness. The approach enables developers to write unit tests as usual while transparently handling the required number of executions.", "conclusion": "ProbTest successfully enables robust, automated testing of probabilistic programs by leveraging the coupon collector's problem to determine the number of necessary test executions. This reduces developer effort and ensures statistical reliability in test outcomes."}}
{"id": "2509.02022", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.02022", "abs": "https://arxiv.org/abs/2509.02022", "authors": ["Bj\u00f8rnar Haugstad J\u00e5tten", "Simon Boye J\u00f8rgensen", "Rasmus Petersen", "Ra\u00fal Pardo"], "title": "Scalable Thread-Safety Analysis of Java Classes with CodeQL", "comment": null, "summary": "In object-oriented languages software developers rely on thread-safe classes\nto implement concurrent applications. However, determining whether a class is\nthread-safe is a challenging task. This paper presents a highly scalable method\nto analyze thread-safety in Java classes. We provide a definition of\nthread-safety for Java classes founded on the correctness principle of the Java\nmemory model, data race freedom. We devise a set of properties for Java classes\nthat are proven to ensure thread-safety. We encode these properties in the\nstatic analysis tool CodeQL to automatically analyze Java source code. We\nperform an evaluation on the top 1000 GitHub repositories. The evaluation\ncomprises 3632865 Java classes; with 1992 classes annotated as @ThreadSafe from\n71 repositories. These repositories include highly popular software such as\nApache Flink (24.6k stars), Facebook Fresco (17.1k stars), PrestoDB (16.2k\nstarts), and gRPC (11.6k starts). Our queries detected thousands of\nthread-safety errors. The running time of our queries is below 2 minutes for\nrepositories up to 200k lines of code, 20k methods, 6000 fields, and 1200\nclasses. We have submitted a selection of detected concurrency errors as PRs,\nand developers positively reacted to these PRs. We have submitted our CodeQL\nqueries to the main CodeQL repository, and they are currently in the process of\nbecoming available as part of GitHub actions. The results demonstrate the\napplicability and scalability of our method to analyze thread-safety in\nreal-world code bases.", "AI": {"tldr": "The paper introduces a scalable CodeQL-based analyzer for thread-safety in Java classes, accurately identifying thousands of concurrency issues across major open-source projects, with fast runtimes and adoption by developers.", "motivation": "Determining thread-safety in Java classes is challenging for software developers, yet increasingly necessary for building robust concurrent applications. Existing methods or manual analysis do not scale well to large codebases, creating a need for an automated, highly scalable approach.", "method": "The authors define thread-safety for Java classes based on the correctness principle of the Java memory model, specifically data race freedom. They devise a set of properties proven to ensure thread-safety and encode these properties in CodeQL, a static analysis tool, for automated source code analysis.", "result": "Applied to the top 1000 GitHub repositories, analyzing over 3.6 million classes (including nearly 2,000 already annotated as @ThreadSafe), the method detected thousands of thread-safety errors. The queries ran efficiently (under 2 minutes for large repos) and several detected errors were validated and accepted by project maintainers. The CodeQL queries are becoming part of mainstream tooling.", "conclusion": "The proposed method and CodeQL-based tool can effectively and efficiently analyze thread-safety in large-scale, real-world Java codebases, demonstrated by the successful identification of thousands of concurrency issues and positive developer feedback."}}
{"id": "2509.02025", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.02025", "abs": "https://arxiv.org/abs/2509.02025", "authors": ["Junda He", "Zhou Yang", "Jieke Shi", "Chengran Yang", "Kisub Kim", "Bowen Xu", "Xin Zhou", "David Lo"], "title": "Curiosity-Driven Testing for Sequential Decision-Making Process", "comment": "Update the Replication Package URL", "summary": "Sequential decision-making processes (SDPs) are fundamental for complex\nreal-world challenges, such as autonomous driving, robotic control, and traffic\nmanagement. While recent advances in Deep Learning (DL) have led to mature\nsolutions for solving these complex problems, SDMs remain vulnerable to\nlearning unsafe behaviors, posing significant risks in safety-critical\napplications. However, developing a testing framework for SDMs that can\nidentify a diverse set of crash-triggering scenarios remains an open challenge.\nTo address this, we propose CureFuzz, a novel curiosity-driven black-box fuzz\ntesting approach for SDMs. CureFuzz proposes a curiosity mechanism that allows\na fuzzer to effectively explore novel and diverse scenarios, leading to\nimproved detection of crashtriggering scenarios. Additionally, we introduce a\nmulti-objective seed selection technique to balance the exploration of novel\nscenarios and the generation of crash-triggering scenarios, thereby optimizing\nthe fuzzing process. We evaluate CureFuzz on various SDMs and experimental\nresults demonstrate that CureFuzz outperforms the state-of-the-art method by a\nsubstantial margin in the total number of faults and distinct types of\ncrash-triggering scenarios. We also demonstrate that the crash-triggering\nscenarios found by CureFuzz can repair SDMs, highlighting CureFuzz as a\nvaluable tool for testing SDMs and optimizing their performance.", "AI": {"tldr": "CureFuzz is a curiosity-driven fuzz testing approach for sequential decision-making models that significantly improves the detection of diverse and novel crash-triggering scenarios, outperforming existing tools and enabling system repairs for safer applications.", "motivation": "Sequential decision-making models are increasingly applied in safety-critical domains, such as autonomous driving and robotics, where unsafe behavior can have severe consequences. Despite advancements in deep learning, current models are still prone to unsafe decisions, and effective testing frameworks to uncover diverse failure modes are lacking.", "method": "The authors introduce CureFuzz, a curiosity-driven black-box fuzz testing framework specifically designed for Sequential Decision-Making Models (SDMs). CureFuzz incorporates a curiosity mechanism to guide the exploration toward novel scenarios and employs a multi-objective seed selection strategy to balance finding new scenarios and triggering crashes efficiently.", "result": "Experimental results show that CureFuzz surpasses state-of-the-art fuzzing methods both in the total number of faults found and the diversity of crash-triggering scenarios identified. Additionally, the scenarios discovered by CureFuzz allow for the repair and improvement of SDMs, demonstrating practical utility.", "conclusion": "CureFuzz is an effective and innovative testing tool for SDMs, capable of uncovering more and varied crash scenarios than previous methods. Its findings can directly contribute to the safety and reliability improvements of SDMs in critical applications."}}
{"id": "2509.02150", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.02150", "abs": "https://arxiv.org/abs/2509.02150", "authors": ["Pin Ji", "Yang Feng", "Zongtai Li", "Xiangchi Zhou", "Jia Liu", "Jun Sun", "Zhihong Zhao"], "title": "Txt2Sce: Scenario Generation for Autonomous Driving System Testing Based on Textual Reports", "comment": null, "summary": "With the rapid advancement of deep learning and related technologies,\nAutonomous Driving Systems (ADSs) have made significant progress and are\ngradually being widely applied in safety-critical fields. However, numerous\naccident reports show that ADSs still encounter challenges in complex\nscenarios. As a result, scenario-based testing has become essential for\nidentifying defects and ensuring reliable performance. In particular,\nreal-world accident reports offer valuable high-risk scenarios for more\ntargeted ADS testing. Despite their potential, existing methods often rely on\nvisual data, which demands large memory and manual annotation. Additionally,\nsince existing methods do not adopt standardized scenario formats (e.g.,\nOpenSCENARIO), the generated scenarios are often tied to specific platforms and\nADS implementations, limiting their scalability and portability. To address\nthese challenges, we propose Txt2Sce, a method for generating test scenarios in\nOpenSCENARIO format based on textual accident reports. Txt2Sce first uses a LLM\nto convert textual accident reports into corresponding OpenSCENARIO scenario\nfiles. It then generates a derivation-based scenario file tree through scenario\ndisassembly, scenario block mutation, and scenario assembly. By utilizing the\nderivation relationships between nodes in the scenario tree, Txt2Sce helps\ndevelopers identify the scenario conditions that trigger unexpected behaviors\nof ADSs. In the experiments, we employ Txt2Sce to generate 33 scenario file\ntrees, resulting in a total of 4,373 scenario files for testing the open-source\nADS, Autoware. The experimental results show that Txt2Sce successfully converts\ntextual reports into valid OpenSCENARIO files, enhances scenario diversity\nthrough mutation, and effectively detects unexpected behaviors of Autoware in\nterms of safety, smartness, and smoothness.", "AI": {"tldr": "Txt2Sce uses a LLM to automatically turn textual accident reports into standardized test scenarios in OpenSCENARIO format, making scenario-based ADS testing more scalable and effective by increasing diversity and uncovering system defects.", "motivation": "Autonomous Driving Systems (ADSs) face challenges in complex scenarios, as highlighted by real-world accident reports. Existing scenario-based testing methods are limited by reliance on visual data (which requires heavy annotation and storage) and lack of standardized scenario formats, making the results platform-specific and not easily portable.", "method": "The authors propose a method called Txt2Sce, which employs a Large Language Model (LLM) to automatically convert textual accident reports into test scenarios in the OpenSCENARIO format. The method includes scenario disassembly, mutation, and re-assembly to generate a derivation-based scenario file tree, enhancing diversity and traceability of scenario variants.", "result": "Txt2Sce generated 33 scenario tree files, totaling 4,373 scenario files, which were used to test the open-source ADS, Autoware. The tool effectively created valid OpenSCENARIO files, increased scenario diversity, and successfully revealed unexpected behaviors in Autoware regarding safety, smartness, and smoothness.", "conclusion": "Txt2Sce offers an efficient and scalable approach to generate diverse, standardized test scenarios for ADSs from textual accident reports, improving testing effectiveness and portability across different platforms."}}
{"id": "2509.02221", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.02221", "abs": "https://arxiv.org/abs/2509.02221", "authors": ["Martin Skoglund", "Fredrik Warg", "Anders Thors\u00e9n", "Sasikumar Punnekkat", "Hans Hansson"], "title": "Formalizing Operational Design Domains with the Pkl Language", "comment": "8 pages, 9 figures, IV 2025", "summary": "The deployment of automated functions that can operate without direct human\nsupervision has changed safety evaluation in domains seeking higher levels of\nautomation. Unlike conventional systems that rely on human operators, these\nfunctions require new assessment frameworks to demonstrate that they do not\nintroduce unacceptable risks under real-world conditions. To make a convincing\nsafety claim, the developer must present a thorough justification argument,\nsupported by evidence, that a function is free from unreasonable risk when\noperated in its intended context. The key concept relevant to the presented\nwork is the intended context, often captured by an Operational Design Domain\nspecification (ODD). ODD formalization is challenging due to the need to\nmaintain flexibility in adopting diverse specification formats while preserving\nconsistency and traceability and integrating seamlessly into the development,\nvalidation, and assessment. This paper presents a way to formalize an ODD in\nthe Pkl language, addressing central challenges in specifying ODDs while\nimproving usability through specialized configuration language features. The\napproach is illustrated with an automotive example but can be broadly applied\nto ensure rigorous assessments of operational contexts.", "AI": {"tldr": "This paper proposes a novel way to formalize operational design domains for automated functions using the Pkl language, addressing usability and traceability challenges, and demonstrates its effectiveness in an automotive scenario.", "motivation": "With increasing automation and reduced human oversight, traditional safety evaluation methods are inadequate. There is a need for a robust, flexible, and traceable framework to specify and assess automated functions' contextual operation to mitigate risks.", "method": "The paper introduces a formalization framework for ODDs using the Pkl configuration language, addressing integration, specification, and validation challenges. An automotive scenario demonstrates its practical application.", "result": "The method provides a consistent yet flexible way to specify ODDs, simplifies configuration, and improves the support for safety justification and assessment. Its broad application potential is highlighted via an automotive use case.", "conclusion": "The proposed approach effectively formalizes Operational Design Domain (ODD) specifications in the Pkl language, improving consistency, traceability, and usability while supporting diverse formats."}}
{"id": "2509.02311", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.02311", "abs": "https://arxiv.org/abs/2509.02311", "authors": ["Martin Skoglund", "Fredrik Warg", "Anders Thoren", "Sasikumar Punnekkat", "Hans Hansson"], "title": "Methodology for Test Case Allocation based on a Formalized ODD", "comment": "12 pages, 8 figures, DECSoS, SAFECOMP 2025", "summary": "The emergence of Connected, Cooperative, and Automated Mobility (CCAM)\nsystems has significantly transformed the safety assessment landscape. Because\nthey integrate automated vehicle functions beyond those managed by a human\ndriver, new methods are required to evaluate their safety. Approaches that\ncompile evidence from multiple test environments have been proposed for\ntype-approval and similar evaluations, emphasizing scenario coverage within the\nsystems Operational Design Domain (ODD). However, aligning diverse test\nenvironment requirements with distinct testing capabilities remains\nchallenging. This paper presents a method for evaluating the suitability of\ntest case allocation to various test environments by drawing on and extending\nan existing ODD formalization with key testing attributes. The resulting\nconstruct integrates ODD parameters and additional test attributes to capture a\ngiven test environments relevant capabilities. This approach supports automatic\nsuitability evaluation and is demonstrated through a case study on an automated\nreversing truck function. The system's implementation fidelity is tied to ODD\nparameters, facilitating automated test case allocation based on each\nenvironments capacity for object-detection sensor assessment.", "AI": {"tldr": "The paper presents a framework for automatically aligning test cases with appropriate test environments for CCAM systems by extending ODDs with testing attributes, demonstrated via a reversing truck case study.", "motivation": "The introduction of Connected, Cooperative, and Automated Mobility (CCAM) systems changes how safety is assessed, as these involve automated functions beyond human driving. Traditional evaluation methods are inadequate for such complex systems. There is a need to properly align and allocate test cases across varied testing environments considering their specific capabilities.", "method": "The paper proposes a method that extends an existing Operational Design Domain (ODD) formalization with specific testing attributes. This construct integrates ODD parameters and additional test attributes to represent the relevant capabilities of different test environments. The approach supports automated suitability evaluation and is showcased using a case study on an automated reversing truck function.", "result": "The proposed approach enables automatic evaluation of how suitable specific test environments are for particular test case allocations. It demonstrates, via a case study, that test case allocation can be automated based on each environment\u2019s capability (e.g., object-detection sensor assessment).", "conclusion": "Extending ODDs with key test attributes creates a robust method for test case allocation in the safety assessment of CCAM systems, facilitating automatic suitability evaluation based on the strengths of available test environments."}}
{"id": "2509.02330", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.02330", "abs": "https://arxiv.org/abs/2509.02330", "authors": ["Yicong Zhao", "Shisong Chen", "Jiacheng Zhang", "Zhixu Li"], "title": "ReCode: Improving LLM-based Code Repair with Fine-Grained Retrieval-Augmented Generation", "comment": "Accepted by CIKM 2025", "summary": "Recent advances in large language models (LLMs) have demonstrated impressive\ncapabilities in code-related tasks, such as code generation and automated\nprogram repair. Despite their promising performance, most existing approaches\nfor code repair suffer from high training costs or computationally expensive\ninference. Retrieval-augmented generation (RAG), with its efficient in-context\nlearning paradigm, offers a more scalable alternative. However, conventional\nretrieval strategies, which are often based on holistic code-text embeddings,\nfail to capture the structural intricacies of code, resulting in suboptimal\nretrieval quality. To address the above limitations, we propose ReCode, a\nfine-grained retrieval-augmented in-context learning framework designed for\naccurate and efficient code repair. Specifically, ReCode introduces two key\ninnovations: (1) an algorithm-aware retrieval strategy that narrows the search\nspace using preliminary algorithm type predictions; and (2) a modular\ndual-encoder architecture that separately processes code and textual inputs,\nenabling fine-grained semantic matching between input and retrieved contexts.\nFurthermore, we propose RACodeBench, a new benchmark constructed from\nreal-world user-submitted buggy code, which addresses the limitations of\nsynthetic benchmarks and supports realistic evaluation. Experimental results on\nRACodeBench and competitive programming datasets demonstrate that ReCode\nachieves higher repair accuracy with significantly reduced inference cost,\nhighlighting its practical value for real-world code repair scenarios.", "AI": {"tldr": "This paper presents ReCode\u2014a retrieval-augmented code repair framework addressing inefficiencies in current LLM approaches by leveraging algorithm-aware and modular retrieval strategies. With a new, realistic code repair benchmark (RACodeBench), ReCode shows superior accuracy and efficiency, making it well-suited for practical code repair tasks.", "motivation": "Existing LLM-based code repair methods require high training and computational resources, and retrieval strategies often fail to capture code's structural details, limiting repair accuracy.", "method": "The paper introduces ReCode, a framework that uses fine-grained retrieval-augmented in-context learning for code repair. It employs an algorithm-aware retrieval strategy and a modular dual-encoder architecture for separate semantic processing of code and text. Additionally, the study proposes RACodeBench, a new benchmark sourced from real-world buggy code.", "result": "ReCode achieves higher code repair accuracy and reduces inference cost compared to existing methods, as demonstrated by experiments on RACodeBench and competitive programming datasets.", "conclusion": "ReCode presents an effective and efficient solution for code repair applying fine-grained retrieval mechanisms, and its success on realistic benchmarks underscores its practical utility."}}
