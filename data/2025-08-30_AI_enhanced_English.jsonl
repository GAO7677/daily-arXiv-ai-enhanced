{"id": "2508.20365", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.20365", "abs": "https://arxiv.org/abs/2508.20365", "authors": ["Naoki Kobayashi", "Ryosuke Sato", "Ayumi Shinohara", "Ryo Yoshinaka"], "title": "Solvable Tuple Patterns and Their Applications to Program Verification", "comment": null, "summary": "Despite the recent progress of automated program verification techniques,\nfully automated verification of programs manipulating recursive data structures\nremains a challenge. We introduce the notion of solvable tuple patterns (STPs)\nto express invariants between list-like recursive data structures. A\ndistinguishing feature of STPs is that they can be efficiently inferred from\nonly a small number of positive samples; no negative samples are required. An\nSMT solver that supports the sequence theory can be used to check that an\ninferred STP is indeed an inductive invariant. After presenting basic\nproperties of STPs and an STP inference algorithm, we show how to incorporate\nthe STP inference into a CHC (Constrained Horn Clauses) solver supporting\nlist-like data structures, which serves as a uniform backend for automated\nprogram verification tools. A CHC solver incorporating the STP inference has\nwon the ADT-LIN category of CHC-COMP 2025 by a big margin.", "AI": {"tldr": "This paper introduces a novel invariant inference technique for programs manipulating list-like recursive data structures. By leveraging solvable tuple patterns, it enables efficient invariant discovery with only positive examples, and strengthens automated verification tools, demonstrated by a significant win in the CHC-COMP 2025 competition.", "motivation": "Automated verification of programs handling recursive data structures is difficult and current solutions are still limited, especially for complex invariants in list-like structures.", "method": "The authors introduce solvable tuple patterns (STPs) as a new way to express invariants for list-like recursive data structures. They present an inference algorithm for STPs which only needs positive samples, and connect this method to SMT (Satisfiability Modulo Theories) solvers that use sequence theory. They further integrate the inference approach into a CHC solver supporting list-like structures.", "result": "The method enables efficient and robust inference of invariants without requiring negative samples. Integration with a CHC solver shows superior performance and contributed to winning the ADT-LIN category of CHC-COMP 2025.", "conclusion": "STPs provide an effective approach to inferring invariants in recursive data structures, improving the capabilities of program verification tools and outperforming prior systems."}}
{"id": "2508.20922", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.20922", "abs": "https://arxiv.org/abs/2508.20922", "authors": ["Markus B\u00f6ck", "J\u00fcrgen Cito"], "title": "Static Factorisation of Probabilistic Programs With User-Labelled Sample Statements and While Loops", "comment": null, "summary": "It is commonly known that any Bayesian network can be implemented as a\nprobabilistic program, but the reverse direction is not so clear. In this work,\nwe address the open question to what extent a probabilistic program with\nuser-labelled sample statements and while loops - features found in languages\nlike Gen, Turing, and Pyro - can be represented graphically. To this end, we\nextend existing operational semantics to support these language features. By\ntranslating a program to its control-flow graph, we define a sound static\nanalysis that approximates the dependency structure of the random variables in\nthe program. As a result, we obtain a static factorisation of the implicitly\ndefined program density, which is equivalent to the known Bayesian network\nfactorisation for programs without loops and constant labels, but constitutes a\nnovel graphical representation for programs that define an unbounded number of\nrandom variables via loops or dynamic labels. We further develop a sound\nprogram slicing technique to leverage this structure to statically enable three\nwell-known optimisations for the considered program class: we reduce the\nvariance of gradient estimates in variational inference and we speed up both\nsingle-site Metropolis Hastings and sequential Monte Carlo. These optimisations\nare proven correct and empirically shown to match or outperform existing\ntechniques.", "AI": {"tldr": "This paper shows that complex probabilistic programs with loops can be represented and analyzed graphically, extending Bayesian network concepts. The approach enables correct, empirically verified optimizations for inference tasks, matching or surpassing current methods.", "motivation": "There is a clear need to understand whether complex probabilistic programs\u2014with features like user-labelled sample statements and while loops\u2014can have an interpretable graphical representation similar to Bayesian networks. Existing solutions do not cover dynamic program constructs, leaving open questions on their analysis and optimization.", "method": "The authors extend operational semantics to accommodate user-labelled sample statements and while loops, then translate probabilistic programs into control-flow graphs. With this, they perform static analysis to approximate dependencies among random variables, and introduce a novel static factorization technique. They also develop a sound program slicing method for leveraging this structure in inference optimization.", "result": "The static factorization coincides with classic Bayesian network structures for loop-less, constant-labelled programs, but introduces a new graphical representation for more dynamic programs. The slicing technique enables three proven optimizations: reduced variance in gradient estimates for variational inference, and improved efficiency in single-site Metropolis Hastings and sequential Monte Carlo algorithms. These optimizations are both theoretically validated and empirically shown to be effective.", "conclusion": "Probabilistic programs with loops and dynamic structure can be analyzed graphically via sound static analysis utilizing control-flow graphs. This enables the application of classic and novel inference optimizations, making complex probabilistic programs more understandable and efficient to execute."}}
{"id": "2508.20340", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.20340", "abs": "https://arxiv.org/abs/2508.20340", "authors": ["Maolin Sun", "Yibiao Yang", "Yuming Zhou"], "title": "Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce Formula Generators", "comment": null, "summary": "Satisfiability Modulo Theory (SMT) solvers are foundational to modern systems\nand programming languages research, providing the foundation for tasks like\nsymbolic execution and automated verification. Because these solvers sit on the\ncritical path, their correctness is essential, and high-quality test formulas\nare key to uncovering bugs. However, while prior testing techniques performed\nwell on earlier solver versions, they struggle to keep pace with rapidly\nevolving features. Recent approaches based on Large Language Models (LLMs) show\npromise in exploring advanced solver capabilities, but two obstacles remain:\nnearly half of the generated formulas are syntactically invalid, and iterative\ninteractions with the LLMs introduce substantial computational overhead. In\nthis study, we present Chimera, a novel LLM-assisted fuzzing framework that\naddresses both issues by shifting from direct formula generation to the\nsynthesis of reusable term (i.e., logical expression) generators. Particularly,\nChimera uses LLMs to (1) automatically extract context-free grammars (CFGs) for\nSMT theories, including solver-specific extensions, from documentation, and (2)\nsynthesize composable Boolean term generators that adhere to these grammars.\nDuring fuzzing, Chimera populates structural skeletons derived from existing\nformulas with the terms iteratively produced by the LLM-synthesized generators.\nThis design ensures syntactic validity while promoting semantic diversity.\nNotably, Chimera requires only one-time LLM interaction investment,\ndramatically reducing runtime cost. We evaluated Chimera on two leading SMT\nsolvers: Z3 and cvc5. Our experiments show that Chimera has identified 43\nconfirmed bugs, 40 of which have already been fixed by developers.", "AI": {"tldr": "Chimera is a new LLM-assisted fuzzing tool for SMT solvers that generates syntactically valid and diverse test formulas more efficiently by using grammar-based term synthesis. It detected dozens of real bugs in Z3 and cvc5, with most quickly fixed, demonstrating practical impact.", "motivation": "SMT solvers are essential for verification tasks in systems and programming languages, but their correctness depends on high-quality test formulas. Existing methods for generating those formulas fall short with new solver features, and LLM-based approaches suffer from syntactic errors and high computational costs.", "method": "This paper presents Chimera, an LLM-assisted fuzzing framework. Instead of generating formulas directly via LLMs, Chimera uses LLMs to extract context-free grammars (CFGs) from documentation and synthesize reusable Boolean term generators. These generators ensure syntactic validity and composability, allowing Chimera to populate skeletons of formulas during fuzzing, with only a single LLM interaction required.", "result": "Chimera was evaluated on the SMT solvers Z3 and cvc5. It successfully discovered 43 confirmed bugs, 40 of which have been fixed by developers, demonstrating improved efficacy and efficiency compared to prior approaches.", "conclusion": "Chimera overcomes previous limitations of LLM-based testing by focusing on grammar-based term generation rather than direct formula synthesis, yielding syntactic validity, semantic diversity, and lower computational cost. Its use has led to the discovery and fixing of numerous bugs in popular SMT solvers."}}
{"id": "2508.20119", "categories": ["cs.SE", "cs.LG", "68T42", "I.2.6; I.2.2; D.2.2"], "pdf": "https://arxiv.org/pdf/2508.20119", "abs": "https://arxiv.org/abs/2508.20119", "authors": ["Daniel M. Yellin"], "title": "Evaluating LLMs on microservice-based applications: how complex is your specification?", "comment": "20 pages + 7 pages appendices. 7 Figures. 8 Tables", "summary": "In this paper we evaluate how far LLMs have advanced in generating code for\nreal-world problems. Specifically, we explore code synthesis for\nmicroservice-based applications, a widely used architecture pattern. We define\na standard template for specifying these applications, and we propose a metric\nfor judging the difficulty level of a specification. The higher the score, the\nmore difficult it is to generate code for the specification. We develop a\nframework to automate the process of testing LLM-synthesized code for a\nmicroservice using unit tests. Our experimental results show that strong LLMs\n(like GPT-3o-mini) do fairly well on medium difficulty specifications but do\nvery poorly on those of higher difficulty levels. This is due to more intricate\nbusiness logic, a greater use of external services, database integration and\ninclusion of non-functional capabilities such as authentication. We analyzed\nthe errors in LLM-synthesized code and report on the key challenges LLMs face\nin generating code for these specifications thereby suggesting future research\ndirections to improve code synthesis for real-world problems.", "AI": {"tldr": "LLMs can generate code for microservice applications of moderate complexity but struggle with more difficult, real-world tasks due to advanced requirements. The paper proposes a framework to benchmark and analyze these limitations, offering insights for further research.", "motivation": "Large Language Models (LLMs) are increasingly used for code generation, but their ability to handle real-world, complex tasks like microservice-based applications is unclear. The paper is motivated to assess LLM capability in this critical and practical coding domain.", "method": "The paper defines a standard template for microservice-based applications, proposes a new metric to assess specification difficulty, and develops a framework to automatically test LLM-generated code using unit tests. It then conducts experiments on LLMs like GPT-3o-mini, analyzing performance at different difficulty levels and reporting specific error types.", "result": "LLMs like GPT-3o-mini can adequately generate code for medium-difficulty microservice specifications, but their performance drops sharply as problem complexity increases due to challenges with advanced business logic, external services, database integration, and non-functional requirements.", "conclusion": "While current strong LLMs show promise, significant limitations remain for complex, real-world code synthesis tasks. Understanding the sources of LLM errors can guide future research to improve their practical programming capabilities."}}
{"id": "2508.20124", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20124", "abs": "https://arxiv.org/abs/2508.20124", "authors": ["Yunlong Feng", "Yang Xu", "Xiao Xu", "Binyuan Hui", "Junyang Lin"], "title": "Towards Better Correctness and Efficiency in Code Generation", "comment": null, "summary": "While code large language models have demonstrated remarkable progress in\ncode generation, the generated code often exhibits poor runtime efficiency,\nlimiting its practical application in performance-sensitive scenarios. To\naddress this limitation, we propose an efficiency-oriented reinforcement\nlearning framework guided by a novel performance reward. Based on this\nframework, we take a deeper dive into the code efficiency problem, identifying\nthen proposing methods to overcome key bottlenecks: (1) Dynamic exploration\novercomes the static data constraints of offline fine-tuning, enabling the\ndiscovery of more efficient code implementations. (2) The error-insensitive\nreinforcement learning method and high-contrast efficiency signals are crucial\nfor mitigating systematic errors and achieving effective optimization. (3)\nOnline exploration is most effective when starting from a high-correctness\nbaseline, as this allows for efficiency improvements without sacrificing\naccuracy. With these discoveries, we finally propose a two-stage tuning method,\nwhich achieves high and balanced performance across correctness and efficiency.\nThe results of experiments show the effectiveness of the method, which improves\ncode correctness by 10.18\\% and runtime efficiency by 7.75\\% on a 7B model,\nachieving performance comparable to much larger model.", "AI": {"tldr": "The paper presents a two-stage RL-based tuning method that significantly improves both the accuracy and runtime efficiency of code generated by large language models, making smaller models perform comparably to larger ones.", "motivation": "Code generation with large language models often produces inefficient code at runtime, which is problematic for performance-sensitive applications. Existing methods based on offline fine-tuning are limited by static data and cannot explore more efficient implementations.", "method": "The authors introduce an efficiency-oriented reinforcement learning framework with a novel performance reward. Key contributions include dynamic exploration to surpass static data constraints, an error-insensitive RL approach aided by high-contrast efficiency signals, and a two-stage tuning process starting from a highly correct baseline to further optimize for efficiency.", "result": "The proposed method improves code correctness by 10.18% and runtime efficiency by 7.75% on a 7B model, yielding results comparable to much larger models.", "conclusion": "The efficiency-oriented RL framework effectively enhances both correctness and runtime efficiency of code generated by large language models, offering a practical approach for deploying code LLMs in performance-sensitive settings."}}
{"id": "2508.20370", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20370", "abs": "https://arxiv.org/abs/2508.20370", "authors": ["Lingzhe Zhang", "Tong Jia", "Kangjin Wang", "Weijie Hong", "Chiming Duan", "Minghua He", "Ying Li"], "title": "Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought", "comment": null, "summary": "As contemporary microservice systems become increasingly popular and\ncomplex-often comprising hundreds or even thousands of fine-grained,\ninterdependent subsystems-they are facing more frequent failures. Ensuring\nsystem reliability thus demands accurate root cause localization. While traces\nand metrics have proven to be effective data sources for this task, existing\nmethods either heavily rely on pre-defined schemas, which struggle to adapt to\nevolving operational contexts, or lack interpretability in their reasoning\nprocess, thereby leaving Site Reliability Engineers (SREs) confused. In this\npaper, we conduct a comprehensive study on how SREs localize the root cause of\nfailures, drawing insights from multiple professional SREs across different\norganizations. Our investigation reveals that human root cause analysis\nexhibits three key characteristics: recursiveness, multi-dimensional expansion,\nand cross-modal reasoning. Motivated by these findings, we introduce RCLAgent,\nan adaptive root cause localization method for microservice systems that\nleverages a multi-agent recursion-of-thought framework. RCLAgent employs a\nnovel recursion-of-thought strategy to guide the LLM's reasoning process,\neffectively integrating data from multiple agents and tool-assisted analysis to\naccurately pinpoint the root cause. Experimental evaluations on various public\ndatasets demonstrate that RCLAgent achieves superior performance by localizing\nthe root cause using only a single request-outperforming state-of-the-art\nmethods that depend on aggregating multiple requests. These results underscore\nthe effectiveness of RCLAgent in enhancing the efficiency and precision of root\ncause localization in complex microservice environments.", "AI": {"tldr": "This paper studies how SREs identify root causes of failures in microservice systems and introduces RCLAgent, an adaptive, interpretable framework using multi-agent LLM reasoning. RCLAgent surpasses prior methods in accuracy and efficiency by pinpointing root causes from just one request.", "motivation": "Modern microservice architectures are highly complex and experience frequent failures. Existing root cause localization methods are either rigid due to reliance on pre-defined schemas or are hard for SREs to interpret, leaving room for improvement in adaptability and explainability.", "method": "The authors conducted a comprehensive study with professional SREs to understand their root cause analysis process, identifying its key characteristics: recursiveness, multi-dimensional expansion, and cross-modal reasoning. Based on these findings, they propose RCLAgent, an adaptive root cause localization method that uses a multi-agent recursion-of-thought framework, leveraging LLMs, multi-agent collaboration, and tool-assisted analyses.", "result": "Experimental results on public datasets show that RCLAgent outperforms current state-of-the-art methods by successfully locating root causes from a single request, unlike others that require multiple requests. This demonstrates greater efficiency and accuracy in complex microservice environments.", "conclusion": "RCLAgent delivers more effective and precise root cause localization for large-scale microservice systems, increasing both efficiency and interpretability thanks to its adaptive, multi-agent recursion-of-thought approach."}}
{"id": "2508.20563", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20563", "abs": "https://arxiv.org/abs/2508.20563", "authors": ["Zheying Zhang", "Tomas Herda", "Victoria Pichler", "Pekka Abrahamsson", "Geir K. Hanssen", "Joshua Kerievsky", "Alex Polyakov", "Mohit Chandna", "Marius Irgens", "Kai-Kristian Kemell", "Ayman Asad Khan", "Crystal Kwok", "Evan Leybourn", "Munish Malik", "Dorota Mleczko", "Morteza Moalagh", "Christopher Morales", "Yuliia Pieskova", "Daniel Plan\u00f6tscher", "Mika Saari", "Anastasiia Tkalich", "Karl Josef Gstettner", "Xiaofeng Wang"], "title": "AI and Agile Software Development: A Research Roadmap from the XP2025 Workshop", "comment": null, "summary": "This paper synthesizes the key findings from a full-day XP2025 workshop on\n\"AI and Agile: From Frustration to Success\", held in Brugg-Windisch,\nSwitzerland. The workshop brought together over 30 interdisciplinary academic\nresearchers and industry practitioners to tackle the concrete challenges and\nemerging opportunities at the intersection of Generative Artificial\nIntelligence (GenAI) and agile software development. Through structured,\ninteractive breakout sessions, participants identified shared pain points like\ntool fragmentation, governance, data quality, and critical skills gaps in AI\nliteracy and prompt engineering. These issues were further analyzed, revealing\nunderlying causes and cross-cutting concerns. The workshop concluded by\ncollaboratively co-creating a multi-thematic research roadmap, articulating\nboth short-term, implementable actions and visionary, long-term research\ndirections. This cohesive agenda aims to guide future investigation and drive\nthe responsible, human-centered integration of GenAI into agile practices.", "AI": {"tldr": "A workshop with academics and practitioners discussed challenges and opportunities of using Generative AI in agile development. Key barriers like tool fragmentation and lack of skills were mapped, and a research roadmap was co-created to guide future work towards responsibly using GenAI in agile practices.", "motivation": "To address the challenges and opportunities arising from the integration of Generative AI (GenAI) into agile software development processes, where existing pain points are poorly understood.", "method": "A full-day interdisciplinary workshop was conducted with over 30 academic and industry participants, featuring structured and interactive breakout sessions to identify, analyze, and prioritize issues at the intersection of GenAI and agile development.", "result": "Key shared challenges such as tool fragmentation, governance, data quality, AI literacy, and prompt engineering skills gaps were identified, analyzed, and synthesized into a collaboratively developed, multi-thematic research roadmap outlining both short-term actionable steps and long-term strategic goals.", "conclusion": "The paper presents a unified research agenda designed to support the responsible and effective integration of GenAI in agile development, providing guidance for both immediate and future research efforts."}}
{"id": "2508.20737", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20737", "abs": "https://arxiv.org/abs/2508.20737", "authors": ["Wei Ma", "Yixiao Yang", "Qiang Hu", "Shi Ying", "Zhi Jin", "Bo Du", "Zhenchang Xing", "Tianlin Li", "Junjie Shi", "Yang Liu", "Linxiao Jiang"], "title": "Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol", "comment": null, "summary": "Applications of Large Language Models~(LLMs) have evolved from simple text\ngenerators into complex software systems that integrate retrieval augmentation,\ntool invocation, and multi-turn interactions. Their inherent non-determinism,\ndynamism, and context dependence pose fundamental challenges for quality\nassurance. This paper decomposes LLM applications into a three-layer\narchitecture: \\textbf{\\textit{System Shell Layer}}, \\textbf{\\textit{Prompt\nOrchestration Layer}}, and \\textbf{\\textit{LLM Inference Core}}. We then assess\nthe applicability of traditional software testing methods in each layer:\ndirectly applicable at the shell layer, requiring semantic reinterpretation at\nthe orchestration layer, and necessitating paradigm shifts at the inference\ncore. A comparative analysis of Testing AI methods from the software\nengineering community and safety analysis techniques from the AI community\nreveals structural disconnects in testing unit abstraction, evaluation metrics,\nand lifecycle management. We identify four fundamental differences that\nunderlie 6 core challenges. To address these, we propose four types of\ncollaborative strategies (\\emph{Retain}, \\emph{Translate}, \\emph{Integrate},\nand \\emph{Runtime}) and explore a closed-loop, trustworthy quality assurance\nframework that combines pre-deployment validation with runtime monitoring.\nBased on these strategies, we offer practical guidance and a protocol proposal\nto support the standardization and tooling of LLM application testing. We\npropose a protocol \\textbf{\\textit{Agent Interaction Communication Language}}\n(AICL) that is used to communicate between AI agents. AICL has the\ntest-oriented features and is easily integrated in the current agent framework.", "AI": {"tldr": "LLM apps need new quality assurance approaches due to their complexity. This paper proposes a three-layer architecture, compares testing frameworks from software and AI fields, outlines key challenges, and introduces the AICL protocol\u2014a standardized communication language for agent-based testing. Practical strategies and protocols for reliable LLM testing are presented.", "motivation": "LLM applications have become complex, integrating retrieval, tool use, and multi-turn interactions, making traditional quality assurance methods insufficient. Ensuring reliable testing and assurance for these dynamic and context-dependent systems is increasingly urgent.", "method": "The paper decomposes LLM applications into three layers: System Shell, Prompt Orchestration, and LLM Inference Core. It analyzes how traditional and AI-based software testing methods apply to each layer, compares approaches from software engineering and AI safety, identifies challenges, and proposes collaborative strategies. It introduces a standardized protocol (AICL) for agent communication focused on testing and integration.", "result": "The analysis reveals four fundamental differences between software engineering and AI safety testing, leading to six key challenges in LLM application assurance. Four collaborative strategies (Retain, Translate, Integrate, Runtime) and a closed-loop assurance framework (combining pre-deployment and runtime testing) are proposed. The AICL protocol is presented as a practical solution for standardization and tooling.", "conclusion": "Traditional software testing methods alone are inadequate for LLM applications due to their unique characteristics. Combining insights from software engineering and AI safety, and utilizing a specialized agent communication protocol like AICL, can facilitate trustworthy and efficient quality assurance for these systems. The paper sets actionable guidance for standardizing and tooling LLM application testing."}}
{"id": "2508.20744", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20744", "abs": "https://arxiv.org/abs/2508.20744", "authors": ["Shabnam Hassani", "Mehrdad Sabetzadeh", "Daniel Amyot"], "title": "From Law to Gherkin: A Human-Centred Quasi-Experiment on the Quality of LLM-Generated Behavioural Specifications from Food-Safety Regulations", "comment": null, "summary": "Context: Laws and regulations increasingly affect software design and quality\nassurance, but legal texts are written in technology-neutral language. This\ncreates challenges for engineers who must develop compliance artifacts such as\nrequirements and acceptance criteria. Manual creation is labor-intensive,\nerror-prone, and requires domain expertise. Advances in Generative AI (GenAI),\nespecially Large Language Models (LLMs), offer a way to automate deriving such\nartifacts.\n  Objective: We present the first systematic human-subject study of LLMs'\nability to derive behavioral specifications from legal texts using a\nquasi-experimental design. These specifications translate legal requirements\ninto a developer-friendly form.\n  Methods: Ten participants evaluated specifications generated from food-safety\nregulations by Claude and Llama. Using Gherkin, a structured BDD language, 60\nspecifications were produced. Each participant assessed 12 across five\ncriteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Each\nspecification was reviewed by two participants, yielding 120 assessments.\n  Results: For Relevance, 75% of ratings were highest and 20% second-highest.\nClarity reached 90% highest. Completeness: 75% highest, 19% second.\nSingularity: 82% highest, 12% second. Time Savings: 68% highest, 24% second. No\nlowest ratings occurred. Mann-Whitney U tests showed no significant differences\nacross participants or models. Llama slightly outperformed Claude in Clarity,\nCompleteness, and Time Savings, while Claude was stronger in Singularity.\nFeedback noted hallucinations and omissions but confirmed the utility of the\nspecifications.\n  Conclusion: LLMs can generate high-quality Gherkin specifications from legal\ntexts, reducing manual effort and providing structured artifacts useful for\nimplementation, assurance, and test generation.", "AI": {"tldr": "This paper shows that Large Language Models can accurately and efficiently generate software requirements from legal texts, streamlining compliance work for developers, with little difference between the tested models.", "motivation": "Legal requirements increasingly shape how software is built, but translating complex, technology-neutral legal language into actionable development artifacts is difficult and usually demands significant expert effort. Automating this process could save time and improve compliance.", "method": "The authors conducted a systematic quasi-experimental study involving ten human participants. They evaluated Gherkin behavioral specifications generated by two popular Large Language Models (Claude and Llama) from food-safety legal texts. A total of 60 specifications were assessed across five quality criteria, with each reviewed by two participants to ensure reliability.", "result": "The generated specifications were rated highly: over 68% of the assessments gave the highest possible score across criteria such as Relevance, Clarity, Completeness, Singularity, and Time Savings. Llama slightly outperformed Claude on most criteria, except for Singularity where Claude led. No lowest ratings occurred, and statistical analysis found no significant differences across participants or models. Some issues with hallucinations and omissions were noted, but overall, the generated specifications were found useful.", "conclusion": "Large Language Models can reliably and efficiently generate high-quality, developer-friendly behavioral specifications from legal texts. This automation reduces manual workload and supports structured software compliance, assurance, and testing processes."}}
{"id": "2508.20774", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20774", "abs": "https://arxiv.org/abs/2508.20774", "authors": ["Markus Funke", "Patricia Lago"], "title": "Towards an Architectural Perspective for Sustainability: Bundle the Needs from Industry", "comment": null, "summary": "Sustainability is increasingly recognized as an emerging quality property in\nsoftware-intensive systems, yet architects lack structured guidance to address\nit effectively throughout the software design phase. Architectural\nperspectives-an architectural knowledge artifact composed of concerns,\nactivities, tactics, pitfalls, and checklists-offer a promising approach to\ntackle such emerging quality properties across architectural views and are also\nindependent of architecture frameworks and industry contexts. In this paper, we\npresent a sustainability perspective vision, i.e., a revised notion of\narchitectural perspective meant to be filled with its own elements to target\nsustainability concerns. We formulate our sustainability perspective vision\nthrough evidence from applying snowballing to seminal literature and from\nconducting a focus group with experts in the field. Our findings confirm the\nrelevance of the different perspective elements in practice and highlight\nimplications for shaping a sustainability perspective that meets industrial\nneeds.", "AI": {"tldr": "The paper proposes a structured approach for architects to integrate sustainability into software design using a revised architectural perspective, validated through literature review and expert input.", "motivation": "Sustainability is becoming a critical quality property in software-intensive systems, but there is a lack of structured guidance for architects to effectively address sustainability during the software design phase.", "method": "The paper introduces the concept of a 'sustainability perspective' for software architecture. This is developed using evidence gathered from snowballing seminal literature and conducting a focus group with domain experts.", "result": "The findings validate the relevance of different architectural perspective elements and indicate how a sustainability perspective can be shaped to address real-world industrial requirements.", "conclusion": "Architectural perspectives can be effectively adapted to provide structured guidance for incorporating sustainability into software architectures, and the proposed vision aligns well with practical needs in industry."}}
{"id": "2508.20902", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20902", "abs": "https://arxiv.org/abs/2508.20902", "authors": ["Baharin A. Jodat", "Khouloud Gaaloul", "Mehrdad Sabetzadeh", "Shiva Nejati"], "title": "Automated Test Oracles for Flaky Cyber-Physical System Simulators: Approach and Evaluation", "comment": null, "summary": "Simulation-based testing of cyber-physical systems (CPS) is costly due to the\ntime-consuming execution of CPS simulators. In addition, CPS simulators may be\nflaky, leading to inconsistent test outcomes and requiring repeated test\nre-execution for reliable test verdicts. Automated test oracles that do not\nrequire system execution are therefore crucial for reducing testing costs.\nIdeally, such test oracles should be interpretable to facilitate human\nunderstanding of test verdicts, and they must be robust against the potential\nflakiness of CPS simulators. In this article, we propose assertion-based test\noracles for CPS as sets of logical and arithmetic predicates defined over the\ninputs of the system under test. Given a test input, our assertion-based test\noracle determines, without requiring test execution, whether the test passes,\nfails, or if the oracle is inconclusive in predicting a verdict. We describe\ntwo methods for generating assertion-based test oracles: one using genetic\nprogramming~(GP) that employs well-known spectrum-based fault localization\n(SBFL) ranking formulas, namely Ochiai, Tarantula, and Naish, as fitness\nfunctions; and the other using decision trees (DT) and decision rules (DR). We\nevaluate our assertion-based test oracles through case studies in the domains\nof aerospace, networking and autonomous driving. We show that test oracles\ngenerated using GP with Ochiai are significantly more accurate than those\nobtained using GP with Tarantula and Naish or using DT or DR. Moreover, this\naccuracy advantage remains even when accounting for the flakiness of the system\nunder test. We further show that the assertion-based test oracles generated by\nGP with Ochiai are robust against flakiness with only 4% average variation in\ntheir accuracy results across four different network and autonomous driving\nsystems with flaky behaviours.", "AI": {"tldr": "Replacing expensive and unreliable CPS simulation-based testing, this paper introduces assertion-based test oracles created via genetic programming and decision models, finding that GP with Ochiai is the most accurate and robust method, especially in flaky simulator situations.", "motivation": "Simulation-based testing for cyber-physical systems (CPS) is resource-intensive due to slow simulation runtimes and inconsistent outcomes from flaky CPS simulators. There is a need for robust, interpretable, and non-execution-based automated test oracles to reduce cost and ensure reliability.", "method": "The paper proposes assertion-based test oracles, which are sets of logical and arithmetic predicates applied to system inputs. Two generation methods are introduced: (1) genetic programming (GP) using spectrum-based fault localization (SBFL) ranking formulas like Ochiai, Tarantula, and Naish as fitness functions, and (2) decision trees (DT) and decision rules (DR).", "result": "GP with Ochiai ranking formula produces the most accurate test oracles, outperforming DT/DR and other GP variants (Tarantula, Naish). This holds true even in the presence of simulator flakiness. The oracles generated by GP with Ochiai have only 4% average accuracy variation in flaky conditions across four diverse CPS domains.", "conclusion": "Assertion-based test oracles generated by GP with Ochiai provide highly accurate and robust predictions for CPS testing, reducing reliance on costly and inconsistent simulations."}}
{"id": "2508.20911", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20911", "abs": "https://arxiv.org/abs/2508.20911", "authors": ["Zuocheng Feng", "Kaiwen Zhang", "Miaomiao Wang", "Yiming Cheng", "Yuandao Cai", "Xiaofeng Li", "Guanjun Liu"], "title": "Deep Learning Based Concurrency Bug Detection and Localization", "comment": null, "summary": "Concurrency bugs, caused by improper synchronization of shared resources in\nmulti-threaded or distributed systems, are notoriously hard to detect and thus\ncompromise software reliability and security. The existing deep learning\nmethods face three main limitations. First, there is an absence of large and\ndedicated datasets of diverse concurrency bugs for them. Second, they lack\nsufficient representation of concurrency semantics. Third, binary\nclassification results fail to provide finer-grained debug information such as\nprecise bug lines. To address these problems, we propose a novel method for\neffective concurrency bug detection as well as localization. We construct a\ndedicated concurrency bug dataset to facilitate model training and evaluation.\nWe then integrate a pre-trained model with a heterogeneous graph neural network\n(GNN), by incorporating a new Concurrency-Aware Code Property Graph (CCPG) that\nconcisely and effectively characterizes concurrency semantics. To further\nfacilitate debugging, we employ SubgraphX, a GNN-based interpretability method,\nwhich explores the graphs to precisely localize concurrency bugs, mapping them\nto specific lines of source code. On average, our method demonstrates an\nimprovement of 10\\% in accuracy and precision and 26\\% in recall compared to\nstate-of-the-art methods across diverse evaluation settings.", "AI": {"tldr": "The paper introduces a novel approach for detecting and localizing concurrency bugs, combining a new graph-based representation and a specialized dataset to greatly improve both bug detection and localization over prior techniques.", "motivation": "Existing deep learning methods for concurrency bug detection suffer from a lack of diverse datasets, insufficient modeling of concurrency semantics, and coarse binary classification results that do not help with precise debugging.", "method": "The authors build a specialized concurrency bug dataset, utilize a pre-trained model combined with a heterogeneous Graph Neural Network (GNN), and introduce the Concurrency-Aware Code Property Graph (CCPG) to better capture concurrency semantics. SubgraphX is also leveraged for fine-grained bug localization.", "result": "The new approach achieves an average increase of 10% in both accuracy and precision, and 26% in recall, compared to leading existing methods on various benchmarks.", "conclusion": "The proposed method outperforms state-of-the-art approaches in both detecting and localizing concurrency bugs, achieving significant improvements in accuracy, precision, and recall."}}
{"id": "2508.20977", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20977", "abs": "https://arxiv.org/abs/2508.20977", "authors": ["Shiwen Shan", "Yintong Huo", "Yuxin Su", "Zhining Wang", "Dan Li", "Zibin Zheng"], "title": "ConfLogger: Enhance Systems' Configuration Diagnosability through Configuration Logging", "comment": "13 pages, 6 figures, accpeted by ICSE '26 (The 48th IEEE/ACM\n  International Conference on Software Engineering)", "summary": "Modern configurable systems offer customization via intricate configuration\nspaces, yet such flexibility introduces pervasive configuration-related issues\nsuch as misconfigurations and latent softwarebugs. Existing diagnosability\nsupports focus on post-failure analysis of software behavior to identify\nconfiguration issues, but none of these approaches look into whether the\nsoftware clue sufficient failure information for diagnosis. To fill in the\nblank, we propose the idea of configuration logging to enhance existing logging\npractices at the source code level. We develop ConfLogger, the first tool that\nunifies configuration-aware static taint analysis with LLM-based log generation\nto enhance software configuration diagnosability. Specifically, our method 1)\nidentifies configuration-sensitive code segments by tracing\nconfiguration-related data flow in the whole project, and 2) generates\ndiagnostic log statements by analyzing configuration code contexts. Evaluation\nresults on eight popular software systems demonstrate the effectiveness of\nConfLogger to enhance configuration diagnosability. Specifically,\nConfLogger-enhanced logs successfully aid a log-based misconfiguration\ndiagnosis tool to achieve 100% accuracy on error localization in 30 silent\nmisconfiguration scenarios, with 80% directly resolvable through explicit\nconfiguration information exposed. In addition, ConfLogger achieves 74%\ncoverage of existing logging points, outperforming baseline LLM-based loggers\nby 12% and 30%. It also gains 8.6% higher in precision, 79.3% higher in recall,\nand 26.2% higher in F1 compared to the state-of-the-art baseline in terms of\nvariable logging while also augmenting diagnostic value. A controlled user\nstudy on 22 cases further validated its utility, speeding up diagnostic time by\n1.25x and improving troubleshooting accuracy by 251.4%.", "AI": {"tldr": "This paper proposes ConfLogger, a tool combining static taint analysis and LLMs, to automatically generate configuration-focused logs in software. On eight systems and in user studies, ConfLogger greatly improved error diagnosability, logging coverage, and troubleshooting accuracy compared to existing methods.", "motivation": "Modern configurable systems are highly customizable, but this flexibility leads to complex configuration-related bugs and issues. Traditional diagnosability methods focus on analyzing failures after they occur, but do not assess whether logs contain enough information to aid in diagnosis. This paper addresses the gap by proposing methods to proactively improve configuration-related logging to support diagnosis.", "method": "The authors introduce ConfLogger, a tool that combines configuration-aware static taint analysis with large language model (LLM)-based log generation. ConfLogger traces configuration data flows throughout the project to identify configuration-sensitive code segments and then generates diagnostic log statements based on the gathered context.", "result": "ConfLogger was evaluated on eight popular software systems. Enhanced logs enabled a misconfiguration diagnosis tool to achieve 100% accuracy in localizing errors in 30 silent misconfiguration scenarios, and 80% of these were directly resolvable due to explicit configuration information. ConfLogger covered 74% of existing logging points, outperforming baseline LLM loggers by 12%-30%, and showed superior variable logging precision (+8.6%), recall (+79.3%), and F1 score (+26.2%). A user study demonstrated a 1.25x speedup in diagnostics and 251.4% improved troubleshooting accuracy.", "conclusion": "ConfLogger significantly enhances diagnosability in configurable systems by proactively generating configuration-aware logs, outperforming state-of-the-art logging approaches in both automated and human troubleshooting scenarios."}}
{"id": "2508.21050", "categories": ["cs.SE", "cs.CY", "K.2; K.6.3; K.4; K.7"], "pdf": "https://arxiv.org/pdf/2508.21050", "abs": "https://arxiv.org/abs/2508.21050", "authors": ["Thomas J. Misa"], "title": "Dynamics of Gender Bias in Software Engineering", "comment": "26 pages, 3 figures", "summary": "The field of software engineering is embedded in both engineering and\ncomputer science, and may embody gender biases endemic to both. This paper\nsurveys software engineering's origins and its long-running attention to\nengineering professionalism, profiling five leaders; it then examines the\nfield's recent attention to gender issues and gender bias. It next\nquantitatively analyzes women's participation as research authors in the\nfield's leading International Conference of Software Engineering (1976-2010),\nfinding a dozen years with statistically significant gender exclusion. Policy\ndimensions of research on gender bias in computing are suggested.", "AI": {"tldr": "This paper highlights the ongoing gender bias within software engineering, analyzing historical trends and author participation at a major conference from 1976\u20132010, and urges new policy directions to combat bias in research.", "motivation": "The paper aims to investigate the presence and impact of gender bias in the traditionally male-dominated fields of engineering and computer science, focusing specifically on software engineering.", "method": "The study first surveys the historical background and professionalism within software engineering, profiling five prominent leaders. It then examines the field\u2019s attention to gender-related issues and conducts a quantitative analysis of female participation as research authors in the International Conference of Software Engineering from 1976 to 2010.", "result": "The analysis reveals statistically significant periods\u2014specifically a dozen years\u2014where there was notable gender exclusion of women as research authors within the conference.", "conclusion": "There is a persistent gender bias in software engineering, especially evident in research authorship. The paper recommends further policy-driven research to address gender bias in computing."}}
