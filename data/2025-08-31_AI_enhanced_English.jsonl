{"id": "2508.20119", "categories": ["cs.SE", "cs.LG", "68T42", "I.2.6; I.2.2; D.2.2"], "pdf": "https://arxiv.org/pdf/2508.20119", "abs": "https://arxiv.org/abs/2508.20119", "authors": ["Daniel M. Yellin"], "title": "Evaluating LLMs on microservice-based applications: how complex is your specification?", "comment": "20 pages + 7 pages appendices. 7 Figures. 8 Tables", "summary": "In this paper we evaluate how far LLMs have advanced in generating code for\nreal-world problems. Specifically, we explore code synthesis for\nmicroservice-based applications, a widely used architecture pattern. We define\na standard template for specifying these applications, and we propose a metric\nfor judging the difficulty level of a specification. The higher the score, the\nmore difficult it is to generate code for the specification. We develop a\nframework to automate the process of testing LLM-synthesized code for a\nmicroservice using unit tests. Our experimental results show that strong LLMs\n(like GPT-3o-mini) do fairly well on medium difficulty specifications but do\nvery poorly on those of higher difficulty levels. This is due to more intricate\nbusiness logic, a greater use of external services, database integration and\ninclusion of non-functional capabilities such as authentication. We analyzed\nthe errors in LLM-synthesized code and report on the key challenges LLMs face\nin generating code for these specifications thereby suggesting future research\ndirections to improve code synthesis for real-world problems.", "AI": {"tldr": "LLMs can generate code for basic and moderately complex microservices, but fail with more difficult, real-world software requirements, highlighting the need for further research and improvements.", "motivation": "Large Language Models (LLMs) are increasingly used for code generation, but their effectiveness in synthesizing code for real-world, complex software like microservice-based applications needs thorough evaluation.", "method": "The authors define a standard template for microservice application specifications, introduce a metric for measuring the difficulty of these specifications, and build a framework for automatically testing the generated code via unit tests. They then evaluate how well LLMs synthesize code of varying difficulty levels, particularly analyzing errors and challenging cases.", "result": "LLMs such as GPT-3o-mini perform decently on medium difficulty specifications, but their performance drops significantly on higher difficulty tasks, especially those involving complex business logic, use of external services, database integration, and non-functional requirements like authentication.", "conclusion": "Current LLMs, while capable for moderately complex coding tasks, struggle significantly with more intricate, real-world software requirements. The authors highlight key challenges and propose areas for future research to improve LLM code synthesis capability."}}
{"id": "2508.20124", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20124", "abs": "https://arxiv.org/abs/2508.20124", "authors": ["Yunlong Feng", "Yang Xu", "Xiao Xu", "Binyuan Hui", "Junyang Lin"], "title": "Towards Better Correctness and Efficiency in Code Generation", "comment": null, "summary": "While code large language models have demonstrated remarkable progress in\ncode generation, the generated code often exhibits poor runtime efficiency,\nlimiting its practical application in performance-sensitive scenarios. To\naddress this limitation, we propose an efficiency-oriented reinforcement\nlearning framework guided by a novel performance reward. Based on this\nframework, we take a deeper dive into the code efficiency problem, identifying\nthen proposing methods to overcome key bottlenecks: (1) Dynamic exploration\novercomes the static data constraints of offline fine-tuning, enabling the\ndiscovery of more efficient code implementations. (2) The error-insensitive\nreinforcement learning method and high-contrast efficiency signals are crucial\nfor mitigating systematic errors and achieving effective optimization. (3)\nOnline exploration is most effective when starting from a high-correctness\nbaseline, as this allows for efficiency improvements without sacrificing\naccuracy. With these discoveries, we finally propose a two-stage tuning method,\nwhich achieves high and balanced performance across correctness and efficiency.\nThe results of experiments show the effectiveness of the method, which improves\ncode correctness by 10.18\\% and runtime efficiency by 7.75\\% on a 7B model,\nachieving performance comparable to much larger model.", "AI": {"tldr": "The paper proposes a new reinforcement learning framework to make code generation models both more correct and efficient, showing strong improvements over previous methods.", "motivation": "Current code large language models generate code with poor runtime efficiency, which restricts their use in performance-critical scenarios. The motivation is to improve both efficiency and correctness of code generation.", "method": "An efficiency-oriented reinforcement learning framework guided by a novel performance reward, including dynamic exploration, error-insensitive methods, and high-contrast efficiency signals, was used, culminating in a two-stage tuning process.", "result": "The experimental results show improvements of 10.18% in code correctness and 7.75% in runtime efficiency for a 7B parameter model, making its performance comparable to much larger models.", "conclusion": "The proposed framework and two-stage tuning method significantly improve both code correctness and runtime efficiency in code large language models, making them perform comparably to much larger models but with better balanced performance."}}
{"id": "2508.20340", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.20340", "abs": "https://arxiv.org/abs/2508.20340", "authors": ["Maolin Sun", "Yibiao Yang", "Yuming Zhou"], "title": "Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce Formula Generators", "comment": null, "summary": "Satisfiability Modulo Theory (SMT) solvers are foundational to modern systems\nand programming languages research, providing the foundation for tasks like\nsymbolic execution and automated verification. Because these solvers sit on the\ncritical path, their correctness is essential, and high-quality test formulas\nare key to uncovering bugs. However, while prior testing techniques performed\nwell on earlier solver versions, they struggle to keep pace with rapidly\nevolving features. Recent approaches based on Large Language Models (LLMs) show\npromise in exploring advanced solver capabilities, but two obstacles remain:\nnearly half of the generated formulas are syntactically invalid, and iterative\ninteractions with the LLMs introduce substantial computational overhead. In\nthis study, we present Chimera, a novel LLM-assisted fuzzing framework that\naddresses both issues by shifting from direct formula generation to the\nsynthesis of reusable term (i.e., logical expression) generators. Particularly,\nChimera uses LLMs to (1) automatically extract context-free grammars (CFGs) for\nSMT theories, including solver-specific extensions, from documentation, and (2)\nsynthesize composable Boolean term generators that adhere to these grammars.\nDuring fuzzing, Chimera populates structural skeletons derived from existing\nformulas with the terms iteratively produced by the LLM-synthesized generators.\nThis design ensures syntactic validity while promoting semantic diversity.\nNotably, Chimera requires only one-time LLM interaction investment,\ndramatically reducing runtime cost. We evaluated Chimera on two leading SMT\nsolvers: Z3 and cvc5. Our experiments show that Chimera has identified 43\nconfirmed bugs, 40 of which have already been fixed by developers.", "AI": {"tldr": "Chimera is an efficient LLM-assisted fuzzing tool for SMT solvers that creates syntactically correct, diverse test inputs, finds more bugs, and uses less computational resources than prior LLM-based methods.", "motivation": "SMT solvers are vital for modern system reliability, but their rapid evolution introduces challenges for testing and bug detection. Existing LLM-based testing approaches face problems with syntactic invalidity and high computational cost.", "method": "The authors propose Chimera, an LLM-assisted fuzzing framework. Chimera leverages LLMs to extract context-free grammars from documentation and synthesize reusable Boolean term generators. Instead of generating test formulas directly, Chimera produces generators that ensure syntactic correctness and semantic diversity, and only needs a single LLM invocation, improving efficiency.", "result": "Chimera was evaluated on Z3 and cvc5 SMT solvers, uncovering 43 confirmed bugs, with 40 already addressed and fixed by developers.", "conclusion": "Chimera demonstrates a more effective, efficient approach to fuzzing SMT solvers, improving both bug-finding capability and resource efficiency by generating syntactically valid and diverse test cases."}}
{"id": "2508.20370", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20370", "abs": "https://arxiv.org/abs/2508.20370", "authors": ["Lingzhe Zhang", "Tong Jia", "Kangjin Wang", "Weijie Hong", "Chiming Duan", "Minghua He", "Ying Li"], "title": "Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought", "comment": null, "summary": "As contemporary microservice systems become increasingly popular and\ncomplex-often comprising hundreds or even thousands of fine-grained,\ninterdependent subsystems-they are facing more frequent failures. Ensuring\nsystem reliability thus demands accurate root cause localization. While traces\nand metrics have proven to be effective data sources for this task, existing\nmethods either heavily rely on pre-defined schemas, which struggle to adapt to\nevolving operational contexts, or lack interpretability in their reasoning\nprocess, thereby leaving Site Reliability Engineers (SREs) confused. In this\npaper, we conduct a comprehensive study on how SREs localize the root cause of\nfailures, drawing insights from multiple professional SREs across different\norganizations. Our investigation reveals that human root cause analysis\nexhibits three key characteristics: recursiveness, multi-dimensional expansion,\nand cross-modal reasoning. Motivated by these findings, we introduce RCLAgent,\nan adaptive root cause localization method for microservice systems that\nleverages a multi-agent recursion-of-thought framework. RCLAgent employs a\nnovel recursion-of-thought strategy to guide the LLM's reasoning process,\neffectively integrating data from multiple agents and tool-assisted analysis to\naccurately pinpoint the root cause. Experimental evaluations on various public\ndatasets demonstrate that RCLAgent achieves superior performance by localizing\nthe root cause using only a single request-outperforming state-of-the-art\nmethods that depend on aggregating multiple requests. These results underscore\nthe effectiveness of RCLAgent in enhancing the efficiency and precision of root\ncause localization in complex microservice environments.", "AI": {"tldr": "Microservice systems are vulnerable to frequent failures, making root cause localization crucial. This paper presents RCLAgent, a new adaptive method inspired by SRE practices, which uses a recursion-of-thought approach with multiple agents and LLMs. RCLAgent delivers superior performance with just one request and advances reliability and accuracy compared to current state-of-the-art methods.", "motivation": "Contemporary microservice systems are highly complex and prone to frequent failures due to their multiple fine-grained, interdependent subsystems. Existing root cause localization methods have limitations such as reliance on rigid schemas or lack of interpretability, leaving Site Reliability Engineers (SREs) challenged in accurately identifying the root causes.", "method": "The paper introduces RCLAgent, an adaptive root cause localization method specifically designed for microservice systems. RCLAgent uses a novel multi-agent recursion-of-thought framework, leveraging the reasoning capabilities of large language models (LLMs) and integrating multi-source data and tool-assisted analysis to guide the localization process.", "result": "Experimental evaluations using various public datasets show that RCLAgent can accurately localize the root cause of failures using only a single system request. RCLAgent outperforms current state-of-the-art methods that require aggregation of multiple requests for effective localization.", "conclusion": "RCLAgent significantly enhances both efficiency and precision in root cause localization within complex microservice systems. Its adaptive, interpretable approach based on lessons from real SRE practices establishes a new benchmark for this task."}}
{"id": "2508.20365", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.20365", "abs": "https://arxiv.org/abs/2508.20365", "authors": ["Naoki Kobayashi", "Ryosuke Sato", "Ayumi Shinohara", "Ryo Yoshinaka"], "title": "Solvable Tuple Patterns and Their Applications to Program Verification", "comment": null, "summary": "Despite the recent progress of automated program verification techniques,\nfully automated verification of programs manipulating recursive data structures\nremains a challenge. We introduce the notion of solvable tuple patterns (STPs)\nto express invariants between list-like recursive data structures. A\ndistinguishing feature of STPs is that they can be efficiently inferred from\nonly a small number of positive samples; no negative samples are required. An\nSMT solver that supports the sequence theory can be used to check that an\ninferred STP is indeed an inductive invariant. After presenting basic\nproperties of STPs and an STP inference algorithm, we show how to incorporate\nthe STP inference into a CHC (Constrained Horn Clauses) solver supporting\nlist-like data structures, which serves as a uniform backend for automated\nprogram verification tools. A CHC solver incorporating the STP inference has\nwon the ADT-LIN category of CHC-COMP 2025 by a big margin.", "AI": {"tldr": "This paper introduces 'solvable tuple patterns' for expressing and efficiently inferring invariants over recursive, list-like data structures from positive samples only. These invariants can be verified using SMT solvers, and integrating this approach into a CHC solver led to state-of-the-art results in automated program verification competitions.", "motivation": "Automated program verification for recursive data structures is still challenging. The proposed approach aims to address this difficulty by enabling efficient inference of invariants for such programs.", "method": "Introducing solvable tuple patterns (STPs), devising an inference algorithm for STPs from positive samples, and incorporating this process into a CHC solver with support for list-like data structures.", "result": "The CHC solver using STP inference significantly outperformed competitors, winning the ADT-LIN category of CHC-COMP 2025 by a large margin.", "conclusion": "The integration of STP inference into a CHC solver improves automated verification for programs with list-like recursive data structures, evidenced by its performance in a competitive setting."}}
{"id": "2508.20563", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20563", "abs": "https://arxiv.org/abs/2508.20563", "authors": ["Zheying Zhang", "Tomas Herda", "Victoria Pichler", "Pekka Abrahamsson", "Geir K. Hanssen", "Joshua Kerievsky", "Alex Polyakov", "Mohit Chandna", "Marius Irgens", "Kai-Kristian Kemell", "Ayman Asad Khan", "Crystal Kwok", "Evan Leybourn", "Munish Malik", "Dorota Mleczko", "Morteza Moalagh", "Christopher Morales", "Yuliia Pieskova", "Daniel Plan\u00f6tscher", "Mika Saari", "Anastasiia Tkalich", "Karl Josef Gstettner", "Xiaofeng Wang"], "title": "AI and Agile Software Development: A Research Roadmap from the XP2025 Workshop", "comment": null, "summary": "This paper synthesizes the key findings from a full-day XP2025 workshop on\n\"AI and Agile: From Frustration to Success\", held in Brugg-Windisch,\nSwitzerland. The workshop brought together over 30 interdisciplinary academic\nresearchers and industry practitioners to tackle the concrete challenges and\nemerging opportunities at the intersection of Generative Artificial\nIntelligence (GenAI) and agile software development. Through structured,\ninteractive breakout sessions, participants identified shared pain points like\ntool fragmentation, governance, data quality, and critical skills gaps in AI\nliteracy and prompt engineering. These issues were further analyzed, revealing\nunderlying causes and cross-cutting concerns. The workshop concluded by\ncollaboratively co-creating a multi-thematic research roadmap, articulating\nboth short-term, implementable actions and visionary, long-term research\ndirections. This cohesive agenda aims to guide future investigation and drive\nthe responsible, human-centered integration of GenAI into agile practices.", "AI": {"tldr": "This paper summarizes insights from an interdisciplinary workshop on integrating Generative AI into agile software development, highlighting major challenges, root causes, and a collaboratively built research roadmap to guide future efforts.", "motivation": "There is a rapidly growing intersection between Generative AI and agile software development, presenting both challenges (such as tool fragmentation, governance, data quality, and skills gaps) and new opportunities. The motivation is to better understand and address these challenges to enable successful integration of AI into agile practices.", "method": "The authors conducted a full-day workshop with over 30 interdisciplinary participants, using structured and interactive breakout sessions to collectively identify issues, analyze their underlying causes, and collaboratively develop a research roadmap for the field.", "result": "Participants identified key pain points (tool fragmentation, governance, data quality, AI literacy, and prompt engineering), analyzed root causes, and jointly produced a multi-thematic research roadmap. This roadmap outlines actionable short-term steps as well as long-term research directions for the responsible, human-centered integration of GenAI in agile development.", "conclusion": "A shared research agenda and concrete action items were established to guide future research and practice at the intersection of GenAI and agile software development, aiming to address current barriers and foster human-centered, responsible integration of AI."}}
{"id": "2508.20922", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.20922", "abs": "https://arxiv.org/abs/2508.20922", "authors": ["Markus B\u00f6ck", "J\u00fcrgen Cito"], "title": "Static Factorisation of Probabilistic Programs With User-Labelled Sample Statements and While Loops", "comment": null, "summary": "It is commonly known that any Bayesian network can be implemented as a\nprobabilistic program, but the reverse direction is not so clear. In this work,\nwe address the open question to what extent a probabilistic program with\nuser-labelled sample statements and while loops - features found in languages\nlike Gen, Turing, and Pyro - can be represented graphically. To this end, we\nextend existing operational semantics to support these language features. By\ntranslating a program to its control-flow graph, we define a sound static\nanalysis that approximates the dependency structure of the random variables in\nthe program. As a result, we obtain a static factorisation of the implicitly\ndefined program density, which is equivalent to the known Bayesian network\nfactorisation for programs without loops and constant labels, but constitutes a\nnovel graphical representation for programs that define an unbounded number of\nrandom variables via loops or dynamic labels. We further develop a sound\nprogram slicing technique to leverage this structure to statically enable three\nwell-known optimisations for the considered program class: we reduce the\nvariance of gradient estimates in variational inference and we speed up both\nsingle-site Metropolis Hastings and sequential Monte Carlo. These optimisations\nare proven correct and empirically shown to match or outperform existing\ntechniques.", "AI": {"tldr": "This paper bridges the gap between probabilistic programming and graphical models for programs with loops and dynamic random variables, enabling improved inference through new static analysis and program slicing techniques.", "motivation": "While Bayesian networks can be encoded as probabilistic programs, it is not clear how (or if) probabilistic programs\u2014especially those with features like dynamic sampling and loops\u2014can be represented graphically and exploited for inference.", "method": "The authors extend operational semantics to handle advanced language features like while loops and sample statements, translate programs to control-flow graphs, and perform static analysis to approximate dependencies. They further design program slicing to assist with three inference optimisations.", "result": "They provide a sound static analysis method that successfully derives a graphical representation for these rich classes of probabilistic programs. With this, they achieve equivalent factorisation to Bayesian networks where possible, and novel graphical models otherwise. Their slicing-based optimisations are proven and empirically validated.", "conclusion": "The paper demonstrates that probabilistic programs with sample statements and while loops can be represented graphically, and introduces a novel static analysis for these cases. It also develops program slicing techniques that enable efficient inference optimisations."}}
{"id": "2508.20737", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20737", "abs": "https://arxiv.org/abs/2508.20737", "authors": ["Wei Ma", "Yixiao Yang", "Qiang Hu", "Shi Ying", "Zhi Jin", "Bo Du", "Zhenchang Xing", "Tianlin Li", "Junjie Shi", "Yang Liu", "Linxiao Jiang"], "title": "Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol", "comment": null, "summary": "Applications of Large Language Models~(LLMs) have evolved from simple text\ngenerators into complex software systems that integrate retrieval augmentation,\ntool invocation, and multi-turn interactions. Their inherent non-determinism,\ndynamism, and context dependence pose fundamental challenges for quality\nassurance. This paper decomposes LLM applications into a three-layer\narchitecture: \\textbf{\\textit{System Shell Layer}}, \\textbf{\\textit{Prompt\nOrchestration Layer}}, and \\textbf{\\textit{LLM Inference Core}}. We then assess\nthe applicability of traditional software testing methods in each layer:\ndirectly applicable at the shell layer, requiring semantic reinterpretation at\nthe orchestration layer, and necessitating paradigm shifts at the inference\ncore. A comparative analysis of Testing AI methods from the software\nengineering community and safety analysis techniques from the AI community\nreveals structural disconnects in testing unit abstraction, evaluation metrics,\nand lifecycle management. We identify four fundamental differences that\nunderlie 6 core challenges. To address these, we propose four types of\ncollaborative strategies (\\emph{Retain}, \\emph{Translate}, \\emph{Integrate},\nand \\emph{Runtime}) and explore a closed-loop, trustworthy quality assurance\nframework that combines pre-deployment validation with runtime monitoring.\nBased on these strategies, we offer practical guidance and a protocol proposal\nto support the standardization and tooling of LLM application testing. We\npropose a protocol \\textbf{\\textit{Agent Interaction Communication Language}}\n(AICL) that is used to communicate between AI agents. AICL has the\ntest-oriented features and is easily integrated in the current agent framework.", "AI": {"tldr": "The paper introduces a three-layer model to understand and test Large Language Model applications, analyzes the limitations of existing software testing approaches, and proposes collaborative strategies and a specialized communication protocol (AICL) for robust quality assurance and standardization.", "motivation": "Large Language Model (LLM) applications have evolved into complex systems, making quality assurance challenging due to their non-determinism, dynamism, and context dependence. Existing software testing approaches may not directly address these issues, prompting the need for new frameworks and protocols.", "method": "The paper decomposes LLM applications into a three-layer architecture: System Shell Layer, Prompt Orchestration Layer, and LLM Inference Core. It assesses traditional software testing methods' applicability at each layer and performs a comparative analysis of testing approaches from both software engineering and AI safety perspectives. The study identifies key differences, proposes collaborative strategies for quality assurance, and presents a protocol (AICL) to standardize agent communication and facilitate testing.", "result": "The authors identify four fundamental differences leading to six core challenges in LLM application testing. They develop four collaborative strategies (Retain, Translate, Integrate, Runtime) and propose a closed-loop quality assurance framework combining pre-deployment validation and runtime monitoring. They also introduce the Agent Interaction Communication Language (AICL) tailored for testability and integration in agent-based systems.", "conclusion": "Traditional testing methods require adaptation or rethinking for LLM systems due to their unique characteristics. The proposed quality assurance framework and AICL facilitate more trustworthy and standardized testing of LLM applications, addressing structural disconnects in current methodologies."}}
{"id": "2508.20744", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20744", "abs": "https://arxiv.org/abs/2508.20744", "authors": ["Shabnam Hassani", "Mehrdad Sabetzadeh", "Daniel Amyot"], "title": "From Law to Gherkin: A Human-Centred Quasi-Experiment on the Quality of LLM-Generated Behavioural Specifications from Food-Safety Regulations", "comment": null, "summary": "Context: Laws and regulations increasingly affect software design and quality\nassurance, but legal texts are written in technology-neutral language. This\ncreates challenges for engineers who must develop compliance artifacts such as\nrequirements and acceptance criteria. Manual creation is labor-intensive,\nerror-prone, and requires domain expertise. Advances in Generative AI (GenAI),\nespecially Large Language Models (LLMs), offer a way to automate deriving such\nartifacts.\n  Objective: We present the first systematic human-subject study of LLMs'\nability to derive behavioral specifications from legal texts using a\nquasi-experimental design. These specifications translate legal requirements\ninto a developer-friendly form.\n  Methods: Ten participants evaluated specifications generated from food-safety\nregulations by Claude and Llama. Using Gherkin, a structured BDD language, 60\nspecifications were produced. Each participant assessed 12 across five\ncriteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Each\nspecification was reviewed by two participants, yielding 120 assessments.\n  Results: For Relevance, 75% of ratings were highest and 20% second-highest.\nClarity reached 90% highest. Completeness: 75% highest, 19% second.\nSingularity: 82% highest, 12% second. Time Savings: 68% highest, 24% second. No\nlowest ratings occurred. Mann-Whitney U tests showed no significant differences\nacross participants or models. Llama slightly outperformed Claude in Clarity,\nCompleteness, and Time Savings, while Claude was stronger in Singularity.\nFeedback noted hallucinations and omissions but confirmed the utility of the\nspecifications.\n  Conclusion: LLMs can generate high-quality Gherkin specifications from legal\ntexts, reducing manual effort and providing structured artifacts useful for\nimplementation, assurance, and test generation.", "AI": {"tldr": "LLMs (Claude and Llama) automatically translated legal regulations into developer-ready Gherkin specifications, showing high quality across key criteria in a systematic evaluation. Model outputs reduced manual workload for compliance tasks and are promising for software engineering, though some errors like hallucinations and omissions remain.", "motivation": "Increasing legal and regulatory requirements impact software design and quality assurance, but translating technology-neutral legal language into actionable development requirements is difficult, time-intensive, and demands expertise. Engineers face challenges creating compliance artifacts, driving interest in automation solutions.", "method": "A systematic human-subject, quasi-experimental study was conducted. Ten participants reviewed behavioral specifications generated by two LLMs (Claude and Llama) from food-safety regulations, using Gherkin language. Sixty specifications were evaluated across five criteria: Relevance, Clarity, Completeness, Singularity, and Time Savings, leading to 120 assessments.", "result": "High ratings were achieved for all criteria: Relevance (75% highest), Clarity (90%), Completeness (75%), Singularity (82%), and Time Savings (68%). Llama slightly outperformed Claude in Clarity, Completeness, and Time Savings, while Claude excelled in Singularity. Participant feedback identified some hallucinations and omissions in outputs, but overall utility was confirmed. No significant differences were detected between models or participants statistically.", "conclusion": "LLMs can reliably generate high-quality, developer-friendly Gherkin specifications from legal texts. This automation significantly reduces manual effort in compliance artifact creation and supports implementation, assurance, and test generation."}}
{"id": "2508.20774", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20774", "abs": "https://arxiv.org/abs/2508.20774", "authors": ["Markus Funke", "Patricia Lago"], "title": "Towards an Architectural Perspective for Sustainability: Bundle the Needs from Industry", "comment": null, "summary": "Sustainability is increasingly recognized as an emerging quality property in\nsoftware-intensive systems, yet architects lack structured guidance to address\nit effectively throughout the software design phase. Architectural\nperspectives-an architectural knowledge artifact composed of concerns,\nactivities, tactics, pitfalls, and checklists-offer a promising approach to\ntackle such emerging quality properties across architectural views and are also\nindependent of architecture frameworks and industry contexts. In this paper, we\npresent a sustainability perspective vision, i.e., a revised notion of\narchitectural perspective meant to be filled with its own elements to target\nsustainability concerns. We formulate our sustainability perspective vision\nthrough evidence from applying snowballing to seminal literature and from\nconducting a focus group with experts in the field. Our findings confirm the\nrelevance of the different perspective elements in practice and highlight\nimplications for shaping a sustainability perspective that meets industrial\nneeds.", "AI": {"tldr": "This paper introduces a structured sustainability perspective for software architecture based on literature review and expert input, confirming its practical relevance and outlining elements to guide sustainable design in industry.", "motivation": "There is a growing need for structured guidance on addressing sustainability in software architecture, as it is becoming an important quality property for software-intensive systems.", "method": "The authors used a combination of literature snowballing and expert focus group discussions to gather evidence and insights for formulating a sustainability architectural perspective.", "result": "The findings validate the usefulness of different architectural perspective elements in addressing sustainability and identify practical implications for developing sustainability-focused architectural guidance that aligns with industry requirements.", "conclusion": "A sustainability perspective for software architecture can be effectively shaped by leveraging structured architectural perspectives, informed by both literature and expert opinions."}}
{"id": "2508.20902", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20902", "abs": "https://arxiv.org/abs/2508.20902", "authors": ["Baharin A. Jodat", "Khouloud Gaaloul", "Mehrdad Sabetzadeh", "Shiva Nejati"], "title": "Automated Test Oracles for Flaky Cyber-Physical System Simulators: Approach and Evaluation", "comment": null, "summary": "Simulation-based testing of cyber-physical systems (CPS) is costly due to the\ntime-consuming execution of CPS simulators. In addition, CPS simulators may be\nflaky, leading to inconsistent test outcomes and requiring repeated test\nre-execution for reliable test verdicts. Automated test oracles that do not\nrequire system execution are therefore crucial for reducing testing costs.\nIdeally, such test oracles should be interpretable to facilitate human\nunderstanding of test verdicts, and they must be robust against the potential\nflakiness of CPS simulators. In this article, we propose assertion-based test\noracles for CPS as sets of logical and arithmetic predicates defined over the\ninputs of the system under test. Given a test input, our assertion-based test\noracle determines, without requiring test execution, whether the test passes,\nfails, or if the oracle is inconclusive in predicting a verdict. We describe\ntwo methods for generating assertion-based test oracles: one using genetic\nprogramming~(GP) that employs well-known spectrum-based fault localization\n(SBFL) ranking formulas, namely Ochiai, Tarantula, and Naish, as fitness\nfunctions; and the other using decision trees (DT) and decision rules (DR). We\nevaluate our assertion-based test oracles through case studies in the domains\nof aerospace, networking and autonomous driving. We show that test oracles\ngenerated using GP with Ochiai are significantly more accurate than those\nobtained using GP with Tarantula and Naish or using DT or DR. Moreover, this\naccuracy advantage remains even when accounting for the flakiness of the system\nunder test. We further show that the assertion-based test oracles generated by\nGP with Ochiai are robust against flakiness with only 4% average variation in\ntheir accuracy results across four different network and autonomous driving\nsystems with flaky behaviours.", "AI": {"tldr": "This paper presents assertion-based test oracles for cyber-physical systems that work without simulation. Using genetic programming, especially with the Ochiai formula, these oracles provide accurate and robust test verdicts, even in inconsistent CPS domains, significantly reducing testing costs and effort.", "motivation": "Testing cyber-physical systems (CPS) is expensive and unreliable due to slow and flaky simulators. There is a need for test oracles that do not require system execution and are both interpretable and robust to simulator inconsistencies.", "method": "The authors propose assertion-based test oracles that use sets of logical and arithmetic predicates over test inputs, thus avoiding simulator execution. They introduce two oracle generation methods: one with genetic programming (GP) using SBFL ranking formulas (Ochiai, Tarantula, Naish) as fitness functions, and another using decision trees (DT) and decision rules (DR). The approaches are evaluated in aerospace, networking, and autonomous driving case studies.", "result": "The GP-based test oracle with Ochiai significantly outperforms alternatives (Tarantula, Naish, DT, DR), in both accuracy and robustness against flaky simulator behaviours, with only 4% average accuracy variation observed across diverse CPS domains.", "conclusion": "Assertion-based test oracles, especially those generated with GP using Ochiai, offer an interpretable, robust, and accurate solution for CPS testing without the need for costly or unreliable simulation runs."}}
{"id": "2508.20911", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20911", "abs": "https://arxiv.org/abs/2508.20911", "authors": ["Zuocheng Feng", "Kaiwen Zhang", "Miaomiao Wang", "Yiming Cheng", "Yuandao Cai", "Xiaofeng Li", "Guanjun Liu"], "title": "Deep Learning Based Concurrency Bug Detection and Localization", "comment": null, "summary": "Concurrency bugs, caused by improper synchronization of shared resources in\nmulti-threaded or distributed systems, are notoriously hard to detect and thus\ncompromise software reliability and security. The existing deep learning\nmethods face three main limitations. First, there is an absence of large and\ndedicated datasets of diverse concurrency bugs for them. Second, they lack\nsufficient representation of concurrency semantics. Third, binary\nclassification results fail to provide finer-grained debug information such as\nprecise bug lines. To address these problems, we propose a novel method for\neffective concurrency bug detection as well as localization. We construct a\ndedicated concurrency bug dataset to facilitate model training and evaluation.\nWe then integrate a pre-trained model with a heterogeneous graph neural network\n(GNN), by incorporating a new Concurrency-Aware Code Property Graph (CCPG) that\nconcisely and effectively characterizes concurrency semantics. To further\nfacilitate debugging, we employ SubgraphX, a GNN-based interpretability method,\nwhich explores the graphs to precisely localize concurrency bugs, mapping them\nto specific lines of source code. On average, our method demonstrates an\nimprovement of 10\\% in accuracy and precision and 26\\% in recall compared to\nstate-of-the-art methods across diverse evaluation settings.", "AI": {"tldr": "This paper proposes a new GNN-based approach, supported by a novel dataset and interpretability techniques, to better detect and localize concurrency bugs, outperforming current methods in accuracy, precision, and recall.", "motivation": "Concurrency bugs are difficult to detect and harm software reliability and security. Deep learning approaches face challenges due to limited datasets, poor representation of concurrency semantics, and coarse detection granularity.", "method": "The paper introduces a novel method combining a pre-trained model with a heterogeneous graph neural network (GNN) and a Concurrency-Aware Code Property Graph (CCPG) to represent concurrency semantics. SubgraphX, a GNN-based interpretability tool, is used for bug localization. A dedicated concurrency bug dataset is also constructed for model training and evaluation.", "result": "The proposed approach achieves an average improvement of 10% in accuracy and precision, and 26% in recall over existing state-of-the-art methods across various settings.", "conclusion": "The method effectively improves the detection and localization of concurrency bugs by addressing key shortcomings of previous approaches and offering more fine-grained debugging information."}}
{"id": "2508.20977", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.20977", "abs": "https://arxiv.org/abs/2508.20977", "authors": ["Shiwen Shan", "Yintong Huo", "Yuxin Su", "Zhining Wang", "Dan Li", "Zibin Zheng"], "title": "ConfLogger: Enhance Systems' Configuration Diagnosability through Configuration Logging", "comment": "13 pages, 6 figures, accpeted by ICSE '26 (The 48th IEEE/ACM\n  International Conference on Software Engineering)", "summary": "Modern configurable systems offer customization via intricate configuration\nspaces, yet such flexibility introduces pervasive configuration-related issues\nsuch as misconfigurations and latent softwarebugs. Existing diagnosability\nsupports focus on post-failure analysis of software behavior to identify\nconfiguration issues, but none of these approaches look into whether the\nsoftware clue sufficient failure information for diagnosis. To fill in the\nblank, we propose the idea of configuration logging to enhance existing logging\npractices at the source code level. We develop ConfLogger, the first tool that\nunifies configuration-aware static taint analysis with LLM-based log generation\nto enhance software configuration diagnosability. Specifically, our method 1)\nidentifies configuration-sensitive code segments by tracing\nconfiguration-related data flow in the whole project, and 2) generates\ndiagnostic log statements by analyzing configuration code contexts. Evaluation\nresults on eight popular software systems demonstrate the effectiveness of\nConfLogger to enhance configuration diagnosability. Specifically,\nConfLogger-enhanced logs successfully aid a log-based misconfiguration\ndiagnosis tool to achieve 100% accuracy on error localization in 30 silent\nmisconfiguration scenarios, with 80% directly resolvable through explicit\nconfiguration information exposed. In addition, ConfLogger achieves 74%\ncoverage of existing logging points, outperforming baseline LLM-based loggers\nby 12% and 30%. It also gains 8.6% higher in precision, 79.3% higher in recall,\nand 26.2% higher in F1 compared to the state-of-the-art baseline in terms of\nvariable logging while also augmenting diagnostic value. A controlled user\nstudy on 22 cases further validated its utility, speeding up diagnostic time by\n1.25x and improving troubleshooting accuracy by 251.4%.", "AI": {"tldr": "ConfLogger, a tool that integrates static taint analysis with LLM-generated logs, greatly improves the diagnosis of configuration errors by enhancing log coverage and utility, leading to faster and more accurate troubleshooting in real software systems.", "motivation": "Configuration flexibility in modern systems leads to misconfigurations and latent software bugs. Existing diagnostic methods focus on analyzing software post-failure but do not ensure that failure information is sufficient for effective diagnosis.", "method": "The authors propose 'configuration logging' and introduce ConfLogger, a tool that combines configuration-aware static taint analysis with LLM-based log generation. This involves detecting configuration-sensitive code segments via data flow tracing and producing diagnostic logs based on the context.", "result": "ConfLogger enhances diagnosability: in tests on eight popular systems, it enabled 100% accuracy for error localization in 30 silent misconfiguration cases, with 80% being directly resolvable. It covers 74% of existing logging points (12-30% more than LLM baselines), and surpasses state-of-the-art baselines in variable logging precision (+8.6%), recall (+79.3%), and F1 (+26.2%). A user study showed 1.25x faster diagnostics and 251.4% better troubleshooting accuracy.", "conclusion": "ConfLogger significantly improves configuration diagnosability through increased log coverage and quality, outperforming existing tools both in automated metrics and user studies. Its approach provides explicit configuration information, streamlining misconfiguration diagnosis and resolution."}}
{"id": "2508.21050", "categories": ["cs.SE", "cs.CY", "K.2; K.6.3; K.4; K.7"], "pdf": "https://arxiv.org/pdf/2508.21050", "abs": "https://arxiv.org/abs/2508.21050", "authors": ["Thomas J. Misa"], "title": "Dynamics of Gender Bias in Software Engineering", "comment": "26 pages, 3 figures", "summary": "The field of software engineering is embedded in both engineering and\ncomputer science, and may embody gender biases endemic to both. This paper\nsurveys software engineering's origins and its long-running attention to\nengineering professionalism, profiling five leaders; it then examines the\nfield's recent attention to gender issues and gender bias. It next\nquantitatively analyzes women's participation as research authors in the\nfield's leading International Conference of Software Engineering (1976-2010),\nfinding a dozen years with statistically significant gender exclusion. Policy\ndimensions of research on gender bias in computing are suggested.", "AI": {"tldr": "This paper explores the history and gender dynamics of software engineering, quantitatively showing significant gender exclusion in authorship at a leading conference over several decades, and calls for policy actions to address persistent gender bias.", "motivation": "Software engineering has roots in both engineering and computer science, fields known to exhibit gender biases. The paper aims to understand how these biases are reflected in software engineering, particularly regarding women's participation.", "method": "The authors conduct a historical survey of the field, profile five key leaders, review recent literature about gender bias, and perform a quantitative analysis of women's representation among authors at the International Conference of Software Engineering (ICSE) from 1976 to 2010.", "result": "The analysis found twelve years within the 34-year span when women's participation as research authors at ICSE was statistically significantly lower, indicating notable gender exclusion. The paper also provides suggestions regarding policy implications for reducing gender bias in computing.", "conclusion": "Gender exclusion has been a recurring and measurable issue in software engineering research. Addressing this exclusion is important for making progress toward gender equity, and the field should consider policy changes to foster greater inclusivity."}}
