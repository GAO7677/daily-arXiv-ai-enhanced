{"id": "2506.15884", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.15884", "abs": "https://arxiv.org/abs/2506.15884", "authors": ["Shamse Tasnim Cynthia", "Nuri Almarimi", "Banani Roy"], "title": "How Do Community Smells Influence Self-Admitted Technical Debt in Machine Learning Projects?", "comment": null, "summary": "Community smells reflect poor organizational practices that often lead to\nsocio-technical issues and the accumulation of Self-Admitted Technical Debt\n(SATD). While prior studies have explored these problems in general software\nsystems, their interplay in machine learning (ML)-based projects remains\nlargely underexamined. In this study, we investigated the prevalence of\ncommunity smells and their relationship with SATD in open-source ML projects,\nanalyzing data at the release level. First, we examined the prevalence of ten\ncommunity smell types across the releases of 155 ML-based systems and found\nthat community smells are widespread, exhibiting distinct distribution patterns\nacross small, medium, and large projects. Second, we detected SATD at the\nrelease level and applied statistical analysis to examine its correlation with\ncommunity smells. Our results showed that certain smells, such as Radio Silence\nand Organizational Silos, are strongly correlated with higher SATD occurrences.\nThird, we considered the six identified types of SATD to determine which\ncommunity smells are most associated with each debt category. Our analysis\nrevealed authority- and communication-related smells often co-occur with\npersistent code and design debt. Finally, we analyzed how the community smells\nand SATD evolve over the releases, uncovering project size-dependent trends and\nshared trajectories. Our findings emphasize the importance of early detection\nand mitigation of socio-technical issues to maintain the long-term quality and\nsustainability of ML-based systems.", "AI": {"tldr": "This paper finds that poor organizational practices ('community smells') are widespread and strongly linked to technical debt in open-source ML projects, especially as projects grow. Early identification and mitigation of these issues are important for maintaining high-quality, sustainable ML systems.", "motivation": "Machine learning (ML)-based projects are becoming increasingly complex, and poor organizational practices can lead to socio-technical issues and technical debt. However, the relationship between community smells (organizational issues) and self-admitted technical debt (SATD) is not well-understood in the context of open-source ML projects.", "method": "The authors analyzed 155 open-source ML-based systems at the release level, identifying ten types of community smells and six categories of SATD. They applied statistical analysis to detect correlations between community smells and SATD, examined how these issues co-occur, and studied their evolution over project releases and across different project sizes.", "result": "Community smells are common in ML projects, with distinct patterns across project sizes. Certain community smells, specifically those related to communication and organizational structure, correlate strongly with higher levels of SATD, particularly code and design debt. Authority- and communication-related smells often co-occur with technical debt types. The trends and trajectories of these issues also depend on project size.", "conclusion": "Early detection and addressing of community smells and SATD are critical for preserving quality and sustainability in ML-based software projects. Socio-technical issues and their evolution over the project lifecycle need careful management, especially as project size influences these issues."}}
{"id": "2506.16101", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.16101", "abs": "https://arxiv.org/abs/2506.16101", "authors": ["Yupeng Jiang", "Shuaiyi Sun", "Xi Zheng"], "title": "Regression Testing Optimization for ROS-based Autonomous Systems: A Comprehensive Review of Techniques", "comment": null, "summary": "Regression testing plays a critical role in maintaining software reliability,\nparticularly for ROS-based autonomous systems (ROSAS), which frequently undergo\ncontinuous integration and iterative development. However, conventional\nregression testing techniques face significant challenges when applied to\nautonomous systems due to their dynamic and non-deterministic behaviors,\ncomplex multi-modal sensor data, asynchronous distributed architectures, and\nstringent safety and real-time constraints. Although numerous studies have\nexplored test optimization in traditional software contexts, regression testing\noptimization specifically for ROSAS remains largely unexplored. To address this\ngap, we present the first comprehensive survey systematically reviewing\nregression testing optimization techniques tailored for ROSAS. We analyze and\ncategorize 122 representative studies into regression test case prioritization,\nminimization, and selection methods. A structured taxonomy is introduced to\nclearly illustrate their applicability and limitations within ROSAS contexts.\nFurthermore, we highlight major challenges specific to regression testing for\nROSAS, including effectively prioritizing tests in response to frequent system\nmodifications, efficiently minimizing redundant tests, and difficulty in\naccurately selecting impacted test cases. Finally, we propose research insights\nand identify promising future directions, such as leveraging frame-to-vector\ncoverage metrics, multi-source foundation models, and neurosymbolic reasoning\nto enhance regression testing efficiency and effectiveness. This survey\nprovides a foundational reference and practical roadmap for advancing the\nstate-of-the-art in regression testing optimization for ROSAS.", "AI": {"tldr": "This paper provides the first comprehensive survey of regression testing optimization techniques for ROS-based autonomous systems (ROSAS), categorizing 122 studies, identifying domain-specific challenges and gaps, and mapping future research directions to enhance testing efficiency and effectiveness in autonomous systems.", "motivation": "Regression testing for ROS-based autonomous systems (ROSAS) is challenging due to dynamic behaviors, complex sensor data, distributed architectures, and strict safety constraints, but existing optimization research in this context is sparse.", "method": "A comprehensive survey was conducted, systematically reviewing and categorizing 122 representative studies on regression testing optimization. The studies are analyzed and grouped by prioritization, minimization, and selection methods, and a structured taxonomy is presented to clarify their applicability and limitations for ROSAS.", "result": "The survey identified the unique challenges of regression testing in ROSAS and mapped out existing methods' strengths and weaknesses. It also highlighted gaps and proposed future research directions, such as new coverage metrics and the use of advanced AI techniques for more effective testing.", "conclusion": "This work offers a foundational survey and practical roadmap, establishing a baseline reference for advancing regression testing optimization in ROSAS."}}
{"id": "2506.16136", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.16136", "abs": "https://arxiv.org/abs/2506.16136", "authors": ["Kai Huang", "Jian Zhang", "Xiaofei Xie", "Chunyang Chen"], "title": "Seeing is Fixing: Cross-Modal Reasoning with Multimodal LLMs for Visual Software Issue Fixing", "comment": null, "summary": "Large language model-(LLM) based automated program repair (APR) techniques\nhave shown promising results in resolving real-world GitHub issue tasks.\nExisting APR systems are primarily evaluated in unimodal settings (e.g.,\nSWE-bench). However, these autonomous systems struggle to resolve multimodal\nproblem scenarios (e.g., SWE-bench M) due to limitations in interpreting and\nleveraging visual information. In multimodal scenarios, LLMs need to rely on\nvisual information in the graphical user interface (GUI) to understand bugs and\ngenerate fixes. To bridge this gap, we propose GUIRepair, a cross-modal\nreasoning approach for resolving multimodal issue scenarios by understanding\nand capturing visual information. Specifically, GUIRepair integrates two key\ncomponents, Image2Code and Code2Image, to enhance fault comprehension and patch\nvalidation. Image2Code extracts relevant project documents based on the issue\nreport, then applies this domain knowledge to generate the reproduced code\nresponsible for the visual symptoms, effectively translating GUI images into\nexecutable context for better fault comprehension. Code2Image replays the\nvisual issue scenario using the reproduced code and captures GUI renderings of\nthe patched program to assess whether the fix visually resolves the issue,\nproviding feedback for patch validation. We evaluate GUIRepair on SWE-bench M,\nand the approach demonstrates significant effectiveness. When utilizing GPT-4o\nas the base model, GUIRepair solves 157 instances, outperforming the best\nopen-source baseline by 26 instances. Furthermore, when using o4-mini as the\nbase model, GUIRepair can achieve even better results and solve 175 instances,\noutperforming the top commercial system by 22 instances. This emphasizes the\nsuccess of our new perspective on incorporating cross-modal reasoning by\nunderstanding and capturing visual information to resolve multimodal issues.", "AI": {"tldr": "GUIRepair introduces cross-modal reasoning to automated program repair, using both textual and visual information to significantly outperform existing methods in fixing issues that involve GUIs.", "motivation": "Existing LLM-based automated program repair techniques struggle to resolve multimodal issues that require interpretation of visual information from GUIs, limiting their effectiveness to unimodal (text-based) scenarios.", "method": "The authors introduce GUIRepair, a cross-modal reasoning system with two core components: Image2Code (translates GUI images into contextually relevant code for fault comprehension) and Code2Image (replays and captures patched program GUI to aid patch validation). This allows the system to understand and utilize visual information in issue resolution.", "result": "When evaluated on the SWE-bench M benchmark, GUIRepair, with GPT-4o as the base model, successfully solved 157 instances, surpassing the best open-source baseline by 26. Using o4-mini, it solved 175 instances, outperforming the top commercial system by 22.", "conclusion": "GUIRepair's integration of cross-modal (visual and textual) reasoning fundamentally improves automated program repair performance on multimodal issues, demonstrating the importance and effectiveness of leveraging visual information for bug understanding and validation."}}
{"id": "2506.16214", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.16214", "abs": "https://arxiv.org/abs/2506.16214", "authors": ["Klara Borowa", "Andrzej Ratkowski", "Roberto Verdecchia"], "title": "The Technical Debt Gamble: A Case Study on Technical Debt in a Large-Scale Industrial Microservice Architecture", "comment": "Preprint accepted to Journal of Systems and Software", "summary": "Microservice architectures provide an intuitive promise of high\nmaintainability and evolvability due to loose coupling. However, these quality\nattributes are notably vulnerable to technical debt (TD). Few studies address\nTD in microservice systems, particularly on a large scale. This research\nexplores how TD manifests in a large-scale microservice-based industrial\nsystem. The research is based on a mixed-method case study of a project\nincluding over 100 microservices and serving over 15k locations. Results are\ncollected via a quantitative method based static code analyzers combined with\nqualitative insights derived from a focus group discussion with the development\nteam and a follow-up interview with the lead architect of the case study\nsystem. Results show that (1) simple static source code analysis can be an\nefficient and effective entry point for holistic TD discovery, (2) inadequate\ncommunication significantly contributes to TD, (3) misalignment between\narchitectural and organizational structures can exacerbate TD accumulation, (4)\nmicroservices can rapidly cycle through TD accumulation and resolution, a\nphenomenon referred to as \"microservice architecture technical debt gamble\".\nFinally, we identify a set of fitting strategies for TD management in\nmicroservice architectures.", "AI": {"tldr": "This paper investigates how technical debt arises and can be managed in large-scale microservice systems, using a case study that blends static code analysis with team interviews. Key findings suggest that straightforward tools like static analyzers are effective at spotting technical debt, but human factors like communication and structure play an equally important role. Strategies are provided for better technical debt management.", "motivation": "Microservice architectures are believed to enhance maintainability and evolvability due to their loose coupling. However, technical debt can threaten these promised benefits, and there is limited research on how technical debt manifests in large-scale microservice systems.", "method": "The study uses a mixed-method case study involving a system with over 100 microservices operating in more than 15,000 locations. It combines quantitative data from static code analyzers with qualitative data from a focus group discussion with the development team and a follow-up interview with the lead architect.", "result": "The study finds that static source code analysis is an effective approach for detecting technical debt at scale. It also identifies inadequate communication and misalignment between architecture and organizational structure as key contributors to technical debt. Additionally, microservices can cycle quickly between accumulating and resolving technical debt, termed as 'microservice architecture technical debt gamble.' The research also proposes strategies for managing technical debt in microservice systems.", "conclusion": "Technical debt is a significant concern in large-scale microservice architectures, driven by communication issues and structural misalignments. Simple static analysis can help identify debt, but ongoing management requires both technical and organizational strategies."}}
{"id": "2506.15875", "categories": ["cs.PL", "cs.AR", "cs.DC", "cs.ET", "D.3; D.1; I.6; J.2"], "pdf": "https://arxiv.org/pdf/2506.15875", "abs": "https://arxiv.org/abs/2506.15875", "authors": ["Dirk Van Essendelft", "Patrick Wingo", "Terry Jordan", "Ryan Smith", "Wissam Saidi"], "title": "A System Level Compiler for Massively-Parallel, Spatial, Dataflow Architectures", "comment": "26 pages, 5 figures, 14 listings", "summary": "We have developed a novel compiler called the Multiple-Architecture Compiler\nfor Advanced Computing Hardware (MACH) designed specifically for\nmassively-parallel, spatial, dataflow architectures like the Wafer Scale\nEngine. Additionally, MACH can execute code on traditional unified-memory\ndevices. MACH addresses the complexities in compiling for spatial architectures\nthrough a conceptual Virtual Machine, a flexible domain-specific language, and\na compiler that can lower high-level languages to machine-specific code in\ncompliance with the Virtual Machine concept. While MACH is designed to be\noperable on several architectures and provide the flexibility for several\nstandard and user-defined data mappings, we introduce the concept with dense\ntensor examples from NumPy and show lowering to the Wafer Scale Engine by\ntargeting Cerebras' hardware specific languages.", "AI": {"tldr": "MACH is a new compiler framework that simplifies programming for advanced parallel dataflow hardware and regular unified-memory systems. By using a virtual machine abstraction and a custom domain-specific language, MACH can lower high-level code, like NumPy tensor operations, to run efficiently on architectures such as Cerebras' Wafer Scale Engine.", "motivation": "There is a growing need for efficient compilation tools targeting massively-parallel spatial dataflow architectures, which pose unique challenges not addressed by traditional compilers developed for unified-memory architectures. Existing tools lack flexibility, generality, and ease of support for new hardware.", "method": "The paper proposes a novel compiler architecture (MACH) built with a conceptual Virtual Machine, a domain-specific language, and a flexible compiler pipeline capable of lowering high-level languages (e.g., NumPy) to hardware-specific implementations, particularly for spatial architectures like the Wafer Scale Engine.", "result": "MACH enables compilation of high-level dense tensor operations (e.g., from NumPy) down to specific implementations on spatial architectures such as Cerebras' Wafer Scale Engine. It demonstrates operability across diverse architectures and incorporation of both standardized and user-defined data mappings.", "conclusion": "MACH successfully provides a unified compilation framework for both advanced spatial dataflow and traditional unified-memory architectures, demonstrating flexible and efficient code generation capabilities."}}
{"id": "2506.16440", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.16440", "abs": "https://arxiv.org/abs/2506.16440", "authors": ["Ebube Alor", "SayedHassan Khatoonabadi", "Emad Shihab"], "title": "Evaluating the Use of LLMs for Documentation to Code Traceability", "comment": null, "summary": "Large Language Models (LLMs) offer new potential for automating\ndocumentation-to-code traceability, yet their capabilities remain\nunderexplored. We present a comprehensive evaluation of LLMs (Claude 3.5\nSonnet, GPT-4o, and o3-mini) in establishing trace links between various\nsoftware documentation (including API references and user guides) and source\ncode. We create two novel datasets from two open-source projects (Unity Catalog\nand Crawl4AI). Through systematic experiments, we assess three key\ncapabilities: (1) trace link identification accuracy, (2) relationship\nexplanation quality, and (3) multi-step chain reconstruction. Results show that\nthe best-performing LLM achieves F1-scores of 79.4% and 80.4% across the two\ndatasets, substantially outperforming our baselines (TF-IDF, BM25, and\nCodeBERT). While fully correct relationship explanations range from 42.9% to\n71.1%, partial accuracy exceeds 97%, indicating that fundamental connections\nare rarely missed. For multi-step chains, LLMs maintain high endpoint accuracy\nbut vary in capturing precise intermediate links. Error analysis reveals that\nmany false positives stem from naming-based assumptions, phantom links, or\novergeneralization of architectural patterns. We demonstrate that task-framing,\nsuch as a one-to-many matching strategy, is critical for performance. These\nfindings position LLMs as powerful assistants for trace discovery, but their\nlimitations could necessitate human-in-the-loop tool design and highlight\nspecific error patterns for future research.", "AI": {"tldr": "LLMs substantially improve automated documentation-to-code traceability over current baselines, but require careful task design and possibly human-in-the-loop approaches due to persistent error patterns, particularly in explanation and multi-step linking tasks.", "motivation": "The motivation for this paper is to address the underexplored capabilities of Large Language Models (LLMs) in automating documentation-to-code traceability in software engineering, which has traditionally been a labor-intensive process.", "method": "The authors conduct systematic experiments using three advanced LLMs (Claude 3.5 Sonnet, GPT-4o, and o3-mini) and compare their performance against classic and neural IR baselines (TF-IDF, BM25, and CodeBERT) on two newly created datasets derived from open-source projects. The study assesses three main capabilities: trace link identification accuracy, relationship explanation quality, and multi-step chain reconstruction.", "result": "The best-performing LLM achieves F1-scores of 79.4% and 80.4% on the two datasets, significantly outperforming the baselines. Relationship explanation quality is solid, with fully correct explanations between 42.9% and 71.1%, and partial accuracy above 97%. Multi-step chain reconstruction shows high endpoint accuracy but more variability in intermediate links. Error analysis identifies common error patterns such as naming assumptions and overgeneralization.", "conclusion": "LLMs are powerful tools for discovering trace links between documentation and code, clearly surpassing current baselines. However, their limitations in explanation quality and chain reconstruction suggest a need for human oversight and further research into mitigating specific error patterns."}}
{"id": "2506.16048", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2506.16048", "abs": "https://arxiv.org/abs/2506.16048", "authors": ["Byeongjee Kang", "Harsh Desai", "Limin Jia", "Brandon Lucia"], "title": "WAMI: Compilation to WebAssembly through MLIR without Losing Abstraction", "comment": null, "summary": "WebAssembly (Wasm) is a portable bytecode format that serves as a compilation\ntarget for high-level languages, enabling their secure and efficient execution\nacross diverse platforms, including web browsers and embedded systems. To\nimprove support for high-level languages without incurring significant code\nsize or performance overheads, Wasm continuously evolves by integrating\nhigh-level features such as Garbage Collection and Stack Switching. However,\nexisting compilation approaches either lack reusable design -- requiring\nredundant implementation efforts for each language -- or lose abstraction by\nlowering high-level constructs into low-level shared representations like LLVM\nIR, which hinder the adoption of high-level features. MLIR compiler\ninfrastructure provides the compilation pipeline with multiple levels of\nabstraction, preserving high-level abstractions throughout the compilation\npipeline, yet the current MLIR pipeline relies on the LLVM backend for Wasm\ncode generation, thereby inheriting LLVM's limitations.\n  This paper presents a novel compilation pipeline for Wasm, featuring Wasm\ndialects explicitly designed to represent high-level Wasm constructs within\nMLIR. Our approach enables direct generation of high-level Wasm code from\ncorresponding high-level MLIR dialects without losing abstraction, providing a\nmodular and extensible way to incorporate high-level Wasm features. We\nillustrate this extensibility through a case study that leverages Stack\nSwitching, a recently introduced high-level feature of Wasm. Performance\nevaluations on PolyBench benchmarks show that our pipeline, benefiting from\noptimizations within the MLIR and Wasm ecosystems, produces code with at most\n7.7\\% slower, and faster in some execution environments, compared to LLVM-based\ncompilers.", "AI": {"tldr": "The paper proposes a Wasm-centric MLIR compilation pipeline that keeps high-level abstractions, making it easier to adopt new Wasm features without significant performance loss compared to traditional LLVM-based compilers.", "motivation": "WebAssembly (Wasm) is evolving to better support high-level language features but current compiler approaches either require duplicated efforts for each language or lose crucial high-level abstractions. Existing pipelines, such as those using MLIR, still depend on LLVM, which does not preserve these abstractions for Wasm code generation.", "method": "The paper introduces a new compilation pipeline for Wasm that utilizes Wasm-specific dialects in MLIR, allowing high-level Wasm constructs to be directly represented and generated. The approach offers modularity and extensibility for adopting Wasm high-level features. This is demonstrated via a case study on the Stack Switching feature, and performance benchmarking is conducted using PolyBench.", "result": "The new pipeline can maintain high-level abstractions and directly generate Wasm code, facilitating easier adoption of Wasm features. Benchmarks show that the generated code is at most 7.7% slower -- and sometimes even faster -- than code created by LLVM-based compilers, indicating competitive performance.", "conclusion": "A Wasm-centric MLIR compilation pipeline can produce efficient, high-level code and support the evolving Wasm feature set while maintaining competitive performance with traditional LLVM-based methods."}}
{"id": "2506.16453", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.16453", "abs": "https://arxiv.org/abs/2506.16453", "authors": ["Buthayna AlMulla", "Maram Assi", "Safwat Hassan"], "title": "Understanding the Challenges and Promises of Developing Generative AI Apps: An Empirical Study", "comment": "45 pages, 24 figures, 7 tables", "summary": "The release of ChatGPT in 2022 triggered a rapid surge in generative\nartificial intelligence mobile apps (i.e., Gen-AI apps). Despite widespread\nadoption, little is known about how end users perceive and evaluate these\nGen-AI functionalities in practice. In this work, we conduct a user-centered\nanalysis of 676,066 reviews from 173 Gen-AI apps on the Google Play Store. We\nintroduce a four-phase methodology, SARA (Selection, Acquisition, Refinement,\nand Analysis), that enables the systematic extraction of user insights using\nprompt-based LLM techniques. First, we demonstrate the reliability of LLMs in\ntopic extraction, achieving 91% accuracy through five-shot prompting and\nnon-informative review filtering. Then, we apply this method to the informative\nreviews, identify the top 10 user-discussed topics (e.g., AI Performance,\nContent Quality, and Content Policy & Censorship) and analyze the key\nchallenges and emerging opportunities. Finally, we examine how these topics\nevolve over time, offering insight into shifting user expectations and\nengagement patterns with Gen-AI apps. Based on our findings and observations,\nwe present actionable implications for developers and researchers.", "AI": {"tldr": "By analyzing over 670,000 reviews from 173 generative AI apps using a novel LLM-based methodology, this paper identifies key user concerns, tracks evolving expectations, and offers guidance for improving future Gen-AI app development.", "motivation": "Despite the growing popularity and widespread adoption of generative AI mobile apps following ChatGPT's release, there is limited understanding of how users actually perceive and evaluate these apps in real-world settings. The study aims to address this gap by systematically analyzing user feedback.", "method": "The authors analyze 676,066 user reviews from 173 Gen-AI apps in the Google Play Store. They introduce a four-phase methodology called SARA (Selection, Acquisition, Refinement, and Analysis), utilizing prompt-based large language model techniques for extracting user insights. The methodology is validated for topic extraction accuracy (91%) using five-shot prompting and review filtering. They identify and track key topics over time to understand user discussions and changing patterns.", "result": "The study demonstrates that large language models can reliably extract topics from user reviews. Ten key topics discussed by users, such as AI Performance, Content Quality, and Content Policy & Censorship, are identified. The evolution of these topics highlights changing user expectations and engagement patterns. The research offers actionable insights for app developers and researchers.", "conclusion": "LLM-driven, prompt-based methodologies are effective for extracting and analyzing large-scale mobile app user feedback, revealing critical user concerns and trends in Gen-AI app use. The insights can inform future app development and research into generative AI user experiences."}}
{"id": "2506.16883", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2506.16883", "abs": "https://arxiv.org/abs/2506.16883", "authors": ["Christoph Jung", "C. F. Bolz-Tereick"], "title": "Low Overhead Allocation Sampling in a Garbage Collected Virtual Machine", "comment": null, "summary": "Compared to the more commonly used time-based profiling, allocation profiling\nprovides an alternate view of the execution of allocation heavy dynamically\ntyped languages. However, profiling every single allocation in a program is\nvery inefficient. We present a sampling allocation profiler that is deeply\nintegrated into the garbage collector of PyPy, a Python virtual machine. This\nintegration ensures tunable low overhead for the allocation profiler, which we\nmeasure and quantify. Enabling allocation sampling profiling with a sampling\nperiod of 4 MB leads to a maximum time overhead of 25% in our benchmarks, over\nun-profiled regular execution.", "AI": {"tldr": "The paper presents a low-overhead, sampling-based allocation profiler for PyPy, integrated into its garbage collector, allowing efficient profiling of allocation-heavy Python programs with a moderate performance cost.", "motivation": "Allocation profiling in dynamically typed, allocation-heavy languages offers important insights not visible in traditional time-based profiling, but existing allocation profiling methods are highly inefficient due to the overhead of tracking every allocation.", "method": "The authors introduce a sampling-based allocation profiler that integrates directly with the garbage collector in PyPy, enabling selective and efficient profiling with adjustable granularity.", "result": "With a sampling period of 4 MB, the profiler incurs a maximum runtime overhead of 25% in the benchmarks tested.", "conclusion": "A garbage-collector-integrated sampling allocation profiler can deliver useful allocation profiling data in PyPy with manageable and tunable overhead, making it a practical tool for performance analysis in allocation-heavy programs."}}
{"id": "2506.16557", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.16557", "abs": "https://arxiv.org/abs/2506.16557", "authors": ["Hern\u00e1n Gagliardi", "Victor Braberman", "Sebastian Uchitel"], "title": "Scaling GR(1) Synthesis via a Compositional Framework for LTL Discrete Event Control", "comment": "To be published in CAV25", "summary": "We present a compositional approach to controller synthesis of discrete event\nsystem controllers with linear temporal logic (LTL) goals. We exploit the\nmodular structure of the plant to be controlled, given as a set of labelled\ntransition systems (LTS), to mitigate state explosion that monolithic\napproaches to synthesis are prone to. Maximally permissive safe controllers are\niteratively built for subsets of the plant LTSs by solving weaker control\nproblems. Observational synthesis equivalence is used to reduce the size of the\ncontrolled subset of the plant by abstracting away local events. The result of\nsynthesis is also compositional, a set of controllers that when run in parallel\nensure the LTL goal. We implement synthesis in the MTSA tool for an expressive\nsubset of LTL, GR(1), and show it computes solutions to that can be up to 1000\ntimes larger than those that the monolithic approach can solve.", "AI": {"tldr": "The paper proposes a modular, compositional approach to synthesizing controllers for large systems with LTL goals, avoiding state explosion and enabling solutions for much larger problems than traditional methods.", "motivation": "Traditional, monolithic controller synthesis methods for discrete event systems with LTL goals suffer from the state explosion problem, making them impractical for large or modular systems. The authors aim to address these scalability challenges.", "method": "A compositional approach is used, leveraging the modular structure of the plant, modeled as labelled transition systems (LTS). Controllers are iteratively synthesized for subsets of the system, using maximally permissive safe controllers and weakened control problems. Observational synthesis equivalence helps abstract away local events, and the result is a set of controllers that work together to satisfy the LTL goals. The approach is implemented for the GR(1) subset of LTL in the MTSA tool.", "result": "The compositional method can handle synthesis problems that are up to 1000 times larger than those solvable by conventional monolithic approaches. The approach leads to practical controller synthesis for much larger systems.", "conclusion": "Compositional controller synthesis, with proper modularization and abstraction, enables tractable solutions for large discrete event systems with LTL goals, outperforming monolithic solutions in scalability and efficiency."}}
{"id": "2506.16586", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.16586", "abs": "https://arxiv.org/abs/2506.16586", "authors": ["Ihor Pysmennyi", "Roman Kyslyi", "Kyrylo Kleshch"], "title": "AI-Driven Tools in Modern Software Quality Assurance: An Assessment of Benefits, Challenges, and Future Directions", "comment": "11 pages, 9 figures", "summary": "Traditional quality assurance (QA) methods face significant challenges in\naddressing the complexity, scale, and rapid iteration cycles of modern software\nsystems and are strained by limited resources available, leading to substantial\ncosts associated with poor quality. The object of this research is the Quality\nAssurance processes for modern distributed software applications. The subject\nof the research is the assessment of the benefits, challenges, and prospects of\nintegrating modern AI-oriented tools into quality assurance processes. We\nperformed comprehensive analysis of implications on both verification and\nvalidation processes covering exploratory test analyses, equivalence\npartitioning and boundary analyses, metamorphic testing, finding\ninconsistencies in acceptance criteria (AC), static analyses, test case\ngeneration, unit test generation, test suit optimization and assessment, end to\nend scenario execution. End to end regression of sample enterprise application\nutilizing AI-agents over generated test scenarios was implemented as a proof of\nconcept highlighting practical use of the study. The results, with only 8.3%\nflaky executions of generated test cases, indicate significant potential for\nthe proposed approaches. However, the study also identified substantial\nchallenges for practical adoption concerning generation of semantically\nidentical coverage, \"black box\" nature and lack of explainability from\nstate-of-the-art Large Language Models (LLMs), the tendency to correct mutated\ntest cases to match expected results, underscoring the necessity for thorough\nverification of both generated artifacts and test execution results. The\nresearch demonstrates AI's transformative potential for QA but highlights the\nimportance of a strategic approach to implementing these technologies,\nconsidering the identified limitations and the need for developing appropriate\nverification methodologies.", "AI": {"tldr": "AI-powered QA tools can greatly improve software quality assurance efficiency and scope, but practical use is challenged by coverage, explainability, and reliability issues, requiring careful, methodical implementation for real-world adoption.", "motivation": "Traditional QA methods struggle with modern software's complexity, scale, rapid iteration, and limited resources, leading to high costs from poor quality.", "method": "The study conducts a comprehensive analysis of integrating AI-oriented tools into QA processes for distributed applications. It examines impacts on verification and validation, covering exploratory testing, metamorphic testing, static analysis, test case/unit test generation, and end-to-end regression testing using AI agents over generated test scenarios as a proof of concept.", "result": "AI-driven QA showed promising results\u2014with only 8.3% flaky test executions\u2014but practical challenges were found. These include issues with achieving semantically identical coverage, black-box nature and limited explainability of LLMs, and their tendency to adapt test cases improperly, all necessitating better verification processes.", "conclusion": "AI technologies have significant transformative potential for QA, but successful adoption requires a strategic approach and development of robust verification methodologies to address the outlined limitations."}}
{"id": "2506.16639", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.16639", "abs": "https://arxiv.org/abs/2506.16639", "authors": ["Boqi Chen", "Aren A. Babikian", "Shuzhao Feng", "D\u00e1niel Varr\u00f3", "Gunter Mussbacher"], "title": "LLM-based Satisfiability Checking of String Requirements by Consistent Data and Checker Generation", "comment": "Accepted at the 33rd IEEE International Requirements Engineering 2025\n  conference", "summary": "Requirements over strings, commonly represented using natural language (NL),\nare particularly relevant for software systems due to their heavy reliance on\nstring data manipulation. While individual requirements can usually be analyzed\nmanually, verifying properties (e.g., satisfiability) over sets of NL\nrequirements is particularly challenging. Formal approaches (e.g., SMT solvers)\nmay efficiently verify such properties, but are known to have theoretical\nlimitations. Additionally, the translation of NL requirements into formal\nconstraints typically requires significant manual effort. Recently, large\nlanguage models (LLMs) have emerged as an alternative approach for formal\nreasoning tasks, but their effectiveness in verifying requirements over strings\nis less studied. In this paper, we introduce a hybrid approach that verifies\nthe satisfiability of NL requirements over strings by using LLMs (1) to derive\na satisfiability outcome (and a consistent string, if possible), and (2) to\ngenerate declarative (i.e., SMT) and imperative (i.e., Python) checkers, used\nto validate the correctness of (1). In our experiments, we assess the\nperformance of four LLMs. Results show that LLMs effectively translate natural\nlanguage into checkers, even achieving perfect testing accuracy for\nPython-based checkers. These checkers substantially help LLMs in generating a\nconsistent string and accurately identifying unsatisfiable requirements,\nleading to more than doubled generation success rate and F1-score in certain\ncases compared to baselines without generated checkers.", "AI": {"tldr": "The paper shows that combining LLMs with automatically generated checkers significantly improves the automated verification of natural language string requirements, more than doubling generation success rates and F1-scores.", "motivation": "Verifying sets of requirements written in natural language (NL), particularly those involving strings, is difficult due to manual analysis limitations, the theoretical limitations of formal approaches (e.g., SMT solvers), and the high manual effort required for translating NL into formal constraints. With LLMs showing promise for formal reasoning tasks, their effectiveness in this domain has not been thoroughly explored.", "method": "The authors propose a hybrid approach that: (1) uses Large Language Models (LLMs) to derive the satisfiability result (and a consistent string if possible) from NL requirements over strings, and (2) employs LLMs to generate declarative (SMT) and imperative (Python) checkers, which then validate the correctness of the outcome from step (1). The approach is experimentally evaluated with four different LLMs.", "result": "LLMs demonstrated effectiveness in translating NL requirements into functional checkers, achieving perfect test accuracy for Python-based checkers. The use of these checkers substantially improved LLMs\u2019 ability to generate consistent strings and to identify unsatisfiable requirements, resulting in more than double the generation success rate and F1-score compared to baselines that did not use checkers.", "conclusion": "Combining LLM-based reasoning with automatically generated checkers provides a practical and effective way to verify the satisfiability of NL requirements over strings, improving outcome reliability compared to both manual methods and purely LLM-based or formal methods without such hybrid validation."}}
{"id": "2506.16650", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.16650", "abs": "https://arxiv.org/abs/2506.16650", "authors": ["Anvith Pabba", "Alex Mathai", "Anindya Chakraborty", "Baishakhi Ray"], "title": "SemAgent: A Semantics Aware Program Repair Agent", "comment": null, "summary": "Large Language Models (LLMs) have shown impressive capabilities in downstream\nsoftware engineering tasks such as Automated Program Repair (APR). In\nparticular, there has been a lot of research on repository-level\nissue-resolution benchmarks such as SWE-Bench. Although there has been\nsignificant progress on this topic, we notice that in the process of solving\nsuch issues, existing agentic systems tend to hyper-localize on immediately\nsuspicious lines of code and fix them in isolation, without a deeper\nunderstanding of the issue semantics, code semantics, or execution semantics.\nConsequently, many existing systems generate patches that overfit to the user\nissue, even when a more general fix is preferable. To address this limitation,\nwe introduce SemAgent, a novel workflow-based procedure that leverages issue,\ncode, and execution semantics to generate patches that are complete -\nidentifying and fixing all lines relevant to the issue. We achieve this through\na novel pipeline that (a) leverages execution semantics to retrieve relevant\ncontext, (b) comprehends issue-semantics via generalized abstraction, (c)\nisolates code-semantics within the context of this abstraction, and (d)\nleverages this understanding in a two-stage architecture: a repair stage that\nproposes fine-grained fixes, followed by a reviewer stage that filters relevant\nfixes based on the inferred issue-semantics. Our evaluations show that our\nmethodology achieves a solve rate of 44.66% on the SWEBench-Lite benchmark\nbeating all other workflow-based approaches, and an absolute improvement of\n7.66% compared to our baseline, which lacks such deep semantic understanding.\nWe note that our approach performs particularly well on issues requiring\nmulti-line reasoning (and editing) and edge-case handling, suggesting that\nincorporating issue and code semantics into APR pipelines can lead to robust\nand semantically consistent repairs.", "AI": {"tldr": "SemAgent is a new workflow for automated program repair that uses deeper understanding of code, issues, and execution context to generate better patches. It outperforms previous methods, particularly on complex, multi-line bug fixes.", "motivation": "Current automated program repair (APR) systems using large language models tend to focus only on the most suspicious lines of code and apply localized fixes, often missing the broader issue semantics, code semantics, or execution semantics. This can lead to overfitting, producing patches that only address a particular user issue without creating a more general or robust solution.", "method": "The authors introduce SemAgent, which uses a novel workflow-based pipeline. This involves (a) using execution semantics to retrieve relevant code context, (b) understanding issue-semantics through abstraction, (c) isolating code-semantics within the defined abstraction, and (d) applying a two-stage architecture: first, a repair stage makes detailed fixes, then a reviewer stage filters fixes according to the inferred semantics of the issue.", "result": "Their approach achieves a 44.66% solve rate on the SWEBench-Lite benchmark, outperforming all other workflow-based methods. It demonstrates an absolute improvement of 7.66% over a baseline system that doesn't use deep semantic understanding. SemAgent works especially well for issues that require reasoning across multiple code lines and for edge-case scenarios.", "conclusion": "By integrating issue, code, and execution semantics in APR systems, more complete and semantically consistent code repairs can be made, resulting in more generalizable and robust fixes than previous approaches."}}
{"id": "2506.16653", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.16653", "abs": "https://arxiv.org/abs/2506.16653", "authors": ["Vladislav Belozerov", "Peter J Barclay", "Askhan Sami"], "title": "LLMs in Coding and their Impact on the Commercial Software Engineering Landscape", "comment": null, "summary": "Large-language-model coding tools are now mainstream in software engineering.\nBut as these same tools move human effort up the development stack, they\npresent fresh dangers: 10% of real prompts leak private data, 42% of generated\nsnippets hide security flaws, and the models can even ``agree'' with wrong\nideas, a trait called sycophancy. We argue that firms must tag and review every\nAI-generated line of code, keep prompts and outputs inside private or\non-premises deployments, obey emerging safety regulations, and add tests that\ncatch sycophantic answers -- so they can gain speed without losing security and\naccuracy.", "AI": {"tldr": "While LLM coding tools are valuable for developer productivity, they also introduce serious risks (data leaks, security flaws, sycophancy). The paper urges careful governance and technical safeguards to use these tools safely and responsibly.", "motivation": "Large language model (LLM) coding tools are widely used, shifting human work higher in software development. However, their use introduces new risks: data leakage, security vulnerabilities, and sycophancy (uncritical agreement with incorrect ideas).", "method": "The authors analyze risks associated with LLM coding tools by examining the rate of data leaks, presence of security flaws in generated code, and the occurrence of sycophantic responses. They propose organizational policies and safety practices for mitigating these risks.", "result": "Their findings reveal that 10% of prompts leak private data, 42% of generated code snippets contain security vulnerabilities, and LLMs can exhibit sycophancy. Specific recommendations are provided for safer integration of such tools into development workflows.", "conclusion": "Firms need to implement robust review and tagging of AI-generated code, ensure private/on-premises deployment, follow safety regulations, and introduce sycophancy-detecting tests to balance efficiency with security and correctness."}}
{"id": "2506.16831", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.16831", "abs": "https://arxiv.org/abs/2506.16831", "authors": ["Filippo Scaramuzza", "Damian A. Tamburri", "Willem-Jan van den Heuvel"], "title": "Accountability of Robust and Reliable AI-Enabled Systems: A Preliminary Study and Roadmap", "comment": "To be published in https://link.springer.com/book/9789819672370", "summary": "This vision paper presents initial research on assessing the robustness and\nreliability of AI-enabled systems, and key factors in ensuring their safety and\neffectiveness in practical applications, including a focus on accountability.\nBy exploring evolving definitions of these concepts and reviewing current\nliterature, the study highlights major challenges and approaches in the field.\nA case study is used to illustrate real-world applications, emphasizing the\nneed for innovative testing solutions. The incorporation of accountability is\ncrucial for building trust and ensuring responsible AI development. The paper\noutlines potential future research directions and identifies existing gaps,\npositioning robustness, reliability, and accountability as vital areas for the\ndevelopment of trustworthy AI systems of the future.", "AI": {"tldr": "This vision paper reviews literature and presents a case study to underline the importance of robustness, reliability, and accountability in AI systems. It identifies challenges, research gaps, and future research needs for building trustworthy, responsible AI.", "motivation": "As AI systems are increasingly deployed in practical and high-stakes applications, ensuring their safety, accountability, and trustworthiness is critical.", "method": "The paper conducts a literature review, analyzes evolving definitions, and presents a real-world case study to identify challenges and illustrate practical applications.", "result": "The study highlights major challenges in current AI system robustness and reliability, demonstrates the practical importance of accountability, and proposes future research directions.", "conclusion": "Robustness, reliability, and accountability are essential for trustworthy AI systems. Addressing challenges and research gaps in these areas is key to responsible AI development."}}
{"id": "2506.16876", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.16876", "abs": "https://arxiv.org/abs/2506.16876", "authors": ["Halit Eris", "Stefan Wagner"], "title": "Revolutionizing Validation and Verification: Explainable Testing Methodologies for Intelligent Automotive Decision-Making Systems", "comment": "Preprint to be published at SE4ADS", "summary": "Autonomous Driving Systems (ADS) use complex decision-making (DM) models with\nmultimodal sensory inputs, making rigorous validation and verification (V&V)\nessential for safety and reliability. These models pose challenges in\ndiagnosing failures, tracing anomalies, and maintaining transparency, with\ncurrent manual testing methods being inefficient and labor-intensive. This\nvision paper presents a methodology that integrates explainability,\ntransparency, and interpretability into V&V processes. We propose refining V&V\nrequirements through literature reviews and stakeholder input, generating\nexplainable test scenarios via large language models (LLMs), and enabling\nreal-time validation in simulation environments. Our framework includes test\noracle, explanation generation, and a test chatbot, with empirical studies\nplanned to evaluate improvements in diagnostic efficiency and transparency. Our\ngoal is to streamline V&V, reduce resources, and build user trust in autonomous\ntechnologies.", "AI": {"tldr": "This vision paper proposes an explainable and transparent framework for validating autonomous driving systems, leveraging large language models and real-time simulations to improve diagnostic efficiency and user trust.", "motivation": "Autonomous Driving Systems rely on complex decision-making with various sensory inputs, making validation and verification crucial. However, diagnosing failures and ensuring transparency is challenging, and manual testing is inefficient.", "method": "The paper proposes a methodology that embeds explainability and interpretability into the V&V process. This involves refining V&V requirements through reviews and stakeholder feedback, utilizing large language models to generate explainable test scenarios, and implementing real-time validation in simulation. The framework includes a test oracle, explanation generation, and a chatbot to aid testing.", "result": "Empirical studies are planned to evaluate if the framework improves diagnostic efficiency and transparency in V&V processes for ADS.", "conclusion": "The methodology aims to streamline validation and verification, reduce resource requirements, and increase user trust in autonomous driving systems by incorporating explainability and transparency."}}
{"id": "2506.16878", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.16878", "abs": "https://arxiv.org/abs/2506.16878", "authors": ["Man Zhang", "Yuechen Li", "Tao Yue", "Kai-Yuan Cai"], "title": "Quantum Optimization for Software Engineering: A Survey", "comment": null, "summary": "Quantum computing, particularly in the area of quantum optimization, is\nsteadily progressing toward practical applications, supported by an expanding\nrange of hardware platforms and simulators. While Software Engineering (SE)\noptimization has a strong foundation, which is exemplified by the active\nSearch-Based Software Engineering (SBSE) community and numerous classical\noptimization methods, the growing complexity of modern software systems and\ntheir engineering processes demands innovative solutions. This Systematic\nLiterature Review (SLR) focuses specifically on studying the literature that\napplies quantum or quantum-inspired algorithms to solve classical SE\noptimization problems. We examine 77 primary studies selected from an initial\npool of 2083 publications obtained through systematic searches of six digital\ndatabases using carefully crafted search strings. Our findings reveal\nconcentrated research efforts in areas such as SE operations and software\ntesting, while exposing significant gaps across other SE activities.\nAdditionally, the SLR uncovers relevant works published outside traditional SE\nvenues, underscoring the necessity of this comprehensive review. Overall, our\nstudy provides a broad overview of the research landscape, empowering the SBSE\ncommunity to leverage quantum advancements in addressing next-generation SE\nchallenges.", "AI": {"tldr": "The paper reviews how quantum and quantum-inspired algorithms are being applied to classic Software Engineering optimization problems, identifying research trends, gaps, and opportunities to adopt quantum approaches in solving next-generation SE challenges.", "motivation": "While classical optimization methods are well-established in Software Engineering (SE), the increasing complexity of modern software systems calls for novel optimization approaches. Quantum computing offers fresh possibilities, but there is little synthesized knowledge about its impact on classical SE optimization problems.", "method": "The study conducts a Systematic Literature Review (SLR), selecting 77 primary studies from an initial pool of 2083 papers, using systematic searches across six digital databases with carefully constructed search strings. The focus is on applications of quantum or quantum-inspired algorithms for SE optimization tasks.", "result": "The review identifies concentrated research in areas like SE operations and software testing, while finding significant gaps in the application of quantum optimization to other SE activities. It also finds pertinent research beyond traditional SE publication venues.", "conclusion": "This SLR offers an extensive overview of the emerging intersection between quantum computing and SE optimization, highlighting existing efforts, knowledge gaps, and opportunities for integrating quantum advancements into SBSE to tackle future software challenges."}}
{"id": "2506.16997", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.16997", "abs": "https://arxiv.org/abs/2506.16997", "authors": ["Hannah Deters", "Laura Reinhardt", "Jakob Droste", "Martin Obaidi", "Kurt Schneider"], "title": "Identifying Explanation Needs: Towards a Catalog of User-based Indicators", "comment": "This paper has been accepted at the research track of the 33rd IEEE\n  International Requirements Engineering Conference (RE 2025)", "summary": "In today's digitalized world, where software systems are becoming\nincreasingly ubiquitous and complex, the quality aspect of explainability is\ngaining relevance. A major challenge in achieving adequate explanations is the\nelicitation of individual explanation needs, as it may be subject to severe\nhypothetical or confirmation biases. To address these challenges, we aim to\nestablish user-based indicators concerning user behavior or system events that\ncan be captured at runtime to determine when a need for explanations arises. In\nthis work, we conducted explorative research in form of an online study to\ncollect self-reported indicators that could indicate a need for explanation. We\ncompiled a catalog containing 17 relevant indicators concerning user behavior,\n8 indicators concerning system events and 14 indicators concerning emotional\nstates or physical reactions. We also analyze the relationships between these\nindicators and different types of need for explanation. The established\nindicators can be used in the elicitation process through prototypes, as well\nas after publication to gather requirements from already deployed applications\nusing telemetry and usage data. Moreover, these indicators can be used to\ntrigger explanations at appropriate moments during the runtime.", "AI": {"tldr": "This paper identifies and categorizes a set of runtime indicators (behavioral, system, emotional) that signal when users may need explanations from complex software. These indicators help improve real-time and post-deployment responsiveness to user explanation needs, supporting more explainable systems.", "motivation": "The paper is motivated by the increasing complexity of software systems and the growing importance of explainability. A central challenge is identifying when users need explanations, since self-reporting can be biased and unreliable.", "method": "The study conducts explorative, online research, gathering self-reported indicators from users that signal a need for explanation. The researchers compile and categorize these indicators into behavioral, system, and emotional/physical types, and analyze their relationships to different explanation needs.", "result": "A catalog of 17 user behavior indicators, 8 system event indicators, and 14 emotional/physical reaction indicators is created. The relationships between these indicators and types of explanation needs are mapped.", "conclusion": "The identified indicators can be used both during development and after deployment of software systems to better elicit user explanation needs. They can improve requirement gathering and enable runtime triggering of explanations for users, enhancing explainability in practice."}}
