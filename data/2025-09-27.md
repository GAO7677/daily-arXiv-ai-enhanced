<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 18]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [ACCeLLiuM: Supervised Fine-Tuning for Automated OpenACC Pragma Generation](https://arxiv.org/abs/2509.20380)
*Samyak Jhaveri,Vanessa Klotzmann,Crista Lopes*

Main category: cs.SE

TL;DR: ACCeLLiuM, two open-weight LLMs fine-tuned on a curated dataset of OpenACC pragmas, substantially improve the automated generation of correct OpenACC directives for GPU programming, outperforming base LLMs and aiding code parallelization.


<details>
  <summary>Details</summary>
Motivation: GPU programming is complex and even with directive-based approaches like OpenACC, significant expertise is needed. The research aims to ease this process by enabling LLMs to automatically generate expert-level OpenACC directives, making GPU offloading more accessible.

Method: The authors introduce ACCeLLiuM, two Large Language Models (LLMs) fine-tuned for generating OpenACC directives for data-parallel loops. They trained these models on a supervised dataset containing 4,033 OpenACC pragma-loop pairs sourced from public C/C++ repositories.

Result: Fine-tuned ACCeLLiuM LLMs significantly outperform base LLMs, generating valid OpenACC pragmas with the correct type in 87% of cases and exact matches in 50%. Even non-exact outputs usually add valuable clauses or rearrange them, enhancing practical utility.

Conclusion: The paper demonstrates that domain-specific fine-tuning enables LLMs to effectively generate correct and useful OpenACC directives, lowering barriers to automated GPU programming. The public release of code, models, and data sets a reproducible standard for future research in this area.

Abstract: The increasing ubiquity of GPUs is accompanied by the increasing complexity
of their hardware and parallel programming frameworks. Directive-based parallel
programming standards like OpenACC simplify GPU programming to some extent by
abstracting away low-level complexities, but a fair amount of expertise is
still required in order to use those directives effectively.
  We introduce ACCeLLiuM, two open weights Large Language Models specifically
fine-tuned for generating expert OpenACC directives for data-parallel loops,
along with the supervised fine-tuning dataset that was used to train them. The
ACCeLLiuM SFT dataset contains 4,033 OpenACC pragma-loop pairs mined from
public GitHub C/C++ repositories, with 3,223 pairs for training and 810 for
testing. Experimental evaluations show a pronounced performance gap in
generating correct OpenACC pragmas between base LLMs and our fine-tuned
versions. On the held-out test set, base LLMs fail to consistently generate
valid pragmas, whereas LLMs fine-tuned on the ACCeLLiuM dataset generate valid
pragmas with the correct directive type for $87\%$ of the data-parallel loops,
and exact pragmas--including directives, clauses, clause order, and clause
variables--for $50\%$ of the cases. Even when not exact, generated pragmas
frequently incorporate the correct clauses in a different order than the
ground-truth label, or include additional clauses that enable finer control
over parallel execution, data movement, and concurrency, offering practical
value beyond strict string-matching. By publicly releasing the code, models,
and dataset as ACCeLLiuM we hope to establish a reproducible benchmark for
LLM-powered OpenACC pragma generation, and lower the barrier to automated GPU
offloading of serially written programs.

</details>


### [2] [State-of-the-Art in Software Security Visualization: A Systematic Review](https://arxiv.org/abs/2509.20385)
*Ishara Devendra,Chaman Wijesiriwardana,Prasad Wimalaratne*

Main category: cs.SE

TL;DR: This paper systematically reviews and categorizes software security visualization techniques, revealing current advancements and gaps. It underscores the need for adaptive and innovative visual tools that better support threat detection and cybersecurity operations as software systems and threats become more complex.


<details>
  <summary>Details</summary>
Motivation: Traditional text and numeric analysis methods are increasingly unable to handle the complexity of modern software security issues. There is a need for better ways to visualize and interpret security data as threat landscapes and software systems evolve.

Method: Systematic review of over 60 recent research papers, followed by classification of security visualization techniques into four categories: graph-based, notation-based, matrix-based, and metaphor-based.

Result: The two main areas of focus in software security visualization were identified: extensive software development visualization and operational/cybersecurity visualization. The paper shows recent advancements and highlights ongoing key issues, emphasizing the need for adaptive, innovative visualization techniques to keep up with evolving threats.

Conclusion: Innovative visualization techniques are required to effectively support threat detection, security strategy, and future research due to the growing complexity of software and security threats.

Abstract: Software security visualization is an interdisciplinary field that combines
the technical complexity of cybersecurity, including threat intelligence and
compliance monitoring, with visual analytics, transforming complex security
data into easily digestible visual formats. As software systems get more
complex and the threat landscape evolves, traditional text-based and numerical
methods for analyzing and interpreting security concerns become increasingly
ineffective. The purpose of this paper is to systematically review existing
research and create a comprehensive taxonomy of software security visualization
techniques through literature, categorizing these techniques into four types:
graph-based, notation-based, matrix-based, and metaphor-based visualization.
This systematic review explores over 60 recent key research papers in software
security visualization, highlighting its key issues, recent advancements, and
prospective future research directions. From the comprehensive analysis, the
two main areas were distinctly highlighted as extensive software development
visualization, focusing on advanced methods for depicting software
architecture: operational security visualization and cybersecurity
visualization. The findings highlight the necessity for innovative
visualization techniques that adapt to the evolving security landscape, with
practical implications for enhancing threat detection, improving security
response strategies, and guiding future research.

</details>


### [3] [Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments](https://arxiv.org/abs/2509.20386)
*Nishant Gaurav,Adit Akarsh,Ankit Ranjan,Manoj Bajaj*

Main category: cs.SE

TL;DR: Dynamic ReAct allows ReAct agents to efficiently pick the right tools from massive tool sets—without overloading memory—by using refined selection strategies, which halves tool loading needs while keeping performance high.


<details>
  <summary>Details</summary>
Motivation: ReAct agents struggle with using a large number of tools due to memory and computational constraints when all tools cannot be loaded at once. There is a key challenge in efficiently selecting relevant tools from a vast pool.

Method: The authors introduce Dynamic ReAct, which includes five architectures designed to refine the tool selection process for ReAct agents. The culminating architecture employs a 'search-and-load' strategy to select tools intelligently while minimizing computation.

Result: Experimental results show up to 50% reduction in tool loading, with task completion accuracy maintained despite not loading all tools at once.

Conclusion: Dynamic ReAct enables ReAct agents to effectively and efficiently use large tool sets beyond the memory limits of LLMs by improving the tool selection process, thus supporting more general-purpose and adaptable AI agents.

Abstract: We present Dynamic ReAct, a novel approach for enabling ReAct agents to ef-
ficiently operate with extensive Model Control Protocol (MCP) tool sets that
exceed the contextual memory limitations of large language models. Our approach
addresses the fundamental challenge of tool selection in environments
containing hundreds or thousands of available tools, where loading all tools
simultaneously is computationally infeasible. We propose and evaluate five
distinct architectures that progressively refine the tool selection process,
culminating in a search-and-load mechanism that achieves intelligent tool
selection with minimal computational overhead. Our experimental results
demonstrate that the proposed approach reduces tool loading by up to 50% while
maintaining task completion accuracy, advancing the path towards truly
general-purpose AI agents capable of dynamically adapting to diverse task
environments.

</details>


### [4] [Towards Systematic Specification and Verification of Fairness Requirements: A Position Paper](https://arxiv.org/abs/2509.20387)
*Qusai Ramadan,Jukka Ruohonen,Abhishek Tiwari,Adam Alami,Zeyd Boukhers*

Main category: cs.SE

TL;DR: Discrimination in software systems is often caused not just by bad algorithms or biased data, but by weakly defined fairness requirements. This paper suggests that knowledge graphs, used successfully in fields like security, could be used to better specify and verify fairness in software, outlining key challenges and next steps.


<details>
  <summary>Details</summary>
Motivation: Improperly designed software systems may discriminate based on protected characteristics due to insufficient specification and verification of fairness requirements, with experts' knowledge often being implicit and hard to formalize.

Method: The paper proposes using knowledge graphs, drawing parallels from their successful application in security engineering, to structure and verify fairness requirements. It provides a roadmap and discusses related challenges and research questions.

Result: The paper lays out a conceptual framework and research approach, but does not report empirical results. It suggests that knowledge graphs offer a promising path for formalizing fairness requirements.

Conclusion: The paper concludes that a knowledge graph-based framework can help formalize and verify fairness requirements in software systems, addressing current gaps in requirement specification.

Abstract: Decisions suggested by improperly designed software systems might be prone to
discriminate against people based on protected characteristics, such as gender
and ethnicity. Previous studies attribute such undesired behavior to flaws in
algorithmic design or biased data. However, these studies ignore that
discrimination is often the result of a lack of well-specified fairness
requirements and their verification. The fact that experts' knowledge about
fairness is often implicit makes the task of specifying precise and verifiable
fairness requirements difficult. In related domains, such as security
engineering, knowledge graphs have been proven to be effective in formalizing
knowledge to assist requirements specification and verification. To address the
lack of formal mechanisms for specifying and verifying fairness requirements,
we propose the development of a knowledge graph-based framework for fairness.
In this paper, we discuss the challenges, research questions, and a road map
towards addressing the research questions.

</details>


### [5] [Online-Optimized RAG for Tool Use and Function Calling](https://arxiv.org/abs/2509.20415)
*Yu Pan,Xiaocheng Li,Hanzhao Wang*

Main category: cs.SE

TL;DR: Online-Optimized RAG is a practical framework for improving tool selection in retrieval-augmented generation systems, adapting embeddings in real-time based on minimal feedback, and strengthening system reliability and success without modifying the underlying LLM.


<details>
  <summary>Details</summary>
Motivation: Current retrieval-augmented generation (RAG) systems often rely on embedding-based query matching to select tools or functions, but suffer from embedding misalignment due to imperfect models or noisy descriptions. This misalignment can cause retrieval errors and task failure in practical applications.

Method: The proposed method, Online-Optimized RAG, uses online gradient-based updates during deployment to continually adapt retrieval embeddings based on minimal user feedback (like task success). It works without modifying the LLM, supports single-/multi-hop tool use, dynamic tool inventories, and improves ranking in $K$-retrieval scenarios, with negligible added latency.

Result: Online-Optimized RAG consistently improves the accuracy of tool selection and the overall success rate across varied tool-use and document-retrieval tasks. Theoretical analysis connects performance to embedding initialization and relevant parameters.

Conclusion: Online-Optimized RAG provides a simple, robust, and self-improving solution for RAG systems, making them more resilient to embedding misalignment in real-world deployments.

Abstract: In many applications, retrieval-augmented generation (RAG) drives tool use
and function calling by embedding the (user) queries and matching them to
pre-specified tool/function descriptions. In this paper, we address an
embedding misalignment issue that often arises in practical applications due to
imperfect embedding models or noisy descriptions; such misalignment may lead to
incorrect retrieval and task failure. We introduce Online-Optimized RAG, a
deployment-time framework that continually adapts retrieval embeddings from
live interactions using minimal feedback (e.g., task success). Online-Optimized
RAG applies lightweight online gradient updates with negligible per-query
latency and requires no changes to the underlying LLM. The method is
plug-and-play: it supports both single- and multi-hop tool use, dynamic tool
inventories, and $K$-retrieval with re-ranking. We provide a problem-dependent
theoretical analysis that quantifies how the method's performance depends on
the initialization quality of the embeddings and other related quantities.
Across diverse tool-use and document-retrieval scenarios, our Online-Optimized
RAG consistently improves tool selection accuracy and end-task success, thus
providing a simple, practical path to robust, self-improving RAG systems.

</details>


### [6] [Formal Verification of Legal Contracts: A Translation-based Approach](https://arxiv.org/abs/2509.20421)
*Reiner Hähnle,Cosimo Laneve,Adele Veschetti*

Main category: cs.SE

TL;DR: Stipula contracts can be automatically and formally verified for correctness by translating them into annotated Java code and using general-purpose verification tools.


<details>
  <summary>Details</summary>
Motivation: The motivation is to ensure the correctness and enforceability of legal contracts (especially asset transfers and obligations) implemented in the Stipula programming language.

Method: The method involves translating Stipula contracts into Java code annotated with Java Modeling Language (JML) specifications, then using the deductive verification tool KeY to formally verify contract correctness.

Result: The approach is able to automatically verify both partial and total correctness for a large subset of Stipula contracts, specifically those with disjoint cycles.

Conclusion: General-purpose deductive verification tools, like KeY, can be successfully applied to formally verify the correctness of domain-specific contract languages via an automatic translation approach.

Abstract: Stipula is a domain-specific programming language designed to model legal
contracts with enforceable properties, especially those involving asset
transfers and obligations. This paper presents a methodology to formally verify
the correctness of Stipula contracts through translation into Java code
annotated with Java Modeling Language specifications. As a verification
backend, the deductive verification tool KeY is used. Both, the translation and
the verification of partial and total correctness for a large subset of Stipula
contracts, those with disjoint cycles, is fully automatic. Our work
demonstrates that a general-purpose deductive verification tool can be used
successfully in a translation approach.

</details>


### [7] [AI-Specific Code Smells: From Specification to Detection](https://arxiv.org/abs/2509.20491)
*Brahim Mahmoudi,Naouel Moha,Quentin Stievenert,Florent Avellaneda*

Main category: cs.SE

TL;DR: AI systems have unique code smells that aren't detected by current tools. SpecDetect4AI combines DSL-based rule specification and extensible static analysis to detect these issues with high accuracy at scale, boosting maintainability and reliability.


<details>
  <summary>Details</summary>
Motivation: AI-based systems introduce unique software issues that current detection tools often overlook, like reproducibility problems and silent failures. Addressing these new code smells is necessary to improve AI software reliability.

Method: SpecDetect4AI is a novel tool combining a declarative DSL for easy specification of code smell rules and a static analysis engine capable of large-scale detection in AI-based codebases. The tool supports extensibility and high-level rule creation.

Result: SpecDetect4AI specified 22 AI-specific code smells and was tested on 826 AI systems (~20 million lines of code), achieving a precision of 88.66% and recall of 88.89%. The system improved performance compared to existing tools and received a usability score of 81.7/100.

Conclusion: SpecDetect4AI efficiently enables the specification and scalable detection of AI-specific code smells. It outperforms prior tools in accuracy and usability and can analyze large AI systems, improving maintainability and reliability.

Abstract: The rise of Artificial Intelligence (AI) is reshaping how software systems
are developed and maintained. However, AI-based systems give rise to new
software issues that existing detection tools often miss. Among these, we focus
on AI-specific code smells, recurring patterns in the code that may indicate
deeper problems such as unreproducibility, silent failures, or poor model
generalization. We introduce SpecDetect4AI, a tool-based approach for the
specification and detection of these code smells at scale. This approach
combines a high-level declarative Domain-Specific Language (DSL) for rule
specification with an extensible static analysis tool that interprets and
detects these rules for AI-based systems. We specified 22 AI-specific code
smells and evaluated SpecDetect4AI on 826 AI-based systems (20M lines of code),
achieving a precision of 88.66% and a recall of 88.89%, outperforming other
existing detection tools. Our results show that SpecDetect4AI supports the
specification and detection of AI-specific code smells through dedicated rules
and can effectively analyze large AI-based systems, demonstrating both
efficiency and extensibility (SUS 81.7/100).

</details>


### [8] [Enhancing Python Programming Education with an AI-Powered Code Helper: Design, Implementation, and Impact](https://arxiv.org/abs/2509.20518)
*Sayed Mahbub Hasan Amiri,Md Mainul Islam*

Main category: cs.SE

TL;DR: This paper presents an AI-driven programming chatbot for students, which outperforms traditional tools in resolving code errors, improving learning efficiency, and boosting student confidence. Its hybrid architecture leverages advanced language models and secure execution, providing a practical blueprint for educational AI systems.


<details>
  <summary>Details</summary>
Motivation: Traditional programming tools and code assistants either lack interactive, robotic, and educational support for students or focus mainly on task completion instead of learning enhancement. This study addresses the need for an AI-based system that emphasizes student learning and understanding.

Method: The proposed solution is an AI-Python-based chatbot that integrates static code analysis, dynamic execution tracing, and large language models (LLMs) for personalized feedback. It uses CodeLlama for code embedding, GPT-4 for natural language exchanges, and Docker-based sandboxing for security. The system was evaluated with a mixed-methods approach over 1,500 student code submissions, complemented by pre- and post-tests and qualitative surveys from 120 students.

Result: The chatbot achieved an 85% error resolution rate, surpassing traditional tools like pylint (62%) and standalone GPT-4 (73%). Students using the chatbot showed a 59.3% reduction in debugging time and a 34% improvement in coding proficiency, especially in complex areas like recursion and exception handling. Student feedback noted strengths in clarity, usability, and increased learning confidence, with minor concerns about latency and security constraints.

Conclusion: The study demonstrates that combining technical innovation with educational goals can lead to highly effective AI-powered learning tools. The chatbot serves as a model for future AI systems that prioritize educational impact, equity, and student skill development rather than only code completion.

Abstract: This is the study that presents an AI-Python-based chatbot that helps
students to learn programming by demonstrating solutions to such problems as
debugging errors, solving syntax problems or converting abstract theoretical
concepts to practical implementations. Traditional coding tools like Integrated
Development Environments (IDEs) and static analyzers do not give robotic help
while AI-driven code assistants such as GitHub Copilot focus on getting things
done. To close this gap, our chatbot combines static code analysis, dynamic
execution tracing, and large language models (LLMs) to provide the students
with relevant and practical advice, hence promoting the learning process. The
chatbots hybrid architecture employs CodeLlama for code embedding, GPT-4 for
natural language interactions, and Docker-based sandboxing for secure
execution. Evaluated through a mixed-methods approach involving 1,500 student
submissions, the system demonstrated an 85% error resolution success rate,
outperforming standalone tools like pylint (62%) and GPT-4 (73%). Quantitative
results revealed a 59.3% reduction in debugging time among users, with pre- and
post-test assessments showing a 34% improvement in coding proficiency,
particularly in recursion and exception handling. Qualitative feedback from 120
students highlighted the chatbots clarity, accessibility, and
confidence-building impact, though critiques included occasional latency and
restrictive code sanitization. By balancing technical innovation with
pedagogical empathy, this research provides a blueprint for AI tools that
prioritize educational equity and long-term skill retention over mere code
completion. The chatbot exemplifies how AI can augment human instruction,
fostering deeper conceptual understanding in programming education.

</details>


### [9] [PromptDebt: A Comprehensive Study of Technical Debt Across LLM Projects](https://arxiv.org/abs/2509.20497)
*Ahmed Aljohani,Hyunsook Do*

Main category: cs.SE

TL;DR: The paper presents a large-scale study of technical debt (SATD) tied to integrating Large Language Models (LLMs) in software. Prompt design is the main debt source, with instruction- and few-shot prompts being particularly problematic. Over half the issues stem from OpenAI API usage. The authors provide a dataset and practical guidance to help developers address technical debt in AI-driven systems.


<details>
  <summary>Details</summary>
Motivation: With the increasing adoption of Large Language Models (LLMs) via APIs like OpenAI, there's a growing concern about technical debt specific to LLM integrations, especially due to rapid development cycles, evolving prompt strategies, and complex configurations. Understanding and managing this technical debt is crucial for robust software engineering.

Method: The authors conducted a large-scale empirical analysis by examining 93,142 Python files that utilize major LLM APIs. They classified and quantified Self-Admitted Technical Debt (SATD) instances, analyzed the origins and types of LLM-specific SATD, and studied which prompt techniques are most susceptible to technical debt.

Result: 54.49% of LLM-specific SATD originated from OpenAI API use and 12.35% from LangChain. The main source of technical debt was prompt design, with 6.61% related to prompt configuration/optimization, and others stemming from hyperparameter tuning and framework integration. Instruction-based prompts (38.6%) and few-shot prompts (18.13%) were found to be most vulnerable to technical debt due to their reliance on clarity and quality. The authors also released a SATD dataset to enable further research.

Conclusion: LLM integrations introduce distinctive technical debt, primarily from prompt design and configuration. Certain prompt techniques, especially instruction-based and few-shot approaches, attract more debt. Awareness and strategic management of this unique technical debt are necessary for reliable LLM-powered systems.

Abstract: Large Language Models (LLMs) are increasingly embedded in software via APIs
like OpenAI, offering powerful AI features without heavy infrastructure. Yet
these integrations bring their own form of self-admitted technical debt (SATD).
In this paper, we present the first large-scale empirical study of LLM-specific
SATD: its origins, prevalence, and mitigation strategies. By analyzing 93,142
Python files across major LLM APIs, we found that 54.49% of SATD instances stem
from OpenAI integrations and 12.35% from LangChain use. Prompt design emerged
as the primary source of LLM-specific SATD, with 6.61% of debt related to
prompt configuration and optimization issues, followed by hyperparameter tuning
and LLM-framework integration. We further explored which prompt techniques
attract the most debt, revealing that instruction-based prompts (38.60%) and
few-shot prompts (18.13%) are particularly vulnerable due to their dependence
on instruction clarity and example quality. Finally, we release a comprehensive
SATD dataset to support reproducibility and offer practical guidance for
managing technical debt in LLM-powered systems.

</details>


### [10] [Enhancing LLM-based Fault Localization with a Functionality-Aware Retrieval-Augmented Generation Framework](https://arxiv.org/abs/2509.20552)
*Xinyu Shi,Zhenhao Li,An Ran Chen*

Main category: cs.SE

TL;DR: FaR-Loc, a framework combining LLMs and retrieval-augmented generation, markedly improves fault localization in software by efficiently leveraging semantic retrieval and code structure-aware embeddings, surpassing multiple baseline methods without retraining.


<details>
  <summary>Details</summary>
Motivation: Fault localization is crucial but slow and difficult in large, complex software systems. Recent LLMs help but lack project-specific knowledge and struggle with large codebases.

Method: FaR-Loc framework integrates LLMs with retrieval-augmented generation (RAG) for method-level fault localization. It includes three main components: LLM-based functionality extraction from failed tests, semantic dense retrieval using code-understanding encoders to embed both descriptions and methods in a shared semantic space, and LLM-based re-ranking of retrieved method candidates.

Result: FaR-Loc significantly outperforms state-of-the-art LLM-based (SoapFL and AutoFL), learning-based, and spectrum-based fault localization methods on the Defects4J benchmark, showing Top-1 and Top-5 accuracy improvements. Using code-structure-aware models (such as UniXcoder) boosts fault localization accuracy by up to 49%.

Conclusion: Integrating LLMs with retrieval-augmented generation and code structure-aware embeddings leads to substantial improvements in fault localization, making the process more efficient and effective in complex software projects.

Abstract: Fault localization (FL) is a critical but time-consuming task in software
debugging, aiming to identify faulty code elements. While recent advances in
large language models (LLMs) have shown promise for FL, they often struggle
with complex systems due to the lack of project-specific knowledge and the
difficulty of navigating large projects. To address these limitations, we
propose FaR-Loc, a novel framework that enhances method-level FL by integrating
LLMs with retrieval-augmented generation (RAG). FaR-Loc consists of three key
components: LLM Functionality Extraction, Semantic Dense Retrieval, and LLM
Re-ranking. First, given a failed test and its associated stack trace, the LLM
Functionality Extraction module generates a concise natural language
description that captures the failing behavior. Next, the Semantic Dense
Retrieval component leverages a pre-trained code-understanding encoder to embed
both the functionality description (natural language) and the covered methods
(code) into a shared semantic space, enabling the retrieval of methods with
similar functional behavior. Finally, the LLM Re-ranking module reorders the
retrieved methods based on their contextual relevance. Our experiments on the
widely used Defects4J benchmark show that FaR-Loc outperforms state-of-the-art
LLM-based baselines SoapFL and AutoFL, by 14.6% and 9.1% in Top-1 accuracy, by
19.2% and 22.1% in Top-5 accuracy, respectively. It also surpasses all
learning-based and spectrum-based baselines across all Top-N metrics without
requiring re-training. Furthermore, we find that pre-trained code embedding
models that incorporate code structure, such as UniXcoder, can significantly
improve fault localization performance by up to 49.0% in Top-1 accuracy.
Finally, we conduct a case study to illustrate the effectiveness of FaR-Loc and
to provide insights for its practical application.

</details>


### [11] [Design, Implementation and Evaluation of a Novel Programming Language Topic Classification Workflow](https://arxiv.org/abs/2509.20631)
*Michael Zhang,Yuan Tian,Mariam Guizani*

Main category: cs.SE

TL;DR: The paper introduces a new SVM-based workflow for detecting and localizing programming language concepts in code, achieving strong F1 scores, and presenting a practical tool for software engineering analysis.


<details>
  <summary>Details</summary>
Motivation: As software projects become larger and more complex, understanding how different programming language concepts are distributed in the code helps in making technical decisions, onboarding new developers, and improving tools and educational content.

Method: The paper develops a workflow that uses a multi-label Support Vector Machine (SVM) model in combination with a sliding window and voting strategy. This approach enables detailed identification and localization of programming language topics within source code. The model is trained using the IBM Project CodeNet dataset.

Result: The approach achieves an average F1 score of 0.90 for topic classification and 0.75 for detailed, code-topic highlighting, demonstrating good performance.

Conclusion: The workflow provides empirical insights and a reusable toolchain for code analysis. It is useful for both researchers and practitioners in software engineering who are focused on data-driven analysis.

Abstract: As software systems grow in scale and complexity, understanding the
distribution of programming language topics within source code becomes
increasingly important for guiding technical decisions, improving onboarding,
and informing tooling and education. This paper presents the design,
implementation, and evaluation of a novel programming language topic
classification workflow. Our approach combines a multi-label Support Vector
Machine (SVM) with a sliding window and voting strategy to enable fine-grained
localization of core language concepts such as operator overloading, virtual
functions, inheritance, and templates. Trained on the IBM Project CodeNet
dataset, our model achieves an average F1 score of 0.90 across topics and 0.75
in code-topic highlight. Our findings contribute empirical insights and a
reusable pipeline for researchers and practitioners interested in code analysis
and data-driven software engineering.

</details>


### [12] [Exploring Engagement in Hybrid Meetings](https://arxiv.org/abs/2509.20780)
*Daniela Grassi,Fabio Calefato,Darja Smite,Nicole Novielli,Filippo Lanubile*

Main category: cs.SE

TL;DR: This study found that remote and onsite workers have similar engagement in hybrid meetings, except remote workers disengage more in longer meetings. Active participation boosts engagement, while big and late-day meetings reduce it. These results inform better meeting practices for hybrid teams.


<details>
  <summary>Details</summary>
Motivation: The shift to hybrid work after COVID-19 has created new communication and collaboration challenges, especially in software development. There are concerns that remote participation in meetings leads to isolation and disengagement, so understanding engagement in hybrid meetings is crucial.

Method: A multimodal approach measuring engagement was used with professionals in three software companies. Data collection included self-reported questionnaires and physiological (biometric) data during hybrid meetings over several weeks.

Result: Regression analyses showed that onsite and remote participants generally had similar engagement levels, but remote participants were less engaged in long meetings. Having an active role improves engagement, while larger meetings and afternoon sessions lower it.

Conclusion: Insights into engagement and disengagement factors in hybrid meetings were provided, along with recommendations for improvements. These findings are relevant beyond software teams, applicable to other knowledge-intensive organizations dealing with hybrid collaboration.

Abstract: Background. The widespread adoption of hybrid work following the COVID-19
pandemic has fundamentally transformed software development practices,
introducing new challenges in communication and collaboration as organizations
transition from traditional office-based structures to flexible working
arrangements. This shift has established a new organizational norm where even
traditionally office-first companies now embrace hybrid team structures. While
remote participation in meetings has become commonplace in this new
environment, it may lead to isolation, alienation, and decreased engagement
among remote team members. Aims. This study aims to identify and characterize
engagement patterns in hybrid meetings through objective measurements, focusing
on the differences between co-located and remote participants. Method. We
studied professionals from three software companies over several weeks,
employing a multimodal approach to measure engagement. Data were collected
through self-reported questionnaires and physiological measurements using
biometric devices during hybrid meetings to understand engagement dynamics.
Results. The regression analyses revealed comparable engagement levels between
onsite and remote participants, though remote participants show lower
engagement in long meetings regardless of participation mode. Active roles
positively correlate with higher engagement, while larger meetings and
afternoon sessions are associated with lower engagement. Conclusions. Our
results offer insights into factors associated with engagement and
disengagement in hybrid meetings, as well as potential meeting improvement
recommendations. These insights are potentially relevant not only for software
teams but also for knowledge-intensive organizations across various sectors
facing similar hybrid collaboration challenges.

</details>


### [13] [Verification Limits Code LLM Training](https://arxiv.org/abs/2509.20837)
*Srishti Gureja,Elena Tommasone,Jingyi He,Sara Hooker,Matthias Gallé,Marzieh Fadaee*

Main category: cs.SE

TL;DR: Rigid automated verification restricts code generation models by filtering out valuable diverse solutions. Strategic recalibration—especially through more varied tests, relaxed criteria, and soft verification—unlocks greater model capability and data diversity, breaking the limits imposed by current practices.


<details>
  <summary>Details</summary>
Motivation: Current code generation models depend heavily on synthetic data, which are validated using automated verifiers. However, the quality and diversity of synthetic training data are limited by the capabilities of these verifiers, creating a 'verification ceiling.' This paper aims to systematically examine how verification strategies influence code generation model performance and to identify ways to improve this process.

Method: The study analyzes three aspects of verification: 1) the impact of test complexity and quantity on code generation; 2) the effect of relaxing pass thresholds and adopting LLM-based soft verification; and 3) controlled comparisons of correct versus incorrect solutions combined with human evaluation. Experiments are conducted to assess each strategy and its contribution to code model performance.

Result: Enriched and diverse test suites improve code generation performance (+3 pass@1), while just increasing quantity offers diminishing returns. Relaxed verification thresholds or soft LLM-based verification recover valuable training data and improve pass@1 scores by 2-4 points, but this depends on strong and diverse test cases. Keeping multiple diverse correct solutions leads to better generalization. Rigid verification practices filter out valuable diversity but removing verification entirely is not viable; recalibration is necessary.

Conclusion: Verification practices for synthetic code data are currently too restrictive, which limits the effectiveness of code generation models. By recalibrating verification strategies (using more diverse and challenging tests, relaxing criteria, and utilizing soft verifiers) and retaining solution diversity, it is possible to break the verification ceiling and significantly improve code model performance.

Abstract: Large language models for code generation increasingly rely on synthetic
data, where both problem solutions and verification tests are generated by
models. While this enables scalable data creation, it introduces a previously
unexplored bottleneck: the verification ceiling, in which the quality and
diversity of training data are fundamentally constrained by the capabilities of
synthetic verifiers. In this work, we systematically study how verification
design and strategies influence model performance. We investigate (i) what we
verify by analyzing the impact of test complexity and quantity: richer test
suites improve code generation capabilities (on average +3 pass@1), while
quantity alone yields diminishing returns, (ii) how we verify by exploring
relaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By
allowing for relaxed thresholds or incorporating LLM-based soft verification,
we can recover valuable training data, leading to a 2-4 point improvement in
pass@1 performance. However, this benefit is contingent upon the strength and
diversity of the test cases used, and (iii) why verification remains necessary
through controlled comparisons of formally correct versus incorrect solutions
and human evaluation: retaining diverse correct solutions per problem yields
consistent generalization gains. Our results show that Verification as
currently practiced is too rigid, filtering out valuable diversity. But it
cannot be discarded, only recalibrated. By combining calibrated verification
with diverse, challenging problem-solution pairs, we outline a path to break
the verification ceiling and unlock stronger code generation models.

</details>


### [14] [PseudoBridge: Pseudo Code as the Bridge for Better Semantic and Logic Alignment in Code Retrieval](https://arxiv.org/abs/2509.20881)
*Yixuan Li,Xinyi Liu,Weidong Yang,Ben Fei,Shuhao Li,Mingjie Zhou,Lipeng Ma*

Main category: cs.SE

TL;DR: PseudoBridge uses pseudo-code as an intermediate layer between natural language queries and code, boosting code search accuracy and robustness across different languages, styles, and domains.


<details>
  <summary>Details</summary>
Motivation: Code search is crucial for software development but still faces challenges bridging the semantic gap between natural language queries and programming languages, and is sensitive to diverse coding styles.

Method: PseudoBridge, the proposed framework, introduces pseudo-code as an intermediate step. It uses large language models (LLMs) to synthesize pseudo-code from natural language, then generates logically equivalent but stylistically diverse code to improve robustness. Code retrieval is then performed by aligning NL, pseudo-code, and code across styles.

Result: Experiments were conducted using 10 different pre-trained language models and tested on 6 programming languages. PseudoBridge outperformed baseline models, particularly in zero-shot transfer to new domains like Solidity and XLCoST.

Conclusion: PseudoBridge effectively bridges natural language and code using pseudo-code, achieving better retrieval accuracy and robustness to code style variations. It demonstrates strong generalization, suggesting its promise as a solution for robust code search.

Abstract: Code search aims to precisely find relevant code snippets that match natural
language queries within massive codebases, playing a vital role in software
development. Recent advances leverage pre-trained language models (PLMs) to
bridge the semantic gap between unstructured natural language (NL) and
structured programming languages (PL), yielding significant improvements over
traditional information retrieval and early deep learning approaches. However,
existing PLM-based methods still encounter key challenges, including a
fundamental semantic gap between human intent and machine execution logic, as
well as limited robustness to diverse code styles. To address these issues, we
propose PseudoBridge, a novel code retrieval framework that introduces
pseudo-code as an intermediate, semi-structured modality to better align NL
semantics with PL logic. Specifically, PseudoBridge consists of two stages.
First, we employ an advanced large language model (LLM) to synthesize
pseudo-code, enabling explicit alignment between NL queries and pseudo-code.
Second, we introduce a logic-invariant code style augmentation strategy and
employ the LLM to generate stylistically diverse yet logically equivalent code
implementations with pseudo-code, then align the code snippets of different
styles with pseudo-code, enhancing model robustness to code style variation. We
build PseudoBridge across 10 different PLMs and evaluate it on 6 mainstream
programming languages. Extensive experiments demonstrate that PseudoBridge
consistently outperforms baselines, achieving significant gains in retrieval
accuracy and generalization, particularly under zero-shot domain transfer
scenarios such as Solidity and XLCoST datasets. These results demonstrate the
effectiveness of explicit logical alignment via pseudo-code and highlight
PseudoBridge's potential as a robust, generalizable solution for code
retrieval.

</details>


### [15] [Designing for Novice Debuggers: A Pilot Study on an AI-Assisted Debugging Tool](https://arxiv.org/abs/2509.21067)
*Oka Kurniawan,Erick Chandra,Christopher M. Poskitt,Yannic Noller,Kenny Tsu Wei Choo,Cyrille Jegourel*

Main category: cs.SE

TL;DR: This paper introduces CodeHinter, a hybrid debugging assistant for novices that merges traditional methods with LLM-based AI support. Tested with undergraduates, it improved error fixing and usability, especially through its error localization feature. The study emphasizes the need for personalized AI tools to boost learning and engagement in debugging.


<details>
  <summary>Details</summary>
Motivation: Novice programmers struggle with debugging, and AI-based automated code repair tools can lead to passive learning and over-reliance on technology. There is a need for a tool that supports active engagement in debugging while leveraging the advantages of AI.

Method: The paper proposes an intuitive debugging assistant called CodeHinter, which combines traditional debugging tools with large language model (LLM)-based techniques. The tool is iteratively designed and tested with undergraduate students to evaluate its effectiveness and usability.

Result: Students found CodeHinter highly effective for fixing semantic errors and easier to use compared to the first version. Error localization remains the most valued feature. The study also highlights the importance of personalizing AI-assisted debugging tools to user profiles.

Conclusion: AI-assisted debugging tools should be personalized for different user profiles to maximize student engagement and effectiveness. CodeHinter demonstrates that combining traditional and AI techniques can facilitate active learning in debugging.

Abstract: Debugging is a fundamental skill that novice programmers must develop.
Numerous tools have been created to assist novice programmers in this process.
Recently, large language models (LLMs) have been integrated with automated
program repair techniques to generate fixes for students' buggy code. However,
many of these tools foster an over-reliance on AI and do not actively engage
students in the debugging process. In this work, we aim to design an intuitive
debugging assistant, CodeHinter, that combines traditional debugging tools with
LLM-based techniques to help novice debuggers fix semantic errors while
promoting active engagement in the debugging process. We present findings from
our second design iteration, which we tested with a group of undergraduate
students. Our results indicate that the students found the tool highly
effective in resolving semantic errors and significantly easier to use than the
first version. Consistent with our previous study, error localization was the
most valuable feature. Finally, we conclude that any AI-assisted debugging tool
should be personalized based on user profiles to optimize their interactions
with students.

</details>


### [16] [An Improved Quantum Software Challenges Classification Approach using Transfer Learning and Explainable AI](https://arxiv.org/abs/2509.21068)
*Nek Dil Khan,Javed Ali Khan,Mobashir Husain,Muhammad Sohail Khan,Arif Ali Khan,Muhammad Azeem Akbar,Shahid Hussain*

Main category: cs.SE

TL;DR: Transformer-based NLP models (BERT, DistilBERT) accurately classify QSE discussion challenges, outperforming traditional ML methods, and SHAP enhances interpretability. Results suggest improved organization of quantum forums, but further empirical studies are needed.


<details>
  <summary>Details</summary>
Motivation: Quantum developers face persistent challenges in optimizing quantum computing and engineering practices, and need better ways to identify and categorize frequent challenges discussed in technical forums for enhanced support and resource organization.

Method: The study extracted 2829 QSE-related questions from Q&A platforms using quantum tags, performed content and grounded theory analysis for annotation, used ChatGPT for validation, and applied fine-tuned transformer algorithms (BERT, DistilBERT, RoBERTa) for classification. Comparative experiments with deep/machine learning models and SHAP for interpretability were also conducted.

Result: Transformer-based models achieved a classification accuracy of 95%, outperforming deep/machine learning models (FNN: 89%, CNN: 86%, LSTM: 84%). SHAP analysis improved model interpretability by linking linguistic features to predictions.

Conclusion: Transformer-based algorithms, specifically BERT and DistilBERT, effectively classify quantum software engineering (QSE) challenges in developer discussions with higher accuracy (95%) compared to traditional deep/machine learning approaches. SHAP further improves model transparency.

Abstract: Quantum Software Engineering (QSE) is a research area practiced by tech
firms. Quantum developers face challenges in optimizing quantum computing and
QSE concepts. They use Stack Overflow (SO) to discuss challenges and label
posts with specialized quantum tags, which often refer to technical aspects
rather than developer posts. Categorizing questions based on quantum concepts
can help identify frequent QSE challenges. We conducted studies to classify
questions into various challenges. We extracted 2829 questions from Q&A
platforms using quantum-related tags. Posts were analyzed to identify frequent
challenges and develop a novel grounded theory. Challenges include Tooling,
Theoretical, Learning, Conceptual, Errors, and API Usage. Through content
analysis and grounded theory, discussions were annotated with common challenges
to develop a ground truth dataset. ChatGPT validated human annotations and
resolved disagreements. Fine-tuned transformer algorithms, including BERT,
DistilBERT, and RoBERTa, classified discussions into common challenges. We
achieved an average accuracy of 95% with BERT DistilBERT, compared to
fine-tuned Deep and Machine Learning (D&ML) classifiers, including Feedforward
Neural Networks (FNN), Convolutional Neural Networks (CNN), and Long Short-Term
Memory networks (LSTM), which achieved accuracies of 89%, 86%, and 84%,
respectively. The Transformer-based approach outperforms the D&ML-based
approach with a 6\% increase in accuracy by processing actual discussions,
i.e., without data augmentation. We applied SHAP (SHapley Additive
exPlanations) for model interpretability, revealing how linguistic features
drive predictions and enhancing transparency in classification. These findings
can help quantum vendors and forums better organize discussions for improved
access and readability. However,empirical evaluation studies with actual
developers and vendors are needed.

</details>


### [17] [Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach](https://arxiv.org/abs/2509.21170)
*Yongda Yu,Guohao Shi,Xianwei Wu,Haochuan He,XueMing Gu,Qianqian Zhao,Kui Liu,Qiushi Wang,Zhao Tian,Haifeng Shen,Guoping Rong*

Main category: cs.SE

TL;DR: MelcotCR improves LLM code review by training models to follow detailed reasoning paths, yielding superior code issue detection even in smaller models. It matches or exceeds larger models' performance by solving context and logic loss in long prompts.


<details>
  <summary>Details</summary>
Motivation: Current LLMs have shown promise in code review automation but lack human-level multidimensional reasoning, mainly due to limited or insufficiently structured fine-tuning data.

Method: The authors propose MelcotCR, a chain-of-thought fine-tuning approach, which trains LLMs to analyze multiple dimensions of code review using long and structured reasoning pathways. It combines Maximum Entropy modeling with pre-defined reasoning paths to address context and logic losses in long prompts.

Result: Empirical evaluations show that a low-parameter base model (14B Qwen2.5), fine-tuned with MelcotCR, outperforms state-of-the-art methods and achieves performance close to much larger models (671B DeepSeek-R1).

Conclusion: MelcotCR enables efficient and accurate code review capabilities for LLMs, even with lower-parameter models, by enhancing multidimensional reasoning through structured chain-of-thought fine-tuning.

Abstract: Large Language Models (LLMs) have shown great potential in supporting
automated code review due to their impressive capabilities in context
understanding and reasoning. However, these capabilities are still limited
compared to human-level cognition because they are heavily influenced by the
training data. Recent research has demonstrated significantly improved
performance through fine-tuning LLMs with code review data. However, compared
to human reviewers who often simultaneously analyze multiple dimensions of code
review to better identify issues, the full potential of these methods is
hampered by the limited or vague information used to fine-tune the models. This
paper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that
trains LLMs with an impressive reasoning ability to analyze multiple dimensions
of code review by harnessing long COT techniques to provide rich structured
information. To address context loss and reasoning logic loss issues that
frequently occur when LLMs process long COT prompts, we propose a solution that
combines the Maximum Entropy (ME) modeling principle with pre-defined reasoning
pathways in MelcotCR to enable more effective utilization of in-context
knowledge within long COT prompts while strengthening the logical tightness of
the reasoning process. Empirical evaluations on our curated MelcotCR dataset
and the public CodeReviewer dataset reveal that a low-parameter base model,
such as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art
methods in terms of the accuracy of detecting and describing code issues, with
its performance remarkably on par with that of the 671B DeepSeek-R1 model.

</details>


### [18] [Semantic Clustering of Civic Proposals: A Case Study on Brazil's National Participation Platform](https://arxiv.org/abs/2509.21292)
*Ronivaldo Ferreira,Guilherme da Silva,Carla Rocha,Gustavo Pinto*

Main category: cs.SE

TL;DR: Organizing citizen input on public platforms is hard due to volume and classification needs. This paper proposes a semi-automated method using BERTopic and large language models, yielding coherent, policy-aligned topics with minimal human labor, thus making public input usable for policymaking.


<details>
  <summary>Details</summary>
Motivation: Digital participation platforms generate large amounts of public input, but much of it remains underused because it's difficult to organize at scale, requires expert involvement, and must align with official classifications.

Method: The authors propose a method that uses BERTopic combined with seed words and automated validation using large language models to classify and structure citizen contributions efficiently.

Result: Initial results show that the generated topics are coherent, aligned with institutional requirements, and require minimal human intervention.

Conclusion: Their approach can help governments effectively process and leverage massive citizen input to support public policy decision-making.

Abstract: Promoting participation on digital platforms such as Brasil Participativo has
emerged as a top priority for governments worldwide. However, due to the sheer
volume of contributions, much of this engagement goes underutilized, as
organizing it presents significant challenges: (1) manual classification is
unfeasible at scale; (2) expert involvement is required; and (3) alignment with
official taxonomies is necessary. In this paper, we introduce an approach that
combines BERTopic with seed words and automatic validation by large language
models. Initial results indicate that the generated topics are coherent and
institutionally aligned, with minimal human effort. This methodology enables
governments to transform large volumes of citizen input into actionable data
for public policy.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [19] [Dual-Language General-Purpose Self-Hosted Visual Language and new Textual Programming Language for Applications](https://arxiv.org/abs/2509.20426)
*Mahmoud Samir Fayed*

Main category: cs.PL

TL;DR: PWCT2 is a new dual-language, self-hosting visual programming language made using a custom textual language, Ring. It provides much faster code generation, efficient storage, and can be built and extended using itself, with positive user reception and promising future potential.


<details>
  <summary>Details</summary>
Motivation: Most visual programming languages (VPLs) are domain-specific and existing general-purpose VPLs are dependent on textual languages for development and improvement, which limits accessibility and self-sufficiency.

Method: The authors designed and developed PWCT2, a dual-language (Arabic/English), general-purpose, self-hosting visual programming language. This involved first developing Ring, a textual, dynamically typed language, and then implementing its compiler and virtual machine using the PWCT visual programming language, later leading to the development of PWCT2 in Ring.

Result: PWCT2 provides substantial improvements: code generation is approximately 36 times faster, and visual source files require 20 times less storage. PWCT2 enables conversion between textual and visual code and is self-hosting. Its distribution has seen over 1,700 users and more than 17,000 hours of usage on Steam with positive feedback.

Conclusion: PWCT2 demonstrates the feasibility of a self-hosting, general-purpose, visual programming language developed in a textual language designed for such extensibility. The dual-language support and code efficiency gains make it a significant advance, and its uptake among users suggests strong potential for further research and development.

Abstract: Most visual programming languages (VPLs) are domain-specific, with few
general-purpose VPLs like Programming Without Coding Technology (PWCT). These
general-purpose VPLs are developed using textual programming languages and
improving them requires textual programming. In this thesis, we designed and
developed PWCT2, a dual-language (Arabic/English), general-purpose,
self-hosting visual programming language. Before doing so, we specifically
designed a textual programming language called Ring for its development. Ring
is a dynamically typed language with a lightweight implementation, offering
syntax customization features. It permits the creation of domain-specific
languages through new features that extend object-oriented programming,
allowing for specialized languages resembling Cascading Style Sheets (CSS) or
Supernova language. The Ring Compiler and Virtual Machine are designed using
the PWCT visual programming language where the visual implementation is
composed of 18,945 components that generate 24,743 lines of C code, which
increases the abstraction level and hides unnecessary details. Using PWCT to
develop Ring allowed us to realize several issues in PWCT, which led to the
development of the PWCT2 visual programming language using the Ring textual
programming language. PWCT2 provides approximately 36 times faster code
generation and requires 20 times less storage for visual source files. It also
allows for the conversion of Ring code into visual code, enabling the creation
of a self-hosting VPL that can be developed using itself. PWCT2 consists of
approximately 92,000 lines of Ring code and comes with 394 visual components.
PWCT2 is distributed to many users through the Steam platform and has received
positive feedback, On Steam, 1772 users have launched the software, and the
total recorded usage time exceeds 17,000 hours, encouraging further research
and development.

</details>


### [20] [Efficient Symbolic Computation vis Hash Consing](https://arxiv.org/abs/2509.20534)
*Bowen Zhu,Aayush Sabharwal,Songchen Tan,Yingbo Ma,Alan Edelman,Christopher Rackauckas*

Main category: cs.PL

TL;DR: Integrating hash consing into JuliaSymbolics significantly reduces memory usage and speeds up symbolic computations, code generation, and evaluation, especially for redundant, complex expressions. This is a key improvement for scaling symbolic tools and supporting future AI-driven mathematics.


<details>
  <summary>Details</summary>
Motivation: Symbolic computation systems face performance and memory problems due to expression swell, caused by redundant storage of structurally identical subexpressions. This issue hampers classical computer algebra systems as well as modern AI-based mathematical tools.

Method: The paper implements hash consing in the JuliaSymbolics toolkit by employing a global weak-reference hash table to canonicalize symbolic expressions and eliminate duplication. This solution is tightly integrated with Julia's metaprogramming and just-in-time compilation features.

Result: Benchmark tests show that this hash consing integration significantly speeds up symbolic operations (up to 3.2x), reduces memory consumption (up to 2x), makes code generation and compilation much faster (up to 5x and 10x, respectively), and dramatically improves numerical evaluation speed for large models (up to 100x). While workloads with few duplicate expressions have smaller gains or slight overhead initially, downstream tasks consistently benefit.

Conclusion: Hash consing in JuliaSymbolics enhances scalability and performance of symbolic computations and is particularly impactful for complex workloads with high redundancy; it also lays groundwork for future integration with e-graphs for equivalence-aware sharing in AI mathematical reasoning.

Abstract: Symbolic computation systems suffer from memory inefficiencies due to
redundant storage of structurally identical subexpressions, commonly known as
expression swell, which degrades performance in both classical computer algebra
and emerging AI-driven mathematical reasoning tools. In this paper, we present
the first integration of hash consing into JuliaSymbolics, a high-performance
symbolic toolkit in Julia, by employing a global weak-reference hash table that
canonicalizes expressions and eliminates duplication. This approach reduces
memory consumption and accelerates key operations such as differentiation,
simplification, and code generation, while seamlessly integrating with Julia's
metaprogramming and just-in-time compilation infrastructure. Benchmark
evaluations across different computational domains reveal substantial
improvements: symbolic computations are accelerated by up to 3.2 times, memory
usage is reduced by up to 2 times, code generation is up to 5 times faster,
function compilation up to 10 times faster, and numerical evaluation up to 100
times faster for larger models. While certain workloads with fewer duplicate
unknown-variable expressions show more modest gains or even slight overhead in
initial computation stages, downstream processing consistently benefits
significantly. These findings underscore the importance of hash consing in
scaling symbolic computation and pave the way for future work integrating hash
consing with e-graphs for enhanced equivalence-aware expression sharing in
AI-driven pipelines.

</details>
