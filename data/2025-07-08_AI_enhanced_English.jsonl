{"id": "2507.03629", "categories": ["cs.PL", "cs.FL", "F.4.3; D.3.1; D.3.4"], "pdf": "https://arxiv.org/pdf/2507.03629", "abs": "https://arxiv.org/abs/2507.03629", "authors": ["S\u00e9rgio Queiroz de Medeiros", "Fabio Mascarenhas"], "title": "Towards Automatic Error Recovery in Parsing Expression", "comment": "arXiv admin note: substantial text overlap with arXiv:1905.02145", "summary": "Error recovery is an essential feature for a parser that should be plugged in\nIntegrated Development Environments (IDEs), which must build Abstract Syntax\nTrees (ASTs) even for syntactically invalid programs in order to offer features\nsuch as automated refactoring and code completion.\n  Parsing Expressions Grammars (PEGs) are a formalism that naturally describes\nrecursive top-down parsers using a restricted form of backtracking. Labeled\nfailures are a conservative extension of PEGs that adds an error reporting\nmechanism for PEG parsers, and these labels can also be associated with\nrecovery expressions to also be an error recovery mechanism. These expressions\ncan use the full expressivity of PEGs to recover from syntactic errors.\n  Manually annotating a large grammar with labels and recovery expressions can\nbe difficult. In this work, we present an algorithm that automatically\nannotates a PEG with labels, and builds their corresponding recovery\nexpressions. We evaluate this algorithm by adding error recovery to the parser\nof the Titan programming language. The results shown that with a small amount\nof manual intervention our algorithm can be used to produce error recovering\nparsers for PEGs where most of the alternatives are disjoint.", "AI": {"tldr": "The paper presents an algorithm for automatic error recovery annotation in PEG parsers, which can be applied with little manual effort to produce robust parsers suitable for IDEs.", "motivation": "Error recovery is crucial for parsers in Integrated Development Environments (IDEs) because they need to generate Abstract Syntax Trees (ASTs) even when handling programs with syntax errors, in order to support features like code completion and automated refactoring.", "method": "The paper proposes an algorithm that automatically annotates Parsing Expressions Grammars (PEGs) with labels and constructs the corresponding recovery expressions. These labels and recovery expressions enable PEG-based parsers to recover from syntax errors, facilitating the construction of error-tolerant ASTs.", "result": "The algorithm is evaluated on the parser for the Titan programming language. The results demonstrate that, with minimal manual intervention, the algorithm is effective at producing error-recovering PEG parsers, particularly when most grammar alternatives are disjoint.", "conclusion": "The proposed algorithm enables automatic error recovery annotation for PEGs, making it feasible to implement robust error-recovering parsers for use in IDEs, with limited manual effort."}}
{"id": "2507.03867", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.03867", "abs": "https://arxiv.org/abs/2507.03867", "authors": ["Yu Xiang Zhu", "Amos Robinson", "Sophia Roshal", "Timothy Mou", "Julian Mackay", "Jonathan Aldrich", "Alex Potanin"], "title": "Semantically Separating Nominal Wyvern for Usability and Decidability", "comment": null, "summary": "The Dependent Object Types (DOT) calculus incorporates concepts from\nfunctional languages (e.g. modules) with traditional object-oriented features\n(e.g. objects, subtyping) to achieve greater expressivity (e.g. F-bounded\npolymorphism). However, this merger of paradigms comes at the cost of subtype\ndecidability. Recent work on bringing decidability to DOT has either sacrificed\nexpressiveness or ease of use. The unrestricted construction of recursive types\nand type bounds has made subtype decidability a much harder problem than in\ntraditional object-oriented programming.\n  Recognizing this, our paper introduces Nominal Wyvern, a DOT-like dependent\ntype system that takes an alternative approach: instead of having a uniform\nstructural syntax like DOT, Nominal Wyvern is designed around a \"semantic\nseparation\" between the nominal declaration of recursive types on the one hand,\nand the structural refinement of those types when they are used on the other.\nThis design naturally guides the user to avoid writing undecidably recursive\nstructural types.\n  From a technical standpoint, this separation also makes guaranteeing\ndecidability possible by allowing for an intuitive adaptation of material/shape\nseparation, a technique for achieving subtype decidability by separating types\nresponsible for subtyping constraints from types that represent concrete data.\nThe result is a type system with syntax and structure familiar to OOP users\nthat achieves decidability without compromising the expressiveness of F-bounded\npolymorphism and module systems as they are used in practice.", "AI": {"tldr": "Nominal Wyvern introduces a nominal/structural separation in type declarations, enabling subtype decidability in DOT-like systems without losing expressive features crucial for practical programming.", "motivation": "The paper addresses the problem that the Dependent Object Types (DOT) calculus, which aims to combine functional and object-oriented language features for greater expressiveness, suffers from undecidable subtyping due to the unrestricted use of recursive types and type bounds. Prior solutions have compromised expressiveness or usability.", "method": "Nominal Wyvern introduces a semantic separation between nominal (named) recursive type declarations and structural type refinements, rather than relying solely on structural syntax. By adapting the material/shape separation technique, the system clearly distinguishes types used for subtyping constraints from those representing data, facilitating decidability.", "result": "The semantic separation enables the design of a DOT-like dependent type system (Nominal Wyvern) that ensures subtyping remains decidable while preserving key expressiveness features like F-bounded polymorphism and practical module systems.", "conclusion": "Nominal Wyvern achieves subtype decidability without sacrificing the expressiveness required for advanced type systems found in both object-oriented and functional languages, providing a more usable and practical foundation for future language designs."}}
{"id": "2507.04298", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.04298", "abs": "https://arxiv.org/abs/2507.04298", "authors": ["Youngju Song", "Minki Cho"], "title": "CCR 2.0: High-level Reasoning for Conditional Refinements", "comment": null, "summary": "In recent years, great progress has been made in the field of formal\nverification for low-level systems. Many of them are based on one of two\npopular approaches: refinement or separation logic. These two approaches are\nvery different in nature and offer complementary benefits in terms of\ncompositionality. Recently, to fuse these benefits in a unified mechanism, a\nnew approach called Conditional Contextual Refinement (CCR 1.0 for short) was\nproposed. In this paper, we advance the model of CCR 1.0 and provide novel and\nintuitive reasoning principles, resulting in: CCR 2.0. Specifically, CCR 2.0\n(i) comes with a better compositionality theorem, having the practical benefit\nof facilitating more proof reuse, and (ii) provides a proof technique that\nhides model-level (i.e., resources of the separation logic) details from the\nuser. Achieving this goal was challenging due to non-trivial counterexamples\nwhich necessitated us to devise novel notions. Our results are formalized in\nCoq.", "AI": {"tldr": "The paper introduces CCR 2.0, an improved framework for formal verification of low-level systems, combining strengths of refinement and separation logic. It enhances proof reuse and abstraction through a new compositionality theorem and user-friendly proof techniques, with all results formalized in Coq.", "motivation": "Formal verification of low-level systems often relies on either refinement or separation logic, each offering unique benefits, especially regarding compositionality. The motivation of the paper is to unify the advantages of these two verification approaches, addressing the limitations in compositional reasoning and proof reuse.", "method": "The paper advances the Conditional Contextual Refinement (CCR) model from version 1.0 to version 2.0, proposing new and intuitive reasoning principles. The methodology includes improving the compositionality theorem to facilitate proof reuse and designing a proof technique that abstracts away separation logic details from users. Novel notions are developed to address non-trivial counterexamples. The work is formalized in Coq.", "result": "CCR 2.0 offers a better compositionality theorem, allowing for increased proof reuse in verification tasks. It also introduces a proof technique that hides resource-model details from the user, making formalization more accessible. The results and reasoning principles are implemented and verified in Coq.", "conclusion": "CCR 2.0 successfully advances the unified framework for compositional reasoning in system verification, overcoming challenges from previous versions and enhancing usability, proof reuse, and abstraction. The formalization in Coq verifies its soundness and practicality."}}
{"id": "2507.04316", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.04316", "abs": "https://arxiv.org/abs/2507.04316", "authors": ["Jay Lee"], "title": "Retargeting an Abstract Interpreter for a New Language by Partial Evaluation", "comment": "Presented at the Student Research Competition (SRC) at PLDI 2025\n  (https://pldi25.sigplan.org/details/pldi-2025-src/1/)", "summary": "It is well-known that abstract interpreters can be systematically derived\nfrom their concrete counterparts using a \"recipe,\" but developing sound static\nanalyzers remains a time-consuming task. Reducing the effort required and\nmechanizing the process of developing analyzers continues to be a significant\nchallenge. Is it possible to automatically retarget an existing abstract\ninterpreter for a new language?\n  We propose a novel technique to automatically derive abstract interpreters\nfor various languages from an existing abstract interpreter. By leveraging\npartial evaluation, we specialize an abstract interpreter for a source\nlanguage. The specialization is performed using the semantics of target\nlanguages written in the source language. Our approach eliminates the need to\ndevelop analyzers for new targets from scratch. We show that our method can\neffectively retarget an abstract interpreter for one language into a correct\nanalyzer for another language.", "AI": {"tldr": "This paper introduces a technique that uses partial evaluation to transform an existing abstract interpreter for one language into a static analyzer for another, automating and simplifying the analyzer development process for new languages.", "motivation": "Developing sound static analyzers is labor-intensive. The motivation is to mechanize and reduce the effort in creating static analyzers for new languages by reusing existing work.", "method": "The method involves leveraging partial evaluation to specialize an existing abstract interpreter for source languages, based on the semantics of target languages represented in the source language.", "result": "The proposed approach successfully and automatically derives correct analyzers for new languages from an existing abstract interpreter.", "conclusion": "The paper concludes that it is feasible to automatically retarget an existing abstract interpreter to new languages using their approach, without the need to manually develop analyzers from scratch."}}
{"id": "2507.03156", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.03156", "abs": "https://arxiv.org/abs/2507.03156", "authors": ["Amr Mohamed", "Maram Assi", "Mariam Guizani"], "title": "The Impact of LLM-Assistants on Software Developer Productivity: A Systematic Literature Review", "comment": "37 pages", "summary": "Large language model assistants (LLM-assistants) present new opportunities to\ntransform software development. Developers are increasingly adopting these\ntools across tasks, including coding, testing, debugging, documentation, and\ndesign. Yet, despite growing interest, there is no synthesis of how\nLLM-assistants affect software developer productivity. In this paper, we\npresent a systematic literature review of 37 peer-reviewed studies published\nbetween January 2014 and December 2024 that examine this impact. Our analysis\nreveals that LLM-assistants offer both considerable benefits and critical\nrisks. Commonly reported gains include minimized code search, accelerated\ndevelopment, and the automation of trivial and repetitive tasks. However,\nstudies also highlight concerns around cognitive offloading, reduced team\ncollaboration, and inconsistent effects on code quality. While the majority of\nstudies (92%) adopt a multi-dimensional perspective by examining at least two\nSPACE dimensions, reflecting increased awareness of the complexity of developer\nproductivity, only 14% extend beyond three dimensions, indicating substantial\nroom for more integrated evaluations. Satisfaction, Performance, and Efficiency\nare the most frequently investigated dimensions, whereas Communication and\nActivity remain underexplored. Most studies are exploratory (64%) and\nmethodologically diverse, but lack longitudinal and team-based evaluations.\nThis review surfaces key research gaps and provides recommendations for future\nresearch and practice. All artifacts associated with this study are publicly\navailable at https://zenodo.org/records/15788502.", "AI": {"tldr": "This systematic review analyzes 37 studies on the use of LLM-assistants in software development. While these tools can boost productivity and automate tasks, they also present risks such as decreased collaboration and inconsistent code quality. Most research focuses on a few productivity dimensions, and there is a need for more holistic and long-term studies.", "motivation": "Despite increasing use of large language model assistants (LLM-assistants) in software development, there is no comprehensive understanding of how these tools impact developer productivity. This paper aims to fill that gap by synthesizing current research findings.", "method": "A systematic literature review was conducted, covering 37 peer-reviewed studies published from January 2014 to December 2024. The studies were analyzed with respect to their findings on LLM-assistants across multiple dimensions of developer productivity.", "result": "LLM-assistants are shown to provide substantial benefits, such as minimizing code search, accelerating development, and automating repetitive tasks. However, they also introduce risks, including cognitive offloading, reduced team collaboration, and varied effects on code quality. Most studies investigate productivity from multiple perspectives but rarely cover all relevant dimensions. There is a lack of longitudinal and team-based studies.", "conclusion": "LLM-assistants can significantly influence software developer productivity, both positively and negatively. Research so far has focused on certain aspects like satisfaction, performance, and efficiency, while other areas remain understudied. The field would benefit from more comprehensive, team-oriented, and longitudinal evaluations. The study provides recommendations for advancing research and practice."}}
{"id": "2507.05234", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05234", "abs": "https://arxiv.org/abs/2507.05234", "authors": ["Jay Lee", "Joongwon Ahn", "Kwangkeun Yi"], "title": "React-tRace: A Semantics for Understanding React Hooks", "comment": "Conditionally accepted to OOPSLA 2025", "summary": "React has become the most widely used web front-end framework, enabling the\ncreation of user interfaces in a declarative and compositional manner. Hooks\nare a set of APIs that manage side effects in functional components in React.\nHowever, their semantics are often seen as opaque to developers, leading to UI\nbugs. In this paper, we formalize the semantics of the essence of React Hooks\nwe name React-tRace, providing a framework that clarifies their behavior. We\ndemonstrate that our model captures the behavior of React, by theoretically\nshowing that it embodies essential properties of Hooks and empirically\ncomparing our React-tRace-definitional interpreter against a test suite.\nFurthermore, we showcase a practical visualization tool based on the\nformalization to demonstrate how developers can better understand the semantics\nof Hooks.", "AI": {"tldr": "This paper formalizes the semantics of React Hooks, validating its model both theoretically and empirically, and provides a visualization tool to help developers understand and use Hooks more effectively.", "motivation": "React Hooks are widely used to manage side effects in modern web applications, but their semantics are often unclear to developers, resulting in bugs and misunderstandings.", "method": "The authors formalize the semantics of React Hooks through a model called React-tRace and validate it theoretically and empirically. They also present a visualization tool grounded in this formalization to aid developer understanding.", "result": "The model (React-tRace) accurately captures essential properties of React Hooks, as shown by comparisons with a test suite. The visualization tool built on this formalization helps developers comprehend Hooks' semantics.", "conclusion": "Formalizing React Hooks' semantics makes their behavior clearer, addresses previous opacities, and provides practical tools to help developers avoid bugs and design issues. The work improves both theoretical understanding and practical development with Hooks."}}
{"id": "2507.03160", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.03160", "abs": "https://arxiv.org/abs/2507.03160", "authors": ["Md Mahade Hasan", "Muhammad Waseem", "Kai-Kristian Kemell", "Jussi Raskua", "Juha Ala-Rantalaa", "Pekka Abrahamsson"], "title": "Assessing Small Language Models for Code Generation: An Empirical Study with Benchmarks", "comment": null, "summary": "The recent advancements of Small Language Models (SLMs) have opened new\npossibilities for efficient code generation. SLMs offer lightweight and\ncost-effective alternatives to Large Language Models (LLMs), making them\nattractive for use in resource-constrained environments. However, empirical\nunderstanding of SLMs, particularly their capabilities, limitations, and\nperformance trade-offs in code generation remains limited. This study presents\na comprehensive empirical evaluation of 20 open-source SLMs ranging from 0.4B\nto 10B parameters on five diverse code-related benchmarks (HumanEval, MBPP,\nMercury, HumanEvalPack, and CodeXGLUE). The models are assessed along three\ndimensions: i) functional correctness of generated code, ii) computational\nefficiency and iii) performance across multiple programming languages. The\nfindings of this study reveal that several compact SLMs achieve competitive\nresults while maintaining a balance between performance and efficiency, making\nthem viable for deployment in resource-constrained environments. However,\nachieving further improvements in accuracy requires switching to larger models.\nThese models generally outperform their smaller counterparts, but they require\nmuch more computational power. We observe that for 10% performance\nimprovements, models can require nearly a 4x increase in VRAM consumption,\nhighlighting a trade-off between effectiveness and scalability. Besides, the\nmultilingual performance analysis reveals that SLMs tend to perform better in\nlanguages such as Python, Java, and PHP, while exhibiting relatively weaker\nperformance in Go, C++, and Ruby. However, statistical analysis suggests these\ndifferences are not significant, indicating a generalizability of SLMs across\nprogramming languages. Based on the findings, this work provides insights into\nthe design and selection of SLMs for real-world code generation tasks.", "AI": {"tldr": "This paper finds that small language models can generate code effectively and efficiently, but surpassing their performance often requires much larger, more resource-intensive models. Performance variations across programming languages are minor, making SLMs practical for diverse real-world code generation tasks.", "motivation": "Small Language Models (SLMs) offer lightweight and efficient alternatives to large models. There is limited empirical analysis on their abilities, especially regarding code generation, functional correctness, computational efficiency, and multilingual performance.", "method": "The paper evaluates 20 open-source SLMs, spanning 0.4B to 10B parameters, across five code-related benchmarks. The assessment includes tests for code correctness, efficiency, and performance in multiple programming languages. Statistical analysis is used to interpret performance differences.", "result": "Some small SLMs perform competitively in code generation while being efficient. Larger models deliver better accuracy but at much higher computational costs, with 10% improvements requiring almost 4x more VRAM. SLMs generally perform well on Python, Java, and PHP, but less so on Go, C++, and Ruby. However, statistical analysis shows these differences are not significant, supporting SLMs' broad applicability.", "conclusion": "Compact SLMs can be effective for code generation in resource-limited settings, striking a balance between accuracy and efficiency. Significant accuracy gains demand much larger models and resources. SLMs show robustness across languages, assisting in the informed selection of models for practical tasks."}}
{"id": "2507.03659", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.03659", "abs": "https://arxiv.org/abs/2507.03659", "authors": ["Valentina Wu", "Alexandra Mendes", "Alexandre Abreu"], "title": "Specification-Guided Repair of Arithmetic Errors in Dafny Programs using LLMs", "comment": null, "summary": "Formal verification offers strong assurances of software correctness.\nHowever, debugging and repairing the underlying faults can be complex and\ntime-consuming when verification fails. Automated Program Repair (APR) aims to\nease this by automatically identifying and fixing faults. Traditional APR\ntechniques often depend on test suites for validation, but these may fail to\ncapture all scenarios. In contrast, formal specifications provide stronger\ncorrectness criteria for effective repairs.\n  We present an innovative APR tool for Dafny, a verification-aware programming\nlanguage that uses formal specifications - including pre-conditions,\npost-conditions, and invariants - as oracles for fault localization and repair.\nAssuming the correctness of the specifications and focusing on arithmetic bugs,\nwe localize faults through a series of steps, which include using Hoare Logic\nto determine the state of each statement within the program and\nstate-of-the-art Large Language Models (LLMs) to synthesize candidate fixes.\nThe chosen models were GPT-4o mini, Llama 3, Mistral 7B, and Llemma 7B.\n  We evaluate our approach using DafnyBench, a benchmark of real-world Dafny\nprograms. Our tool achieves 89.6% accuracy in fault localization, with GPT-4o\nmini yielding the highest repair success rate (74.18%). These results highlight\nthe potential of combining formal reasoning with LLM-driven program synthesis\nfor automated program repair.", "AI": {"tldr": "The paper presents a tool that uses formal specs and LLMs to automatically repair bugs in Dafny programs, showing strong fault localization (89.6%) and solid repair success with GPT-4o mini (74.18%).", "motivation": "Debugging and repairing software can be difficult when formal verification fails. Traditional automated program repair methods rely on test suites, which may not guarantee correctness. There is a need for an approach that leverages the stronger assurances provided by formal specifications.", "method": "The authors introduce an APR tool for Dafny, a verification-aware language. The tool assumes specifications are correct and targets arithmetic faults, using Hoare Logic for fault localization and Large Language Models (LLMs) like GPT-4o mini, Llama 3, Mistral 7B, and Llemma 7B to synthesize candidate repairs. Validation and experiments are conducted on the DafnyBench suite.", "result": "The tool achieves 89.6% accuracy in fault localization. For repairs, the best model (GPT-4o mini) achieves a 74.18% success rate.", "conclusion": "Combining formal reasoning from specifications with LLM-based synthesis is effective for automated program repair, providing high fault localization accuracy and strong repair capabilities, particularly for arithmetic bugs in verified programs."}}
{"id": "2507.03263", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.03263", "abs": "https://arxiv.org/abs/2507.03263", "authors": ["Haiqiao Gu", "Yiliang Zhao", "Kai Gao", "Minghui Zhou"], "title": "Analyzing C/C++ Library Migrations at the Package-level: Prevalence, Domains, Targets and Rationals across Seven Package Management Tools", "comment": null, "summary": "Library migration happens when a library can not meet the project's\nrequirements and is non-trivial to accomplish. To mitigate the problem,\nsubstantial efforts have been devoted to understanding its characteristics and\nrecommending alternative libraries, especially for programming language (PL)\necosystems with a central package hosting platform, such as Python (PyPI).\nHowever, to the best of our knowledge, understanding of C/C++ library\nmigrations is still lacking, possibly due to challenges resulting from the\nfragmented and complicated dependency management practices in the C/C++\necosystem. To bridge this knowledge gap, this paper analyzes 19,943 C/C++\nprojects that utilize different package management tools and establishes the\nfirst C/C++ library migration dataset. Based on the dataset, we investigate the\nprevalence, domains, target library, and rationale of C/C++ library migrations\nand compare the results with three widely investigated PLs: Python, JavaScript,\nand Java. We find that the overall trend in the number of C/C++ library\nmigrations is similar to Java. Migrations across different package management\ntools are also observed. In C/C++, library migrations mainly occur in GUI,\nBuild, and OS development, but are rare in domains (e.g., Testing and Logging)\nthat dominate library migrations in the three compared PLs. 83.46\\% of C/C++\nsource libraries only have one migration target, suggesting that our library\nmigration dataset could be used directly to recommend migration targets. We\nfind four C/C++-specific migration reasons, such as less compile time and\nunification of dependency management, revealing the unique dependency\nmanagement requirements in C/C++ projects. We believe our findings can help\nC/C++ developers make more informed library migration decisions and shed light\non the design of C/C++ library migration tools.", "AI": {"tldr": "This paper presents the first comprehensive study and dataset of C/C++ library migrations, revealing trends, key migration domains, and unique migration motivations, and provides actionable insights for tool development and decision-making in the C/C++ ecosystem.", "motivation": "There is insufficient understanding of library migrations in C/C++ due to fragmented and complex dependency management, unlike ecosystems like Python's, which have centralized management and are better studied.", "method": "The study analyzed 19,943 C/C++ projects across different package management tools, establishing a new dataset of C/C++ library migrations. It investigated the prevalence, domains, migration targets, and rationale for library migrations, and compared them to Python, JavaScript, and Java.", "result": "The overall trend and number of C/C++ library migrations is similar to Java. Migrations happen across different package management tools, mainly in GUI, build, and OS development, and rarely in domains like testing and logging. 83.46% of C/C++ source libraries have only one migration target. Four distinct C/C++-specific migration reasons were found, including reduced compile time and dependency management unification.", "conclusion": "The findings provide insights for C/C++ developers about migration trends and targets, and demonstrate unique dependency management requirements. The established dataset can directly support migration target recommendations and guide the design of library migration tools."}}
{"id": "2507.03328", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.03328", "abs": "https://arxiv.org/abs/2507.03328", "authors": ["S. Lee", "C. Myers", "A. Yang", "T. Zhang", "S. J. L. Billinge"], "title": "scikit-package -- software packaging standards and roadmap for sharing reproducible scientific software", "comment": "GitHub: https://github.com/scikit-package/scikit-package Doc:\n  https://scikit-package.github.io/scikit-package/", "summary": "Scientific advancement relies on the ability to share and reproduce results.\nWhen data analysis or calculations are carried out using software written by\nscientists there are special challenges around code versions, quality and code\nsharing. scikit-package provides a roadmap to facilitate code reuse and sharing\nwith minimal effort through tutorials coupled with automated and centralized\nreusable workflows. The goal of the project is to provide pedagogical and\npractical tools for scientists who are not professionally trained software\nengineers to write more reusable and maintainable software code. Code reuse can\noccur at multiple levels of complexity-from turning a code block into a\nfunction within a single script, to publishing a publicly installable, fully\ntested, and documented software package scikit-package provides a community\nmaintained set of tools, and a roadmap, to help scientists bring their software\nhigher levels of reproducibility and shareability.", "AI": {"tldr": "scikit-package is introduced to help scientists with limited programming background reuse and share their research code more easily, providing tutorials, tools, and workflows to improve reproducibility and software quality.", "motivation": "Scientists often face difficulties in sharing and reproducing research because of challenges with code versioning, quality, and distribution, especially when the software is written by non-professional developers.", "method": "The paper introduces 'scikit-package'\u2014a project providing tutorials, automated workflows, and centralized resources to help scientists make their code more reusable and shareable. It combines pedagogical materials with practical tools.", "result": "scikit-package offers community-maintained tools and a structured roadmap that enables scientists to improve code reusability and reproducibility at varying levels of complexity.", "conclusion": "scikit-package empowers scientists without formal software engineering training to create better, more maintainable research software, ultimately improving result reproducibility and software sharing."}}
{"id": "2507.03405", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.03405", "abs": "https://arxiv.org/abs/2507.03405", "authors": ["Krishna Ronanki", "Simon Arvidsson", "Johan Axell"], "title": "Prompt Engineering Guidelines for Using Large Language Models in Requirements Engineering", "comment": "Accepted for publication at the 51st Euromicro Conference Series on\n  Software Engineering and Advanced Applications (SEAA) 2025", "summary": "The rapid emergence of generative AI models like Large Language Models (LLMs)\nhas demonstrated its utility across various activities, including within\nRequirements Engineering (RE). Ensuring the quality and accuracy of\nLLM-generated output is critical, with prompt engineering serving as a key\ntechnique to guide model responses. However, existing literature provides\nlimited guidance on how prompt engineering can be leveraged, specifically for\nRE activities. The objective of this study is to explore the applicability of\nexisting prompt engineering guidelines for the effective usage of LLMs within\nRE. To achieve this goal, we began by conducting a systematic review of primary\nliterature to compile a non-exhaustive list of prompt engineering guidelines.\nThen, we conducted interviews with RE experts to present the extracted\nguidelines and gain insights on the advantages and limitations of their\napplication within RE. Our literature review indicates a shortage of prompt\nengineering guidelines for domain-specific activities, specifically for RE. Our\nproposed mapping contributes to addressing this shortage. We conclude our study\nby identifying an important future line of research within this field.", "AI": {"tldr": "Generative AI models are increasingly used in Requirements Engineering, but there is little guidance on prompt engineering for this domain. This study compiles existing guidelines and assesses their relevance through expert interviews, finding a gap and mapping guidelines to RE. It concludes by calling for more research and domain-specific guidance.", "motivation": "There is a surge of interest in using generative AI, specifically Large Language Models (LLMs), in Requirements Engineering (RE). However, current literature lacks detailed guidance on how prompt engineering can be effectively leveraged for RE activities.", "method": "The study conducts a systematic review of existing literature to compile guidelines for prompt engineering. It follows this by interviewing RE experts to discuss the extracted guidelines, assessing their strengths and weaknesses in RE-specific contexts.", "result": "The literature review revealed a significant shortage of prompt engineering guidelines tailored for domain-specific activities such as RE. The study offers a mapping of available guidelines to RE, addressing this gap.", "conclusion": "This paper identifies a need for more research and specialized guidelines on prompt engineering in Requirements Engineering. The mapping provided and insights from experts contribute to filling this gap, and the study highlights future research directions for developing more effective, RE-specific prompt engineering best practices."}}
{"id": "2507.03515", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.03515", "abs": "https://arxiv.org/abs/2507.03515", "authors": ["Radouane Bouchekir", "Michell Guzman Cancimance"], "title": "Enhancing Uncertainty Quantification for Runtime Safety Assurance Using Causal Risk Analysis and Operational Design Domain", "comment": null, "summary": "Ensuring the runtime safety of autonomous systems remains challenging due to\ndeep learning components' inherent uncertainty and their sensitivity to\nenvironmental changes. In this paper, we propose an enhancement of traditional\nuncertainty quantification by explicitly incorporating environmental conditions\nusing risk-based causal analysis. We leverage Hazard Analysis and Risk\nAssessment (HARA) and fault tree modeling to identify critical operational\nconditions affecting system functionality. These conditions, together with\nuncertainties from the data and model, are integrated into a unified Bayesian\nNetwork (BN). At runtime, this BN is instantiated using real-time environmental\nobservations to infer a probabilistic distribution over the safety estimation.\nThis distribution enables the computation of both expected performance and its\nassociated variance, providing a dynamic and context-aware measure of\nuncertainty. We demonstrate our approach through a case study of the Object\nDetection (OD) component in an Automated Valet Parking (AVP).", "AI": {"tldr": "The paper proposes a Bayesian Network-based method that unifies system and environmental uncertainty quantification for autonomous systems, boosting runtime safety by dynamically adapting to real-world operating conditions, as shown in an Automated Valet Parking use case.", "motivation": "Autonomous systems need robust runtime safety, but deep learning components introduce uncertainty and are sensitive to environment changes. Traditional uncertainty quantification methods do not explicitly factor in varying environmental conditions.", "method": "The paper enhances traditional uncertainty quantification by integrating risk-based causal analysis. Specifically, it employs Hazard Analysis and Risk Assessment (HARA) and fault tree modeling to identify crucial operational and environmental conditions, which are then combined with data and model uncertainties in a Bayesian Network. This network is instantiated at runtime with actual environmental data to dynamically estimate safety and uncertainty.", "result": "The approach provides a probabilistic safety estimation that is dynamically updated based on real-time environmental observations, allowing for both expected performance and variance to be computed in a context-sensitive manner. The method is demonstrated in a case study on the Object Detection module of an Automated Valet Parking system.", "conclusion": "Incorporating environmental context into uncertainty quantification for autonomous systems enables more reliable and adaptive runtime safety measures. The proposed Bayesian Network framework dynamically reflects both known system hazards and real-time environmental uncertainties."}}
{"id": "2507.03527", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.03527", "abs": "https://arxiv.org/abs/2507.03527", "authors": ["Dulaji Hidellaarachchi", "John Grundy", "Rashina Hoda"], "title": "The Role of Humour in Software Engineering -- A Literature Review and Preliminary Taxonomy", "comment": "Accepted to publish in Journal of Software Systems (JSS) New Idea\n  Track 2025 (23 pages, 1 figure)", "summary": "Humour has long been recognized as a key factor in enhancing creativity,\ngroup effectiveness, and employee well-being across various domains. However,\nits occurrence and impact within software engineering (SE) teams remains\nunder-explored. This paper introduces a comprehensive, literature review-based\ntaxonomy exploring the characterisation and use of humour in SE teams, with the\ngoal of boosting productivity, improving communication, and fostering a\npositive work environment while emphasising the responsible use of humour to\nmitigate its potential negative impacts. Drawing from a wide array of studies\nin psychology, sociology, and organizational behaviour, our proposed framework\ncategorizes humour into distinct theories, styles, models, and scales, offering\nSE professionals and researchers a structured approach to understanding humour\nin their work. This study also addresses the unique challenges of applying\nhumour in SE, highlighting its potential benefits while acknowledging the need\nfor further empirical validation in this context. Ultimately, our study aims to\npave the way for more cohesive, creative, and psychologically supportive SE\nenvironments through the strategic use of humour.", "AI": {"tldr": "This paper reviews and categorizes the use of humour in software engineering teams, presenting a framework to better understand its forms and impact. It finds that humour can boost productivity and well-being but requires careful and responsible use, plus further study in SE settings.", "motivation": "The paper is motivated by the recognition that humour can enhance creativity, group effectiveness, and employee well-being, yet its role and impact in software engineering (SE) teams remain understudied. The authors aim to fill this gap by providing a structured understanding of humour in SE contexts.", "method": "The paper uses a comprehensive literature review, drawing from studies in psychology, sociology, and organizational behaviour, to build a taxonomy categorizing humour in SE teams. The taxonomy includes theories, styles, models, and scales related to humour.", "result": "The outcome is a framework that categorizes humour for SE professionals and researchers, providing a structured approach to understanding and using humour in SE teams. The study also highlights both the potential benefits and the need for empirical validation of humour's impact in SE.", "conclusion": "The study concludes that understanding and strategically applying humour can foster more cohesive, creative, and supportive SE environments. However, further research is needed to empirically validate these benefits in SE contexts."}}
{"id": "2507.03536", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.03536", "abs": "https://arxiv.org/abs/2507.03536", "authors": ["Adam Tornhill", "Markus Borg", "Nadim Hagatulah", "Emma S\u00f6derberg"], "title": "ACE: Automated Technical Debt Remediation with Validated Large Language Model Refactorings", "comment": "Published in proceedings of the 1st International Workshop on\n  Artificial Intelligence for Integrated Development Environments (AI-IDE)\n  (2025)", "summary": "The remarkable advances in AI and Large Language Models (LLMs) have enabled\nmachines to write code, accelerating the growth of software systems. However,\nthe bottleneck in software development is not writing code but understanding\nit; program understanding is the dominant activity, consuming approximately 70%\nof developers' time. This implies that improving existing code to make it\neasier to understand has a high payoff and - in the age of AI-assisted coding -\nis an essential activity to ensure that a limited pool of developers can keep\nup with ever-growing codebases. This paper introduces Augmented Code\nEngineering (ACE), a tool that automates code improvements using validated LLM\noutput. Developed through a data-driven approach, ACE provides reliable\nrefactoring suggestions by considering both objective code quality improvements\nand program correctness. Early feedback from users suggests that AI-enabled\nrefactoring helps mitigate code-level technical debt that otherwise rarely gets\nacted upon.", "AI": {"tldr": "ACE leverages LLMs to automate code improvements, focusing on code clarity and correctness. It provides reliable refactoring suggestions, helping developers address technical debt and making codebases easier to sustain as they grow.", "motivation": "Software development is increasingly aided by AI and Large Language Models (LLMs), allowing machines to write code quickly. Despite this, the main bottleneck is not writing code, but understanding it, which occupies about 70% of developers\u2019 time. Enhancing code understandability is thus highly valuable, especially as codebases expand and developer resources remain limited.", "method": "The paper introduces Augmented Code Engineering (ACE), an automated tool that uses validated LLM output for code improvement. The tool employs a data-driven approach to generate reliable code refactoring suggestions, focusing on both objective improvements to code quality and preservation of program correctness.", "result": "ACE provides automated, reliable code refactoring suggestions. Early user feedback indicates that AI-enabled refactoring assists in reducing technical debt at the code level, which oftentimes remains unresolved.", "conclusion": "AI-assisted tools like ACE can significantly ease the burden of code comprehension by automating code improvement and refactoring, leading to more maintainable software and helping developers manage increasingly large and complex codebases."}}
{"id": "2507.03620", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG", "68T50", "I.2.7; D.2.3"], "pdf": "https://arxiv.org/pdf/2507.03620", "abs": "https://arxiv.org/abs/2507.03620", "authors": ["Francisca Lemos", "Victor Alves", "Filipa Ferraz"], "title": "Is It Time To Treat Prompts As Code? A Multi-Use Case Study For Prompt Optimization Using DSPy", "comment": "20 pages with 1 figure", "summary": "Although prompt engineering is central to unlocking the full potential of\nLarge Language Models (LLMs), crafting effective prompts remains a\ntime-consuming trial-and-error process that relies on human intuition. This\nstudy investigates Declarative Self-improving Python (DSPy), an optimization\nframework that programmatically creates and refines prompts, applied to five\nuse cases: guardrail enforcement, hallucination detection in code, code\ngeneration, routing agents, and prompt evaluation. Each use case explores how\nprompt optimization via DSPy influences performance. While some cases\ndemonstrated modest improvements - such as minor gains in the guardrails use\ncase and selective enhancements in hallucination detection - others showed\nnotable benefits. The prompt evaluation criterion task demonstrated a\nsubstantial performance increase, rising accuracy from 46.2% to 64.0%. In the\nrouter agent case, the possibility of improving a poorly performing prompt and\nof a smaller model matching a stronger one through optimized prompting was\nexplored. Although prompt refinement increased accuracy from 85.0% to 90.0%,\nusing the optimized prompt with a cheaper model did not improve performance.\nOverall, this study's findings suggest that DSPy's systematic prompt\noptimization can enhance LLM performance, particularly when instruction tuning\nand example selection are optimized together. However, the impact varies by\ntask, highlighting the importance of evaluating specific use cases in prompt\noptimization research.", "AI": {"tldr": "Automated prompt optimization with DSPy can boost LLM performance in some tasks, particularly when combining instruction tuning and example selection, but results are inconsistent across different use cases. Task-specific evaluation remains crucial.", "motivation": "Prompt engineering for Large Language Models (LLMs) is crucial but often inefficient, relying on manual trial and error. This paper is motivated by the need to automate and improve the effectiveness and efficiency of prompt optimization for LLMs.", "method": "The study investigates DSPy, a declarative and programmatic prompt optimization framework, by applying it to five specific use cases: guardrail enforcement, hallucination detection in code, code generation, routing agents, and prompt evaluation. The paper evaluates how DSPy can automate prompt creation and refinement for these tasks.", "result": "The results are mixed: some tasks, like guardrail enforcement and hallucination detection, saw only minor improvements, while tasks like prompt evaluation demonstrated significant performance increases (e.g., accuracy rising from 46.2% to 64.0%). Additionally, prompt refinement improved router agent task accuracy from 85.0% to 90.0%, but did not enable smaller models to catch up to larger ones simply by optimizing prompts.", "conclusion": "DSPy's systematic, programmatic prompt optimization can enhance the performance of LLMs, especially when combining instruction tuning with example selection. However, the degree of improvement is highly task-dependent, stressing the need for targeted evaluation of prompt optimization techniques."}}
{"id": "2507.04173", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.04173", "abs": "https://arxiv.org/abs/2507.04173", "authors": ["Henri A\u00efdasso", "Francis Bordeleau", "Ali Tizghadam"], "title": "Efficient Detection of Intermittent Job Failures Using Few-Shot Learning", "comment": "Accepted at the 41st International Conference on Software Maintenance\n  and Evolution - ICSME 2025, Industry Track", "summary": "One of the main challenges developers face in the use of continuous\nintegration (CI) and deployment pipelines is the occurrence of intermittent job\nfailures, which result from unexpected non-deterministic issues (e.g., flaky\ntests or infrastructure problems) rather than regular code-related errors such\nas bugs. Prior studies developed machine-learning (ML) models trained on large\ndatasets of job logs to classify job failures as either intermittent or\nregular. As an alternative to costly manual labeling of large datasets, the\nstate-of-the-art (SOTA) approach leveraged a heuristic based on\nnon-deterministic job reruns. However, this method mislabels intermittent job\nfailures as regular in contexts where rerunning suspicious job failures is not\nan explicit policy, and therefore limits the SOTA's performance in practice. In\nfact, our manual analysis of 2,125 job failures from 5 industrial and 1\nopen-source projects reveals that, on average, 32\\% of intermittent job\nfailures are mislabeled as regular. To address these limitations, this paper\nintroduces a novel approach to intermittent job failure detection using\nfew-shot learning (FSL). Specifically, we fine-tune a small language model\nusing a few number of manually labeled log examples to generate rich\nembeddings, which are then used to train an ML classifier. Our FSL-based\napproach achieves 70-88\\% F1-score with only 12 shots in all projects,\noutperforming the SOTA, which proved ineffective (34-52\\% F1-score) in 4\nprojects. Overall, this study underlines the importance of data quality over\nquantity and provides a more efficient and practical framework for the\ndetection of intermittent job failures in organizations.", "AI": {"tldr": "The paper presents a few-shot learning approach to detect intermittent job failures in CI pipelines, outperforming existing methods with higher F1-scores (70-88% vs 34-52%) using far fewer manual labels, thus emphasizing data quality over quantity for practical industrial adoption.", "motivation": "Developers struggle to manage intermittent job failures in continuous integration (CI) pipelines, as these are often caused by non-deterministic issues (like flaky tests or infrastructure problems) rather than actual code bugs. Existing machine learning (ML) models rely on large annotated datasets, and the state-of-the-art (SOTA) heuristic for data labeling\u2014based on job reruns\u2014leads to a high rate of mislabeling when reruns are not policy, limiting accuracy and usefulness.", "method": "The proposed method introduces a few-shot learning (FSL) approach. The researchers fine-tuned a small language model with a small number of manually labeled job logs (few-shot examples) to generate feature-rich embeddings. These embeddings are used to train a machine learning classifier for distinguishing intermittent from regular job failures.", "result": "The FSL-based method achieved F1-scores between 70% and 88% using only 12 labeled examples across all studied projects, which is significantly higher than the SOTA (34-52% F1-score in 4 projects). Manual analysis suggested that on average, 32% of intermittent failures were mislabeled with the SOTA method.", "conclusion": "The study demonstrates that their few-shot learning approach greatly improves detection of intermittent job failures in CI pipelines, outperforming current SOTA methods with significantly less manual labeling effort. Data quality is more critical than quantity, and their framework is more efficient and practical for industry use."}}
{"id": "2507.04185", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.04185", "abs": "https://arxiv.org/abs/2507.04185", "authors": ["Aniket Kesari", "Travis Breaux", "Tom Norton", "Sarah Santos", "Anmol Singhal"], "title": "From Legal Text to Tech Specs: Generative AI's Interpretation of Consent in Privacy Law", "comment": "10 pages, 1 figure, 20th International Conference on Artificial\n  Intelligence and Law (ICAIL 2025)", "summary": "Privacy law and regulation have turned to \"consent\" as the legitimate basis\nfor collecting and processing individuals' data. As governments have rushed to\nenshrine consent requirements in their privacy laws, such as the California\nConsumer Privacy Act (CCPA), significant challenges remain in understanding how\nthese legal mandates are operationalized in software. The opaque nature of\nsoftware development processes further complicates this translation. To address\nthis, we explore the use of Large Language Models (LLMs) in requirements\nengineering to bridge the gap between legal requirements and technical\nimplementation. This study employs a three-step pipeline that involves using an\nLLM to classify software use cases for compliance, generating LLM modifications\nfor non-compliant cases, and manually validating these changes against legal\nstandards. Our preliminary findings highlight the potential of LLMs in\nautomating compliance tasks, while also revealing limitations in their\nreasoning capabilities. By benchmarking LLMs against real-world use cases, this\nresearch provides insights into leveraging AI-driven solutions to enhance legal\ncompliance of software.", "AI": {"tldr": "This paper investigates using LLMs to bridge the gap between privacy law and software, finding they can automate compliance tasks but have reasoning limits, highlighting both their utility and current weaknesses for legal compliance in software engineering.", "motivation": "There is a gap between privacy law requirements (such as consent for data processing) and their practical implementation in software, due to the complexity of translating legal mandates into code and the opacity of software development processes.", "method": "The study uses a three-step pipeline leveraging Large Language Models (LLMs): (1) classifying software use cases for legal compliance, (2) generating LLM-driven modifications for non-compliance, and (3) manual validation of these modifications against legal standards.", "result": "Preliminary findings show that LLMs have potential to automate some compliance tasks but also possess limitations in their reasoning, with benchmark results reflecting both their strengths and areas needing improvement.", "conclusion": "LLMs can be valuable tools to automate and support legal compliance in software engineering, although their limitations must be considered. Benchmarking LLMs provides practical insights for improving AI-driven compliance solutions."}}
{"id": "2507.04354", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.04354", "abs": "https://arxiv.org/abs/2507.04354", "authors": ["Yanzhou Mu", "Juan Zhai", "Chunrong Fang", "Xiang Chen", "Zhixiang Cao", "Peiran Yang", "Kexin Zhao", "An Guo", "Zhenyu Chen"], "title": "Improving Deep Learning Framework Testing with Model-Level Metamorphic Testing", "comment": "23 pages, 5 figures", "summary": "Deep learning (DL) frameworks are essential to DL-based software systems, and\nframework bugs may lead to substantial disasters, thus requiring effective\ntesting. Researchers adopt DL models or single interfaces as test inputs and\nanalyze their execution results to detect bugs. However, floating-point errors,\ninherent randomness, and the complexity of test inputs make it challenging to\nanalyze execution results effectively, leading to existing methods suffering\nfrom a lack of suitable test oracles. Some researchers utilize metamorphic\ntesting to tackle this challenge. They design Metamorphic Relations (MRs) based\non input data and parameter settings of a single framework interface to\ngenerate equivalent test inputs, ensuring consistent execution results between\noriginal and generated test inputs. Despite their promising effectiveness, they\nstill face certain limitations. (1) Existing MRs overlook structural\ncomplexity, limiting test input diversity. (2) Existing MRs focus on limited\ninterfaces, which limits generalization and necessitates additional\nadaptations. (3) Their detected bugs are related to the result consistency of\nsingle interfaces and far from those exposed in multi-interface combinations\nand runtime metrics (e.g., resource usage). To address these limitations, we\npropose ModelMeta, a model-level metamorphic testing method for DL frameworks\nwith four MRs focused on the structure characteristics of DL models. ModelMeta\naugments seed models with diverse interface combinations to generate test\ninputs with consistent outputs, guided by the QR-DQN strategy. It then detects\nbugs through fine-grained analysis of training loss/gradients, memory/GPU\nusage, and execution time.", "AI": {"tldr": "ModelMeta enhances bug detection in deep learning frameworks by using model-level metamorphic testing focused on structural diversity and multi-interface combinations, overcoming limitations of prior methods.", "motivation": "Deep learning framework bugs can cause significant problems, but effective testing is difficult due to floating-point errors, randomness, and complex test inputs. Current methods struggle because there are no suitable test oracles.", "method": "The paper proposes ModelMeta, a model-level metamorphic testing method for deep learning frameworks. It introduces four metamorphic relations (MRs) based on the structural characteristics of DL models. ModelMeta generates diverse test inputs via interface combinations, applies the QR-DQN strategy for guidance, and conducts detailed analysis of training loss, gradients, resource usage, and execution time to detect bugs.", "result": "ModelMeta addresses the main limitations of prior metamorphic testing approaches by increasing test input diversity, generalizing across more interfaces, and detecting bugs related to multi-interface interactions and runtime metrics.", "conclusion": "ModelMeta is an effective and general method to improve bug detection in DL frameworks, going beyond previous single-interface and limited-metric strategies by leveraging model-structure-based metamorphic testing and multi-metric analysis."}}
{"id": "2507.04360", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.04360", "abs": "https://arxiv.org/abs/2507.04360", "authors": ["Yanzhou Mu", "Juan Zhai", "Chunrong Fang", "Xiang Chen", "Zhixiang Cao", "Peiran Yang", "Yinglong Zou", "Tao Zheng", "Zhenyu Chen"], "title": "DevMuT: Testing Deep Learning Framework via Developer Expertise-Based Mutation", "comment": "12 pages, 8 figures", "summary": "Deep learning (DL) frameworks are the fundamental infrastructure for various\nDL applications. Framework defects can profoundly cause disastrous accidents,\nthus requiring sufficient detection. In previous studies, researchers adopt DL\nmodels as test inputs combined with mutation to generate more diverse models.\nThough these studies demonstrate promising results, most detected defects are\nconsidered trivial (i.e., either treated as edge cases or ignored by the\ndevelopers). To identify important bugs that matter to developers, we propose a\nnovel DL framework testing method DevMuT, which generates models by adopting\nmutation operators and constraints derived from developer expertise. DevMuT\nsimulates developers'common operations in development and detects more diverse\ndefects within more stages of the DL model lifecycle (e.g., model training and\ninference). We evaluate the performance of DevMuT on three widely used DL\nframeworks (i.e., PyTorch, JAX, and Mind- Spore) with 29 DL models from nine\ntypes of industry tasks. The experiment results show that DevMuT outperforms\nstate-of-the-art baselines: it can achieve at least 71.68% improvement on\naverage in the diversity of generated models and 28.20% improvement on average\nin the legal rates of generated models. Moreover, DevMuT detects 117 defects,\n63 of which are confirmed, 24 are fixed, and eight are of high value confirmed\nby developers. Finally, DevMuT has been deployed in the MindSpore community\nsince December 2023. These demonstrate the effectiveness of DevMuT in detecting\ndefects that are close to the real scenes and are of concern to developers.", "AI": {"tldr": "DevMuT is a novel DL framework testing method that uses developer-inspired mutations to find more relevant and diverse bugs in popular frameworks, outperforming existing tools and already being adopted in industry.", "motivation": "Existing deep learning framework testing approaches often detect only trivial defects that are frequently ignored by developers, failing to uncover high-impact and relevant bugs. There is a need for testing methods that expose issues significant to developers, reflecting real-world development and usage scenarios.", "method": "The paper introduces DevMuT, a DL framework testing technique that combines mutation operators and constraints derived from developer expertise. By simulating typical developer operations during different stages of the DL model lifecycle (including training and inference), it generates more diverse and relevant test models.", "result": "DevMuT was tested on three popular DL frameworks (PyTorch, JAX, MindSpore) across 29 models from nine industry application categories. It demonstrated an average improvement of at least 71.68% in model diversity and 28.20% in the legal rate of generated models compared to state-of-the-art methods. DevMuT detected 117 defects, with 63 confirmed and 24 already fixed; eight high-value bugs were acknowledged by developers. DevMuT has been integrated into the MindSpore community since December 2023.", "conclusion": "DevMuT is an effective framework testing tool that identifies more diverse and developer-relevant defects than existing methods, contributing practically actionable value by exposing issues that matter most to DL framework developers and being adopted by industry communities."}}
{"id": "2507.04390", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.04390", "abs": "https://arxiv.org/abs/2507.04390", "authors": ["Vanesya Aura Ardity", "Yusuf Sulistyo Nugroho", "Syful Islam"], "title": "Exploring React Library Related Questions on Stack Overflow: Answered vs. Unanswered", "comment": "6 pages, 9 figures, 7 tables, conference paper", "summary": "React is a popular JavaScript framework in modern web application\ndevelopment. Due to its high performance and efficiency, many developers use\nthis framework. Although React library offers many advantages, it is not\nwithout its challenges. When using React library, developers often face\nproblems where they often seek solutions through question-and-answer forums,\nsuch as Stack Overflow (SO). However, despite its high popularity, many\nReact-related questions on SO remain unanswered. Thus, this study aims to\nanalyze the factors associated with question answerability and difficulty\nlevels of React-related questions on SO. To facilitate our study, Exploratory\nData Analysis was applied to 534,820 questions, where they are filtered based\non 23 React-related tags. We implemented a quantitative approach through text\nmining and statistical analysis. A logistic regression model was used to\nidentify attributes associated with question answerability, while a simple\nlinear regression model was employed to examine the correlation between user\nreputations and performance difficulty scores (PD Score). The results show that\nsome attributes, such as number of views, code snippet inclusion, number of\nlines of code, and user reputation, positively affect the likelihood of\nquestion answerability. In contrast, the number of comments, question lengths,\nand presence of images in React-related questions reduce the probability of a\nquestion receiving responses from users. Further investigation indicates a\nnegative correlation between user reputations and PD Score, where reputation\nincrease corresponds to -0.092 reduction in PD score, signaling experienced\nusers tend to propose more complex technical inquiries. This study provides\ninsights into the characteristics of technical question-and-answer platforms,\nsuch as SO, that users need to consider the answerability factors when posting\nquestions related to React.", "AI": {"tldr": "The paper analyzes over half a million React-related Stack Overflow questions, revealing that concise questions with code snippets and higher user reputation are more likely to be answered, while longer questions and images decrease answerability. High-reputation users tend to ask tougher questions. Users should tailor questions for better responses.", "motivation": "React is widely used in modern web development, leading to many developers seeking help on Stack Overflow (SO). However, a significant portion of React-related questions remain unanswered, highlighting the need to understand the factors that affect question answerability and difficulty.", "method": "The study uses Exploratory Data Analysis on 534,820 Stack Overflow questions filtered by 23 React-related tags. It applies text mining and statistical analysis. Logistic regression is used to find factors affecting answerability, and simple linear regression analyses the relationship between user reputation and question difficulty (PD Score).", "result": "Attributes such as the number of views, code snippets, lines of code, and user reputation increase the probability of a question being answered. Conversely, more comments, longer questions, and the inclusion of images decrease the probability of receiving responses. There is a negative correlation between user reputation and PD Score, meaning higher-reputation users tend to ask more complex questions.", "conclusion": "Certain aspects of how questions are asked on Stack Overflow significantly impact their likelihood of being answered. The study suggests users should consider these factors, like including code snippets and keeping questions concise, to improve their chances of receiving answers to React-related questions."}}
{"id": "2507.04422", "categories": ["cs.SE", "cs.AI", "D.2.7; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.04422", "abs": "https://arxiv.org/abs/2507.04422", "authors": ["Guoming Long", "Jingzhi Gong", "Hui Fang", "Tao Chen"], "title": "Learning Software Bug Reports: A Systematic Literature Review", "comment": "Accepted by TOSEM", "summary": "The recent advancement of artificial intelligence, especially machine\nlearning (ML), has significantly impacted software engineering research,\nincluding bug report analysis. ML aims to automate the understanding,\nextraction, and correlation of information from bug reports. Despite its\ngrowing importance, there has been no comprehensive review in this area. In\nthis paper, we present a systematic literature review covering 1,825 papers,\nselecting 204 for detailed analysis. We derive seven key findings: 1) Extensive\nuse of CNN, LSTM, and $k$NN for bug report analysis, with advanced models like\nBERT underutilized due to their complexity. 2) Word2Vec and TF-IDF are popular\nfor feature representation, with a rise in deep learning approaches. 3) Stop\nword removal is the most common preprocessing, with structural methods rising\nafter 2020. 4) Eclipse and Mozilla are the most frequently evaluated software\nprojects. 5) Bug categorization is the most common task, followed by bug\nlocalization and severity prediction. 6) There is increasing attention on\nspecific bugs like non-functional and performance bugs. 7) Common evaluation\nmetrics are F1-score, Recall, Precision, and Accuracy, with $k$-fold\ncross-validation preferred for model evaluation. 8) Many studies lack robust\nstatistical tests. We also identify six promising future research directions to\nprovide useful insights for practitioners.", "AI": {"tldr": "This paper provides a comprehensive review of 204 papers on machine learning for bug report analysis, highlighting common methods, trends, and gaps, and suggesting future research directions.", "motivation": "Artificial intelligence, particularly machine learning (ML), has greatly influenced software engineering research tasks such as bug report analysis. There has been notable progress in automating bug report understanding and classification, but no comprehensive review has yet analyzed the scope, methods, and trends in ML for bug report analysis. This paper addresses this research gap.", "method": "The authors conducted a systematic literature review of 1,825 papers, narrowing down to 204 for detailed study. They extracted information regarding machine learning models used, feature representation methods, data preprocessing techniques, software systems evaluated, types of tasks (e.g., bug categorization), evaluation metrics, model evaluation techniques, and statistical testing. Key trends and gaps in the literature were synthesized.", "result": "Key findings include: (1) CNN, LSTM, and $k$NN are extensively used for bug report analysis, while advanced models like BERT are underutilized due to complexity. (2) Word2Vec and TF-IDF are common for feature representation, with deep learning methods on the rise. (3) Stop word removal dominates preprocessing, but structural modeling is increasing post-2020. (4) Eclipse and Mozilla are the most commonly evaluated projects. (5) Bug categorization is the most frequent task, followed by localization and severity prediction. (6) Focus on non-functional and performance bugs is increasing. (7) Standard metrics and $k$-fold cross-validation are widely used for evaluation, but (8) many studies lack robust statistical analyses.", "conclusion": "ML is widely used for bug report analysis with evolving trends towards deep learning and structural preprocessing. Several challenges remain, such as limited use of advanced language models and weak statistical validation. The paper identifies six future research directions to guide practitioners in the field."}}
{"id": "2507.04548", "categories": ["cs.SE", "cs.AI", "cs.LG", "D.2.11; D.2.7; I.2.7; I.5.4"], "pdf": "https://arxiv.org/pdf/2507.04548", "abs": "https://arxiv.org/abs/2507.04548", "authors": ["Renato Cordeiro Ferreira", "Dayanne Gomes", "Vitor Tamae", "Francisco Wernke", "Alfredo Goldman"], "title": "SPIRA: Building an Intelligent System for Respiratory Insufficiency Detection", "comment": "4 pages, 1 figure (1 diagram), published at ISE 2022", "summary": "Respiratory insufficiency is a medic symptom in which a person gets a reduced\namount of oxygen in the blood. This paper reports the experience of building\nSPIRA: an intelligent system for detecting respiratory insufficiency from\nvoice. It compiles challenges faced in two succeeding implementations of the\nsame architecture, summarizing lessons learned on data collection, training,\nand inference for future projects in similar systems.", "AI": {"tldr": "The paper describes building and testing SPIRA, an AI system to detect respiratory insufficiency from voice, and shares valuable insights on challenges and best practices for similar future work.", "motivation": "The motivation is to enable early and non-invasive detection of respiratory insufficiency using voice, potentially providing a simple and scalable screening tool for medical use.", "method": "The authors implemented two versions of an intelligent system (SPIRA) designed to detect respiratory insufficiency from voice signals. They faced and documented multiple challenges related to data collection, model training, and inference, drawing on real-world experiences in both iterations.", "result": "The project resulted in two working iterations of the SPIRA system and a compilation of practical lessons regarding the implementation, focusing on crucial elements like data acquisition, model improvement, and deployment challenges.", "conclusion": "The paper concludes by summarizing key lessons learned during the development of SPIRA, focusing on data collection, training, and inference, which can benefit future projects aiming to detect health conditions from voice."}}
{"id": "2507.04555", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.04555", "abs": "https://arxiv.org/abs/2507.04555", "authors": ["Gabriella Waters"], "title": "Testing, Evaluation, Verification and Validation (TEVV) of Digital Twins: A Comprehensive Framework", "comment": "1 figure, 41 pages, 3 tables", "summary": "Digital twins have emerged as a powerful technology for modeling and\nsimulating complex systems across various domains (Fuller et al., 2020; Tao et\nal., 2019). As virtual representations of physical assets, processes, or\nsystems, digital twins enable real-time monitoring, predictive analysis, and\noptimization. However, as digital twins become more sophisticated and integral\nto decision-making processes, ensuring their accuracy, reliability, and ethical\nimplementation is essential. This paper presents a comprehensive framework for\nthe Testing, Evaluation, Verification and Validation (TEVV) of digital twins to\naddress the unique challenges posed by these dynamic and complex virtual\nmodels.", "AI": {"tldr": "The paper introduces a new TEVV framework to ensure digital twins, used for modeling and simulating complex systems, remain accurate, reliable, and ethically implemented as their use expands.", "motivation": "Digital twins are becoming increasingly sophisticated and critical for decision-making across many domains, making it crucial to ensure their accuracy, reliability, and ethical use.", "method": "The paper proposes a comprehensive framework specifically for Testing, Evaluation, Verification, and Validation (TEVV) tailored to digital twins, considering the unique challenges these dynamic models pose.", "result": "The result is a structured and comprehensive framework that addresses how to test, evaluate, verify, and validate digital twins to ensure they function accurately, reliably, and ethically.", "conclusion": "By providing a dedicated TEVV framework, the paper aims to enhance the credibility and trustworthiness of digital twins as they are integrated into essential processes across various fields."}}
{"id": "2507.04857", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.04857", "abs": "https://arxiv.org/abs/2507.04857", "authors": ["Weiqi Wang", "Marie Farrell", "Lucas C. Cordeiro", "Liping Zhao"], "title": "Supporting Software Formal Verification with Large Language Models: An Experimental Study", "comment": "Accepted for publication in 2025 IEEE 33rd International Requirements\n  Engineering Conference (RE)", "summary": "Formal methods have been employed for requirements verification for a long\ntime. However, it is difficult to automatically derive properties from natural\nlanguage requirements. SpecVerify addresses this challenge by integrating large\nlanguage models (LLMs) with formal verification tools, providing a more\nflexible mechanism for expressing requirements. This framework combines Claude\n3.5 Sonnet with the ESBMC verifier to form an automated workflow. Evaluated on\nnine cyber-physical systems from Lockheed Martin, SpecVerify achieves 46.5%\nverification accuracy, comparable to NASA's CoCoSim, but with lower false\npositives. Our framework formulates assertions that extend beyond the\nexpressive power of LTL and identifies falsifiable cases that are missed by\nmore traditional methods. Counterexample analysis reveals CoCoSim's limitations\nstemming from model connection errors and numerical approximation issues. While\nSpecVerify advances verification automation, our comparative study of Claude,\nChatGPT, and Llama shows that high-quality requirements documentation and human\nmonitoring remain critical, as models occasionally misinterpret specifications.\nOur results demonstrate that LLMs can significantly reduce the barriers to\nformal verification, while highlighting the continued importance of\nhuman-machine collaboration in achieving optimal results.", "AI": {"tldr": "SpecVerify uses AI language models and formal verification tools to automate checking of system requirements written in natural language. It matches state-of-the-art results with fewer mistakes, identifies issues missed by existing tools, and shows human review is still necessary for best results.", "motivation": "Automatically deriving formal properties from natural language requirements is a longstanding challenge in requirements verification. Existing formal methods struggle to flexibly and accurately translate these requirements, motivating an approach that leverages the capabilities of large language models.", "method": "SpecVerify is proposed as a framework integrating large language models (LLMs), specifically Claude 3.5 Sonnet, with the ESBMC formal verification tool. The pipeline automates the translation of natural language requirements into formal assertions, which are then verified. The evaluation involved nine cyber-physical system models from Lockheed Martin. Comparative studies include CoCoSim and other LLMs (Claude, ChatGPT, Llama).", "result": "SpecVerify achieved 46.5% verification accuracy, on par with NASA's CoCoSim but with fewer false positives. The framework generated assertions extending beyond LTL capabilities and detected cases missed by traditional tools. Analysis highlighted CoCoSim's pitfalls with model connections and numerical approximations. The study also found that the success of LLM-based verification depends on high-quality documentation and human oversight, as models can misinterpret.", "conclusion": "Integrating LLMs with formal verification tools can automate and improve requirements verification by reducing false positives and capturing more expressive properties. However, human monitoring remains essential because current LLMs are not foolproof. Human-machine collaboration is key to optimizing formal verification outcomes."}}
{"id": "2507.04871", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.04871", "abs": "https://arxiv.org/abs/2507.04871", "authors": ["Jerome Pfeiffer", "Jingxi Zhang", "Benoit Combemale", "Judith Michael", "Bernhard Rumpe", "Manuel Wimmer", "Andreas Wortmann"], "title": "Towards a Unifying Reference Model for Digital Twins of Cyber-Physical Systems", "comment": null, "summary": "Digital twins are sophisticated software systems for the representation,\nmonitoring, and control of cyber-physical systems, including automotive,\navionics, smart manufacturing, and many more. Existing definitions and\nreference models of digital twins are overly abstract, impeding their\ncomprehensive understanding and implementation guidance. Consequently, a\nsignificant gap emerges between abstract concepts and their industrial\nimplementations. We analyze popular reference models for digital twins and\ncombine these into a significantly detailed unifying reference model for\ndigital twins that reduces the concept-implementation gap to facilitate their\nengineering in industrial practice. This enhances the understanding of the\nconcepts of digital twins and their relationships and guides developers to\nimplement digital twins effectively.", "AI": {"tldr": "The paper develops a unified and detailed reference model for digital twins by analyzing and combining various existing abstract models, helping bridge the gap between conceptual understanding and industrial application.", "motivation": "Existing digital twin definitions and reference models are overly abstract, which hinders comprehensive understanding and practical implementation. This lack of clarity creates a significant gap between conceptualization and industrial deployment.", "method": "The authors analyze popular reference models for digital twins and synthesize them into a more detailed, unified reference model. This new model aims to bridge the gap between abstract concepts and their real-world implementations.", "result": "The study presents a significantly detailed and unifying reference model for digital twins that aids in better understanding and helps developers implement digital twins more effectively in practice.", "conclusion": "The proposed unified reference model narrows the gap between digital twin concepts and practical implementation, providing better guidance for industrial engineering and fostering effective digital twin development."}}
{"id": "2507.05100", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05100", "abs": "https://arxiv.org/abs/2507.05100", "authors": ["Haoran Wei", "Nazim Madhavji", "John Steinbacher"], "title": "Understanding Everything as Code: A Taxonomy and Conceptual Model", "comment": "Accepted by the 19th ACM/IEEE International Symposium on Empirical\n  Software Engineering and Measurement (ESEM 2025), Technical Papers track", "summary": "Background: Everything as Code (EaC) is an emerging paradigm aiming to codify\nall aspects of modern software systems. Despite its growing popularity,\ncomprehensive industry standards and peer-reviewed research clarifying its\nscope and guiding its adoption remain scarce. Aims: This study systematically\nanalyzes existing knowledge and perceptions of EaC, clarifies its scope and\nboundaries, and provides structured guidance for researchers and practitioners.\nMethod: We conducted a large-scale multivocal literature review (MLR),\nsynthesizing academic and grey literature sources. Findings were analyzed\nquantitatively and thematically. Based on this analysis, we developed a\ntaxonomy and conceptual model of EaC, validated through collaboration with\nindustry experts. Results: The resulting taxonomy comprises 25 distinct EaC\npractices organized into six layers based on industry awareness and functional\nroles. The conceptual model illustrates focus areas, overlaps, and interactions\namong these EaC practices within the software delivery lifecycle. Additionally,\npractical code examples demonstrating the implementation of these practices\nwere developed in collaboration with industry experts. Conclusions: This work\naddresses the current scarcity of academic discourse on EaC by providing the\nfirst comprehensive taxonomy and conceptual model. These contributions enhance\nconceptual clarity, offer actionable guidance to practitioners, and lay the\ngroundwork for future research in this emerging domain.", "AI": {"tldr": "The paper systematically reviews and organizes all aspects of Everything as Code, resulting in a validated taxonomy and conceptual model that clarify the domain and provide actionable guidance for practitioners and researchers.", "motivation": "Everything as Code (EaC) is gaining traction, but there is a notable lack of industry standards and scholarly research that clearly defines its scope and offers guidance for adoption. The study aims to fill this gap by analyzing existing knowledge, clarifying what EaC encompasses, and structuring actionable insights for both researchers and practitioners.", "method": "A large-scale multivocal literature review (MLR) was conducted, synthesizing both academic and grey literature. The findings were analyzed using quantitative and thematic approaches. From this analysis, a taxonomy and conceptual model of EaC were created and validated with input from industry experts.", "result": "The study produced a taxonomy containing 25 distinct EaC practices, organized into six layers based on their functional role and industry awareness. A conceptual model that shows the focus areas, overlaps, and interactions between these practices in the software delivery lifecycle was provided. Also, practical code examples were developed in collaboration with industry experts.", "conclusion": "This work provides the first comprehensive taxonomy and conceptual model for Everything as Code, filling the academic gap and offering clear guidance for practitioners. It also establishes a foundation for further research in this emerging field."}}
{"id": "2507.05200", "categories": ["cs.SE", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.05200", "abs": "https://arxiv.org/abs/2507.05200", "authors": ["Susmita Das", "Madhusudan Ghosh", "Priyanka Swami", "Debasis Ganguly", "Gul Calikli"], "title": "In-Context Learning as an Effective Estimator of Functional Correctness of LLM-Generated Code", "comment": null, "summary": "When applying LLM-based code generation to software development projects that\nfollow a feature-driven or rapid application development approach, it becomes\nnecessary to estimate the functional correctness of the generated code in the\nabsence of test cases. Just as a user selects a relevant document from a ranked\nlist of retrieved ones, a software generation workflow requires a developer to\nchoose (and potentially refine) a generated solution from a ranked list of\nalternative solutions, ordered by their posterior likelihoods. This implies\nthat estimating the quality of a ranked list -- akin to estimating \"relevance\"\nfor query performance prediction (QPP) in IR -- is also crucial for generative\nsoftware development, where quality is defined in terms of \"functional\ncorrectness\". In this paper, we propose an in-context learning (ICL) based\napproach for code quality estimation. Our findings demonstrate that providing\nfew-shot examples of functionally correct code from a training set enhances the\nperformance of existing QPP approaches as well as a zero-shot-based approach\nfor code quality estimation.", "AI": {"tldr": "The paper proposes using a few-shot in-context learning approach to better estimate the functional correctness of LLM-generated code, showing it outperforms existing zero-shot and QPP techniques in the absence of test cases.", "motivation": "In LLM-based code generation for development workflows, it is challenging to estimate functional correctness of generated code when test cases are unavailable, especially for workflows involving feature-driven or rapid application development.", "method": "This paper proposes an in-context learning (ICL) approach for code quality estimation, supplying few-shot functionally correct code examples to enhance prediction.", "result": "Providing few-shot, functionally correct code examples improves the performance of both existing QPP approaches and zero-shot-based code quality estimation methods.", "conclusion": "Incorporating few-shot in-context examples of correct code enables more accurate estimation of functional correctness for ranked lists of generated code, supporting better selection in generative software development workflows."}}
{"id": "2507.05245", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05245", "abs": "https://arxiv.org/abs/2507.05245", "authors": ["Fatema Tuz Zohra", "Brittany Johnson"], "title": "An Investigation into Maintenance Support for Neural Networks", "comment": "Revised version accepted at the HumanAISE Workshop, co-located with\n  FSE 2025", "summary": "As the potential for neural networks to augment our daily lives grows,\nensuring their quality through effective testing, debugging, and maintenance is\nessential. This is especially the case as we acknowledge the prospects of\nnegative impacts from these technologies. Traditional software engineering\nmethods, such as testing and debugging, have proven effective in maintaining\nsoftware quality; however, they reveal significant research and practice gaps\nin maintaining neural networks. In particular, there is a limited understanding\nof how practitioners currently address challenges related to understanding and\nmitigating undesirable behaviors in neural networks. In our ongoing research,\nwe explore the current state of research and practice in maintaining neural\nnetworks by curating insights from practitioners through a preliminary study\ninvolving interviews and supporting survey responses. Our findings thus far\nindicate that existing tools primarily concentrate on building and training\nmodels. While these tools can be beneficial, they often fall short of\nsupporting practitioners' understanding and addressing the underlying causes of\nunexpected model behavior. By evaluating current procedures and identifying the\nlimitations of traditional methodologies, our study aims to offer a\ndeveloper-centric perspective on where current practices fall short and\nhighlight opportunities for improving maintenance support in neural networks.", "AI": {"tldr": "The paper identifies gaps in current neural network maintenance tools and practices, mainly that they focus more on development than on understanding and resolving problematic behaviors, suggesting a need for better support tools.", "motivation": "Neural networks are increasingly integrated into daily life, raising the need for effective testing, debugging, and maintenance due to their potential negative impacts. Traditional software engineering methods have gaps when applied to neural networks, particularly in addressing undesirable behaviors. There is limited knowledge about how practitioners manage these challenges.", "method": "The study uses qualitative research involving interviews and survey responses from practitioners to understand their experiences and challenges in maintaining neural networks.", "result": "Findings show that while current tools are focused on building and training neural network models, they inadequately support practitioners in understanding and addressing the root causes of unexpected or undesirable model behaviors.", "conclusion": "Traditional methodologies and existing tools fall short in supporting neural network maintenance. A developer-centric reassessment is needed, and new tools and methods must be developed to address current shortcomings and improve support for neural network maintenance."}}
