<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 8]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [ChatGPT for Code Refactoring: Analyzing Topics, Interaction, and Effective Prompts](https://arxiv.org/abs/2509.08090)
*Eman Abdullah AlOmar,Luo Xu,Sofia Martinez,Anthony Peruma,Mohamed Wiem Mkaouer,Christian D. Newman,Ali Ouni*

Main category: cs.SE

TL;DR: The paper investigates how developers communicate their refactoring needs to ChatGPT, analyzing 715 interactions from nearly 30K records. They use text mining and intention analysis, yielding insights into developer behavior and LLM response, potentially guiding future tooling improvements.


<details>
  <summary>Details</summary>
Motivation: There is a lack of knowledge about how developers communicate their refactoring needs when interacting with large language models (LLMs) like ChatGPT. Previous research has focused on the performance of LLMs in refactoring, but not on the way developers express their requirements or intentions.

Method: The authors use text mining techniques to analyze 715 refactoring-related interactions extracted from a dataset of 29,778 ChatGPT prompts and responses. They also examine the explicit intentions of developers regarding refactoring within these interactions.

Result: The study provides insight into how developers identify code areas for improvement and communicate these needs to ChatGPT, as well as how ChatGPT responds to these expressed requirements.

Conclusion: This research enhances the understanding of developer-ChatGPT interactions for code refactoring, which can inform improvements in LLMs and their interfaces to better support developer needs during refactoring tasks.

Abstract: Large Language Models (LLMs), such as ChatGPT, have become widely popular and
widely used in various software engineering tasks such as refactoring, testing,
code review, and program comprehension. Although recent studies have examined
the effectiveness of LLMs in recommending and suggesting refactoring, there is
a limited understanding of how developers express their refactoring needs when
interacting with ChatGPT. In this paper, our goal is to explore interactions
related to refactoring between developers and ChatGPT to better understand how
developers identify areas for improvement in code, and how ChatGPT addresses
developers' needs. Our approach involves text mining 715 refactoring-related
interactions from 29,778 ChatGPT prompts and responses, as well as the analysis
of developers' explicit refactoring intentions.

</details>


### [2] [Safety Factories -- a Manifesto](https://arxiv.org/abs/2509.08285)
*Carmen Cârlan,Daniel Ratiu,Michael Wagner*

Main category: cs.SE

TL;DR: The paper argues for integrating automated, rigorous safety engineering ('safety factories') into modern software development pipelines to better support the rapid and safe evolution of cyber-physical systems.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the increasing complexity and criticality of software in cyber-physical systems, which demands improved rigor and automation in safety engineering to match the pace of rapid software development.

Method: The paper proposes the use of 'safety factories'—integrating safety engineering methods and tooling into software development pipelines. It also suggests adopting formal, machine-processable models for safety outputs, automating consistency checks, and documentation.

Result: The result is an advocacy for bridging the current gap between software development and safety engineering by transferring best practices from the former to the latter, aiming for more efficient and reliable safety processes embedded within modern development workflows.

Conclusion: The paper concludes that integrating safety engineering directly into software pipelines, through automation and formal models, can help keep up with fast software evolution and improve safety assurance in safety-critical systems.

Abstract: Modern cyber-physical systems are operated by complex software that
increasingly takes over safety-critical functions. Software enables rapid
iterations and continuous delivery of new functionality that meets the
ever-changing expectations of users. As high-speed development requires
discipline, rigor, and automation, software factories are used. These entail
methods and tools used for software development, such as build systems and
pipelines. To keep up with the rapid evolution of software, we need to bridge
the disconnect in methods and tools between software development and safety
engineering today. We need to invest more in formality upfront - capturing
safety work products in semantically rich models that are machine-processable,
defining automatic consistency checks, and automating the generation of
documentation - to benefit later. Transferring best practices from software to
safety engineering is worth exploring. We advocate for safety factories, which
integrate safety tooling and methods into software development pipelines.

</details>


### [3] [The Impact of Team Diversity in Agile Development Education](https://arxiv.org/abs/2509.08389)
*Marco Torchiano,Riccardo Coppola,Antonio Vetro',Xhoi Musaj*

Main category: cs.SE

TL;DR: Gender diversity in agile student teams improves project success, while nationality diversity has little effect. Combined, they may introduce communication challenges, but overall, diversity promotion does not harm team performance in project-based software engineering courses.


<details>
  <summary>Details</summary>
Motivation: Software engineering is a male-dominated field, and gender diversity has been highlighted as essential for equality, productivity, and innovation. However, other forms of diversity, such as nationality and ethnicity, have been understudied. This paper seeks to fill that gap by examining the impact of gender and nationality diversity (and their interaction) on team project outcomes in an educational context.

Method: Researchers analyzed 51 teams over three years in an agile software development project-based course. They computed three Diversity indexes (Gender, Nationality, and their co-presence) and used statistical analysis to assess how these factors affect team performance.

Result: The study found a moderate, statistically significant link between gender diversity and project success. Nationality diversity had a negative but negligible effect on outcomes. However, when gender and nationality diversity were both present, team performance slightly decreased, likely due to increased communication barriers and differing cultural norms.

Conclusion: Promoting diversity (gender and nationality) in student teams does not harm educational performance or achievement, but their co-presence may cause communication challenges. Multiple diversity dimensions and their interactions should be considered in educational team formation.

Abstract: Software Engineering is mostly a male-dominated sector, where gender
diversity is a key feature for improving equality of opportunities,
productivity, and innovation. Other diversity aspects, including but not
limited to nationality and ethnicity, are often understudied.In this work we
aim to assess the impact of team diversity, focusing mainly on gender and
nationality, in the context of an agile software development project-based
course. We analyzed 51 teams over three academic years, measuring three
different Diversity indexes - regarding Gender, Nationality and their
co-presence - to examine how different aspects of diversity impact the quality
of team project outcomes.Statistical analysis revealed a moderate,
statistically significant correlation between gender diversity and project
success, aligning with existing literature. Diversity in nationality showed a
negative but negligible effect on project results, indicating that promoting
these aspects does not harm students' performance. Analyzing their co-presence
within a team, gender and nationality combined had a negative impact, likely
due to increased communication barriers and differing cultural norms.This study
underscores the importance of considering multiple diversity dimensions and
their interactions in educational settings. Our findings, overall, show that
promoting diversity in teams does not negatively impact their performance and
achievement of educational goals.

</details>


### [4] [AutoStub: Genetic Programming-Based Stub Creation for Symbolic Execution](https://arxiv.org/abs/2509.08524)
*Felix Mächtle,Nils Loose,Jan-Niclas Serr,Jonas Sander,Thomas Eisenbarth*

Main category: cs.SE

TL;DR: AutoStub automatically creates symbolic stubs for external functions using Genetic Programming, enabling symbolic execution to handle previously intractable code and improving automated software testing effectiveness.


<details>
  <summary>Details</summary>
Motivation: Symbolic execution is limited by its inability to handle external functions, often requiring costly, manual, or context-dependent solutions that reduce automation and scalability in software testing.

Method: The method involves detecting when symbolic execution encounters external functions, generating training data via random inputs/outputs, and using Genetic Programming to synthesize expressions that serve as symbolic stubs approximating the function's behavior.

Result: AutoStub achieves over 90% accuracy for 55% of the external functions evaluated, and can infer nuanced behaviors that help uncover critical edge cases.

Conclusion: AutoStub, which uses Genetic Programming to automatically generate symbolic stubs, enables symbolic execution to analyze programs with external functions without manual intervention. This advances path coverage and reveals important edge cases for software testing.

Abstract: Symbolic execution is a powerful technique for software testing, but suffers
from limitations when encountering external functions, such as native methods
or third-party libraries. Existing solutions often require additional context,
expensive SMT solvers, or manual intervention to approximate these functions
through symbolic stubs. In this work, we propose a novel approach to
automatically generate symbolic stubs for external functions during symbolic
execution that leverages Genetic Programming. When the symbolic executor
encounters an external function, AutoStub generates training data by executing
the function on randomly generated inputs and collecting the outputs. Genetic
Programming then derives expressions that approximate the behavior of the
function, serving as symbolic stubs. These automatically generated stubs allow
the symbolic executor to continue the analysis without manual intervention,
enabling the exploration of program paths that were previously intractable. We
demonstrate that AutoStub can automatically approximate external functions with
over 90% accuracy for 55% of the functions evaluated, and can infer
language-specific behaviors that reveal edge cases crucial for software
testing.

</details>


### [5] [Beyond the Binary: The System of All-round Evaluation of Research and Its Practices in China](https://arxiv.org/abs/2509.08546)
*Yu Zhu,Jiyuan Ye*

Main category: cs.SE

TL;DR: The paper introduces the SAER framework to transcend the qualitative/quantitative divide in research evaluation, proposing a holistic, multi-dimensional system influenced by Chinese theory, aimed at guiding global reforms.


<details>
  <summary>Details</summary>
Motivation: There is no macro-level, systematic evaluation theory to guide evaluation practices, hindering global research evaluation system reforms. Current approaches are polarized between qualitative and quantitative methods.

Method: Review of historical research evaluation development and introduction of the System of All-round Evaluation of Research (SAER) framework, which integrates form, content, and utility evaluations within three dimensions and six elements.

Result: The SAER framework offers a comprehensive and dialectical system for research evaluation, moving beyond the binary qualitative/quantitative split and incorporating Chinese evaluation theory to enrich global practices.

Conclusion: SAER provides a theoretical breakthrough for global research evaluation reform, helping reconcile oppositional evaluation methods and offering valuable references for international systems.

Abstract: The lack of a macro-level, systematic evaluation theory to guide the
implementation of evaluation practices has become a key bottleneck in the
reform of global research evaluation systems. By reviewing the historical
development of research evaluation, this paper highlights the current binary
opposition between qualitative and quantitative methods in evaluation
practices. This paper introduces the System of All-round Evaluation of Research
(SAER), a framework that integrates form, content, and utility evaluations with
six key elements. SAER offers a theoretical breakthrough by transcending the
binary, providing a comprehensive foundation for global evaluation reforms. The
comprehensive system proposes a trinity of three evaluation dimensions,
combined with six evaluation elements, which would help academic evaluators and
researchers reconcile binary oppositions in evaluation methods. The system
highlights the dialectical wisdom and experience embedded in Chinese research
evaluation theory, offering valuable insights and references for the reform and
advancement of global research evaluation systems.

</details>


### [6] [Minimal Data, Maximum Clarity: A Heuristic for Explaining Optimization](https://arxiv.org/abs/2509.08667)
*Amirali Rayegan,Tim Menzies*

Main category: cs.SE

TL;DR: EZR is a new, efficient framework for software optimization that uses active learning and decision trees to achieve strong results with fewer labels and provides very clear explanations, outperforming standard explainable AI tools.


<details>
  <summary>Details</summary>
Motivation: Software engineering faces major challenges with huge configuration spaces and expensive/unstable labeling processes, making efficient, interpretable optimization difficult.

Method: The paper proposes EZR, a modular framework combining active sampling, learning (via Naive Bayes), and explanation (via decision trees) into a single pipeline. It introduces the Maximum Clarity Heuristic to use less but more informative data for building understandable and effective optimization models.

Result: EZR achieves over 90% of the best-known optimization results across 60 datasets, using significantly fewer labeled examples. Its explanations, provided by decision trees, are clearer and more actionable than standard XAI methods like LIME, SHAP, and BreakDown.

Conclusion: Using less but more informative data can yield highly label-efficient, interpretable, and effective optimization for software systems, outperforming traditional methods both in accuracy and clarity of explanation. All materials are made publicly available for reproducibility.

Abstract: Efficient, interpretable optimization is a critical but underexplored
challenge in software engineering, where practitioners routinely face vast
configuration spaces and costly, error-prone labeling processes. This paper
introduces EZR, a novel and modular framework for multi-objective optimization
that unifies active sampling, learning, and explanation within a single,
lightweight pipeline. Departing from conventional wisdom, our Maximum Clarity
Heuristic demonstrates that using less (but more informative) data can yield
optimization models that are both effective and deeply understandable. EZR
employs an active learning strategy based on Naive Bayes sampling to
efficiently identify high-quality configurations with a fraction of the labels
required by fully supervised approaches. It then distills optimization logic
into concise decision trees, offering transparent, actionable explanations for
both global and local decision-making. Extensive experiments across 60
real-world datasets establish that EZR reliably achieves over 90% of the
best-known optimization performance in most cases, while providing clear,
cohort-based rationales that surpass standard attribution-based explainable AI
(XAI) methods (LIME, SHAP, BreakDown) in clarity and utility. These results
endorse "less but better"; it is both possible and often preferable to use
fewer (but more informative) examples to generate label-efficient optimization
and explanations in software systems. To support transparency and
reproducibility, all code and experimental materials are publicly available at
https://github.com/amiiralii/Minimal-Data-Maximum-Clarity.

</details>


### [7] [SWE-Mirror: Scaling Issue-Resolving Datasets by Mirroring Issues Across Repositories](https://arxiv.org/abs/2509.08724)
*Junhao Wang,Daoguang Zan,Shulin Xin,Siyao Liu,Yurong Wu,Kai Shen*

Main category: cs.SE

TL;DR: The paper presents SWE-Mirror, a pipeline designed to create large and authentic datasets for issue-resolving tasks by mirroring real GitHub issues into Gym environments. The approach produces high-quality training data, significantly improves coding agents' issue-resolving performance, and sets new SOTA on established benchmarks.


<details>
  <summary>Details</summary>
Motivation: Creating large-scale, verifiable training datasets for issue-resolving tasks is hard; existing methods either have low success, high overhead, or miss authentic, real issues. There's a need to maximize use of both existing Gym environments and real GitHub issue data.

Method: The paper introduces SWE-Mirror, a pipeline that extracts the essence of real-world issues, mirrors them into repositories with Gym environments, and reanimates them as verifiable tasks. This reuses Gym setups and authentic GitHub issue history to create a large, scalable, authentic dataset.

Result: Using SWE-Mirror, the authors curated a dataset of 60,671 issue-resolving tasks across 40 repositories and 4 languages. The dataset improved the training and effectiveness of coding agents. Extending the dataset to 12,000+ high-quality trajectories led to significant improvements in issue-resolve rates (+21.8% for 7B, +46.0% for 32B) for Qwen2.5-Coder-Instruct LLMs on the OpenHands agent framework.

Conclusion: SWE-Mirror enables the creation of large-scale, verifiable datasets of real issue-resolving tasks, showing clear benefits in training coding agents and achieving new state-of-the-art results on issue resolution benchmarks.

Abstract: Creating large-scale verifiable training datasets for issue-resolving tasks
is a critical yet notoriously difficult challenge. Existing methods on
automating the Gym environment setup process for real-world issues suffer from
low success rates and high overhead. Meanwhile, synthesizing new tasks within
existing Gym environments leaves the vast pool of authentic, human-reported
problems untapped. To maximize the utilization of existing Gym environments and
also the rich data of issue-resolving history on GitHub, we introduce
SWE-Mirror, a pipeline that distills a real-world issue's semantic essence,
mirrors it into another repository with a configured Gym environment, and
re-animates it as a verifiable issue-resolving task. SWE-Mirror reuses existing
Gym environments along with the vast pool of issue-resolving history hosted on
GitHub to construct a large-scale dataset of mirrored authentic and verifiable
tasks. Applying SWE-Mirror to 40 repositories across 4 languages, we have
curated a dataset with 60,671 issue-resolving tasks and demonstrated the value
of our dataset by training and evaluating coding agents at various scale.
Post-training experiments show that models trained with the dataset exhibit
improvements in issue-resolving capabilities. Furthermore, by extending the
dataset size to over 12,000 high-quality trajectories, we established a new
state-of-the-art (SOTA) among Qwen2.5-Coder-Instruct based LLMs on the
OpenHands agent framework, which increases the resolve rate on
SWE-Bench-Verified by +21.8% for the 7B model and +46.0% for the 32B model and
validates the effectiveness of our approach.

</details>


### [8] [Handling Open-Vocabulary Constructs in Formalizing Specifications: Retrieval-Augmented Parsing with Expert Knowledge](https://arxiv.org/abs/2509.08808)
*Mohammad Saqib Hasan,Sayontan Ghosh,Dhruv Verma,Geoff Kuenning,Erez Zadok,Scott A. Smolka,Niranjan Balasubramanian*

Main category: cs.SE

TL;DR: This paper tackles open-vocabulary constructs in natural language to formal language parsing by introducing a dynamic knowledge-augmented approach (ROLex). Models benefit from expert knowledge added at inference time, without retraining, leading to notable performance improvements across multiple tasks.


<details>
  <summary>Details</summary>
Motivation: Models converting NL specifications to formal languages struggle with open-vocabulary constructs that are not known during training. Utilizing dynamic, inference-time expert contributions without retraining is desirable for accuracy and adaptability.

Method: The paper introduces dynamic knowledge-augmented parsing (DKAP), using a retrieval-augmented parser called ROLex. Expert-provided constructs are stored in a key-value lexicon, which grows at inference time. The approach combines a retriever and generator trained via synthetic and augmented data, and proposes strategies to focus on relevant retrieved knowledge.

Result: ROLex leverages dynamic expert knowledge effectively, improving baseline models' performance on three formalization tasks: NL2LTL, NL2Code, and NL2CMD. The evaluations demonstrate the difficulty of DKAP, but also show the effectiveness of the proposed method.

Conclusion: DKAP is a challenging problem for existing models, but the proposed ROLex approach, which incorporates dynamic expert knowledge, significantly improves parsing performance across different formalization tasks.

Abstract: We study the problem of Open-Vocabulary Constructs(OVCs) -- ones not known
beforehand -- in the context of converting natural language (NL) specifications
into formal languages (e.g., temporal logic or code). Models fare poorly on
OVCs due to a lack of necessary knowledge a priori. In such situations, a
domain expert can provide correct constructs at inference time based on their
preferences or domain knowledge. Our goal is to effectively reuse this
inference-time, expert-provided knowledge for future parses without retraining
the model. We present dynamic knowledge-augmented parsing(DKAP), where in
addition to the input sentence, the model receives (dynamically growing) expert
knowledge as a key-value lexicon that associates NL phrases with correct OVC
constructs. We propose ROLex, a retrieval-augmented parsing approach that uses
this lexicon. A retriever and a generator are trained to find and use the
key-value store to produce the correct parse. A key challenge lies in curating
data for this retrieval-augmented parser. We utilize synthetic data generation
and the data augmentation techniques on annotated (NL sentence, FL statement)
pairs to train the augmented parser. To improve training effectiveness, we
propose multiple strategies to teach models to focus on the relevant subset of
retrieved knowledge. Finally, we introduce a new evaluation paradigm modeled
after the DKAP problem and simulate the scenario across three formalization
tasks (NL2LTL, NL2Code, and NL2CMD). Our evaluations show that DKAP is a
difficult challenge, and ROLex helps improve the performance of baseline models
by using dynamic expert knowledge effectively.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [9] [XML Prompting as Grammar-Constrained Interaction: Fixed-Point Semantics, Convergence Guarantees, and Human-AI Protocols](https://arxiv.org/abs/2509.08182)
*Faruk Alpay,Taylan Alpay*

Main category: cs.PL

TL;DR: This paper provides a rigorous logical framework for steering LLMs with XML-structured prompts, ensuring well-formed outputs and protocol convergence. It formalizes grammar-constrained decoding and multi-stage human-AI interactions, presenting proofs and practical recipes that unify and extend current deployment methods.


<details>
  <summary>Details</summary>
Motivation: Structured prompting using XML tags is widely used in real-world systems to guide large language models (LLMs) to generate outputs that adhere to specific schemas. There is a need for formal frameworks that unify and analyze various approaches to XML-based prompting, especially to understand their theoretical properties and practical deployment.

Method: The paper introduces a logic-first framework for XML prompting that incorporates grammar-constrained decoding, fixed-point semantics on hierarchical prompt lattices, and convergent human-AI interaction models. Mathematical formalisms such as refinement orders, Knaster-Tarski fixed points, and Banach-style convergence are applied to XML trees. The framework is instantiated using context-free grammars for XML schemas, and practical deployments are demonstrated via multi-layer human-AI interaction recipes.

Result: The authors prove that their framework ensures well-formed, schema-adherent outputs from LLMs and maintains task performance. They mathematically demonstrate convergence and fixed-point properties for prompt iterations and provide practical recipes for multi-stage human-AI interaction and agentic tool use. Their results unify recent advances in the field, showing how grammar-constrained decoding and verification protocols benefit XML-based LLM steering.

Conclusion: The logic-first, mathematically complete approach to XML prompting delivers both theoretical guarantees of output adherence and practical schemes for robust, schema-controlled LLM interactions, supporting advanced deployment patterns.

Abstract: Structured prompting with XML tags has emerged as an effective way to steer
large language models (LLMs) toward parseable, schema-adherent outputs in
real-world systems. We develop a logic-first treatment of XML prompting that
unifies (i) grammar-constrained decoding, (ii) fixed-point semantics over
lattices of hierarchical prompts, and (iii) convergent human-AI interaction
loops. We formalize a complete lattice of XML trees under a refinement order
and prove that monotone prompt-to-prompt operators admit least fixed points
(Knaster-Tarski) that characterize steady-state protocols; under a task-aware
contraction metric on trees, we further prove Banach-style convergence of
iterative guidance. We instantiate these results with context-free grammars
(CFGs) for XML schemas and show how constrained decoding guarantees
well-formedness while preserving task performance. A set of multi-layer
human-AI interaction recipes demonstrates practical deployment patterns,
including multi-pass "plan $\to$ verify $\to$ revise" routines and agentic tool
use. We provide mathematically complete proofs and tie our framework to recent
advances in grammar-aligned decoding, chain-of-verification, and programmatic
prompting.

</details>
