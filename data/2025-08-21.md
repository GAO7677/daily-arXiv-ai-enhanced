<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [You Don't Know Until You Click:Automated GUI Testing for Production-Ready Software Evaluation](https://arxiv.org/abs/2508.14104)
*Yutong Bian,Xianhao Lin,Yupeng Xie,Tianyang Liu,Mingchen Zhuge,Siyuan Lu,Haoming Tang,Jinlin Wang,Jiayi Zhang,Jiaqi Chen,Xiangru Tang,Yongxin Ni,Sirui Hong,Chenglin Wu*

Main category: cs.SE

TL;DR: RealDevWorld introduces an automated, agent-based framework for evaluating production-ready software created by LLMs, moving beyond static checks to real interaction-based assessment. It provides nuanced, human-aligned feedback and achieves high accuracy and correlation with expert reviews, enabling scalable and realistic evaluation of software development tasks.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for evaluating code agents and LLMs in software development do not adequately measure the interactive and runtime qualities essential for production-ready applications. These benchmarks typically use static or binary checks, which overlook dynamic behaviors and usability that emerge during real user interactions.

Method: The authors propose RealDevWorld, a novel evaluation framework consisting of two components: RealDevBench, which includes 194 diverse software engineering tasks with multimodal challenges; and AppEvalPilot, an agent-based system that interacts with GUIs to assess applications for functional correctness, visual fidelity, and runtime behavior. This framework provides automatic, fine-grained, and holistic evaluation that closely mimics real-world use.

Result: RealDevWorld achieves a high accuracy of 0.92 and strong correlation (0.85) with human expert assessments, substantially reducing the need for manual review. Its evaluation results are more nuanced and human-aligned, enabling scalable assessment of production-level software from LLMs.

Conclusion: The RealDevWorld framework fills a critical gap in the evaluation of code agents and LLMs by automatically and effectively assessing the real-world usability of generated software applications, making the evaluation process more robust, scalable, and aligned with expert judgments.

Abstract: Large Language Models (LLMs) and code agents in software development are
rapidly evolving from generating isolated code snippets to producing
full-fledged software applications with graphical interfaces, interactive
logic, and dynamic behaviors. However, current benchmarks fall short in
evaluating such production-ready software, as they often rely on static checks
or binary pass/fail scripts, failing to capture the interactive behaviors and
runtime dynamics that define real-world usability - qualities that only emerge
when an application is actively used. This is the blind spot of current
evaluation: you don't know if an app works until you click through it, interact
with it, and observe how it responds. To bridge this gap, we introduce
RealDevWorld, a novel evaluation framework for automated end-to-end assessment
of LLMs' ability to generate production-ready repositories from scratch. It
features two key components: (1) RealDevBench, a diverse collection of 194
open-ended software engineering tasks across multiple domains, incorporating
multimodal elements to reflect real-world complexity; and (2) AppEvalPilot, a
new agent-as-a-judge evaluation system that simulates realistic, GUI-based user
interactions to automatically and holistically assess software functional
correctness, visual fidelity, and runtime behavior. The framework delivers
fine-grained, task-specific diagnostic feedback, supporting nuanced evaluation
beyond simple success/failure judgments. Empirical results show that
RealDevWorld delivers effective, automatic, and human-aligned evaluations,
achieving an accuracy of 0.92 and a correlation of 0.85 with expert human
assessments, while significantly reducing the reliance on manual review. This
enables scalable, human-aligned assessment of production-level software
generated by LLMs. Our code is available on GitHub.

</details>


### [2] [Ambiguity Resolution with Human Feedback for Code Writing Tasks](https://arxiv.org/abs/2508.14114)
*Aditey Nandan,Viraj Kumar*

Main category: cs.SE

TL;DR: The paper presents a system that detects ambiguities in code task specifications, collects human feedback, and generates clarified code. Evaluation shows it improves ambiguity resolution, benefiting both programming and CS education.


<details>
  <summary>Details</summary>
Motivation: Code writing specifications in natural language are often ambiguous, making it difficult for programmers to understand task requirements. Resolving such ambiguities is crucial for accurate code development.

Method: The paper introduces a prototype system using ARHF (Ambiguity Resolution with Human Feedback), which identifies ambiguous inputs in task specifications, collects human feedback on desired code behavior, and generates code that reflects this clarified understanding.

Result: The prototype system was evaluated for its effectiveness in resolving specification ambiguities. The implications of these kinds of assistive systems for enhancing Computer Science education were also discussed.

Conclusion: The ARHF-based prototype demonstrates that human-in-the-loop feedback can effectively clarify ambiguities in code writing tasks expressed in natural language, potentially improving programmer accuracy and educational outcomes.

Abstract: Specifications for code writing tasks are usually expressed in natural
language and may be ambiguous. Programmers must therefore develop the ability
to recognize ambiguities in task specifications and resolve them by asking
clarifying questions. We present and evaluate a prototype system, based on a
novel technique (ARHF: Ambiguity Resolution with Human Feedback), that (1)
suggests specific inputs on which a given task specification may be ambiguous,
(2) seeks limited human feedback about the code's desired behavior on those
inputs, and (3) uses this feedback to generate code that resolves these
ambiguities. We evaluate the efficacy of our prototype, and we discuss the
implications of such assistive systems on Computer Science education.

</details>


### [3] [Measuring LLM Code Generation Stability via Structural Entropy](https://arxiv.org/abs/2508.14288)
*Yewei Song,Tiezhu Sun,Xunzhu Tang,Prateek Rajput,Tegawende F. Bissyande,Jacques Klein*

Main category: cs.SE

TL;DR: This paper proposes new metrics—based on AST structural entropy—to evaluate the stability of code generated by large language models. These metrics require no reference solutions or code execution and effectively reveal differences in model reliability, offering a simple yet robust addition to code generation evaluation.


<details>
  <summary>Details</summary>
Motivation: Evaluating the reliability of code generation from large language models is crucial for their safe adoption in real software development. Existing metrics either require reference solutions, are language-specific, or depend on code execution, which can be costly or infeasible.

Method: The authors introduce AST-driven structural entropy metrics by analyzing the depth-bounded subtrees of abstract syntax trees (ASTs) generated from multiple code outputs for a fixed prompt. They use Jensen-Shannon divergence and a Structural Cross-Entropy ratio to quantify stability. These are computed in both structural (control-flow) and token-aware (detail-level) variants, and are independent of reference solutions, programming language, or code execution.

Result: The authors show that their proposed method reveals subtle differences in code generation consistency and robustness among leading LLMs. Their benchmarks highlight the method's ability to discern model reliability where traditional metrics may not.

Conclusion: The AST-driven structural entropy metrics provide a lightweight, execution-free, and reference-free way to assess the stability and reliability of code-generating LLMs. This approach adds a valuable perspective to existing evaluation tools and better informs the practical deployment of LLMs for coding tasks.

Abstract: Assessing the stability of code generation from large language models (LLMs)
is essential for judging their reliability in real-world development. We extend
prior "structural-entropy concepts" to the program domain by pairing entropy
with abstract syntax tree (AST) analysis. For any fixed prompt, we collect the
multiset of depth-bounded subtrees of AST in each generated program and treat
their relative frequencies as a probability distribution. We then measure
stability in two complementary ways: (i) Jensen-Shannon divergence, a
symmetric, bounded indicator of structural overlap, and (ii) a Structural
Cross-Entropy ratio that highlights missing high-probability patterns. Both
metrics admit structural-only and token-aware variants, enabling separate views
on control-flow shape and identifier-level variability. Unlike pass@k, BLEU, or
CodeBLEU, our metrics are reference-free, language-agnostic, and
execution-independent. We benchmark several leading LLMs on standard code
generation tasks, demonstrating that AST-driven structural entropy reveals
nuances in model consistency and robustness. The method runs in O(n,d) time
with no external tests, providing a lightweight addition to the code-generation
evaluation toolkit.

</details>


### [4] [Static Analysis as a Feedback Loop: Enhancing LLM-Generated Code Beyond Correctness](https://arxiv.org/abs/2508.14419)
*Scott Blyth,Sherlock A. Licorish,Christoph Treude,Markus Wagner*

Main category: cs.SE

TL;DR: The paper shows that by using iterative static analysis feedback with tools like Bandit and Pylint, LLMs such as GPT-4o can drastically reduce code quality issues, going beyond just functional correctness to produce significantly safer, more reliable, and readable code.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for code generation by LLMs focus mostly on functional correctness, ignoring other key aspects of code quality such as security, reliability, readability, and maintainability. This creates a gap in comprehensive code evaluation.

Method: The paper uses the PythonSecurityEval benchmark to evaluate LLM-generated code across multiple quality dimensions. It proposes an iterative, static analysis-driven prompting algorithm that employs tools like Bandit and Pylint to detect and address code quality issues during code generation.

Result: Experiments with GPT-4o show significant improvements after applying the proposed method: security issues drop from over 40% to 13%, readability violations from over 80% to 11%, and reliability warnings from over 50% to 11% within ten iterations.

Conclusion: Integrating static analysis feedback into LLM code generation enables these models to produce code that is not only functionally correct but also high quality across security, reliability, and readability dimensions.

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
code generation, achieving high scores on benchmarks such as HumanEval and
MBPP. However, these benchmarks primarily assess functional correctness and
neglect broader dimensions of code quality, including security, reliability,
readability, and maintainability. In this work, we systematically evaluate the
ability of LLMs to generate high-quality code across multiple dimensions using
the PythonSecurityEval benchmark. We introduce an iterative static
analysis-driven prompting algorithm that leverages Bandit and Pylint to
identify and resolve code quality issues. Our experiments with GPT-4o show
substantial improvements: security issues reduced from >40% to 13%, readability
violations from >80% to 11%, and reliability warnings from >50% to 11% within
ten iterations. These results demonstrate that LLMs, when guided by static
analysis feedback, can significantly enhance code quality beyond functional
correctness.

</details>


### [5] [Design and Evaluation of a Scalable Data Pipeline for AI-Driven Air Quality Monitoring in Low-Resource Settings](https://arxiv.org/abs/2508.14451)
*Richard Sserujongi,Daniel Ogenrwot,Nicholas Niwamanya,Noah Nsimbe,Martin Bbaale,Benjamin Ssempala,Noble Mutabazi,Raja Fidel Wabinyai,Deo Okure,Engineer Bainomugisha*

Main category: cs.SE

TL;DR: AirQo presents a robust, open-source air quality data system combining real-time and batch processing in Africa, achieving high performance under challenging conditions, and offers a replicable model for similar efforts in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: There is an increased demand for scalable and resilient environmental data systems, especially in data-scarce and resource-constrained areas, due to the growing adoption of low-cost sensors and AI applications.

Method: The paper details the design, implementation, and evaluation of the AirQo data pipeline—a modular, cloud-native ETL system using open-source technologies (Apache Airflow, Kafka, BigQuery). It integrates data from sensors and APIs, enabling automated calibration, forecasting, and analytics, and is designed to handle both real-time and batch air quality data processing.

Result: The pipeline successfully ingests, transforms, and distributes millions of air quality measurements each month from over 400 devices, maintaining low latency, high throughput, and strong data availability even in environments with limited power and connectivity. Performance across key operational and architectural metrics is demonstrated.

Conclusion: The open-source AirQo data pipeline provides a practical, scalable blueprint for building sustainable air quality monitoring platforms in resource-limited regions, contributing valuable methods and lessons for similar environmental intelligence initiatives.

Abstract: The increasing adoption of low-cost environmental sensors and AI-enabled
applications has accelerated the demand for scalable and resilient data
infrastructures, particularly in data-scarce and resource-constrained regions.
This paper presents the design, implementation, and evaluation of the AirQo
data pipeline: a modular, cloud-native Extract-Transform-Load (ETL) system
engineered to support both real-time and batch processing of heterogeneous air
quality data across urban deployments in Africa. It is Built using open-source
technologies such as Apache Airflow, Apache Kafka, and Google BigQuery. The
pipeline integrates diverse data streams from low-cost sensors, third-party
weather APIs, and reference-grade monitors to enable automated calibration,
forecasting, and accessible analytics. We demonstrate the pipeline's ability to
ingest, transform, and distribute millions of air quality measurements monthly
from over 400 monitoring devices while achieving low latency, high throughput,
and robust data availability, even under constrained power and connectivity
conditions. The paper details key architectural features, including workflow
orchestration, decoupled ingestion layers, machine learning-driven sensor
calibration, and observability frameworks. Performance is evaluated across
operational metrics such as resource utilization, ingestion throughput,
calibration accuracy, and data availability, offering practical insights into
building sustainable environmental data platforms. By open-sourcing the
platform and documenting deployment experiences, this work contributes a
reusable blueprint for similar initiatives seeking to advance environmental
intelligence through data engineering in low-resource settings.

</details>


### [6] [What You See Is What It Does: A Structural Pattern for Legible Software](https://arxiv.org/abs/2508.14511)
*Eagon Meng,Daniel Jackson*

Main category: cs.SE

TL;DR: Current software is hard to understand and maintain. The paper introduces a new concept-synchronization pattern and DSL for better modularity and legibility, making code easier to generate and manage, especially for LLMs. RealWorld benchmark shows its effectiveness.


<details>
  <summary>Details</summary>
Motivation: Current software structures are often hard to understand and modify due to a lack of legibility and modularity. LLMs present new opportunities but also limitations, which require reevaluating conventional software design.

Method: The paper proposes a new structural pattern based on concepts (independent services) and synchronizations (event-based rules), alongside a domain-specific language for expressing these synchronizations. The approach is illustrated and evaluated using the RealWorld benchmark case study.

Result: The proposed pattern improves the legibility and modularity of software. Behavioral features become easier to express granularly and declaratively, and can be readily generated by LLMs, addressing issues of incrementality, integrity, and transparency.

Conclusion: Adopting concept-synchronization patterns and a synchronization DSL offers a robust way to build software that alleviates the shortcomings of current practices, particularly when leveraging LLMs for code generation and maintenance.

Abstract: The opportunities offered by LLM coders (and their current limitations)
demand a reevaluation of how software is structured. Software today is often
"illegible" - lacking a direct correspondence between code and observed
behavior - and insufficiently modular, leading to a failure of three key
requirements of robust coding: incrementality (the ability to deliver small
increments by making localized changes), integrity (avoiding breaking prior
increments) and transparency (making clear what has changed at build time, and
what actions have happened at runtime).
  A new structural pattern offers improved legibility and modularity. Its
elements are concepts and synchronizations: fully independent services and
event-based rules that mediate between them. A domain-specific language for
synchronizations allows behavioral features to be expressed in a granular and
declarative way (and thus readily generated by an LLM). A case study of the
RealWorld benchmark is used to illustrate and evaluate the approach.

</details>


### [7] [Preguss: It Analyzes, It Specifies, It Verifies](https://arxiv.org/abs/2508.14532)
*Zhongyi Wang,Tengjie Lin,Mingshuai Chen,Mingqi Yang,Haokun Li,Xiao Yi,Shengchao Qin,Jianwei Yin*

Main category: cs.SE

TL;DR: Fully automated verification of big, complex systems is tough. Preguss combines static analysis and LLMs to generate, refine, and prioritize verification tasks, offering a scalable pathway to improved automation in formal methods.


<details>
  <summary>Details</summary>
Motivation: Automated verification of large-scale software and hardware systems remains a challenging goal, mainly due to limitations in current methods, such as large language models, which struggle with scalability and complex specification inference.

Method: The paper proposes Preguss, a modular framework combining static analysis and deductive verification. It uses RTE-guided identification and prioritization of units for verification, together with LLM-powered synthesis of interprocedural specifications at the unit level.

Result: Preguss enables a more scalable and automated approach to formal specification generation and refinement, leveraging strengths of both static analysis and LLMs.

Conclusion: Preguss presents a promising direction towards fully automated verification of large, complex software systems by enhancing specification generation and refinement.

Abstract: Fully automated verification of large-scale software and hardware systems is
arguably the holy grail of formal methods. Large language models (LLMs) have
recently demonstrated their potential for enhancing the degree of automation in
formal verification by, e.g., generating formal specifications as essential to
deductive verification, yet exhibit poor scalability due to context-length
limitations and, more importantly, the difficulty of inferring complex,
interprocedural specifications. This paper outlines Preguss - a modular,
fine-grained framework for automating the generation and refinement of formal
specifications. Preguss synergizes between static analysis and deductive
verification by orchestrating two components: (i) potential runtime error
(RTE)-guided construction and prioritization of verification units, and (ii)
LLM-aided synthesis of interprocedural specifications at the unit level. We
envisage that Preguss paves a compelling path towards the automated
verification of large-scale programs.

</details>


### [8] [Post-hoc LLM-Supported Debugging of Distributed Processes](https://arxiv.org/abs/2508.14540)
*Dennis Schiese,Andreas Both*

Main category: cs.SE

TL;DR: This paper presents an AI-powered method to automate and enhance software debugging by generating natural-language explanations from process data. Tested on Java but applicable to any language, the open-source tool helps developers understand and fix errors more easily.


<details>
  <summary>Details</summary>
Motivation: Manual debugging in modern software systems is highly resource-intensive and inefficient, especially as these systems become more complex and distributed.

Method: The paper introduces an approach that uses a system's process data combined with generative AI to generate natural-language explanations. These explanations are formed using actual process data, interface information, and documentation to help developers efficiently understand system behavior and errors.

Result: A demonstrator implementing this approach was developed for a component-based Java system. It is language-agnostic and aims to help developers comprehend system processes even without detailed knowledge of the system. The demonstrator is released as an open-source web application.

Conclusion: The approach and demonstrator significantly ease the debugging process by providing natural-language explanations to guide developers, improving efficiency and accessibility.

Abstract: In this paper, we address the problem of manual debugging, which nowadays
remains resource-intensive and in some parts archaic. This problem is
especially evident in increasingly complex and distributed software systems.
Therefore, our objective of this work is to introduce an approach that can
possibly be applied to any system, at both the macro- and micro-level, to ease
this debugging process. This approach utilizes a system's process data, in
conjunction with generative AI, to generate natural-language explanations.
These explanations are generated from the actual process data, interface
information, and documentation to guide the developers more efficiently to
understand the behavior and possible errors of a process and its sub-processes.
Here, we present a demonstrator that employs this approach on a component-based
Java system. However, our approach is language-agnostic. Ideally, the generated
explanations will provide a good understanding of the process, even if
developers are not familiar with all the details of the considered system. Our
demonstrator is provided as an open-source web application that is freely
accessible to all users.

</details>


### [9] [Towards LLM-generated explanations for Component-based Knowledge Graph Question Answering Systems](https://arxiv.org/abs/2508.14553)
*Dennis Schiese,Aleksandr Perevalov,Andreas Both*

Main category: cs.SE

TL;DR: This paper proposes a method to increase the explainability of component-based QA systems by verbalizing their data flows (using RDF and SPARQL) through template-based and LLM-generated explanations. Experiments show that LLM-driven explanations are better received by users, making complex QA processes more transparent and comprehensible.


<details>
  <summary>Details</summary>
Motivation: As software systems become increasingly complex, it is difficult for both developers and users to understand and explain decisions made by these systems, particularly for AI-driven, component-based Question Answering (QA) systems.

Method: The paper presents an approach that uses the input and output data flows of QA system components (represented as SPARQL queries and RDF triples) to generate explanations. This is done using two strategies: template-based settings (as a baseline) and automatic generation using Large Language Models (LLMs) in different configurations.

Result: Experimental results show that explanations generated using LLMs are of high quality and generally outperform those generated via the template-based baseline, according to user ratings.

Conclusion: The approach allows for automatic generation of understandable explanations for QA component behaviors and decisions, demonstrated to be effective when using RDF and SPARQL as the context for explanations. LLM-based explanations are rated superior to template-based ones.

Abstract: Over time, software systems have reached a level of complexity that makes it
difficult for their developers and users to explain particular decisions made
by them. In this paper, we focus on the explainability of component-based
systems for Question Answering (QA). These components often conduct processes
driven by AI methods, in which behavior and decisions cannot be clearly
explained or justified, s.t., even for QA experts interpreting the executed
process and its results is hard. To address this challenge, we present an
approach that considers the components' input and output data flows as a source
for representing the behavior and provide explanations for the components,
enabling users to comprehend what happened. In the QA framework used here, the
data flows of the components are represented as SPARQL queries (inputs) and RDF
triples (outputs). Hence, we are also providing valuable insights on
verbalization regarding these data types. In our experiments, the approach
generates explanations while following template-based settings (baseline) or
via the use of Large Language Models (LLMs) with different configurations
(automatic generation). Our evaluation shows that the explanations generated
via LLMs achieve high quality and mostly outperform template-based approaches
according to the users' ratings. Therefore, it enables us to automatically
explain the behavior and decisions of QA components to humans while using RDF
and SPARQL as a context for explanations.

</details>


### [10] [Towards a DSL to Formalize Multimodal Requirements](https://arxiv.org/abs/2508.14631)
*Marcos Gomez-Vazquez,Jordi Cabot*

Main category: cs.SE

TL;DR: This paper presents MERLAN, a DSL and tool that lets engineers specify requirements for multimodal interfaces and auto-generate compatible system implementations, addressing gaps in current requirement specification for systems with diverse user inputs.


<details>
  <summary>Details</summary>
Motivation: There is a growing prevalence of multimodal systems utilizing text, audio, and image inputs due to advancements in machine learning. However, requirements specification for such systems is challenging given the lack of suitable languages and methods for these diverse interactions, risking user needs not being met.

Method: The paper introduces MERLAN, a domain-specific language (DSL) for defining requirements of multimodal interfaces. It presents the language metamodel, a textual syntax via ANTLR grammar, and a prototype tool for requirements engineering and automatic system generation.

Result: MERLAN and its supporting tool enable requirements engineers to specify multimodal requirements and automatically generate compliant system implementations using an agentic framework.

Conclusion: MERLAN effectively addresses the challenges of specifying requirements for multimodal systems, improving the alignment between user needs and AI-enhanced implementations.

Abstract: Multimodal systems, which process multiple input types such as text, audio,
and images, are becoming increasingly prevalent in software systems, enabled by
the huge advancements in Machine Learning. This triggers the need to easily
define the requirements linked to these new types of user interactions,
potentially involving more than one modality at the same time. This remains an
open challenge due to the lack of languages and methods adapted to the diverse
nature of multimodal interactions, with the risk of implementing AI-enhanced
systems that do not properly satisfy the user needs.
  In this sense, this paper presents MERLAN, a Domain-Specific Language (DSL)
to specify the requirements for these new types of multimodal interfaces. We
present the metamodel for such language together with a textual syntax
implemented as an ANTLR grammar. A prototype tool enabling requirements
engineers to write such requirements and automatically generate a possible
implementation of a system compliant with them on top of an agentic framework
is also provided.

</details>


### [11] [Assessing the Quality and Security of AI-Generated Code: A Quantitative Analysis](https://arxiv.org/abs/2508.14727)
*Abbas Sabra,Olivier Schmitt,Joseph Tyler*

Main category: cs.SE

TL;DR: Functional benchmarks do not guarantee the security or quality of LLM-generated code. Static analysis is vital for identifying unseen defects, as all major LLMs tested exhibit shared vulnerabilities and code problems even when their code passes unit tests.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand not just the functional correctness of code generated by Large Language Models (LLMs), but also the quality and security of such code. Prior studies mainly assessed if LLM-generated code works, but overlooked potential defects or security vulnerabilities inherent in the outputs.

Method: The study takes code generated by five well-known LLMs for 4,442 Java assignments and subjects these outputs to static analysis using SonarQube. This approach quantitatively identifies bugs, security vulnerabilities, and code smells in the LLM-generated code. The researchers also compare functional performance (unit test Pass@1 rate) with the quality/security metrics.

Result: LLMs generate code with systemic defects, such as bugs, security vulnerabilities (including hard-coded passwords and path traversal), and code smells, regardless of their functional performance. There is no direct correlation between how well a model does in functional tests and the actual quality/security of its outputs. All models showed similar weaknesses, suggesting shared limitations in current LLM code generation methods.

Conclusion: Static analysis is crucial for exposing latent quality and security issues in LLM-generated code, as functional benchmarks alone cannot capture these risks. Organizations relying on LLMs for software development should use static analysis tools to safeguard their code bases.

Abstract: This study presents a quantitative evaluation of the code quality and
security of five prominent Large Language Models (LLMs): Claude Sonnet 4,
Claude 3.7 Sonnet, GPT-4o, Llama 3.2 90B, and OpenCoder 8B. While prior
research has assessed the functional performance of LLM-generated code, this
research tested LLM output from 4,442 Java coding assignments through
comprehensive static analysis using SonarQube. The findings suggest that
although LLMs can generate functional code, they also introduce a range of
software defects, including bugs, security vulnerabilities, and code smells.
These defects do not appear to be isolated; rather, they may represent shared
weaknesses stemming from systemic limitations within current LLM code
generation methods. In particular, critically severe issues, such as hard-coded
passwords and path traversal vulnerabilities, were observed across multiple
models. These results indicate that LLM-generated code requires verification in
order to be considered production-ready. This study found no direct correlation
between a model's functional performance (measured by Pass@1 rate of unit
tests) and the overall quality and security of its generated code, measured by
the number of SonarQube issues in benchmark solutions that passed the
functional tests. This suggests that functional benchmark performance score is
not a good indicator of overall code quality and security. The goal of this
study is not to rank LLM performance but to highlight that all evaluated models
appear to share certain weaknesses. Consequently, these findings support the
view that static analysis can be a valuable instrument for detecting latent
defects and an important safeguard for organizations that deploy AI in software
development.

</details>


### [12] [Challenges of Virtual Validation and Verification for Automotive Functions](https://arxiv.org/abs/2508.14747)
*Beatriz Cabrero-Daniel,Mazen Mohamad*

Main category: cs.SE

TL;DR: Through expert workshops and surveys, this paper identifies 17 main challenges in vehicle simulation verification, pinpoints existing solutions, highlights remaining problems, and outlines future research directions for more effective safety simulations.


<details>
  <summary>Details</summary>
Motivation: Verification and validation of vehicles via simulation is crucial for safety, but achieving realistic and effective simulations is complex. The motivation is to identify and address the key challenges in this domain.

Method: The authors conducted a workshop with field experts to brainstorm challenges, then distributed a survey to consolidate findings, gather insights, and explore potential solutions.

Result: Seventeen key challenges were identified, alongside proposed solutions and analysis of research priorities and implementation barriers. Although insufficient resources were not initially seen as a major issue, increasing resource usage became a critical need during solution discussions. Some challenges remain unresolved, while others already have known solutions.

Conclusion: The study helped shift attention from already-solved problems towards unresolved challenges and next steps for research, aiming to benefit the broader community by sharing these insights.

Abstract: Verification and validation of vehicles is a complex yet critical process,
particularly for ensuring safety and coverage through simulations. However,
achieving realistic and useful simulations comes with significant challenges.
To explore these challenges, we conducted a workshop with experts in the field,
allowing them to brainstorm key obstacles. Following this, we distributed a
survey to consolidate findings and gain further insights into potential
solutions. The experts identified 17 key challenges, along with proposed
solutions, an assessment of whether they represent next steps for research, and
the roadblocks to their implementation. While a lack of resources was not
initially highlighted as a major challenge, utilizing more resources emerged as
a critical necessity when experts discussed solutions. Interestingly, we
expected some of these challenges to have already been addressed or to have
systematic solutions readily available, given the collective expertise in the
field. Many of the identified problems already have known solutions, allowing
us to shift focus towards unresolved challenges and share the next steps with
the broader community.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [13] [Tuning Random Generators: Property-Based Testing as Probabilistic Programming](https://arxiv.org/abs/2508.14394)
*Ryan Tjoa,Poorva Garg,Harrison Goldstein,Todd Millstein,Benjamin Pierce,Guy Van den Broeck*

Main category: cs.PL

TL;DR: The paper introduces Loaded Dice, a probabilistic programming tool for automatic tuning of input generators in property-based testing, resulting in much faster and more effective bug discovery.


<details>
  <summary>Details</summary>
Motivation: Current property-based testing (PBT) relies on manually tuning test input generators, which is tedious and difficult. Users must hand-pick generator weights to achieve effective test input distributions, limiting testing quality and efficiency.

Method: The authors develop automatic and offline generator tuning techniques, using a discrete probabilistic programming system called Loaded Dice. This system supports differentiation and parameter learning to optimize generator weights according to user-defined objective functions.

Result: The approach effectively optimizes generator distributions for chosen objectives, such as desired distributions, diversity, and validity. When tested on PBT benchmarks, automatically-tuned generators deliver a 3.1-7.4x speedup in bug finding compared to untuned ones.

Conclusion: Automatic tuning of test input generators via probabilistic programming significantly improves the efficacy and efficiency of property-based testing, reducing manual effort and increasing bug-finding capability.

Abstract: Property-based testing validates software against an executable specification
by evaluating it on randomly generated inputs. The standard way that PBT users
generate test inputs is via generators that describe how to sample test inputs
through random choices. To achieve a good distribution over test inputs, users
must tune their generators, i.e., decide on the weights of these individual
random choices. Unfortunately, it is very difficult to understand how to choose
individual generator weights in order to achieve a desired distribution, so
today this process is tedious and limits the distributions that can be
practically achieved.
  In this paper, we develop techniques for the automatic and offline tuning of
generators. Given a generator with undetermined symbolic weights and an
objective function, our approach automatically learns values for these weights
that optimize for the objective. We describe useful objective functions that
allow users to (1) target desired distributions and (2) improve the diversity
and validity of their test cases. We have implemented our approach in a novel
discrete probabilistic programming system, Loaded Dice, that supports
differentiation and parameter learning, and use it as a language for
generators. We empirically demonstrate that our approach is effective at
optimizing generator distributions according to the specified objective
functions. We also perform a thorough evaluation on PBT benchmarks,
demonstrating that, when automatically tuned for diversity and validity, the
generators exhibit a 3.1-7.4x speedup in bug finding.

</details>


### [14] [Close is Good Enough: Component-Based Synthesis Modulo Logical Similarity](https://arxiv.org/abs/2508.14614)
*Ashish Mishra,Suresh Jagannathan*

Main category: cs.PL

TL;DR: The paper introduces a new method for component-based program synthesis that reduces redundant search by reasoning about similarity in solution paths using Qualified Tree Automata, resulting in a more powerful and efficient synthesis tool called \name.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the challenge of CBS algorithms struggling as constraints become stricter and library specifications more expressive, shrinking the feasible solution space and making redundant path exploration wasteful.

Method: The authors enhance CBS search with logical similarity reasoning using refinement-type specifications and introduce Qualified Tree Automata to manage and record the search process more efficiently. This approach is implemented in a tool called \name, which is comprehensively evaluated against prior work.

Result: The proposed tool \name successfully synthesizes solutions to complex CBS queries, surpassing the performance and capabilities of current state-of-the-art tools.

Conclusion: The proposed approach using Qualified Tree Automata and reasoning about logical similarities among execution paths enables more effective component-based synthesis by avoiding redundant or similar search paths, outperforming existing methods.

Abstract: Component-based synthesis (CBS) aims to generate loop-free programs from a
set of libraries whose methods are annotated with specifications and whose
output must satisfy a set of logical constraints, expressed as a query. The
effectiveness of a CBS algorithm critically depends on the severity of the
constraints imposed by the query. The more exact these constraints are, the
sparser the space of feasible solutions. This maxim also applies when we enrich
the expressiveness of the specifications affixed to library methods. In both
cases, the search must now contend with constraints that may only hold over a
small number of the possible execution paths that can be enumerated by a CBS
procedure.
  In this paper, we address this challenge by equipping CBS search with the
ability to reason about logical similarities among the paths it explores. Our
setting considers library methods equipped with refinement-type specifications
that enrich ordinary base types with a set of rich logical qualifiers to
constrain the set of values accepted by that type. We perform a search over a
tree automata variant called Qualified Tree Automata that intelligently records
information about enumerated terms, leveraging subtyping constraints over the
refinement types associated with these terms to enable reasoning about
similarity among candidate solutions as search proceeds, thereby avoiding
exploration of semantically similar paths.
  We present an implementation of this idea in a tool called \name, and provide
a comprehensive evaluation that demonstrates \name's ability to synthesize
solutions to complex CBS queries that go well-beyond the capabilities of the
existing state-of-the-art.

</details>
