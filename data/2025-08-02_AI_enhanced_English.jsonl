{"id": "2507.23151", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.23151", "abs": "https://arxiv.org/abs/2507.23151", "authors": ["Louis Rustenholz", "Pedro Lopez-Garcia", "Manuel V. Hermenegildo"], "title": "Abstractions of Sequences, Functions and Operators", "comment": "Under consideration for publication in STTT", "summary": "We present theoretical and practical results on the order theory of lattices\nof functions, focusing on Galois connections that abstract (sets of) functions\n- a topic known as higher-order abstract interpretation.\n  We are motivated by the challenge of inferring closed-form bounds on\nfunctions which are defined recursively, i.e. as the fixed point of an operator\nor, equivalently, as the solution to a functional equation. This has multiple\napplications in program analysis (e.g. cost analysis, loop acceleration,\ndeclarative language analysis) and in hybrid systems governed by differential\nequations.\n  Our main contribution is a new family of constraint-based abstract domains\nfor abstracting numerical functions, B-bound domains, which abstract a function\nf by a conjunction of bounds from a preselected set of boundary functions. They\nallow inferring highly non-linear numerical invariants, which classical\nnumerical abstract domains struggle with. We uncover a convexity property in\nthe constraint space that simplifies, and, in some cases, fully automates,\ntransfer function design.\n  We also introduce domain abstraction, a functor that lifts arbitrary mappings\nin value space to Galois connections in function space. This supports\nabstraction from symbolic to numerical functions (i.e. size abstraction), and\nenables dimensionality reduction of equations.\n  We base our constructions of transfer functions on a simple operator\nlanguage, starting with sequences, and extending to more general functions,\nincluding multivariate, piecewise, and non-discrete domains.", "AI": {"tldr": "The paper proposes B-bound abstract domains and a domain abstraction functor for analyzing recursively defined functions, enabling the inference of complex numerical invariants and simplifying transfer function creation, with applicability in program and hybrid systems analysis.", "motivation": "The paper is motivated by the difficulty of inferring closed-form bounds on recursively defined functions, especially as fixed points of operators or as solutions to functional equations. This challenge has important applications in areas such as program analysis, cost analysis, loop acceleration, declarative language analysis, and the analysis of hybrid systems with differential equations.", "method": "The authors propose a new family of constraint-based abstract domains\u2014B-bound domains\u2014that abstract a function by conjoining bounds from a specified set of boundary functions. They identify a convexity property in the constraint space, which can simplify and sometimes automate transfer function design. Additionally, they introduce 'domain abstraction,' a functor that lifts arbitrary mappings in value space to Galois connections in function space, thereby enabling abstraction from symbolic to numerical functions and dimensionality reduction. The framework is built using a simple operator language covering various function types.", "result": "The B-bound domains allow for the inference of highly non-linear numerical invariants that are often out of reach for classical numerical abstract domains. The convexity property discovered can greatly facilitate transfer function design. The domain abstraction functor broadens the abstraction approaches available, making dimensionality reduction and abstraction from symbolic to numerical functions possible in a principled way.", "conclusion": "The paper advances both theoretical foundations and practical methods for higher-order abstract interpretation via new abstract domains and abstraction functors, enabling effective analysis of complex, recursively defined functions in several practical domains."}}
{"id": "2507.23205", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23205", "abs": "https://arxiv.org/abs/2507.23205", "authors": ["Hebi Li", "Forrest Sheng Bao", "Qi Xiao", "Jin Tian"], "title": "Kernel-FFI: Transparent Foreign Function Interfaces for Interactive Notebooks", "comment": null, "summary": "Foreign Function Interfaces (FFIs) are essential for enabling\ninteroperability between programming languages, yet existing FFI solutions are\nill-suited for the dynamic, interactive workflows prevalent in modern notebook\nenvironments such as Jupyter. Current approaches require extensive manual\nconfiguration, introduce significant boilerplate, and often lack support for\nrecursive calls and object-oriented programming (OOP) constructs-features\ncritical for productive, multi-language development.\n  We present Kernel-FFI, a transparent, language-agnostic framework that\nenables seamless cross-language function calls and object manipulation within\ninteractive notebooks. Kernel-FFI employs source-level transformation to\nautomatically rewrite cross-language invocations, eliminating the need for\nmanual bindings or boilerplate. Kernel-FFI provides robust support for OOP by\nenabling foreign object referencing and automatic resource management across\nlanguage boundaries. Furthermore, to address the blocking nature of Jupyter\nkernels and support recursive and asynchronous foreign calls, we introduce a\nnovel side-channel communication mechanism. Our tool will be open-sourced and\navailable at https://codepod.io/docs/kernel-ffi", "AI": {"tldr": "Kernel-FFI is a new open-source framework enabling easy, seamless cross-language function calls and object manipulation in Jupyter notebooks, eliminating manual binding and supporting advanced features like recursion and OOP.", "motivation": "Existing FFI solutions are cumbersome for the dynamic, interactive workflows found in notebook environments like Jupyter. They typically require manual setup, are verbose, do not support recursive calls, and lack full object-oriented programming (OOP) functionality across languages.", "method": "The authors introduce Kernel-FFI, a language-agnostic framework that utilizes source-level transformation to automatically rewrite cross-language invocations. This avoids the need for manual bindings and boilerplate code, and supports OOP by referencing foreign objects and managing resources automatically. They also develop a novel side-channel communication mechanism to support recursive and asynchronous calls within blocking Jupyter kernels.", "result": "Kernel-FFI enables seamless, transparent cross-language function calls and object manipulation in interactive notebooks, providing full OOP support and removing the traditional manual and boilerplate barriers. The framework is open-sourced for public use.", "conclusion": "Kernel-FFI makes multi-language development more efficient and user-friendly in notebook environments, supporting advanced features like recursive calls and OOP constructs with minimal manual intervention."}}
{"id": "2507.23087", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23087", "abs": "https://arxiv.org/abs/2507.23087", "authors": ["Fabian Stiehle", "Hans Weytjens", "Ingo Weber"], "title": "On LLM-Assisted Generation of Smart Contracts from Business Processes", "comment": "Accepted at the Workshop on Distributed Ledger Technologies in\n  Business Process Management, At the International Conference for Business\n  Process Management (BPM), 2025", "summary": "Large language models (LLMs) have changed the reality of how software is\nproduced. Within the wider software engineering community, among many other\npurposes, they are explored for code generation use cases from different types\nof input. In this work, we present an exploratory study to investigate the use\nof LLMs for generating smart contract code from business process descriptions,\nan idea that has emerged in recent literature to overcome the limitations of\ntraditional rule-based code generation approaches. However, current LLM-based\nwork evaluates generated code on small samples, relying on manual inspection,\nor testing whether code compiles but ignoring correct execution. With this\nwork, we introduce an automated evaluation framework and provide empirical data\nfrom larger data sets of process models. We test LLMs of different types and\nsizes in their capabilities of achieving important properties of process\nexecution, including enforcing process flow, resource allocation, and\ndata-based conditions. Our results show that LLM performance falls short of the\nperfect reliability required for smart contract development. We suggest future\nwork to explore responsible LLM integrations in existing tools for code\ngeneration to ensure more reliable output. Our benchmarking framework can serve\nas a foundation for developing and evaluating such integrations.", "AI": {"tldr": "LLMs are promising for smart contract code generation, but current models are not reliable enough. The authors propose a new evaluation framework and find that better integration and development are needed to ensure safe, correct code.", "motivation": "There is increasing interest in using large language models (LLMs) to automatically generate smart contract code from business process descriptions because traditional rule-based code generation methods have limitations. Current LLM-based approaches are often evaluated with small datasets and superficial checks, making it unclear how reliably they produce correct smart contracts.", "method": "The paper presents an exploratory study using an automated evaluation framework to empirically test LLMs of various types and sizes. The models were assessed on their ability to correctly enforce core smart contract properties\u2014such as process flow, resource allocation, and data-based conditions\u2014using large datasets of process models.", "result": "LLMs were found to be unreliable for smart contract generation, as their performance did not meet the stringent reliability standards necessary for this application.", "conclusion": "While LLMs have potential, they currently do not offer sufficient reliability for critical smart contract code generation tasks. There is a need for more responsible and robust integration of LLMs into existing code generation tools, and the paper\u2019s benchmarking framework can aid in developing and testing safer approaches."}}
{"id": "2507.23118", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23118", "abs": "https://arxiv.org/abs/2507.23118", "authors": ["Mattia Di Profio", "Mingjun Zhong", "Yaji Sripada", "Marcel Jaspars"], "title": "FlowETL: An Autonomous Example-Driven Pipeline for Data Engineering", "comment": null, "summary": "The Extract, Transform, Load (ETL) workflow is fundamental for populating and\nmaintaining data warehouses and other data stores accessed by analysts for\ndownstream tasks. A major shortcoming of modern ETL solutions is the extensive\nneed for a human-in-the-loop, required to design and implement\ncontext-specific, and often non-generalisable transformations. While related\nwork in the field of ETL automation shows promising progress, there is a lack\nof solutions capable of automatically designing and applying these\ntransformations. We present FlowETL, a novel example-based autonomous ETL\npipeline architecture designed to automatically standardise and prepare input\ndatasets according to a concise, user-defined target dataset. FlowETL is an\necosystem of components which interact together to achieve the desired outcome.\nA Planning Engine uses a paired input-output datasets sample to construct a\ntransformation plan, which is then applied by an ETL worker to the source\ndataset. Monitoring and logging provide observability throughout the entire\npipeline. The results show promising generalisation capabilities across 14\ndatasets of various domains, file structures, and file sizes.", "AI": {"tldr": "FlowETL automates the process of creating ETL data transformations using example pairs, reducing reliance on manual engineering and successfully generalizing across different datasets.", "motivation": "Modern ETL workflows require significant human intervention to create context-specific data transformations, making the process labor-intensive and difficult to generalize.", "method": "The authors introduce FlowETL, an autonomous ETL pipeline that uses example-based learning to generate transformation plans. A Planning Engine constructs transformation logic from paired input-output samples, which an ETL worker applies to raw data. The system provides monitoring and logging for full observability.", "result": "FlowETL demonstrated strong generalization across 14 datasets with varied domains, structures, and sizes, indicating its potential to automate standard ETL tasks.", "conclusion": "FlowETL effectively reduces the manual effort needed in designing and applying ETL transformations, successfully generalizing automated data processing across multiple contexts."}}
{"id": "2507.23120", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23120", "abs": "https://arxiv.org/abs/2507.23120", "authors": ["Jordi Cabot"], "title": "Vibe Modeling: Challenges and Opportunities", "comment": null, "summary": "There is a pressing need for better development methods and tools to keep up\nwith the growing demand and increasing complexity of new software systems. New\ntypes of user interfaces, the need for intelligent components, sustainability\nconcerns, ... bring new challenges that we need to handle. In the last years,\nmodel-driven engineering (MDE) has been key to improving the quality and\nproductivity of software development, but models themselves are becoming\nincreasingly complex to specify and manage. At the same time, we are witnessing\nthe growing popularity of vibe coding approaches that rely on Large Language\nModels (LLMs) to transform natural language descriptions into running code at\nthe expenses of code vulnerabilities, scalability issues and maintainability\nconcerns. In this paper, we introduce the concept of \\textit{vibe modeling} as\na novel approach to integrate the best of both worlds (AI and MDE) to speed up\nthe development of reliable complex systems. We outline the key concepts of\nvibe modeling and highlight the opportunities and open challenges it presents\nfor the future of modeling.", "AI": {"tldr": "The paper proposes 'vibe modeling', a hybrid approach utilizing both AI (LLMs) and model-driven engineering to address complexity in modern software development, offering future opportunities but also highlighting ongoing challenges.", "motivation": "Growing software complexity and demands require improved development methods. Current techniques, including model-driven engineering (MDE), help improve quality and productivity but face challenges as models become more complex. At the same time, modern AI-based coding (\"vibe coding\") has usability and reliability drawbacks.", "method": "The paper introduces 'vibe modeling', a new paradigm combining the strengths of AI-driven coding (with LLMs) and traditional model-driven engineering, aiming to mitigate the limitations of each approach.", "result": "The paper outlines core concepts of 'vibe modeling' and discusses both its potential benefits and the challenges associated with integrating AI and MDE for robust software development.", "conclusion": "Vibe modeling has the potential to accelerate the development of reliable, complex systems by fusing AI and MDE methodologies, but presents new research opportunities and open challenges for the modeling community."}}
{"id": "2507.23168", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23168", "abs": "https://arxiv.org/abs/2507.23168", "authors": ["Elmira Onagh", "Maleknaz Nayebi"], "title": "Extension Decisions in Open Source Software Ecosystem", "comment": "Paper published in JSS journal", "summary": "GitHub Marketplace is expanding by approximately 41% annually, with new\ntools; however, many additions replicate existing functionality. We study this\nphenomenon in the platform's largest segment, Continuous Integration (CI), by\nlinking 6,983 CI Actions to 3,869 providers and mining their version histories.\nOur graph model timestamps every functionality's debut, tracks its adoption,\nand clusters redundant tools. We find that approximately 65% of new CI Actions\nreplicate existing capabilities, typically within six months, and that a small\nset of first-mover Actions accounts for most subsequent forks and extensions.\nThese insights enable developers to choose the optimal moment to launch, target\nunmet functionality, and help maintainers eliminate redundant tools. We publish\nthe complete graph and dataset to encourage longitudinal research on innovation\nand competition in software ecosystems, and to provide practitioners with a\ndata-driven roadmap for identifying emerging trends and guiding product\nstrategy.", "AI": {"tldr": "Most new CI tools in GitHub Marketplace copy existing ones quickly, with a few original tools dominating the ecosystem. This research offers data-driven guidance to avoid redundancy and spot innovation opportunities.", "motivation": "GitHub Marketplace, especially in the Continuous Integration (CI) segment, is experiencing rapid growth, but much of this growth involves tools with overlapping functionality. Understanding the patterns of redundancy and innovation can help developers and maintainers make informed decisions.", "method": "The study links 6,983 CI Actions to 3,869 providers, analyzing their version histories. A graph model is constructed to timestamp each functionality's introduction, monitor adoption, and identify clusters of redundant tools.", "result": "The analysis shows that about 65% of new CI Actions duplicate existing functionalities, usually within six months. Additionally, a few early Actions lead most subsequent forks and extensions.", "conclusion": "The findings provide actionable insights for developers and maintainers to optimize launch timing, focus on unmet needs, and reduce redundancy. The published dataset and models aid further research and strategic decision-making in software ecosystems."}}
{"id": "2507.23178", "categories": ["cs.SE", "cs.AI", "I.2.5"], "pdf": "https://arxiv.org/pdf/2507.23178", "abs": "https://arxiv.org/abs/2507.23178", "authors": ["Siyuan Liu", "Zhice Yang", "Huangxun Chen"], "title": "AutoBridge: Automating Smart Device Integration with Centralized Platform", "comment": "14 pages, 12 figures, under review", "summary": "Multimodal IoT systems coordinate diverse IoT devices to deliver\nhuman-centered services. The ability to incorporate new IoT devices under the\nmanagement of a centralized platform is an essential requirement. However, it\nrequires significant human expertise and effort to program the complex IoT\nintegration code that enables the platform to understand and control the device\nfunctions. Therefore, we propose AutoBridge to automate IoT integration code\ngeneration. Specifically, AutoBridge adopts a divide-and-conquer strategy: it\nfirst generates device control logic by progressively retrieving\ndevice-specific knowledge, then synthesizes platformcompliant integration code\nusing platform-specific knowledge. To ensure correctness, AutoBridge features a\nmulti-stage debugging pipeline, including an automated debugger for virtual IoT\ndevice testing and an interactive hardware-in-the-loop debugger that requires\nonly binary user feedback (yes and no) for real-device verification. We\nevaluate AutoBridge on a benchmark of 34 IoT devices across two open-source IoT\nplatforms. The results demonstrate that AutoBridge can achieves an average\nsuccess rate of 93.87% and an average function coverage of 94.87%, without any\nhuman involvement. With minimal binary yes and no feedback from users, the code\nis then revised to reach 100% function coverage. A user study with 15\nparticipants further shows that AutoBridge outperforms expert programmers by\n50% to 80% in code accuracy, even when the programmers are allowed to use\ncommercial code LLMs.", "AI": {"tldr": "AutoBridge is an automated IoT integration framework that drastically reduces human effort and expertise required for device onboarding. It delivers high accuracy and full function coverage across diverse platforms, outperforming expert programmers and current code-generation tools.", "motivation": "Integrating new IoT devices into centralized platforms requires complex and labor-intensive programming, demanding significant human expertise. Automating this integration process can make IoT system adoption and maintenance much easier.", "method": "The authors propose AutoBridge, an automated framework for IoT integration code generation. AutoBridge uses a divide-and-conquer approach: it progressively gathers device-specific information to generate control logic, then synthesizes integration code compatible with different IoT platforms. AutoBridge employs a multi-stage debugging process, including automated virtual testing and an interactive, low-effort hardware-in-the-loop debugger that relies on simple yes/no feedback from users for real-device validation.", "result": "AutoBridge was evaluated using 34 IoT devices across two open-source platforms, achieving an average success rate of 93.87% and function coverage of 94.87% without human intervention. With minimal binary user feedback, it achieved 100% function coverage. In a user study with 15 participants, AutoBridge's generated code was 50%\u201380% more accurate than code written by expert programmers, even when they used commercial code LLMs.", "conclusion": "AutoBridge significantly automates and improves the integration of new IoT devices into centralized IoT platforms, reducing human effort and boosting code accuracy and coverage."}}
{"id": "2507.23269", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.23269", "abs": "https://arxiv.org/abs/2507.23269", "authors": ["Peter Fettke", "Fabiana Fournier", "Lior Limonad", "Andreas Metzger", "Stefanie Rinderle-Ma", "Barbara Weber"], "title": "XABPs: Towards eXplainable Autonomous Business Processes", "comment": null, "summary": "Autonomous business processes (ABPs), i.e., self-executing workflows\nleveraging AI/ML, have the potential to improve operational efficiency, reduce\nerrors, lower costs, improve response times, and free human workers for more\nstrategic and creative work. However, ABPs may raise specific concerns\nincluding decreased stakeholder trust, difficulties in debugging, hindered\naccountability, risk of bias, and issues with regulatory compliance. We argue\nfor eXplainable ABPs (XABPs) to address these concerns by enabling systems to\narticulate their rationale. The paper outlines a systematic approach to XABPs,\ncharacterizing their forms, structuring explainability, and identifying key BPM\nresearch challenges towards XABPs.", "AI": {"tldr": "The paper argues that while autonomous business processes powered by AI/ML offer major efficiency gains, they also introduce challenges like trust and accountability. The authors propose a systematic framework for making such processes explainable (XABPs), detail various forms and structures of explainability, and highlight key research challenges for integration into business process management.", "motivation": "Autonomous business processes (ABPs) promise significant improvements in efficiency and cost, but present challenges regarding stakeholder trust, accountability, explainability, and regulatory compliance due to their reliance on AI/ML.", "method": "The paper proposes a systematic approach to 'eXplainable ABPs' (XABPs), which involves characterizing different forms of explainability, structuring frameworks for explainability, and identifying key research challenges in business process management (BPM) to achieve XABPs.", "result": "Key forms and structures of explainability within autonomous business processes are outlined, along with a set of BPM research challenges that must be addressed for the realization of XABPs.", "conclusion": "Explainable ABPs are essential for building trust, accountability, and regulatory compliance in autonomous workflows, and the paper's approach lays a research roadmap to integrate explainability into ABPs."}}
{"id": "2507.23348", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23348", "abs": "https://arxiv.org/abs/2507.23348", "authors": ["Han Li", "Yuling Shi", "Shaoxin Lin", "Xiaodong Gu", "Heng Lian", "Xin Wang", "Yantao Jia", "Tao Huang", "Qianxiang Wang"], "title": "SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution", "comment": "Our code and data are available at\n  https://github.com/YerbaPage/SWE-Debate", "summary": "Issue resolution has made remarkable progress thanks to the advanced\nreasoning capabilities of large language models (LLMs). Recently, agent-based\nframeworks such as SWE-agent have further advanced this progress by enabling\nautonomous, tool-using agents to tackle complex software engineering tasks.\nWhile existing agent-based issue resolution approaches are primarily based on\nagents' independent explorations, they often get stuck in local solutions and\nfail to identify issue patterns that span across different parts of the\ncodebase. To address this limitation, we propose SWE-Debate, a competitive\nmulti-agent debate framework that encourages diverse reasoning paths and\nachieves more consolidated issue localization. SWE-Debate first creates\nmultiple fault propagation traces as localization proposals by traversing a\ncode dependency graph. Then, it organizes a three-round debate among\nspecialized agents, each embodying distinct reasoning perspectives along the\nfault propagation trace. This structured competition enables agents to\ncollaboratively converge on a consolidated fix plan. Finally, this consolidated\nfix plan is integrated into an MCTS-based code modification agent for patch\ngeneration. Experiments on the SWE-bench benchmark show that SWE-Debate\nachieves new state-of-the-art results in open-source agent frameworks and\noutperforms baselines by a large margin.", "AI": {"tldr": "SWE-Debate is a new multi-agent debate system for fixing code issues. By enabling agent competition and collaboration, it identifies and resolves complex software bugs better than previous methods, achieving state-of-the-art performance on benchmarks.", "motivation": "Existing agent-based issue resolution methods in software engineering mainly rely on independent agent explorations. These approaches struggle with local optima, missing broader issue patterns across the entire codebase, which limits their effectiveness in resolving complex issues.", "method": "The authors propose SWE-Debate: a competitive multi-agent framework. SWE-Debate first generates multiple fault propagation traces through code dependency graph traversal, serving as localization proposals. Then, it conducts a structured, three-round debate among specialized agents representing different reasoning approaches along the traces. The winning, consensus-based fix plan is then fed into a Monte Carlo Tree Search (MCTS)-based agent to generate concrete code patches.", "result": "On the SWE-bench benchmark, SWE-Debate achieved new state-of-the-art results among open-source agent frameworks and significantly outperformed existing baseline methods.", "conclusion": "SWE-Debate\u2019s multi-agent debate framework consolidates diverse reasoning paths, enabling more robust and accurate issue localization and resolution, thus advancing the capabilities of LLM-powered autonomous agents in software engineering."}}
{"id": "2507.23356", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23356", "abs": "https://arxiv.org/abs/2507.23356", "authors": ["Shmulik Froimovich", "Raviv Gal", "Wesam Ibraheem", "Avi Ziv"], "title": "Quality Evaluation of COBOL to Java Code Transformation", "comment": "Submitted to ASE 2025", "summary": "We present an automated evaluation system for assessing COBOL-to-Java code\ntranslation within IBM's watsonx Code Assistant for Z (WCA4Z). The system\naddresses key challenges in evaluating LLM-based translators, including model\nopacity and the complexity of translation quality assessment. Our approach\ncombines analytic checkers with LLM-as-a-judge (LaaJ) techniques to deliver\nscalable, multi-faceted evaluations. The system supports continuous integration\nworkflows, enables large-scale benchmarking, and reduces reliance on manual\nreview. We describe the system architecture, evaluation strategies, and\nreporting mechanisms that provide actionable insights for developers and\nproject managers, facilitating the evolution of high-quality, modernized\ncodebases.", "AI": {"tldr": "The paper introduces an automated system that combines analytical and LLM-based techniques to efficiently evaluate COBOL-to-Java translation quality in IBM's WCA4Z, reducing manual effort and providing valuable feedback to improve code modernization.", "motivation": "Evaluating the quality of COBOL-to-Java code translation is challenging due to the opacity of large language models (LLMs) and the complexity involved in assessing translation accuracy. Manual review is time-consuming and does not scale well, creating a need for automated, reliable, and scalable evaluation systems.", "method": "The authors developed an automated evaluation system that integrates analytic checkers with LLM-as-a-judge (LaaJ) techniques. This combination allows for comprehensive, multi-faceted evaluation of translation quality within IBM's watsonx Code Assistant for Z (WCA4Z) environment. The system is designed to support continuous integration, large-scale benchmarking, and automated reporting.", "result": "The system enables scalable, automated evaluation of code translations, supports large-scale benchmarking, and integrates well with CI workflows. It reduces the need for manual review and provides actionable insights to developers and project managers for improving code quality.", "conclusion": "The proposed system effectively addresses key barriers in evaluating LLM-based code translators by automating the assessment process, delivering multi-dimensional insights, and fostering the modernization and quality evolution of codebases."}}
{"id": "2507.23361", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23361", "abs": "https://arxiv.org/abs/2507.23361", "authors": ["Silin Chen", "Shaoxin Lin", "Xiaodong Gu", "Yuling Shi", "Heng Lian", "Longfei Yun", "Dong Chen", "Weiguo Sun", "Lin Cao", "Qianxiang Wang"], "title": "SWE-Exp: Experience-Driven Software Issue Resolution", "comment": "Our code and data are available at\n  https://github.com/YerbaPage/SWE-Exp", "summary": "Recent advances in large language model (LLM) agents have shown remarkable\nprogress in software issue resolution, leveraging advanced techniques such as\nmulti-agent collaboration and Monte Carlo Tree Search (MCTS). However, current\nagents act as memoryless explorers - treating each problem separately without\nretaining or reusing knowledge from previous repair experiences. This leads to\nredundant exploration of failed trajectories and missed chances to adapt\nsuccessful issue resolution methods to similar problems. To address this\nproblem, we introduce SWE-Exp, an experience - enhanced approach that distills\nconcise and actionable experience from prior agent trajectories, enabling\ncontinuous learning across issues. Our method introduces a multi-faceted\nexperience bank that captures both successful and failed repair attempts.\nSpecifically, it extracts reusable issue resolution knowledge at different\nlevels - from high-level problem comprehension to specific code changes.\nExperiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%\nPass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach\nestablishes a new paradigm in which automated software engineering agents\nsystematically accumulate and leverage repair expertise, fundamentally shifting\nfrom trial-and-error exploration to strategic, experience-driven issue\nresolution.", "AI": {"tldr": "SWE-Exp enables software repair agents to continuously learn from past successes and failures, resulting in higher resolution rates and a shift from trial-and-error to experience-driven issue resolution.", "motivation": "Current large language model (LLM) agents for software issue resolution treat each problem separately and do not retain or reuse knowledge from previous experiences. This results in inefficient, redundant exploration and missed opportunities to apply past successful strategies.", "method": "The authors propose SWE-Exp, an experience-enhanced approach. It builds a multi-faceted experience bank that distills actionable knowledge from both successful and failed repair attempts across various levels, from high-level understanding to specific code edits. This enables agents to learn continuously from past experiences and apply this knowledge to new problems.", "result": "Experiments indicate SWE-Exp achieves a new state-of-the-art resolution rate of 41.6% Pass@1 on the SWE-bench-Verified benchmark using open-source agent frameworks.", "conclusion": "SWE-Exp demonstrates that by systematically accumulating and leveraging repair expertise, LLM-based agents can strategically resolve issues rather than relying solely on trial-and-error. This establishes a new experience-driven paradigm for automated software engineering agents."}}
{"id": "2507.23370", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23370", "abs": "https://arxiv.org/abs/2507.23370", "authors": ["Trae Research Team", "Pengfei Gao", "Zhao Tian", "Xiangxin Meng", "Xinchen Wang", "Ruida Hu", "Yuanan Xiao", "Yizhou Liu", "Zhao Zhang", "Junjie Chen", "Cuiyun Gao", "Yun Lin", "Yingfei Xiong", "Chao Peng", "Xia Liu"], "title": "Trae Agent: An LLM-based Agent for Software Engineering with Test-time Scaling", "comment": "Pengfei Gao and Zhao Tian contributed equally to this technical\n  report", "summary": "Software issue resolution is a critical challenge in software engineering and\nhas garnered increasing attention in recent years. With the rapid advancement\nof large language models (LLMs), substantial progress has been made in\naddressing real-world software engineering tasks. Recent studies have\nintroduced ensemble reasoning techniques to enhance the performance of\nLLM-based issue resolution. However, existing prompting-based methods still\nface limitations in effectively exploring large ensemble spaces and lack the\ncapacity for repository-level understanding, both of which constrain their\noverall effectiveness. In this paper, we propose Trae Agent, the first\nagent-based ensemble reasoning approach for repository-level issue resolution.\nTrae Agent formulates our goal as an optimal solution search problem and\naddresses two key challenges, i.e., large ensemble spaces and repository-level\nunderstanding, through modular agents for generation, pruning, and selection.\nWe conduct extensive experiments using three leading LLMs on the widely-adopted\nSWE-bench benchmark, comparing Trae Agent against four state-of-the-art\nensemble reasoning techniques. Experimental results demonstrate that Trae Agent\nconsistently achieves superior performance, with an average improvement of\n10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first\nplace on the SWE-bench Verified leaderboard, with a notable Pass@1 score of\n75.20%. We are pleased to release Trae Agent as an open-source project to\nsupport the research community, with all resources available at\nhttps://github.com/bytedance/trae-agent.", "AI": {"tldr": "Trae Agent is an agent-based ensemble reasoning approach for software issue resolution at the repository level. It addresses major shortcomings of previous methods, outperforming state-of-the-art baselines on SWE-bench and achieving a leading Pass@1 score. The tool is open-sourced for the community.", "motivation": "Software issue resolution remains a significant challenge in software engineering. While large language models (LLMs) have shown promise, existing ensemble reasoning techniques for LLM-based issue resolution are limited by inefficient exploration of ensemble spaces and lack of repository-level comprehension. This paper aims to overcome these persistent limitations.", "method": "The paper introduces 'Trae Agent,' an agent-based ensemble reasoning approach specifically designed for repository-level issue resolution. Trae Agent treats the problem as an optimal solution search and utilizes modular agents for the generation, pruning, and selection of solutions. Extensive comparative experiments are conducted using three popular LLMs on the SWE-bench benchmark, evaluating Trae Agent against four state-of-the-art ensemble reasoning methods.", "result": "Experimental results show that Trae Agent outperforms all compared baseline techniques, achieving an average of 10.22% higher Pass@1 score. It secured first place on the SWE-bench Verified leaderboard with a Pass@1 score of 75.20%.", "conclusion": "Trae Agent demonstrates the effectiveness of an agent-based modular approach in addressing the limitations of current prompting-based ensemble methods for LLM-based software issue resolution. The method achieves state-of-the-art results and has been released as open source for community use and further research."}}
{"id": "2507.23425", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23425", "abs": "https://arxiv.org/abs/2507.23425", "authors": ["Daphn\u00e9 Larrivain", "Shinhyung Yang", "Wilhelm Hasselbring"], "title": "Dynamic and Static Analysis of Python Software with Kieker Including Reconstructed Architectures", "comment": "9 pages, 9 figures", "summary": "The Kieker observability framework is a tool that provides users with the\nmeans to design a custom observability pipeline for their application.\nOriginally tailored for Java, supporting Python with Kieker is worthwhile.\nPython's popularity has exploded over the years, thus making structural\ninsights of Python applications highly valuable. Our Python analysis pipeline\ncombines static and dynamic analysis in order to build a complete picture of a\ngiven system.", "AI": {"tldr": "The paper extends the Kieker observability framework, originally for Java, to support Python. By combining static and dynamic analysis, it allows users to build custom observability pipelines and gain structural insights into Python applications.", "motivation": "The motivation behind this paper is the increasing popularity of Python as a programming language and the resulting need for structural insights into Python applications. The Kieker observability framework, traditionally focused on Java, lacks native support for Python, making such support desirable.", "method": "The method involves developing a Python analysis pipeline which integrates both static and dynamic analysis to provide comprehensive observability of Python applications within the Kieker framework.", "result": "The framework now enables users to design custom observability pipelines for Python applications, offering both static and dynamic insights into their structure and operation.", "conclusion": "Supporting Python in the Kieker observability framework expands its applicability and utility, allowing developers to gain valuable insights into Python systems similar to what has previously been possible for Java applications."}}
{"id": "2507.23640", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23640", "abs": "https://arxiv.org/abs/2507.23640", "authors": ["Samah Kansab", "Mohammed Sayagh", "Francis Bordeleau", "Ali Tizghadam"], "title": "An Empirical Study on the Amount of Changes Required for Merge Request Acceptance", "comment": null, "summary": "Code review (CR) is essential to software development, helping ensure that\nnew code is properly integrated. However, the CR process often involves\nsignificant effort, including code adjustments, responses to reviewers, and\ncontinued implementation. While past studies have examined CR delays and\niteration counts, few have investigated the effort based on the volume of code\nchanges required, especially in the context of GitLab Merge Requests (MRs),\nwhich remains underexplored. In this paper, we define and measure CR effort as\nthe amount of code modified after submission, using a dataset of over 23,600\nMRs from four GitLab projects. We find that up to 71% of MRs require\nadjustments after submission, and 28% of these involve changes to more than 200\nlines of code. Surprisingly, this effort is not correlated with review time or\nthe number of participants. To better understand and predict CR effort, we\ntrain an interpretable machine learning model using metrics across multiple\ndimensions: text features, code complexity, developer experience, review\nhistory, and branching. Our model achieves strong performance (AUC 0.84-0.88)\nand reveals that complexity, experience, and text features are key predictors.\nHistorical project characteristics also influence current review effort. Our\nfindings highlight the feasibility of using machine learning to explain and\nanticipate the effort needed to integrate code changes during review.", "AI": {"tldr": "This paper shows that substantial code changes are often needed after code review in GitLab MRs, and that these changes can be predicted with high accuracy using machine learning models that consider code, developer, and historical project features.", "motivation": "Code review is a crucial but labor-intensive part of software development, yet little is known about how much effort measured in code changes is required after review, especially for GitLab Merge Requests. The motivation is to better understand, quantify, and predict this effort to streamline the code review process.", "method": "The authors quantified code review effort as the amount of code modified after submission, analyzing a dataset of 23,600+ GitLab Merge Requests from four projects. They examined the relationship between CR effort and factors like review time and participants. They trained an interpretable machine learning model using metrics (text features, code complexity, developer experience, review history, and branching) to predict review effort.", "result": "Up to 71% of MRs require post-submission adjustments; 28% of these require large changes (>200 lines). CR effort is not correlated with review time or number of participants. The machine learning model predicted review effort with strong accuracy (AUC 0.84-0.88), with complexity, developer experience, and text features as the strongest predictors. Project history also impacts effort.", "conclusion": "Significant post-submission code modifications are frequent in code review. Predicting review effort with interpretable machine learning is feasible and can highlight important contributing factors, enabling teams to anticipate and manage integration effort more effectively."}}
