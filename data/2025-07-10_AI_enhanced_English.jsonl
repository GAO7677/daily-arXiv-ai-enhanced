{"id": "2507.06343", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.06343", "abs": "https://arxiv.org/abs/2507.06343", "authors": ["Huynh Khanh Vi Tran", "Nauman bin Ali", "Michael Unterkalmsteiner", "J\u00fcrgen B\u00f6rstler", "Panagiota Chatzipetrou"], "title": "Quality attributes of test cases and test suites -- importance & challenges from practitioners' perspectives", "comment": null, "summary": "Context: The quality of the test suites and the constituent test cases\nsignificantly impacts confidence in software testing. While research has\nidentified several quality attributes of test cases and test suites, there is a\nneed for a better understanding of their relative importance in practice.\nObjective: We investigate practitioners' perceptions regarding the relative\nimportance of quality attributes of test cases and test suites and the\nchallenges they face in ensuring the perceived important quality attributes.\nMethod: We conducted an industrial survey using a questionnaire based on the\nquality attributes identified in an extensive literature review. We used a\nsampling strategy that leverages LinkedIn to draw a large and heterogeneous\nsample of professionals with experience in software testing. Results: We\ncollected 354 responses from practitioners with a wide range of experience. We\nfound that the majority of practitioners rated Fault Detection, Usability,\nMaintainability, Reliability, and Coverage to be the most important quality\nattributes. Resource Efficiency, Reusability, and Simplicity received the most\ndivergent opinions, which, according to our analysis, depend on the\nsoftware-testing contexts. We identified common challenges that apply to the\nimportant attributes, namely inadequate definition, lack of useful metrics,\nlack of an established review process, and lack of external support.\nConclusion: The findings point out where practitioners actually need further\nsupport with respect to achieving high-quality test cases and test suites under\ndifferent software testing contexts. The findings can serve as a guideline for\nacademic researchers when looking for research directions on the topic. The\nfindings can also be used to encourage companies to provide more support to\npractitioners to achieve high-quality test cases and test suites.", "AI": {"tldr": "A large-scale survey of software testers found that practitioners value certain test suite and case attributes (like Fault Detection and Usability) most, but face common challenges achieving them. These insights can guide future research and company support efforts.", "motivation": "While several quality attributes of test cases and test suites are known, their relative importance in real-world practice is less clear. There is a need to understand which attributes practitioners value most and what challenges they face in achieving these attributes.", "method": "The authors conducted an industrial survey using a questionnaire that was informed by a comprehensive literature review. The survey targeted a large, diverse group of software testing professionals via LinkedIn, ultimately gathering responses from 354 practitioners with varied experience levels.", "result": "Practitioners identified Fault Detection, Usability, Maintainability, Reliability, and Coverage as the most important quality attributes for test cases and test suites. Opinions diverged most for Resource Efficiency, Reusability, and Simplicity, likely due to differing software testing contexts. Main challenges included inadequate attribute definition, lack of useful metrics, absence of a review process, and insufficient external support.", "conclusion": "The study pinpoints areas where practitioners need more support to achieve high-quality test cases and test suites, highlighting gaps between recognized attributes and actual practices. The results offer practical guidance for both future research and industry practices to better support software testers."}}
{"id": "2507.06354", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.06354", "abs": "https://arxiv.org/abs/2507.06354", "authors": ["Huynh Khanh Vi Tran", "Nauman bin Ali", "Michael Unterkalmsteiner", "J\u00fcrgen B\u00f6rstler"], "title": "A proposal and assessment of an improved heuristic for the Eager Test smell detection", "comment": null, "summary": "Context: The evidence for the prevalence of test smells at the unit testing\nlevel has relied on the accuracy of detection tools, which have seen intense\nresearch in the last two decades. The Eager Test smell, one of the most\nprevalent, is often identified using simplified detection rules that\npractitioners find inadequate. Objective: We aim to improve the rules for\ndetecting the Eager Test smell. Method: We reviewed the literature on test\nsmells to analyze the definitions and detection rules of the Eager Test smell.\nWe proposed a novel, unambiguous definition of the test smell and a heuristic\nto address the limitations of the existing rules. We evaluated our heuristic\nagainst existing detection rules by manually applying it to 300 unit test cases\nin Java. Results: Our review identified 56 relevant studies. We found that\ninadequate interpretations of original definitions of the Eager Test smell led\nto imprecise detection rules, resulting in a high level of disagreement in\ndetection outcomes. Also, our heuristic detected patterns of eager and\nnon-eager tests that existing rules missed. Conclusion: Our heuristic captures\nthe essence of the Eager Test smell more precisely; hence, it may address\npractitioners' concerns regarding the adequacy of existing detection rules.", "AI": {"tldr": "Current methods for detecting the Eager Test smell in unit tests are inadequate. This paper proposes a clearer definition and a new heuristic, which more accurately and consistently identifies these smells in Java unit tests than previous rules.", "motivation": "The motivation is to address the inadequacy of current detection rules for the Eager Test smell in unit tests, as practitioners find existing methods insufficient and leading to inconsistent detection results.", "method": "The authors reviewed the literature to analyze definitions and detection rules for the Eager Test smell, proposed a novel and clear definition along with a heuristic to improve detection, and manually applied this heuristic to 300 Java unit test cases to evaluate its effectiveness against existing rules.", "result": "The study found that previous interpretations and detection rules were often imprecise, resulting in inconsistent outcomes. The new heuristic successfully identified both eager and non-eager tests that existing rules missed, capturing the Eager Test smell more accurately.", "conclusion": "The proposed heuristic provides a more precise and practitioner-aligned method for detecting the Eager Test smell, potentially solving earlier problems with inadequate detection and improving consistency in smell identification."}}
{"id": "2507.06463", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.06463", "abs": "https://arxiv.org/abs/2507.06463", "authors": ["Atieh Barati Nia", "Mohammad Dindoost", "David A. Bader"], "title": "Evaluating Efficiency and Novelty of LLM-Generated Code for Graph Analysis", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to automate software\ndevelopment, yet most prior evaluations focus on functional correctness or\nhigh-level languages such as Python. We present the first systematic study of\nLLMs' ability to generate efficient C implementations of graph-analysis\nroutines--code that must satisfy the stringent runtime and memory constraints.\nEight state-of-the-art models (OpenAI ChatGPT o3 and o4-mini-high, Anthropic\nClaude 4 Sonnet and Sonnet Extended, Google Gemini 2.5 Flash and Pro, xAI Grok\n3-Think, and DeepSeek DeepThink R1) are benchmarked by two distinct approaches.\nThe first approach checks the ability of LLMs in generating an algorithm\noutperforming other present algorithms in the benchmark. The second approach\nevaluates the ability of LLMs to generate graph algorithms for integration into\nthe benchmark. Results show that Claude Sonnet 4 Extended achieves the best\nresult in the case of ready-to-use code generation and efficiency,\noutperforming human-written baselines in triangle counting. The study confirms\nthat contemporary LLMs excel at optimizing and integrating established\nalgorithms but not inventing novel techniques. We provide prompts, the first\napproach's generated code, and measurement scripts to foster reproducible\nresearch.", "AI": {"tldr": "The paper systematically benchmarks top LLMs on efficient C code generation for graph analysis. Claude Sonnet 4 Extended outperforms others and even beats humans on some tasks, showing LLMs are strong in optimizing known algorithms but weak at inventing new ones.", "motivation": "Large Language Models (LLMs) are increasingly utilized for automating software development, but prior research has primarily focused on functional correctness or on high-level languages like Python, leaving a gap in understanding their practical capability for generating efficient, low-level code, especially under strict runtime and memory demands.", "method": "The authors conducted a systematic evaluation of eight leading LLMs by tasking them with generating efficient C code for graph-analysis routines. They benchmarked the models with two approaches: (1) generating algorithms that outperform current ones in benchmarks, and (2) creating graph algorithms suitable for direct integration. Performance was compared against human-written baselines, focusing on efficiency and ease of integration.", "result": "Claude Sonnet 4 Extended achieved the best results for ready-to-use code generation and efficiency, even surpassing human-written baselines in the triangle counting task. The study found that while modern LLMs are effective at optimizing and integrating existing algorithms, they struggle to create genuinely novel methods.", "conclusion": "Contemporary LLMs can match or exceed human experts in implementing and optimizing established algorithms under strict performance constraints but are not yet capable of inventing fundamentally new techniques. The research also supports reproducibility by providing resources like prompts, generated code, and measurement scripts."}}
{"id": "2507.06704", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.06704", "abs": "https://arxiv.org/abs/2507.06704", "authors": ["Lloyd Montgomery"], "title": "Issue Tracking Ecosystems: Context and Best Practices", "comment": "300 pages, Dissertation for the doctoral degree Dr. rer. nat. at the\n  Faculty of Mathematics, Informatics, and Natural Sciences, Department of\n  Informatics, University of Hamburg, Hamburg, Germany", "summary": "Issue Tracking Systems (ITSs), such as GitHub and Jira, are popular tools\nthat support Software Engineering (SE) organisations through the management of\n``issues'', which represent different SE artefacts such as requirements,\ndevelopment tasks, and maintenance items. ITSs also support internal linking\nbetween issues, and external linking to other tools and information sources.\nThis provides SE organisations key forms of documentation, including forwards\nand backwards traceability (e.g., Feature Requests linked to sprint releases\nand code commits linked to Bug Reports). An Issue Tracking Ecosystem (ITE) is\nthe aggregate of the central ITS and the related SE artefacts, stakeholders,\nand processes -- with an emphasis on how these contextual factors interact with\nthe ITS. The quality of ITEs is central to the success of these organisations\nand their software products. There are challenges, however, within ITEs,\nincluding complex networks of interlinked artefacts and diverse workflows.\nWhile ITSs have been the subject of study in SE research for decades, ITEs as a\nwhole need further exploration.\n  In this thesis, I undertake the challenge of understanding ITEs at a broader\nlevel, addressing these questions regarding complexity and diversity. I\ninterviewed practitioners and performed archival analysis on a diverse set of\nITSs. These analyses revealed the context-dependent nature of ITE problems,\nhighlighting the need for context-specific ITE research. While previous work\nhas produced many solutions to specific ITS problems, these solutions are not\nconsistently framed in a context-rich and comparable way, leading to a desire\nfor more aligned solutions across research and practice. To address this\nemergent information and lack of alignment, I created the Best Practice\nOntology for ITEs. <... truncated due to arXiv abstract character limit ...>", "AI": {"tldr": "This thesis explores the complexity and diversity of Issue Tracking Ecosystems in software engineering, showing existing solutions lack context-awareness. It introduces a Best Practice Ontology to help align research and practice for better management of these complex systems.", "motivation": "Issue Tracking Systems (ITSs) like GitHub and Jira are central to software engineering organizations, supporting the management and traceability of various software artifacts and tasks. However, the broader ecosystem\u2014termed the Issue Tracking Ecosystem (ITE)\u2014encounters challenges related to the complexity and diversity of interconnected artifacts, stakeholders, and workflows. There is a gap in research addressing these broader, context-dependent issues in ITEs.", "method": "The author interviewed practitioners and conducted archival analyses on a diverse set of ITSs to understand their broader ecosystems. The research focused on surfacing challenges and context-dependent problems within ITEs by leveraging qualitative and archival data.", "result": "Findings indicate that ITE problems are highly context-dependent and that previous research has not sufficiently addressed this complexity. Existing solutions often lack a context-rich, comparable framework. To bridge this gap, the author developed the Best Practice Ontology for ITEs.", "conclusion": "A deeper and more context-sensitive understanding of ITEs is essential. The creation of a Best Practice Ontology represents a step towards more consistent and aligned research and practice in the field, paving the way for improved frameworks and solutions for managing ITEs."}}
{"id": "2507.06360", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.06360", "abs": "https://arxiv.org/abs/2507.06360", "authors": ["Dustin Jamner", "Gabriel Kammer", "Ritam Nag", "Adam Chlipala"], "title": "Pyrosome: Verified Compilation for Modular Metatheory", "comment": null, "summary": "We present Pyrosome, a generic framework for modular language metatheory that\nembodies a novel approach to extensible semantics and compilation, implemented\nin Coq. Common techniques for semantic reasoning are often tied to the specific\nstructures of the languages and compilers that they support. In Pyrosome,\nverified compilers are fully extensible, meaning that to extend a language\n(even with a new kind of effect) simply requires defining and verifying the\ncompilation of the new feature, reusing the old correctness theorem for all\nother cases. The novel enabling idea is an inductive formulation of equivalence\npreservation that supports the addition of new rules to the source language,\ntarget language, and compiler.\n  Pyrosome defines a formal, deeply embedded notion of programming languages\nwith semantics given by dependently sorted equational theories, so all\ncompiler-correctness proofs boil down to type-checking and equational\nreasoning. We support vertical composition of any compilers expressed in our\nframework in addition to feature extension. As a case study, we present a\nmultipass compiler from System F with simple references, through CPS\ntranslation and closure conversion. Specifically, we demonstrate how we can\nbuild such a compiler incrementally by starting with a compiler for simply\ntyped lambda-calculus and adding natural numbers, the unit type, recursive\nfunctions, and a global heap, then extending judgments with a type environment\nand adding type abstraction, all while reusing the original theorems. We also\npresent a linear version of the simply typed CPS pass and compile a small\nimperative language to the simply typed target to show how Pyrosome handles\nsubstructural typing and imperative features.", "AI": {"tldr": "Pyrosome is a Coq-based framework for modular language metatheory that simplifies the development and extension of verified compilers. It allows for easy language and compiler extension, reusing existing correctness proofs, and supports both functional and imperative language features through a flexible, deeply embedded approach.", "motivation": "Current techniques for semantic reasoning about programming languages and their compilers are often tightly coupled to specific languages or compiler structures, making extensibility and reuse of proofs difficult. There is a need for a flexible framework that supports easy language and compiler extension while preserving correctness guarantees.", "method": "The authors propose Pyrosome, a framework implemented in Coq, that enables modular language metatheory using a novel inductive formulation of equivalence preservation. Pyrosome represents programming languages as deeply embedded systems with semantics described by dependently sorted equational theories. The framework allows for the vertical composition of compilers and incremental extension of languages and their compilers, while preserving and reusing existing correctness theorems.", "result": "Pyrosome allows verified compilers to be fully extensible; adding new features only requires defining and verifying the compilation of the new feature, with existing correctness proofs remaining valid for unchanged aspects. As a demonstration, the authors start from a compiler for simply typed lambda calculus and incrementally add features such as natural numbers, recursive functions, and a heap, extending judgments as needed. They also present a linear-style CPS translation and compilation of a small imperative language with substructural typing, showing the versatility of the framework.", "conclusion": "Pyrosome provides an effective, extensible foundation for formalizing language semantics and verified compilation. It enables modular reasoning and proof reuse across language extensions, supporting both functional and imperative features, and simplifies the development and verification of complex, extensible compilers."}}
{"id": "2507.06762", "categories": ["cs.SE", "K.6.3"], "pdf": "https://arxiv.org/pdf/2507.06762", "abs": "https://arxiv.org/abs/2507.06762", "authors": ["Nathalia Barbosa", "Paulo Borba", "L\u00e9uson Da Silva"], "title": "Leveraging LLMs for Semantic Conflict Detection via Unit Test Generation", "comment": "Comments: 11 pages, in Portuguese language. 3 figures. Submitted to\n  SAST 2025 (X Simp\\'osio Brasileiro de Teste de Software Sistem\\'atico e\n  Automatizado)", "summary": "Semantic conflicts arise when a developer introduces changes to a codebase\nthat unintentionally affect the behavior of changes integrated in parallel by\nother developers. Traditional merge tools are unable to detect such conflicts,\nso complementary tools like SMAT have been proposed. SMAT relies on generating\nand executing unit tests: if a test fails on the base version, passes on a\ndeveloper's modified version, but fails again after merging with another\ndeveloper's changes, a semantic conflict is indicated. While SMAT is effective\nat detecting conflicts, it suffers from a high rate of false negatives, partly\ndue to the limitations of unit test generation tools such as Randoop and\nEvosuite. To investigate whether large language models (LLMs) can overcome\nthese limitations, we propose and integrate a new test generation tool based on\nCode Llama 70B into SMAT. We explore the model's ability to generate tests\nusing different interaction strategies, prompt contents, and parameter\nconfigurations. Our evaluation uses two samples: a benchmark with simpler\nsystems from related work, and a more significant sample based on complex,\nreal-world systems. We assess the effectiveness of the new SMAT extension in\ndetecting conflicts. Results indicate that, although LLM-based test generation\nremains challenging and computationally expensive in complex scenarios, there\nis promising potential for improving semantic conflict detection.\n  --\n  Conflitos sem^anticos surgem quando um desenvolvedor introduz mudan\\c{c}as em\numa base de c\\'odigo que afetam, de forma n~ao intencional, o comportamento de\naltera\\c{c}~oes integradas em paralelo por outros desenvolvedores. Ferramentas\ntradicionais de merge n~ao conseguem detectar esse tipo de conflito, por isso\nferramentas complementares como o SMAT foram propostas. O SMAT depende da\ngera\\c{c}~ao e execu\\c{c}~ao de testes de unidade: se um teste falha na vers~ao\nbase, passa na vers~ao modificada por um desenvolvedor, mas volta a falhar\nap\\'os o merge com as mudan\\c{c}as de outro desenvolvedor, um conflito\nsem^antico \\'e identificado. Embora o SMAT seja eficaz na detec\\c{c}~ao de\nconflitos, apresenta alta taxa de falsos negativos, em parte devido \\`as\nlimita\\c{c}~oes das ferramentas de gera\\c{c}~ao de testes como Randoop e\nEvosuite. Para investigar se modelos de linguagem de grande porte (LLMs) podem\nsuperar essas limita\\c{c}~oes, propomos e integramos ao SMAT uma nova\nferramenta de gera\\c{c}~ao de testes baseada no Code Llama 70B. Exploramos a\ncapacidade do modelo de gerar testes utilizando diferentes estrat\\'egias de\nintera\\c{c}~ao, conte\\'udos de prompts e configura\\c{c}~oes de par^ametros.\nNossa avalia\\c{c}~ao utiliza duas amostras: um benchmark com sistemas mais\nsimples, usados em trabalhos relacionados, e uma amostra mais significativa\nbaseada em sistemas complexos e reais. Avaliamos a efic\\'acia da nova extens~ao\ndo SMAT na detec\\c{c}~ao de conflitos. Os resultados indicam que, embora a\ngera\\c{c}~ao de testes por LLM em cen\\'arios complexos ainda seja desafiadora e\ncustosa computacionalmente, h\\'a potencial promissor para aprimorar a\ndetec\\c{c}~ao de conflitos sem^anticos.", "AI": {"tldr": "This paper proposes using a large language model (Code Llama 70B) to generate unit tests for detecting code merge semantic conflicts, integrated into the SMAT tool. While generating tests with LLMs is still challenging for complex projects, initial results are promising for better conflict detection.", "motivation": "Traditional merge tools cannot detect semantic conflicts in code that emerge when developers make parallel changes. While tools like SMAT exist, their dependency on automated unit test generation tools with limitations results in many false negatives.", "method": "The authors proposed a novel unit test generation tool for semantic conflict detection in code merges, utilizing Code Llama 70B (a large language model). They integrated this tool into SMAT and evaluated it using both simple benchmarks and complex real-world systems, varying interaction strategies, prompts, and configuration parameters.", "result": "LLM-based (Code Llama 70B) test generation is challenging and computationally expensive for complex scenarios, but shows promise for improving the detection of semantic conflicts compared to previous unit test generation methods.", "conclusion": "Integrating LLM-based test generation with SMAT has potential for enhancing semantic conflict detection, although technical challenges remain, particularly in more complex software systems."}}
{"id": "2507.06456", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.06456", "abs": "https://arxiv.org/abs/2507.06456", "authors": ["Scott Kovach", "Praneeth Kolichala", "Kyle A. Miller", "David Broman", "Fredrik Kjolstad"], "title": "Fast Collection Operations from Indexed Stream Fusion", "comment": null, "summary": "We present a system of efficient methods for traversing and combining\nassociative collection data structures. A distinguishing feature of the system\nis that, like traditional sequential iterator libraries, it does not require\nspecialized compiler infrastructure or staged compilation for efficiency and\ncomposability. By using a representation based on indexed streams, the library\ncan express complex joins over input collections while using no intermediate\nallocations. We implement the library for the Lean, Morphic, and Rust\nprogramming languages and provide a mechanized proof of functional correctness\nin Lean.", "AI": {"tldr": "The paper presents a system for efficiently traversing and combining associative collection data structures. Unlike other approaches, it does not require special compiler support. It uses indexed streams to avoid extra allocations, is implemented in Lean, Morphic, and Rust, and is formally proven correct in Lean.", "motivation": "Current approaches for traversing and combining associative collection data structures often require specialized compiler features or staged compilation to achieve both efficiency and composability. There is a need for a method that simplifies library usage without sacrificing performance or requiring complex infrastructure.", "method": "The authors design a system using indexed streams to represent traversals and combinations of associative collections. This system eliminates the need for intermediate allocations and does not depend on specialized compiler support. The library is implemented in Lean, Morphic, and Rust, with functional correctness formally proven in Lean.", "result": "The proposed library efficiently supports complex joins on a variety of input collections without intermediate allocations or the need for advanced compiler infrastructure. Its functional correctness is validated by a mechanized proof in Lean.", "conclusion": "The presented system offers an efficient, composable, and practically implementable solution for associative collection traversal and combination across multiple languages, verified by rigorous formal proof."}}
{"id": "2507.06881", "categories": ["cs.SE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.06881", "abs": "https://arxiv.org/abs/2507.06881", "authors": ["Brian R Larson", "Ehsan Ahmad"], "title": "Formalization of the AADL Run-Time Services with Time", "comment": "35 pages, 13 figures", "summary": "The Architecture Analysis & Design Language (AADL) is an architecture\ndescription language for design of cyber-physical systems--machines controlled\nby software. The AADL standard, SAE International AS5506D, describes Run-Time\nServices (RTS) to be provided to execute AADL models in accordance with\nsemantics defined by the standard. The RTS of primary concern are transport\nservices and timing services. Although, the study presented in [1] sets a\nfoundation for the formal semantics of AADL, but without modeling time. This\npaper extends and simplifies this formalization using a modal logic defined by\na Kripke structure, to explicitly include time. The RTS defined in the AADL\nstandard are also expanded to support reactive state-transition machines of the\nBehavior Specification annex standard language (BA) and its closely-related,\nformally-defined counterpart, the Behavior Language for Embedded Systems with\nSoftware (BLESS). An example of AADL RTS with time, implemented by the High\nAssurance Modeling and Rapid Engineering for Embedded Systems (HAMR) for\nstate-transition machine behavior written in BLESS, is also presented.", "AI": {"tldr": "This paper extends previous AADL formalizations to explicitly include time, using modal logic and Kripke structures, and broadens runtime service support to reactive state-transition machines. Validation is provided through a HAMR-based example for BLESS behaviors.", "motivation": "Previous formalizations of the Architecture Analysis & Design Language (AADL) did not account for modeling time, despite time being crucial for cyber-physical systems. The paper aims to address this gap by incorporating explicit timing into the AADL's runtime services formalization.", "method": "The authors extend previous formalization efforts using modal logic defined by a Kripke structure to explicitly include time in the AADL model semantics. They also expand and adapt runtime services to support reactive state-transitions as specified in both the BA annex and the BLESS language. An example implementation using HAMR for BLESS is provided.", "result": "The paper provides a simplified and time-explicit formalization of AADL runtime services. It extends the semantics to support reactive state-transition machines via the BA annex and BLESS. The approach is validated with a HAMR implementation example.", "conclusion": "By integrating explicit time modeling into AADL\u2019s formal semantics and extending support for reactive behaviors (BA and BLESS), the paper strengthens the foundation for precise design and analysis of cyber-physical systems using AADL."}}
{"id": "2507.06584", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.06584", "abs": "https://arxiv.org/abs/2507.06584", "authors": ["Qiong Feng", "Xiaotian Ma", "Ziyuan Feng", "Marat Akhin", "Wei Song", "Peng Liang"], "title": "Finding Compiler Bugs through Cross-Language Code Generator and Differential Testing", "comment": "The 40th ACM SIGPLAN International Conference on Object-Oriented\n  Programming, Systems, Languages, and Applications (OOPSLA)", "summary": "Compilers play a central role in translating high-level code into executable\nprograms, making their correctness essential for ensuring code safety and\nreliability. While extensive research has focused on verifying the correctness\nof compilers for single-language compilation, the correctness of cross-language\ncompilation - which involves the interaction between two languages and their\nrespective compilers - remains largely unexplored. To fill this research gap,\nwe propose CrossLangFuzzer, a novel framework that introduces a universal\nintermediate representation (IR) for JVM-based languages and automatically\ngenerates cross-language test programs with diverse type parameters and complex\ninheritance structures. After generating the initial IR, CrossLangFuzzer\napplies three mutation techniques - LangShuffler, FunctionRemoval, and\nTypeChanger - to enhance program diversity. By evaluating both the original and\nmutated programs across multiple compiler versions, CrossLangFuzzer\nsuccessfully uncovered 10 confirmed bugs in the Kotlin compiler, 4 confirmed\nbugs in the Groovy compiler, 7 confirmed bugs in the Scala 3 compiler, 2\nconfirmed bugs in the Scala 2 compiler, and 1 confirmed bug in the Java\ncompiler. Among all mutators, TypeChanger is the most effective, detecting 11\nof the 24 compiler bugs. Furthermore, we analyze the symptoms and root causes\nof cross-compilation bugs, examining the respective responsibilities of\nlanguage compilers when incorrect behavior occurs during cross-language\ncompilation. To the best of our knowledge, this is the firstwork specifically\nfocused on identifying and diagnosing compiler bugs in cross-language\ncompilation scenarios. Our research helps to understand these challenges and\ncontributes to improving compiler correctness in multi-language environments.", "AI": {"tldr": "The paper introduces a novel fuzzing framework for finding bugs in compilers used for cross-language JVM-based development, successfully uncovering multiple bugs and highlighting the importance of rigorous cross-language compiler verification.", "motivation": "While compiler correctness is vital for reliable code, most prior research addresses only single-language compilation, ignoring the growing importance of cross-language compilation scenarios that occur when different JVM-based languages interact. This leaves a major gap in ensuring multi-language code reliability.", "method": "The authors present CrossLangFuzzer, a new framework that uses a universal intermediate representation (IR) for JVM languages to automatically generate and mutate cross-language test programs. The approach includes three mutation strategies (LangShuffler, FunctionRemoval, and TypeChanger) to increase test diversity, and executes both the original and mutated programs across several compiler versions to find bugs.", "result": "CrossLangFuzzer discovered 24 compiler bugs across major JVM-based languages (Kotlin, Groovy, Scala 2, Scala 3, Java), with the TypeChanger mutator proving to be the most effective. The authors also provide an analysis of the symptoms and causes of the cross-language compilation bugs and highlight how responsibilities are divided between compilers.", "conclusion": "This work pioneers systematic testing for cross-language compiler correctness, demonstrating the effectiveness of IR-based fuzzing for discovering real bugs, and lays groundwork for higher reliability in multi-language software development environments."}}
{"id": "2507.06980", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.06980", "abs": "https://arxiv.org/abs/2507.06980", "authors": ["Binquan Zhang", "Li Zhang", "Zhiwen Luo", "Yuxin Du", "Fang Liu", "Song Wang", "Lin Shi"], "title": "Are They All Good? Evaluating the Quality of CoTs in LLM-based Code Generation", "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance in code\ngeneration, particularly when augmented with chain-of-thought (CoT) prompting\ntechniques. They break down requirements into intermediate reasoning steps,\nwhich act as design rationales to guide LLMs in writing code like human\nprogrammers. Thus, the quality of these steps is crucial for ensuring the\ncorrectness and reliability of the generated code. However, little is known\nabout the quality of CoT generated by LLMs. To what extent can we trust the\nthoughts generated by LLMs? How good are they? This paper empirically explores\nthe external and internal factors of why LLMs generate unsatisfactory CoTs by\nanalyzing 1,023 failed code samples on two widely used code generation\nbenchmarks. We also evaluate their impact on code generation performance by\nanalyzing 210 CoT-code pairs and refining the unsatisfied CoTs by prompting\nLLMs. Our study reveals three key findings: (1) External factors (53.60%), such\nas unclear requirements and lack of context, mainly affect CoT quality, while\ninternal factors (40.10%) stem from LLMs' misunderstanding prompts. (2) Even\nwhen CoTs are correct, 18.5% of the generated code contains errors due to\ninstruction-following issues; conversely, 11.90% of correct code is paired with\nflawed CoTs. (3) Refining low-quality CoTs is feasible, i.e., LLMs improve when\ngiven detailed problem descriptions. These findings highlight key challenges in\nCoT-based code generation and suggest directions for improving LLM reasoning\nand reliability.", "AI": {"tldr": "The paper analyzes why chain-of-thought (CoT) prompting sometimes fails in code generation with LLMs, identifies key reasons (mostly vague task descriptions and some model misunderstanding), and shows that clarifying problems helps\u2014improving prompt and task design is key to more reliable LLM code generation.", "motivation": "The motivation is to understand and improve the reliability and quality of chain-of-thought (CoT) prompting in large language models (LLMs) for code generation tasks, since CoTs act as intermediate reasoning steps that are crucial to generating correct and reliable code.", "method": "The authors conduct an empirical analysis by examining 1,023 failed code samples from two common code generation benchmarks to explore both external and internal reasons behind unsatisfactory CoTs. They further study the impact of CoT quality by analyzing 210 CoT-code pairs and test the effects of refining unsatisfactory CoTs via additional LLM prompting.", "result": "(1) Most CoT quality issues come from external factors such as vague requirements and lack of context (53.60%), followed by internal misunderstandings by LLMs (40.10%). (2) 18.5% of code generated from correct CoTs still contains errors due to poor instruction following, whereas 11.90% of correct outputs are nevertheless based on flawed CoTs. (3) Improving CoT quality by refining prompts and problem descriptions leads to better LLM performance in code generation.", "conclusion": "CoT quality in LLM-based code generation is affected mostly by external task formulation factors, but also by internal misunderstanding. Even with correct reasoning, errors in code can arise. Refining CoTs by clarifying problem descriptions improves reliability, indicating that better task definition and prompting strategies are needed for robust CoT-based code generation."}}
{"id": "2507.06939", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.06939", "abs": "https://arxiv.org/abs/2507.06939", "authors": ["Guilherme Espada", "Alcides Fonseca"], "title": "Sound Interval-Based Synthesis for Probabilistic Programs", "comment": null, "summary": "Probabilistic programming has become a standard practice to model stochastic\nevents and learn about the behavior of nature in different scientific contexts,\nranging from Genetics and Ecology to Linguistics and Psychology. However,\ndomain practitioners (such as biologists) also need to be experts in statistics\nin order to select which probabilistic model is suitable for a given particular\nproblem, relying then on probabilistic inference engines such as Stan, Pyro or\nEdward to fine-tune the parameters of that particular model. Probabilistic\nProgramming would be more useful if the model selection is made automatic,\nwithout requiring statistics expertise from the end user. Automatically\nselecting the model is challenging because of the large search space of\nprobabilistic programs needed to be explored, because the fact that most of\nthat search space contains invalid programs, and because invalid programs may\nonly be detected in some executions, due to its probabilistic nature. We\npropose a type system to statically reject invalid probabilistic programs, a\ntype-directed synthesis algorithm that guarantees that generated programs are\ntype-safe by construction, and an heuristic search procedure to handle the vast\nsearch space. We collect a number of probabilistic programs from the\nliterature, and use them to compare our method with both a type-agnostic random\nsearch, and a data-guided method from the literature (DaPPer). Our results show\nthat our technique both outperforms random search and DaPPer, specially on more\ncomplex programs. This drastic performance difference in synthesis allows for\nfast sampling of programs and enables techniques that previously suffered from\nthe complexity of synthesis, such as Genetic Programming, to be applied.", "AI": {"tldr": "The paper introduces a type-directed synthesis method for probabilistic programming, which automatically generates valid and type-safe programs, outperforming random and data-guided methods, and making probabilistic modeling more accessible to non-statisticians.", "motivation": "Domain practitioners often lack deep statistical expertise, making it difficult to select suitable probabilistic models. Current probabilistic programming tools require users to be both domain experts and statistical experts. Automating model selection would make probabilistic programming more accessible, but this is challenging due to the large and complex search space of possible programs, many of which are invalid due to their probabilistic nature.", "method": "The authors propose a type system to statically reject invalid probabilistic programs and a type-directed synthesis algorithm that ensures type-safe programs are generated by construction. Additionally, they implement a heuristic search technique to efficiently explore the vast space of possible programs.", "result": "By evaluating their approach on a collection of probabilistic programs from the literature, the authors show that their method outperforms both a type-agnostic random search and a data-guided method (DaPPer), especially with more complex programs. Their method enables faster program synthesis and makes techniques like Genetic Programming feasible for use in probabilistic programming.", "conclusion": "The proposed method significantly improves the automation, efficiency, and reliability of probabilistic model selection and program synthesis, lowering the barrier for domain experts to apply probabilistic programming without needing deep statistical knowledge."}}
{"id": "2507.07026", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.07026", "abs": "https://arxiv.org/abs/2507.07026", "authors": ["Sadia Afrin Mim", "Fatema Tuz Zohra", "Justin Smith", "Brittany Johnson"], "title": "Exploring Fairness Interventions in Open Source Projects", "comment": "Revised version accepted at the 1st International Workshop on\n  Fairness in Software Systems(SANER 2025)", "summary": "The deployment of biased machine learning (ML) models has resulted in adverse\neffects in crucial sectors such as criminal justice and healthcare. To address\nthese challenges, a diverse range of machine learning fairness interventions\nhave been developed, aiming to mitigate bias and promote the creation of more\nequitable models. Despite the growing availability of these interventions,\ntheir adoption in real-world applications remains limited, with many\npractitioners unaware of their existence. To address this gap, we\nsystematically identified and compiled a dataset of 62 open source fairness\ninterventions and identified active ones. We conducted an in-depth analysis of\ntheir specifications and features to uncover considerations that may drive\npractitioner preference and to identify the software interventions actively\nmaintained in the open source ecosystem. Our findings indicate that 32% of\nthese interventions have been actively maintained within the past year, and 50%\nof them offer both bias detection and mitigation capabilities, mostly during\ninprocessing.", "AI": {"tldr": "The paper analyzed the landscape of 62 open-source ML fairness interventions, finding that only a third are actively maintained and half support both detection and mitigation of bias. Lack of practitioner awareness and tool maintenance limits their practical use.", "motivation": "Biased machine learning models cause adverse effects in important areas such as criminal justice and healthcare. Although many fairness interventions have been developed, practitioners often remain unaware of their existence, limiting their real-world adoption.", "method": "The authors systematically identified and compiled a dataset of 62 open-source machine learning fairness interventions, assessed their activity and maintenance, and performed an in-depth analysis of their features and specifications.", "result": "They found that only 32% of these interventions have been actively maintained in the past year, and half provide both bias detection and mitigation, mostly focusing on in-processing approaches.", "conclusion": "Despite a variety of available open-source fairness interventions, their real-world impact is limited by low adoption and maintenance. Active efforts are needed to increase awareness and support for these tools among practitioners."}}
{"id": "2507.07045", "categories": ["cs.SE", "cs.SI", "68T05", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.07045", "abs": "https://arxiv.org/abs/2507.07045", "authors": ["Ugur Ari"], "title": "5C Prompt Contracts: A Minimalist, Creative-Friendly, Token-Efficient Design Framework for Individual and SME LLM Usage", "comment": "5 pages, 5 tables. Includes comparative experimental results across\n  OpenAI, Anthropic, DeepSeek, and Gemini LLMs", "summary": "The progression from traditional prompt engineering to a more rigorous\ndiscipline of prompt design marks a pivotal shift in human-LLM interaction. As\nLarge Language Models (LLMs) become increasingly embedded in mission-critical\napplications, there emerges a pressing need for frameworks that are not only\nexplicit and systematic but also minimal enough to remain practical and broadly\naccessible. While many existing approaches address prompt structuring through\nelaborate Domain-Specific Languages (DSLs) or multi-layered templates, such\nmethods can impose significant token and cognitive overhead, potentially\nconstraining the model's creative capacity. In this context, we propose the 5C\nPrompt Contract, a framework that distills prompt design into five intuitive\ncomponents: Character, Cause, Constraint, Contingency, and Calibration. This\nminimal cognitive schema explicitly integrates fallback and output optimization\ndirectives, fostering reliable, interpretable, and creatively flexible AI\ninteractions. Experimental results demonstrate that the 5C framework\nconsistently achieves superior input token efficiency while maintaining rich\nand consistent outputs across diverse LLM architectures (OpenAI, Anthropic,\nDeepSeek, and Gemini), making it particularly suited for individuals and\nSmall-to-Medium Enterprises (SMEs) with limited AI engineering resources.", "AI": {"tldr": "The paper proposes the 5C Prompt Contract, a simple framework for designing prompts for LLMs that boosts efficiency and creativity while reducing overhead, making it ideal for users and SMEs with limited resources.", "motivation": "As Large Language Models (LLMs) become increasingly embedded in critical applications, there is a growing need for prompt design frameworks that are systematic, explicit, and practical, without excessive complexity or token overhead.", "method": "The paper introduces the 5C Prompt Contract, a prompt design framework composed of five components: Character, Cause, Constraint, Contingency, and Calibration. This framework aims to integrate fallback and output optimization directly into the prompt, promoting clarity and efficiency.", "result": "Experimental results show that the 5C framework offers superior input token efficiency and maintains rich, consistent outputs across various LLM platforms (OpenAI, Anthropic, DeepSeek, Gemini). It is especially beneficial for users and SMEs with limited resources.", "conclusion": "The 5C Prompt Contract provides a minimal but comprehensive and interpretable schema for prompt design, improving efficiency and flexibility in LLM interactions. It is practical, widely applicable, and supports creative outcomes without introducing significant overhead."}}
