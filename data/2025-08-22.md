<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 10]
- [cs.PL](#cs.PL) [Total: 8]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [On the need to perform comprehensive evaluations of automated program repair benchmarks: Sorald case study](https://arxiv.org/abs/2508.15135)
*Sumudu Liyanage,Sherlock A. Licorish,Markus Wagner,Stephen G. MacDonell*

Main category: cs.SE

TL;DR: APR tools like Sorald can fix code violations but may introduce new faults and degrade code quality. Comprehensive evaluation frameworks are needed to identify both benefits and risks of APR tools.


<details>
  <summary>Details</summary>
Motivation: Automated program repair (APR) tools are increasingly important for improving code quality, especially in the era of large language models (LLMs). However, current evaluations often focus only on whether APR tools remove violations, without considering if the tools introduce new problems or negatively affect the code.

Method: This study develops and applies a comprehensive evaluation framework to assess APR tools, using Sorald as a proof of concept. The framework examines not only the removal of violations but also the introduction of new faults, impact on code correctness, and code structure. The evaluation involved fixing 3,529 SonarQube violations in 2,393 Java code snippets from Stack Overflow.

Result: Sorald was effective at fixing the targeted rule violations, but also introduced 2,120 new faults (32 bugs and 2,088 code smells), caused a 24% unit test failure rate, and degraded code structure.

Conclusion: Evaluation of APR tools should not focus solely on violation clearance but must also consider side effects such as new faults, reduced functional correctness, and code structure degradation to ensure safe and beneficial use.

Abstract: In supporting the development of high-quality software, especially necessary
in the era of LLMs, automated program repair (APR) tools aim to improve code
quality by automatically addressing violations detected by static analysis
profilers. Previous research tends to evaluate APR tools only for their ability
to clear violations, neglecting their potential introduction of new (sometimes
severe) violations, changes to code functionality and degrading of code
structure. There is thus a need for research to develop and assess
comprehensive evaluation frameworks for APR tools. This study addresses this
research gap, and evaluates Sorald (a state-of-the-art APR tool) as a proof of
concept. Sorald's effectiveness was evaluated in repairing 3,529 SonarQube
violations across 30 rules within 2,393 Java code snippets extracted from Stack
Overflow. Outcomes show that while Sorald fixes specific rule violations, it
introduced 2,120 new faults (32 bugs, 2088 code smells), reduced code
functional correctness--as evidenced by a 24% unit test failure rate--and
degraded code structure, demonstrating the utility of our framework. Findings
emphasize the need for evaluation methodologies that capture the full spectrum
of APR tool effects, including side effects, to ensure their safe and effective
adoption.

</details>


### [2] [Foundational Design Principles and Patterns for Building Robust and Adaptive GenAI-Native Systems](https://arxiv.org/abs/2508.15411)
*Frederik Vandeputte*

Main category: cs.SE

TL;DR: This paper advocates combining GenAI abilities with software engineering principles to build robust systems, introduces core design pillars and architecture patterns, and emphasizes further research and validation.


<details>
  <summary>Details</summary>
Motivation: Generative AI (GenAI) offers transformative potential but faces challenges regarding reliability, efficiency, and unpredictability in practical system development.

Method: The paper proposes a paradigm shift combining GenAI's cognitive abilities with traditional software engineering principles. It introduces five design pillars and new architectural patterns (GenAI-native cells, organic substrates, programmable routers) as a framework.

Result: A conceptual framework of GenAI-native systems with design principles and architectural patterns is presented, along with a discussion of technical, user, economic, and legal impacts. The framework's effectiveness remains to be validated and refined.

Conclusion: The paper emphasizes the need for future research and community involvement to implement, experiment with, and improve GenAI-native systems based on the proposed principles.

Abstract: Generative AI (GenAI) has emerged as a transformative technology,
demonstrating remarkable capabilities across diverse application domains.
However, GenAI faces several major challenges in developing reliable and
efficient GenAI-empowered systems due to its unpredictability and inefficiency.
This paper advocates for a paradigm shift: future GenAI-native systems should
integrate GenAI's cognitive capabilities with traditional software engineering
principles to create robust, adaptive, and efficient systems.
  We introduce foundational GenAI-native design principles centered around five
key pillars -- reliability, excellence, evolvability, self-reliance, and
assurance -- and propose architectural patterns such as GenAI-native cells,
organic substrates, and programmable routers to guide the creation of resilient
and self-evolving systems. Additionally, we outline the key ingredients of a
GenAI-native software stack and discuss the impact of these systems from
technical, user adoption, economic, and legal perspectives, underscoring the
need for further validation and experimentation. Our work aims to inspire
future research and encourage relevant communities to implement and refine this
conceptual framework.

</details>


### [3] [An Empirical Study of Knowledge Distillation for Code Understanding Tasks](https://arxiv.org/abs/2508.15423)
*Ruiqi Wang,Zezhou Yang,Cuiyun Gao,Xin Xia,Qing Liao*

Main category: cs.SE

TL;DR: Knowledge distillation, especially feature-based methods with code-specific teacher models, can significantly compress and accelerate code-understanding PLMs, allowing student models to match up to 98% of the teacher's performance while using much fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Deploying pre-trained language models (PLMs) for code understanding faces challenges due to computational demands and latency. Knowledge distillation (KD) promises to make inference more efficient but its potential in code-related tasks is underexplored.

Method: The paper systematically investigates knowledge distillation for code understanding tasks, analyzing both logit-based and feature-based KD methods. Experiments are conducted using eight student models and two teacher PLMs on three downstream tasks, comparing performance across different approaches and architectures.

Result: KD consistently provides significant performance improvements over standard fine-tuning for student models of various sizes. Code-specific teacher PLMs are more effective. Feature-based KD methods enable student models to achieve up to 98% of the teacher performance with only 5% of the parameters. Architectural similarity between teacher and student models does not guarantee better results.

Conclusion: Knowledge distillation is highly effective for compressing and accelerating code-understanding PLMs, with feature-based methods and code-specific teachers offering the best outcomes. Future work should explore deeper aspects of KD efficiency and performance for code tasks.

Abstract: Pre-trained language models (PLMs) have emerged as powerful tools for code
understanding. However, deploying these PLMs in large-scale applications faces
practical challenges due to their computational intensity and inference
latency. Knowledge distillation (KD), a promising model compression and
acceleration technique, addresses these limitations by transferring knowledge
from large teacher models to compact student models, enabling efficient
inference while preserving most of the teacher models' capabilities. While this
technique has shown remarkable success in natural language processing and
computer vision domains, its potential for code understanding tasks remains
largely underexplored.
  In this paper, we systematically investigate the effectiveness and usage of
KD in code understanding tasks. Our study encompasses two popular types of KD
methods, i.e., logit-based and feature-based KD methods, experimenting across
eight student models and two teacher PLMs from different domains on three
downstream tasks. The experimental results indicate that KD consistently offers
notable performance boosts across student models with different sizes compared
with standard fine-tuning. Notably, code-specific PLM demonstrates better
effectiveness as the teacher model. Among all KD methods, the latest
feature-based KD methods exhibit superior performance, enabling student models
to retain up to 98% teacher performance with merely 5% parameters. Regarding
student architecture, our experiments reveal that similarity with teacher
architecture does not necessarily lead to better performance. We further
discuss the efficiency and behaviors in the KD process and inference, summarize
the implications of findings, and identify promising future directions.

</details>


### [4] [SynthCoder: A Synthetical Strategy to Tune LLMs for Code Completion](https://arxiv.org/abs/2508.15495)
*Dongjun Yu,Xiao Yan,Zhenrui Li,Jipeng Xiao,Haochuan He,Yongda Yu,Hao Zhang,Guoping Rong,Xiaobo Huang*

Main category: cs.SE

TL;DR: SynthCoder is a new code completion model that uses advanced data construction, contextual enrichment, and a two-stage training process to outperform existing solutions on multiple benchmarks and addresses common issues like code repetition.


<details>
  <summary>Details</summary>
Motivation: Current optimization methods for code completion models often involve trade-offs, improving performance on some datasets while degrading it on others. There is a need for a solution that achieves state-of-the-art performance without sacrificing consistency across benchmarks.

Method: SynthCoder integrates industry best practices for code completion by constructing a diverse dataset using AST extraction and developer heuristics. It incorporates cross-file context with BM25 and call graph analysis. The training follows a two-stage process: fine-tuning with Curriculum Learning and final alignment using Direct Preference Optimization with preference pairs via Rejection Sampling.

Result: SynthCoder outperforms previous models on repository-level code completion benchmarks (aiXcoder, ExecRepoBench, CrossCodeEval, CoLT). The custom training set also reduces the problem of models repeating existing code.

Conclusion: SynthCoder achieves state-of-the-art code completion performance across metrics and reduces undesirable model tendencies like repetition, demonstrating the effectiveness of its integrated methodology.

Abstract: Code completion is a prominent application of Large Language Models (LLMs) in
software engineering. Due to the near real-time response requirements of this
task, base models with small to medium-sized parameters are typically employed,
supplemented by various optimization and post-training techniques. However,
these optimization methods often have trade-offs, leading to a seesaw effect
where performance improvements on certain datasets or metrics are accompanied
by degradations on others -- sometimes even falling below the baseline model's
performance. This paper proposes SynthCoder, a model that integrates leading
industry practices to achieve state-of-the-art performance on the
Fill-in-the-Middle (FIM) code completion task. In specific, we first construct
a diverse dataset by combining Abstract Syntax Tree (AST) node extraction with
heuristics that simulate developer behavior. Then we enrich our training corpus
with cross-file contextual information using the BM25 algorithm and call
graphs, enhancing the model's ability to perform code completion in both
file-level and repository-level scenarios. As the last step, we employ a
two-stage training process using the Seed-Coder-8B-Base as the base model.
First, we fine-tune the model using Curriculum Learning technology. Following
this, we perform alignment using Direct Preference Optimization (DPO) with
preference pairs generated through Rejection Sampling. Experimental results
demonstrate that our final model excels on mainstream repository-level code
completion benchmarks, including aiXcoder, ExecRepoBench, CrossCodeEval, and
CoLT. Furthermore, our carefully curated training set effectively mitigates the
model's tendency to just repeat existing code, a common issue existing in
various code completion models.

</details>


### [5] [Towards the Assessment of Task-based Chatbots: From the TOFU-R Snapshot to the BRASATO Curated Dataset](https://arxiv.org/abs/2508.15496)
*Elena Masserini,Diego Clerissi,Daniela Micucci,João R. Campos,Leonardo Mariani*

Main category: cs.SE

TL;DR: The paper introduces two datasets of Rasa-based chatbots from GitHub to address the lack of resources in chatbot reliability research, offering curated collections and tools to support further study.


<details>
  <summary>Details</summary>
Motivation: Reliability, security, and robustness of chatbots is underexplored due to absence of large-scale, high-quality datasets.

Method: Presented two datasets (TOFU-R and BRASATO) and tool support for their creation and maintenance.

Result: TOFU-R: Snapshot of Rasa-based chatbots from GitHub; BRASATO: Curated selection of chatbots chosen for dialogue complexity, functional complexity, and utility.

Conclusion: These datasets and the supporting tools ease reproducibility and enable research into chatbot reliability.

Abstract: Task-based chatbots are increasingly being used to deliver real services, yet
assessing their reliability, security, and robustness remains underexplored,
also due to the lack of large-scale, high-quality datasets. The emerging
automated quality assessment techniques targeting chatbots often rely on
limited pools of subjects, such as custom-made toy examples, or outdated, no
longer available, or scarcely popular agents, complicating the evaluation of
such techniques. In this paper, we present two datasets and the tool support
necessary to create and maintain these datasets. The first dataset is RASA
TASK-BASED CHATBOTS FROM GITHUB (TOFU-R), which is a snapshot of the Rasa
chatbots available on GitHub, representing the state of the practice in
open-source chatbot development with Rasa. The second dataset is BOT RASA
COLLECTION (BRASATO), a curated selection of the most relevant chatbots for
dialogue complexity, functional complexity, and utility, whose goal is to ease
reproducibility and facilitate research on chatbot reliability.

</details>


### [6] [Evaluation Guidelines for Empirical Studies in Software Engineering involving LLMs](https://arxiv.org/abs/2508.15503)
*Sebastian Baltes,Florian Angermeir,Chetan Arora,Marvin Muñoz Barón,Chunyang Chen,Lukas Böhme,Fabio Calefato,Neil Ernst,Davide Falessi,Brian Fitzgerald,Davide Fucci,Marcos Kalinowski,Stefano Lambiase,Daniel Russo,Mircea Lungu,Lutz Prechelt,Paul Ralph,Christoph Treude,Stefan Wagner*

Main category: cs.SE

TL;DR: This paper proposes a taxonomy and eight guidelines to increase transparency and reproducibility in software engineering studies involving LLMs, helping researchers navigate and address the unique challenges of working with these models.


<details>
  <summary>Details</summary>
Motivation: Reproducibility and replicability are increasingly difficult for software engineering studies involving LLMs due to factors like model non-determinism, lack of transparency in training data, and constantly evolving architectures.

Method: The paper introduces a taxonomy of LLM-based study types and formulates eight concrete guidelines for designing and reporting empirical studies that integrate LLMs. The guidelines focus on transparency and clear reporting, and have criteria categorized as essential and desired.

Result: A comprehensive set of eight guidelines is provided for empirical SE studies using LLMs, covering aspects like LLM disclosure, version/configuration reporting, prompt/log disclosure, human validation, baselines, benchmarks, and open sharing of limitations. The resources are made available online for community input and ongoing updates.

Conclusion: Adhering to the proposed guidelines and taxonomy will help the research community overcome LLM-specific challenges for reproducibility and replicability, promoting open and transparent science in the context of LLM-based software engineering studies. The guidelines serve as a living resource continually shaped by the community.

Abstract: Large language models (LLMs) are increasingly being integrated into software
engineering (SE) research and practice, yet their non-determinism, opaque
training data, and evolving architectures complicate the reproduction and
replication of empirical studies. We present a community effort to scope this
space, introducing a taxonomy of LLM-based study types together with eight
guidelines for designing and reporting empirical studies involving LLMs. The
guidelines present essential (must) criteria as well as desired (should)
criteria and target transparency throughout the research process. Our
recommendations, contextualized by our study types, are: (1) to declare LLM
usage and role; (2) to report model versions, configurations, and fine-tuning;
(3) to document tool architectures; (4) to disclose prompts and interaction
logs; (5) to use human validation; (6) to employ an open LLM as a baseline; (7)
to report suitable baselines, benchmarks, and metrics; and (8) to openly
articulate limitations and mitigations. Our goal is to enable reproducibility
and replicability despite LLM-specific barriers to open science. We maintain
the study types and guidelines online as a living resource for the community to
use and shape (llm-guidelines.org).

</details>


### [7] [QUPER-MAn: Benchmark-Guided Target Setting for Maintainability Requirements](https://arxiv.org/abs/2508.15512)
*Markus Borg,Martin Larsson,Philip Breid,Nadim Hagatulah*

Main category: cs.SE

TL;DR: Source code maintainability is vital but often overlooked. The authors study industry practices and propose QUPER-MAn, a model for setting maintainability targets, aiming to make maintainability a primary engineering goal.


<details>
  <summary>Details</summary>
Motivation: Maintainability of source code is crucial for long-term software development, yet it often does not get the attention it deserves. The authors are motivated by the need to address this gap and improve how maintainability is handled within requirements engineering.

Method: The authors conduct an exploratory study of industry practices concerning requirements engineering for maintainability. They confirm previous findings that maintainability is often undervalued. To address this, they introduce QUPER-MAn, an adaptation of the QUPER model, developed using a design science approach.

Result: The study finds that maintainability is still a secondary concern in practice. Explicit requirements focus on coding conventions, and maintainability assessment tools are mainly used for implicit requirements. QUPER-MAn is proposed to help organizations set clear, actionable maintainability goals.

Conclusion: QUPER-MAn could help organizations move from neglecting maintainability to actively managing and improving it through informed, responsible engineering decisions.

Abstract: Maintainable source code is essential for sustainable development in any
software organization. Unfortunately, many studies show that maintainability
often receives less attention than its importance warrants. We argue that
requirements engineering can address this gap the problem by fostering
discussions and setting appropriate targets in a responsible manner. In this
preliminary work, we conducted an exploratory study of industry practices
related to requirements engineering for maintainability. Our findings confirm
previous studies: maintainability remains a second-class quality concern.
Explicit requirements often make sweeping references to coding conventions.
Tools providing maintainability proxies are common but typically only used in
implicit requirements related to engineering practices. To address this, we
propose QUPER-MAn, a maintainability adaption of the QUPER model, which was
originally developed to help organizations set targets for performance
requirements. Developed using a design science approach, QUPER-MAn, integrates
maintainability benchmarks and supports target setting. We posit that it can
shift maintainability from an overlooked development consequence to an actively
managed goal driven by informed and responsible engineering decisions.

</details>


### [8] [A Novel Mutation Based Method for Detecting FPGA Logic Synthesis Tool Bugs](https://arxiv.org/abs/2508.15536)
*Yi Zhang,He Jiang,Xiaochen Li,Shikai Guo,Peiyu Zou,Zun Wang*

Main category: cs.SE

TL;DR: VERMEI enhances the testing of FPGA logic synthesis tools, using innovative techniques to generate complex test cases. It outperforms existing methods and has uncovered several new bugs in widely used EDA tools.


<details>
  <summary>Details</summary>
Motivation: Defects in FPGA logic synthesis tools can lead to unpredictable behaviors and security risks, making robust testing essential. Existing automatic testing methods often fail to generate test programs with sufficient semantic and logical complexity.

Method: VERMEI is a new testing method composed of three modules: preprocessing (which identifies inactive code through simulation and coverage analysis), equivalent mutation (which generates complex program variants by modifying zombie logic using Bayesian sampling from historical designs), and bug identification (using differential testing to compare outcomes).

Result: VERMEI was empirically evaluated on popular synthesis tools (Yosys, Vivado, Quartus), outperforming prior approaches. Over five months, it reported 15 bugs, out of which vendors confirmed 9 as new.

Conclusion: VERMEI is a more effective approach for testing FPGA logic synthesis tools, able to uncover complex bugs missed by existing methods, and has real-world impact with several newly identified bugs.

Abstract: FPGA (Field-Programmable Gate Array) logic synthesis tools are key components
in the EDA (Electronic Design Automation) toolchain. They convert hardware
designs written in description languages such as Verilog into gate-level
representations for FPGAs. However, defects in these tools may lead to
unexpected behaviors and pose security risks. Therefore, it is crucial to
harden these tools through testing. Although several methods have been proposed
to automatically test FPGA logic synthesis tools, the challenge remains of
insufficient semantic and logical complexity in test programs. In this paper,
we propose VERMEI, a new method for testing FPGA logic synthesis tools. VERMEI
consists of three modules: preprocessing, equivalent mutation, and bug
identification. The preprocessing module identifies zombie logic (inactive code
with no impact on the circuit output) in seed programs through simulation and
coverage analysis. The equivalent mutation module generates equivalent variants
of seed programs by pruning or inserting logic fragments in zombie areas. It
uses Bayesian sampling to extract logic fragments from historical Verilog
designs, making the generated variants have complex control flows and
structures. The bug identification module, based on differential testing,
compares the synthesized outputs of seed and variant programs to identify bugs.
Experiments on Yosys, Vivado, and Quartus demonstrate that VERMEI outperforms
the state-of-the-art methods. Within five months, VERMEI reported 15 bugs to
vendors, 9 of which were confirmed as new.

</details>


### [9] [Establishing Technical Debt Management -- A Five-Step Workshop Approach and an Action Research Study](https://arxiv.org/abs/2508.15570)
*Marion Wiese,Kamila Serwa,Anastasia Besier,Ariane S. Marion-Jetten,Eva Bittner*

Main category: cs.SE

TL;DR: The paper demonstrates that a hands-on, workshop-based TD management process can boost and sustain awareness of technical debt in development teams, offering actionable techniques for broader application.


<details>
  <summary>Details</summary>
Motivation: Technical debt (TD) management is often researched but seldom adopted in industry, leading to a gap between research findings and practical implementation. The authors aim to bridge this gap by establishing an effective TD management process that can be practically applied in an IT company.

Method: The authors implemented action research involving five action cycles over 16 months with an IT team developing signal processing solutions. They assessed TD awareness through questionnaires, observation of team meetings, a psychology-based method (TD-SAGAT) for measuring awareness, and backlog data evaluation.

Result: Practitioners favored addressing TD items that are easy to fix (low-hanging fruits), and prioritization was based on system evolution and cost calculations. Addition of simple reminders in backlog items (like checkboxes or templates) resulted in sustained growth in TD awareness.

Conclusion: A workshop-based, practical approach to technical debt management is feasible and leads to sustainable changes in team behavior and processes. The study also introduced new, transferable ideas for TDM, such as re-submission dates, dedicated checkboxes, and visual tools for prioritization.

Abstract: Context. Technical debt (TD) items are constructs in a software system
providing short-term benefits but hindering future changes. TD management (TDM)
is frequently researched but rarely adopted in practice. Goal. This study aimed
to establish a TDM process in an IT company based on a predefined workshop
concept. We analyzed which research approaches practitioners adopted for each
TD activity and the TDM's long-term effect on TD awareness. Method. We used
action research (five action cycles in 16 months) with an IT team that creates
IT solutions for signal processing. To examine TD awareness, we (1) analyzed
questionnaires completed during each workshop, (2) observed team meetings, (3)
adopted a method from psychology for measuring awareness in decision-making
situations called TD-SAGAT, and (4) evaluated the backlog data. Results.
Practitioners preferred TD repayment and prioritization based on the system's
evolution and cost calculations, i.e., repayment of so-called low-hanging
fruits. Reminders in the backlog items, such as checkboxes or text templates,
led to a sustainable rise in TD awareness. Conclusions. We showed that a
workshop-based approach is feasible and leads to sustainable process changes.
New ideas for TDM applicable to other IT teams emerged, e.g., using a
re-submission date, using a Talked about TD checkbox, and using visualizations
for TD prioritization.

</details>


### [10] [From PREVENTion to REACTion: Enhancing Failure Resolution in Naval Systems](https://arxiv.org/abs/2508.15584)
*Maria Teresa Rossi,Leonardo Mariani,Oliviero Riganelli*

Main category: cs.SE

TL;DR: This paper explores applying PREVENT (failure prediction) and REACT (troubleshooting) to naval industrial systems, showing successful integration and offering insights for deploying such methods in other industries.


<details>
  <summary>Details</summary>
Motivation: Industrial systems frequently experience failures due to factors like wear, misuse, or faults. Early detection and troubleshooting are crucial to minimize downtime and maintain production efficiency.

Method: The authors utilize a failure prediction method called PREVENT and introduce a troubleshooting module named REACT. They apply these methods to naval systems developed by Fincantieri, integrating anomaly detection with troubleshooting procedures.

Result: The integration of anomaly detection and troubleshooting procedures was successfully demonstrated on naval systems. The study provided practical insights and lessons learned from the deployment of the methods.

Conclusion: The combination of PREVENT and REACT offers a practical approach for early failure prediction and troubleshooting, which can be extended to other industrial systems. The paper discusses lessons learned to facilitate broader adoption.

Abstract: Complex and large industrial systems often misbehave, for instance, due to
wear, misuse, or faults. To cope with these incidents, it is important to
timely detect their occurrences, localize the sources of the problems, and
implement the appropriate countermeasures. This paper reports our experience
with a state-of-the-art failure prediction method, PREVENT, and its extension
with a troubleshooting module, REACT, applied to naval systems developed by
Fincantieri. Our results show how to integrate anomaly detection with
troubleshooting procedures. We conclude by discussing a lesson learned, which
may help deploy and extend these analyses to other industrial products.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [11] [Homomorphism Calculus for User-Defined Aggregations](https://arxiv.org/abs/2508.15109)
*Ziteng Wang,Ruijie Fang,Linus Zheng,Dixin Tang,Isil Dillig*

Main category: cs.PL

TL;DR: This paper introduces a homomorphism calculus to automatically verify if user-defined aggregation functions (UDAFs) used in frameworks like Spark and Flink can be executed efficiently in parallel. Their algorithm not only identifies suitable UDAFs but also constructs merge operators, and achieves better performance than current leading solutions.


<details>
  <summary>Details</summary>
Motivation: Efficient execution of user-defined aggregation functions (UDAFs) in data processing frameworks requires satisfying the homomorphism property, which is essential for merging partial results in parallel and incremental computations. Ensuring and automating the detection of this property is challenging.

Method: The paper introduces a novel homomorphism calculus that can verify or refute if a UDAF is a dataframe homomorphism, and constructs corresponding merge operators for qualifying UDAFs. An algorithm based on this calculus is implemented and evaluated.

Result: The proposed calculus-based algorithm was tested on real-world UDAFs and shown to significantly outperform two leading synthesizers.

Conclusion: The homomorphism calculus enables more effective and automated verification of UDAFs for homomorphism properties, thus facilitating their efficient use in data processing frameworks.

Abstract: Data processing frameworks like Apache Spark and Flink provide built-in
support for user-defined aggregation functions (UDAFs), enabling the
integration of domain-specific logic. However, for these frameworks to support
\emph{efficient} UDAF execution, the function needs to satisfy a
\emph{homomorphism property}, which ensures that partial results from
independent computations can be merged correctly. Motivated by this problem,
this paper introduces a novel \emph{homomorphism calculus} that can both verify
and refute whether a UDAF is a dataframe homomorphism. If so, our calculus also
enables the construction of a corresponding merge operator which can be used
for incremental computation and parallel execution. We have implemented an
algorithm based on our proposed calculus and evaluate it on real-world UDAFs,
demonstrating that our approach significantly outperforms two leading
synthesizers.

</details>


### [12] [Software Model Checking via Summary-Guided Search (Extended Version)](https://arxiv.org/abs/2508.15137)
*Ruijie Fang,Zachary Kincaid,Thomas Reps*

Main category: cs.PL

TL;DR: GPS is a new software model checker using static analysis summaries for guided search and test generation. It efficiently finds bugs, especially in complex programs, and outperforms leading tools in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Traditional software model checking can be inefficient, especially for programs with complex, input-dependent error paths. There is a need for more scalable and effective model-checking algorithms that can efficiently prove safety or find bugs in complex software.

Method: The paper introduces GPS, a novel software model-checking algorithm. GPS employs a compositional, summary-based static analysis to guide its search of program states. It uses these summaries for pruning infeasible paths and for generating new test cases, thereby exploring new program states. GPS features a two-layered search strategy and introduces an instrumentation technique to ensure refutational completeness.

Result: GPS was benchmarked on a suite of programs from SV-COMP and past literature. It outperformed state-of-the-art model checkers (including top SV-COMP performers), solving more benchmarks and doing so faster.

Conclusion: GPS offers an efficient and effective approach to software model checking, excelling at finding deep, input-dependent bugs while also capable of proving safety. Its two-layered search and static analysis summaries enable superior performance and completeness compared to existing tools.

Abstract: In this work, we describe a new software model-checking algorithm called GPS.
GPS treats the task of model checking a program as a directed search of the
program states, guided by a compositional, summary-based static analysis. The
summaries produced by static analysis are used both to prune away infeasible
paths and to drive test generation to reach new, unexplored program states. GPS
can find both proofs of safety and counter-examples to safety (i.e., inputs
that trigger bugs), and features a novel two-layered search strategy that
renders it particularly efficient at finding bugs in programs featuring long,
input-dependent error paths. To make GPS refutationally complete (in the sense
that it will find an error if one exists, if it is allotted enough time), we
introduce an instrumentation technique and show that it helps GPS achieve
refutation-completeness without sacrificing overall performance. We benchmarked
GPS on a suite of benchmarks including both programs from the Software
Verification Competition (SV-COMP) and from prior literature, and found that
our implementation of GPS outperforms state-of-the-art software model checkers
(including the top performers in SV-COMP ReachSafety-Loops category), both in
terms of the number of benchmarks solved and in terms of running time.

</details>


### [13] [Big-Stop Semantics: A Simple Way to Get the Benefits of Small-Step Semantics in a Big-Step Judgment](https://arxiv.org/abs/2508.15157)
*David M Kahn,Jan Hoffmann,Runming Li*

Main category: cs.PL

TL;DR: The paper presents big-stop semantics, a simple extension to big-step semantics that can describe diverging computations like small-step semantics, without sacrificing its usual simplicity.


<details>
  <summary>Details</summary>
Motivation: Practitioners prefer big-step semantics for their ergonomic advantages, but these semantics traditionally lack the ability to capture certain program behaviors, especially divergence, which small-step semantics can describe.

Method: The paper introduces an extension called big-stop semantics, which adds a few inductive rules to the standard big-step inference rules, allowing the semantics to capture diverging computations without error states. This method is applied to various forms of PCF and an imperative language.

Result: Big-stop semantics successfully extends big-step semantics to account for divergent computations, making it equivalent to the reflexive-transitive closure of small-step transitions. The approach achieves this with minimal changes, preserving ergonomics and avoiding complex alternatives like coinduction or excessive additional rules.

Conclusion: Big-stop semantics provides an elegant and ergonomic solution to capturing divergence in big-step semantics, addressing a known limitation of traditional big-step approaches and simplifying the specification of dynamic program behaviors.

Abstract: As evident in the programming language literature, many practitioners favor
specifying dynamic program behavior using big-step over small-step semantics.
Unlike small-step semantics, which must dwell on every intermediate program
state, big-step semantics conveniently jump directly to the ever-important
result of the computation. Big-step semantics also typically involve fewer
inference rules than their small-step counterparts. However, in exchange for
ergonomics, big-step semantics give up power: Small-step semantics describes
program behaviors that are outside the grasp of big-step semantics, notably
divergence. This work presents a little-known extension of big-step semantics
with inductive definitions that captures diverging computations without
introducing error states. This big-stop semantics is illustrated for typed,
untyped, and effectful variants of PCF, as well as a while-loop-based
imperative language. Big-stop semantics extends the standard big-step inference
rules with a few additional rules to define an evaluation judgment that is
equivalent to the reflexive-transitive closure of small-step transitions. This
simple extension contrasts with other solutions in the literature which
sacrifice ergonomics by introducing many additional inference rules, global
state, and/or less-commonly-understood reasoning principles like coinduction.

</details>


### [14] [Probabilistic Inference for Datalog with Correlated Inputs](https://arxiv.org/abs/2508.15166)
*Jingbo Wang,Shashin Halalingaiah,Weiyi Chen,Chao Wang,Isil Dillig*

Main category: cs.PL

TL;DR: This paper presents Praline, an extension to Datalog designed for accurate probabilistic inference with correlated input facts. It frames inference as a constrained optimization problem and introduces a scalable δ-exact algorithm. Empirical results show improved scalability and precision on complex, real-world tasks.


<details>
  <summary>Details</summary>
Motivation: Existing probabilistic logic programming languages cannot handle statistical correlations among input facts, limiting the precision of inference in many real-world scenarios.

Method: The paper introduces Praline, an extension to Datalog, and formulates the inference task as a constrained optimization problem. It further proposes a scalable δ-exact inference algorithm using constraint solving, static analysis, and iterative refinement.

Result: The proposed algorithms scale effectively to large programs and provide tight probability bounds, as demonstrated in real-world benchmarks including side-channel analysis.

Conclusion: Praline enables precise probabilistic inference under correlated inputs, and the methods introduced achieve both scalability and accuracy in practical applications.

Abstract: Probabilistic extensions of logic programming languages, such as ProbLog,
integrate logical reasoning with probabilistic inference to evaluate
probabilities of output relations; however, prior work does not account for
potential statistical correlations among input facts. This paper introduces
Praline, a new extension to Datalog designed for precise probabilistic
inference in the presence of (partially known) input correlations. We formulate
the inference task as a constrained optimization problem, where the solution
yields sound and precise probability bounds for output facts. However, due to
the complexity of the resulting optimization problem, this approach alone often
does not scale to large programs. To address scalability, we propose a more
efficient $\delta$-exact inference algorithm that leverages constraint solving,
static analysis, and iterative refinement. Our empirical evaluation on
challenging real-world benchmarks, including side-channel analysis,
demonstrates that our method not only scales effectively but also delivers
tight probability bounds.

</details>


### [15] [Exploring the Theory and Practice of Concurrency in the Entity-Component-System Pattern](https://arxiv.org/abs/2508.15264)
*Patrick Redmond,Jonathan Castello,José Manuel Calderón Trilla,Lindsey Kuper*

Main category: cs.PL

TL;DR: The paper formally models the Entity-Component-System (ECS) design pattern, showing that it supports deterministic concurrency, unlike most real-world implementations. The findings suggest opportunities for new ECS frameworks to better exploit concurrency guarantees.


<details>
  <summary>Details</summary>
Motivation: The ECS pattern is widely used in game development for its modularity, flexibility, and performance. However, it remains under-explained and poorly understood outside of specific domains, and explanations are often too implementation-specific or metaphorical. The authors aim to provide a more formal, abstract understanding of ECS.

Method: The authors design a formal model called Core ECS to abstract and generalize the key ideas of the ECS pattern, stripping away implementation specifics. They use this model to analyze program behavior, focusing on deterministic concurrency, and survey real-world ECS frameworks for comparison.

Result: The study identifies a class of Core ECS programs that are deterministic regardless of scheduling, demonstrating that ECS can be used for deterministic concurrent programming. The analysis of real-world ECS frameworks reveals that current implementations do not fully exploit these deterministic concurrency opportunities.

Conclusion: There is significant potential for new ECS frameworks to provide improved deterministic concurrency based on the formal principles identified in Core ECS. Existing frameworks do not yet take full advantage of these possibilities.

Abstract: The Entity-Component-System (ECS) software design pattern, long used in game
development, encourages a clean separation of identity (entities), data
properties (components), and computational behaviors (systems). Programs
written using the ECS pattern are naturally concurrent, and the pattern offers
modularity, flexibility, and performance benefits that have led to a
proliferation of ECS frameworks. Nevertheless, the ECS pattern is little-known
and not well understood outside of a few domains. Existing explanations of the
ECS pattern tend to be mired in the concrete details of particular ECS
frameworks, or they explain the pattern in terms of imperfect metaphors or in
terms of what it is not. We seek a rigorous understanding of the ECS pattern
via the design of a formal model, Core ECS, that abstracts away the details of
specific implementations to reveal the essence of software using the ECS
pattern. We identify a class of Core ECS programs that behave deterministically
regardless of scheduling, enabling use of the ECS pattern as a
deterministic-by-construction concurrent programming model. With Core ECS as a
point of comparison, we then survey several real-world ECS frameworks and find
that they all leave opportunities for deterministic concurrency unexploited.
Our findings point out a space for new ECS implementation techniques that
better leverage such opportunities.

</details>


### [16] [Fair Termination for Resource-Aware Active Objects](https://arxiv.org/abs/2508.15333)
*Francesco Dagnino,Paola Giannini,Violet Ka I Pun,Ulises Torrella*

Main category: cs.PL

TL;DR: The paper introduces a formal calculus and type system for active object models in distributed systems, ensuring resource-awareness and fair termination by combining existing semantics and session-based techniques.


<details>
  <summary>Details</summary>
Motivation: Active object systems are essential for modeling distributed and concurrent systems, which require careful consideration of resource management and termination guarantees. Existing models lack robust resource-aware formalizations that ensure fair termination.

Method: The authors develop a core calculus tailored to resource-aware active objects. They integrate graded semantics and type system techniques (common in sequential programming) with fair termination approaches from synchronous session theory.

Result: The proposed type system soundly guarantees that well-typed programs in this model will fairly terminate; that is, every computation will eventually reach termination under the constraints specified.

Conclusion: This work advances the modeling of distributed computation by providing a formal framework that enforces resource-awareness and fair termination in active object systems. The type system bridges the gap between termination guarantees and resource management.

Abstract: Active object systems are a model of distributed computation that has been
adopted for modelling distributed systems and business process workflows. This
field of modelling is, in essence, concurrent and resource-aware, motivating
the development of resource-aware formalisations on the active object model.
The contributions of this work are the development of a core calculus for
resource-aware active objects together with a type system ensuring that
well-typed programs are fairly terminating, i.e., they can always eventually
terminate. To achieve this, we combine techniques from graded semantics and
type systems, which are quite well understood for sequential programs, with
those for fair termination, which have been developed for synchronous~sessions.

</details>


### [17] [Compositional Symbolic Execution for the Next 700 Memory Models (Extended Version)](https://arxiv.org/abs/2508.15576)
*Andreas Lööw,Seung Hoon Park,Daniele Nantes-Sobrinho,Sacha-Élie Ayoun,Opale Sjöstedt,Philippa Gardner*

Main category: cs.PL

TL;DR: The paper introduces and mechanizes a new formal foundation for compositional symbolic execution tools that are parameterized by different memory models, validated against real models (like C and CHERI), supporting both verification and bug-finding logics, and ensuring standards-based interoperability.


<details>
  <summary>Details</summary>
Motivation: Compositional symbolic execution (CSE) platforms use separation logic (SL) and incorrectness separation logic (ISL) for verification and bug-finding across programming languages. Previous work, especially Gillian, highlights that custom memory models offer flexibility and better support for various languages and analyses. However, there is no formal theoretical foundation for CSE platforms that can be parameterized by memory models.

Method: The authors present a new, mechanized formal foundation for memory-model-parametric CSE platforms. They implement this in the interactive theorem prover Rocq, validate it by instantiating various memory models (including C and CHERI), and ensure their approach supports both SL and ISL based analyses. They ground their formalism in standard definitions to ensure cross-tool compatibility.

Result: The new foundation is mechanized, validated with several concrete memory models, supports both types of analyses (SL and ISL), and is compatible with existing tools and definitions. This proffers a robust, formal backbone for future and existing CSE platforms that require memory model flexibility.

Conclusion: This work fills an important gap by providing a robust, mechanized, and validated formal foundation for memory-model-parametric CSE platforms, supporting a wider range of analyses and ensuing better interoperability and verification reliability.

Abstract: Multiple successful compositional symbolic execution (CSE) tools and
platforms exploit separation logic (SL) for compositional verification and/or
incorrectness separation logic (ISL) for compositional bug-finding, including
VeriFast, Viper, Gillian, CN, and Infer-Pulse. Previous work on the Gillian
platform, the only CSE platform that is parametric on the memory model, meaning
that it can be instantiated to different memory models, suggests that the
ability to use custom memory models allows for more flexibility in supporting
analysis of a wide range of programming languages, for implementing custom
automation, and for improving performance. However, the literature lacks a
satisfactory formal foundation for memory-model-parametric CSE platforms.
  In this paper, inspired by Gillian, we provide a new formal foundation for
memory-model-parametric CSE platforms. Our foundation advances the state of the
art in four ways. First, we mechanise our foundation (in the interactive
theorem prover Rocq). Second, we validate our foundation by instantiating it to
a broad range of memory models, including models for C and CHERI. Third,
whereas previous memory-model-parametric work has only covered SL analyses, we
cover both SL and ISL analyses. Fourth, our foundation is based on standard
definitions of SL and ISL (including definitions of function specification
validity, to ensure sound interoperation with other tools and platforms also
based on standard definitions).

</details>


### [18] [Active Learning for Neurosymbolic Program Synthesis](https://arxiv.org/abs/2508.15750)
*Celeste Barnaby,Qiaochu Chen,Ramya Ramalingam,Osbert Bastani,Isil Dillig*

Main category: cs.PL

TL;DR: This paper introduces SmartLabel, an active learning tool for neurosymbolic program synthesis that uses a new evaluation method (CCE) to effectively handle neural mispredictions. SmartLabel identifies correct programs in 98% of test cases with minimal user interaction, outperforming previous approaches.


<details>
  <summary>Details</summary>
Motivation: Existing active learning techniques for program synthesis struggle when neural components are involved, as their mispredictions can lead to unintended program outputs. There's a need for methods that can robustly handle these neural mispredictions in neurosymbolic synthesis.

Method: The method introduces 'constrained conformal evaluation' (CCE), which iteratively refines the set of candidate programs by accounting for neural network mispredictions and user feedback, aiming for observational equivalence.

Result: SmartLabel was tested on three domains and achieved correct synthesis (ground truth program) for 98% of benchmarks, requiring fewer than 5 user interactions on average, while prior methods succeeded on no more than 65% of benchmarks.

Conclusion: The proposed active learning method, implemented in SmartLabel, substantially outperforms prior techniques in neurosymbolic program synthesis by identifying the correct program in 98% of cases, with fewer user interactions.

Abstract: The goal of active learning for program synthesis is to synthesize the
desired program by asking targeted questions that minimize user interaction.
While prior work has explored active learning in the purely symbolic setting,
such techniques are inadequate for the increasingly popular paradigm of
neurosymbolic program synthesis, where the synthesized program incorporates
neural components. When applied to the neurosymbolic setting, such techniques
can -- and, in practice, do -- return an unintended program due to
mispredictions of neural components. This paper proposes a new active learning
technique that can handle the unique challenges posed by neural network
mispredictions. Our approach is based upon a new evaluation strategy called
constrained conformal evaluation (CCE), which accounts for neural
mispredictions while taking into account user-provided feedback. Our proposed
method iteratively makes CCE more precise until all remaining programs are
guaranteed to be observationally equivalent. We have implemented this method in
a tool called SmartLabel and experimentally evaluated it on three neurosymbolic
domains. Our results demonstrate that SmartLabel identifies the ground truth
program for 98% of the benchmarks, requiring under 5 rounds of user interaction
on average. In contrast, prior techniques for active learning are only able to
converge to the ground truth program for at most 65% of the benchmarks.

</details>
